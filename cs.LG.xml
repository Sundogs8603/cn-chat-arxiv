<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#31946;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21306;&#22495;&#35782;&#21035;&#21644;&#30446;&#26631;&#26816;&#27979;&#20004;&#20010;&#38454;&#27573;&#30340;&#26550;&#26500;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#26816;&#27979;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.15455</link><description>&lt;p&gt;
&#26032;&#30340;&#27169;&#31946;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
New Foggy Object Detecting Model. (arXiv:2401.15455v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#31946;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21306;&#22495;&#35782;&#21035;&#21644;&#30446;&#26631;&#26816;&#27979;&#20004;&#20010;&#38454;&#27573;&#30340;&#26550;&#26500;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#26816;&#27979;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38477;&#20302;&#33021;&#35265;&#24230;&#26465;&#20214;&#19979;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;&#25216;&#26415;&#22312;&#35782;&#21035;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#26102;&#19981;&#22815;&#20934;&#30830;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#31946;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#21306;&#22495;&#35782;&#21035;&#21644;&#22312;&#36825;&#20123;&#21306;&#22495;&#20013;&#26816;&#27979;&#30446;&#26631;&#30340;&#20004;&#38454;&#27573;&#26550;&#26500;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#26816;&#27979;&#26102;&#38388;&#26041;&#38754;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection in reduced visibility has become a prominent research area. The existing techniques are not accurate enough in recognizing objects under such circumstances. This paper introduces a new foggy object detection method through a two-staged architecture of region identification from input images and detecting objects in such regions. The paper confirms notable improvements of the proposed method's accuracy and detection time over existing techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#25554;&#20540;&#21644;&#26680;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#36830;&#32493;&#22788;&#29702;&#25928;&#26524;&#20272;&#35745;&#12290;&#36890;&#36807;&#22686;&#21152;&#29420;&#31435;&#37319;&#26679;&#30340;&#22788;&#29702;&#21644;&#25512;&#26029;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#26469;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#28151;&#28102;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21453;&#20107;&#23454;&#39044;&#27979;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#20845;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.15447</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#25554;&#20540;&#21644;&#26680;&#24179;&#28369;&#20272;&#35745;&#36830;&#32493;&#22788;&#29702;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Continuous Treatment Effect Estimation Using Gradient Interpolation and Kernel Smoothing. (arXiv:2401.15447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#25554;&#20540;&#21644;&#26680;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#36830;&#32493;&#22788;&#29702;&#25928;&#26524;&#20272;&#35745;&#12290;&#36890;&#36807;&#22686;&#21152;&#29420;&#31435;&#37319;&#26679;&#30340;&#22788;&#29702;&#21644;&#25512;&#26029;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#26469;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#28151;&#28102;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21453;&#20107;&#23454;&#39044;&#27979;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#20845;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20010;&#24615;&#21270;&#36830;&#32493;&#22788;&#29702;&#25928;&#26524;&#65288;ICTE&#65289;&#20272;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#35266;&#27979;&#25968;&#25454;&#39044;&#27979;&#20219;&#20309;&#36830;&#32493;&#20540;&#22788;&#29702;&#23545;&#20010;&#20307;&#30340;&#25928;&#26524;&#12290;&#36825;&#20010;&#20272;&#35745;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#35757;&#32451;&#25968;&#25454;&#20013;&#22788;&#29702;&#20998;&#37197;&#19982;&#20010;&#20307;&#21327;&#21464;&#37327;&#30340;&#28508;&#22312;&#28151;&#28102;&#65292;&#32780;&#22312;&#25512;&#26029;ICTE&#26102;&#38656;&#35201;&#23545;&#29420;&#31435;&#37319;&#26679;&#30340;&#22788;&#29702;&#36827;&#34892;&#39044;&#27979;&#12290;&#19982;&#20043;&#21069;&#20381;&#36182;&#20110;&#27491;&#21017;&#21270;&#22120;&#25110;&#19981;&#31283;&#23450;&#30340;GAN&#35757;&#32451;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#20027;&#24352;&#30452;&#25509;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#22686;&#21152;&#29420;&#31435;&#37319;&#26679;&#30340;&#22788;&#29702;&#21644;&#25512;&#26029;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#26469;&#22686;&#24378;&#35757;&#32451;&#20010;&#20307;&#12290; &#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#31574;&#30053;&#25512;&#26029;&#21453;&#20107;&#23454;&#32467;&#26524;&#65306;&#23545;&#25509;&#36817;&#35266;&#23519;&#21040;&#30340;&#22788;&#29702;&#36827;&#34892;&#26799;&#24230;&#25554;&#20540;&#65292;&#20197;&#21450;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#26680;&#24179;&#28369;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20943;&#23567;&#25512;&#26029;&#30340;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21453;&#20107;&#23454;&#39044;&#27979;&#24615;&#33021;&#19978;&#32988;&#36807;&#20845;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the Individualized continuous treatment effect (ICTE) estimation problem where we predict the effect of any continuous-valued treatment on an individual using observational data. The main challenge in this estimation task is the potential confounding of treatment assignment with an individual's covariates in the training data, whereas during inference ICTE requires prediction on independently sampled treatments. In contrast to prior work that relied on regularizers or unstable GAN training, we advocate the direct approach of augmenting training individuals with independently sampled treatments and inferred counterfactual outcomes. We infer counterfactual outcomes using a two-pronged strategy: a Gradient Interpolation for close-to-observed treatments, and a Gaussian Process based Kernel Smoothing which allows us to downweigh high variance inferences. We evaluate our method on five benchmarks and show that our method outperforms six state-of-the-art methods on the counterfactu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#20061;&#31181;&#22522;&#20934;&#22270;&#20998;&#31867;&#27169;&#22411;&#30340;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#22240;&#26524;&#24615;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#21644;&#24212;&#29992;GNN&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.15444</link><description>&lt;p&gt;
&#36808;&#21521;&#22240;&#26524;&#20998;&#31867;&#65306;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Causal Classification: A Comprehensive Study on Graph Neural Networks. (arXiv:2401.15444v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15444
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#20061;&#31181;&#22522;&#20934;&#22270;&#20998;&#31867;&#27169;&#22411;&#30340;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#22240;&#26524;&#24615;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#21644;&#24212;&#29992;GNN&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25506;&#32034;&#19981;&#26029;&#25193;&#22823;&#65292;&#30001;&#20110;&#20854;&#36890;&#29992;&#36924;&#36817;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#23427;&#20204;&#22312;&#22240;&#26524;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#23427;&#20204;&#34987;&#26399;&#26395;&#33021;&#22815;&#26174;&#33879;&#22686;&#24378;&#24120;&#35265;&#30340;&#22522;&#20110;&#22270;&#30340;&#20219;&#21153;&#65292;&#22914;&#20998;&#31867;&#21644;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22240;&#26524;&#22686;&#24378;&#30340;GNN&#26694;&#26550;&#30340;&#30740;&#31350;&#20173;&#28982;&#19981;&#22815;&#28145;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#20061;&#31181;&#22522;&#20934;&#22270;&#20998;&#31867;&#27169;&#22411;&#65292;&#22312;&#36328;&#36234;&#19977;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#19971;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#23427;&#20204;&#30340;&#24378;&#24230;&#21644;&#28789;&#27963;&#24615;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#30830;&#23450;&#22240;&#26524;&#24615;&#23545;GNN&#39044;&#27979;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#38656;&#35201;&#36827;&#19968;&#27493;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#20110;&#28145;&#21270;&#23545;GNN&#22312;&#19981;&#21516;&#25968;&#25454;&#20013;&#24515;&#39046;&#22495;&#30340;&#29702;&#35299;&#21644;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exploration of Graph Neural Networks (GNNs) for processing graph-structured data has expanded, particularly their potential for causal analysis due to their universal approximation capabilities. Anticipated to significantly enhance common graph-based tasks such as classification and prediction, the development of a causally enhanced GNN framework is yet to be thoroughly investigated. Addressing this shortfall, our study delves into nine benchmark graph classification models, testing their strength and versatility across seven datasets spanning three varied domains to discern the impact of causality on the predictive prowess of GNNs. This research offers a detailed assessment of these models, shedding light on their efficiency, and flexibility in different data environments, and highlighting areas needing advancement. Our findings are instrumental in furthering the understanding and practical application of GNNs in diverse datacentric fields
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#27969;&#35328;&#20256;&#25773;&#20849;&#23398;&#20064;&#30340;&#22810;&#21442;&#25968;MRI&#33041;&#32959;&#30244;&#20998;&#21106;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23454;&#29616;&#20102;&#22312;&#19981;&#20849;&#20139;&#31169;&#23494;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21307;&#30103;&#20013;&#24515;&#20043;&#38388;&#30340;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#20849;&#23398;&#20064;&#26469;&#20248;&#21270;&#19981;&#21516;&#31449;&#28857;&#20043;&#38388;&#30340;&#25968;&#25454;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.15434</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#27969;&#35328;&#20256;&#25773;&#20849;&#23398;&#20064;&#30340;&#22810;&#21442;&#25968;MRI&#33041;&#32959;&#30244;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Decentralized Gossip Mutual Learning (GML) for brain tumor segmentation on multi-parametric MRI. (arXiv:2401.15434v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#27969;&#35328;&#20256;&#25773;&#20849;&#23398;&#20064;&#30340;&#22810;&#21442;&#25968;MRI&#33041;&#32959;&#30244;&#20998;&#21106;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23454;&#29616;&#20102;&#22312;&#19981;&#20849;&#20139;&#31169;&#23494;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21307;&#30103;&#20013;&#24515;&#20043;&#38388;&#30340;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#20849;&#23398;&#20064;&#26469;&#20248;&#21270;&#19981;&#21516;&#31449;&#28857;&#20043;&#38388;&#30340;&#25968;&#25454;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#21307;&#30103;&#20013;&#24515;&#20043;&#38388;&#33021;&#22815;&#22312;&#19981;&#20849;&#20139;&#31169;&#23494;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#26381;&#21153;&#22120;&#25925;&#38556;&#21644;&#23616;&#37096;&#25968;&#25454;&#19979;&#24615;&#33021;&#19981;&#20339;&#30340;&#39118;&#38505;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#38598;&#20013;&#24335;&#27169;&#22411;&#32858;&#21512;&#30340;&#29305;&#24615;&#23548;&#33268;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#20013;&#24515;&#21270;&#30340;&#27969;&#35328;&#20256;&#25773;&#20849;&#23398;&#20064;&#65288;GML&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#27969;&#35328;&#21327;&#35758;&#36827;&#34892;&#28857;&#23545;&#28857;&#36890;&#20449;&#12290;&#27492;&#22806;&#65292;GML&#36890;&#36807;&#20849;&#23398;&#20064;&#40723;&#21169;&#27599;&#20010;&#31449;&#28857;&#20248;&#21270;&#20854;&#26412;&#22320;&#27169;&#22411;&#20197;&#24212;&#23545;&#19981;&#21516;&#31449;&#28857;&#20043;&#38388;&#30340;&#25968;&#25454;&#21464;&#21270;&#12290;&#22312;&#20351;&#29992;&#26469;&#33258;BraTS 2021&#25968;&#25454;&#38598;&#30340;&#22235;&#20010;&#20020;&#24202;&#31449;&#28857;&#30340;146&#20010;&#26696;&#20363;&#36827;&#34892;&#32959;&#30244;&#20998;&#21106;&#30340;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GML&#32988;&#36807;&#26412;&#22320;&#27169;&#22411;&#65292;&#24182;&#22312;&#21482;&#26377;25&#65285;&#36890;&#20449;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;FedAvg&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) enables collaborative model training among medical centers without sharing private data. However, traditional FL risks on server failures and suboptimal performance on local data due to the nature of centralized model aggregation. To address these issues, we present Gossip Mutual Learning (GML), a decentralized framework that uses Gossip Protocol for direct peer-to-peer communication. In addition, GML encourages each site to optimize its local model through mutual learning to account for data variations among different sites. For the task of tumor segmentation using 146 cases from four clinical sites in BraTS 2021 dataset, we demonstrated GML outperformed local models and achieved similar performance as FedAvg with only 25% communication overhead.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22823;&#27169;&#22411;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#20687;&#22686;&#24378;&#12289;&#25991;&#26412;&#22686;&#24378;&#21644;&#37197;&#23545;&#25968;&#25454;&#22686;&#24378;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#22823;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#26159;&#35299;&#20915;&#22823;&#27169;&#22411;&#35757;&#32451;&#20013;&#25968;&#25454;&#36136;&#37327;&#19981;&#36275;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.15422</link><description>&lt;p&gt;
&#22823;&#27169;&#22411;&#26102;&#20195;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Data Augmentation in Large Model Era. (arXiv:2401.15422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15422
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22823;&#27169;&#22411;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#20687;&#22686;&#24378;&#12289;&#25991;&#26412;&#22686;&#24378;&#21644;&#37197;&#23545;&#25968;&#25454;&#22686;&#24378;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#22823;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#26159;&#35299;&#20915;&#22823;&#27169;&#22411;&#35757;&#32451;&#20013;&#25968;&#25454;&#36136;&#37327;&#19981;&#36275;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#35821;&#35328;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#36817;&#20284;&#20154;&#31867;&#32423;&#26234;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#28508;&#21147;&#65292;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#22823;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25345;&#32493;&#26356;&#26032;&#65292;&#29616;&#26377;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#20648;&#22791;&#21487;&#33021;&#24456;&#24555;&#29992;&#23613;&#12290;&#36825;&#20010;&#25361;&#25112;&#20652;&#29983;&#20102;&#22823;&#37327;&#20851;&#20110;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#21033;&#29992;&#22823;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36229;&#36234;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;&#26412;&#25991;&#32508;&#21512;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#22823;&#27169;&#22411;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#35814;&#23613;&#22238;&#39038;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#30456;&#20851;&#30740;&#31350;&#20998;&#20026;&#22270;&#20687;&#22686;&#24378;&#12289;&#25991;&#26412;&#22686;&#24378;&#21644;&#37197;&#23545;&#25968;&#25454;&#22686;&#24378;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;&#22823;&#27169;&#22411;&#25968;&#25454;&#22686;&#24378;&#30456;&#20851;&#30340;&#21508;&#31181;&#25968;&#25454;&#21518;&#22788;&#29702;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large models, encompassing large language and diffusion models, have shown exceptional promise in approximating human-level intelligence, garnering significant interest from both academic and industrial spheres. However, the training of these large models necessitates vast quantities of high-quality data, and with continuous updates to these models, the existing reservoir of high-quality data may soon be depleted. This challenge has catalyzed a surge in research focused on data augmentation methods. Leveraging large models, these data augmentation techniques have outperformed traditional approaches. This paper offers an exhaustive review of large model-driven data augmentation methods, adopting a comprehensive perspective. We begin by establishing a classification of relevant studies into three main categories: image augmentation, text augmentation, and paired data augmentation. Following this, we delve into various data post-processing techniques pertinent to large model-based data au
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#22312;Industry 4.0&#32972;&#26223;&#19979;&#21033;&#29992;MATLAB Simulink&#23545;&#24863;&#24212;&#30005;&#26426;&#25925;&#38556;&#36827;&#34892;&#26816;&#27979;&#21644;&#35782;&#21035;&#12290;&#29983;&#25104;&#20102;&#21253;&#21547;&#22235;&#31181;&#25925;&#38556;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#24212;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#21644;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2401.15417</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#25925;&#38556;&#35786;&#26029;&#24863;&#24212;&#30005;&#26426;
&lt;/p&gt;
&lt;p&gt;
Fault Diagnosis on Induction Motor using Machine Learning and Signal Processing. (arXiv:2401.15417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#22312;Industry 4.0&#32972;&#26223;&#19979;&#21033;&#29992;MATLAB Simulink&#23545;&#24863;&#24212;&#30005;&#26426;&#25925;&#38556;&#36827;&#34892;&#26816;&#27979;&#21644;&#35782;&#21035;&#12290;&#29983;&#25104;&#20102;&#21253;&#21547;&#22235;&#31181;&#25925;&#38556;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#24212;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Industry 4.0&#30340;&#32972;&#26223;&#19979;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#26469;&#26816;&#27979;&#21644;&#35782;&#21035;&#24863;&#24212;&#30005;&#26426;&#25925;&#38556;&#26159;&#36991;&#20813;&#24037;&#21378;&#24178;&#25200;&#21644;&#20572;&#26426;&#30340;&#26377;&#20215;&#20540;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;MATLAB Simulink&#23545;&#24863;&#24212;&#30005;&#26426;&#30340;&#25925;&#38556;&#26816;&#27979;&#21644;&#35782;&#21035;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;MATLAB Simulink&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#19977;&#30456;&#24863;&#24212;&#30005;&#26426;&#27169;&#22411;&#65292;&#29983;&#25104;&#20102;&#20581;&#24247;&#21644;&#25925;&#38556;&#30005;&#26426;&#25968;&#25454;&#12290;&#25910;&#38598;&#30340;&#25968;&#25454;&#21253;&#25324;&#23450;&#23376;&#30005;&#27969;&#12289;&#36716;&#23376;&#30005;&#27969;&#12289;&#36755;&#20837;&#21151;&#29575;&#12289;&#28369;&#24046;&#12289;&#36716;&#23376;&#36716;&#36895;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#24863;&#24212;&#30005;&#26426;&#20013;&#20135;&#29983;&#20102;&#22235;&#31181;&#25925;&#38556;&#65306;&#24320;&#36335;&#25925;&#38556;&#12289;&#30701;&#36335;&#25925;&#38556;&#12289;&#36807;&#36733;&#21644;&#26029;&#35010;&#36716;&#23376;&#26465;&#12290;&#25105;&#20204;&#24635;&#20849;&#25910;&#38598;&#20102;15&#19975;&#20010;&#25968;&#25454;&#28857;&#65292;&#20581;&#24247;&#25968;&#25454;&#21644;&#25925;&#38556;&#25968;&#25454;&#27604;&#20363;&#20026;60:40&#12290;&#25105;&#20204;&#24212;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;(FFT)&#26469;&#26816;&#27979;&#21644;&#35782;&#21035;&#20581;&#24247;&#21644;&#19981;&#20581;&#24247;&#29366;&#24577;&#65292;&#24182;&#22312;&#25968;&#25454;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#29305;&#24449;&#12290;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#34987;&#35757;&#32451;&#20110;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection and identification of induction motor faults using machine learning and signal processing is a valuable approach to avoiding plant disturbances and shutdowns in the context of Industry 4.0. In this work, we present a study on the detection and identification of induction motor faults using machine learning and signal processing with MATLAB Simulink. We developed a model of a three-phase induction motor in MATLAB Simulink to generate healthy and faulty motor data. The data collected included stator currents, rotor currents, input power, slip, rotor speed, and efficiency. We generated four faults in the induction motor: open circuit fault, short circuit fault, overload, and broken rotor bars. We collected a total of 150,000 data points with a 60-40% ratio of healthy to faulty motor data. We applied Fast Fourier Transform (FFT) to detect and identify healthy and unhealthy conditions and added a distinctive feature in our data. The generated dataset was trained different mach
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22810;&#20219;&#21153;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#24863;&#24212;&#30005;&#21160;&#26426;&#22768;&#23398;&#21442;&#25968;&#39044;&#27979;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#39069;&#23450;&#22768;&#21387;&#12289;&#21709;&#24230;&#12289;&#31895;&#31961;&#24230;&#21644;&#23574;&#38160;&#24230;&#20316;&#20026;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2401.15377</link><description>&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#23545;&#24863;&#24212;&#30005;&#21160;&#26426;&#22768;&#23398;&#34892;&#20026;&#30340;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Validation of artificial neural networks to model the acoustic behaviour of induction motors. (arXiv:2401.15377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22810;&#20219;&#21153;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#24863;&#24212;&#30005;&#21160;&#26426;&#22768;&#23398;&#21442;&#25968;&#39044;&#27979;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#39069;&#23450;&#22768;&#21387;&#12289;&#21709;&#24230;&#12289;&#31895;&#31961;&#24230;&#21644;&#23574;&#38160;&#24230;&#20316;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#30005;&#21160;&#24863;&#24212;&#30005;&#26426;&#30340;&#22768;&#38899;&#36136;&#37327;&#26159;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#29305;&#21035;&#26159;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20154;&#20204;&#26292;&#38706;&#22312;&#30001;&#22122;&#38899;&#25490;&#25918;&#24341;&#36215;&#30340;&#36523;&#20307;&#21644;&#24515;&#29702;&#19981;&#36866;&#20013;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#20854;&#23545;&#20154;&#20204;&#30340;&#24515;&#29702;&#24433;&#21709;&#26368;&#23567;&#21270;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35780;&#20272;&#22810;&#20219;&#21153;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#24863;&#24212;&#30005;&#21160;&#26426;&#22768;&#23398;&#21442;&#25968;&#21516;&#26102;&#39044;&#27979;&#30340;&#24314;&#27169;&#25216;&#26415;&#30340;&#20351;&#29992;&#12290;&#20351;&#29992;&#20102;&#22810;&#31181;&#36755;&#20837;&#65292;&#20363;&#22914;&#30005;&#21160;&#26426;&#21151;&#29575;&#20449;&#21495;&#30340;&#30005;&#23398;&#37327;&#21644;&#26497;&#25968;&#65292;&#32780;&#19981;&#26159;&#23558;&#30005;&#21160;&#26426;&#30340;&#22122;&#38899;&#19982;&#29615;&#22659;&#22122;&#38899;&#20998;&#31163;&#12290;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#35780;&#20272;&#24863;&#24212;&#30005;&#21160;&#26426;&#30340;&#22768;&#23398;&#36136;&#37327;&#65292;&#36890;&#36807;&#20351;&#29992;&#31561;&#25928;&#22768;&#21387;&#12289;&#21709;&#24230;&#12289;&#31895;&#31961;&#24230;&#21644;&#23574;&#38160;&#24230;&#20316;&#20026;&#36755;&#20986;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25299;&#25169;&#32467;&#26500;&#26469;&#35780;&#20272;&#24863;&#24212;&#30005;&#21160;&#26426;&#30340;&#22768;&#23398;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, the sound quality of electric induction motors is a hot topic in the research field. Specially, due to its high number of applications, the population is exposed to physical and psychological discomfort caused by the noise emission. Therefore, it is necessary to minimise its psychological impact on the population. In this way, the main goal of this work is to evaluate the use of multitask artificial neural networks as a modelling technique for simultaneously predicting psychoacoustic parameters of induction motors. Several inputs are used, such as, the electrical magnitudes of the motor power signal and the number of poles, instead of separating the noise of the electric motor from the environmental noise. Two different kind of artificial neural networks are proposed to evaluate the acoustic quality of induction motors, by using the equivalent sound pressure, the loudness, the roughness and the sharpness as outputs. Concretely, two different topologies have been con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LARA&#30340;&#33258;&#21160;&#20998;&#26512;&#31995;&#32479;&#65292;&#29992;&#20110;&#38271;&#26399;&#20135;&#21069;&#30005;&#23376;&#32974;&#20799;&#24515;&#29575;&#30417;&#27979;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22788;&#29702;&#38271;&#26399;&#30340;FHR&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#23545;&#32974;&#20799;&#29366;&#24577;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.15337</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#20449;&#24687;&#34701;&#21512;&#30340;&#38271;&#26399;&#20135;&#21069;&#30005;&#23376;&#32974;&#20799;&#24515;&#29575;&#30417;&#27979;&#20581;&#24247;&#30417;&#27979;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning with Information Fusion and Model Interpretation for Health Monitoring of Fetus based on Long-term Prenatal Electronic Fetal Heart Rate Monitoring Data. (arXiv:2401.15337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LARA&#30340;&#33258;&#21160;&#20998;&#26512;&#31995;&#32479;&#65292;&#29992;&#20110;&#38271;&#26399;&#20135;&#21069;&#30005;&#23376;&#32974;&#20799;&#24515;&#29575;&#30417;&#27979;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22788;&#29702;&#38271;&#26399;&#30340;FHR&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#23545;&#32974;&#20799;&#29366;&#24577;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32974;&#20799;&#24515;&#29575;&#65288;FHR&#65289;&#30340;&#38271;&#26399;&#30417;&#27979;&#22312;&#20135;&#21069;&#26399;&#38388;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20854;&#20013;&#30005;&#23376;FHR&#30417;&#27979;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#19982;&#30701;&#26399;&#30417;&#27979;&#30456;&#27604;&#65292;&#36825;&#31181;&#36830;&#32493;&#30417;&#27979;&#21487;&#20197;&#25910;&#38598;&#26356;&#38271;&#26102;&#38388;&#30340;&#32974;&#20799;&#24515;&#29575;&#25968;&#25454;&#65292;&#20174;&#32780;&#26356;&#20840;&#38754;&#22320;&#20102;&#35299;&#32974;&#20799;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#38271;&#26399;&#20135;&#21069;&#32974;&#20799;&#24515;&#29575;&#30417;&#27979;&#30340;&#35299;&#37322;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#32570;&#20047;&#30456;&#24212;&#30340;&#20020;&#24202;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;&#36830;&#32493;&#30417;&#27979;&#20135;&#29983;&#30340;&#22823;&#37327;&#25968;&#25454;&#22312;&#25163;&#21160;&#20998;&#26512;&#26102;&#23545;&#20020;&#24202;&#24037;&#20316;&#36896;&#25104;&#20102;&#37325;&#22823;&#36127;&#25285;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LARA&#65288;&#38271;&#26399;&#20135;&#21069;&#39118;&#38505;&#20998;&#26512;&#31995;&#32479;&#65289;&#30340;&#33258;&#21160;&#20998;&#26512;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#27861;&#12290;LARA&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#25104;&#29087;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#65292;&#23427;&#23558;&#38271;&#26399;&#30340;FHR&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term fetal heart rate (FHR) monitoring during the antepartum period, increasingly popularized by electronic FHR monitoring, represents a growing approach in FHR monitoring. This kind of continuous monitoring, in contrast to the short-term one, collects an extended period of fetal heart data. This offers a more comprehensive understanding of fetus's conditions. However, the interpretation of long-term antenatal fetal heart monitoring is still in its early stages, lacking corresponding clinical standards. Furthermore, the substantial amount of data generated by continuous monitoring imposes a significant burden on clinical work when analyzed manually. To address above challenges, this study develops an automatic analysis system named LARA (Long-term Antepartum Risk Analysis system) for continuous FHR monitoring, combining deep learning and information fusion methods. LARA's core is a well-established convolutional neural network (CNN) model. It processes long-term FHR data as input 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;L-AutoDA&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35774;&#35745;&#20915;&#31574;&#22411;&#23545;&#25239;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36845;&#20195;&#20132;&#20114;&#65292;L-AutoDA&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#31454;&#20105;&#24615;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#26174;&#31034;&#20986;&#22312;&#25104;&#21151;&#29575;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.15335</link><description>&lt;p&gt;
L-AutoDA: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#20915;&#31574;&#22411;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks. (arXiv:2401.15335v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;L-AutoDA&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35774;&#35745;&#20915;&#31574;&#22411;&#23545;&#25239;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36845;&#20195;&#20132;&#20114;&#65292;L-AutoDA&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#31454;&#20105;&#24615;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#26174;&#31034;&#20986;&#22312;&#25104;&#21151;&#29575;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23545;&#25239;&#25915;&#20987;&#23545;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#21644;&#23433;&#20840;&#24615;&#25552;&#20986;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#20915;&#31574;&#22411;&#25915;&#20987;&#21482;&#38656;&#35201;&#27169;&#22411;&#30340;&#20915;&#31574;&#21453;&#39304;&#65292;&#32780;&#19981;&#38656;&#35201;&#35814;&#32454;&#30340;&#27010;&#29575;&#25110;&#20998;&#25968;&#65292;&#22240;&#27492;&#29305;&#21035;&#38590;&#20197;&#38450;&#24481;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;L-AutoDA&#65288;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20915;&#31574;&#22411;&#23545;&#25239;&#25915;&#20987;&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#33258;&#21160;&#35774;&#35745;&#36825;&#20123;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#36827;&#21270;&#26694;&#26550;&#20013;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36845;&#20195;&#20132;&#20114;&#65292;L-AutoDA&#33021;&#22815;&#39640;&#25928;&#22320;&#33258;&#21160;&#35774;&#35745;&#20986;&#31454;&#20105;&#24615;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#20943;&#23569;&#20154;&#24037;&#24037;&#20316;&#37327;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;L-AutoDA&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#20986;&#22312;&#25104;&#21151;&#29575;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#25239;&#25915;&#20987;&#29983;&#25104;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of machine learning, adversarial attacks present a significant challenge to model robustness and security. Decision-based attacks, which only require feedback on the decision of a model rather than detailed probabilities or scores, are particularly insidious and difficult to defend against. This work introduces L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), a novel approach leveraging the generative capabilities of Large Language Models (LLMs) to automate the design of these attacks. By iteratively interacting with LLMs in an evolutionary framework, L-AutoDA automatically designs competitive attack algorithms efficiently without much human effort. We demonstrate the efficacy of L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline methods in both success rate and computational efficiency. Our findings underscore the potential of language models as tools for adversarial attack generation and highli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#36793;&#30028;&#27861;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#25214;&#21040;&#21487;&#35777;&#26126;&#26368;&#20248;&#31232;&#30095;&#29983;&#23384;&#26641;&#27169;&#22411;&#65292;&#23545;&#20110;&#28041;&#21450;&#20154;&#31867;&#20581;&#24247;&#30340;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#20998;&#26512;&#21644;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.15330</link><description>&lt;p&gt;
&#26368;&#20248;&#31232;&#30095;&#29983;&#23384;&#26641;
&lt;/p&gt;
&lt;p&gt;
Optimal Sparse Survival Trees. (arXiv:2401.15330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#36793;&#30028;&#27861;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#25214;&#21040;&#21487;&#35777;&#26126;&#26368;&#20248;&#31232;&#30095;&#29983;&#23384;&#26641;&#27169;&#22411;&#65292;&#23545;&#20110;&#28041;&#21450;&#20154;&#31867;&#20581;&#24247;&#30340;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#20998;&#26512;&#21644;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28041;&#21450;&#20154;&#31867;&#20581;&#24247;&#30340;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#20998;&#26512;&#21644;&#20915;&#31574;&#20013;&#65292;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#21307;&#29983;&#12289;&#21307;&#38498;&#12289;&#21046;&#33647;&#20844;&#21496;&#21644;&#29983;&#29289;&#25216;&#26415;&#20844;&#21496;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#20854;&#21560;&#24341;&#20154;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25429;&#25417;&#22797;&#26434;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#25104;&#29983;&#23384;&#26641;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#65288;&#25110;&#36138;&#23146;&#65289;&#31639;&#27861;&#65292;&#23384;&#22312;&#29983;&#25104;&#27425;&#20248;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#36793;&#30028;&#27861;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#25214;&#21040;&#21487;&#35777;&#26126;&#26368;&#20248;&#31232;&#30095;&#29983;&#23384;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability is crucial for doctors, hospitals, pharmaceutical companies and biotechnology corporations to analyze and make decisions for high stakes problems that involve human health. Tree-based methods have been widely adopted for \textit{survival analysis} due to their appealing interpretablility and their ability to capture complex relationships. However, most existing methods to produce survival trees rely on heuristic (or greedy) algorithms, which risk producing sub-optimal models. We present a dynamic-programming-with-bounds approach that finds provably-optimal sparse survival tree models, frequently in only a few seconds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26032;&#22411;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#30340;&#23450;&#20301;&#38382;&#39064;&#65292;&#36890;&#36807;&#32771;&#34385;&#30005;&#21147;&#32593;&#25968;&#25454;&#30340;&#38750;&#27431;&#27663;&#31354;&#38388;&#23646;&#24615;&#20013;&#30340;&#22266;&#26377;&#25299;&#25169;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#22270;&#27874;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25915;&#20987;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15321</link><description>&lt;p&gt;
&#22312;&#32771;&#34385;&#21040;&#19981;&#23436;&#25972;&#30340;&#25299;&#25169;&#20449;&#24687;&#30340;&#21069;&#25552;&#19979;&#23450;&#20301;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#30340;&#30740;&#31350;&#65306;&#19968;&#31181;&#26102;&#31354;&#22270;&#27874;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Localization of Dummy Data Injection Attacks in Power Systems Considering Incomplete Topological Information: A Spatio-Temporal Graph Wavelet Convolutional Neural Network Approach. (arXiv:2401.15321v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26032;&#22411;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#30340;&#23450;&#20301;&#38382;&#39064;&#65292;&#36890;&#36807;&#32771;&#34385;&#30005;&#21147;&#32593;&#25968;&#25454;&#30340;&#38750;&#27431;&#27663;&#31354;&#38388;&#23646;&#24615;&#20013;&#30340;&#22266;&#26377;&#25299;&#25169;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#22270;&#27874;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25915;&#20987;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#22411;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#31283;&#23450;&#36816;&#34892;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#30001;&#20110;&#27880;&#20837;&#24694;&#24847;&#25968;&#25454;&#21644;&#21512;&#27861;&#25968;&#25454;&#20043;&#38388;&#30340;&#27431;&#27663;&#31354;&#38388;&#36317;&#31163;&#26497;&#23567;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#24456;&#38590;&#20934;&#30830;&#26816;&#27979;&#36825;&#20123;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#24120;&#22312;&#25915;&#20987;&#21518;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25110;&#20165;&#20381;&#36182;&#20110;&#27431;&#27663;&#31354;&#38388;&#29305;&#24449;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#30005;&#21147;&#32593;&#25968;&#25454;&#30340;&#38750;&#27431;&#27663;&#31354;&#38388;&#23646;&#24615;&#20013;&#30340;&#22266;&#26377;&#25299;&#25169;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#25915;&#20987;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#26032;&#22411;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#30340;&#22797;&#26434;&#25968;&#23398;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of novel the dummy data injection attack (DDIA) poses a severe threat to the secure and stable operation of power systems. These attacks are particularly perilous due to the minimal Euclidean spatial separation between the injected malicious data and legitimate data, rendering their precise detection challenging using conventional distance-based methods. Furthermore, existing research predominantly focuses on various machine learning techniques, often analyzing the temporal data sequences post-attack or relying solely on Euclidean spatial characteristics. Unfortunately, this approach tends to overlook the inherent topological correlations within the non-Euclidean spatial attributes of power grid data, consequently leading to diminished accuracy in attack localization. To address this issue, this study takes a comprehensive approach. Initially, it examines the underlying principles of these new DDIAs on power systems. Here, an intricate mathematical model of the DDIA is de
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#21943;&#28293;&#25216;&#26415;&#30456;&#32467;&#21512;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#21644;3D&#39640;&#26031;&#21943;&#28293;&#65292;&#21487;&#20197;&#22312;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#36896;&#20986;&#26080;&#21487;&#27604;&#25311;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#23454;&#29616;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#31649;&#29702;&#21644;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2401.15318</link><description>&lt;p&gt;
&#39640;&#26031;&#21943;&#28293;&#65306;&#21033;&#29992;&#39640;&#26031;&#39128;&#33853;&#21160;&#24577;&#21512;&#25104;&#27969;&#20307;
&lt;/p&gt;
&lt;p&gt;
Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting. (arXiv:2401.15318v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15318
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#21943;&#28293;&#25216;&#26415;&#30456;&#32467;&#21512;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#21644;3D&#39640;&#26031;&#21943;&#28293;&#65292;&#21487;&#20197;&#22312;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#36896;&#20986;&#26080;&#21487;&#27604;&#25311;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#23454;&#29616;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#31649;&#29702;&#21644;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#19982;3D&#39640;&#26031;&#21943;&#28293;&#65288;3DGS&#65289;&#30456;&#32467;&#21512;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#22312;&#20351;&#29992;3DGS&#37325;&#24314;&#30340;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#24314;&#26032;&#25928;&#26524;&#12290;&#21033;&#29992;&#39640;&#26031;&#21943;&#28293;&#21644;&#22522;&#20110;&#20301;&#32622;&#30340;&#21160;&#21147;&#23398;&#65288;PBD&#65289;&#22312;&#24213;&#23618;&#34920;&#31034;&#20013;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#20197;&#36830;&#36143;&#30340;&#26041;&#24335;&#31649;&#29702;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#12290;&#31867;&#20284;&#20110;&#39640;&#26031;&#30528;&#33394;&#22120;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#27861;&#32447;&#22686;&#24378;&#27599;&#20010;&#39640;&#26031;&#26680;&#65292;&#23558;&#26680;&#30340;&#26041;&#21521;&#19982;&#34920;&#38754;&#27861;&#32447;&#23545;&#40784;&#65292;&#20197;&#25913;&#36827;PBD&#27169;&#25311;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#28040;&#38500;&#20102;&#22266;&#20307;&#26059;&#36716;&#21464;&#24418;&#20135;&#29983;&#30340;&#23574;&#23792;&#22122;&#22768;&#12290;&#23427;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#28210;&#26579;&#38598;&#25104;&#21040;&#27969;&#20307;&#30340;&#21160;&#24577;&#34920;&#38754;&#21453;&#23556;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#30495;&#23454;&#22320;&#22797;&#29616;&#21160;&#24577;&#27969;&#20307;&#19978;&#30340;&#34920;&#38754;&#20142;&#28857;&#65292;&#24182;&#20419;&#36827;&#22330;&#26223;&#23545;&#35937;&#19982;&#27969;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian splatting and position-based dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views. For more information, pl
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#27010;&#29575;&#22522;&#20934;&#65292;&#29992;&#20110;&#27604;&#36739;AI&#22825;&#27668;&#27169;&#22411;&#30340;&#27010;&#29575;&#25216;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#28382;&#21518;&#38598;&#21512;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#22810;&#20010;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#24182;&#19982;&#25805;&#20316;&#22522;&#20934;&#36827;&#34892;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2401.15305</link><description>&lt;p&gt;
AI&#22825;&#27668;&#27169;&#22411;&#30340;&#23454;&#29992;&#27010;&#29575;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Practical Probabilistic Benchmark for AI Weather Models. (arXiv:2401.15305v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15305
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#27010;&#29575;&#22522;&#20934;&#65292;&#29992;&#20110;&#27604;&#36739;AI&#22825;&#27668;&#27169;&#22411;&#30340;&#27010;&#29575;&#25216;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#28382;&#21518;&#38598;&#21512;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#22810;&#20010;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#24182;&#19982;&#25805;&#20316;&#22522;&#20934;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22825;&#27668;&#30340;&#28151;&#27788;&#24615;&#36136;&#65292;&#22825;&#27668;&#39044;&#25253;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#30340;&#20998;&#24067;&#32780;&#19981;&#26159;&#20570;&#20986;&#21333;&#20010;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#22810;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#22825;&#27668;&#27169;&#22411;&#65292;&#22768;&#31216;&#22312;&#25216;&#26415;&#19978;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22823;&#22810;&#20351;&#29992;&#30830;&#23450;&#24615;&#25216;&#33021;&#35780;&#20998;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#23545;&#23427;&#20204;&#30340;&#27010;&#29575;&#25216;&#33021;&#30693;&#20043;&#29978;&#23569;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#27010;&#29575;&#24847;&#20041;&#19978;&#20844;&#24179;&#27604;&#36739;AI&#22825;&#27668;&#27169;&#22411;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#38598;&#21512;&#21021;&#22987;&#21270;&#30340;&#36873;&#25321;&#12289;&#29366;&#24577;&#23450;&#20041;&#21644;&#22122;&#22768;&#27880;&#20837;&#26041;&#27861;&#30340;&#21464;&#21270;&#20250;&#20135;&#29983;&#28151;&#28102;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#25152;&#28041;&#21450;&#30340;&#25968;&#25454;&#37327;&#65292;&#21363;&#20351;&#26159;&#33719;&#24471;&#38598;&#21512;&#39044;&#25253;&#30340;&#22522;&#32447;&#20063;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#24037;&#31243;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20960;&#21313;&#24180;&#21069;&#30340;&#27010;&#24565; - &#28382;&#21518;&#38598;&#21512;&#65292;&#36890;&#36807;&#19968;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#30830;&#23450;&#24615;&#39044;&#27979;&#24211;&#26500;&#24314;&#38598;&#21512;&#12290;&#36825;&#20801;&#35768;&#23545;&#39046;&#20808;&#30340;AI&#22825;&#27668;&#27169;&#22411;&#30340;&#27010;&#29575;&#25216;&#33021;&#36827;&#34892;&#31532;&#19968;&#20010;&#26080;&#21442;&#25968;&#27604;&#36739;&#65292;&#24182;&#19982;&#25805;&#20316;&#22522;&#20934;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the weather is chaotic, forecasts aim to predict the distribution of future states rather than make a single prediction. Recently, multiple data driven weather models have emerged claiming breakthroughs in skill. However, these have mostly been benchmarked using deterministic skill scores, and little is known about their probabilistic skill. Unfortunately, it is hard to fairly compare AI weather models in a probabilistic sense, since variations in choice of ensemble initialization, definition of state, and noise injection methodology become confounding. Moreover, even obtaining ensemble forecast baselines is a substantial engineering challenge given the data volumes involved. We sidestep both problems by applying a decades-old idea -- lagged ensembles -- whereby an ensemble can be constructed from a moderately-sized library of deterministic forecasts. This allows the first parameter-free intercomparison of leading AI weather models' probabilistic skill against an operational base
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#26368;&#23567;&#22343;&#26041;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;LMS-GNN&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#32447;&#20272;&#35745;&#26102;&#21464;&#22270;&#20449;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;LMS-GNN&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#22312;&#32447;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.15304</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#26368;&#23567;&#22343;&#26041;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22312;&#32447;&#22270;&#20449;&#21495;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adaptive Least Mean Squares Graph Neural Networks and Online Graph Signal Estimation. (arXiv:2401.15304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15304
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#26368;&#23567;&#22343;&#26041;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;LMS-GNN&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#32447;&#20272;&#35745;&#26102;&#21464;&#22270;&#20449;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;LMS-GNN&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#22312;&#32447;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#22810;&#24212;&#29992;&#20013;&#65292;&#20174;&#26377;&#22122;&#22768;&#30340;&#37096;&#20998;&#35266;&#27979;&#20013;&#22312;&#32447;&#39044;&#27979;&#21516;&#26102;&#23384;&#22312;&#20110;&#31354;&#38388;&#21644;&#26102;&#38388;&#20013;&#30340;&#22810;&#21464;&#37327;&#20449;&#21495;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#32447;&#20272;&#35745;&#26102;&#21464;&#22270;&#20449;&#21495;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#26368;&#23567;&#22343;&#26041;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;LMS-GNN&#65289;&#12290;LMS-GNN&#26088;&#22312;&#25429;&#25417;&#26102;&#38388;&#21464;&#21270;&#65292;&#24182;&#22312;&#20449;&#21495;&#21463;&#21040;&#22122;&#22768;&#21644;&#32570;&#22833;&#20540;&#24178;&#25200;&#30340;&#26465;&#20214;&#19979;&#24314;&#31435;&#31354;&#38388;&#26102;&#38388;&#20132;&#20114;&#12290;LMS-GNN&#26159;&#33258;&#36866;&#24212;&#22270;&#28388;&#27874;&#22120;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#32467;&#21512;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#20013;&#65292;LMS-GNN&#30340;&#21069;&#21521;&#20256;&#25773;&#31867;&#20284;&#20110;&#33258;&#36866;&#24212;&#22270;&#28388;&#27874;&#22120;&#65292;&#20854;&#20013;&#36755;&#20986;&#22522;&#20110;&#35266;&#27979;&#21644;&#39044;&#27979;&#20043;&#38388;&#30340;&#35823;&#24046;&#65292;&#31867;&#20284;&#20110;GNN&#12290;&#28388;&#27874;&#22120;&#31995;&#25968;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#65292;&#31867;&#20284;&#20110;GNN&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#28201;&#24230;&#25968;&#25454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;LMS-GNN&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#22312;&#32447;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The online prediction of multivariate signals, existing simultaneously in space and time, from noisy partial observations is a fundamental task in numerous applications. We propose an efficient Neural Network architecture for the online estimation of time-varying graph signals named the Adaptive Least Mean Squares Graph Neural Networks (LMS-GNN). LMS-GNN aims to capture the time variation and bridge the cross-space-time interactions under the condition that signals are corrupted by noise and missing values. The LMS-GNN is a combination of adaptive graph filters and Graph Neural Networks (GNN). At each time step, the forward propagation of LMS-GNN is similar to adaptive graph filters where the output is based on the error between the observation and the prediction similar to GNN. The filter coefficients are updated via backpropagation as in GNN. Experimenting on real-world temperature data reveals that our LMS-GNN achieves more accurate online predictions compared to graph-based methods
&lt;/p&gt;</description></item><item><title>SupplyGraph&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#25968;&#25454;&#65292;&#29992;&#20110;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2401.15299</link><description>&lt;p&gt;
SupplyGraph: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks. (arXiv:2401.15299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15299
&lt;/p&gt;
&lt;p&gt;
SupplyGraph&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#25968;&#25454;&#65292;&#29992;&#20110;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#22914;&#36816;&#36755;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;GNNs&#24212;&#29992;&#20110;&#20379;&#24212;&#38142;&#32593;&#32476;&#26041;&#38754;&#65292;&#30446;&#21069;&#23578;&#32570;&#20047;&#30740;&#31350;&#12290;&#20379;&#24212;&#38142;&#32593;&#32476;&#22312;&#32467;&#26500;&#19978;&#31867;&#20284;&#20110;&#22270;&#24418;&#65292;&#20351;&#20854;&#25104;&#20026;&#24212;&#29992;GNN&#26041;&#27861;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#36825;&#20026;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#24320;&#36767;&#20102;&#26080;&#38480;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#27492;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#22312;&#20110;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#20419;&#36827;&#20351;&#29992;GNN&#26469;&#30740;&#31350;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#30340;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#29992;&#20110;&#29983;&#20135;&#30446;&#30340;&#30340;&#20379;&#24212;&#38142;&#35268;&#21010;&#30340;&#26102;&#38388;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have gained traction across different domains such as transportation, bio-informatics, language processing, and computer vision. However, there is a noticeable absence of research on applying GNNs to supply chain networks. Supply chain networks are inherently graph-like in structure, making them prime candidates for applying GNN methodologies. This opens up a world of possibilities for optimizing, predicting, and solving even the most complex supply chain problems. A major setback in this approach lies in the absence of real-world benchmark datasets to facilitate the research and resolution of supply chain problems using GNNs. To address the issue, we present a real-world benchmark dataset for temporal tasks, obtained from one of the leading FMCG companies in Bangladesh, focusing on supply chain planning for production purposes. The dataset includes temporal data as node features to enable sales predictions, production planning, and the identification of fa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22810;&#35302;&#21457;&#21518;&#38376;&#25915;&#20987;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23041;&#32961;&#12290;&#36890;&#36807;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#35302;&#21457;&#25915;&#20987;&#65292;&#21253;&#25324;&#24182;&#34892;&#12289;&#39034;&#24207;&#21644;&#28151;&#21512;&#25915;&#20987;&#65292;&#25991;&#31456;&#25581;&#31034;&#20102;&#19981;&#21516;&#35302;&#21457;&#22120;&#23545;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#20849;&#23384;&#12289;&#35206;&#20889;&#21644;&#20132;&#21449;&#28608;&#27963;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#21333;&#35302;&#21457;&#25915;&#20987;&#23481;&#26131;&#24341;&#36215;&#35206;&#20889;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.15295</link><description>&lt;p&gt;
&#22810;&#35302;&#21457;&#21518;&#38376;&#25915;&#20987;&#65306;&#26356;&#22810;&#35302;&#21457;&#22120;&#65292;&#26356;&#22810;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Multi-Trigger Backdoor Attacks: More Triggers, More Threats. (arXiv:2401.15295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22810;&#35302;&#21457;&#21518;&#38376;&#25915;&#20987;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23041;&#32961;&#12290;&#36890;&#36807;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#35302;&#21457;&#25915;&#20987;&#65292;&#21253;&#25324;&#24182;&#34892;&#12289;&#39034;&#24207;&#21644;&#28151;&#21512;&#25915;&#20987;&#65292;&#25991;&#31456;&#25581;&#31034;&#20102;&#19981;&#21516;&#35302;&#21457;&#22120;&#23545;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#20849;&#23384;&#12289;&#35206;&#20889;&#21644;&#20132;&#21449;&#28608;&#27963;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#21333;&#35302;&#21457;&#25915;&#20987;&#23481;&#26131;&#24341;&#36215;&#35206;&#20889;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#65288;&#39044;&#65289;&#35757;&#32451;&#21644;&#37096;&#32626;&#30340;&#20027;&#35201;&#23041;&#32961;&#12290;&#23613;&#31649;&#21518;&#38376;&#25915;&#20987;&#22312;&#19968;&#20123;&#30740;&#31350;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#25506;&#35752;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#37117;&#38598;&#20013;&#22312;&#20351;&#29992;&#21333;&#20010;&#31867;&#22411;&#30340;&#35302;&#21457;&#22120;&#26469;&#27745;&#26579;&#25968;&#25454;&#38598;&#30340;&#21333;&#35302;&#21457;&#25915;&#20987;&#19978;&#12290;&#21487;&#20197;&#35828;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#21518;&#38376;&#25915;&#20987;&#21487;&#33021;&#26356;&#21152;&#22797;&#26434;&#65292;&#20363;&#22914;&#65292;&#21516;&#19968;&#25968;&#25454;&#38598;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#23545;&#25163;&#65292;&#22914;&#26524;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#36739;&#39640;&#30340;&#20215;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22810;&#35302;&#21457;&#25915;&#20987;&#35774;&#32622;&#19979;&#21518;&#38376;&#25915;&#20987;&#30340;&#23454;&#38469;&#23041;&#32961;&#65292;&#22810;&#20010;&#23545;&#25163;&#21033;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#35302;&#21457;&#22120;&#26469;&#27745;&#26579;&#21516;&#19968;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25552;&#20986;&#21644;&#30740;&#31350;&#24182;&#34892;&#12289;&#39034;&#24207;&#21644;&#28151;&#21512;&#25915;&#20987;&#36825;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#35302;&#21457;&#25915;&#20987;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#21516;&#35302;&#21457;&#22120;&#23545;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#20849;&#23384;&#12289;&#35206;&#20889;&#21644;&#20132;&#21449;&#28608;&#27963;&#25928;&#26524;&#30340;&#37325;&#35201;&#35748;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21333;&#35302;&#21457;&#25915;&#20987;&#24448;&#24448;&#23481;&#26131;&#24341;&#36215;&#35206;&#20889;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks have emerged as a primary threat to (pre-)training and deployment of deep neural networks (DNNs). While backdoor attacks have been extensively studied in a body of works, most of them were focused on single-trigger attacks that poison a dataset using a single type of trigger. Arguably, real-world backdoor attacks can be much more complex, e.g., the existence of multiple adversaries for the same dataset if it is of high value. In this work, we investigate the practical threat of backdoor attacks under the setting of \textbf{multi-trigger attacks} where multiple adversaries leverage different types of triggers to poison the same dataset. By proposing and investigating three types of multi-trigger attacks, including parallel, sequential, and hybrid attacks, we provide a set of important understandings of the coexisting, overwriting, and cross-activating effects between different triggers on the same dataset. Moreover, we show that single-trigger attacks tend to cause over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#26469;&#35299;&#20915;&#29699;&#38754;&#19978;&#30340;&#25955;&#28857;&#25968;&#25454;&#25311;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#21152;&#26435;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#36924;&#36817;&#24615;&#33021;&#65292;&#25104;&#21151;&#25512;&#23548;&#20986;&#20102;&#24102;&#26435;&#37325;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#26368;&#20248;&#35823;&#24046;&#20272;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#19968;&#20123;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.15294</link><description>&lt;p&gt;
&#29699;&#38754;&#19978;&#25955;&#28857;&#25968;&#25454;&#25311;&#21512;&#30340;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integral Operator Approaches for Scattered Data Fitting on Spheres. (arXiv:2401.15294v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#26469;&#35299;&#20915;&#29699;&#38754;&#19978;&#30340;&#25955;&#28857;&#25968;&#25454;&#25311;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#21152;&#26435;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#36924;&#36817;&#24615;&#33021;&#65292;&#25104;&#21151;&#25512;&#23548;&#20986;&#20102;&#24102;&#26435;&#37325;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#26368;&#20248;&#35823;&#24046;&#20272;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#19968;&#20123;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20102;&#29699;&#38754;&#19978;&#30340;&#25955;&#28857;&#25968;&#25454;&#25311;&#21512;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#21152;&#26435;&#35889;&#28388;&#27874;&#31639;&#27861;&#65288;&#21253;&#25324;Tikhonov&#27491;&#21017;&#21270;&#12289;Landaweber&#36845;&#20195;&#12289;&#35889;&#25130;&#26029;&#21644;&#36845;&#20195;Tikhonov&#65289;&#22312;&#25311;&#21512;&#21487;&#33021;&#23384;&#22312;&#30340;&#26080;&#30028;&#38543;&#26426;&#22122;&#22768;&#30340;&#22024;&#26434;&#25968;&#25454;&#26102;&#30340;&#36924;&#36817;&#24615;&#33021;&#12290;&#20026;&#20102;&#20998;&#26512;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#65292;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#25955;&#28857;&#25968;&#25454;&#25311;&#21512;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#37319;&#26679;&#19981;&#31561;&#24335;&#26041;&#27861;&#21644;&#35268;&#33539;&#38598;&#26041;&#27861;&#30340;&#24310;&#20280;&#12290;&#36890;&#36807;&#25552;&#20379;&#31639;&#23376;&#24046;&#24322;&#21644;&#25968;&#20540;&#31215;&#20998;&#35268;&#21017;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#25512;&#23548;&#20986;&#24102;&#26435;&#37325;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;Sobolev&#31867;&#22411;&#35823;&#24046;&#20272;&#35745;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#35823;&#24046;&#20272;&#35745;&#19981;&#21463;&#25991;&#29486;&#20013;Tikhonov&#27491;&#21017;&#21270;&#30340;&#39281;&#21644;&#29616;&#35937;&#12289;&#29616;&#26377;&#35823;&#24046;&#20998;&#26512;&#20013;&#30340;&#26412;&#22320;&#31354;&#38388;&#23631;&#38556;&#21644;&#19981;&#21516;&#23884;&#20837;&#31354;&#38388;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#32780;&#27835;&#20043;&#30340;&#26041;&#26696;&#65292;&#20197;&#25552;&#21319;&#21152;&#26435;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on scattered data fitting problems on spheres. We study the approximation performance of a class of weighted spectral filter algorithms, including Tikhonov regularization, Landaweber iteration, spectral cut-off, and iterated Tikhonov, in fitting noisy data with possibly unbounded random noise. For the analysis, we develop an integral operator approach that can be regarded as an extension of the widely used sampling inequality approach and norming set method in the community of scattered data fitting. After providing an equivalence between the operator differences and quadrature rules, we succeed in deriving optimal Sobolev-type error estimates of weighted spectral filter algorithms. Our derived error estimates do not suffer from the saturation phenomenon for Tikhonov regularization in the literature, native-space-barrier for existing error analysis and adapts to different embedding spaces. We also propose a divide-and-conquer scheme to equip weighted spectral filter 
&lt;/p&gt;</description></item><item><title>SkipViT&#36890;&#36807;&#20196;&#29260;&#32423;&#36339;&#36291;&#36830;&#25509;&#23558;&#19981;&#37325;&#35201;&#30340;&#22270;&#20687;&#20196;&#29260;&#20998;&#31163;&#65292;&#20197;&#25552;&#39640;Vision Transformers&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#32780;&#19981;&#24433;&#21709;&#26368;&#32456;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.15293</link><description>&lt;p&gt;
SkipViT: &#20351;&#29992;&#20196;&#29260;&#32423;&#36339;&#36291;&#36830;&#25509;&#21152;&#36895;Vision Transformers
&lt;/p&gt;
&lt;p&gt;
SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection. (arXiv:2401.15293v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15293
&lt;/p&gt;
&lt;p&gt;
SkipViT&#36890;&#36807;&#20196;&#29260;&#32423;&#36339;&#36291;&#36830;&#25509;&#23558;&#19981;&#37325;&#35201;&#30340;&#22270;&#20687;&#20196;&#29260;&#20998;&#31163;&#65292;&#20197;&#25552;&#39640;Vision Transformers&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#32780;&#19981;&#24433;&#21709;&#26368;&#32456;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision transformers&#34987;&#35748;&#20026;&#27604;CNN&#27169;&#22411;&#26356;&#20855;&#35745;&#31639;&#21644;&#25968;&#25454;&#23494;&#38598;&#24615;&#12290;&#36825;&#20123;Transformer&#27169;&#22411;&#65292;&#22914;ViT&#65292;&#38656;&#35201;&#25152;&#26377;&#36755;&#20837;&#22270;&#20687;&#20196;&#29260;&#26469;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#20196;&#29260;&#24182;&#19981;&#20449;&#24687;&#20016;&#23500;&#65292;&#21487;&#33021;&#21253;&#21547;&#26080;&#20851;&#30340;&#32972;&#26223;&#25110;&#19981;&#37325;&#35201;&#30340;&#22330;&#26223;&#31561;&#26080;&#20851;&#20449;&#24687;&#12290;&#36825;&#20123;&#20196;&#29260;&#34987;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#65288;MHSA&#65289;&#24573;&#30053;&#65292;&#23548;&#33268;MHSA&#21644;&#21069;&#39304;&#32593;&#32476;&#65288;FFN&#65289;&#20013;&#23384;&#22312;&#35768;&#22810;&#20887;&#20313;&#21644;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36825;&#20123;&#19981;&#37325;&#35201;&#30340;&#20196;&#29260;&#20998;&#31163;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#20302;&#25104;&#26412;&#35745;&#31639;&#36335;&#24452;&#21457;&#36865;&#65292;&#26469;&#20248;&#21270;&#19981;&#24517;&#35201;&#30340;&#20132;&#20114;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20250;&#32473;ViT&#27169;&#22411;&#28155;&#21152;&#20219;&#20309;&#21442;&#25968;&#65292;&#24182;&#26088;&#22312;&#22312;&#35757;&#32451;&#21534;&#21520;&#37327;&#21644;&#26368;&#32456;&#27169;&#22411;&#30340;Top-1&#20934;&#30830;&#29575;&#25439;&#22833;&#20026;0%&#20043;&#38388;&#25214;&#21040;&#26368;&#20339;&#24179;&#34913;&#12290;&#25105;&#20204;&#23545;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;ViT-small&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SkipViT&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers are known to be more computationally and data-intensive than CNN models. These transformer models such as ViT, require all the input image tokens to learn the relationship among them. However, many of these tokens are not informative and may contain irrelevant information such as unrelated background or unimportant scenery. These tokens are overlooked by the multi-head self-attention (MHSA), resulting in many redundant and unnecessary computations in MHSA and the feed-forward network (FFN). In this work, we propose a method to optimize the amount of unnecessary interactions between unimportant tokens by separating and sending them through a different low-cost computational path. Our method does not add any parameters to the ViT model and aims to find the best trade-off between training throughput and achieving a 0% loss in the Top-1 accuracy of the final model. Our experimental results on training ViT-small from scratch show that SkipViT is capable of effectively dr
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#19979;&#37325;&#26500;&#20855;&#26377;&#22359;&#31232;&#30095;&#24615;&#30340;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25193;&#22823;&#20102;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15292</link><description>&lt;p&gt;
&#22312;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#19979;&#30340;&#33258;&#36866;&#24212;&#22359;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Block sparse regularization under arbitrary linear transform. (arXiv:2401.15292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15292
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#19979;&#37325;&#26500;&#20855;&#26377;&#22359;&#31232;&#30095;&#24615;&#30340;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25193;&#22823;&#20102;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#26410;&#30693;&#22359;&#32467;&#26500;&#19979;&#30340;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#19979;&#30340;&#22359;&#31232;&#30095;&#20449;&#21495;&#37325;&#26500;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26159;&#29616;&#26377;&#26041;&#27861;LOP-$\ell_2$/$\ell_1$&#30340;&#25512;&#24191;&#65292;&#21487;&#20197;&#22312;&#38750;&#21487;&#36870;&#21464;&#25442;&#19979;&#37325;&#26500;&#20855;&#26377;&#22359;&#31232;&#30095;&#24615;&#30340;&#20449;&#21495;&#65292;&#32780;LOP-$\ell_2$/$\ell_1$&#19981;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#22823;&#20102;&#22359;&#31232;&#30095;&#27491;&#21017;&#21270;&#30340;&#33539;&#22260;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21508;&#31181;&#20449;&#21495;&#22788;&#29702;&#39046;&#22495;&#20013;&#24212;&#29992;&#26356;&#21152;&#28789;&#27963;&#21644;&#24378;&#22823;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#26469;&#27714;&#35299;&#35813;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20854;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#30340;&#26465;&#20214;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a convex signal reconstruction method for block sparsity under arbitrary linear transform with unknown block structure. The proposed method is a generalization of the existing method LOP-$\ell_2$/$\ell_1$ and can reconstruct signals with block sparsity under non-invertible transforms, unlike LOP-$\ell_2$/$\ell_1$. Our work broadens the scope of block sparse regularization, enabling more versatile and powerful applications across various signal processing domains. We derive an iterative algorithm for solving proposed method and provide conditions for its convergence to the optimal solution. Numerical experiments demonstrate the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#20026;&#26368;&#26032;&#29256;&#26412;&#30340;MIMIC&#25968;&#25454;&#38598;&#65288;MIMIC-IV&#65289;&#25552;&#20379;&#22522;&#20934;&#27979;&#35797;&#65292;&#22635;&#34917;&#35813;&#39046;&#22495;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#26102;&#38388;&#24207;&#21015;&#26631;&#31614;&#25968;&#25454;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.15290</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#35268;&#24459;&#19988;&#31232;&#30095;&#30340;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;MIMIC-IV&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking with MIMIC-IV, an irregular, spare clinical time series dataset. (arXiv:2401.15290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15290
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#20026;&#26368;&#26032;&#29256;&#26412;&#30340;MIMIC&#25968;&#25454;&#38598;&#65288;MIMIC-IV&#65289;&#25552;&#20379;&#22522;&#20934;&#27979;&#35797;&#65292;&#22635;&#34917;&#35813;&#39046;&#22495;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#26102;&#38388;&#24207;&#21015;&#26631;&#31614;&#25968;&#25454;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#24182;&#19988;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#23558;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#24212;&#29992;&#20110;&#35813;&#39046;&#22495;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#36825;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#39046;&#22495;&#20063;&#24341;&#21457;&#20102;&#23545;EHR&#30340;&#21487;&#35775;&#38382;&#24615;&#30340;&#38656;&#27714;&#12290;MIMIC&#65288;&#21307;&#23398;&#20449;&#24687;&#24066;&#22330;&#65289;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21463;&#27426;&#36814;&#30340;&#12289;&#20844;&#24320;&#30340;&#12289;&#20813;&#36153;&#30340;EHR&#25968;&#25454;&#38598;&#65292;&#20197;&#21407;&#22987;&#26684;&#24335;&#25552;&#20379;&#65292;&#24182;&#24050;&#22312;&#26080;&#25968;&#30740;&#31350;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#21463;&#27426;&#36814;&#31243;&#24230;&#65292;&#21364;&#32570;&#20047;&#22522;&#20934;&#27979;&#35797;&#24037;&#20316;&#65292;&#29305;&#21035;&#26159;&#19982;&#26368;&#36817;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#26102;&#38388;&#24207;&#21015;&#26631;&#31614;&#25968;&#25454;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#30456;&#32467;&#21512;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#25552;&#20379;&#26368;&#26032;&#29256;&#26412;&#30340;MIMIC&#25968;&#25454;&#38598;MIMIC-IV&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#32570;&#12290;&#25105;&#20204;&#36824;&#23545;&#24050;&#32463;&#23436;&#25104;&#30340;&#38024;&#23545;MIMIC-III&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#25991;&#29486;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health record (EHR) is more and more popular, and it comes with applying machine learning solutions to resolve various problems in the domain. This growing research area also raises the need for EHRs accessibility. Medical Information Mart for Intensive Care (MIMIC) dataset is a popular, public, and free EHR dataset in a raw format that has been used in numerous studies. However, despite of its popularity, it is lacking benchmarking work, especially with recent state of the art works in the field of deep learning with time-series tabular data. The aim of this work is to fill this lack by providing a benchmark for latest version of MIMIC dataset, MIMIC-IV. We also give a detailed literature survey about studies that has been already done for MIIMIC-III.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#32593;&#32476;&#27969;&#37327;&#21644;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#21644;&#26816;&#27979;&#21202;&#32034;&#36719;&#20214;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#39640;&#27700;&#24179;&#30340;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15285</link><description>&lt;p&gt;
&#36890;&#36807;&#32593;&#32476;&#27969;&#37327;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#32531;&#35299;&#21202;&#32034;&#36719;&#20214;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Ransomware threat mitigation through network traffic analysis and machine learning techniques. (arXiv:2401.15285v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#32593;&#32476;&#27969;&#37327;&#21644;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#21644;&#26816;&#27979;&#21202;&#32034;&#36719;&#20214;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#39640;&#27700;&#24179;&#30340;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#21202;&#32034;&#36719;&#20214;&#36827;&#34892;&#32593;&#32476;&#25915;&#20987;&#30340;&#24773;&#20917;&#26126;&#26174;&#22686;&#21152;&#12290;&#25915;&#20987;&#32773;&#21033;&#29992;&#36825;&#31181;&#24694;&#24847;&#36719;&#20214;&#20405;&#20837;&#32593;&#32476;&#24182;&#25439;&#23475;&#35745;&#31639;&#26426;&#31995;&#32479;&#12290;&#36825;&#32473;&#21508;&#31181;&#32452;&#32455;&#24102;&#26469;&#20102;&#37325;&#22823;&#21644;&#38271;&#26399;&#30340;&#25439;&#23475;&#65292;&#21253;&#25324;&#25919;&#24220;&#12289;&#31169;&#33829;&#20844;&#21496;&#21644;&#26222;&#36890;&#29992;&#25143;&#12290;&#36825;&#20123;&#25915;&#20987;&#36890;&#24120;&#23548;&#33268;&#25935;&#24863;&#20449;&#24687;&#30340;&#20002;&#22833;&#25110;&#27844;&#38706;&#65292;&#27491;&#24120;&#36816;&#33829;&#30340;&#20013;&#26029;&#20197;&#21450;&#25345;&#20037;&#30340;&#28431;&#27934;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#19968;&#31181;&#22312;&#35745;&#31639;&#26426;&#32593;&#32476;&#20013;&#35782;&#21035;&#21644;&#36776;&#21035;&#21202;&#32034;&#36719;&#20214;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#20998;&#26512;&#32593;&#32476;&#27969;&#37327;&#30340;&#27169;&#24335;&#12290;&#36890;&#36807;&#25910;&#38598;&#21644;&#30740;&#31350;&#36825;&#20123;&#27969;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#26816;&#27979;&#21202;&#32034;&#36719;&#20214;&#12290;&#23454;&#26045;&#36825;&#31181;&#26041;&#27861;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#22522;&#20110;&#32593;&#32476;&#27969;&#37327;&#20934;&#30830;&#22320;&#23450;&#20301;&#21202;&#32034;&#36719;&#20214;&#65292;&#23454;&#29616;&#39640;&#27700;&#24179;&#30340;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a noticeable increase in cyberattacks using ransomware. Attackers use this malicious software to break into networks and harm computer systems. This has caused significant and lasting damage to various organizations, including government, private companies, and regular users. These attacks often lead to the loss or exposure of sensitive information, disruptions in normal operations, and persistent vulnerabilities. This paper focuses on a method for recognizing and identifying ransomware in computer networks. The approach relies on using machine learning algorithms and analyzing the patterns of network traffic. By collecting and studying this traffic, and then applying machine learning models, we can accurately identify and detect ransomware. The results of implementing this method show that machine learning algorithms can effectively pinpoint ransomware based on network traffic, achieving high levels of precision and accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#25919;&#31574;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65288;FedSARSA&#65289;&#65292;&#21033;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#26469;&#35299;&#20915;&#39532;&#23572;&#21487;&#22827;&#21462;&#26679;&#12289;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#31561;&#25216;&#26415;&#25361;&#25112;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20851;&#20110;&#26377;&#38480;&#26102;&#38388;&#24615;&#33021;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.15273</link><description>&lt;p&gt;
&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#30340;&#25919;&#31574;&#24322;&#26500;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning. (arXiv:2401.15273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#25919;&#31574;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65288;FedSARSA&#65289;&#65292;&#21033;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#26469;&#35299;&#20915;&#39532;&#23572;&#21487;&#22827;&#21462;&#26679;&#12289;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#31561;&#25216;&#26415;&#25361;&#25112;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20851;&#20110;&#26377;&#38480;&#26102;&#38388;&#24615;&#33021;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#65288;FRL&#65289;&#26159;&#19968;&#31181;&#21033;&#29992;&#19981;&#21516;&#20195;&#29702;&#30340;&#20449;&#24687;&#26469;&#38477;&#20302;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#21069;&#26223;&#20809;&#26126;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#24403;&#27599;&#20010;&#20195;&#29702;&#19982;&#19968;&#20010;&#21487;&#33021;&#19981;&#21516;&#30340;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#26102;&#65292;&#20851;&#20110;FRL&#31639;&#27861;&#30340;&#38750;&#28176;&#36827;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#29702;&#35770;&#19978;&#30340;&#20102;&#35299;&#12290;&#36825;&#31181;&#32467;&#26524;&#30340;&#32570;&#20047;&#21487;&#20197;&#24402;&#22240;&#20110;&#21508;&#31181;&#25216;&#26415;&#25361;&#25112;&#21450;&#20854;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65306;&#39532;&#23572;&#21487;&#22827;&#21462;&#26679;&#12289;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12289;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#20197;&#33410;&#30465;&#36890;&#20449;&#12289;&#20195;&#29702;&#30340;MDP&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#36716;&#31227;&#26680;&#30340;&#24322;&#36136;&#24615;&#20197;&#21450;&#36830;&#32493;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#22312;&#25919;&#31574;&#19978;&#30340;&#35774;&#32622;&#20013;&#65292;&#34892;&#20026;&#25919;&#31574;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#36827;&#19968;&#27493;&#20351;&#20998;&#26512;&#22797;&#26434;&#21270;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FedSARSA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#32852;&#37030;&#25919;&#31574;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#24182;&#25552;&#20379;&#20840;&#38754;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated reinforcement learning (FRL) has emerged as a promising paradigm for reducing the sample complexity of reinforcement learning tasks by exploiting information from different agents. However, when each agent interacts with a potentially different environment, little to nothing is known theoretically about the non-asymptotic performance of FRL algorithms. The lack of such results can be attributed to various technical challenges and their intricate interplay: Markovian sampling, linear function approximation, multiple local updates to save communication, heterogeneity in the reward functions and transition kernels of the agents' MDPs, and continuous state-action spaces. Moreover, in the on-policy setting, the behavior policies vary with time, further complicating the analysis. In response, we introduce FedSARSA, a novel federated on-policy reinforcement learning scheme, equipped with linear function approximation, to address these challenges and provide a comprehensive finite-ti
&lt;/p&gt;</description></item><item><title>SimFair&#26159;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20844;&#24179;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#27169;&#25311;&#21644;&#36870;&#21521;&#24314;&#27169;&#26469;&#35299;&#20915;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#26377;&#25928;&#20445;&#25345;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15270</link><description>&lt;p&gt;
SimFair&#65306;&#29289;&#29702;&#24341;&#23548;&#30340;&#20844;&#24179;&#24863;&#30693;&#23398;&#20064;&#19982;&#27169;&#25311;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models. (arXiv:2401.15270v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15270
&lt;/p&gt;
&lt;p&gt;
SimFair&#26159;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20844;&#24179;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#27169;&#25311;&#21644;&#36870;&#21521;&#24314;&#27169;&#26469;&#35299;&#20915;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#26377;&#25928;&#20445;&#25345;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24863;&#30693;&#24050;&#32463;&#25104;&#20026;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#36127;&#36131;&#20219;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#22522;&#30784;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#24615;&#33021;&#19981;&#24179;&#31561;&#26159;&#30001;&#20110;&#19981;&#21516;&#21306;&#22495;&#20998;&#24067;&#30340;&#21464;&#21270;&#24341;&#36215;&#30340;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20986;&#25552;&#39640;&#20844;&#24179;&#21487;&#36801;&#31227;&#24615;&#30340;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#27809;&#26377;&#26469;&#33258;&#26032;&#21306;&#22495;&#30340;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#65292;&#36825;&#23545;&#20110;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#23581;&#35797;&#26159;&#19968;&#20010;&#29942;&#39048;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22522;&#20110;&#29289;&#29702;&#26426;&#21046;&#27169;&#22411;&#24050;&#32463;&#22312;&#35768;&#22810;&#20855;&#26377;&#37325;&#22823;&#31038;&#20250;&#24433;&#21709;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SimFair&#65292;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20844;&#24179;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#29289;&#29702;&#35268;&#21017;&#30340;&#27169;&#25311;&#21644;&#36870;&#21521;&#24314;&#27169;&#21040;&#35757;&#32451;&#35774;&#35745;&#20013;&#26469;&#24357;&#34917;&#25968;&#25454;&#38480;&#21046;&#12290;&#20197;&#28201;&#24230;&#39044;&#27979;&#20026;&#20363;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;SimFair&#22312;&#20445;&#25345;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness-awareness has emerged as an essential building block for the responsible use of artificial intelligence in real applications. In many cases, inequity in performance is due to the change in distribution over different regions. While techniques have been developed to improve the transferability of fairness, a solution to the problem is not always feasible with no samples from the new regions, which is a bottleneck for pure data-driven attempts. Fortunately, physics-based mechanistic models have been studied for many problems with major social impacts. We propose SimFair, a physics-guided fairness-aware learning framework, which bridges the data limitation by integrating physical-rule-based simulation and inverse modeling into the training design. Using temperature prediction as an example, we demonstrate the effectiveness of the proposed SimFair in fairness preservation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#20010;&#20154;&#21644;&#22242;&#20307;&#30340;&#20559;&#22909;&#65292;&#35299;&#20915;&#32958;&#33039;&#20998;&#37197;&#20013;&#30340;&#32039;&#36843;&#25361;&#25112;&#65292;&#24182;&#35780;&#20272;&#20559;&#22909;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15268</link><description>&lt;p&gt;
&#26397;&#30528;&#31283;&#23450;&#30340;&#21033;&#30410;&#30456;&#20851;&#26041;&#19968;&#33268;&#21270;&#26426;&#22120;&#23398;&#20064;&#20559;&#22909;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Preferences for Stakeholder-aligned Machine Learning. (arXiv:2401.15268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#20010;&#20154;&#21644;&#22242;&#20307;&#30340;&#20559;&#22909;&#65292;&#35299;&#20915;&#32958;&#33039;&#20998;&#37197;&#20013;&#30340;&#32039;&#36843;&#25361;&#25112;&#65292;&#24182;&#35780;&#20272;&#20559;&#22909;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32958;&#33039;&#20998;&#37197;&#30340;&#32039;&#36843;&#25361;&#25112;&#65292;&#21363;&#38656;&#27714;&#22686;&#38271;&#19982;&#21033;&#30410;&#30456;&#20851;&#26041;&#20215;&#20540;&#30340;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#31181;&#23398;&#20064;&#20010;&#20154;&#21644;&#22242;&#20307;&#20851;&#20110;&#32958;&#33039;&#20998;&#37197;&#30340;&#20559;&#22909;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#8220;&#25104;&#23545;&#32958;&#33039;&#24739;&#32773;&#22312;&#32447;&#35843;&#26597;&#8221;&#30340;&#25968;&#25454;&#65292;&#32467;&#21512;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20010;&#20154;&#12289;&#22242;&#20307;&#21644;&#31283;&#23450;&#24615;&#19977;&#20010;&#23618;&#38754;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#24182;&#36890;&#36807;&#20960;&#31181;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;&#20010;&#20154;&#23618;&#38754;&#27169;&#22411;&#39044;&#27979;&#20010;&#20307;&#21442;&#19982;&#32773;&#30340;&#20559;&#22909;&#65292;&#22242;&#20307;&#23618;&#38754;&#27169;&#22411;&#27719;&#24635;&#21442;&#19982;&#32773;&#20559;&#22909;&#65292;&#31283;&#23450;&#24615;&#23618;&#38754;&#27169;&#22411;&#26159;&#22242;&#20307;&#21319;&#32423;&#30340;&#25193;&#23637;&#65292;&#35780;&#20272;&#36825;&#20123;&#20559;&#22909;&#38543;&#26102;&#38388;&#30340;&#31283;&#23450;&#24615;&#12290;&#36890;&#36807;&#23558;&#21033;&#30410;&#30456;&#20851;&#26041;&#30340;&#20559;&#22909;&#32435;&#20837;&#32958;&#33039;&#20998;&#37197;&#36807;&#31243;&#65292;&#25105;&#20204;&#24076;&#26395;&#25512;&#21160;&#20262;&#29702;&#32500;&#24230;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to the pressing challenge of kidney allocation, characterized by growing demands for organs, this research sets out to develop a data-driven solution to this problem, which also incorporates stakeholder values. The primary objective of this study is to create a method for learning both individual and group-level preferences pertaining to kidney allocations. Drawing upon data from the 'Pairwise Kidney Patient Online Survey.' Leveraging two distinct datasets and evaluating across three levels - Individual, Group and Stability - we employ machine learning classifiers assessed through several metrics. The Individual level model predicts individual participant preferences, the Group level model aggregates preferences across participants, and the Stability level model, an extension of the Group level, evaluates the stability of these preferences over time. By incorporating stakeholder preferences into the kidney allocation process, we aspire to advance the ethical dimensions of o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;$\ell_\infty$-&#25200;&#21160;&#19979;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#35777;&#26126;&#24403;&#30495;&#23454;&#21442;&#25968;&#20026;0&#26102;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#20272;&#35745;&#22120;&#22312;&#35813;&#25200;&#21160;&#19979;&#30340;&#26497;&#38480;&#20998;&#24067;&#21487;&#33021;&#22312;0&#22788;&#26377;&#19968;&#20010;&#27491;&#27010;&#29575;&#36136;&#37327;&#65292;&#25552;&#20379;&#20102;&#31232;&#30095;&#24615;&#24674;&#22797;&#33021;&#21147;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#36807;&#31243;&#8212;&#8212;&#33258;&#36866;&#24212;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15262</link><description>&lt;p&gt;
&#22312;$\ell_\infty$-&#25200;&#21160;&#19979;&#23545;&#25239;&#24615;&#35757;&#32451;&#20272;&#35745;&#22120;&#30340;&#28176;&#36817;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Asymptotic Behavior of Adversarial Training Estimator under $\ell_\infty$-Perturbation. (arXiv:2401.15262v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;$\ell_\infty$-&#25200;&#21160;&#19979;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#35777;&#26126;&#24403;&#30495;&#23454;&#21442;&#25968;&#20026;0&#26102;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#20272;&#35745;&#22120;&#22312;&#35813;&#25200;&#21160;&#19979;&#30340;&#26497;&#38480;&#20998;&#24067;&#21487;&#33021;&#22312;0&#22788;&#26377;&#19968;&#20010;&#27491;&#27010;&#29575;&#36136;&#37327;&#65292;&#25552;&#20379;&#20102;&#31232;&#30095;&#24615;&#24674;&#22797;&#33021;&#21147;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#36807;&#31243;&#8212;&#8212;&#33258;&#36866;&#24212;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#35757;&#32451;&#34987;&#25552;&#20986;&#26469;&#25269;&#24481;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#27169;&#22411;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;$\ell_\infty$-&#25200;&#21160;&#19979;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#36825;&#20010;&#38382;&#39064;&#26368;&#36817;&#24341;&#36215;&#20102;&#24456;&#22810;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#20272;&#35745;&#22120;&#30340;&#28176;&#36817;&#34892;&#20026;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#30495;&#23454;&#21442;&#25968;&#20026;0&#26102;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#20272;&#35745;&#22120;&#22312;$\ell_\infty$-&#25200;&#21160;&#19979;&#30340;&#26497;&#38480;&#20998;&#24067;&#21487;&#33021;&#22312;0&#22788;&#26377;&#19968;&#20010;&#27491;&#27010;&#29575;&#36136;&#37327;&#65292;&#20026;&#30456;&#20851;&#30340;&#31232;&#30095;&#24615;&#24674;&#22797;&#33021;&#21147;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#36807;&#31243;&#8212;&#8212;&#33258;&#36866;&#24212;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#22312;$\ell_\infty$-&#25200;&#21160;&#19979;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#36807;&#31243;&#21487;&#20197;&#23454;&#29616;&#28176;&#36817;&#26080;&#20559;&#24615;&#21644;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#31232;&#30095;&#24615;&#24674;&#22797;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training has been proposed to hedge against adversarial attacks in machine learning and statistical models. This paper focuses on adversarial training under $\ell_\infty$-perturbation, which has recently attracted much research attention. The asymptotic behavior of the adversarial training estimator is investigated in the generalized linear model. The results imply that the limiting distribution of the adversarial training estimator under $\ell_\infty$-perturbation could put a positive probability mass at $0$ when the true parameter is $0$, providing a theoretical guarantee of the associated sparsity-recovery ability. Alternatively, a two-step procedure is proposed -adaptive adversarial training, which could further improve the performance of adversarial training under $\ell_\infty$-perturbation. Specifically, the proposed procedure could achieve asymptotic unbiasedness and variable-selection consistency. Numerical experiments are conducted to show the sparsity-recovery a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20219;&#24847;&#39044;&#27979;&#22120;&#26500;&#24314;&#32447;&#24615;&#27169;&#22411;&#21442;&#25968;&#30340;&#26377;&#38480;&#26679;&#26412;&#32622;&#20449;&#21306;&#38388;&#12290;&#35813;&#26041;&#27861;&#23545;&#22122;&#22768;&#30340;&#35201;&#27714;&#24456;&#23569;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#20005;&#26684;&#32447;&#24615;&#20989;&#25968;&#20559;&#24046;&#19968;&#23450;&#38408;&#20540;&#30340;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36827;&#34892;&#40065;&#26834;&#20248;&#21270;&#65292;&#24182;&#25552;&#21462;&#29305;&#23450;&#21442;&#25968;&#22352;&#26631;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20063;&#33021;&#29992;&#20110;&#20551;&#35774;&#26816;&#39564;&#12290;</title><link>http://arxiv.org/abs/2401.15254</link><description>&lt;p&gt;
&#29992;&#20219;&#24847;&#39044;&#27979;&#22120;&#26500;&#24314;&#32447;&#24615;&#22238;&#24402;&#21442;&#25968;&#30340;&#26377;&#38480;&#26679;&#26412;&#32622;&#20449;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Finite Sample Confidence Regions for Linear Regression Parameters Using Arbitrary Predictors. (arXiv:2401.15254v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20219;&#24847;&#39044;&#27979;&#22120;&#26500;&#24314;&#32447;&#24615;&#27169;&#22411;&#21442;&#25968;&#30340;&#26377;&#38480;&#26679;&#26412;&#32622;&#20449;&#21306;&#38388;&#12290;&#35813;&#26041;&#27861;&#23545;&#22122;&#22768;&#30340;&#35201;&#27714;&#24456;&#23569;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#20005;&#26684;&#32447;&#24615;&#20989;&#25968;&#20559;&#24046;&#19968;&#23450;&#38408;&#20540;&#30340;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36827;&#34892;&#40065;&#26834;&#20248;&#21270;&#65292;&#24182;&#25552;&#21462;&#29305;&#23450;&#21442;&#25968;&#22352;&#26631;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20063;&#33021;&#29992;&#20110;&#20551;&#35774;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20219;&#24847;&#39044;&#27979;&#22120;&#26500;&#24314;&#32447;&#24615;&#27169;&#22411;&#21442;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;&#22122;&#22768;&#30340;&#35201;&#27714;&#24456;&#23569;&#65292;&#24182;&#19988;&#21487;&#20197;&#25193;&#23637;&#21040;&#20005;&#26684;&#32447;&#24615;&#20989;&#25968;&#20559;&#24046;&#19968;&#23450;&#38408;&#20540;&#30340;&#20989;&#25968;&#65292;&#20174;&#32780;&#36866;&#24212;&#20102;&#24191;&#27867;&#32780;&#23454;&#29992;&#30340;&#20989;&#25968;&#38598;&#21512;&#12290;&#24471;&#20986;&#30340;&#32622;&#20449;&#21306;&#38388;&#21487;&#20197;&#20316;&#20026;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26694;&#26550;&#20013;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#20174;&#32780;&#23454;&#29616;&#32447;&#24615;&#30446;&#26631;&#30340;&#20248;&#21270;&#12290;&#36825;&#31181;&#34920;&#31034;&#26041;&#24335;&#33021;&#22815;&#36827;&#34892;&#40065;&#26834;&#20248;&#21270;&#65292;&#24182;&#25552;&#21462;&#29305;&#23450;&#21442;&#25968;&#22352;&#26631;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#32622;&#20449;&#21306;&#38388;&#21487;&#33021;&#20026;&#31354;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20551;&#35774;&#26816;&#39564;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#35777;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore a novel methodology for constructing confidence regions for parameters of linear models, using predictions from any arbitrary predictor. Our framework requires minimal assumptions on the noise and can be extended to functions deviating from strict linearity up to some adjustable threshold, thereby accommodating a comprehensive and pragmatically relevant set of functions. The derived confidence regions can be cast as constraints within a Mixed Integer Linear Programming framework, enabling optimisation of linear objectives. This representation enables robust optimization and the extraction of confidence intervals for specific parameter coordinates. Unlike previous methods, the confidence region can be empty, which can be used for hypothesis testing. Finally, we validate the empirical applicability of our method on synthetic data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#29702;&#35770;&#35282;&#24230;&#25506;&#35752;&#20102;&#22312;&#39044;&#35757;&#32451;&#20013;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25913;&#36827;&#34920;&#31034;&#30340;&#24605;&#36335;&#65292;&#24182;&#35777;&#26126;&#20102;&#29305;&#24449;&#20928;&#21270;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15248</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#20013;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25913;&#36827;&#34920;&#31034;&#65306;&#20174;&#29702;&#35770;&#35282;&#24230;&#20986;&#21457;
&lt;/p&gt;
&lt;p&gt;
Better Representations via Adversarial Training in Pre-Training: A Theoretical Perspective. (arXiv:2401.15248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15248
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#29702;&#35770;&#35282;&#24230;&#25506;&#35752;&#20102;&#22312;&#39044;&#35757;&#32451;&#20013;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25913;&#36827;&#34920;&#31034;&#30340;&#24605;&#36335;&#65292;&#24182;&#35777;&#26126;&#20102;&#29305;&#24449;&#20928;&#21270;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#34987;&#35748;&#20026;&#21487;&#20197;&#20026;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#29983;&#25104;&#36890;&#29992;&#34920;&#31034;&#65292;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#29616;&#26377;&#25991;&#29486;&#20363;&#22914;\cite{kim2020adversarial}&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#19979;&#28216;&#20219;&#21153;&#21487;&#20197;&#32487;&#25215;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#31181;&#40065;&#26834;&#24615;&#32487;&#25215;&#29616;&#35937;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#25581;&#31034;&#20102;&#22312;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#29305;&#24449;&#20928;&#21270;&#22312;&#36830;&#25509;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;(i)&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#65292;&#27599;&#20010;&#38544;&#34255;&#33410;&#28857;&#20542;&#21521;&#20110;&#36873;&#25321;&#21482;&#26377;&#19968;&#20010;&#65288;&#25110;&#20960;&#20010;&#65289;&#29305;&#24449;&#65307;(ii)&#22312;&#27809;&#26377;&#23545;&#25239;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#38544;&#34255;&#33410;&#28857;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#36825;&#20010;&#35266;&#23519;&#23545;&#20110;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23545;&#27604;&#23398;&#20064;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;&#36890;&#36807;&#20928;&#21270;&#33410;&#28857;&#65292;&#20107;&#23454;&#35777;&#26126;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20165;&#20165;&#36827;&#34892;&#24178;&#20928;&#35757;&#32451;&#23601;&#36275;&#20197;&#23454;&#29616;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training is known to generate universal representations for downstream tasks in large-scale deep learning such as large language models. Existing literature, e.g., \cite{kim2020adversarial}, empirically observe that the downstream tasks can inherit the adversarial robustness of the pre-trained model. We provide theoretical justifications for this robustness inheritance phenomenon. Our theoretical results reveal that feature purification plays an important role in connecting the adversarial robustness of the pre-trained model and the downstream tasks in two-layer neural networks. Specifically, we show that (i) with adversarial training, each hidden node tends to pick only one (or a few) feature; (ii) without adversarial training, the hidden nodes can be vulnerable to attacks. This observation is valid for both supervised pre-training and contrastive learning. With purified nodes, it turns out that clean training is enough to achieve adversarial robustness in downstream tasks.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#21322;&#25935;&#24863;&#29305;&#24449;&#30340;&#24046;&#20998;&#38544;&#31169;&#24191;&#21578;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#30495;&#23454;&#24191;&#21578;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.15246</link><description>&lt;p&gt;
&#20351;&#29992;&#21322;&#25935;&#24863;&#29305;&#24449;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#24191;&#21578;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Differentially Private Ad Prediction Models with Semi-Sensitive Features. (arXiv:2401.15246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15246
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#21322;&#25935;&#24863;&#29305;&#24449;&#30340;&#24046;&#20998;&#38544;&#31169;&#24191;&#21578;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#30495;&#23454;&#24191;&#21578;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20197;&#25968;&#23383;&#24191;&#21578;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#20026;&#20986;&#21457;&#28857;&#65292;&#20171;&#32461;&#20102;&#20351;&#29992;&#21322;&#25935;&#24863;&#29305;&#24449;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25915;&#20987;&#32773;&#24050;&#30693;&#19968;&#37096;&#20998;&#29305;&#24449;&#65288;&#22240;&#27492;&#26080;&#38656;&#20445;&#25252;&#65289;&#65292;&#32780;&#21097;&#20313;&#30340;&#29305;&#24449;&#20197;&#21450;&#26631;&#31614;&#23545;&#20110;&#25915;&#20987;&#32773;&#26469;&#35828;&#26159;&#26410;&#30693;&#30340;&#65292;&#38656;&#35201;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#12290;&#36825;&#20010;&#20219;&#21153;&#25554;&#20540;&#20102;&#20351;&#29992;&#20840;&#24046;&#20998;&#38544;&#31169;&#65288;&#38656;&#35201;&#20445;&#25252;&#26631;&#31614;&#21644;&#25152;&#26377;&#29305;&#24449;&#65289;&#25110;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#65288;&#25152;&#26377;&#29305;&#24449;&#34987;&#35748;&#20026;&#26159;&#24050;&#30693;&#30340;&#65292;&#21482;&#38656;&#20445;&#25252;&#26631;&#31614;&#65289;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26469;&#35757;&#32451;&#20855;&#26377;&#21322;&#25935;&#24863;&#29305;&#24449;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#24191;&#21578;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25928;&#29992;&#26041;&#38754;&#36229;&#36807;&#20102;&#65288;i&#65289;&#22312;&#25152;&#26377;&#29305;&#24449;&#19978;&#36816;&#34892;&#30340;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#22522;&#32447;&#21644;&#65288;ii&#65289;&#20165;&#22312;&#24050;&#30693;&#29305;&#24449;&#19978;&#36816;&#34892;&#30340;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65288;&#32780;&#20002;&#24323;&#20102;&#26410;&#30693;&#30340;&#29305;&#24449;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by problems arising in digital advertising, we introduce the task of training differentially private (DP) machine learning models with semi-sensitive features. In this setting, a subset of the features is known to the attacker (and thus need not be protected) while the remaining features as well as the label are unknown to the attacker and should be protected by the DP guarantee. This task interpolates between training the model with full DP (where the label and all features should be protected) or with label DP (where all the features are considered known, and only the label should be protected). We present a new algorithm for training DP models with semi-sensitive features. Through an empirical evaluation on real ads datasets, we demonstrate that our algorithm surpasses in utility the baselines of (i) DP stochastic gradient descent (DP-SGD) run on all features (known and unknown), and (ii) a label DP algorithm run only on the known features (while discarding the unknown one
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#25554;&#20214;&#65292;&#21487;&#20197;&#22312;Blender 3D&#24314;&#27169;&#24037;&#20855;&#19978;&#28155;&#21152;&#27425;&#34920;&#38754;&#25955;&#23556;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Mitsuba&#28210;&#26579;&#22120;&#36827;&#34892;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#25554;&#20214;&#33021;&#22815;&#20934;&#30830;&#12289;&#32039;&#23494;&#21644;&#39640;&#25928;&#22320;&#21487;&#35270;&#21270;&#22343;&#21248;&#21644;&#24322;&#36136;&#27425;&#34920;&#38754;&#25955;&#23556;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.15245</link><description>&lt;p&gt;
GenPluSSS&#65306;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#27979;&#37327;&#27425;&#34920;&#38754;&#25955;&#23556;&#34920;&#31034;&#30340;&#25554;&#20214;
&lt;/p&gt;
&lt;p&gt;
GenPluSSS: A Genetic Algorithm Based Plugin for Measured Subsurface Scattering Representation. (arXiv:2401.15245v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#25554;&#20214;&#65292;&#21487;&#20197;&#22312;Blender 3D&#24314;&#27169;&#24037;&#20855;&#19978;&#28155;&#21152;&#27425;&#34920;&#38754;&#25955;&#23556;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Mitsuba&#28210;&#26579;&#22120;&#36827;&#34892;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#25554;&#20214;&#33021;&#22815;&#20934;&#30830;&#12289;&#32039;&#23494;&#21644;&#39640;&#25928;&#22320;&#21487;&#35270;&#21270;&#22343;&#21248;&#21644;&#24322;&#36136;&#27425;&#34920;&#38754;&#25955;&#23556;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;Blender 3D&#24314;&#27169;&#24037;&#20855;&#19978;&#28155;&#21152;&#22343;&#21248;&#21644;&#24322;&#36136;&#12289;&#20809;&#23398;&#21402;&#24230;&#30340;&#21322;&#36879;&#26126;&#26448;&#26009;&#34920;&#31034;&#30340;&#25554;&#20214;&#12290;&#35813;&#25554;&#20214;&#30340;&#24037;&#20316;&#21407;&#29702;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#21644;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#27425;&#34920;&#38754;&#25955;&#23556;&#26041;&#27861;&#65288;GenSSS&#65289;&#30340;&#32452;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#25554;&#20214;&#20351;&#29992;&#24320;&#28304;&#28210;&#26579;&#36719;&#20214;Mitsuba&#28210;&#26579;&#22120;&#36827;&#34892;&#23454;&#29616;&#12290;&#35813;&#25554;&#20214;&#22312;&#27979;&#24471;&#30340;&#27425;&#34920;&#38754;&#25955;&#23556;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25554;&#20214;&#33021;&#22815;&#20934;&#30830;&#12289;&#32039;&#23494;&#21644;&#39640;&#25928;&#22320;&#21487;&#35270;&#21270;&#22343;&#21248;&#21644;&#24322;&#36136;&#27425;&#34920;&#38754;&#25955;&#23556;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a plugin that adds a representation of homogeneous and heterogeneous, optically thick, translucent materials on the Blender 3D modeling tool. The working principle of this plugin is based on a combination of Genetic Algorithm (GA) and Singular Value Decomposition (SVD)-based subsurface scattering method (GenSSS). The proposed plugin has been implemented using Mitsuba renderer, which is an open source rendering software. The proposed plugin has been validated on measured subsurface scattering data. It's shown that the proposed plugin visualizes homogeneous and heterogeneous subsurface scattering effects, accurately, compactly and computationally efficiently.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#26041;&#24046;&#22343;&#34913;&#30340;&#36817;&#26368;&#20248;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#24179;&#28369;&#20215;&#20540;&#26356;&#26032;&#21644;&#20048;&#35266;&#23454;&#34892;&#32773;&#31639;&#27861;&#65292;&#20197;&#21450;&#23545;&#25968;&#38556;&#30861;&#27491;&#21017;&#21270;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#19968;&#33324;&#24615;&#21644;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#35745;&#31639;&#21327;&#26041;&#24046;&#22343;&#34913;&#30340;&#36817;&#26368;&#20248;&#25910;&#25947;&#36895;&#24230;$\tilde{O}(T^{-1})$&#12290;</title><link>http://arxiv.org/abs/2401.15240</link><description>&lt;p&gt;
&#21327;&#26041;&#24046;&#22343;&#34913;&#30340;&#36817;&#26368;&#20248;&#31574;&#30053;&#20248;&#21270;&#22312;&#19968;&#33324;&#24615;&#21644;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Policy Optimization for Correlated Equilibrium in General-Sum Markov Games. (arXiv:2401.15240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#26041;&#24046;&#22343;&#34913;&#30340;&#36817;&#26368;&#20248;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#24179;&#28369;&#20215;&#20540;&#26356;&#26032;&#21644;&#20048;&#35266;&#23454;&#34892;&#32773;&#31639;&#27861;&#65292;&#20197;&#21450;&#23545;&#25968;&#38556;&#30861;&#27491;&#21017;&#21270;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#19968;&#33324;&#24615;&#21644;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#35745;&#31639;&#21327;&#26041;&#24046;&#22343;&#34913;&#30340;&#36817;&#26368;&#20248;&#25910;&#25947;&#36895;&#24230;$\tilde{O}(T^{-1})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#35745;&#31639;&#22810;&#20154;&#19968;&#33324;&#24615;&#21644;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#21327;&#26041;&#24046;&#22343;&#34913;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#20043;&#21069;&#30340;&#32467;&#26524;&#23454;&#29616;&#20102;$O(T^{-1/2})$&#25910;&#25947;&#36895;&#24230;&#21040;&#21327;&#26041;&#24046;&#22343;&#34913;&#21644;$O(T^{-3/4})$&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#21040;&#36739;&#24369;&#30340;&#30095;&#26494;&#21327;&#26041;&#24046;&#22343;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#38750;&#32806;&#21512;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#36825;&#20004;&#20010;&#32467;&#26524;&#65292;&#20351;&#20854;&#36798;&#21040;&#35745;&#31639;&#21327;&#26041;&#24046;&#22343;&#34913;&#30340;&#36817;&#26368;&#20248;$\tilde{O}(T^{-1})$&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#20027;&#35201;&#22240;&#32032;&#65288;i&#65289;&#24179;&#28369;&#30340;&#20215;&#20540;&#26356;&#26032;&#21644;&#65288;ii&#65289;&#20855;&#26377;&#23545;&#25968;&#38556;&#30861;&#27491;&#21017;&#21270;&#22120;&#30340;&#20048;&#35266;&#23454;&#34892;&#32773;&#31639;&#27861;&#26500;&#24314;&#32780;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study policy optimization algorithms for computing correlated equilibria in multi-player general-sum Markov Games. Previous results achieve $O(T^{-1/2})$ convergence rate to a correlated equilibrium and an accelerated $O(T^{-3/4})$ convergence rate to the weaker notion of coarse correlated equilibrium. In this paper, we improve both results significantly by providing an uncoupled policy optimization algorithm that attains a near-optimal $\tilde{O}(T^{-1})$ convergence rate for computing a correlated equilibrium. Our algorithm is constructed by combining two main elements (i) smooth value updates and (ii) the optimistic-follow-the-regularized-leader algorithm with the log barrier regularizer.
&lt;/p&gt;</description></item><item><title>MEA-Defender&#26159;&#19968;&#31181;&#25269;&#24481;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#24378;&#22823;&#27700;&#21360;&#65292;&#36890;&#36807;&#23558;&#20004;&#20010;&#28304;&#31867;&#30340;&#26679;&#26412;&#32452;&#21512;&#22312;&#19968;&#36215;&#20135;&#29983;&#27700;&#21360;&#65292;&#24182;&#30830;&#20445;&#27700;&#21360;&#30340;&#36755;&#20986;&#22495;&#19982;&#20027;&#20219;&#21153;&#26679;&#26412;&#30340;&#36755;&#20986;&#22495;&#30456;&#21516;&#65292;&#23454;&#29616;&#20102;&#23545;DNN&#27169;&#22411;&#30693;&#35782;&#20135;&#26435;&#30340;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2401.15239</link><description>&lt;p&gt;
MEA-Defender:&#19968;&#31181;&#25269;&#24481;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#24378;&#22823;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
MEA-Defender: A Robust Watermark against Model Extraction Attack. (arXiv:2401.15239v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15239
&lt;/p&gt;
&lt;p&gt;
MEA-Defender&#26159;&#19968;&#31181;&#25269;&#24481;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#24378;&#22823;&#27700;&#21360;&#65292;&#36890;&#36807;&#23558;&#20004;&#20010;&#28304;&#31867;&#30340;&#26679;&#26412;&#32452;&#21512;&#22312;&#19968;&#36215;&#20135;&#29983;&#27700;&#21360;&#65292;&#24182;&#30830;&#20445;&#27700;&#21360;&#30340;&#36755;&#20986;&#22495;&#19982;&#20027;&#20219;&#21153;&#26679;&#26412;&#30340;&#36755;&#20986;&#22495;&#30456;&#21516;&#65292;&#23454;&#29616;&#20102;&#23545;DNN&#27169;&#22411;&#30693;&#35782;&#20135;&#26435;&#30340;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#20102;&#35768;&#22810;&#20855;&#26377;&#39640;&#20215;&#20540;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#12290;&#20026;&#20102;&#20445;&#25252;&#21407;&#22987;&#25152;&#26377;&#32773;&#23545;&#36825;&#20123;DNN&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;(IP)&#65292;&#22522;&#20110;&#21518;&#38376;&#30340;&#27700;&#21360;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#31181;&#27700;&#21360;&#22312;&#24212;&#23545;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#26102;&#22833;&#36133;&#65292;&#36825;&#31181;&#25915;&#20987;&#21033;&#29992;&#36755;&#20837;&#26679;&#26412;&#26597;&#35810;&#30446;&#26631;&#27169;&#22411;&#24182;&#33719;&#24471;&#30456;&#24212;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20351;&#29992;&#36825;&#20123;&#36755;&#20837;-&#36755;&#20986;&#23545;&#26469;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27700;&#21360;&#26041;&#27861;&#65292;&#21517;&#20026;MEA-Defender&#65292;&#29992;&#20110;&#20445;&#25252;DNN&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#20813;&#21463;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#36755;&#20837;&#22495;&#20013;&#26469;&#33258;&#20004;&#20010;&#28304;&#31867;&#30340;&#26679;&#26412;&#32452;&#21512;&#22312;&#19968;&#36215;&#33719;&#24471;&#27700;&#21360;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#27700;&#21360;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#24471;&#27700;&#21360;&#30340;&#36755;&#20986;&#22495;&#22788;&#20110;&#20027;&#20219;&#21153;&#26679;&#26412;&#30340;&#36755;&#20986;&#22495;&#20043;&#20869;&#12290;&#30001;&#20110;&#25105;&#20204;&#27700;&#21360;&#30340;&#36755;&#20837;&#22495;&#21644;&#36755;&#20986;&#22495;&#37117;&#26159;&#20027;&#20219;&#21153;&#26679;&#26412;&#19981;&#21487;&#25110;&#32570;&#30340;&#37096;&#20998;&#65292;&#22240;&#27492;&#27700;&#21360;&#23558;&#34987;&#25552;&#21462;&#20986;&#26469;&#24182;&#24212;&#29992;&#21040;&#30446;&#26631;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, numerous highly-valuable Deep Neural Networks (DNNs) have been trained using deep learning algorithms. To protect the Intellectual Property (IP) of the original owners over such DNN models, backdoor-based watermarks have been extensively studied. However, most of such watermarks fail upon model extraction attack, which utilizes input samples to query the target model and obtains the corresponding outputs, thus training a substitute model using such input-output pairs. In this paper, we propose a novel watermark to protect IP of DNN models against model extraction, named MEA-Defender. In particular, we obtain the watermark by combining two samples from two source classes in the input domain and design a watermark loss function that makes the output domain of the watermark within that of the main task samples. Since both the input domain and the output domain of our watermark are indispensable parts of those of the main task samples, the watermark will be extracted into the sto
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;TabTransformer&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#20381;&#36182;&#20851;&#31995;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#28040;&#38500;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.15238</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#65306;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning with Tabular Data: A Self-supervised Approach. (arXiv:2401.15238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15238
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;TabTransformer&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#20381;&#36182;&#20851;&#31995;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#28040;&#38500;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21033;&#29992;TabTransformer&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#35757;&#32451;&#34920;&#26684;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#34920;&#26684;&#25968;&#25454;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;GBDT&#65292;&#34429;&#28982;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#25105;&#20204;&#30340;&#35770;&#25991;&#30740;&#31350;&#20102;&#19987;&#20026;&#34920;&#26684;&#25968;&#25454;&#20248;&#21270;&#30340;TabTransformer&#30340;&#26377;&#25928;&#24615;&#12290;TabTransformer&#36890;&#36807;&#21033;&#29992;Transformer&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#20351;&#29992;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;TabTransformer&#36890;&#36807;&#21019;&#24314;&#20195;&#29702;&#30417;&#30563;&#20219;&#21153;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#28040;&#38500;&#20102;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#30446;&#26631;&#26159;&#25214;&#21040;&#26368;&#26377;&#25928;&#30340;TabTransformer&#27169;&#22411;&#26469;&#34920;&#31034;&#20998;&#31867;&#21644;&#25968;&#20540;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#22312;Transformer&#20013;&#26500;&#24314;&#19981;&#21516;&#36755;&#20837;&#35774;&#32622;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have described a novel approach for training tabular data using the TabTransformer model with self-supervised learning. Traditional machine learning models for tabular data, such as GBDT are being widely used though our paper examines the effectiveness of the TabTransformer which is a Transformer based model optimised specifically for tabular data. The TabTransformer captures intricate relationships and dependencies among features in tabular data by leveraging the self-attention mechanism of Transformers. We have used a self-supervised learning approach in this study, where the TabTransformer learns from unlabelled data by creating surrogate supervised tasks, eliminating the need for the labelled data. The aim is to find the most effective TabTransformer model representation of categorical and numerical features. To address the challenges faced during the construction of various input settings into the Transformers. Furthermore, a comparative analysis is also been conducted to exami
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#36229;&#20302;&#21151;&#32791;&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#36827;&#34892;&#39640;&#25928;&#30340;&#35270;&#35273;&#23039;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#23558;&#20004;&#31181;&#20855;&#26377;&#19981;&#21516;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19982;&#33258;&#36866;&#24212;&#20998;&#31867;&#27169;&#22359;&#32467;&#21512;&#20351;&#29992;&#65292;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#35745;&#31639;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#36873;&#25321;&#21512;&#36866;&#30340;&#32593;&#32476;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23039;&#24577;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.15236</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#36229;&#20302;&#21151;&#32791;&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#30340;&#39640;&#25928;&#35270;&#35273;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adaptive Deep Learning for Efficient Visual Pose Estimation aboard Ultra-low-power Nano-drones. (arXiv:2401.15236v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#36229;&#20302;&#21151;&#32791;&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#36827;&#34892;&#39640;&#25928;&#30340;&#35270;&#35273;&#23039;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#23558;&#20004;&#31181;&#20855;&#26377;&#19981;&#21516;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19982;&#33258;&#36866;&#24212;&#20998;&#31867;&#27169;&#22359;&#32467;&#21512;&#20351;&#29992;&#65292;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#35745;&#31639;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#36873;&#25321;&#21512;&#36866;&#30340;&#32593;&#32476;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23039;&#24577;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#20110;10&#21400;&#31859;&#30452;&#24452;&#30340;&#32435;&#31859;&#26080;&#20154;&#26426;&#30001;&#20110;&#20854;&#36866;&#29992;&#20110;&#36739;&#22823;&#30340;&#39134;&#34892;&#26080;&#20154;&#26426;&#26080;&#27861;&#21040;&#36798;&#30340;&#29421;&#31364;&#29615;&#22659;&#21644;&#20154;&#31867;&#38468;&#36817;&#30340;&#29305;&#28857;&#65292;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20854;&#24494;&#23567;&#30340;&#22806;&#24418;&#20063;&#24102;&#26469;&#20102;&#19968;&#20010;&#20027;&#35201;&#30340;&#32570;&#28857;&#65306;&#36229;&#38480;&#30340;&#20869;&#23384;&#21644;&#22788;&#29702;&#22120;&#29992;&#20110;&#20854;&#24863;&#30693;&#27969;&#31243;&#30340;&#26426;&#36733;&#25191;&#34892;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#24378;&#35843;&#35745;&#31639;&#25928;&#29575;&#21644;&#33410;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#20915;&#23450;&#19968;&#20010;&#23436;&#20840;&#24037;&#20316;&#30340;&#38381;&#29615;&#31995;&#32479;&#21644;&#19968;&#20010;&#22833;&#36133;&#30340;&#38381;&#29615;&#31995;&#32479;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#26497;&#20854;&#26377;&#38480;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#26426;&#21046;&#65292;&#29992;&#20110;&#39640;&#25928;&#25191;&#34892;&#22522;&#20110;&#35270;&#35273;&#30340;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#20219;&#21153;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#20004;&#31181;&#20855;&#26377;&#19981;&#21516;&#22238;&#24402;&#24615;&#33021;&#19982;&#35745;&#31639;&#25104;&#26412;&#25240;&#34935;&#30340;&#26368;&#26032;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;CNN&#19982;&#19968;&#20010;&#33258;&#36866;&#24212;&#20998;&#31867;&#27169;&#22359;&#32452;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#35745;&#31639;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21512;&#36866;&#30340;&#32593;&#32476;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23039;&#24577;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sub-10cm diameter nano-drones are gaining momentum thanks to their applicability in scenarios prevented to bigger flying drones, such as in narrow environments and close to humans. However, their tiny form factor also brings their major drawback: ultra-constrained memory and processors for the onboard execution of their perception pipelines. Therefore, lightweight deep learning-based approaches are becoming increasingly popular, stressing how computational efficiency and energy-saving are paramount as they can make the difference between a fully working closed-loop system and a failing one. In this work, to maximize the exploitation of the ultra-limited resources aboard nano-drones, we present a novel adaptive deep learning-based mechanism for the efficient execution of a vision-based human pose estimation task. We leverage two State-of-the-Art (SoA) convolutional neural networks (CNNs) with different regression performance vs. computational costs trade-offs. By combining these CNNs wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CascadedGaze&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#25552;&#21462;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22270;&#20687;&#24674;&#22797;&#20013;&#20840;&#23616;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#21367;&#31215;&#23618;&#20043;&#38388;&#24341;&#20837;&#23567;&#30340;&#21367;&#31215;&#26680;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#21040;&#20840;&#23616;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#22270;&#20687;&#21435;&#22122;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.15235</link><description>&lt;p&gt;
CascadedGaze: &#22270;&#20687;&#24674;&#22797;&#20013;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#25552;&#21462;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CascadedGaze: Efficiency in Global Context Extraction for Image Restoration. (arXiv:2401.15235v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CascadedGaze&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#25552;&#21462;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22270;&#20687;&#24674;&#22797;&#20013;&#20840;&#23616;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#21367;&#31215;&#23618;&#20043;&#38388;&#24341;&#20837;&#23567;&#30340;&#21367;&#31215;&#26680;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#21040;&#20840;&#23616;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#22270;&#20687;&#21435;&#22122;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20381;&#36182;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21367;&#31215;&#36816;&#31639;&#31526;&#30340;&#23616;&#37096;&#24615;&#36136;&#65292;&#23427;&#20204;&#24456;&#38590;&#25429;&#25417;&#21040;&#20840;&#23616;&#20449;&#24687;&#12290;Transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20248;&#21183;&#22312;&#20110;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#21364;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#22270;&#20687;&#24674;&#22797;&#30740;&#31350;&#38598;&#20013;&#22312;&#36890;&#36807;&#21464;&#31181;Transformer&#35299;&#20915;&#24615;&#33021;&#21644;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#30340;&#24179;&#34913;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CascadedGaze&#32593;&#32476;&#65288;CGNet&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#37319;&#29992;&#20102;&#20840;&#23616;&#19978;&#19979;&#25991;&#25552;&#21462;&#22120;&#65288;GCE&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#22270;&#20687;&#24674;&#22797;&#20840;&#23616;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;GCE&#27169;&#22359;&#36890;&#36807;&#22312;&#21367;&#31215;&#23618;&#20043;&#38388;&#20351;&#29992;&#23567;&#30340;&#21367;&#31215;&#26680;&#26469;&#23398;&#20064;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21435;&#22122;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65288;&#21253;&#25324;&#30495;&#23454;&#22270;&#20687;&#21435;&#22122;&#21644;&#65289;&#30340;&#24615;&#33021;&#20248;&#20110;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image restoration tasks traditionally rely on convolutional neural networks. However, given the local nature of the convolutional operator, they struggle to capture global information. The promise of attention mechanisms in Transformers is to circumvent this problem, but it comes at the cost of intensive computational overhead. Many recent studies in image restoration have focused on solving the challenge of balancing performance and computational cost via Transformer variants. In this paper, we present CascadedGaze Network (CGNet), an encoder-decoder architecture that employs Global Context Extractor (GCE), a novel and efficient way to capture global information for image restoration. The GCE module leverages small kernels across convolutional layers to learn global dependencies, without requiring self-attention. Extensive experimental results show that our approach outperforms a range of state-of-the-art methods on denoising benchmark datasets including both real image denoising and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#20315;&#20848;&#24503;&#22320;&#21306;&#29983;&#29289;&#35780;&#20272;&#22320;&#22270;&#39046;&#22495;&#30340;&#30740;&#31350;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#24352;&#23494;&#38598;&#26631;&#35760;&#30340;&#30495;&#23454;&#22320;&#38754;&#22320;&#22270;&#12290;</title><link>http://arxiv.org/abs/2401.15223</link><description>&lt;p&gt;
&#20315;&#20848;&#24503;&#22320;&#21306;&#30340;&#29983;&#29289;&#35780;&#20272;&#22320;&#22270;&#65306;&#19968;&#39033;Sentinel-2&#36965;&#24863;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Biological Valuation Map of Flanders: A Sentinel-2 Imagery Analysis. (arXiv:2401.15223v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#20315;&#20848;&#24503;&#22320;&#21306;&#29983;&#29289;&#35780;&#20272;&#22320;&#22270;&#39046;&#22495;&#30340;&#30740;&#31350;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#24352;&#23494;&#38598;&#26631;&#35760;&#30340;&#30495;&#23454;&#22320;&#38754;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#36965;&#24863;&#20998;&#26512;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#22303;&#22320;&#21033;&#29992;/&#22303;&#22320;&#35206;&#30422;&#65288;LULC&#65289;&#39046;&#22495;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#21355;&#26143;&#22270;&#20687;&#20998;&#26512;&#30340;&#21327;&#21516;&#20316;&#29992;&#22312;&#36825;&#19968;&#39046;&#22495;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#29983;&#20135;&#21147;&#65292;&#26377;&#22810;&#20010;&#30740;&#31350;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#35813;&#39046;&#22495;&#30340;&#19968;&#20010;&#26174;&#33879;&#25361;&#25112;&#26159;&#23545;&#24191;&#38420;&#39046;&#22303;&#19978;&#30340;&#22303;&#22320;&#20351;&#29992;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#26144;&#23556;&#65292;&#20854;&#20013;&#20934;&#30830;&#30340;&#22303;&#22320;&#21033;&#29992;&#25968;&#25454;&#30340;&#33719;&#21462;&#21644;&#22320;&#38754;&#30495;&#23454;&#22303;&#22320;&#21033;&#29992;&#26631;&#31614;&#30340;&#21487;&#38752;&#24615;&#24102;&#26469;&#20102;&#37325;&#22823;&#22256;&#38590;&#12290;&#20363;&#22914;&#65292;&#22312;&#27604;&#21033;&#26102;&#30340;&#20315;&#20848;&#24503;&#22320;&#21306;&#65292;&#25552;&#20379;&#35814;&#32454;&#20934;&#30830;&#30340;&#20687;&#32032;&#32423;&#26631;&#35760;&#25968;&#25454;&#38598;&#23588;&#20854;&#26377;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#22312;&#20840;&#29699;&#35768;&#22810;&#22320;&#21306;&#65292;&#38024;&#23545;&#27492;&#31867;&#30740;&#31350;&#32570;&#20047;&#35268;&#33539;&#21270;&#21644;&#24418;&#24335;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;&#24037;&#20316;&#27969;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20315;&#20848;&#24503;&#22320;&#21306;&#37197;&#23545;&#30340;Sentinel-2&#21355;&#26143;&#22270;&#30340;&#23494;&#38598;&#26631;&#35760;&#22320;&#38754;&#30495;&#30456;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, machine learning has become crucial in remote sensing analysis, particularly in the domain of Land-use/Land-cover (LULC). The synergy of machine learning and satellite imagery analysis has demonstrated significant productivity in this field, as evidenced by several studies. A notable challenge within this area is the semantic segmentation mapping of land usage over extensive territories, where the accessibility of accurate land-use data and the reliability of ground truth land-use labels pose significant difficulties. For example, providing a detailed and accurate pixel-wise labeled dataset of the Flanders region, a first-level administrative division of Belgium, can be particularly insightful. Yet there is a notable lack of regulated, formalized datasets and workflows for such studies in many regions globally. This paper introduces a comprehensive approach to addressing these gaps. We present a densely labeled ground truth map of Flanders paired with Sentinel-2 satell
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.15222</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#20197;&#38463;&#29255;&#31867;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#30149;&#20363;&#26816;&#27979;&#20026;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection. (arXiv:2401.15222v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#23454;&#20307;&#30340;&#35821;&#20041;&#21487;&#33021;&#20250;&#21463;&#21040;&#20462;&#39280;&#35821;&#30340;&#26174;&#33879;&#25913;&#21464;&#65292;&#21253;&#25324;&#23454;&#20307;&#30340;&#21542;&#23450;&#12289;&#19981;&#30830;&#23450;&#24615;&#12289;&#26465;&#20214;&#24615;&#12289;&#20005;&#37325;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#29616;&#26377;&#30340;&#30830;&#23450;&#20020;&#24202;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#27169;&#22411;&#28041;&#21450;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#25110;&#29305;&#24449;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#26159;&#29420;&#31435;&#35757;&#32451;&#27599;&#20010;&#20462;&#39280;&#35821;&#30340;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#21464;&#25442;&#22120;&#26550;&#26500;&#35774;&#35745;&#65292;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;SemEval 2015&#20219;&#21153;14&#35821;&#26009;&#24211;&#21644;&#19968;&#20010;&#26032;&#30340;&#38463;&#29255;&#31867;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#65288;OUD&#65289;&#25968;&#25454;&#38598;&#19978;&#20849;&#21516;&#23398;&#20064;&#21644;&#39044;&#27979;&#20462;&#39280;&#35821;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;SemEval&#20849;&#20139;&#30340;&#20462;&#39280;&#35821;&#20197;&#21450;OUD&#29305;&#23450;&#30340;&#26032;&#20462;&#39280;&#35821;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#19982;&#20197;&#21069;&#21457;&#34920;&#30340;&#31995;&#32479;&#30340;&#25928;&#26524;&#65292;&#24182;&#35780;&#20272;&#20102;&#20165;&#20849;&#20139;&#37096;&#20998;&#20020;&#24202;&#20462;&#39280;&#35821;&#26102;&#30340;&#20020;&#24202;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#30340;&#21487;&#34892;&#24615;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26469;&#33258;SemEval 2015&#30340;ShARe&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: The semantics of entities extracted from a clinical text can be dramatically altered by modifiers, including entity negation, uncertainty, conditionality, severity, and subject. Existing models for determining modifiers of clinical entities involve regular expression or features weights that are trained independently for each modifier.  Methods: We develop and evaluate a multi-task transformer architecture design where modifiers are learned and predicted jointly using the publicly available SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that contains modifiers shared with SemEval as well as novel modifiers specific for OUD. We evaluate the effectiveness of our multi-task learning approach versus previously published systems and assess the feasibility of transfer learning for clinical entity modifiers when only a portion of clinical modifiers are shared.  Results: Our approach achieved state-of-the-art results on the ShARe corpus from SemEval 2015 T
&lt;/p&gt;</description></item><item><title>HiFT&#26159;&#19968;&#31181;&#20998;&#23618;&#20840;&#21442;&#25968;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#20165;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#26356;&#26032;&#21442;&#25968;&#30340;&#23376;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#23454;&#29616;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#26631;&#20934;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15207</link><description>&lt;p&gt;
HiFT:&#19968;&#31181;&#20998;&#23618;&#20840;&#21442;&#25968;&#24494;&#35843;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy. (arXiv:2401.15207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15207
&lt;/p&gt;
&lt;p&gt;
HiFT&#26159;&#19968;&#31181;&#20998;&#23618;&#20840;&#21442;&#25968;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#20165;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#26356;&#26032;&#21442;&#25968;&#30340;&#23376;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#23454;&#29616;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#26631;&#20934;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#38271;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21442;&#25968;&#38656;&#35201;&#21344;&#29992;&#22823;&#37327;GPU&#20869;&#23384;&#12290;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#22120;&#20197;&#33410;&#30465;GPU&#20869;&#23384;&#65292;&#20294;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#38750;&#38646;&#38454;&#20248;&#21270;&#22120;&#22312;&#22823;&#22810;&#25968;&#19979;&#28216;&#20219;&#21153;&#19978;&#26356;&#23481;&#26131;&#25910;&#25947;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29420;&#31435;&#20110;&#20248;&#21270;&#22120;&#30340;&#31471;&#21040;&#31471;&#20998;&#23618;&#24494;&#35843;&#31574;&#30053;HiFT&#65292;&#23427;&#20165;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#26356;&#26032;&#21442;&#25968;&#30340;&#23376;&#38598;&#12290; HiFT&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#23384;&#20648;&#22312;GPU&#20869;&#23384;&#20013;&#30340;&#26799;&#24230;&#21644;&#20248;&#21270;&#22120;&#29366;&#24577;&#21442;&#25968;&#30340;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;GPU&#20869;&#23384;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;HiFT&#23454;&#29616;&#20102;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#26631;&#20934;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#65288;2&#65289;HiFT&#25903;&#25345;&#21253;&#25324;&#22312;&#20869;&#30340;&#21508;&#31181;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Full-parameter fine-tuning has become the go-to choice for adapting language models (LMs) to downstream tasks due to its excellent performance. As LMs grow in size, fine-tuning the full parameters of LMs requires a prohibitively large amount of GPU memory. Existing approaches utilize zeroth-order optimizer to conserve GPU memory, which can potentially compromise the performance of LMs as non-zero order optimizers tend to converge more readily on most downstream tasks. In this paper, we propose a novel optimizer-independent end-to-end hierarchical fine-tuning strategy, HiFT, which only updates a subset of parameters at each training step. HiFT can significantly reduce the amount of gradients and optimizer state parameters residing in GPU memory at the same time, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFT achieves comparable performance to parameter-efficient fine-tuning and standard full parameter fine-tuning. (2) HiFT supports various optimizers including
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#32852;&#37030;&#22270;&#24418;&#21464;&#21387;&#22120;&#65288;FedGT&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#20013;&#32570;&#23569;&#38142;&#25509;&#21644;&#23376;&#22270;&#24322;&#26500;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.15203</link><description>&lt;p&gt;
FedGT: &#21487;&#25193;&#23637;&#22270;&#24418;&#21464;&#21387;&#22120;&#30340;&#32852;&#37030;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
FedGT: Federated Node Classification with Scalable Graph Transformer. (arXiv:2401.15203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#32852;&#37030;&#22270;&#24418;&#21464;&#21387;&#22120;&#65288;FedGT&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#20013;&#32570;&#23569;&#38142;&#25509;&#21644;&#23376;&#22270;&#24322;&#26500;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#34987;&#24191;&#27867;&#29992;&#20110;&#24314;&#27169;&#20851;&#31995;&#25968;&#25454;&#12290;&#38543;&#30528;&#22270;&#34920;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#65292;&#23384;&#20648;&#21644;&#35745;&#31639;&#22810;&#20010;&#26412;&#22320;&#31995;&#32479;&#20013;&#30340;&#23376;&#22270;&#34920;&#25104;&#20026;&#19968;&#31181;&#36235;&#21183;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20363;&#22914;&#36890;&#36807;&#22312;&#26412;&#22320;&#23376;&#22270;&#19978;&#36827;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#38598;&#20013;&#26381;&#21153;&#22120;&#32858;&#21512;GNN&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#65306;&#65288;1&#65289;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#20013;&#32570;&#23569;&#26412;&#22320;&#23376;&#22270;&#20043;&#38388;&#30340;&#38142;&#25509;&#12290;&#36825;&#21487;&#33021;&#20005;&#37325;&#24433;&#21709;&#36981;&#24490;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#26356;&#26032;&#33410;&#28857;/&#36793;&#29305;&#24449;&#30340;GNN&#30340;&#24615;&#33021;&#12290; &#65288;2&#65289;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#23376;&#22270;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23376;&#22270;&#26469;&#33258;&#25972;&#20010;&#22270;&#30340;&#19981;&#21516;&#37096;&#20998;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#32852;&#37030;&#22270;&#21464;&#21387;&#22120;&#65288;FedGT&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#28151;&#21512;&#27880;&#24847;&#21147;&#26041;&#26696;&#26469;&#38477;&#20302;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs are widely used to model relational data. As graphs are getting larger and larger in real-world scenarios, there is a trend to store and compute subgraphs in multiple local systems. For example, recently proposed \emph{subgraph federated learning} methods train Graph Neural Networks (GNNs) distributively on local subgraphs and aggregate GNN parameters with a central server. However, existing methods have the following limitations: (1) The links between local subgraphs are missing in subgraph federated learning. This could severely damage the performance of GNNs that follow message-passing paradigms to update node/edge features. (2) Most existing methods overlook the subgraph heterogeneity issue, brought by subgraphs being from different parts of the whole graph. To address the aforementioned challenges, we propose a scalable \textbf{Fed}erated \textbf{G}raph \textbf{T}ransformer (\textbf{FedGT}) in the paper. Firstly, we design a hybrid attention scheme to reduce the complexity 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26469;&#33258;SCANIA&#20844;&#21496;&#30340;&#30495;&#23454;&#19990;&#30028;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#39044;&#27979;&#24615;&#32500;&#25252;&#22330;&#26223;&#12290;&#23427;&#20855;&#26377;&#24222;&#22823;&#30340;&#26679;&#26412;&#25968;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#26102;&#38388;&#20449;&#24687;&#65292;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#26631;&#20934;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.15199</link><description>&lt;p&gt;
SCANIA&#32452;&#20214;X&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#39044;&#27979;&#24615;&#32500;&#25252;&#30340;&#30495;&#23454;&#19990;&#30028;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SCANIA Component X Dataset: A Real-World Multivariate Time Series Dataset for Predictive Maintenance. (arXiv:2401.15199v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15199
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26469;&#33258;SCANIA&#20844;&#21496;&#30340;&#30495;&#23454;&#19990;&#30028;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#39044;&#27979;&#24615;&#32500;&#25252;&#22330;&#26223;&#12290;&#23427;&#20855;&#26377;&#24222;&#22823;&#30340;&#26679;&#26412;&#25968;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#26102;&#38388;&#20449;&#24687;&#65292;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#26631;&#20934;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26469;&#33258;SCANIA&#29790;&#20856;&#20844;&#21496;&#30340;&#21345;&#36710;&#36710;&#38431;&#20013;&#21311;&#21517;&#21457;&#21160;&#26426;&#37096;&#20214;&#65288;&#31216;&#20026;Component X&#65289;&#30340;&#30495;&#23454;&#19990;&#30028;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#22810;&#31181;&#21464;&#37327;&#65292;&#25429;&#25417;&#20102;&#35814;&#32454;&#30340;&#25805;&#20316;&#25968;&#25454;&#12289;&#32500;&#20462;&#35760;&#24405;&#21644;&#21345;&#36710;&#35268;&#26684;&#65292;&#21516;&#26102;&#36890;&#36807;&#21311;&#21517;&#22788;&#29702;&#20445;&#25345;&#26426;&#23494;&#24615;&#12290;&#23427;&#38750;&#24120;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#22914;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#29983;&#23384;&#20998;&#26512;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#27979;&#24615;&#32500;&#25252;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#24222;&#22823;&#30340;&#26679;&#26412;&#25968;&#37327;&#21644;&#20197;&#30452;&#26041;&#22270;&#21644;&#35745;&#25968;&#22120;&#24418;&#24335;&#30340;&#22810;&#26679;&#21270;&#29305;&#24449;&#65292;&#20197;&#21450;&#21253;&#21547;&#26102;&#38388;&#20449;&#24687;&#65292;&#20351;&#24471;&#36825;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#22312;&#35813;&#39046;&#22495;&#20013;&#29420;&#29305;&#12290;&#21457;&#24067;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#30446;&#26631;&#26159;&#35753;&#24191;&#22823;&#30740;&#31350;&#20154;&#21592;&#26377;&#21487;&#33021;&#20351;&#29992;&#26469;&#33258;&#19968;&#23478;&#22269;&#38469;&#30693;&#21517;&#20844;&#21496;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26631;&#20934;&#22522;&#20934;&#29992;&#20110;&#39044;&#27979;&#24615;&#32500;&#25252;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a description of a real-world, multivariate time series dataset collected from an anonymized engine component (called Component X) of a fleet of trucks from SCANIA, Sweden. This dataset includes diverse variables capturing detailed operational data, repair records, and specifications of trucks while maintaining confidentiality by anonymization. It is well-suited for a range of machine learning applications, such as classification, regression, survival analysis, and anomaly detection, particularly when applied to predictive maintenance scenarios. The large population size and variety of features in the format of histograms and numerical counters, along with the inclusion of temporal information, make this real-world dataset unique in the field. The objective of releasing this dataset is to give a broad range of researchers the possibility of working with real-world data from an internationally well-known company and introduce a standard benchmark to the predictive ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;AMuSE&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#22312;&#32676;&#20307;&#23545;&#35805;&#20013;&#25429;&#25417;&#24182;&#20998;&#26512;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#12290;&#27169;&#22411;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#22810;&#20010;&#27169;&#24577;&#30340;&#22806;&#22260;&#21644;&#20013;&#22830;&#32593;&#32476;&#23454;&#29616;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2401.15164</link><description>&lt;p&gt;
AMuSE: &#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#20998;&#26512;&#29992;&#20110;&#32676;&#20307;&#23545;&#35805;&#20013;&#30340;&#35828;&#35805;&#32773;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
AMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in Group Conversations. (arXiv:2401.15164v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;AMuSE&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#22312;&#32676;&#20307;&#23545;&#35805;&#20013;&#25429;&#25417;&#24182;&#20998;&#26512;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#12290;&#27169;&#22411;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#22810;&#20010;&#27169;&#24577;&#30340;&#22806;&#22260;&#21644;&#20013;&#22830;&#32593;&#32476;&#23454;&#29616;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#20855;&#26377;&#33258;&#28982;&#20154;&#26426;&#20132;&#20114;&#33021;&#21147;&#30340;&#26234;&#33021;&#20195;&#29702;&#26102;&#65292;&#20998;&#26512;&#32676;&#20307;&#23545;&#35805;&#20013;&#30340;&#20010;&#20307;&#24773;&#24863;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#34429;&#28982;&#21487;&#38752;&#30340;&#24773;&#24863;&#35782;&#21035;&#25216;&#26415;&#20381;&#36182;&#20110;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#65289;&#65292;&#20294;&#36825;&#20123;&#27169;&#24577;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#20197;&#21450;&#21463;&#20010;&#20307;&#34892;&#20026;&#27169;&#24335;&#24433;&#21709;&#30340;&#21160;&#24577;&#36328;&#27169;&#24577;&#20132;&#20114;&#20351;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#21464;&#24471;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#32676;&#20307;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#22256;&#38590;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#22240;&#20026;&#24773;&#24863;&#21450;&#20854;&#26102;&#38388;&#28436;&#21464;&#19981;&#20165;&#21463;&#20010;&#20307;&#24433;&#21709;&#65292;&#36824;&#21463;&#21040;&#35266;&#20247;&#21453;&#24212;&#21644;&#36827;&#34892;&#20013;&#23545;&#35805;&#30340;&#32972;&#26223;&#31561;&#22806;&#37096;&#29615;&#22659;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#27169;&#24577;&#29305;&#23450;&#30340;&#22806;&#22260;&#32593;&#32476;&#21644;&#20013;&#22830;&#32593;&#32476;&#30340;&#20132;&#20114;&#38598;&#21512;&#26469;&#25429;&#25417;&#19981;&#21516;&#32423;&#21035;&#30340;&#31354;&#38388;&#25277;&#35937;&#20013;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#36890;&#36807;&#20854;&#22806;&#22260;&#38190;&#20540;&#23545;&#27880;&#20837;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing individual emotions during group conversation is crucial in developing intelligent agents capable of natural human-machine interaction. While reliable emotion recognition techniques depend on different modalities (text, audio, video), the inherent heterogeneity between these modalities and the dynamic cross-modal interactions influenced by an individual's unique behavioral patterns make the task of emotion recognition very challenging. This difficulty is compounded in group settings, where the emotion and its temporal evolution are not only influenced by the individual but also by external contexts like audience reaction and context of the ongoing conversation. To meet this challenge, we propose a Multimodal Attention Network that captures cross-modal interactions at various levels of spatial abstraction by jointly learning its interactive bunch of mode-specific Peripheral and Central networks. The proposed MAN injects cross-modal attention via its Peripheral key-value pairs 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;T-Rex&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#37329;&#34701;&#25351;&#25968;&#36319;&#36394;&#20013;&#36873;&#25321;&#23569;&#25968;&#30456;&#20851;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#26368;&#36817;&#37051;&#24809;&#32602;&#26426;&#21046;&#65292;&#21487;&#38752;&#25511;&#21046;&#35823;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36807;&#21435;20&#24180;&#20869;&#22522;&#20110;&#23569;&#37327;&#32929;&#31080;&#20934;&#30830;&#36319;&#36394;&#26631;&#20934;&#26222;&#23572;500&#25351;&#25968;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.15139</link><description>&lt;p&gt;
FDR&#25511;&#21046;&#30340;&#31232;&#30095;&#37329;&#34701;&#25351;&#25968;&#36319;&#36394;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
FDR-Controlled Portfolio Optimization for Sparse Financial Index Tracking. (arXiv:2401.15139v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;T-Rex&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#37329;&#34701;&#25351;&#25968;&#36319;&#36394;&#20013;&#36873;&#25321;&#23569;&#25968;&#30456;&#20851;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#26368;&#36817;&#37051;&#24809;&#32602;&#26426;&#21046;&#65292;&#21487;&#38752;&#25511;&#21046;&#35823;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36807;&#21435;20&#24180;&#20869;&#22522;&#20110;&#23569;&#37327;&#32929;&#31080;&#20934;&#30830;&#36319;&#36394;&#26631;&#20934;&#26222;&#23572;500&#25351;&#25968;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#22914;&#37329;&#34701;&#25351;&#25968;&#36319;&#36394;&#25110;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#65292;&#20851;&#38190;&#26159;&#22312;&#20445;&#25345;&#23545;&#35823;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#30340;&#25511;&#21046;&#30340;&#21516;&#26102;&#36873;&#25321;&#23569;&#25968;&#30456;&#20851;&#21464;&#37327;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#21464;&#37327;&#20043;&#38388;&#32463;&#24120;&#23384;&#22312;&#24378;&#20381;&#36182;&#20851;&#31995;&#65288;&#20363;&#22914;&#32929;&#31080;&#25910;&#30410;&#65289;&#65292;&#36825;&#21487;&#33021;&#20250;&#21066;&#24369;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;&#27169;&#22411;X knockoff&#26041;&#27861;&#25110;T-Rex&#36873;&#25321;&#22120;&#65289;&#30340;FDR&#25511;&#21046;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;T-Rex&#26694;&#26550;&#65292;&#20197;&#36866;&#24212;&#39640;&#24230;&#30456;&#20851;&#21464;&#37327;&#30340;&#37325;&#21472;&#32452;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#26368;&#36817;&#37051;&#24809;&#32602;&#26426;&#21046;&#38598;&#25104;&#21040;&#26694;&#26550;&#20013;&#23454;&#29616;&#30340;&#65292;&#35813;&#26426;&#21046;&#33021;&#22815;&#22312;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#27700;&#24179;&#19978;&#21487;&#38752;&#25511;&#21046;FDR&#12290;&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;&#30340;&#23454;&#20363;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#36807;&#21435;20&#24180;&#20869;&#22522;&#20110;&#23569;&#37327;&#32929;&#31080;&#20934;&#30830;&#36319;&#36394;&#26631;&#20934;&#26222;&#23572;500&#25351;&#25968;&#30340;&#33021;&#21147;&#12290;&#22312;CRAN&#19978;&#25552;&#20379;&#20102;R&#21253;TRexSelector&#30340;&#24320;&#28304;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In high-dimensional data analysis, such as financial index tracking or biomedical applications, it is crucial to select the few relevant variables while maintaining control over the false discovery rate (FDR). In these applications, strong dependencies often exist among the variables (e.g., stock returns), which can undermine the FDR control property of existing methods like the model-X knockoff method or the T-Rex selector. To address this issue, we have expanded the T-Rex framework to accommodate overlapping groups of highly correlated variables. This is achieved by integrating a nearest neighbors penalization mechanism into the framework, which provably controls the FDR at the user-defined target level. A real-world example of sparse index tracking demonstrates the proposed method's ability to accurately track the S&amp;P 500 index over the past 20 years based on a small number of stocks. An open-source implementation is provided within the R package TRexSelector on CRAN.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#22522;&#20110;OSINT&#30340;&#32593;&#32476;&#23041;&#32961;&#24847;&#35782;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#20108;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15127</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#22522;&#20110;OSINT&#30340;&#32593;&#32476;&#23041;&#32961;&#24847;&#35782;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness. (arXiv:2401.15127v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#22522;&#20110;OSINT&#30340;&#32593;&#32476;&#23041;&#32961;&#24847;&#35782;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#20108;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#65292;&#20851;&#20110;&#26032;&#20852;&#23041;&#32961;&#30340;&#30693;&#35782;&#20849;&#20139;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#26500;&#25104;&#20102;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#26426;&#36935;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#12289;GPT4all&#12289;Dolly&#12289;Stanford Alpaca&#12289;Alpaca-LoRA&#21644;Falcon&#31561;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#35782;&#21035;&#24320;&#28304;&#24773;&#25253;&#20013;&#19982;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#30340;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20108;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20316;&#20026;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20174;Twitter&#25910;&#38598;&#30340;&#32463;&#36807;&#20805;&#20998;&#39564;&#35777;&#30340;&#25968;&#25454;&#65292;&#35813;&#25968;&#25454;&#26469;&#28304;&#20110;&#20197;&#24448;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#26041;&#38754;&#65292;&#21830;&#19994;&#27169;&#22411;Chatbot GPT-4&#23454;&#29616;&#20102;&#21487;&#25509;&#21463;&#30340;F1&#20998;&#25968;0.94&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;GPT4all&#23454;&#29616;&#20102;F1&#20998;&#25968;0.90&#12290;&#28982;&#32780;&#65292;&#23601;&#32593;&#32476;&#23433;&#20840;&#23454;&#20307;&#35782;&#21035;&#32780;&#35328;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowledge sharing about emerging threats is crucial in the rapidly advancing field of cybersecurity and forms the foundation of Cyber Threat Intelligence. In this context, Large Language Models are becoming increasingly significant in the field of cybersecurity, presenting a wide range of opportunities. This study explores the capability of chatbots such as ChatGPT, GPT4all, Dolly,Stanford Alpaca, Alpaca-LoRA, and Falcon to identify cybersecurity-related text within Open Source Intelligence. We assess the capabilities of existing chatbot models for Natural Language Processing tasks. We consider binary classification and Named Entity Recognition as tasks. This study analyzes well-established data collected from Twitter, derived from previous research efforts. Regarding cybersecurity binary classification, Chatbot GPT-4 as a commercial model achieved an acceptable F1-score of 0.94, and the open-source GPT4all model achieved an F1-score of 0.90. However, concerning cybersecurity entity re
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#30693;&#35782;&#33976;&#39311;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;AnomalyLLM&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#32593;&#32476;&#27169;&#20223;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#36890;&#36807;&#27604;&#36739;&#23398;&#29983;&#32593;&#32476;&#21644;&#25945;&#24072;&#32593;&#32476;&#30340;&#29305;&#24449;&#24046;&#24322;&#26469;&#26816;&#27979;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2401.15123</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection. (arXiv:2401.15123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15123
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#30693;&#35782;&#33976;&#39311;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;AnomalyLLM&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#32593;&#32476;&#27169;&#20223;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#36890;&#36807;&#27604;&#36739;&#23398;&#29983;&#32593;&#32476;&#21644;&#25945;&#24072;&#32593;&#32476;&#30340;&#29305;&#24449;&#24046;&#24322;&#26469;&#26816;&#27979;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#21487;&#29992;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#33719;&#24471;&#21487;&#27867;&#21270;&#30340;&#34920;&#31034;&#26144;&#23556;&#65292;&#36825;&#19982;&#21482;&#26377;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#20917;&#20914;&#31361;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;AnomalyLLM&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#23398;&#29983;&#32593;&#32476;&#34987;&#35757;&#32451;&#25104;&#27169;&#20223;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#29305;&#24449;&#12290;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;&#24403;&#23398;&#29983;&#32593;&#32476;&#19982;&#25945;&#24072;&#32593;&#32476;&#30340;&#29305;&#24449;&#24046;&#24322;&#24456;&#22823;&#26102;&#65292;&#23601;&#26816;&#27979;&#21040;&#24322;&#24120;&#12290;&#20026;&#20102;&#36991;&#20813;&#23398;&#29983;&#32593;&#32476;&#23398;&#20064;&#21040;&#25945;&#24072;&#32593;&#32476;&#23545;&#24322;&#24120;&#26679;&#26412;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#20851;&#38190;&#31574;&#30053;&#12290;1) &#23558;&#20856;&#22411;&#20449;&#21495;&#34701;&#20837;&#23398;&#29983;&#32593;&#32476;&#65292;&#20197;&#24041;&#22266;&#27491;&#24120;&#29305;&#24449;&#25552;&#21462;&#12290;2) &#21152;&#26435;&#25945;&#24072;&#32593;&#32476;&#21644;&#23398;&#29983;&#32593;&#32476;&#30340;&#29305;&#24449;&#65292;&#20197;&#20943;&#23569;&#24322;&#24120;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised methods have gained prominence in time series anomaly detection due to the scarcity of available annotations. Nevertheless, they typically demand extensive training data to acquire a generalizable representation map, which conflicts with scenarios of a few available samples, thereby limiting their performance. To overcome the limitation, we propose \textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly detection approach where the student network is trained to mimic the features of the large language model (LLM)-based teacher network that is pretrained on large-scale datasets. During the testing phase, anomalies are detected when the discrepancy between the features of the teacher and student networks is large. To circumvent the student network from learning the teacher network's feature of anomalous samples, we devise two key strategies. 1) Prototypical signals are incorporated into the student network to consolidate the normal feature extraction. 2) W
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#27169;&#25311;&#24182;&#26377;&#25928;&#27169;&#25311;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#30340;NeuralMD&#26041;&#27861;&#65292;&#37319;&#29992;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24314;&#27169;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15122</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#32423;&#23545;&#31216;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics. (arXiv:2401.15122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15122
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#27169;&#25311;&#24182;&#26377;&#25928;&#27169;&#25311;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#30340;NeuralMD&#26041;&#27861;&#65292;&#37319;&#29992;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24314;&#27169;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#20272;&#35745;&#36816;&#36755;&#24615;&#33021;&#21644;&#25506;&#32034;&#21475;&#34955;&#20301;&#28857;&#12290;&#36890;&#36807;&#25913;&#36827;&#25968;&#20540;&#26041;&#27861;&#20197;&#21450;&#26368;&#36817;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#22686;&#24378;MD&#27169;&#25311;&#30340;&#25928;&#29575;&#24050;&#32463;&#26377;&#20102;&#24456;&#38271;&#30340;&#21382;&#21490;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#20934;&#30830;&#24314;&#27169;&#25193;&#23637;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#25311;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NeuralMD&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;ML&#36741;&#21161;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#29702;&#30340;&#26041;&#27861;&#65292;&#23558;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#19968;&#20010;&#20351;&#29992;&#21521;&#37327;&#26694;&#26550;&#28385;&#36275;&#32676;&#23545;&#31216;&#24615;&#24182;&#25429;&#33719;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;BindingNet&#27169;&#22411;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#20010;&#22686;&#24378;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#23398;&#20064;&#36712;&#36857;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In drug discovery, molecular dynamics (MD) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities, estimating transport properties, and exploring pocket sites. There has been a long history of improving the efficiency of MD simulations through better numerical methods and, more recently, by augmenting them with machine learning (ML) methods. Yet, challenges remain, such as accurate modeling of extended-timescale simulations. To address this issue, we propose NeuralMD, the first ML surrogate that can facilitate numerical MD and provide accurate simulations of protein-ligand binding dynamics. We propose a principled approach that incorporates a novel physics-informed multi-grained group symmetric framework. Specifically, we propose (1) a BindingNet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions, and (2) an augmented neural differential equation solver that learns the trajectory und
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28014;&#28857;&#36816;&#31639;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20108;&#36827;&#21046;&#38408;&#20540;&#21333;&#20803;&#25110;ReLU&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#35760;&#24518;&#20219;&#20309;&#23454;&#25968;&#36755;&#20837;/&#36755;&#20986;&#23545;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23567;&#35823;&#24046;&#20869;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.15121</link><description>&lt;p&gt;
ReLU&#21644;Step&#32593;&#32476;&#22312;&#28014;&#28857;&#36816;&#31639;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Expressive Power of ReLU and Step Networks under Floating-Point Operations. (arXiv:2401.15121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28014;&#28857;&#36816;&#31639;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20108;&#36827;&#21046;&#38408;&#20540;&#21333;&#20803;&#25110;ReLU&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#35760;&#24518;&#20219;&#20309;&#23454;&#25968;&#36755;&#20837;/&#36755;&#20986;&#23545;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23567;&#35823;&#24046;&#20869;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32467;&#26524;&#20551;&#35774;&#23454;&#25968;&#36755;&#20837;&#21644;&#21442;&#25968;&#20197;&#21450;&#22312;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#36807;&#31243;&#20013;&#36827;&#34892;&#31934;&#30830;&#36816;&#31639;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#22312;&#21482;&#33021;&#34920;&#31034;&#23454;&#25968;&#30340;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#65292;&#24182;&#19988;&#36827;&#34892;&#19981;&#31934;&#30830;&#30340;&#36816;&#31639;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#26356;&#23454;&#38469;&#30340;&#35774;&#32622;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65306;&#20351;&#29992;&#28014;&#28857;&#25968;&#21644;&#28014;&#28857;&#36816;&#31639;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#32452;&#32467;&#26524;&#20551;&#35774;&#28014;&#28857;&#36816;&#31639;&#20013;&#65292;&#28014;&#28857;&#25968;&#30340;&#26377;&#25928;&#20301;&#25968;&#30001;&#26377;&#38480;&#20301;&#34920;&#31034;&#65292;&#20294;&#20854;&#25351;&#25968;&#21487;&#20197;&#21462;&#20219;&#20309;&#25972;&#25968;&#20540;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#20108;&#36827;&#21046;&#38408;&#20540;&#21333;&#20803;&#25110;ReLU&#21487;&#20197;&#35760;&#24518;&#20219;&#20309;&#26377;&#38480;&#30340;&#36755;&#20837;/&#36755;&#20986;&#23545;&#65292;&#24182;&#21487;&#20197;&#22312;&#23567;&#35823;&#24046;&#20869;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#28014;&#28857;&#36816;&#31639;&#19979;&#20851;&#20110;&#35760;&#24518;&#21644;&#36890;&#29992;&#36924;&#36817;&#30340;&#31867;&#20284;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of the expressive power of neural networks has investigated the fundamental limits of neural networks. Most existing results assume real-valued inputs and parameters as well as exact operations during the evaluation of neural networks. However, neural networks are typically executed on computers that can only represent a tiny subset of the reals and apply inexact operations. In this work, we analyze the expressive power of neural networks under a more realistic setup: when we use floating-point numbers and operations. Our first set of results assumes floating-point operations where the significand of a float is represented by finite bits but its exponent can take any integer value. Under this setup, we show that neural networks using a binary threshold unit or ReLU can memorize any finite input/output pairs and can approximate any continuous function within a small error. We also show similar results on memorization and universal approximation when floating-point operations u
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#35299;&#37322;&#20102;&#26368;&#20808;&#36827;&#30340;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;13&#20010;&#36755;&#20837;&#29305;&#24449;&#19982;3,142&#20010;&#32654;&#22269;&#21439;&#30340;&#19977;&#24180;&#26085;&#26696;&#20363;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#33021;&#22815;&#22312;&#36807;&#21435;&#20004;&#21608;&#30340;&#22522;&#30784;&#19978;&#39044;&#27979;&#25509;&#19979;&#26469;&#20004;&#21608;&#30340;COVID-19&#24863;&#26579;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#24863;&#26579;&#23545;8&#20010;&#20154;&#21475;&#24180;&#40836;&#32452;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15119</link><description>&lt;p&gt;
&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#21644;COVID-19&#24863;&#26579;&#23545;&#20154;&#21475;&#24180;&#40836;&#32452;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Interpreting Time Series Transformer Models and Sensitivity Analysis of Population Age Groups to COVID-19 Infections. (arXiv:2401.15119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15119
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#35299;&#37322;&#20102;&#26368;&#20808;&#36827;&#30340;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;13&#20010;&#36755;&#20837;&#29305;&#24449;&#19982;3,142&#20010;&#32654;&#22269;&#21439;&#30340;&#19977;&#24180;&#26085;&#26696;&#20363;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#33021;&#22815;&#22312;&#36807;&#21435;&#20004;&#21608;&#30340;&#22522;&#30784;&#19978;&#39044;&#27979;&#25509;&#19979;&#26469;&#20004;&#21608;&#30340;COVID-19&#24863;&#26579;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#24863;&#26579;&#23545;8&#20010;&#20154;&#21475;&#24180;&#40836;&#32452;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#34892;&#20026;&#21644;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#23398;&#20064;&#27169;&#24335;&#20197;&#36827;&#34892;&#23454;&#26102;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;Transformer&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#20351;&#35299;&#37322;&#20010;&#20307;&#29305;&#24449;&#23545;&#39044;&#27979;&#24433;&#21709;&#30340;&#25361;&#25112;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#30340;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#26469;&#35299;&#37322;&#26368;&#20808;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#12290;&#20026;&#20102;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;3142&#20010;&#32654;&#22269;&#21439;&#30340;&#19977;&#24180;&#26085;&#26696;&#20363;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20845;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#36873;&#25321;&#20102;&#26368;&#20339;&#30340;COVID-19&#24863;&#26579;&#39044;&#27979;&#27169;&#22411;&#12290;&#20351;&#29992;&#36807;&#21435;&#20004;&#21608;&#30340;13&#20010;&#36755;&#20837;&#29305;&#24449;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#27979;&#25509;&#19979;&#26469;&#20004;&#21608;&#30340;&#30149;&#20363;&#25968;&#37327;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#39044;&#27979;&#23545;8&#20010;&#20154;&#21475;&#24180;&#40836;&#32452;&#30340;&#25935;&#24863;&#24615;&#65292;&#20197;&#21450;&#39640;&#24230;&#21160;&#24577;&#30340;&#22810;&#21464;&#37327;&#24863;&#26579;&#25968;&#25454;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#25200;&#21160;&#30340;&#35299;&#37322;&#26041;&#27861;&#19982;&#30456;&#20851;&#24037;&#20316;&#36827;&#34892;&#27604;&#36739;&#65292;&#24635;&#20849;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Interpreting deep learning time series models is crucial in understanding the model's behavior and learning patterns from raw data for real-time decision-making. However, the complexity inherent in transformer-based time series models poses challenges in explaining the impact of individual features on predictions. In this study, we leverage recent local interpretation methods to interpret state-of-the-art time series models. To use real-world datasets, we collected three years of daily case data for 3,142 US counties. Firstly, we compare six transformer-based models and choose the best prediction model for COVID-19 infection. Using 13 input features from the last two weeks, we can predict the cases for the next two weeks. Secondly, we present an innovative way to evaluate the prediction sensitivity to 8 population age groups over highly dynamic multivariate infection data. Thirdly, we compare our proposed perturbation-based interpretation method with related work, including a total of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22312;&#32447;&#20247;&#21253;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#26631;&#27880;&#32773;&#20013;&#32858;&#21512;&#22797;&#26434;&#27880;&#37322;&#65292;&#36890;&#36807;&#25512;&#26029;&#26631;&#27880;&#32773;&#30340;&#20934;&#30830;&#24615;&#26469;&#25913;&#21892;&#25104;&#26412;-&#36136;&#37327;&#26435;&#34913;&#12290;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#22312;&#30495;&#23454;&#20247;&#21253;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.15116</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#22312;&#32447;&#20247;&#21253;&#19982;&#22797;&#26434;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Crowdsourcing with Complex Annotations. (arXiv:2401.15116v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22312;&#32447;&#20247;&#21253;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#26631;&#27880;&#32773;&#20013;&#32858;&#21512;&#22797;&#26434;&#27880;&#37322;&#65292;&#36890;&#36807;&#25512;&#26029;&#26631;&#27880;&#32773;&#30340;&#20934;&#30830;&#24615;&#26469;&#25913;&#21892;&#25104;&#26412;-&#36136;&#37327;&#26435;&#34913;&#12290;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#22312;&#30495;&#23454;&#20247;&#21253;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#21253;&#24179;&#21488;&#20351;&#29992;&#21508;&#31181;&#30495;&#23454;&#24615;&#21457;&#29616;&#31639;&#27861;&#26469;&#32858;&#21512;&#26469;&#33258;&#22810;&#20010;&#26631;&#27880;&#32773;&#30340;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#65292;&#20027;&#35201;&#25361;&#25112;&#26159;&#20915;&#23450;&#26159;&#21542;&#20026;&#27599;&#20010;&#39033;&#30446;&#35831;&#27714;&#26356;&#22810;&#30340;&#27880;&#37322;&#65292;&#20197;&#39640;&#25928;&#22320;&#26435;&#34913;&#25104;&#26412;&#65288;&#21363;&#27880;&#37322;&#25968;&#37327;&#65289;&#21644;&#32858;&#21512;&#27880;&#37322;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22312;&#32447;&#20247;&#21253;&#29615;&#22659;&#30340;&#36890;&#29992;&#22797;&#26434;&#27880;&#37322;&#65288;&#22914;&#36793;&#30028;&#26694;&#21644;&#20998;&#31867;&#36335;&#24452;&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26631;&#27880;&#32773;&#30340;&#39044;&#26399;&#24179;&#22343;&#30456;&#20284;&#24230;&#19982;&#20182;&#20204;&#30340;&#20934;&#30830;&#24615;&#22312;"&#32473;&#23450;&#25253;&#21578;&#30340;&#26631;&#31614;"&#26465;&#20214;&#19979;&#26159;&#32447;&#24615;&#20851;&#31995;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#25512;&#26029;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#30340;&#25253;&#21578;&#26631;&#31614;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#22312;Meta&#30340;&#30495;&#23454;&#20247;&#21253;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22312;&#32447;&#31639;&#27861;&#22312;&#25552;&#21319;&#25104;&#26412;-&#36136;&#37327;&#26435;&#34913;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crowdsourcing platforms use various truth discovery algorithms to aggregate annotations from multiple labelers. In an online setting, however, the main challenge is to decide whether to ask for more annotations for each item to efficiently trade off cost (i.e., the number of annotations) for quality of the aggregated annotations. In this paper, we propose a novel approach for general complex annotation (such as bounding boxes and taxonomy paths), that works in an online crowdsourcing setting. We prove that the expected average similarity of a labeler is linear in their accuracy \emph{conditional on the reported label}. This enables us to infer reported label accuracy in a broad range of scenarios. We conduct extensive evaluations on real-world crowdsourcing data from Meta and show the effectiveness of our proposed online algorithms in improving the cost-quality trade-off.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#65292;&#22312;&#22810;&#31181;&#22320;&#24418;&#21644;&#20256;&#24863;&#22120;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#24182;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15113</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#23454;&#29616;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;
&lt;/p&gt;
&lt;p&gt;
Towards Global Glacier Mapping with Deep Learning and Open Earth Observation Data. (arXiv:2401.15113v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#65292;&#22312;&#22810;&#31181;&#22320;&#24418;&#21644;&#20256;&#24863;&#22120;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#24182;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#23545;&#20110;&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#36807;&#31243;&#21463;&#21040;&#20912;&#24029;&#22810;&#26679;&#24615;&#12289;&#38590;&#20197;&#20998;&#31867;&#30340;&#30862;&#30707;&#21644;&#22823;&#25968;&#25454;&#22788;&#29702;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Glacier-VisionTransformer-U-Net (GlaViTU)&#65292;&#19968;&#20010;&#21367;&#31215;-Transformer&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20116;&#31181;&#21033;&#29992;&#24320;&#25918;&#21355;&#26143;&#24433;&#20687;&#36827;&#34892;&#22810;&#26102;&#30456;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#31574;&#30053;&#12290;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#36328;&#20256;&#24863;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31574;&#30053;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;IoU&#65288;&#20132;&#24182;&#27604;&#65289;&gt; 0.85&#65292;&#24182;&#19988;&#22312;&#20197;&#20912;&#38634;&#20026;&#20027;&#30340;&#22320;&#21306;&#22686;&#21152;&#21040;&#20102;&gt; 0.90&#65292;&#32780;&#22312;&#39640;&#23665;&#20122;&#27954;&#31561;&#30862;&#30707;&#20016;&#23500;&#30340;&#21306;&#22495;&#21017;&#38477;&#33267;&gt; 0.75&#12290;&#27492;&#22806;&#65292;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#21363;&#22238;&#27874;&#21644;&#24178;&#28041;&#30456;&#24178;&#24230;&#65292;&#21487;&#20197;&#25552;&#39640;&#25152;&#26377;&#21487;&#29992;&#22320;&#21306;&#30340;&#20934;&#30830;&#24615;&#12290;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#20351;&#39044;&#27979;&#26356;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate global glacier mapping is critical for understanding climate change impacts. It is challenged by glacier diversity, difficult-to-classify debris and big data processing. Here we propose Glacier-VisionTransformer-U-Net (GlaViTU), a convolutional-transformer deep learning model, and five strategies for multitemporal global-scale glacier mapping using open satellite imagery. Assessing the spatial, temporal and cross-sensor generalisation shows that our best strategy achieves intersection over union &gt;0.85 on previously unobserved images in most cases, which drops to &gt;0.75 for debris-rich areas such as High-Mountain Asia and increases to &gt;0.90 for regions dominated by clean ice. Additionally, adding synthetic aperture radar data, namely, backscatter and interferometric coherence, increases the accuracy in all regions where available. The calibrated confidence for glacier extents is reported making the predictions more reliable and interpretable. We also release a benchmark dataset 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#25913;&#21892;&#33016;&#37096;X&#23556;&#32447;&#35786;&#26029;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#21033;&#29992;&#31934;&#24515;&#36873;&#25321;&#30340;&#27491;&#36127;&#26679;&#26412;&#29983;&#25104;&#20844;&#24179;&#30340;&#22270;&#20687;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2401.15111</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25913;&#21892;&#33258;&#21160;&#21270;&#33016;&#37096;X&#23556;&#32447;&#35786;&#26029;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Fairness of Automated Chest X-ray Diagnosis by Contrastive Learning. (arXiv:2401.15111v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15111
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#25913;&#21892;&#33016;&#37096;X&#23556;&#32447;&#35786;&#26029;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#21033;&#29992;&#31934;&#24515;&#36873;&#25321;&#30340;&#27491;&#36127;&#26679;&#26412;&#29983;&#25104;&#20844;&#24179;&#30340;&#22270;&#20687;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#22312;&#25918;&#23556;&#23398;&#39046;&#22495;&#20013;&#65292;&#26377;&#38480;&#30340;&#30740;&#31350;&#25506;&#32034;&#35299;&#20915;&#21644;&#25552;&#21319;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#20855;&#20307;&#26041;&#27861;&#25110;&#36884;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;AI&#27169;&#22411;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#20943;&#23569;&#33016;&#37096;X&#23556;&#32447;&#35786;&#26029;&#20013;&#30340;&#20559;&#24046;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#22312;&#36825;&#39033;&#22238;&#39038;&#24615;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65306;&#21307;&#23398;&#24433;&#20687;&#21644;&#25968;&#25454;&#36164;&#28304;&#20013;&#24515;&#65288;MIDRC&#65289;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;77,887&#24352;&#26469;&#33258;27,796&#21517;&#24739;&#32773;&#30340;CXR&#22270;&#20687;&#65292;&#25910;&#38598;&#25130;&#33267;2023&#24180;4&#26376;20&#26085;&#65292;&#29992;&#20110;COVID-19&#35786;&#26029;&#65307;&#22269;&#31435;&#21355;&#29983;&#30740;&#31350;&#38498;&#33016;&#37096;X&#23556;&#32447;&#65288;NIH-CXR&#65289;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;112,120&#24352;&#26469;&#33258;30,805&#21517;&#24739;&#32773;&#22312;1992&#24180;&#33267;2015&#24180;&#38388;&#25910;&#38598;&#30340;CXR&#22270;&#20687;&#12290;&#22312;NIH-CXR&#25968;&#25454;&#38598;&#20013;&#65292;&#33016;&#37096;&#24322;&#24120;&#21253;&#25324;&#32954;&#19981;&#24352;&#12289;&#24515;&#33039;&#22686;&#22823;&#12289;&#31215;&#28082;&#12289;&#28024;&#28070;&#12289;&#32959;&#22359;&#12289;&#32467;&#33410;&#12289;&#32954;&#28814;&#12289;&#27668;&#33016;&#12289;&#23454;&#21464;&#12289;&#27700;&#32959;&#12289;&#32954;&#27668;&#32959;&#12289;&#32420;&#32500;&#21270;&#12289;&#33016;&#33180;&#22686;&#21402;&#25110;&#30109;&#27668;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#31934;&#24515;&#36873;&#25321;&#30340;&#27491;&#36127;&#26679;&#26412;&#36827;&#34892;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#29983;&#25104;&#20844;&#24179;&#30340;&#22270;&#20687;&#23884;&#20837;&#65292;&#28982;&#21518;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Limited studies exploring concrete methods or approaches to tackle and enhance model fairness in the radiology domain. Our proposed AI model utilizes supervised contrastive learning to minimize bias in CXR diagnosis.  Materials and Methods: In this retrospective study, we evaluated our proposed method on two datasets: the Medical Imaging and Data Resource Center (MIDRC) dataset with 77,887 CXR images from 27,796 patients collected as of April 20, 2023 for COVID-19 diagnosis, and the NIH Chest X-ray (NIH-CXR) dataset with 112,120 CXR images from 30,805 patients collected between 1992 and 2015. In the NIH-CXR dataset, thoracic abnormalities include atelectasis, cardiomegaly, effusion, infiltration, mass, nodule, pneumonia, pneumothorax, consolidation, edema, emphysema, fibrosis, pleural thickening, or hernia. Our proposed method utilizes supervised contrastive learning with carefully selected positive and negative samples to generate fair image embeddings, which are fine-tuned f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#24555;&#36895;&#20805;&#30005;&#30005;&#21160;&#36710;&#20013;&#24515;&#30340;&#21160;&#24577;&#23450;&#20215;&#31454;&#20105;&#12290;&#36890;&#36807;&#39044;&#27979;&#24615;&#36141;&#20080;&#30005;&#21147;&#38656;&#27714;&#21644;&#35774;&#23450;&#31454;&#20105;&#24615;&#20215;&#26684;&#31574;&#30053;&#65292;&#20805;&#30005;&#31449;&#21487;&#20197;&#22312;&#31454;&#20105;&#20013;&#36827;&#34892;&#26377;&#25928;&#23450;&#20215;&#12290;</title><link>http://arxiv.org/abs/2401.15108</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#31454;&#20105;&#20013;&#20026;&#24555;&#36895;&#20805;&#30005;&#30005;&#21160;&#36710;&#20013;&#24515;&#30340;&#21160;&#24577;&#23450;&#20215;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Deep Reinforcement Learning for Dynamic Pricing by Fast-charging Electric Vehicle Hubs in ccompetition. (arXiv:2401.15108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#24555;&#36895;&#20805;&#30005;&#30005;&#21160;&#36710;&#20013;&#24515;&#30340;&#21160;&#24577;&#23450;&#20215;&#31454;&#20105;&#12290;&#36890;&#36807;&#39044;&#27979;&#24615;&#36141;&#20080;&#30005;&#21147;&#38656;&#27714;&#21644;&#35774;&#23450;&#31454;&#20105;&#24615;&#20215;&#26684;&#31574;&#30053;&#65292;&#20805;&#30005;&#31449;&#21487;&#20197;&#22312;&#31454;&#20105;&#20013;&#36827;&#34892;&#26377;&#25928;&#23450;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#20805;&#30005;&#31449;&#23558;&#25104;&#20026;&#20840;&#29699;&#26032;&#24314;&#20132;&#36890;&#30005;&#27668;&#21270;&#22522;&#30784;&#35774;&#26045;&#30340;&#19968;&#37096;&#20998;&#12290;&#36825;&#20123;&#20805;&#30005;&#31449;&#23558;&#25215;&#36733;&#35768;&#22810;&#30452;&#27969;&#24555;&#36895;&#20805;&#30005;&#35774;&#22791;&#65292;&#20165;&#21487;&#20379;&#30005;&#21160;&#36710;&#36742;&#20805;&#30005;&#20351;&#29992;&#12290;&#31867;&#20284;&#20110;&#27773;&#27833;&#21152;&#27833;&#31449;&#65292;&#21516;&#19968;&#22320;&#21306;&#30340;&#24555;&#36895;&#20805;&#30005;&#31449;&#23558;&#26681;&#25454;&#31454;&#20105;&#35843;&#25972;&#20215;&#26684;&#20197;&#21560;&#24341;&#21516;&#19968;&#32676;&#30005;&#21160;&#36710;&#20027;&#12290;&#36825;&#20123;&#20805;&#30005;&#31449;&#23558;&#19982;&#30005;&#21147;&#32593;&#32476;&#36827;&#34892;&#20132;&#20114;&#65292;&#36890;&#36807;&#39044;&#27979;&#24615;&#36141;&#20080;&#22312;&#21069;&#19968;&#22825;&#30005;&#21147;&#24066;&#22330;&#19978;&#30340;&#30005;&#21147;&#38656;&#27714;&#65292;&#24182;&#22312;&#23454;&#26102;&#24066;&#22330;&#19978;&#28385;&#36275;&#24046;&#39069;&#38656;&#27714;&#12290;&#20805;&#30005;&#31449;&#21487;&#33021;&#37197;&#22791;&#34917;&#20805;&#30005;&#27744;&#20648;&#33021;&#31995;&#32479;&#29992;&#20110;&#22871;&#21033;&#12290;&#26412;&#25991;&#38024;&#23545;&#20805;&#30005;&#31449;&#31454;&#20105;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#27493;&#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#24577;&#23450;&#20215;&#26041;&#27861;&#12290;&#39318;&#20808;&#36890;&#36807;&#27714;&#35299;&#38543;&#26426;&#30340;&#21069;&#19968;&#22825;&#30005;&#21147;&#38656;&#27714;&#27169;&#22411;&#24471;&#21040;&#32435;&#20837;&#25215;&#35834;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#28216;&#25103;&#24314;&#27169;&#20026;&#31454;&#20105;&#26469;&#24471;&#21040;&#20805;&#30005;&#31449;&#30340;&#20215;&#26684;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast-charging hubs for electric vehicles will soon become part of the newly built infrastructure for transportation electrification across the world. These hubs are expected to host many DC fast-charging stations and will admit EVs only for charging. Like the gasoline refueling stations, fast-charging hubs in a neighborhood will dynamically vary their prices to compete for the same pool of EV owners. These hubs will interact with the electric power network by making purchase commitments for a significant part of their power needs in the day-ahead (DA) electricity market and meeting the difference from the real-time (RT) market. Hubs may have supplemental battery storage systems (BSS), which they will use for arbitrage. In this paper, we develop a two-step data-driven dynamic pricing methodology for hubs in price competition. We first obtain the DA commitment by solving a stochastic DA commitment model. Thereafter we obtain the hub pricing strategies by modeling the game as a competitiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#32500;&#26446;&#32676;&#19978;&#20248;&#21270;&#21160;&#24577;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21160;&#24577;&#31995;&#32479;&#34920;&#31034;&#20026;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#22312;&#26446;&#32676;&#19978;&#21046;&#23450;&#20248;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22312;&#26446;&#20195;&#25968;&#32423;&#21035;&#34920;&#31034;&#31995;&#32479;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#19968;&#20010;&#20363;&#23376;&#20013;&#65292;&#22788;&#29702;&#20102;&#21018;&#20307;&#25511;&#21046;&#30340;&#26368;&#20248;&#21183;&#33021;&#22609;&#24418;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#25511;&#21046;&#22120;&#26469;&#39564;&#35777;&#26368;&#32456;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.15107</link><description>&lt;p&gt;
&#22312;&#26446;&#32676;&#19978;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23545;SE(3)&#30340;&#20248;&#21270;&#28508;&#21147;&#22609;&#36896;
&lt;/p&gt;
&lt;p&gt;
Optimal Potential Shaping on SE(3) via Neural ODEs on Lie Groups. (arXiv:2401.15107v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#32500;&#26446;&#32676;&#19978;&#20248;&#21270;&#21160;&#24577;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21160;&#24577;&#31995;&#32479;&#34920;&#31034;&#20026;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#22312;&#26446;&#32676;&#19978;&#21046;&#23450;&#20248;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22312;&#26446;&#20195;&#25968;&#32423;&#21035;&#34920;&#31034;&#31995;&#32479;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#19968;&#20010;&#20363;&#23376;&#20013;&#65292;&#22788;&#29702;&#20102;&#21018;&#20307;&#25511;&#21046;&#30340;&#26368;&#20248;&#21183;&#33021;&#22609;&#24418;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#25511;&#21046;&#22120;&#26469;&#39564;&#35777;&#26368;&#32456;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#26377;&#38480;&#32500;&#26446;&#32676;&#19978;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#23558;&#21160;&#24577;&#31995;&#32479;&#37325;&#26032;&#34920;&#36848;&#20026;&#25152;&#35859;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(neural ODEs)&#65292;&#24182;&#22312;&#26446;&#32676;&#19978;&#21046;&#23450;&#20248;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#25968;&#20540;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#25193;&#23637;&#65292;&#24182;&#36866;&#29992;&#20110;&#20219;&#20309;&#26377;&#38480;&#32500;&#26446;&#32676;&#65292;&#21253;&#25324;&#30697;&#38453;&#26446;&#32676;&#12290;&#36890;&#36807;&#22312;&#26446;&#20195;&#25968;&#32423;&#21035;&#34920;&#31034;&#31995;&#32479;&#65292;&#20943;&#23569;&#20102;&#26799;&#24230;&#35745;&#31639;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#19968;&#20010;&#24191;&#27867;&#30340;&#20363;&#23376;&#20013;&#65292;&#22788;&#29702;&#20102;&#23545;&#21018;&#20307;&#25511;&#21046;&#30340;&#26368;&#20248;&#21183;&#33021;&#22609;&#24418;&#12290;&#23558;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#34920;&#36848;&#20026;&#23545;&#26446;&#32676;SE(3)&#19978;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#30340;&#20248;&#21270;&#65292;&#24182;&#23545;&#25511;&#21046;&#22120;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#12290;&#26368;&#21518;&#65292;&#22312;&#29366;&#24577;&#35843;&#33410;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#26368;&#32456;&#30340;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a novel approach for the optimization of dynamic systems on finite-dimensional Lie groups. We rephrase dynamic systems as so-called neural ordinary differential equations (neural ODEs), and formulate the optimization problem on Lie groups. A gradient descent optimization algorithm is presented to tackle the optimization numerically. Our algorithm is scalable, and applicable to any finite dimensional Lie group, including matrix Lie groups. By representing the system at the Lie algebra level, we reduce the computational cost of the gradient computation. In an extensive example, optimal potential energy shaping for control of a rigid body is treated. The optimal control problem is phrased as an optimization of a neural ODE on the Lie group SE(3), and the controller is iteratively optimized. The final controller is validated on a state-regulation task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#22686;&#24378;&#30340;&#20113;&#21435;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#21644;&#26041;&#27861;&#19978;&#36827;&#34892;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#23545;&#36229;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#20013;&#20113;&#23618;&#30340;&#20934;&#30830;&#21435;&#38500;&#21644;&#35814;&#32454;&#35821;&#20041;&#20869;&#23481;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2401.15105</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#22686;&#24378;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#20113;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
Diffusion Enhancement for Cloud Removal in Ultra-Resolution Remote Sensing Imagery. (arXiv:2401.15105v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#22686;&#24378;&#30340;&#20113;&#21435;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#21644;&#26041;&#27861;&#19978;&#36827;&#34892;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#23545;&#36229;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#20013;&#20113;&#23618;&#30340;&#20934;&#30830;&#21435;&#38500;&#21644;&#35814;&#32454;&#35821;&#20041;&#20869;&#23481;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#23618;&#30340;&#23384;&#22312;&#20005;&#37325;&#24433;&#21709;&#20102;&#20809;&#23398;&#36965;&#24863;&#65288;RS&#65289;&#22270;&#20687;&#30340;&#36136;&#37327;&#21644;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#20113;&#21435;&#38500;&#65288;CR&#65289;&#25216;&#26415;&#22312;&#20934;&#30830;&#37325;&#24314;&#22270;&#20687;&#30340;&#35270;&#35273;&#30495;&#23454;&#24615;&#21644;&#35814;&#32454;&#35821;&#20041;&#20869;&#23481;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#21644;&#26041;&#27861;&#19978;&#36827;&#34892;&#25913;&#36827;&#30340;&#26041;&#26696;&#12290;&#22312;&#25968;&#25454;&#26041;&#38754;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;CUHK Cloud Removal (CUHK-CR) &#30340;0.5m&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#22522;&#20934;&#24182;&#21152;&#20837;&#20102;&#20016;&#23500;&#30340;&#32454;&#33410;&#32441;&#29702;&#21644;&#22810;&#26679;&#21270;&#30340;&#20113;&#35206;&#30422;&#65292;&#20026;&#35774;&#35745;&#21644;&#35780;&#20272;CR&#27169;&#22411;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#22522;&#30784;&#12290;&#20174;&#26041;&#27861;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Diffusion Enhancement&#65288;DE&#65289;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;CR&#26694;&#26550;&#65292;&#29992;&#20110;&#36827;&#34892;&#28176;&#36827;&#32441;&#29702;&#32454;&#33410;&#24674;&#22797;&#65292;&#20197;&#20943;&#36731;&#35757;&#32451;&#38590;&#24230;&#24182;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26435;&#37325;&#20998;&#37197;&#65288;WA&#65289;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
The presence of cloud layers severely compromises the quality and effectiveness of optical remote sensing (RS) images. However, existing deep-learning (DL)-based Cloud Removal (CR) techniques encounter difficulties in accurately reconstructing the original visual authenticity and detailed semantic content of the images. To tackle this challenge, this work proposes to encompass enhancements at the data and methodology fronts. On the data side, an ultra-resolution benchmark named CUHK Cloud Removal (CUHK-CR) of 0.5m spatial resolution is established. This benchmark incorporates rich detailed textures and diverse cloud coverage, serving as a robust foundation for designing and assessing CR models. From the methodology perspective, a novel diffusion-based framework for CR called Diffusion Enhancement (DE) is proposed to perform progressive texture detail recovery, which mitigates the training difficulty with improved inference accuracy. Additionally, a Weight Allocation (WA) network is dev
&lt;/p&gt;</description></item><item><title>PruneSymNet&#26159;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#36138;&#23146;&#20462;&#21098;&#31639;&#27861;&#25552;&#21462;&#23376;&#32593;&#32476;&#20197;&#33719;&#24471;&#25152;&#38656;&#30340;&#31526;&#21495;&#34920;&#36798;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.15103</link><description>&lt;p&gt;
PruneSymNet&#65306;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#30340;&#31526;&#21495;&#31070;&#32463;&#32593;&#32476;&#21644;&#20462;&#21098;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
PruneSymNet: A Symbolic Neural Network and Pruning Algorithm for Symbolic Regression. (arXiv:2401.15103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15103
&lt;/p&gt;
&lt;p&gt;
PruneSymNet&#26159;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#36138;&#23146;&#20462;&#21098;&#31639;&#27861;&#25552;&#21462;&#23376;&#32593;&#32476;&#20197;&#33719;&#24471;&#25152;&#38656;&#30340;&#31526;&#21495;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26088;&#22312;&#20174;&#25968;&#25454;&#20013;&#23548;&#20986;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#34920;&#36798;&#24335;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#35299;&#37322;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PruneSymNet&#30340;&#31526;&#21495;&#32593;&#32476;&#65292;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#12290;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#28608;&#27963;&#20989;&#25968;&#30001;&#24120;&#35265;&#30340;&#22522;&#26412;&#20989;&#25968;&#21644;&#36816;&#31639;&#31526;&#32452;&#25104;&#12290;&#25972;&#20010;&#32593;&#32476;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#23376;&#32593;&#32476;&#23545;&#24212;&#19968;&#20010;&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#21462;&#36825;&#20123;&#23376;&#32593;&#32476;&#20197;&#33719;&#24471;&#25152;&#38656;&#30340;&#31526;&#21495;&#34920;&#36798;&#24335;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#23146;&#20462;&#21098;&#31639;&#27861;&#65292;&#23558;&#32593;&#32476;&#21098;&#25104;&#23376;&#32593;&#32476;&#65292;&#21516;&#26102;&#30830;&#20445;&#25968;&#25454;&#25311;&#21512;&#30340;&#31934;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#36138;&#23146;&#20462;&#21098;&#31639;&#27861;&#27599;&#27425;&#20462;&#21098;&#37117;&#20445;&#30041;&#25439;&#22833;&#26368;&#23567;&#30340;&#36793;&#65292;&#20294;&#36138;&#23146;&#31639;&#27861;&#36890;&#24120;&#26080;&#27861;&#24471;&#21040;&#26368;&#20248;&#35299;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#26463;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression aims to derive interpretable symbolic expressions from data in order to better understand and interpret data. %which plays an important role in knowledge discovery and interpretable machine learning.  In this study, a symbolic network called PruneSymNet is proposed for symbolic regression. This is a novel neural network whose activation function consists of common elementary functions and operators. The whole network is differentiable and can be trained by gradient descent method. Each subnetwork in the network corresponds to an expression, and our goal is to extract such subnetworks to get the desired symbolic expression.  Therefore, a greedy pruning algorithm is proposed to prune the network into a subnetwork while ensuring the accuracy of data fitting. The proposed greedy pruning algorithm preserves the edge with the least loss in each pruning, but greedy algorithm often can not get the optimal solution. In order to alleviate this problem, we combine beam search 
&lt;/p&gt;</description></item><item><title>Hi-Core&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#30693;&#35782;&#36801;&#31227;&#26469;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;Hi-Core&#23637;&#29616;&#20102;&#36739;&#24378;&#30340;&#30693;&#35782;&#36801;&#31227;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.15098</link><description>&lt;p&gt;
Hi-Core: &#38754;&#21521;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#23618;&#27425;&#21270;&#30693;&#35782;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning. (arXiv:2401.15098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15098
&lt;/p&gt;
&lt;p&gt;
Hi-Core&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#30693;&#35782;&#36801;&#31227;&#26469;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;Hi-Core&#23637;&#29616;&#20102;&#36739;&#24378;&#30340;&#30693;&#35782;&#36801;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#65288;Continual Reinforcement Learning, CRL&#65289;&#36171;&#20104;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20174;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20445;&#30041;&#20808;&#21069;&#30340;&#30693;&#35782;&#24182;&#21033;&#29992;&#23427;&#26469;&#20419;&#36827;&#26410;&#26469;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#19987;&#27880;&#20110;&#22312;&#31867;&#20284;&#20219;&#21153;&#20043;&#38388;&#20256;&#36755;&#20302;&#23618;&#27425;&#30340;&#30693;&#35782;&#65292;&#24573;&#35270;&#20102;&#20154;&#31867;&#35748;&#30693;&#25511;&#21046;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#23548;&#33268;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#36801;&#31227;&#19981;&#36275;&#12290;&#20026;&#20102;&#22686;&#24378;&#39640;&#23618;&#27425;&#30340;&#30693;&#35782;&#36801;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning)&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#30001;&#20004;&#23618;&#32467;&#26500;&#32452;&#25104;&#65306;1) &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Model, LLM&#65289;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;2) &#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#30693;&#35782;&#24211;&#65288;&#31574;&#30053;&#24211;&#65289;&#26469;&#23384;&#20648;&#21487;&#20197;&#29992;&#20110;&#23618;&#27425;&#21270;&#30693;&#35782;&#36801;&#31227;&#30340;&#31574;&#30053;&#12290;&#22312;MiniGr&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual reinforcement learning (CRL) empowers RL agents with the ability to learn from a sequence of tasks, preserving previous knowledge and leveraging it to facilitate future learning. However, existing methods often focus on transferring low-level knowledge across similar tasks, which neglects the hierarchical structure of human cognitive control, resulting in insufficient knowledge transfer across diverse tasks. To enhance high-level knowledge transfer, we propose a novel framework named Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning), which is structured in two layers: 1) the high-level policy formulation which utilizes the powerful reasoning ability of the Large Language Model (LLM) to set goals and 2) the low-level policy learning through RL which is oriented by high-level goals. Moreover, the knowledge base (policy library) is constructed to store policies that can be retrieved for hierarchical knowledge transfer. Experiments conducted in MiniGr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#36827;&#21046;&#24863;&#30693;&#26426;&#30340;&#23481;&#37327;&#38382;&#39064;&#65292;&#22312;&#30830;&#23450;&#20102;&#19978;&#30028;&#21644;&#19979;&#30028;&#21518;&#65292;&#32473;&#20986;&#20102;&#35777;&#26126;&#35813;&#23481;&#37327;&#23567;&#20110;0.847&#30340;&#26465;&#20214;&#19968;&#38454;&#30697;&#26041;&#27861;&#19982;&#24050;&#30693;&#32467;&#26524;&#30340;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.15092</link><description>&lt;p&gt;
&#20108;&#36827;&#21046;&#24863;&#30693;&#26426;&#23481;&#37327;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A note on the capacity of the binary perceptron. (arXiv:2401.15092v1 [math.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15092
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#36827;&#21046;&#24863;&#30693;&#26426;&#30340;&#23481;&#37327;&#38382;&#39064;&#65292;&#22312;&#30830;&#23450;&#20102;&#19978;&#30028;&#21644;&#19979;&#30028;&#21518;&#65292;&#32473;&#20986;&#20102;&#35777;&#26126;&#35813;&#23481;&#37327;&#23567;&#20110;0.847&#30340;&#26465;&#20214;&#19968;&#38454;&#30697;&#26041;&#27861;&#19982;&#24050;&#30693;&#32467;&#26524;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#20108;&#36827;&#21046;&#24863;&#30693;&#26426;&#30340;&#23481;&#37327;&#945;c&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;Krauth&#21644;Mezard&#65288;1989&#65289;&#29468;&#27979;&#20102;&#945;c&#30340;&#26126;&#30830;&#20540;&#65292;&#22823;&#32422;&#31561;&#20110;0.833&#65292;&#26368;&#36817;Ding&#21644;Sun&#65288;2019&#65289;&#24314;&#31435;&#20102;&#19982;&#27492;&#39044;&#27979;&#30456;&#31526;&#30340;&#20005;&#26684;&#19979;&#30028;&#12290;&#20851;&#20110;&#19978;&#30028;&#65292;Kim&#21644;Roche&#65288;1998&#65289;&#20197;&#21450;Talagrand&#65288;1999&#65289;&#20998;&#21035;&#26174;&#31034;&#945;c &lt; 0.996&#65292;&#32780;Krauth&#21644;Mezard&#27010;&#36848;&#20102;&#19968;&#20010;&#21487;&#20197;&#29992;&#20110;&#26174;&#31034;&#945;c &lt; 0.847&#30340;&#35770;&#35777;&#12290;&#36825;&#20010;&#35828;&#26126;&#30340;&#30446;&#30340;&#26159;&#35760;&#24405;&#19968;&#20010;&#23436;&#25972;&#30340;&#35777;&#26126;&#945;c &lt; 0.847&#30340;&#35777;&#26126;&#12290;&#35813;&#35777;&#26126;&#26159;&#19968;&#31181;&#26465;&#20214;&#19968;&#38454;&#30697;&#26041;&#27861;&#19982;&#24050;&#30693;&#30340;&#29699;&#24418;&#24863;&#30693;&#26426;&#32467;&#26524;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining the capacity $\alpha_c$ of the Binary Perceptron is a long-standing problem. Krauth and Mezard (1989) conjectured an explicit value of $\alpha_c$, approximately equal to .833, and a rigorous lower bound matching this prediction was recently established by Ding and Sun (2019). Regarding the upper bound, Kim and Roche (1998) and Talagrand (1999) independently showed that $\alpha_c$ &lt; .996, while Krauth and Mezard outlined an argument which can be used to show that $\alpha_c$ &lt; .847. The purpose of this expository note is to record a complete proof of the bound $\alpha_c$ &lt; .847. The proof is a conditional first moment method combined with known results on the spherical perceptron
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36890;&#29992;&#23436;&#20840;&#31561;&#21464;&#37327;&#21152;&#36895;&#26448;&#26009;&#23646;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#28857;&#36317;&#31163;&#20998;&#24067;(PDD)&#20316;&#20026;&#23398;&#20064;&#31639;&#27861;&#30340;&#34920;&#31034;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20462;&#25913;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#21033;&#29992;PDD&#12290;</title><link>http://arxiv.org/abs/2401.15089</link><description>&lt;p&gt;
&#20351;&#29992;&#36890;&#29992;&#23436;&#20840;&#31561;&#21464;&#37327;&#21152;&#36895;&#26448;&#26009;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Accelerating Material Property Prediction using Generically Complete Isometry Invariants. (arXiv:2401.15089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36890;&#29992;&#23436;&#20840;&#31561;&#21464;&#37327;&#21152;&#36895;&#26448;&#26009;&#23646;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#28857;&#36317;&#31163;&#20998;&#24067;(PDD)&#20316;&#20026;&#23398;&#20064;&#31639;&#27861;&#30340;&#34920;&#31034;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20462;&#25913;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#21033;&#29992;PDD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#26448;&#26009;&#25110;&#26230;&#20307;&#23646;&#24615;&#39044;&#27979;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#23545;&#20256;&#32479;&#27169;&#25311;&#26041;&#27861;&#30340;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26367;&#20195;&#12290;&#23545;&#20110;&#36825;&#20123;&#31639;&#27861;&#30340;&#20851;&#38190;&#31532;&#19968;&#27493;&#26159;&#21608;&#26399;&#24615;&#26230;&#20307;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#31867;&#20284;&#30340;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#31561;&#29289;&#20307;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#21407;&#23376;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#34920;&#31034;&#21487;&#20197;&#22522;&#20110;&#26377;&#38480;&#28857;&#20113;&#36827;&#34892;&#35299;&#37322;&#65292;&#20294;&#26159;&#21608;&#26399;&#24615;&#26230;&#20307;&#30340;&#23610;&#23544;&#26159;&#26080;&#38480;&#30340;&#65292;&#25152;&#20197;&#23427;&#20204;&#30340;&#34920;&#31034;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#28857;&#36317;&#31163;&#20998;&#24067;(PDD)&#65292;&#36825;&#26159;&#19968;&#31181;&#36830;&#32493;&#19988;&#36890;&#29992;&#30340;&#23436;&#20840;&#31561;&#21464;&#37327;&#65292;&#29992;&#20316;&#25105;&#20204;&#23398;&#20064;&#31639;&#27861;&#30340;&#34920;&#31034;&#12290;&#23613;&#31649;PDD&#22312;&#21306;&#20998;&#21608;&#26399;&#24615;&#28857;&#38598;&#30340;&#31561;&#21464;&#24615;&#19978;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20854;&#27809;&#26377;&#32771;&#34385;&#22522;&#30784;&#26448;&#26009;&#30340;&#32452;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#20462;&#25913;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#21033;&#29992;PDD&#21644;...
&lt;/p&gt;
&lt;p&gt;
Material or crystal property prediction using machine learning has grown popular in recent years as it provides a computationally efficient replacement to classical simulation methods. A crucial first step for any of these algorithms is the representation used for a periodic crystal. While similar objects like molecules and proteins have a finite number of atoms and their representation can be built based upon a finite point cloud interpretation, periodic crystals are unbounded in size, making their representation more challenging. In the present work, we adapt the Pointwise Distance Distribution (PDD), a continuous and generically complete isometry invariant for periodic point sets, as a representation for our learning algorithm. While the PDD is effective in distinguishing periodic point sets up to isometry, there is no consideration for the composition of the underlying material. We develop a transformer model with a modified self-attention mechanism that can utilize the PDD and inc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#21644;&#23454;&#26045;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#26377;&#38480;&#24773;&#20917;&#19979;&#30340;&#33258;&#21160;&#26426;&#22120;&#29366;&#24577;&#30417;&#27979;&#21644;&#32500;&#25252;&#31995;&#32479;&#65292;&#36890;&#36807;&#24320;&#21457;&#25104;&#26412;&#25928;&#30410;&#30340;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#21644;&#29305;&#24449;&#24037;&#31243;&#21644;&#25968;&#25454;&#38477;&#32500;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21457;&#23637;&#20013;&#22269;&#23478;&#32570;&#20047;&#39044;&#27979;&#24615;&#32500;&#25252;&#21644;&#32844;&#19994;&#20581;&#24247;&#23433;&#20840;&#25991;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.15088</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#35774;&#35745;&#21644;&#23454;&#26045;&#33258;&#21160;&#26426;&#22120;&#29366;&#24577;&#30417;&#27979;&#21644;&#32500;&#25252;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Design &amp; Implementation of Automatic Machine Condition Monitoring and Maintenance System in Limited Resource Situations. (arXiv:2401.15088v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#21644;&#23454;&#26045;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#26377;&#38480;&#24773;&#20917;&#19979;&#30340;&#33258;&#21160;&#26426;&#22120;&#29366;&#24577;&#30417;&#27979;&#21644;&#32500;&#25252;&#31995;&#32479;&#65292;&#36890;&#36807;&#24320;&#21457;&#25104;&#26412;&#25928;&#30410;&#30340;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#21644;&#29305;&#24449;&#24037;&#31243;&#21644;&#25968;&#25454;&#38477;&#32500;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21457;&#23637;&#20013;&#22269;&#23478;&#32570;&#20047;&#39044;&#27979;&#24615;&#32500;&#25252;&#21644;&#32844;&#19994;&#20581;&#24247;&#23433;&#20840;&#25991;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;&#26102;&#20195;&#65292;&#33258;&#21160;&#21270;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26426;&#26800;&#35774;&#22791;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#26679;&#21487;&#20197;&#24320;&#21457;&#19968;&#20010;&#35686;&#21578;&#31995;&#32479;&#65292;&#22312;&#21457;&#29983;&#28798;&#38590;&#24615;&#25439;&#22351;&#20043;&#21069;&#37319;&#21462;&#36866;&#24403;&#30340;&#34892;&#21160;&#12290;&#20840;&#29699;&#20351;&#29992;&#20102;&#19968;&#20123;&#26426;&#22120;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#65292;&#20294;&#23427;&#20204;&#26114;&#36149;&#19988;&#38656;&#35201;&#32463;&#36807;&#22521;&#35757;&#30340;&#20154;&#21592;&#25805;&#20316;&#21644;&#20998;&#26512;&#12290;&#30001;&#20110;&#21457;&#23637;&#20013;&#22269;&#23478;&#22522;&#30784;&#35774;&#26045;&#19981;&#36275;&#12289;&#32570;&#20047;&#29087;&#32451;&#20154;&#21592;&#12289;&#36130;&#21153;&#21361;&#26426;&#31561;&#21407;&#22240;&#65292;&#39044;&#27979;&#24615;&#32500;&#25252;&#21644;&#32844;&#19994;&#20581;&#24247;&#23433;&#20840;&#25991;&#21270;&#19981;&#21487;&#24471;&#12290;&#20174;&#24320;&#21457;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#24320;&#22987;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#25506;&#35752;&#20102;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#36164;&#28304;&#23545;&#33258;&#21160;&#21270;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32467;&#21512;&#23567;&#27874;&#12289;&#24494;&#31215;&#20998;&#21644;&#20449;&#21495;&#22788;&#29702;&#30340;&#27010;&#24565;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#29305;&#24449;&#24037;&#31243;&#21644;&#25968;&#25454;&#38477;&#32500;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#33258;&#21160;&#21270;&#25972;&#20010;&#36807;&#31243;&#65292;&#23436;&#25104;&#20102;&#25152;&#26377;&#24517;&#35201;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of the fourth industrial revolution, it is essential to automate fault detection and diagnosis of machineries so that a warning system can be developed that will help to take an appropriate action before any catastrophic damage. Some machines health monitoring systems are used globally but they are expensive and need trained personnel to operate and analyse. Predictive maintenance and occupational health and safety culture are not available due to inadequate infrastructure, lack of skilled manpower, financial crisis, and others in developing countries. Starting from developing a cost-effective DAS for collecting fault data in this study, the effect of limited data and resources has been investigated while automating the process. To solve this problem, A feature engineering and data reduction method has been developed combining the concepts from wavelets, differential calculus, and signal processing. Finally, for automating the whole process, all the necessary theoretical and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#27700;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22312;&#20445;&#25345;&#31616;&#27905;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#27969;&#37327;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14521</link><description>&lt;p&gt;
&#20197;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;-&#27010;&#24565;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron. (arXiv:2401.14521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#27700;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22312;&#20445;&#25345;&#31616;&#27905;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#27969;&#37327;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#31616;&#27905;&#21487;&#35299;&#37322;&#30340;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#37319;&#29992;&#22522;&#20110;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#30340;&#26377;&#21521;&#22270;&#32467;&#26500;&#20316;&#20026;&#22522;&#26412;&#35745;&#31639;&#21333;&#20803;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#21333;&#20010;&#20301;&#32622;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#65288;&#28145;&#24230;&#65289;&#65292;&#32780;&#19981;&#26159;&#23545;&#22823;&#26679;&#26412;&#38598;&#27700;&#21306;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#24191;&#24230;&#12290;&#30446;&#26631;&#26159;&#21457;&#29616;&#19968;&#20010;&#26368;&#23567;&#30340;&#34920;&#31034;&#65288;&#21333;&#20803;&#29366;&#24577;&#25968;&#21644;&#27969;&#37327;&#36335;&#24452;&#25968;&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#33021;&#22815;&#35299;&#37322;&#32473;&#23450;&#38598;&#27700;&#21306;&#36755;&#20837;&#29366;&#24577;&#21644;&#36755;&#20986;&#34892;&#20026;&#30340;&#20027;&#35201;&#36807;&#31243;&#65292;&#29305;&#21035;&#24378;&#35843;&#27169;&#25311;&#20840;&#33539;&#22260;&#65288;&#39640;&#12289;&#20013;&#12289;&#20302;&#65289;&#30340;&#27969;&#37327;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#21306;&#22495;&#65292;&#37319;&#29992;&#31867;&#20284;HyMod&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;3&#20010;&#21333;&#20803;&#29366;&#24577;&#21644;2&#20010;&#20027;&#35201;&#27969;&#21160;&#36335;&#24452;&#65292;&#33021;&#22815;&#23454;&#29616;&#36825;&#26679;&#30340;&#34920;&#31034;&#65292;&#20294;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#27700;&#25991;&#22270;&#30340;&#26102;&#38388;&#21644;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the applicability of machine learning technologies to the development of parsimonious, interpretable, catchment-scale hydrologic models using directed-graph architectures based on the mass-conserving perceptron (MCP) as the fundamental computational unit. Here, we focus on architectural complexity (depth) at a single location, rather than universal applicability (breadth) across large samples of catchments. The goal is to discover a minimal representation (numbers of cell-states and flow paths) that represents the dominant processes that can explain the input-state-output behaviors of a given catchment, with particular emphasis given to simulating the full range (high, medium, and low) of flow dynamics. We find that a HyMod-like architecture with three cell-states and two major flow pathways achieves such a representation at our study location, but that the additional incorporation of an input-bypass mechanism significantly improves the timing and shape of the hydrograph
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;MCTS&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;SR-GPT&#65292;&#22312;&#21457;&#29616;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#20844;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.14424</link><description>&lt;p&gt;
&#36890;&#36807;GPT&#24341;&#23548;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#25968;&#23398;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search. (arXiv:2401.14424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14424
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;MCTS&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;SR-GPT&#65292;&#22312;&#21457;&#29616;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#20844;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#30740;&#31350;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#25214;&#21040;&#19968;&#20010;&#31616;&#27905;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#20844;&#24335;&#26469;&#20934;&#30830;&#25551;&#36848;&#25968;&#25454;&#20013;&#27599;&#20010;&#21464;&#37327;&#19982;&#39044;&#27979;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20063;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#65292;&#26159;&#19968;&#20010;NP&#22256;&#38590;&#38382;&#39064;&#12290;&#21435;&#24180;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;sota&#12290;&#34429;&#28982;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#22312;&#24674;&#22797;&#30446;&#26631;&#34920;&#36798;&#24335;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#25913;&#36827;&#65292;&#20294;&#26159;&#22312;MCTS&#36807;&#31243;&#20013;&#32570;&#20047;&#24341;&#23548;&#20005;&#37325;&#38459;&#30861;&#20102;&#20854;&#25628;&#32034;&#25928;&#29575;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#31639;&#27861;&#22312;MCTS&#30340;&#25628;&#32034;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#32593;&#32476;&#65292;&#20294;&#26159;&#36825;&#20010;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#24456;&#24046;&#12290;&#20026;&#20102;&#24179;&#34913;&#25928;&#29575;&#21644;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SR-GPT&#65292;&#32467;&#21512;&#20102;AlphaZero&#30340;&#24605;&#24819;&#12290;SR-GPT&#26159;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;&#65292;&#23558;MCTS&#19982;&#19968;&#20010;&#36890;&#29992;&#24615;&#36739;&#22909;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding a concise and interpretable mathematical formula that accurately describes the relationship between each variable and the predicted value in the data is a crucial task in scientific research, as well as a significant challenge in artificial intelligence. This problem is referred to as symbolic regression, which is an NP-hard problem. Last year, a symbolic regression method based on Monte Carlo Tree Search (MCTS) was proposed and sota was obtained on multiple datasets. While this algorithm has shown considerable improvement in recovering target expressions compared to previous methods, the lack of guidance during the MCTS process severely hampers its search efficiency. Recently, some algorithms have added a pre-trained policy network to guide the search of MCTS, but the pre-trained policy network generalizes poorly. To balance efficiency and generality, we propose SR-GPT combining ideas from AlphaZero. SR-GPT is a new symbolic regression algorithm that combines MCTS with a Gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#29615;&#22659;&#20013;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#30340;&#20840;&#26632;&#26041;&#27861;&#65292;&#26426;&#22120;&#20154;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26694;&#26550;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#23454;&#36341;&#23398;&#20064;&#36866;&#24212;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26032;&#23545;&#35937;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#20302;&#25104;&#26412;&#30340;&#31227;&#21160;&#25805;&#20316;&#30828;&#20214;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2401.14403</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#31227;&#21160;&#25805;&#20316;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#30340;&#21487;&#20851;&#33410;&#29289;&#20307;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adaptive Mobile Manipulation for Articulated Objects In the Open World. (arXiv:2401.14403v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#29615;&#22659;&#20013;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#30340;&#20840;&#26632;&#26041;&#27861;&#65292;&#26426;&#22120;&#20154;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26694;&#26550;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#23454;&#36341;&#23398;&#20064;&#36866;&#24212;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26032;&#23545;&#35937;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#20302;&#25104;&#26412;&#30340;&#31227;&#21160;&#25805;&#20316;&#30828;&#20214;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#30340;&#26080;&#32467;&#26500;&#29615;&#22659;&#20013;&#37096;&#32626;&#26426;&#22120;&#20154;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#20154;&#36890;&#24120;&#21482;&#22312;&#23553;&#38381;&#30340;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#36827;&#34892;&#30740;&#31350;&#65292;&#20043;&#21069;&#30340;&#31227;&#21160;&#25805;&#20316;&#24037;&#20316;&#20063;&#20165;&#38480;&#20110;&#25342;&#21462;&#12289;&#31227;&#21160;&#12289;&#25918;&#32622;&#65292;&#36825;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#21482;&#26159;&#20912;&#23665;&#19968;&#35282;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24320;&#25918;&#19990;&#30028;&#31227;&#21160;&#25805;&#20316;&#31995;&#32479;&#65292;&#37319;&#29992;&#20840;&#26632;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#20851;&#33410;&#29289;&#20307;&#30340;&#25805;&#20316;&#65292;&#20363;&#22914;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#38376;&#12289;&#26588;&#23376;&#12289;&#25277;&#23625;&#21644;&#20912;&#31665;&#12290;&#26426;&#22120;&#20154;&#21033;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#20808;&#20174;&#19968;&#23567;&#32452;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#28982;&#21518;&#36890;&#36807;&#22312;&#32447;&#23454;&#36341;&#23398;&#20064;&#26469;&#22788;&#29702;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26032;&#23545;&#35937;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#20302;&#25104;&#26412;&#30340;&#31227;&#21160;&#25805;&#20316;&#30828;&#20214;&#24179;&#21488;&#65292;&#33021;&#22815;&#22312;&#26080;&#32467;&#26500;&#29615;&#22659;&#20013;&#36827;&#34892;&#23433;&#20840;&#21644;&#33258;&#20027;&#30340;&#22312;&#32447;&#36866;&#24212;&#65292;&#25104;&#26412;&#32422;&#20026;20,000&#32654;&#20803;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;20&#20010;&#21487;&#20851;&#33410;&#30340;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem. However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area. In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments. The robot utilizes an adaptive learning framework to initially learns from a small set of data through behavior cloning, followed by learning from online practice on novel objects that fall outside the training distribution. We also develop a low-cost mobile manipulation hardware platform capable of safe and autonomous online adaptation in unstructured environments with a cost of around 20,000 USD. In our experiments we utilize 20 articulate obje
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#32467;&#26500;&#20808;&#39564;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#37096;&#20998;&#24050;&#30693;&#39640;&#26031;&#22270;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#22270;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#29983;&#25104;&#26679;&#26412;&#26102;&#21033;&#29992;&#36864;&#28779;&#26391;&#26684;&#32500;&#33021;&#25193;&#25955;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#21518;&#39564;&#20998;&#24067;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.14340</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#32467;&#26500;&#20808;&#39564;&#30340;&#37096;&#20998;&#24050;&#30693;&#39640;&#26031;&#22270;&#27169;&#22411;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimation of partially known Gaussian graphical models with score-based structural priors. (arXiv:2401.14340v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#32467;&#26500;&#20808;&#39564;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#37096;&#20998;&#24050;&#30693;&#39640;&#26031;&#22270;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#22270;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#29983;&#25104;&#26679;&#26412;&#26102;&#21033;&#29992;&#36864;&#28779;&#26391;&#26684;&#32500;&#33021;&#25193;&#25955;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#21518;&#39564;&#20998;&#24067;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25903;&#25345;&#20272;&#35745;&#37096;&#20998;&#24050;&#30693;&#30340;&#39640;&#26031;&#22270;&#27169;&#22411;&#65292;&#24182;&#19988;&#32467;&#21512;&#20102;&#20851;&#20110;&#24213;&#23618;&#22270;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#28857;&#20272;&#35745;&#26041;&#27861;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#25110;&#26368;&#22823;&#21518;&#39564;&#20934;&#21017;&#65292;&#24182;&#20351;&#29992;&#65288;&#31616;&#21333;&#30340;&#65289;&#31934;&#24230;&#30697;&#38453;&#20808;&#39564;&#26469;&#25552;&#20379;&#28857;&#20272;&#35745;&#12290;&#25105;&#20204;&#32771;&#34385;&#23545;&#22270;&#36827;&#34892;&#20808;&#39564;&#65292;&#24182;&#20381;&#36182;&#36864;&#28779;&#26391;&#26684;&#32500;&#33021;&#25193;&#25955;&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#12290;&#30001;&#20110;&#26391;&#26684;&#32500;&#33021;&#37319;&#26679;&#22120;&#38656;&#35201;&#35775;&#38382;&#24213;&#23618;&#22270;&#20808;&#39564;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#22240;&#27492;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#26377;&#25928;&#22320;&#20174;&#22270;&#25968;&#25454;&#38598;&#65288;&#20107;&#20808;&#21487;&#29992;&#25110;&#20174;&#24050;&#30693;&#20998;&#24067;&#29983;&#25104;&#65289;&#20272;&#35745;&#24471;&#20998;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel algorithm for the support estimation of partially known Gaussian graphical models that incorporates prior information about the underlying graph. In contrast to classical approaches that provide a point estimate based on a maximum likelihood or a maximum a posteriori criterion using (simple) priors on the precision matrix, we consider a prior on the graph and rely on annealed Langevin diffusion to generate samples from the posterior distribution. Since the Langevin sampler requires access to the score function of the underlying graph prior, we use graph neural networks to effectively estimate the score from a graph dataset (either available beforehand or generated from a known distribution). Numerical experiments demonstrate the benefits of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCompress&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#26435;&#37325;&#32858;&#31867;&#21644;&#26381;&#21153;&#22120;&#31471;&#30693;&#35782;&#33976;&#39311;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#22312;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#39640;&#24230;&#21487;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.14211</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#26435;&#37325;&#32858;&#31867;&#21644;&#26381;&#21153;&#22120;&#31471;&#33976;&#39311;&#23454;&#29616;&#39640;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Learning through Adaptive Weight Clustering and Server-Side Distillation. (arXiv:2401.14211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCompress&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#26435;&#37325;&#32858;&#31867;&#21644;&#26381;&#21153;&#22120;&#31471;&#30693;&#35782;&#33976;&#39311;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#22312;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#39640;&#24230;&#21487;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#26395;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#22810;&#20010;&#35774;&#22791;&#20849;&#21516;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#37325;&#22797;&#30340;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#36890;&#20449;&#23548;&#33268;&#20102;&#36807;&#22810;&#30340;&#36890;&#20449;&#25104;&#26412;&#65292;&#36825;&#32473;&#32852;&#37030;&#23398;&#20064;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#20363;&#22914;&#31232;&#30095;&#21270;&#21644;&#26435;&#37325;&#32858;&#31867;&#65292;&#28982;&#32780;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#38656;&#35201;&#20462;&#25913;&#24213;&#23618;&#30340;&#27169;&#22411;&#32858;&#21512;&#26041;&#26696;&#25110;&#32773;&#28041;&#21450;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#21518;&#32773;&#19981;&#20165;&#35843;&#25972;&#20102;&#27169;&#22411;&#30340;&#21387;&#32553;&#29575;&#65292;&#36824;&#38480;&#21046;&#20102;&#27169;&#22411;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;FedCompress&#65292;&#23427;&#32467;&#21512;&#20102;&#21160;&#24577;&#26435;&#37325;&#32858;&#31867;&#21644;&#26381;&#21153;&#22120;&#31471;&#30693;&#35782;&#33976;&#39311;&#65292;&#20197;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21516;&#26102;&#23398;&#20064;&#39640;&#24230;&#21487;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a promising technique for the collaborative training of deep neural networks across multiple devices while preserving data privacy. Despite its potential benefits, FL is hindered by excessive communication costs due to repeated server-client communication during training. To address this challenge, model compression techniques, such as sparsification and weight clustering are applied, which often require modifying the underlying model aggregation schemes or involve cumbersome hyperparameter tuning, with the latter not only adjusts the model's compression rate but also limits model's potential for continuous improvement over growing data. In this paper, we propose FedCompress, a novel approach that combines dynamic weight clustering and server-side knowledge distillation to reduce communication costs while learning highly generalizable models. Through a comprehensive evaluation on diverse public datasets, we demonstrate the efficacy of our approach compared to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#23454;&#29616;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#33258;&#21160;&#21270;&#28040;&#36153;&#32773;&#30340;&#31454;&#26631;&#21644;&#31649;&#29702;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#20877;&#29983;&#33021;&#28304;&#38646;&#36793;&#38469;&#25104;&#26412;&#21644;&#29289;&#29702;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13947</link><description>&lt;p&gt;
&#32593;&#32476;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy Trading. (arXiv:2401.13947v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#23454;&#29616;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#33258;&#21160;&#21270;&#28040;&#36153;&#32773;&#30340;&#31454;&#26631;&#21644;&#31649;&#29702;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#20877;&#29983;&#33021;&#28304;&#38646;&#36793;&#38469;&#25104;&#26412;&#21644;&#29289;&#29702;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20998;&#24067;&#24335;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#33021;&#37327;&#20648;&#23384;&#36164;&#28304;&#36827;&#34892;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#34987;&#38271;&#26399;&#35748;&#20026;&#26159;&#25552;&#39640;&#33021;&#28304;&#31995;&#32479;&#24377;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#28040;&#36153;&#32773;&#21644;&#33258;&#32473;&#33258;&#36275;&#32773;&#65288;&#20855;&#26377;&#33021;&#28304;&#21457;&#30005;&#36164;&#28304;&#30340;&#20154;&#65289;&#32570;&#20047;&#36827;&#34892;&#37325;&#22797;&#28857;&#23545;&#28857;&#20132;&#26131;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38646;&#36793;&#38469;&#25104;&#26412;&#22312;&#30830;&#23450;&#20844;&#24179;&#24066;&#22330;&#20215;&#26684;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#33258;&#21160;&#21270;&#28040;&#36153;&#32773;&#23545;&#22826;&#38451;&#33021;&#20809;&#20239;&#21644;&#33021;&#37327;&#20648;&#23384;&#36164;&#28304;&#30340;&#31454;&#26631;&#21644;&#31649;&#29702;&#65292;&#22312;&#19968;&#31181;&#21033;&#29992;&#20379;&#38656;&#27604;&#30340;&#28857;&#23545;&#28857;&#28165;&#31639;&#26426;&#21046;&#19979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MARL&#26694;&#26550;&#22914;&#20309;&#25972;&#21512;&#29289;&#29702;&#32593;&#32476;&#32422;&#26463;&#20197;&#23454;&#29616;&#30005;&#21387;&#25511;&#21046;&#65292;&#20174;&#32780;&#30830;&#20445;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#30340;&#29289;&#29702;&#21487;&#34892;&#24615;&#65292;&#24182;&#20026;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#26045;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilizing distributed renewable and energy storage resources in local distribution networks via peer-to-peer (P2P) energy trading has long been touted as a solution to improve energy systems' resilience and sustainability. Consumers and prosumers (those who have energy generation resources), however, do not have the expertise to engage in repeated P2P trading, and the zero-marginal costs of renewables present challenges in determining fair market prices. To address these issues, we propose multi-agent reinforcement learning (MARL) frameworks to help automate consumers' bidding and management of their solar PV and energy storage resources, under a specific P2P clearing mechanism that utilizes the so-called supply-demand ratio. In addition, we show how the MARL frameworks can integrate physical network constraints to realize voltage control, hence ensuring physical feasibility of the P2P energy trading and paving way for real-world implementations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;NVIDIA&#24320;&#21457;&#30340;TTS&#27169;&#22411;&#65292;&#21033;&#29992;RAD-MMM&#21644;P-Flow&#23454;&#29616;&#20102;&#22810;&#35821;&#35328;TTS&#30340;&#35757;&#32451;&#65292;&#20854;&#20013;P-Flow&#22312;&#38646;&#26679;&#26412;TTS&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;2024&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2401.13851</link><description>&lt;p&gt;
&#21033;&#29992;&#22768;&#38899;&#20811;&#38534;&#23558;NVIDIA&#30340;&#22810;&#35821;&#35328;TTS&#31995;&#32479;&#25193;&#23637;&#21040;&#21360;&#24230;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Scaling NVIDIA's multi-speaker multi-lingual TTS systems with voice cloning to Indic Languages. (arXiv:2401.13851v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NVIDIA&#24320;&#21457;&#30340;TTS&#27169;&#22411;&#65292;&#21033;&#29992;RAD-MMM&#21644;P-Flow&#23454;&#29616;&#20102;&#22810;&#35821;&#35328;TTS&#30340;&#35757;&#32451;&#65292;&#20854;&#20013;P-Flow&#22312;&#38646;&#26679;&#26412;TTS&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;2024&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;NVIDIA&#20026;MMITS-VC&#65288;&#22810;&#35821;&#35328;&#12289;&#22810;&#35821;&#31181;&#21360;&#24230;TTS&#19982;&#22768;&#38899;&#20811;&#38534;&#65289;2024&#25361;&#25112;&#24320;&#21457;&#30340;TTS&#27169;&#22411;&#12290;&#22312;1&#21644;2&#36712;&#36947;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;RAD-MMM&#22312;&#30446;&#26631;&#35828;&#35805;&#20154;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;TTS&#30340;&#35757;&#32451;&#12290;&#22312;&#31532;3&#36712;&#36947;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;P-Flow&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;TTS&#30340;&#35757;&#32451;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#25361;&#25112;&#25968;&#25454;&#38598;&#21644;&#22806;&#37096;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#25152;&#26377;&#25552;&#20132;&#20351;&#29992;HiFi-GAN vocoders&#12290;RAD-MMM&#22312;1&#21644;2&#36712;&#36947;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#32780;P-Flow&#22312;&#31532;3&#36712;&#36947;&#19978;&#25490;&#21517;&#31532;&#19968;&#65292;&#24179;&#22343;&#24847;&#35265;&#20998;(MOS)&#20026;4.4&#65292;&#35828;&#35805;&#20154;&#30456;&#20284;&#24230;&#20998;&#25968;(SMOS)&#20026;3.62&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe the TTS models developed by NVIDIA for the MMITS-VC (Multi-speaker, Multi-lingual Indic TTS with Voice Cloning) 2024 Challenge. In Tracks 1 and 2, we utilize RAD-MMM to perform few-shot TTS by training additionally on 5 minutes of target speaker data. In Track 3, we utilize P-Flow to perform zero-shot TTS by training on the challenge dataset as well as external datasets. We use HiFi-GAN vocoders for all submissions. RAD-MMM performs competitively on Tracks 1 and 2, while P-Flow ranks first on Track 3, with mean opinion score (MOS) 4.4 and speaker similarity score (SMOS) of 3.62.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.13802</link><description>&lt;p&gt;
&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#38754;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13802
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#20195;&#30721;&#29983;&#25104;&#12290;LLMs&#20027;&#35201;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;/&#23569;&#26679;&#26412;&#33539;&#24335;&#20013;&#34987;&#29992;&#20110;&#25351;&#23548;&#27169;&#22411;&#23436;&#25104;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#65288;CCD&#65289;&#36825;&#19968;&#38750;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. %\textbf{Goal:} GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are `generative' tasks. However, there is limited research on the usage of LLMs for `non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect \textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We \textcolor{blac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21028;&#26029;&#20004;&#20010;&#26631;&#20934;&#27491;&#24577;&#38543;&#26426;&#21521;&#37327;&#26159;&#21542;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#20284;&#28982;&#27604;&#30340;&#20108;&#38454;&#30697;&#65292;&#24182;&#21457;&#29616;&#20102;&#19982;&#25972;&#25968;&#20998;&#21106;&#20989;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.13429</link><description>&lt;p&gt;
&#30456;&#20851;&#38543;&#26426;&#21521;&#37327;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detection of Correlated Random Vectors. (arXiv:2401.13429v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21028;&#26029;&#20004;&#20010;&#26631;&#20934;&#27491;&#24577;&#38543;&#26426;&#21521;&#37327;&#26159;&#21542;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#20284;&#28982;&#27604;&#30340;&#20108;&#38454;&#30697;&#65292;&#24182;&#21457;&#29616;&#20102;&#19982;&#25972;&#25968;&#20998;&#21106;&#20989;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21028;&#26029;&#20004;&#20010;&#26631;&#20934;&#27491;&#24577;&#38543;&#26426;&#21521;&#37327;$\mathsf{X}\in\mathbb{R}^{n}$&#21644;$\mathsf{Y}\in\mathbb{R}^{n}$&#26159;&#21542;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#36825;&#34987;&#34920;&#36848;&#20026;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#22312;&#38646;&#20551;&#35774;&#19979;&#65292;&#36825;&#20123;&#21521;&#37327;&#26159;&#32479;&#35745;&#29420;&#31435;&#30340;&#65292;&#32780;&#22312;&#22791;&#25321;&#20551;&#35774;&#19979;&#65292;$\mathsf{X}$&#21644;&#38543;&#26426;&#22343;&#21248;&#32622;&#25442;&#30340;$\mathsf{Y}$&#26159;&#20855;&#26377;&#30456;&#20851;&#31995;&#25968;$\rho$&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20449;&#24687;&#35770;&#19978;&#19981;&#21487;&#33021;&#21644;&#21487;&#33021;&#30340;&#26368;&#20248;&#27979;&#35797;&#38408;&#20540;&#65292;&#20316;&#20026;$n$&#21644;$\rho$&#30340;&#20989;&#25968;&#12290;&#20026;&#20102;&#24471;&#20986;&#25105;&#20204;&#30340;&#20449;&#24687;&#35770;&#19979;&#30028;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#27491;&#20132;&#22810;&#39033;&#24335;&#23637;&#24320;&#26469;&#35780;&#20272;&#20284;&#28982;&#27604;&#30340;&#20108;&#38454;&#30697;&#30340;&#26032;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#25581;&#31034;&#20102;&#19982;&#25972;&#25968;&#20998;&#21106;&#20989;&#25968;&#20043;&#38388;&#30340;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19978;&#36848;&#35774;&#32622;&#30340;&#22810;&#32500;&#27867;&#21270;&#65292;&#20854;&#20013;&#25105;&#20204;&#35266;&#23519;&#21040;&#20004;&#20010;&#25968;&#25454;&#24211;/&#30697;&#38453;&#65292;&#32780;&#19981;&#26159;&#20004;&#20010;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the problem of deciding whether two standard normal random vectors $\mathsf{X}\in\mathbb{R}^{n}$ and $\mathsf{Y}\in\mathbb{R}^{n}$ are correlated or not. This is formulated as a hypothesis testing problem, where under the null hypothesis, these vectors are statistically independent, while under the alternative, $\mathsf{X}$ and a randomly and uniformly permuted version of $\mathsf{Y}$, are correlated with correlation $\rho$. We analyze the thresholds at which optimal testing is information-theoretically impossible and possible, as a function of $n$ and $\rho$. To derive our information-theoretic lower bounds, we develop a novel technique for evaluating the second moment of the likelihood ratio using an orthogonal polynomials expansion, which among other things, reveals a surprising connection to integer partition functions. We also study a multi-dimensional generalization of the above setting, where rather than two vectors we observe two databases/matrices
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#25511;&#21046;&#36719;&#24418;&#21464;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#31995;&#32479;DittoGym&#65292;&#35813;&#31995;&#32479;&#38656;&#35201;&#23545;&#26426;&#22120;&#20154;&#30340;&#24418;&#24577;&#36827;&#34892;&#32454;&#31890;&#24230;&#21464;&#21270;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.13231</link><description>&lt;p&gt;
DittoGym:&#23398;&#20064;&#25511;&#21046;&#36719;&#24418;&#21464;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DittoGym: Learning to Control Soft Shape-Shifting Robots. (arXiv:2401.13231v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13231
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#25511;&#21046;&#36719;&#24418;&#21464;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#31995;&#32479;DittoGym&#65292;&#35813;&#31995;&#32479;&#38656;&#35201;&#23545;&#26426;&#22120;&#20154;&#30340;&#24418;&#24577;&#36827;&#34892;&#32454;&#31890;&#24230;&#21464;&#21270;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#20849;&#21516;&#35774;&#35745;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#30340;&#24418;&#24577;&#20248;&#21270;&#19982;&#23398;&#20064;&#30340;&#31574;&#30053;&#20849;&#21516;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#65292;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#23545;&#20110;&#36719;&#26426;&#22120;&#20154;&#26469;&#35828;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#29305;&#21035;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#36719;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#26032;&#39062;&#30340;&#21046;&#36896;&#25216;&#26415;&#23454;&#29616;&#23398;&#20064;&#21040;&#30340;&#24418;&#24577;&#21644;&#25191;&#34892;&#22120;&#12290;&#21463;&#33258;&#28982;&#30028;&#21644;&#26368;&#36817;&#30340;&#26032;&#22411;&#26426;&#22120;&#20154;&#35774;&#35745;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#26356;&#36827;&#19968;&#27493;&#25506;&#32034;&#26032;&#22411;&#21487;&#37325;&#26500;&#26426;&#22120;&#20154;&#65292;&#21363;&#22312;&#20854;&#23551;&#21629;&#20869;&#21487;&#20197;&#25913;&#21464;&#24418;&#24577;&#30340;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#23558;&#21487;&#37325;&#26500;&#36719;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#24418;&#24335;&#21270;&#20026;&#39640;&#32500;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#21516;&#19968;action&#31354;&#38388;&#20013;&#32479;&#19968;&#24418;&#24577;&#21464;&#21270;&#12289;&#36816;&#21160;&#21644;&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#65292;&#24182;&#24341;&#20837;&#21512;&#36866;&#30340;&#31895;&#21040;&#32454;&#30340;&#35838;&#31243;&#34920;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21457;&#29616;&#23454;&#29616;&#23545;&#26368;&#32456;&#26426;&#22120;&#20154;&#36827;&#34892;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;DittoGym&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#21487;&#37325;&#26500;&#36719;&#26426;&#22120;&#20154;&#30340;&#20840;&#38754;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#38656;&#35201;&#23545;&#24418;&#24577;&#36827;&#34892;&#32454;&#31890;&#24230;&#21464;&#21270;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot co-design, where the morphology of a robot is optimized jointly with a learned policy to solve a specific task, is an emerging area of research. It holds particular promise for soft robots, which are amenable to novel manufacturing techniques that can realize learned morphologies and actuators. Inspired by nature and recent novel robot designs, we propose to go a step further and explore the novel reconfigurable robots, defined as robots that can change their morphology within their lifetime. We formalize control of reconfigurable soft robots as a high-dimensional reinforcement learning (RL) problem. We unify morphology change, locomotion, and environment interaction in the same action space, and introduce an appropriate, coarse-to-fine curriculum that enables us to discover policies that accomplish fine-grained control of the resulting robots. We also introduce DittoGym, a comprehensive RL benchmark for reconfigurable soft robots that require fine-grained morphology changes to a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#31561;&#22240;&#32032;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#27969;&#37327;&#65292;&#24182;&#29992;&#20110;&#25351;&#23548;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20013;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.13098</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#21147;&#20449;&#24687;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#39044;&#27979;&#38750;&#26412;&#22320;&#29289;&#31181;&#33337;&#33334;&#20132;&#36890;&#27969;&#37327;&#21644;&#20837;&#20405;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Gravity-Informed Deep Learning Framework for Predicting Ship Traffic Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge. (arXiv:2401.13098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13098
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#31561;&#22240;&#32032;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#27969;&#37327;&#65292;&#24182;&#29992;&#20110;&#25351;&#23548;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20013;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#20307;&#20013;&#30340;&#20837;&#20405;&#29289;&#31181;&#23545;&#20840;&#29699;&#29615;&#22659;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#30001;&#20110;&#20132;&#36890;&#21644;&#36152;&#26131;&#22686;&#21152;&#65292;&#38750;&#26412;&#22303;&#29289;&#31181;&#24050;&#32463;&#24341;&#20837;&#20102;&#26032;&#30340;&#29615;&#22659;&#65292;&#23548;&#33268;&#29983;&#24577;&#31995;&#32479;&#30772;&#22351;&#65292;&#24182;&#23548;&#33268;&#20892;&#19994;&#12289;&#26519;&#19994;&#21644;&#28180;&#19994;&#26041;&#38754;&#30340;&#32463;&#27982;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#25216;&#26415;&#20197;&#20943;&#36731;&#36825;&#20123;&#20837;&#20405;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#20132;&#36890;&#27969;&#37327;&#65292;&#24182;&#20197;&#27492;&#25351;&#23548;&#36890;&#36807;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20256;&#25773;&#30340;&#20837;&#20405;&#29289;&#31181;&#39118;&#38505;&#35780;&#20272;&#12290;&#21463;&#22269;&#38469;&#36152;&#26131;&#37325;&#21147;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#24433;&#21709;&#33337;&#33334;&#27963;&#21160;&#21487;&#33021;&#24615;&#21644;&#24433;&#21709;&#30340;&#21508;&#31181;&#22240;&#32032;&#65292;&#22914;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#20043;&#38388;&#30340;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20998;&#26512;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#32593;&#32476;&#65292;&#25105;&#20204;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#20837;&#20405;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invasive species in water bodies pose a major threat to the environment and biodiversity globally. Due to increased transportation and trade, non-native species have been introduced to new environments, causing damage to ecosystems and leading to economic losses in agriculture, forestry, and fisheries. Therefore, there is a pressing need for risk assessment and management techniques to mitigate the impact of these invasions. This study aims to develop a new physics-inspired model to forecast maritime shipping traffic and thus inform risk assessment of invasive species spread through global transportation networks. Inspired by the gravity model for international trades, our model considers various factors that influence the likelihood and impact of vessel activities, such as shipping flux density, distance between ports, trade flow, and centrality measures of transportation hubs. Additionally, by analyzing the risk network of invasive species, we provide a comprehensive framework for as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#31232;&#30095;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25311;&#21512;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.13034</link><description>&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#30340;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Locality Sensitive Sparse Encoding for Learning World Models Online. (arXiv:2401.13034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#31232;&#30095;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25311;&#21512;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#25968;&#25454;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#65292;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#39640;&#25928;&#30340;&#31232;&#30095;&#26356;&#26032;&#65292;&#22312;&#24179;&#34913;&#27169;&#22411;&#23481;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#20248;&#21270;&#25311;&#21512;&#25152;&#26377;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring an accurate world model online for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26576;&#20123;&#38590;&#20197;&#27169;&#25311;&#24213;&#23618;&#29366;&#24577;&#21464;&#37327;&#30340;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20351;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12923</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#20915;&#19968;&#20123;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep multitask neural networks for solving some stochastic optimal control problems. (arXiv:2401.12923v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26576;&#20123;&#38590;&#20197;&#27169;&#25311;&#24213;&#23618;&#29366;&#24577;&#21464;&#37327;&#30340;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20351;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#29992;&#20110;&#20351;&#29992;&#30456;&#20851;&#30340;&#21453;&#21521;&#21160;&#24577;&#35268;&#21010;&#21407;&#29702;&#35299;&#20915;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#27169;&#25311;&#24213;&#23618;&#29366;&#24577;&#21464;&#37327;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#38382;&#39064;&#20013;&#65292;&#36825;&#31181;&#27169;&#25311;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#23548;&#33268;&#29366;&#24577;&#21464;&#37327;&#31354;&#38388;&#30340;&#31163;&#25955;&#21270;&#21644;&#38656;&#35201;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#12290;&#24403;&#22788;&#29702;&#22823;&#30340;&#29366;&#24577;&#21464;&#37327;&#31354;&#38388;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#21464;&#24471;&#20302;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#36825;&#31181;&#31867;&#22411;&#30340;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35757;&#32451;&#25105;&#20204;&#30340;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#22312;&#20219;&#21153;&#20043;&#38388;&#21160;&#24577;&#24179;&#34913;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#34893;&#29983;&#21697;&#23450;&#20215;&#38382;&#39064;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing neural network-based approaches for solving stochastic optimal control problems using the associated backward dynamic programming principle rely on the ability to simulate the underlying state variables. However, in some problems, this simulation is infeasible, leading to the discretization of state variable space and the need to train one neural network for each data point. This approach becomes computationally inefficient when dealing with large state variable spaces. In this paper, we consider a class of this type of stochastic optimal control problems and introduce an effective solution employing multitask neural networks. To train our multitask neural network, we introduce a novel scheme that dynamically balances the learning across tasks. Through numerical experiments on real-world derivatives pricing problems, we prove that our method outperforms state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#21644;&#27604;&#20363;&#31867;&#24179;&#34913;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#38024;&#23545;&#23567;&#30446;&#26631;&#30340;&#29289;&#20307;&#26816;&#27979;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#24037;&#19994;&#22330;&#26223;&#20013;&#25910;&#38598;&#21644;&#27880;&#37322;&#23567;&#30446;&#26631;&#25968;&#25454;&#30340;&#38590;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#27604;&#20363;&#31867;&#24179;&#34913;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.12729</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#21644;&#27604;&#20363;&#31867;&#24179;&#34913;&#25216;&#26415;&#25552;&#39640;&#23567;&#30446;&#26631;&#30340;&#29289;&#20307;&#26816;&#27979;&#24615;&#33021;&#65306;&#22312;&#24037;&#19994;&#22330;&#26223;&#20013;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios. (arXiv:2401.12729v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12729
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#21644;&#27604;&#20363;&#31867;&#24179;&#34913;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#38024;&#23545;&#23567;&#30446;&#26631;&#30340;&#29289;&#20307;&#26816;&#27979;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#24037;&#19994;&#22330;&#26223;&#20013;&#25910;&#38598;&#21644;&#27880;&#37322;&#23567;&#30446;&#26631;&#25968;&#25454;&#30340;&#38590;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#27604;&#20363;&#31867;&#24179;&#34913;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#22312;&#25552;&#21462;&#23616;&#37096;&#31867;&#21035;&#20449;&#24687;&#26041;&#38754;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#65292;&#24182;&#22312;&#24037;&#19994;&#20013;&#26377;&#22810;&#31181;&#24212;&#29992;&#12290;&#23613;&#31649;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#22312;&#20013;&#31561;&#21644;&#22823;&#22411;&#30446;&#26631;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#22312;&#23567;&#30446;&#26631;&#19978;&#34920;&#29616;&#19981;&#36275;&#12290;&#22312;&#22823;&#22810;&#25968;&#24037;&#19994;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#25910;&#38598;&#21644;&#27880;&#37322;&#23567;&#30446;&#26631;&#25968;&#25454;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#19988;&#23481;&#26131;&#20986;&#29616;&#20154;&#20026;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#24448;&#24448;&#19981;&#24179;&#34913;&#65292;&#32463;&#24120;&#23548;&#33268;&#27169;&#22411;&#25910;&#25947;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27880;&#20837;&#39069;&#22806;&#30340;&#25968;&#25454;&#28857;&#26469;&#25913;&#21892;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#25910;&#38598;&#21644;&#27880;&#37322;&#23567;&#30446;&#26631;&#25968;&#25454;&#28857;&#30340;&#22256;&#38590;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#24179;&#34913;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#27604;&#20363;&#31867;&#24179;&#34913;&#25216;&#26415;&#30340;&#25928;&#26524;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#30340;&#26377;&#25928;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object Detection (OD) has proven to be a significant computer vision method in extracting localized class information and has multiple applications in the industry. Although many of the state-of-the-art (SOTA) OD models perform well on medium and large sized objects, they seem to under perform on small objects. In most of the industrial use cases, it is difficult to collect and annotate data for small objects, as it is time-consuming and prone to human errors. Additionally, those datasets are likely to be unbalanced and often result in an inefficient model convergence. To tackle this challenge, this study presents a novel approach that injects additional data points to improve the performance of the OD models. Using synthetic data generation, the difficulties in data collection and annotations for small object data points can be minimized and to create a dataset with balanced distribution. This paper discusses the effects of a simple proportional class-balancing technique, to enable be
&lt;/p&gt;</description></item><item><title>NeuroSynt&#26159;&#19968;&#20010;&#29992;&#20110;&#21453;&#24212;&#21512;&#25104;&#30340;&#31070;&#32463;&#31526;&#21495;&#32452;&#21512;&#27714;&#35299;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#31070;&#32463;&#21644;&#31526;&#21495;&#26041;&#27861;&#26080;&#32541;&#38598;&#25104;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;NeuroSynt&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35268;&#33539;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#24403;&#21069;SYNTCOMP&#22522;&#20934;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.12131</link><description>&lt;p&gt;
NeuroSynt&#65306;&#19968;&#31181;&#29992;&#20110;&#21453;&#24212;&#21512;&#25104;&#30340;&#31070;&#32463;&#31526;&#21495;&#32452;&#21512;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
NeuroSynt: A Neuro-symbolic Portfolio Solver for Reactive Synthesis. (arXiv:2401.12131v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12131
&lt;/p&gt;
&lt;p&gt;
NeuroSynt&#26159;&#19968;&#20010;&#29992;&#20110;&#21453;&#24212;&#21512;&#25104;&#30340;&#31070;&#32463;&#31526;&#21495;&#32452;&#21512;&#27714;&#35299;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#31070;&#32463;&#21644;&#31526;&#21495;&#26041;&#27861;&#26080;&#32541;&#38598;&#25104;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;NeuroSynt&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35268;&#33539;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#24403;&#21069;SYNTCOMP&#22522;&#20934;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroSynt&#30340;&#31070;&#32463;&#31526;&#21495;&#32452;&#21512;&#27714;&#35299;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#21453;&#24212;&#21512;&#25104;&#12290;&#27714;&#35299;&#22120;&#30340;&#26680;&#24515;&#26159;&#31070;&#32463;&#21644;&#31526;&#21495;&#26041;&#27861;&#22312;&#35299;&#20915;&#21453;&#24212;&#21512;&#25104;&#38382;&#39064;&#26102;&#30340;&#26080;&#32541;&#38598;&#25104;&#12290;&#20026;&#20102;&#30830;&#20445;&#27491;&#30830;&#24615;&#65292;&#31070;&#32463;&#24341;&#25806;&#19982;&#39564;&#35777;&#24213;&#23618;&#31070;&#32463;&#27169;&#22411;&#39044;&#27979;&#30340;&#27169;&#22411;&#26816;&#26597;&#22120;&#30456;&#32467;&#21512;&#12290;NeuroSynt&#30340;&#24320;&#28304;&#23454;&#29616;&#25552;&#20379;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#26032;&#30340;&#31070;&#32463;&#21644;&#26368;&#26032;&#30340;&#31526;&#21495;&#26041;&#27861;&#36827;&#34892;&#21453;&#24212;&#21512;&#25104;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35268;&#33539;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#21319;&#20102;&#26368;&#26032;&#30340;&#21453;&#24212;&#21512;&#25104;&#27714;&#35299;&#22120;&#65292;&#24182;&#20026;&#24403;&#21069;SYNTCOMP&#22522;&#20934;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce NeuroSynt, a neuro-symbolic portfolio solver framework for reactive synthesis. At the core of the solver lies a seamless integration of neural and symbolic approaches to solving the reactive synthesis problem. To ensure soundness, the neural engine is coupled with model checkers verifying the predictions of the underlying neural models. The open-source implementation of NeuroSynt provides an integration framework for reactive synthesis in which new neural and state-of-the-art symbolic approaches can be seamlessly integrated. Extensive experiments demonstrate its efficacy in handling challenging specifications, enhancing the state-of-the-art reactive synthesis solvers, with NeuroSynt contributing novel solves in the current SYNTCOMP benchmarks.
&lt;/p&gt;</description></item><item><title>TurboSVM-FL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#32858;&#21512;&#31574;&#30053;&#65292;&#36890;&#36807;SVM&#32858;&#21512;&#20026;&#25042;&#24816;&#23458;&#25143;&#31471;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#12290;&#36825;&#31181;&#31574;&#30053;&#22312;&#19981;&#22686;&#21152;&#23458;&#25143;&#31471;&#35745;&#31639;&#36127;&#25285;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12012</link><description>&lt;p&gt;
TurboSVM-FL: &#36890;&#36807;SVM&#32858;&#21512;&#20026;&#25042;&#24816;&#23458;&#25143;&#31471;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients. (arXiv:2401.12012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12012
&lt;/p&gt;
&lt;p&gt;
TurboSVM-FL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#32858;&#21512;&#31574;&#30053;&#65292;&#36890;&#36807;SVM&#32858;&#21512;&#20026;&#25042;&#24816;&#23458;&#25143;&#31471;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#12290;&#36825;&#31181;&#31574;&#30053;&#22312;&#19981;&#22686;&#21152;&#23458;&#25143;&#31471;&#35745;&#31639;&#36127;&#25285;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#22312;&#36817;&#24180;&#26469;&#33719;&#24471;&#20102;&#24378;&#28872;&#30340;&#25512;&#21160;&#21147;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#20013;&#22830;&#26381;&#21153;&#22120;&#23450;&#26399;&#36890;&#36807;&#23458;&#25143;&#31471;&#21327;&#35843;&#27169;&#22411;&#65292;&#24182;&#32858;&#21512;&#30001;&#23458;&#25143;&#31471;&#22312;&#26412;&#22320;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#26412;&#22320;&#25968;&#25454;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#32852;&#37030;&#23398;&#20064;&#30340;&#23454;&#26045;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#23548;&#33268;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#12290;&#25910;&#25947;&#36895;&#24230;&#24930;&#22312;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#23588;&#20026;&#38382;&#39064;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#21487;&#33021;&#21463;&#21040;&#35745;&#31639;&#33021;&#21147;&#21644;&#23384;&#20648;&#31354;&#38388;&#30340;&#20005;&#37325;&#38480;&#21046;&#65292;&#22240;&#27492;&#23545;&#23458;&#25143;&#31471;&#20135;&#29983;&#39069;&#22806;&#35745;&#31639;&#25110;&#20869;&#23384;&#36127;&#25285;&#30340;&#26041;&#27861;&#65292;&#22914;&#36741;&#21161;&#30446;&#26631;&#39033;&#21644;&#26356;&#22823;&#30340;&#35757;&#32451;&#36845;&#20195;&#27425;&#25968;&#65292;&#21487;&#33021;&#19981;&#23454;&#38469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#32858;&#21512;&#31574;&#30053;TurboSVM-FL&#65292;&#23427;&#19981;&#20250;&#32473;&#23458;&#25143;&#31471;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#36127;&#25285;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years. In federated learning, a central server periodically coordinates models with clients and aggregates the models trained locally by clients without necessitating access to local data. Despite its potential, the implementation of federated learning continues to encounter several challenges, predominantly the slow convergence that is largely due to data heterogeneity. The slow convergence becomes particularly problematic in cross-device federated learning scenarios where clients may be strongly limited by computing power and storage space, and hence counteracting methods that induce additional computation or memory cost on the client side such as auxiliary objective terms and larger training iterations can be impractical. In this paper, we propose a novel federated aggregation strategy, TurboSVM-FL, that poses no additional computation burden on the client side and c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TTG-NN&#65289;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25345;&#20037;&#21516;&#35843;&#12289;&#22270;&#21367;&#31215;&#21644;&#24352;&#37327;&#36816;&#31639;&#65292;&#21516;&#26102;&#25429;&#25417;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#65288;TT&#65289;&#21644;Tensor&#35270;&#22270;&#22270;&#65288;TG&#65289;&#32467;&#26500;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.12007</link><description>&lt;p&gt;
Tensor&#35270;&#22270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Tensor-view Topological Graph Neural Network. (arXiv:2401.12007v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12007
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TTG-NN&#65289;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25345;&#20037;&#21516;&#35843;&#12289;&#22270;&#21367;&#31215;&#21644;&#24352;&#37327;&#36816;&#31639;&#65292;&#21516;&#26102;&#25429;&#25417;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#65288;TT&#65289;&#21644;Tensor&#35270;&#22270;&#22270;&#65288;TG&#65289;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#31867;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#23398;&#20064;&#20219;&#21153;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36817;&#24180;&#26469;&#22312;&#22270;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#35768;&#22810;&#37325;&#35201;&#30340;&#22270;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;GNNs&#22312;&#24615;&#33021;&#19978;&#22788;&#20110;&#26368;&#21069;&#27839;&#65292;&#20294;&#23427;&#20204;&#21482;&#20351;&#29992;&#20102;&#27599;&#20010;&#33410;&#28857;&#21608;&#22260;&#38750;&#24120;&#26377;&#38480;&#30340;&#37051;&#22495;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#20002;&#22833;&#21644;&#36807;&#22810;&#35745;&#31639;&#30340;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TTG-NN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25345;&#20037;&#21516;&#35843;&#12289;&#22270;&#21367;&#31215;&#21644;&#24352;&#37327;&#36816;&#31639;&#30340;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#21516;&#26102;&#25429;&#25417;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#65288;TT&#65289;&#21644;Tensor&#35270;&#22270;&#22270;&#65288;TG&#65289;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#22312;&#35745;&#31639;&#19978;&#20805;&#20998;&#21033;&#29992;&#20102;&#22270;&#30340;&#25299;&#25169;&#21644;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph classification is an important learning task for graph-structured data. Graph neural networks (GNNs) have recently gained growing attention in graph learning and have shown significant improvements in many important graph problems. Despite their state-of-the-art performances, existing GNNs only use local information from a very limited neighborhood around each node, suffering from loss of multi-modal information and overheads of excessive computation. To address these issues, we propose a novel Tensor-view Topological Graph Neural Network (TTG-NN), a class of simple yet effective topological deep learning built upon persistent homology, graph convolution, and tensor operations. This new method incorporates tensor learning to simultaneously capture Tensor-view Topological (TT), as well as Tensor-view Graph (TG) structural information on both local and global levels. Computationally, to fully exploit graph topology and structure, we propose two flexible TT and TG representation lea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#24212;&#29992;&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#24605;&#24819;&#33021;&#22815;&#23454;&#29616;&#22312;&#20943;&#23569;&#21442;&#25968;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#25191;&#34892;&#25928;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#23398;&#29983;&#32593;&#32476;&#23398;&#20064;&#21040;&#22797;&#26434;&#30340;&#20132;&#36890;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.11798</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#20132;&#36890;&#39044;&#27979;&#19978;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction. (arXiv:2401.11798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#24212;&#29992;&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#24605;&#24819;&#33021;&#22815;&#23454;&#29616;&#22312;&#20943;&#23569;&#21442;&#25968;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#25191;&#34892;&#25928;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#23398;&#29983;&#32593;&#32476;&#23398;&#20064;&#21040;&#22797;&#26434;&#30340;&#20132;&#36890;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#23454;&#26102;&#20132;&#36890;&#39044;&#27979;&#23545;&#20943;&#23569;&#20132;&#36890;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#39044;&#27979;&#20132;&#36890;&#29366;&#20917;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31354;&#38388;-&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ST-GNN&#65289;&#23558;&#23454;&#26102;&#20132;&#36890;&#25968;&#25454;&#24314;&#27169;&#20026;&#26102;&#38388;&#22270;&#12290;&#23613;&#31649;ST-GNN&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#20026;&#23454;&#38469;&#20132;&#36890;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#23454;&#26102;&#39044;&#27979;&#26102;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#37492;&#20110;&#23454;&#26102;&#25968;&#25454;&#21160;&#24577;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;ST-GNN&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25104;&#26412;&#20989;&#25968;&#65292;&#26088;&#22312;&#20351;&#29992;&#22797;&#26434;&#32593;&#32476;&#65288;&#25945;&#24072;&#65289;&#30340;&#33976;&#39311;&#25968;&#25454;&#26469;&#35757;&#32451;&#20855;&#26377;&#36739;&#23569;&#21442;&#25968;&#30340;&#32593;&#32476;&#65288;&#23398;&#29983;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20934;&#30830;&#24615;&#25509;&#36817;&#25945;&#24072;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65292;&#23558;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#34701;&#20837;&#23398;&#29983;&#32593;&#32476;&#65292;&#20351;&#23398;&#29983;&#33021;&#22815;&#23398;&#20064;&#21040;&#25945;&#24072;&#24863;&#30693;&#30340;&#22797;&#26434;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#38754;&#20020;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient real-time traffic prediction is crucial for reducing transportation time. To predict traffic conditions, we employ a spatio-temporal graph neural network (ST-GNN) to model our real-time traffic data as temporal graphs. Despite its capabilities, it often encounters challenges in delivering efficient real-time predictions for real-world traffic data. Recognizing the significance of timely prediction due to the dynamic nature of real-time data, we employ knowledge distillation (KD) as a solution to enhance the execution time of ST-GNNs for traffic prediction. In this paper, We introduce a cost function designed to train a network with fewer parameters (the student) using distilled data from a complex network (the teacher) while maintaining its accuracy close to that of the teacher. We use knowledge distillation, incorporating spatial-temporal correlations from the teacher network to enable the student to learn the complex patterns perceived by the teacher. However, a challenge a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20302;&#23433;&#20840;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#20197;&#20934;&#30830;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.11792</link><description>&lt;p&gt;
&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20302;&#23433;&#20840;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#20197;&#20934;&#30830;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26234;&#33021;&#39550;&#39542;&#31995;&#32479;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#24403;&#21069;&#29615;&#22659;&#21644;&#36710;&#36742;&#29366;&#24577;&#21160;&#24577;&#21046;&#23450;&#36866;&#24403;&#30340;&#39550;&#39542;&#31574;&#30053;&#65292;&#21516;&#26102;&#30830;&#20445;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#23433;&#20840;&#24615;&#20302;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#65292;&#32780;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#26159;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#30340;&#21069;&#25552;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#32780;&#22810;&#26679;&#22330;&#26223;&#19979;&#30340;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#12290;&#25105;&#20204;&#30340;SGADS&#19982;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#32467;&#21512;&#65292;&#20351;&#26234;&#33021;&#36710;&#36742;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#31034;&#33539;&#30456;&#32467;&#21512;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
An intelligent driving system should be capable of dynamically formulating appropriate driving strategies based on the current environment and vehicle status, while ensuring the security and reliability of the system. However, existing methods based on reinforcement learning and imitation learning suffer from low safety, poor generalization, and inefficient sampling. Additionally, they cannot accurately predict future driving trajectories, and the accurate prediction of future driving trajectories is a precondition for making optimal decisions. To solve these problems, in this paper, we introduce a Safe and Generalized end-to-end Autonomous Driving System (SGADS) for complex and various scenarios. Our SGADS incorporates variational inference with normalizing flows, enabling the intelligent vehicle to accurately predict future driving trajectories. Moreover, we propose the formulation of robust safety constraints. Furthermore, we combine reinforcement learning with demonstrations to aug
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11648</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#20998;&#23618;&#27491;&#21017;&#21270;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11648
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#30340;&#35786;&#26029;&#26159;&#19968;&#39033;&#24517;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#21046;&#23450;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#30340;&#20027;&#21160;&#26410;&#26469;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#35768;&#22810;&#30740;&#31350;&#24182;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;EHR&#25968;&#25454;&#22266;&#26377;&#30340;&#24322;&#26500;&#21644;&#20998;&#23618;&#29305;&#24449;&#65292;&#24517;&#28982;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NECHO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20998;&#23618;&#27491;&#21017;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23450;&#21046;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#19968;&#23545;&#21452;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#34701;&#21512;&#28085;&#30422;&#21307;&#23398;&#20195;&#30721;&#12289;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#20020;&#24202;&#31508;&#35760;&#30340;&#22810;&#26041;&#38754;&#20449;&#24687;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#22260;&#32469;&#30528;&#21307;&#23398;&#20195;&#30721;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#21307;&#23398;&#26412;&#20307;&#20013;&#30340;&#29238;&#32423;&#20449;&#24687;&#26469;&#35268;&#33539;&#29305;&#23450;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#23398;&#20064;EHR&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#23545;MIMIC-III&#25968;&#25454;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>SPAND&#26159;&#19968;&#20010;&#21033;&#29992;&#32593;&#32476;&#21160;&#24577;&#30340;&#30561;&#30496;&#39044;&#27979;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#22270;&#32593;&#32476;&#21644;&#31227;&#21160;&#35774;&#22791;&#25968;&#25454;&#26469;&#39044;&#27979;&#19979;&#19968;&#22825;&#30340;&#30561;&#30496;&#25345;&#32493;&#26102;&#38388;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2401.11113</link><description>&lt;p&gt;
SPAND: &#20351;&#29992;&#32593;&#32476;&#21160;&#24577;&#30340;&#30561;&#30496;&#39044;&#27979;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SPAND: Sleep Prediction Architecture using Network Dynamics. (arXiv:2401.11113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11113
&lt;/p&gt;
&lt;p&gt;
SPAND&#26159;&#19968;&#20010;&#21033;&#29992;&#32593;&#32476;&#21160;&#24577;&#30340;&#30561;&#30496;&#39044;&#27979;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#22270;&#32593;&#32476;&#21644;&#31227;&#21160;&#35774;&#22791;&#25968;&#25454;&#26469;&#39044;&#27979;&#19979;&#19968;&#22825;&#30340;&#30561;&#30496;&#25345;&#32493;&#26102;&#38388;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#34892;&#20026;&#23545;&#20581;&#24247;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#23545;&#36523;&#24515;&#20581;&#24247;&#30340;&#25351;&#31034;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#21033;&#29992;&#26222;&#36941;&#23384;&#22312;&#30340;&#20256;&#24863;&#22120;&#30417;&#27979;&#21644;&#39044;&#27979;&#30561;&#30496;&#34892;&#20026;&#65292;&#21487;&#20197;&#24110;&#21161;&#31649;&#29702;&#30561;&#30496;&#24182;&#36861;&#36394;&#30456;&#20851;&#20581;&#24247;&#29366;&#20917;&#12290;&#34429;&#28982;&#30561;&#30496;&#34892;&#20026;&#21462;&#20915;&#20110;&#20010;&#20307;&#30340;&#29983;&#29702;&#29366;&#20917;&#65292;&#20294;&#20063;&#21463;&#21040;&#25968;&#23383;&#23186;&#20307;&#20351;&#29992;&#12289;&#31038;&#20132;&#32593;&#32476;&#20256;&#26579;&#20197;&#21450;&#21608;&#22260;&#22825;&#27668;&#31561;&#22806;&#37096;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SPAND&#65288;Sleep Prediction Architecture using Network Dynamics&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22270;&#32593;&#32476;&#20013;&#30340;&#31038;&#20132;&#20256;&#26579;&#26469;&#39044;&#27979;&#30561;&#30496;&#34892;&#20026;&#30340;&#31995;&#32479;&#65292;&#24182;&#23558;&#20854;&#19982;&#20174;&#26222;&#36941;&#23384;&#22312;&#30340;&#31227;&#21160;&#35774;&#22791;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#20013;&#25552;&#21462;&#30340;&#29983;&#29702;&#21644;&#25163;&#26426;&#25968;&#25454;&#38598;&#25104;&#65292;&#20197;&#39044;&#27979;&#19979;&#19968;&#22825;&#30340;&#30561;&#30496;&#25345;&#32493;&#26102;&#38388;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#27880;&#24847;&#26426;&#21046;&#65292;&#20811;&#26381;&#20102;&#21253;&#21547;&#19982;&#30561;&#30496;&#34892;&#20026;&#26080;&#20851;&#30340;&#36830;&#25509;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#30340;&#23616;&#38480;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#31361;&#26174;&#20986;&#35813;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep behavior significantly impacts health and acts as an indicator of physical and mental well-being. Monitoring and predicting sleep behavior with ubiquitous sensors may therefore assist in both sleep management and tracking of related health conditions. While sleep behavior depends on, and is reflected in the physiology of a person, it is also impacted by external factors such as digital media usage, social network contagion, and the surrounding weather. In this work, we propose SPAND (Sleep Prediction Architecture using Network Dynamics), a system that exploits social contagion in sleep behavior through graph networks and integrates it with physiological and phone data extracted from ubiquitous mobile and wearable devices for predicting next-day sleep labels about sleep duration. Our architecture overcomes the limitations of large-scale graphs containing connections irrelevant to sleep behavior by devising an attention mechanism. The extensive experimental evaluation highlights th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;TAMS&#21644;Next-Generation Reservoir Computing&#25216;&#26415;&#65292;&#21033;&#29992;&#31232;&#20107;&#20214;&#31639;&#27861;&#20272;&#35745;&#26469;&#28304;&#20110;&#25968;&#25454;&#30340;&#30830;&#23450;&#20989;&#25968;&#65292;&#26469;&#35745;&#31639;&#22823;&#35199;&#27915;&#32463;&#24230;&#32763;&#36716;&#29615;&#27969;&#65288;AMOC&#65289;&#22312;&#25351;&#23450;&#26102;&#38388;&#31383;&#21475;&#20869;&#23849;&#28291;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.10800</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31232;&#20107;&#20214;&#31639;&#27861;&#20272;&#35745;AMOC&#36716;&#25442;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Estimation of AMOC transition probabilities using a machine learning based rare-event algorithm. (arXiv:2401.10800v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;TAMS&#21644;Next-Generation Reservoir Computing&#25216;&#26415;&#65292;&#21033;&#29992;&#31232;&#20107;&#20214;&#31639;&#27861;&#20272;&#35745;&#26469;&#28304;&#20110;&#25968;&#25454;&#30340;&#30830;&#23450;&#20989;&#25968;&#65292;&#26469;&#35745;&#31639;&#22823;&#35199;&#27915;&#32463;&#24230;&#32763;&#36716;&#29615;&#27969;&#65288;AMOC&#65289;&#22312;&#25351;&#23450;&#26102;&#38388;&#31383;&#21475;&#20869;&#23849;&#28291;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35199;&#27915;&#32463;&#24230;&#32763;&#36716;&#29615;&#27969;&#65288;AMOC&#65289;&#26159;&#20840;&#29699;&#27668;&#20505;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#20020;&#30028;&#22240;&#32032;&#65292;&#21487;&#20197;&#22312;&#20840;&#29699;&#21464;&#26262;&#19979;&#23849;&#28291;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20351;&#29992;&#19968;&#31181;&#31232;&#20107;&#20214;&#31639;&#27861;&#65288;Trajectory-Adaptive Multilevel Splitting&#65292;TAMS&#65289;&#35745;&#31639;AMOC&#22312;&#25351;&#23450;&#26102;&#38388;&#31383;&#21475;&#20869;&#23849;&#28291;&#30340;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;TAMS&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;&#24471;&#20998;&#20989;&#25968;&#30340;&#36873;&#25321;&#12290;&#34429;&#28982;&#24050;&#30693;&#26368;&#20339;&#24471;&#20998;&#20989;&#25968;&#30340;&#23450;&#20041;&#65292;&#31216;&#20026;&#8220;&#30830;&#23450;&#20989;&#25968;&#8221;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#20808;&#39564;&#22320;&#35745;&#31639;&#23427;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;TAMS&#19982;&#19979;&#19968;&#20195;&#27700;&#24211;&#35745;&#31639;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#20174;&#31232;&#20107;&#20214;&#31639;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#20272;&#35745;&#30830;&#23450;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;AMOC&#30340;&#38543;&#26426;&#30418;&#27169;&#22411;&#20013;&#27979;&#35797;&#20102;&#36825;&#31181;&#25216;&#26415;&#65292;&#35813;&#27169;&#22411;&#23384;&#22312;&#20004;&#31181;&#36716;&#21464;&#31867;&#22411;&#65292;&#31216;&#20026;F&#65288;&#24555;&#36895;&#65289;&#36716;&#21464;&#21644;S&#65288;&#32531;&#24930;&#65289;&#36716;&#21464;&#12290;F&#36716;&#21464;&#30340;&#32467;&#26524;&#19982;&#37027;&#20123;&#36827;&#34892;&#20102;&#26377;&#21033;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Atlantic Meridional Overturning Circulation (AMOC) is an important component of the global climate, known to be a tipping element, as it could collapse under global warming. The main objective of this study is to compute the probability that the AMOC collapses within a specified time window, using a rare-event algorithm called Trajectory-Adaptive Multilevel Splitting (TAMS). However, the efficiency and accuracy of TAMS depend on the choice of the score function. Although the definition of the optimal score function, called ``committor function" is known, it is impossible in general to compute it a priori. Here, we combine TAMS with a Next-Generation Reservoir Computing technique that estimates the committor function from the data generated by the rare-event algorithm. We test this technique in a stochastic box model of the AMOC for which two types of transition exist, the so-called F(ast)-transitions and S(low)-transitions. Results for the F-transtions compare favorably with those 
&lt;/p&gt;</description></item><item><title>BioDiffusion&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#21512;&#25104;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#38750;&#31283;&#24577;&#30340;&#22810;&#21464;&#37327;&#20449;&#21495;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#21512;&#25104;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22797;&#26434;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10282</link><description>&lt;p&gt;
BioDiffusion&#65306;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#21512;&#25104;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis. (arXiv:2401.10282v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10282
&lt;/p&gt;
&lt;p&gt;
BioDiffusion&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#21512;&#25104;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#38750;&#31283;&#24577;&#30340;&#22810;&#21464;&#37327;&#20449;&#21495;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#21512;&#25104;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22797;&#26434;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#38754;&#20020;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#12289;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12289;&#26631;&#31614;&#22797;&#26434;&#24615;&#21644;&#27979;&#37327;&#22122;&#22768;&#30340;&#24178;&#25200;&#31561;&#38382;&#39064;&#12290;&#36825;&#20123;&#25361;&#25112;&#32463;&#24120;&#38459;&#30861;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#20339;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BioDiffusion&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#21512;&#25104;&#22810;&#21464;&#37327;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#36827;&#34892;&#20248;&#21270;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;BioDiffusion&#22312;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#38750;&#31283;&#24577;&#30340;&#22810;&#21464;&#37327;&#20449;&#21495;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#29992;&#20110;&#26080;&#26465;&#20214;&#12289;&#26631;&#31614;&#26465;&#20214;&#21644;&#20449;&#21495;&#26465;&#20214;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#21033;&#29992;&#36825;&#20123;&#21512;&#25104;&#30340;&#20449;&#21495;&#20026;&#19978;&#36848;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;&#23545;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#36827;&#34892;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#65292;&#24378;&#35843;&#20854;&#22312;&#19982;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19982;&#24403;&#21069;&#20027;&#27969;&#30340;&#26102;&#38388;&#31995;&#20449;&#24687;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;BioDiffusion&#22312;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning tasks involving biomedical signals frequently grapple with issues such as limited data availability, imbalanced datasets, labeling complexities, and the interference of measurement noise. These challenges often hinder the optimal training of machine learning algorithms. Addressing these concerns, we introduce BioDiffusion, a diffusion-based probabilistic model optimized for the synthesis of multivariate biomedical signals. BioDiffusion demonstrates excellence in producing high-fidelity, non-stationary, multivariate signals for a range of tasks including unconditional, label-conditional, and signal-conditional generation. Leveraging these synthesized signals offers a notable solution to the aforementioned challenges. Our research encompasses both qualitative and quantitative assessments of the synthesized data quality, underscoring its capacity to bolster accuracy in machine learning tasks tied to biomedical signals. Furthermore, when juxtaposed with current leading tim
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23616;&#37096;&#31232;&#30095;&#27169;&#22411;ContraLSP&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31435;&#26679;&#26412;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;ContraLSP&#22312;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.08552</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#24615;&#21644;&#23616;&#37096;&#31232;&#30095;&#25200;&#21160;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Explaining Time Series via Contrastive and Locally Sparse Perturbations. (arXiv:2401.08552v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08552
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23616;&#37096;&#31232;&#30095;&#27169;&#22411;ContraLSP&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31435;&#26679;&#26412;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;ContraLSP&#22312;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#26159;&#19968;&#20010;&#22797;&#21512;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#37325;&#35201;&#20301;&#32622;&#24182;&#21305;&#37197;&#22797;&#26434;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#34429;&#28982;&#20043;&#21069;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#23427;&#20204;&#30340;&#25200;&#21160;&#21487;&#33021;&#26080;&#27861;&#20943;&#36731;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#24322;&#36136;&#26679;&#26412;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ContraLSP&#65292;&#36825;&#26159;&#19968;&#20010;&#23616;&#37096;&#31232;&#30095;&#27169;&#22411;&#65292;&#24341;&#20837;&#23545;&#31435;&#26679;&#26412;&#26469;&#26500;&#24314;&#26080;&#20449;&#24687;&#30340;&#25200;&#21160;&#65292;&#20294;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#20445;&#25345;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#26679;&#26412;&#29305;&#23450;&#30340;&#31232;&#30095;&#38376;&#65292;&#29983;&#25104;&#26356;&#22810;&#20108;&#36827;&#21046;&#20559;&#26012;&#21644;&#24179;&#28369;&#30340;&#36974;&#32617;&#65292;&#21487;&#20197;&#36731;&#26494;&#32508;&#21512;&#26102;&#38388;&#36235;&#21183;&#24182;&#31616;&#27905;&#22320;&#36873;&#25321;&#26174;&#33879;&#29305;&#24449;&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;ContraLSP&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#22312;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;\url{https://github.com/zichuan-liu/ContraL}&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining multivariate time series is a compound challenge, as it requires identifying important locations in the time series and matching complex temporal patterns. Although previous saliency-based methods addressed the challenges, their perturbation may not alleviate the distribution shift issue, which is inevitable especially in heterogeneous samples. We present ContraLSP, a locally sparse model that introduces counterfactual samples to build uninformative perturbations but keeps distribution using contrastive learning. Furthermore, we incorporate sample-specific sparse gates to generate more binary-skewed and smooth masks, which easily integrate temporal trends and select the salient features parsimoniously. Empirical studies on both synthetic and real-world datasets show that ContraLSP outperforms state-of-the-art models, demonstrating a substantial improvement in explanation quality for time series data. The source code is available at \url{https://github.com/zichuan-liu/ContraL
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#25193;&#25955;&#30456;&#20851;&#20809;&#35889;&#23398;&#20013;&#30340;&#34880;&#27969;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#22122;&#22768;&#40065;&#26834;&#24615;&#20998;&#26512;&#23637;&#31034;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05580</link><description>&lt;p&gt;
&#21152;&#24378;&#25193;&#25955;&#30456;&#20851;&#20809;&#35889;&#23398;&#20013;&#30340;&#34880;&#27969;&#35780;&#20272;&#65306;&#19968;&#31181;&#24102;&#26377;&#22122;&#22768;&#40065;&#26834;&#24615;&#20998;&#26512;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A Transfer Learning Approach with Noise Robustness Analysis. (arXiv:2401.05580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#25193;&#25955;&#30456;&#20851;&#20809;&#35889;&#23398;&#20013;&#30340;&#34880;&#27969;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#22122;&#22768;&#40065;&#26834;&#24615;&#20998;&#26512;&#23637;&#31034;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#30456;&#20851;&#20809;&#35889;&#23398;&#65288;DCS&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#38750;&#20405;&#20837;&#24615;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#36817;&#32418;&#22806;&#30456;&#24178;&#28857;&#20809;&#28304;&#29031;&#23556;&#26469;&#26816;&#27979;&#20809;&#35889;&#21464;&#21270;&#26469;&#27979;&#37327;&#32452;&#32455;&#34880;&#27969;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#27979;&#37327;&#34880;&#27969;&#25351;&#25968;&#65288;BFi&#65289;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#19968;&#20010;&#26377;&#20851;&#35813;&#26041;&#27861;&#25104;&#21151;&#19982;&#21542;&#30340;&#38382;&#39064;&#26159;&#20854;&#22312;&#28041;&#21450;&#19981;&#21516;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#21644;&#21508;&#31181;&#19981;&#21516;&#20020;&#24202;&#24212;&#29992;&#21644;&#35774;&#32622;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20559;&#24046;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;SNR&#23545;&#23398;&#20064;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#36801;&#31227;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#19981;&#21516;&#28155;&#21152;&#22122;&#22768;&#27700;&#24179;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#27169;&#25311;&#19981;&#21516;&#30340;SNR&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#20197;1x64&#30340;&#33258;&#30456;&#20851;&#26354;&#32447;&#20026;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;BFi&#21644;&#30456;&#20851;&#21442;&#25968;beta&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffuse correlation spectroscopy (DCS) is an emerging noninvasive technique that measures the tissue blood flow, by using near-infrared coherent point-source illumination to detect spectral changes. While machine learning has demonstrated significant potential for measuring blood flow index (BFi), an open question concerning the success of this approach pertains to its robustness in scenarios involving deviations between datasets with varying Signal-to-Noise Ratios (SNRs) originating from diverse clinical applications and various setups. This study proposes a transfer learning approach, aims to assess the influence of SNRs on the generalization ability of learned features, and demonstrate the robustness for transfer learning. A synthetic dataset with varying levels of added noise is utilized to simulate different SNRs. The proposed network takes a 1x64 autocorrelation curve as input and generates BFi and the correlation parameter beta. The proposed model demonstrates excellent performa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26497;&#38480;&#23398;&#20064;&#26426;&#24555;&#36895;&#31934;&#30830;&#20998;&#26512;&#33041;&#34880;&#27969;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#29616;&#26377;&#31639;&#27861;&#30340;&#32508;&#21512;&#27604;&#36739;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#23427;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22122;&#22768;&#21644;&#20809;&#23398;&#21442;&#25968;&#19979;&#37117;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#19982;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#30701;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;&#36825;&#31181;&#31574;&#30053;&#36866;&#29992;&#20110;&#22312;&#32447;&#35757;&#32451;&#30340;&#36793;&#32536;&#35745;&#31639;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.05578</link><description>&lt;p&gt;
&#36890;&#36807;&#26497;&#38480;&#23398;&#20064;&#26426;&#24555;&#36895;&#20998;&#26512;&#33041;&#34880;&#27969;
&lt;/p&gt;
&lt;p&gt;
Fast Cerebral Blood Flow Analysis via Extreme Learning Machine. (arXiv:2401.05578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26497;&#38480;&#23398;&#20064;&#26426;&#24555;&#36895;&#31934;&#30830;&#20998;&#26512;&#33041;&#34880;&#27969;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#29616;&#26377;&#31639;&#27861;&#30340;&#32508;&#21512;&#27604;&#36739;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#23427;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22122;&#22768;&#21644;&#20809;&#23398;&#21442;&#25968;&#19979;&#37117;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#19982;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#30701;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;&#36825;&#31181;&#31574;&#30053;&#36866;&#29992;&#20110;&#22312;&#32447;&#35757;&#32451;&#30340;&#36793;&#32536;&#35745;&#31639;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#24555;&#36895;&#31934;&#30830;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#30456;&#20851;&#20809;&#35889;&#23398;&#65288;DCS&#65289;&#21644;&#26497;&#38480;&#23398;&#20064;&#26426;&#65288;ELM&#65289;&#26469;&#20998;&#26512;&#33041;&#34880;&#27969;&#65288;CBF&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ELM&#21644;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#32508;&#21512;&#25351;&#26631;&#23545;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#21322;&#26080;&#31351;&#21644;&#22810;&#23618;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22122;&#22768;&#27700;&#24179;&#21644;&#20809;&#23398;&#21442;&#25968;&#19979;&#65292;ELM&#22987;&#32456;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#36845;&#20195;&#25311;&#21512;&#31639;&#27861;&#12290;&#36890;&#36807;&#19982;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#65292;ELM&#33719;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;ELM&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27809;&#26377;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#65292;&#23548;&#33268;&#35757;&#32451;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26356;&#24555;&#12290;&#36825;&#31181;&#25552;&#20986;&#30340;&#31574;&#30053;&#22312;&#22312;&#32447;&#35757;&#32451;&#30340;&#36793;&#32536;&#35745;&#31639;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a rapid and precise analytical approach for analyzing cerebral blood flow (CBF) using Diffuse Correlation Spectroscopy (DCS) with the application of the Extreme Learning Machine (ELM). Our evaluation of ELM and existing algorithms involves a comprehensive set of metrics. We assess these algorithms using synthetic datasets for both semi-infinite and multi-layer models. The results demonstrate that ELM consistently achieves higher fidelity across various noise levels and optical parameters, showcasing robust generalization ability and outperforming iterative fitting algorithms. Through a comparison with a computationally efficient neural network, ELM attains comparable accuracy with reduced training and inference times. Notably, the absence of a back-propagation process in ELM during training results in significantly faster training speeds compared to existing neural network approaches. This proposed strategy holds promise for edge computing applications with online training
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#36951;&#20256;&#32534;&#31243;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#39044;&#27979;&#21322;&#33258;&#30952;&#30952;&#26426;&#30340;&#20135;&#37327;&#12290;&#26032;&#30340;&#36951;&#20256;&#32534;&#31243;&#21464;&#20307;&#21487;&#20197;&#25552;&#21462;&#22810;&#20010;&#26041;&#31243;&#24335;&#65292;&#20174;&#32780;&#31934;&#30830;&#39044;&#27979;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#32676;&#38598;&#30340;&#20135;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05382</link><description>&lt;p&gt;
&#29992;&#20110;&#39044;&#27979;&#21322;&#33258;&#30952;&#30952;&#26426;&#20135;&#37327;&#30340;&#25913;&#36827;&#36951;&#20256;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
An improved genetic programming for predicting semi autogenous grinding mill throughput. (arXiv:2401.05382v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05382
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#36951;&#20256;&#32534;&#31243;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#39044;&#27979;&#21322;&#33258;&#30952;&#30952;&#26426;&#30340;&#20135;&#37327;&#12290;&#26032;&#30340;&#36951;&#20256;&#32534;&#31243;&#21464;&#20307;&#21487;&#20197;&#25552;&#21462;&#22810;&#20010;&#26041;&#31243;&#24335;&#65292;&#20174;&#32780;&#31934;&#30830;&#39044;&#27979;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#32676;&#38598;&#30340;&#20135;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#33258;&#30952;&#30952;&#26426;&#22312;&#30719;&#29289;&#22788;&#29702;&#21378;&#30340;&#30952;&#30719;&#22238;&#36335;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20934;&#30830;&#39044;&#27979;&#21322;&#33258;&#30952;&#30952;&#26426;&#30340;&#20135;&#37327;&#26159;&#19968;&#20010;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#32463;&#39564;&#27169;&#22411;&#26469;&#39044;&#27979;&#21322;&#33258;&#30952;&#30952;&#26426;&#30340;&#20135;&#37327;&#65292;&#20294;&#26159;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#20570;&#27492;&#39044;&#27979;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#21457;&#25381;&#12290;&#19982;&#20381;&#36182;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#23454;&#39564;&#25968;&#25454;&#30340;&#32463;&#39564;&#24314;&#27169;&#19981;&#21516;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#21033;&#29992;&#22312;&#27491;&#24120;&#36816;&#34892;&#26399;&#38388;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#36951;&#20256;&#32534;&#31243;&#65288;Genetic Programming&#65292;GP&#65289;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20854;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;&#36879;&#26126;&#30340;&#26041;&#31243;&#24335;&#26469;&#31934;&#30830;&#39044;&#27979;&#30952;&#26426;&#30340;&#20135;&#37327;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;GP&#24212;&#29992;&#20110;&#39044;&#27979;&#21322;&#33258;&#30952;&#30952;&#26426;&#20135;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#20116;&#31181;&#26032;&#30340;GP&#21464;&#31181;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#36825;&#20123;&#21464;&#31181;&#25552;&#21462;&#20102;&#22810;&#20010;&#26041;&#31243;&#24335;&#65292;&#27599;&#20010;&#26041;&#31243;&#24335;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#32676;&#38598;&#30340;&#20135;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-autogenous grinding (SAG) mills play a pivotal role in the grinding circuit of mineral processing plants. Accurate prediction of SAG mill throughput as a crucial performance metric is of utmost importance. While empirical models have been developed in previous studies for SAG mill throughput prediction, the potential of applying machine learning (ML) techniques for this purpose remains underexplored. Unlike empirical modelling, which relies on expensive and time-consuming experimental data, ML techniques can utilize data collected during regular operations. Genetic programming (GP) is one of ML techniques that offers the advantage of providing a transparent equation for precise mill throughput prediction. This study explores the application of GP to predict SAG mill throughput and introduces five new GP variants to enhance prediction performance. These variants extract multiple equations, each accurately predicting mill throughput for specific clusters of training data. These equa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22495;&#27867;&#21270;&#27010;&#24565;&#65292;&#32467;&#21512;&#22810;&#32423;&#29305;&#24449;&#23545;&#40784;&#30340;&#24605;&#24819;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05363</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#32423;&#22495;&#23545;&#40784;&#23454;&#29616;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;
&lt;/p&gt;
&lt;p&gt;
Generalizable Sleep Staging via Multi-level Domain Alignment. (arXiv:2401.05363v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22495;&#27867;&#21270;&#27010;&#24565;&#65292;&#32467;&#21512;&#22810;&#32423;&#29305;&#24449;&#23545;&#40784;&#30340;&#24605;&#24819;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#23545;&#20110;&#30561;&#30496;&#35780;&#20272;&#21644;&#30142;&#30149;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#30456;&#21516;&#25968;&#25454;&#38598;&#30340;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22495;&#27867;&#21270;&#27010;&#24565;&#21040;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;&#20219;&#21153;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#21040;&#29616;&#26377;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37319;&#29992;&#29305;&#24449;&#23545;&#40784;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SleepDG&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;&#23616;&#37096;&#26174;&#33879;&#29305;&#24449;&#21644;&#26102;&#24207;&#29305;&#24449;&#23545;&#20110;&#30561;&#30496;&#20998;&#26399;&#37117;&#24456;&#37325;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#29305;&#24449;&#23545;&#40784;&#65292;&#23558;&#26102;&#20195;&#32423;&#21644;&#24207;&#21015;&#32423;&#29305;&#24449;&#23545;&#40784;&#26469;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26102;&#20195;&#32423;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#65292;&#23545;&#19981;&#21516;&#30561;&#30496;&#26102;&#20195;&#30340;&#29305;&#24449;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic sleep staging is essential for sleep assessment and disorder diagnosis. Most existing methods depend on one specific dataset and are limited to be generalized to other unseen datasets, for which the training data and testing data are from the same dataset. In this paper, we introduce domain generalization into automatic sleep staging and propose the task of generalizable sleep staging which aims to improve the model generalization ability to unseen datasets. Inspired by existing domain generalization methods, we adopt the feature alignment idea and propose a framework called SleepDG to solve it. Considering both of local salient features and sequential features are important for sleep staging, we propose a Multi-level Feature Alignment combining epoch-level and sequence-level feature alignment to learn domain-invariant feature representations. Specifically, we design an Epoch-level Feature Alignment to align the feature distribution of each single sleep epoch among different 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21453;&#20363;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#35780;&#20998;&#20989;&#25968;&#23398;&#20064;&#33391;&#22909;&#65292;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#20173;&#28982;&#26080;&#27861;&#29983;&#25104;&#25509;&#36817;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#21482;&#33021;&#20135;&#29983;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#39640;&#26031;&#27169;&#31946;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.04856</link><description>&lt;p&gt;
&#19968;&#20010;&#22909;&#30340;&#35780;&#20998;&#24182;&#19981;&#20250;&#23548;&#33268;&#19968;&#20010;&#22909;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Good Score Does not Lead to A Good Generative Model. (arXiv:2401.04856v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21453;&#20363;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#35780;&#20998;&#20989;&#25968;&#23398;&#20064;&#33391;&#22909;&#65292;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#20173;&#28982;&#26080;&#27861;&#29983;&#25104;&#25509;&#36817;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#21482;&#33021;&#20135;&#29983;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#39640;&#26031;&#27169;&#31946;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#26159;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#19968;&#31181;&#20027;&#35201;&#26041;&#27861;&#65292;&#20197;&#20854;&#33021;&#22815;&#20174;&#22797;&#26434;&#30340;&#39640;&#32500;&#25968;&#25454;&#20998;&#24067;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#32780;&#38395;&#21517;&#12290;&#35813;&#26041;&#27861;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#19988;&#26377;&#30528;&#20005;&#26684;&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#36136;&#30340;&#25903;&#25345;&#12290;&#29305;&#21035;&#26159;&#24050;&#32463;&#35777;&#26126;&#65292;&#22914;&#26524;&#23398;&#20064;&#21040;&#30340;&#24213;&#23618;&#35780;&#20998;&#20989;&#25968;&#33391;&#22909;&#65292;SGMs&#33021;&#22815;&#29983;&#25104;&#25509;&#36817;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#36825;&#34920;&#26126;&#20102;SGM&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#25104;&#21151;&#20043;&#22788;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21453;&#20363;&#12290;&#36890;&#36807;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#35780;&#20998;&#20989;&#25968;&#23398;&#20064;&#24471;&#24456;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;SGMs&#21482;&#33021;&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#39640;&#26031;&#27169;&#31946;&#26679;&#26412;&#65292;&#27169;&#25311;&#26680;&#23494;&#24230;&#20272;&#35745;&#30340;&#25928;&#26524;&#12290;&#36825;&#19968;&#21457;&#29616;&#19982;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#21457;&#29616;&#30456;&#19968;&#33268;&#65292;&#25581;&#31034;&#20102;SGMs&#21487;&#33021;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#35760;&#24518;&#25928;&#24212;&#24182;&#19988;&#26080;&#27861;&#29983;&#25104;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based Generative Models (SGMs) is one leading method in generative modeling, renowned for their ability to generate high-quality samples from complex, high-dimensional data distributions. The method enjoys empirical success and is supported by rigorous theoretical convergence properties. In particular, it has been shown that SGMs can generate samples from a distribution that is close to the ground-truth if the underlying score function is learned well, suggesting the success of SGM as a generative model. We provide a counter-example in this paper. Through the sample complexity argument, we provide one specific setting where the score function is learned well. Yet, SGMs in this setting can only output samples that are Gaussian blurring of training data points, mimicking the effects of kernel density estimation. The finding resonates a series of recent finding that reveal that SGMs can demonstrate strong memorization effect and fail to generate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#20248;&#21270;&#35757;&#32451;&#36807;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25506;&#32034;&#20840;&#23616;&#21644;&#23616;&#37096;&#32447;&#24615;&#26494;&#24347;&#26469;&#20135;&#29983;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.03451</link><description>&lt;p&gt;
&#22312;&#35757;&#32451;&#36807;&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#20248;&#21270;&#65306;&#36827;&#34892;&#19968;&#27425;&#36731;&#26494;&#30340;&#28459;&#27493;
&lt;/p&gt;
&lt;p&gt;
Optimization Over Trained Neural Networks: Taking a Relaxing Walk. (arXiv:2401.03451v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#20248;&#21270;&#35757;&#32451;&#36807;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25506;&#32034;&#20840;&#23616;&#21644;&#23616;&#37096;&#32447;&#24615;&#26494;&#24347;&#26469;&#20135;&#29983;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#35757;&#32451;&#20043;&#22806;&#65292;&#25968;&#23398;&#20248;&#21270;&#20063;&#34987;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#35299;&#20915;&#35757;&#32451;&#36807;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#22914;&#39564;&#35777;&#12289;&#21387;&#32553;&#21644;&#24102;&#23398;&#20064;&#32422;&#26463;&#30340;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#32593;&#32476;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#30001;&#20110;&#24369;&#32447;&#24615;&#26494;&#24347;&#21644;&#23494;&#38598;&#32422;&#26463;&#30697;&#38453;&#65292;&#27714;&#35299;&#36825;&#20123;&#38382;&#39064;&#24456;&#24555;&#21464;&#24471;&#22256;&#38590;&#12290;&#36817;&#24180;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#21106;&#24179;&#38754;&#31639;&#27861;&#12289;&#37325;&#36848;&#21644;&#22522;&#20110;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#20840;&#23616;&#21644;&#23616;&#37096;&#32447;&#24615;&#26494;&#24347;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26356;&#21487;&#25193;&#23637;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#22312;&#36755;&#20837;&#12289;&#28145;&#24230;&#21644;&#31070;&#32463;&#20803;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;MILP&#27714;&#35299;&#22120;&#21644;&#20808;&#21069;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30456;&#31454;&#20105;&#65292;&#21516;&#26102;&#20135;&#29983;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Besides training, mathematical optimization is also used in deep learning to model and solve formulations over trained neural networks for purposes such as verification, compression, and optimization with learned constraints. However, solving these formulations soon becomes difficult as the network size grows due to the weak linear relaxation and dense constraint matrix. We have seen improvements in recent years with cutting plane algorithms, reformulations, and an heuristic based on Mixed-Integer Linear Programming (MILP). In this work, we propose a more scalable heuristic based on exploring global and local linear relaxations of the neural network model. Our heuristic is competitive with a state-of-the-art MILP solver and the prior heuristic while producing better solutions with increases in input, depth, and number of neurons.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30697;&#38453;&#20056;&#31215;&#24577;(MPS)&#30340;&#28040;&#24687;&#20256;&#36882;&#31574;&#30053;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20960;&#20309;&#22270;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.01801</link><description>&lt;p&gt;
&#19968;&#20010;&#37327;&#23376;&#21551;&#21457;&#30340;&#29992;&#20110;&#20960;&#20309;&#24314;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A quatum inspired neural network for geometric modeling. (arXiv:2401.01801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01801
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30697;&#38453;&#20056;&#31215;&#24577;(MPS)&#30340;&#28040;&#24687;&#20256;&#36882;&#31574;&#30053;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20960;&#20309;&#22270;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#29289;&#29702;&#31995;&#32479;&#26500;&#24819;&#20026;3D&#22810;&#20307;&#28857;&#20113;&#65292;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#65292;&#22914;SE(3)/E(3)&#31561;&#25928;GNN&#65292;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#20204;&#39640;&#25928;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#20351;&#23427;&#20204;&#33021;&#22815;&#29087;&#32451;&#22320;&#23545;&#20998;&#23376;&#21644;&#26230;&#20307;&#26448;&#26009;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20960;&#20309;GNN&#21482;&#25552;&#20379;&#20102;&#22810;&#20307;&#31995;&#32479;&#30340;&#24179;&#22343;&#22330;&#36817;&#20284;&#65292;&#23553;&#35013;&#22312;&#20004;&#20307;&#28040;&#24687;&#20256;&#36882;&#20013;&#65292;&#22240;&#27492;&#22312;&#25429;&#25417;&#36825;&#20123;&#20960;&#20309;&#22270;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#26377;&#25152;&#27424;&#32570;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#35745;&#31639;&#29289;&#29702;&#23398;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#39640;&#38454;&#24352;&#37327;&#26469;&#22788;&#29702;&#22810;&#20307;&#31995;&#32479;&#30340;&#24352;&#37327;&#32593;&#32476;&#34987;&#24341;&#20837;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#24352;&#37327;&#21270;&#32593;&#32476;&#25972;&#21512;&#21040;GNN&#30340;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#20013;&#38754;&#20020;&#30528;&#21487;&#25193;&#23637;&#24615;&#21644;&#23545;&#31216;&#24615;&#20445;&#25345;&#65288;&#22914;&#32622;&#25442;&#21644;&#26059;&#36716;&#65289;&#30340;&#25361;&#25112;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31561;&#21464;&#30697;&#38453;&#20056;&#31215;&#24577;(MPS)&#30340;&#28040;&#24687;&#20256;&#36882;&#31574;&#30053;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
By conceiving physical systems as 3D many-body point clouds, geometric graph neural networks (GNNs), such as SE(3)/E(3) equivalent GNNs, have showcased promising performance. In particular, their effective message-passing mechanics make them adept at modeling molecules and crystalline materials. However, current geometric GNNs only offer a mean-field approximation of the many-body system, encapsulated within two-body message passing, thus falling short in capturing intricate relationships within these geometric graphs. To address this limitation, tensor networks, widely employed by computational physics to handle manybody systems using high-order tensors, have been introduced. Nevertheless, integrating these tensorized networks into the message-passing framework of GNNs faces scalability and symmetry conservation (e.g., permutation and rotation) challenges. In response, we introduce an innovative equivariant Matrix Product State (MPS)-based message-passing strategy, through achieving a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;$2\times 2$&#21452;&#26354;PDE&#30340;Backstepping&#31070;&#32463;&#25805;&#20316;&#21592;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#32806;&#21512;&#30340;Goursat&#24418;&#24335;PDE&#65292;&#24182;&#24314;&#31435;&#20102;&#20174;&#26893;&#34987;PDE&#21151;&#33021;&#31995;&#25968;&#21040;&#26680;PDE&#35299;&#30340;&#26144;&#23556;&#30340;&#36830;&#32493;&#24615;&#65292;&#35777;&#26126;&#20102;DeepONet&#36924;&#36817;&#26680;PDE&#35299;&#30340;&#23384;&#22312;&#24615;</title><link>http://arxiv.org/abs/2312.16762</link><description>&lt;p&gt;
Backstepping&#31070;&#32463;&#25805;&#20316;&#21592;&#29992;&#20110;$2\times 2$&#21452;&#26354;PDEs
&lt;/p&gt;
&lt;p&gt;
Backstepping Neural Operators for $2\times 2$ Hyperbolic PDEs. (arXiv:2312.16762v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;$2\times 2$&#21452;&#26354;PDE&#30340;Backstepping&#31070;&#32463;&#25805;&#20316;&#21592;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#32806;&#21512;&#30340;Goursat&#24418;&#24335;PDE&#65292;&#24182;&#24314;&#31435;&#20102;&#20174;&#26893;&#34987;PDE&#21151;&#33021;&#31995;&#25968;&#21040;&#26680;PDE&#35299;&#30340;&#26144;&#23556;&#30340;&#36830;&#32493;&#24615;&#65292;&#35777;&#26126;&#20102;DeepONet&#36924;&#36817;&#26680;PDE&#35299;&#30340;&#23384;&#22312;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#38750;&#32447;&#24615;&#25805;&#20316;&#21592;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;DeepONet&#65292;&#22312;&#21333;&#20010;Goursat&#24418;&#24335;&#30340;PDE&#25511;&#21046;&#21333;&#20010;&#21453;&#39304;&#22686;&#30410;&#20989;&#25968;&#30340;PDE&#21453;&#21521;&#35774;&#35745;&#20013;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#33021;&#21147;&#12290;&#22312;&#32806;&#21512;PDE&#30340;&#36793;&#30028;&#25511;&#21046;&#20013;&#65292;&#32806;&#21512;&#30340;Goursat&#24418;&#24335;PDE&#25511;&#21046;&#20004;&#20010;&#25110;&#22810;&#20010;&#22686;&#30410;&#26680; - &#36825;&#26159;&#36804;&#20170;&#20026;&#27490;DeepONet&#26410;&#35299;&#20915;&#30340;PDE&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#31616;&#21333;&#30340;&#36870;&#21521;&#20256;&#25773;$2\times 2$&#32806;&#21512;&#31995;&#32479;&#26469;&#25171;&#24320;&#36229;&#21367;&#31215;$2\times 2$&#26680;PDE&#31995;&#32479;&#30340;&#36817;&#20284;&#20027;&#39064;&#65292;&#20854;&#25511;&#21046;&#20013;&#20986;&#29616;&#20102;Goursat&#24418;&#24335;&#30340;PDE&#31995;&#32479;&#12290;&#24212;&#29992;&#21253;&#25324;&#30707;&#27833;&#38075;&#20117;&#12289;&#27973;&#27700;&#27874;&#30340;Saint-Venant&#27169;&#22411;&#20197;&#21450;&#23494;&#38598;&#20132;&#36890;&#27969;&#20013;&#30340;Aw-Rascle-Zhang&#27169;&#22411;&#30340;&#20572;&#36710;&#21644;&#34892;&#39542;&#19981;&#31283;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20174;&#65288;&#24635;&#20849;&#20116;&#20010;&#65289;&#26893;&#34987;PDE&#30340;&#21151;&#33021;&#31995;&#25968;&#21040;&#26680;PDE&#35299;&#30340;&#26144;&#23556;&#30340;&#36830;&#32493;&#24615;&#65292;&#35777;&#26126;&#20102;DeepONet&#36924;&#36817;&#26680;PDE&#35299;&#30340;&#23384;&#22312;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep neural network approximation of nonlinear operators, commonly referred to as DeepONet, has proven capable of approximating PDE backstepping designs in which a single Goursat-form PDE governs a single feedback gain function. In boundary control of coupled PDEs, coupled Goursat-form PDEs govern two or more gain kernels -- a PDE structure unaddressed thus far with DeepONet. In this note, we open the subject of approximating systems of gain kernel PDEs for hyperbolic PDE plants by considering a simple counter-convecting $2\times 2$ coupled system in whose control a $2\times 2$ kernel PDE systems in Goursat form arises. Applications include oil drilling, Saint-Venant model of shallow water waves, and Aw-Rascle-Zhang model of stop-and-go instability in congested traffic flow. In this paper we establish the continuity of the mapping from (a total of five) plant PDE functional coefficients to the kernel PDE solutions, prove the existence of an arbitrarily close DeepONet approximation to t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#21463;&#25928;&#29575;&#38480;&#21046;&#30340;&#25928;&#29992;-&#38544;&#31169;&#21452;&#30446;&#26631;&#20248;&#21270;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#25928;&#29992;-&#38544;&#31169;&#30340;&#26435;&#34913;&#65292;&#24573;&#35270;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#20854;&#20182;&#24433;&#21709;&#22240;&#32032;&#12290;&#35813;&#30740;&#31350;&#23545;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2312.16554</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#21463;&#25928;&#29575;&#38480;&#21046;&#30340;&#25928;&#29992;-&#38544;&#31169;&#21452;&#30446;&#26631;&#20248;&#21270;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Analysis of Efficiency Constrained Utility-Privacy Bi-Objective Optimization in Federated Learning. (arXiv:2312.16554v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#21463;&#25928;&#29575;&#38480;&#21046;&#30340;&#25928;&#29992;-&#38544;&#31169;&#21452;&#30446;&#26631;&#20248;&#21270;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#25928;&#29992;-&#38544;&#31169;&#30340;&#26435;&#34913;&#65292;&#24573;&#35270;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#20854;&#20182;&#24433;&#21709;&#22240;&#32032;&#12290;&#35813;&#30740;&#31350;&#23545;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#20010;&#20307;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#23398;&#20064;&#20849;&#20139;&#27169;&#22411;&#12290;FL&#20013;&#30340;&#25928;&#29992;&#12289;&#38544;&#31169;&#21644;&#35757;&#32451;&#25928;&#29575;&#38382;&#39064;&#24050;&#24341;&#36215;&#37325;&#35201;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#24046;&#20998;&#38544;&#31169;&#24050;&#25104;&#20026;FL&#20013;&#19968;&#31181;&#20027;&#27969;&#25216;&#26415;&#65292;&#20445;&#25252;&#20010;&#20307;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#21516;&#26102;&#24433;&#21709;&#25928;&#29992;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;&#22312;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#65288;DPFL&#65289;&#20013;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#25928;&#29992;-&#38544;&#31169;&#30340;&#26435;&#34913;&#65292;&#32780;&#24573;&#35270;&#20102;&#21450;&#26102;&#23436;&#25104;&#25152;&#24517;&#38656;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#24046;&#20998;&#38544;&#31169;&#36890;&#36807;&#22312;&#27599;&#36718;&#36890;&#20449;&#20013;&#23545;&#36873;&#23450;&#30340;&#23458;&#25143;&#31471;&#24341;&#20837;&#21463;&#25511;&#30340;&#38543;&#26426;&#24615;&#65288;&#22122;&#22768;&#65289;&#26469;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#30740;&#31350;&#20102;&#22122;&#22768;&#27700;&#24179;&#65288;$\sigma$&#65289;&#21644;&#36890;&#20449;&#36718;&#25968;&#65288;$T$&#65289;&#23545;&#38544;&#31169;-&#25928;&#29992;&#21160;&#24577;&#30340;&#24433;&#21709;&#65292;&#20294;&#24573;&#35270;&#20102;&#20854;&#20182;&#24433;&#21709;&#22240;&#32032;&#65292;&#22914;&#26679;&#26412;&#27604;&#20363;&#65288;$q$&#65292;&#21363;&#36873;&#23450;&#23458;&#25143;&#31471;&#30340;&#27604;&#20363;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple clients to collaboratively learn a shared model without sharing their individual data. Concerns about utility, privacy, and training efficiency in FL have garnered significant research attention. Differential privacy has emerged as a prevalent technique in FL, safeguarding the privacy of individual user data while impacting utility and training efficiency. Within Differential Privacy Federated Learning (DPFL), previous studies have primarily focused on the utility-privacy trade-off, neglecting training efficiency, which is crucial for timely completion. Moreover, differential privacy achieves privacy by introducing controlled randomness (noise) on selected clients in each communication round. Previous work has mainly examined the impact of noise level ($\sigma$) and communication rounds ($T$) on the privacy-utility dynamic, overlooking other influential factors like the sample ratio ($q$, the proportion of selected clients). This paper systemati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36890;&#20449;&#26041;&#26696;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#20132;&#27969;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#21457;&#36865;&#20010;&#24615;&#21270;&#30340;&#28040;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#21512;&#20316;&#21644;&#22242;&#38431;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.15600</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Context-aware Communication for Multi-agent Reinforcement Learning. (arXiv:2312.15600v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15600
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36890;&#20449;&#26041;&#26696;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#20132;&#27969;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#21457;&#36865;&#20010;&#24615;&#21270;&#30340;&#28040;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#21512;&#20316;&#21644;&#22242;&#38431;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#65292;&#26377;&#25928;&#30340;&#36890;&#20449;&#21327;&#35758;&#23545;&#20110;&#20419;&#36827;&#21512;&#20316;&#21644;&#25552;&#39640;&#22242;&#38431;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#21033;&#29992;&#36890;&#20449;&#65292;&#35768;&#22810;&#20197;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#23558;&#26412;&#22320;&#20449;&#24687;&#21387;&#32553;&#25104;&#19968;&#26465;&#28040;&#24687;&#24182;&#24191;&#25773;&#32473;&#25152;&#26377;&#21487;&#36798;&#30340;&#26234;&#33021;&#20307;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#21487;&#33021;&#26080;&#27861;&#20026;&#20010;&#20307;&#26234;&#33021;&#20307;&#25552;&#20379;&#36275;&#22815;&#12289;&#20851;&#38190;&#21644;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#22312;&#24102;&#23485;&#20005;&#37325;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#12290;&#36825;&#28608;&#21169;&#25105;&#20204;&#20026;MARL&#24320;&#21457;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36890;&#20449;&#26041;&#26696;&#65292;&#26088;&#22312;&#21521;&#19981;&#21516;&#30340;&#26234;&#33021;&#20307;&#21457;&#36865;&#20010;&#24615;&#21270;&#30340;&#28040;&#24687;&#12290;&#25105;&#20204;&#30340;&#36890;&#20449;&#21327;&#35758;&#21517;&#20026;CACOM&#65292;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#38454;&#27573;&#20013;&#65292;&#26234;&#33021;&#20307;&#20197;&#24191;&#25773;&#26041;&#24335;&#20132;&#25442;&#31895;&#30053;&#34920;&#31034;&#65292;&#20026;&#31532;&#20108;&#20010;&#38454;&#27573;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#32039;&#38543;&#20854;&#21518;&#65292;&#26234;&#33021;&#20307;&#22312;&#31532;&#20108;&#20010;&#38454;&#27573;&#20013;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20026;&#25509;&#25910;&#32773;&#36873;&#25321;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#28040;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#23398;&#20064;&#30340;&#27493;&#38271;&#37327;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective communication protocols in multi-agent reinforcement learning (MARL) are critical to fostering cooperation and enhancing team performance. To leverage communication, many previous works have proposed to compress local information into a single message and broadcast it to all reachable agents. This simplistic messaging mechanism, however, may fail to provide adequate, critical, and relevant information to individual agents, especially in severely bandwidth-limited scenarios. This motivates us to develop context-aware communication schemes for MARL, aiming to deliver personalized messages to different agents. Our communication protocol, named CACOM, consists of two stages. In the first stage, agents exchange coarse representations in a broadcast fashion, providing context for the second stage. Following this, agents utilize attention mechanisms in the second stage to selectively generate messages personalized for the receivers. Furthermore, we employ the learned step size quant
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#23884;&#20837;&#23545;&#31561;&#21464;&#37327;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#23884;&#20837;&#26041;&#27861;&#19982;&#23545;&#31216;&#32676;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#19981;&#21516;&#34920;&#31034;&#23545;&#32593;&#32476;&#21487;&#34920;&#36798;&#24615;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#20998;&#31867;&#20934;&#30830;&#29575;&#19982;&#23884;&#20837;&#26041;&#27861;&#26377;&#26126;&#30830;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.13250</link><description>&lt;p&gt;
&#25968;&#25454;&#23884;&#20837;&#22312;&#31561;&#21464;&#37327;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The role of data embedding in equivariant quantum convolutional neural networks. (arXiv:2312.13250v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#23884;&#20837;&#23545;&#31561;&#21464;&#37327;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#23884;&#20837;&#26041;&#27861;&#19982;&#23545;&#31216;&#32676;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#19981;&#21516;&#34920;&#31034;&#23545;&#32593;&#32476;&#21487;&#34920;&#36798;&#24615;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#20998;&#31867;&#20934;&#30830;&#29575;&#19982;&#23884;&#20837;&#26041;&#27861;&#26377;&#26126;&#30830;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26159;&#25351;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#23545;&#31216;&#24615;&#26469;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#21487;&#35757;&#32451;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#36825;&#20010;&#24605;&#24819;&#24050;&#32463;&#34987;&#24341;&#20837;&#21040;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#24418;&#25104;&#20102;&#31561;&#21464;&#37327;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;EQNNs&#65289;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32463;&#20856;&#21040;&#37327;&#23376;&#23884;&#20837;&#23545;&#31561;&#21464;&#37327;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;EQCNNs&#65289;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25968;&#25454;&#23884;&#20837;&#26041;&#27861;&#19982;&#23545;&#31216;&#32676;&#34920;&#31034;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#20998;&#26512;&#25913;&#21464;&#34920;&#31034;&#23545;EQCNN&#21487;&#34920;&#36798;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#27604;&#36739;EQCNN&#19982;&#19977;&#31181;&#19981;&#21516;&#22522;&#30784;&#25490;&#21015;&#24133;&#24230;&#23884;&#20837;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#19982;&#38750;&#31561;&#21464;&#37327;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QCNN&#65289;&#30340;&#32467;&#26524;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20998;&#31867;&#20934;&#30830;&#29575;&#19982;&#23884;&#20837;&#26041;&#27861;&#26377;&#26126;&#30830;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric deep learning refers to the scenario in which the symmetries of a dataset are used to constrain the parameter space of a neural network and thus, improve their trainability and generalization. Recently this idea has been incorporated into the field of quantum machine learning, which has given rise to equivariant quantum neural networks (EQNNs). In this work, we investigate the role of classical-to-quantum embedding on the performance of equivariant quantum convolutional neural networks (EQCNNs) for the classification of images. We discuss the connection between the data embedding method and the resulting representation of a symmetry group and analyze how changing representation affects the expressibility of an EQCNN. We numerically compare the classification accuracy of EQCNNs with three different basis-permuted amplitude embeddings to the one obtained from a non-equivariant quantum convolutional neural network (QCNN). Our results show a clear dependence of classification acc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#29256;&#26412;&#65292;&#29992;&#20110;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#23384;&#38382;&#39064;&#19978;&#23384;&#22312;&#30340;&#22256;&#25200;&#12290;&#19982;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#30340;Graph Transformer&#30456;&#27604;&#65292;&#24212;&#29992;&#20102;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#30830;&#23454;&#21487;&#20197;&#31283;&#23450;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.11730</link><description>&lt;p&gt;
&#24378;&#21270;&#22270;&#36716;&#25442;&#22120;&#19982;&#27491;&#21017;&#21270;&#20851;&#27880;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Stronger Graph Transformer with Regularized Attention Scores. (arXiv:2312.11730v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#29256;&#26412;&#65292;&#29992;&#20110;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#23384;&#38382;&#39064;&#19978;&#23384;&#22312;&#30340;&#22256;&#25200;&#12290;&#19982;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#30340;Graph Transformer&#30456;&#27604;&#65292;&#24212;&#29992;&#20102;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#30830;&#23454;&#21487;&#20197;&#31283;&#23450;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20197;&#20854;&#20869;&#23384;&#28040;&#32791;&#22823;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#26368;&#36817;&#21457;&#29616;&#65292;&#22522;&#20110;Transformer&#30340;GNN&#31216;&#20026;Graph Transformer&#22312;&#23384;&#22312;&#38271;&#31243;&#20381;&#36182;&#24615;&#26102;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#25968;&#25454;&#21644;Transformer&#26550;&#26500;&#30456;&#32467;&#21512;&#23548;&#33268;&#20102;&#35760;&#24518;&#38382;&#39064;&#26356;&#21152;&#20005;&#37325;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#8221;&#30340;&#29256;&#26412;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#20943;&#36731;GT&#30340;&#20869;&#23384;&#28322;&#20986;&#38382;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19981;&#28165;&#26970;&#22312;&#20301;&#32622;&#32534;&#30721;&#30340;&#22522;&#30784;&#19978;&#26159;&#21542;&#26377;&#36793;&#32536;&#27491;&#21017;&#21270;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#28982;&#32780;&#65292;&#26174;&#28982;&#65292;&#24212;&#29992;&#25105;&#20204;&#30340;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#30830;&#23454;&#21487;&#20197;&#31283;&#23450;&#22320;&#25913;&#21892;GT&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#30340;GT&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks are notorious for its memory consumption. A recent Transformer-based GNN called Graph Transformer is shown to obtain superior performances when long range dependencies exist. However, combining graph data and Transformer architecture led to a combinationally worse memory issue. We propose a novel version of "edge regularization technique" that alleviates the need for Positional Encoding and ultimately alleviate GT's out of memory issue. We observe that it is not clear whether having an edge regularization on top of positional encoding is helpful. However, it seems evident that applying our edge regularization technique indeed stably improves GT's performance compared to GT without Positional Encoding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26102;&#38388;&#21464;&#25442;&#22120;&#21516;&#26102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26356;&#22909;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.11714</link><description>&lt;p&gt;
&#26102;&#38388;&#21464;&#25442;&#22120;&#65306;&#34701;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Time-Transformer: Integrating Local and Global Features for Better Time Series Generation. (arXiv:2312.11714v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26102;&#38388;&#21464;&#25442;&#22120;&#21516;&#26102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26356;&#22909;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26159;&#35299;&#20915;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#38388;&#29305;&#24615;&#65292;&#21253;&#25324;&#26412;&#22320;&#30456;&#20851;&#24615;&#21644;&#20840;&#23616;&#20381;&#36182;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#26410;&#33021;&#26377;&#25928;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;'&#26102;&#38388;&#21464;&#25442;&#22120;AAE'&#65292;&#23427;&#30001;&#19968;&#20010;&#23545;&#25239;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;AAE&#65289;&#21644;&#19968;&#20010;&#21517;&#20026;'&#26102;&#38388;&#21464;&#25442;&#22120;'&#30340;&#26032;&#35774;&#35745;&#26550;&#26500;&#32452;&#25104;&#12290;&#26102;&#38388;&#21464;&#25442;&#22120;&#39318;&#20808;&#36890;&#36807;&#23618;&#27425;&#24182;&#34892;&#35774;&#35745;&#21516;&#26102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#32467;&#21512;&#20102;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#21644;Transformer&#30340;&#33021;&#21147;&#65292;&#20998;&#21035;&#25552;&#21462;&#26412;&#22320;&#29305;&#24449;&#21644;&#20840;&#23616;&#20381;&#36182;&#24615;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#21521;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#22312;&#20004;&#20010;&#20998;&#25903;&#20043;&#38388;&#25552;&#20379;&#20114;&#34917;&#30340;&#24341;&#23548;&#65292;&#24182;&#23454;&#29616;&#26412;&#22320;&#29305;&#24449;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#21512;&#36866;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating time series data is a promising approach to address data deficiency problems. However, it is also challenging due to the complex temporal properties of time series data, including local correlations as well as global dependencies. Most existing generative models have failed to effectively learn both the local and global properties of time series data. To address this open problem, we propose a novel time series generative model named 'Time-Transformer AAE', which consists of an adversarial autoencoder (AAE) and a newly designed architecture named 'Time-Transformer' within the decoder. The Time-Transformer first simultaneously learns local and global features in a layer-wise parallel design, combining the abilities of Temporal Convolutional Networks and Transformer in extracting local features and global dependencies respectively. Second, a bidirectional cross attention is proposed to provide complementary guidance across the two branches and achieve proper fusion between loc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;KL&#32422;&#26463;&#19979;&#30340;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#23454;&#36341;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.11456</link><description>&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#30340;&#36845;&#20195;&#20559;&#22909;&#23398;&#20064;&#65306;&#22312;KL&#32422;&#26463;&#19979;&#23558;&#29702;&#35770;&#19982;&#23454;&#36341;&#32852;&#31995;&#36215;&#26469;&#30340;RLHF
&lt;/p&gt;
&lt;p&gt;
Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint. (arXiv:2312.11456v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;KL&#32422;&#26463;&#19979;&#30340;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#23454;&#36341;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#19982;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#23545;&#40784;&#36807;&#31243;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26631;&#20934;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;&#21363;&#21453;&#21521;KL&#27491;&#21017;&#21270;&#30340;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#29992;&#20110;RLHF&#12290;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#65292;&#20294;&#23545;&#36825;&#20010;&#20844;&#24335;&#30340;&#20005;&#26684;&#29702;&#35770;&#20998;&#26512;&#20173;&#28982;&#24456;&#24320;&#25918;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#22312;&#31163;&#32447;&#12289;&#22312;&#32447;&#21644;&#28151;&#21512;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#29702;&#35770;&#20445;&#35777;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#26397;&#30528;&#23454;&#38469;&#24212;&#29992;&#30340;&#26041;&#21521;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23545;&#20449;&#24687;&#29702;&#35770;&#31574;&#30053;&#25913;&#36827;&#39044;&#35328;&#30340;&#31283;&#20581;&#36817;&#20284;&#65292;&#33258;&#28982;&#22320;&#20135;&#29983;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;RLHF&#31639;&#27861;&#12290;&#36825;&#21253;&#25324;&#22312;&#32447;&#22330;&#26223;&#20013;&#30340;&#36845;&#20195;&#29256;&#26412;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#31639;&#27861;&#65292;&#20197;&#21450;&#31163;&#32447;&#24773;&#26223;&#19979;&#30340;&#22810;&#27493;&#25298;&#32477;&#25277;&#26679;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#23545;&#40784;&#23454;&#39564;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the theoretical framework of the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees.  Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#25581;&#31034;&#25209;&#27425;&#24402;&#19968;&#21270;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#24182;&#20171;&#32461;&#20102;&#27979;&#35797;&#26102;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;TEMA&#65289;&#26041;&#27861;&#26469;&#24357;&#34917;&#35757;&#32451;&#21644;&#27979;&#35797;&#25209;&#27425;&#20043;&#38388;&#30340;&#31867;&#21035;&#22810;&#26679;&#24615;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20934;&#30830;&#30340;&#30446;&#26631;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2312.09486</link><description>&lt;p&gt;
&#25581;&#31034;&#29992;&#20110;&#30495;&#23454;&#27979;&#35797;&#26102;&#36866;&#24212;&#30340;&#25209;&#27425;&#24402;&#19968;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unraveling Batch Normalization for Realistic Test-Time Adaptation. (arXiv:2312.09486v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#25581;&#31034;&#25209;&#27425;&#24402;&#19968;&#21270;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#24182;&#20171;&#32461;&#20102;&#27979;&#35797;&#26102;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;TEMA&#65289;&#26041;&#27861;&#26469;&#24357;&#34917;&#35757;&#32451;&#21644;&#27979;&#35797;&#25209;&#27425;&#20043;&#38388;&#30340;&#31867;&#21035;&#22810;&#26679;&#24615;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20934;&#30830;&#30340;&#30446;&#26631;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;&#25209;&#27425;&#24402;&#19968;&#21270;&#26469;&#20943;&#23567;&#39046;&#22495;&#24046;&#24322;&#65292;&#20294;&#26159;&#24403;&#20351;&#29992;&#30495;&#23454;&#30340;&#23567;&#25209;&#37327;&#26102;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20250;&#38477;&#20302;&#65292;&#22240;&#20026;&#30446;&#26631;&#20272;&#35745;&#19981;&#20934;&#30830;&#12290;&#30001;&#20110;&#20197;&#21069;&#30340;&#23581;&#35797;&#20165;&#20165;&#26159;&#24341;&#20837;&#28304;&#32479;&#35745;&#25968;&#25454;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#27492;&#30446;&#26631;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#22522;&#26412;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#20351;&#24471;&#27979;&#35797;&#26102;&#39046;&#22495;&#21464;&#21270;&#38382;&#39064;&#26410;&#35299;&#20915;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#38477;&#32423;&#38382;&#39064;&#12290;&#36890;&#36807;&#25581;&#31034;&#25209;&#27425;&#24402;&#19968;&#21270;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#20934;&#30830;&#30340;&#30446;&#26631;&#32479;&#35745;&#20027;&#35201;&#26469;&#33258;&#20110;&#25209;&#27425;&#20013;&#31867;&#21035;&#22810;&#26679;&#24615;&#30340;&#22823;&#24133;&#20943;&#23569;&#12290;&#26681;&#25454;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30452;&#25509;&#30340;&#24037;&#20855;&#8212;&#8212;&#27979;&#35797;&#26102;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;TEMA&#65289;&#65292;&#26469;&#24357;&#34917;&#35757;&#32451;&#21644;&#27979;&#35797;&#25209;&#27425;&#20043;&#38388;&#31867;&#21035;&#22810;&#26679;&#24615;&#30340;&#24046;&#36317;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;TEMA&#21487;&#36866;&#24212;&#22320;&#25193;&#23637;&#20102;&#20856;&#22411;&#26041;&#27861;&#30340;&#33539;&#22260;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#25209;&#27425;&#30340;&#33539;&#22260;&#65292;&#20197;&#21253;&#21547;&#19968;&#20010;&#22810;&#26679;&#30340;&#31867;&#21035;&#20449;&#24687;&#38598;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#30340;&#30446;&#26631;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent test-time adaptations exhibit efficacy by adjusting batch normalization to narrow domain disparities, their effectiveness diminishes with realistic mini-batches due to inaccurate target estimation. As previous attempts merely introduce source statistics to mitigate this issue, the fundamental problem of inaccurate target estimation still persists, leaving the intrinsic test-time domain shifts unresolved. This paper delves into the problem of mini-batch degradation. By unraveling batch normalization, we discover that the inexact target statistics largely stem from the substantially reduced class diversity in batch. Drawing upon this insight, we introduce a straightforward tool, Test-time Exponential Moving Average (TEMA), to bridge the class diversity gap between training and testing batches. Importantly, our TEMA adaptively extends the scope of typical methods beyond the current batch to incorporate a diverse set of class information, which in turn boosts an accurate targe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36827;&#21270;&#27785;&#31215;&#27744;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#29992;&#20110;&#30740;&#31350;&#21160;&#29289;&#36866;&#24212;&#29615;&#22659;&#30340;&#26426;&#21046;&#12290;&#36825;&#31181;&#27169;&#22411;&#22522;&#20110;&#20803;&#22686;&#24378;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28436;&#21270;&#21644;&#21457;&#23637;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21033;&#29992;&#36827;&#21270;&#27785;&#31215;&#27744;&#26469;&#21152;&#36895;&#21644;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2312.06695</link><description>&lt;p&gt;
&#36827;&#21270;&#27785;&#31215;&#27744;&#29992;&#20110;&#20803;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Evolving Reservoirs for Meta Reinforcement Learning. (arXiv:2312.06695v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36827;&#21270;&#27785;&#31215;&#27744;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#29992;&#20110;&#30740;&#31350;&#21160;&#29289;&#36866;&#24212;&#29615;&#22659;&#30340;&#26426;&#21046;&#12290;&#36825;&#31181;&#27169;&#22411;&#22522;&#20110;&#20803;&#22686;&#24378;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28436;&#21270;&#21644;&#21457;&#23637;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21033;&#29992;&#36827;&#21270;&#27785;&#31215;&#27744;&#26469;&#21152;&#36895;&#21644;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#22312;&#20854;&#19968;&#29983;&#20013;&#32463;&#24120;&#23637;&#31034;&#20986;&#23545;&#29615;&#22659;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#24418;&#24577;&#21644;&#31070;&#32463;&#32467;&#26500;&#30340;&#28436;&#21270;&#12290;&#36825;&#20123;&#32467;&#26500;&#25429;&#25417;&#21040;&#20102;&#20195;&#38469;&#20043;&#38388;&#20849;&#20139;&#30340;&#29615;&#22659;&#29305;&#24449;&#65292;&#20197;&#21152;&#36895;&#21644;&#24341;&#23548;&#19968;&#29983;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#30740;&#31350;&#23454;&#29616;&#36825;&#19968;&#36807;&#31243;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20803;&#22686;&#24378;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#20316;&#20026;&#28436;&#21270;&#21644;&#21457;&#23637;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#22411;&#12290;&#22312;&#28436;&#21270;&#23610;&#24230;&#19978;&#65292;&#25105;&#20204;&#28436;&#21270;&#27785;&#31215;&#27744;&#65292;&#36825;&#26159;&#19968;&#26063;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#19982;&#24120;&#35268;&#32593;&#32476;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#20248;&#21270;&#30340;&#19981;&#26159;&#31361;&#35302;&#26435;&#37325;&#65292;&#32780;&#26159;&#25511;&#21046;&#32467;&#26524;&#32593;&#32476;&#26550;&#26500;&#30340;&#23439;&#35266;&#32423;&#21035;&#23646;&#24615;&#30340;&#36229;&#21442;&#25968;&#12290;&#22312;&#21457;&#23637;&#23610;&#24230;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#36827;&#21270;&#27785;&#31215;&#27744;&#26469;&#20419;&#36827;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#23398;&#20064;&#34892;&#20026;&#31574;&#30053;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#65292;&#27785;&#31215;&#27744;&#32534;&#30721;&#29615;&#22659;&#30340;&#20449;&#24687;&#20197;&#20248;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Animals often demonstrate a remarkable ability to adapt to their environments during their lifetime. They do so partly due to the evolution of morphological and neural structures. These structures capture features of environments shared between generations to bias and speed up lifetime learning. In this work, we propose a computational model for studying a mechanism that can enable such a process. We adopt a computational framework based on meta reinforcement learning as a model of the interplay between evolution and development. At the evolutionary scale, we evolve reservoirs, a family of recurrent neural networks that differ from conventional networks in that one optimizes not the synaptic weights, but hyperparameters controlling macro-level properties of the resulting network architecture. At the developmental scale, we employ these evolved reservoirs to facilitate the learning of a behavioral policy through Reinforcement Learning (RL). Within an RL agent, a reservoir encodes the en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#21305;&#37197;&#19979;&#30028;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.04027</link><description>&lt;p&gt;
&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
The sample complexity of multi-distribution learning. (arXiv:2312.04027v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#21305;&#37197;&#19979;&#30028;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20998;&#24067;&#23398;&#20064;&#23558;&#32463;&#20856;&#30340;PAC&#23398;&#20064;&#25512;&#24191;&#21040;&#22788;&#29702;&#26469;&#33258;&#22810;&#20010;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#32473;&#23450;&#19968;&#32452;$k$&#20010;&#25968;&#25454;&#20998;&#24067;&#21644;&#19968;&#20010;VC&#32500;&#24230;&#20026;$d$&#30340;&#20551;&#35774;&#31867;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#20551;&#35774;&#65292;&#20351;&#24471;&#22312;$k$&#20010;&#20998;&#24067;&#19978;&#30340;&#26368;&#22823;&#24635;&#20307;&#25439;&#22833;&#26368;&#23567;&#65292;&#35823;&#24046;&#19981;&#36229;&#36807;$\epsilon$&#12290;&#26412;&#25991;&#36890;&#36807;&#32473;&#20986;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#31639;&#27861;$\widetilde{O}((d+k)\epsilon^{-2}) \cdot (k/\epsilon)^{o(1)}$&#26469;&#35299;&#20915;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;&#36825;&#20010;&#32467;&#26524;&#19982;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#35299;&#20915;&#20102;Awasthi&#12289;Haghtalab&#21644;Zhao&#22312;COLT 2023&#20013;&#25552;&#20986;&#30340;&#24320;&#25918;&#38382;&#39064; [AHZ23]&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-distribution learning generalizes the classic PAC learning to handle data coming from multiple distributions. Given a set of $k$ data distributions and a hypothesis class of VC dimension $d$, the goal is to learn a hypothesis that minimizes the maximum population loss over $k$ distributions, up to $\epsilon$ additive error. In this paper, we settle the sample complexity of multi-distribution learning by giving an algorithm of sample complexity $\widetilde{O}((d+k)\epsilon^{-2}) \cdot (k/\epsilon)^{o(1)}$. This matches the lower bound up to sub-polynomial factor and resolves the COLT 2023 open problem of Awasthi, Haghtalab and Zhao [AHZ23].
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#36923;&#36753;&#32422;&#26463;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20266;&#35821;&#20041;&#25439;&#22833;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#36755;&#20986;&#30340;&#23616;&#37096;&#36817;&#20284;&#19978;&#20248;&#21270;&#32422;&#26463;&#30340;&#20284;&#28982;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.03905</link><description>&lt;p&gt;
&#20855;&#26377;&#36923;&#36753;&#32422;&#26463;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20266;&#35821;&#20041;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints. (arXiv:2312.03905v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#36923;&#36753;&#32422;&#26463;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20266;&#35821;&#20041;&#25439;&#22833;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#36755;&#20986;&#30340;&#23616;&#37096;&#36817;&#20284;&#19978;&#20248;&#21270;&#32422;&#26463;&#30340;&#20284;&#28982;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#65288;neuro-symbolic AI&#65289;&#22635;&#34917;&#20102;&#32431;&#31526;&#21495;&#21644;&#31070;&#32463;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#36825;&#36890;&#24120;&#38656;&#35201;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#20998;&#24067;&#26041;&#38754;&#26368;&#22823;&#21270;&#23545;&#31526;&#21495;&#32422;&#26463;&#30340;&#20284;&#28982;&#12290;&#36825;&#20123;&#36755;&#20986;&#20998;&#24067;&#36890;&#24120;&#34987;&#20551;&#35774;&#20026;&#23436;&#20840;&#22240;&#23376;&#21270;&#30340;&#12290;&#36825;&#38480;&#21046;&#20102;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#22312;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#33258;&#22238;&#24402;&#20998;&#24067;&#65288;&#20363;&#22914;transformers&#65289;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#26679;&#30340;&#20998;&#24067;&#19979;&#65292;&#29978;&#33267;&#31616;&#21333;&#32422;&#26463;&#30340;&#27010;&#29575;&#20284;&#28982;&#30340;&#35745;&#31639;&#26159;#P-hard&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#19981;&#26159;&#35797;&#22270;&#23558;&#32422;&#26463;&#24378;&#21152;&#22312;&#25972;&#20010;&#36755;&#20986;&#20998;&#24067;&#19978;&#65292;&#32780;&#26159;&#22312;&#20854;&#38543;&#26426;&#30340;&#23616;&#37096;&#36817;&#20284;&#19978;&#36825;&#26679;&#20570;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#25105;&#20204;&#22312;&#20197;&#27169;&#22411;&#26679;&#26412;&#20026;&#20013;&#24515;&#30340;&#22522;&#20110;&#20266;&#20284;&#28982;&#30340;&#36817;&#20284;&#20013;&#20248;&#21270;&#32422;&#26463;&#30340;&#20284;&#28982;&#12290;&#25105;&#20204;&#30340;&#36817;&#20284;&#26159;&#22240;&#23376;&#21270;&#30340;&#65292;&#21487;&#20197;&#37325;&#29992;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#26159;&#39640;&#25928;&#35745;&#31639;&#31070;&#32463;&#31526;&#21495;&#25439;&#22833;&#30340;&#20027;&#35201;&#21407;&#21017;&#12290;&#27492;&#22806;&#65292;&#23427;&#26159;&#19968;&#20010;&#23616;&#37096;&#30340;&#65292;&#39640;&#20445;&#30495;&#24230;&#30340;&#20284;&#28982;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic AI bridges the gap between purely symbolic and neural approaches to learning. This often requires maximizing the likelihood of a symbolic constraint w.r.t the neural network's output distribution. Such output distributions are typically assumed to be fully-factorized. This limits the applicability of neuro-symbolic learning to the more expressive autoregressive distributions, e.g., transformers. Under such distributions, computing the likelihood of even simple constraints is #P-hard. Instead of attempting to enforce the constraint on the entire output distribution, we propose to do so on a random, local approximation thereof. More precisely, we optimize the likelihood of the constraint under a pseudolikelihood-based approximation centered around a model sample. Our approximation is factorized, allowing the reuse of solutions to sub-problems, a main tenet for efficiently computing neuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of the likelihoo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiGPrompt&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#20013;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2312.03731</link><description>&lt;p&gt;
&#22810;&#20010;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#22270;&#24418;&#25552;&#31034;&#30340;MultiGPrompt
&lt;/p&gt;
&lt;p&gt;
MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs. (arXiv:2312.03731v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiGPrompt&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#20013;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#21487;&#20197;&#22266;&#26377;&#22320;&#23545;Web&#19978;&#30456;&#20114;&#36830;&#25509;&#30340;&#23545;&#35937;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#25903;&#25345;&#19968;&#31995;&#21015;Web&#24212;&#29992;&#65292;&#27604;&#22914;&#32593;&#32476;&#20998;&#26512;&#21644;&#20869;&#23481;&#25512;&#33616;&#12290;&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20027;&#27969;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#31471;&#21040;&#31471;&#30417;&#30563;&#26694;&#26550;&#20013;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#19982;&#20219;&#21153;&#29305;&#23450;&#26631;&#31614;&#30340;&#21487;&#29992;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#20026;&#20102;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#24182;&#22686;&#24378;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#22522;&#20110;&#33258;&#30417;&#30563;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#32780;&#25552;&#31034;&#21017;&#34987;&#25552;&#20986;&#26469;&#36827;&#19968;&#27493;&#32553;&#23567;&#39044;&#35757;&#32451;&#20219;&#21153;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#30446;&#26631;&#24046;&#36317;&#12290;&#34429;&#28982;&#24050;&#32463;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#22270;&#24418;&#23398;&#20064;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#25506;&#32034;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#21033;&#29992;&#21333;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#23548;&#33268;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#21487;&#33021;&#23398;&#20064;&#30340;&#36890;&#29992;&#30693;&#35782;&#30340;&#23376;&#38598;&#21463;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;MultiGPrompt&#65292;&#29992;&#20110;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#22270;&#24418;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs can inherently model interconnected objects on the Web, thereby facilitating a series of Web applications, such as web analyzing and content recommendation. Recently, Graph Neural Networks (GNNs) have emerged as a mainstream technique for graph representation learning. However, their efficacy within an end-to-end supervised framework is significantly tied to the availabilityof task-specific labels. To mitigate labeling costs and enhance robustness in few-shot settings, pre-training on self-supervised tasks has emerged as a promising method, while prompting has been proposed to further narrow the objective gap between pretext and downstream tasks. Although there has been some initial exploration of prompt-based learning on graphs, they primarily leverage a single pretext task, resulting in a limited subset of general knowledge that could be learned from the pre-training data. Hence, in this paper, we propose MultiGPrompt, a novel multi-task pre-training and prompting framework to
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#22312;JAX&#20013;&#25193;&#23637;&#20102;&#33258;&#21160;&#24494;&#20998;&#21151;&#33021;&#65292;&#20351;&#20854;&#33021;&#22815;&#33258;&#21160;&#24494;&#20998;&#39640;&#38454;&#20989;&#25968;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#21407;&#22987;&#31639;&#23376;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20989;&#25968;&#24494;&#20998;&#30340;&#32447;&#24615;&#21270;&#21644;&#36716;&#32622;&#35268;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#24037;&#20855;&#22312;&#20989;&#25968;&#23548;&#25968;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#21644;&#31616;&#21333;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.18727</link><description>&lt;p&gt;
JAX&#20013;&#30340;&#33258;&#21160;&#20989;&#25968;&#24494;&#20998;
&lt;/p&gt;
&lt;p&gt;
Automatic Functional Differentiation in JAX. (arXiv:2311.18727v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18727
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;JAX&#20013;&#25193;&#23637;&#20102;&#33258;&#21160;&#24494;&#20998;&#21151;&#33021;&#65292;&#20351;&#20854;&#33021;&#22815;&#33258;&#21160;&#24494;&#20998;&#39640;&#38454;&#20989;&#25968;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#21407;&#22987;&#31639;&#23376;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20989;&#25968;&#24494;&#20998;&#30340;&#32447;&#24615;&#21270;&#21644;&#36716;&#32622;&#35268;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#24037;&#20855;&#22312;&#20989;&#25968;&#23548;&#25968;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#21644;&#31616;&#21333;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25193;&#23637;&#20102;JAX&#30340;&#21151;&#33021;&#65292;&#20351;&#20854;&#33021;&#22815;&#33258;&#21160;&#24494;&#20998;&#39640;&#38454;&#20989;&#25968;&#65288;&#20989;&#25968;&#31639;&#23376;&#21644;&#31639;&#31526;&#65289;&#12290;&#36890;&#36807;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#25968;&#32452;&#30340;&#25512;&#24191;&#65292;&#25105;&#20204;&#26080;&#32541;&#22320;&#20351;&#29992;JAX&#30340;&#29616;&#26377;&#21407;&#35821;&#31995;&#32479;&#26469;&#23454;&#29616;&#39640;&#38454;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#21407;&#22987;&#31639;&#23376;&#65292;&#20316;&#20026;&#26500;&#36896;&#20960;&#31181;&#20851;&#38190;&#31867;&#22411;&#30340;&#20989;&#25968;&#31639;&#23376;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#23545;&#20110;&#27599;&#20010;&#24341;&#20837;&#30340;&#21407;&#22987;&#31639;&#23376;&#65292;&#25105;&#20204;&#25512;&#23548;&#24182;&#23454;&#29616;&#20102;&#32447;&#24615;&#21270;&#21644;&#36716;&#32622;&#35268;&#21017;&#65292;&#19982;JAX&#30340;&#21069;&#21521;&#21644;&#21453;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#30340;&#20869;&#37096;&#21327;&#35758;&#20445;&#25345;&#19968;&#33268;&#12290;&#36825;&#20010;&#22686;&#24378;&#21151;&#33021;&#20801;&#35768;&#20351;&#29992;&#20256;&#32479;&#29992;&#20110;&#20989;&#25968;&#30340;&#30456;&#21516;&#35821;&#27861;&#36827;&#34892;&#20989;&#25968;&#24494;&#20998;&#12290;&#24471;&#21040;&#30340;&#20989;&#25968;&#26799;&#24230;&#26412;&#36523;&#23601;&#26159;&#21487;&#20197;&#22312;python&#20013;&#35843;&#29992;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#24212;&#29992;&#23637;&#31034;&#20102;&#36825;&#20010;&#24037;&#20855;&#30340;&#25928;&#26524;&#21644;&#31616;&#21333;&#24615;&#65292;&#20854;&#20013;&#20989;&#25968;&#23548;&#25968;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#27492;&#24037;&#20316;&#30340;&#28304;&#20195;&#30721;&#24050;&#22312;https://github.com/sail-sg/autofd&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend JAX with the capability to automatically differentiate higher-order functions (functionals and operators). By representing functions as a generalization of arrays, we seamlessly use JAX's existing primitive system to implement higher-order functions. We present a set of primitive operators that serve as foundational building blocks for constructing several key types of functionals. For every introduced primitive operator, we derive and implement both linearization and transposition rules, aligning with JAX's internal protocols for forward and reverse mode automatic differentiation. This enhancement allows for functional differentiation in the same syntax traditionally use for functions. The resulting functional gradients are themselves functions ready to be invoked in python. We showcase this tool's efficacy and simplicity through applications where functional derivatives are indispensable. The source code of this work is released at https://github.com/sail-sg/autofd .
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#22635;&#20805;&#32570;&#22833;&#25968;&#25454;&#26102;&#23558;&#26631;&#31614;&#19982;&#36755;&#20837;&#22534;&#21472;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22635;&#20805;&#25928;&#26524;&#65292;&#24182;&#21516;&#26102;&#22635;&#20805;&#26631;&#31614;&#21644;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.16877</link><description>&lt;p&gt;
&#20351;&#29992;&#35757;&#32451;&#26631;&#31614;&#36827;&#34892;&#22635;&#20805;&#21644;&#36890;&#36807;&#26631;&#31614;&#22635;&#20805;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Imputation using training labels and classification via label imputation. (arXiv:2311.16877v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#22635;&#20805;&#32570;&#22833;&#25968;&#25454;&#26102;&#23558;&#26631;&#31614;&#19982;&#36755;&#20837;&#22534;&#21472;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22635;&#20805;&#25928;&#26524;&#65292;&#24182;&#21516;&#26102;&#22635;&#20805;&#26631;&#31614;&#21644;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#32570;&#22833;&#25968;&#25454;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#22635;&#20805;&#26041;&#27861;&#26469;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#37117;&#26377;&#26631;&#31614;&#65292;&#20294;&#24120;&#35265;&#30340;&#22635;&#20805;&#26041;&#27861;&#36890;&#24120;&#21482;&#20381;&#36182;&#20110;&#36755;&#20837;&#32780;&#24573;&#30053;&#26631;&#31614;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#23558;&#26631;&#31614;&#22534;&#21472;&#21040;&#36755;&#20837;&#20013;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#36755;&#20837;&#30340;&#22635;&#20805;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#23558;&#39044;&#27979;&#30340;&#27979;&#35797;&#26631;&#31614;&#21021;&#22987;&#21270;&#20026;&#32570;&#22833;&#20540;&#65292;&#24182;&#23558;&#26631;&#31614;&#19982;&#36755;&#20837;&#22534;&#21472;&#22312;&#19968;&#36215;&#36827;&#34892;&#22635;&#20805;&#12290;&#36825;&#26679;&#21487;&#20197;&#21516;&#26102;&#22635;&#20805;&#26631;&#31614;&#21644;&#36755;&#20837;&#12290;&#32780;&#19988;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#26080;&#38656;&#20219;&#20309;&#20808;&#21069;&#30340;&#22635;&#20805;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#36830;&#32493;&#22411;&#12289;&#20998;&#31867;&#22411;&#25110;&#28151;&#21512;&#22411;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing data is a common problem in practical settings. Various imputation methods have been developed to deal with missing data. However, even though the label is usually available in the training data, the common practice of imputation usually only relies on the input and ignores the label. In this work, we illustrate how stacking the label into the input can significantly improve the imputation of the input. In addition, we propose a classification strategy that initializes the predicted test label with missing values and stacks the label with the input for imputation. This allows imputing the label and the input at the same time. Also, the technique is capable of handling data training with missing labels without any prior imputation and is applicable to continuous, categorical, or mixed-type data. Experiments show promising results in terms of accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20934;&#30830;&#21487;&#38752;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25925;&#38556;&#33410;&#28857;&#20256;&#25773;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2311.16522</link><description>&lt;p&gt;
&#30005;&#21147;&#31995;&#32479;&#20013;&#21160;&#24577;&#25925;&#38556;&#29305;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dynamic Fault Characteristics Evaluation in Power Grid. (arXiv:2311.16522v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20934;&#30830;&#21487;&#38752;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25925;&#38556;&#33410;&#28857;&#20256;&#25773;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#24378;&#36816;&#32500;&#30340;&#26234;&#33021;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#30693;&#35782;&#22270;&#35889;&#26469;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#12290;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#20026;&#20102;&#39564;&#35777;&#33410;&#28857;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#65292;&#36824;&#36827;&#34892;&#20102;&#27599;&#20010;&#33410;&#28857;&#36755;&#20986;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20223;&#30495;&#22330;&#26223;&#20013;&#20934;&#30830;&#22320;&#23450;&#20301;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#24314;&#27169;&#21487;&#20197;&#23450;&#24615;&#22320;&#32771;&#23519;&#25925;&#38556;&#22914;&#20309;&#22312;&#33410;&#28857;&#38388;&#20256;&#25773;&#65292;&#20026;&#20998;&#26512;&#25925;&#38556;&#33410;&#28857;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance the intelligence degree in operation and maintenance, a novel method for fault detection in power grids is proposed. The proposed GNN-based approach first identifies fault nodes through a specialized feature extraction method coupled with a knowledge graph. By incorporating temporal data, the method leverages the status of nodes from preceding and subsequent time periods to help current fault detection. To validate the effectiveness of the node features, a correlation analysis of the output features from each node was conducted. The results from experiments show that this method can accurately locate fault nodes in simulation scenarios with a remarkable accuracy. Additionally, the graph neural network based feature modeling allows for a qualitative examination of how faults spread across nodes, which provides valuable insights for analyzing fault nodes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#27468;&#35789;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#29983;&#25104;&#36866;&#21512;&#27468;&#35789;&#27468;&#26354;&#30340;&#33410;&#25293;&#35760;&#21495;&#65292;&#24182;&#25581;&#31034;&#28508;&#22312;&#30340;&#33410;&#22863;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2311.15480</link><description>&lt;p&gt;
&#20351;&#29992;&#27468;&#35789;&#33258;&#21160;&#30830;&#23450;&#26032;&#26354;&#35889;&#30340;&#33410;&#25293;&#35760;&#21495;
&lt;/p&gt;
&lt;p&gt;
Automatic Time Signature Determination for New Scores Using Lyrics for Latent Rhythmic Structure. (arXiv:2311.15480v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.15480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#27468;&#35789;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#29983;&#25104;&#36866;&#21512;&#27468;&#35789;&#27468;&#26354;&#30340;&#33410;&#25293;&#35760;&#21495;&#65292;&#24182;&#25581;&#31034;&#28508;&#22312;&#30340;&#33410;&#22863;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;(AIGC)&#30340;&#20852;&#36259;&#24613;&#21095;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#23545;&#38899;&#20048;&#32452;&#25104;&#37096;&#20998;&#22914;&#33410;&#25293;&#35760;&#21495;&#36827;&#34892;&#36275;&#22815;&#30340;&#30740;&#31350;&#65292;&#20197;&#21046;&#23450;&#26032;&#20316;&#21697;&#30340;&#31639;&#27861;&#30830;&#23450;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#27468;&#35789;&#27468;&#26354;&#12290;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#24573;&#35270;&#20102;&#38899;&#20048;&#32454;&#33410;&#65292;&#32780;&#38899;&#20048;&#32454;&#33410;&#23545;&#20110;&#26500;&#24314;&#24378;&#22823;&#30340;&#26694;&#26550;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#33410;&#25293;&#35760;&#21495;&#20026;&#27468;&#26354;&#30340;&#20960;&#20046;&#25152;&#26377;&#26041;&#38754;(&#21253;&#25324;&#30701;&#35821;&#21644;&#38899;&#31526;)&#24314;&#31435;&#20102;&#22522;&#30784;&#30340;&#33410;&#22863;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#27468;&#35789;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#29983;&#25104;&#36866;&#21512;&#27468;&#35789;&#27468;&#26354;&#30340;&#33410;&#25293;&#35760;&#21495;&#65292;&#24182;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25581;&#31034;&#28508;&#22312;&#30340;&#33410;&#22863;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#31181;&#19982;&#21457;&#29616;&#27468;&#35789;&#27169;&#24335;&#21644;&#21019;&#24314;&#21516;&#26102;&#21253;&#21547;&#27468;&#35789;&#12289;&#33410;&#22863;&#21644;&#32479;&#35745;&#20449;&#24687;&#30340;&#26032;&#29305;&#24449;&#30456;&#20851;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
There has recently been a sharp increase in interest in Artificial Intelligence-Generated Content (AIGC). Despite this, musical components such as time signatures have not been studied sufficiently to form an algorithmic determination approach for new compositions, especially lyrical songs. This is likely because of the neglect of musical details, which is critical for constructing a robust framework. Specifically, time signatures establish the fundamental rhythmic structure for almost all aspects of a song, including the phrases and notes. In this paper, we propose a novel approach that only uses lyrics as input to automatically generate a fitting time signature for lyrical songs and uncover the latent rhythmic structure utilizing explainable machine learning models. In particular, we devise multiple methods that are associated with discovering lyrical patterns and creating new features that simultaneously contain lyrical, rhythmic, and statistical information. In this approach, the b
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;PRGD&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;&#20998;&#31867;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#26102;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#24555;&#36895;&#36793;&#30028;&#26368;&#22823;&#21270;&#65292;&#19982;&#29616;&#26377;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.14387</link><description>&lt;p&gt;
&#36890;&#36807;&#28176;&#36827;&#33539;&#25968;&#37325;&#26032;&#32553;&#25918;&#23454;&#29616;&#25351;&#25968;&#32423;&#24555;&#36895;&#36793;&#30028;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling. (arXiv:2311.14387v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14387
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;PRGD&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;&#20998;&#31867;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#26102;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#24555;&#36895;&#36793;&#30028;&#26368;&#22823;&#21270;&#65292;&#19982;&#29616;&#26377;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#22312;&#20998;&#31867;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#30340;&#36793;&#30028;&#26368;&#22823;&#21270;&#20559;&#24046;&#12290;&#25105;&#20204;&#23545;&#19982;&#65288;&#24402;&#19968;&#21270;&#30340;&#65289;&#26799;&#24230;&#30456;&#20851;&#30340;&#36895;&#24230;&#22330;&#30340;&#29305;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#22312;&#36793;&#30028;&#26368;&#22823;&#21270;&#20013;&#30340;&#20316;&#29992;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28176;&#36827;&#37325;&#26032;&#32553;&#25918;&#26799;&#24230;&#19979;&#38477;&#65288;PRGD&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;PRGD&#21487;&#20197;&#20197;&#25351;&#25968;&#32423;&#24555;&#36895;&#22686;&#22823;&#36793;&#30028;&#12290;&#36825;&#19982;&#30446;&#21069;&#25152;&#26377;&#29616;&#26377;&#31639;&#27861;&#24418;&#25104;&#20102;&#40092;&#26126;&#23545;&#27604;&#65292;&#21518;&#32773;&#20197;&#32531;&#24930;&#30340;&#22810;&#39033;&#24335;&#36895;&#29575;&#26368;&#22823;&#21270;&#36793;&#30028;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#28201;&#21644;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#20687;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#21644;&#24402;&#19968;&#21270;&#26799;&#24230;&#19979;&#38477;&#65288;NGD&#65289;&#36825;&#26679;&#30340;&#29616;&#26377;&#31639;&#27861;&#22312;&#39640;&#25928;&#26368;&#22823;&#21270;&#36793;&#30028;&#26102;&#20250;&#20986;&#29616;&#22833;&#36133;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;PRGD&#22312;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the margin-maximization bias exhibited by gradient-based algorithms in classifying linearly separable data. We present an in-depth analysis of the specific properties of the velocity field associated with (normalized) gradients, focusing on their role in margin maximization. Inspired by this analysis, we propose a novel algorithm called Progressive Rescaling Gradient Descent (PRGD) and show that PRGD can maximize the margin at an {\em exponential rate}. This stands in stark contrast to all existing algorithms, which maximize the margin at a slow {\em polynomial rate}. Specifically, we identify mild conditions on data distribution under which existing algorithms such as gradient descent (GD) and normalized gradient descent (NGD) {\em provably fail} in maximizing the margin efficiently. To validate our theoretical findings, we present both synthetic and real-world experiments. Notably, PRGD also shows promise in enhancing the generalization performance when a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#20307;&#29305;&#24449;&#65292;&#22312;&#20998;&#37197;&#32593;&#32476;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20998;&#37197;&#25991;&#26412;&#20013;&#36827;&#34892;&#21305;&#37197;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#30005;&#21147;&#20998;&#37197;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#20013;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.08724</link><description>&lt;p&gt;
&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Construction in Power Distribution Networks. (arXiv:2311.08724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#20307;&#29305;&#24449;&#65292;&#22312;&#20998;&#37197;&#32593;&#32476;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20998;&#37197;&#25991;&#26412;&#20013;&#36827;&#34892;&#21305;&#37197;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#30005;&#21147;&#20998;&#37197;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#20013;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#20307;&#29305;&#24449;&#65292;&#21253;&#25324;&#20854;&#35821;&#20041;&#12289;&#38899;&#38901;&#21644;&#21477;&#27861;&#29305;&#24449;&#65292;&#22312;&#20998;&#37197;&#32593;&#32476;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20998;&#37197;&#25991;&#26412;&#20013;&#36827;&#34892;&#21305;&#37197;&#12290;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22686;&#24378;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#23558;&#20998;&#37197;&#25991;&#26412;&#23454;&#20307;&#19982;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#21305;&#37197;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#30005;&#21147;&#20998;&#37197;&#22330;&#26223;&#20013;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38142;&#25509;&#21508;&#31181;&#23454;&#20307;&#31867;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#30005;&#21147;&#20998;&#37197;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a method for knowledge graph construction in power distribution networks. This method leverages entity features, which involve their semantic, phonetic, and syntactic characteristics, in both the knowledge graph of distribution network and the dispatching texts. An enhanced model based on Convolutional Neural Network, is utilized for effectively matching dispatch text entities with those in the knowledge graph. The effectiveness of this model is evaluated through experiments in real-world power distribution dispatch scenarios. The results indicate that, compared with the baselines, the proposed model excels in linking a variety of entity types, demonstrating high overall accuracy in power distribution knowledge graph construction task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#65292;&#21033;&#29992;&#28508;&#22312;&#26102;&#38388;&#36807;&#31243;&#26469;&#27169;&#25311;&#22797;&#26434;&#30142;&#30149;&#36712;&#36857;&#12290;&#36890;&#36807;&#32467;&#21512;&#21307;&#23398;&#30693;&#35782;&#21644;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#21644;&#20840;&#38754;&#20998;&#26512;&#30142;&#30149;&#36712;&#36857;&#65292;&#24182;&#29992;&#20110;&#36827;&#19968;&#27493;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#20020;&#24202;&#20551;&#35774;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2311.08149</link><description>&lt;p&gt;
&#20351;&#29992;&#21547;&#26377;&#21322;&#30417;&#30563;&#28508;&#22312;&#36807;&#31243;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24314;&#27169;&#22797;&#26434;&#30142;&#30149;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Modeling Complex Disease Trajectories using Deep Generative Models with Semi-Supervised Latent Processes. (arXiv:2311.08149v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#65292;&#21033;&#29992;&#28508;&#22312;&#26102;&#38388;&#36807;&#31243;&#26469;&#27169;&#25311;&#22797;&#26434;&#30142;&#30149;&#36712;&#36857;&#12290;&#36890;&#36807;&#32467;&#21512;&#21307;&#23398;&#30693;&#35782;&#21644;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#21644;&#20840;&#38754;&#20998;&#26512;&#30142;&#30149;&#36712;&#36857;&#65292;&#24182;&#29992;&#20110;&#36827;&#19968;&#27493;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#20020;&#24202;&#20551;&#35774;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#26102;&#38388;&#36807;&#31243;&#30340;&#28145;&#24230;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#21644;&#20840;&#38754;&#20998;&#26512;&#22797;&#26434;&#30142;&#30149;&#36712;&#36857;&#12290;&#25105;&#20204;&#26088;&#22312;&#25214;&#21040;&#33021;&#22815;&#35299;&#37322;&#35266;&#23519;&#21040;&#30340;&#30142;&#30149;&#36712;&#36857;&#30340;&#26377;&#24847;&#20041;&#30340;&#26102;&#38388;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20197;&#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#20840;&#38754;&#30340;&#26041;&#24335;&#36827;&#34892;&#20998;&#26512;&#12290;&#20026;&#20102;&#25552;&#39640;&#36825;&#20123;&#26102;&#38388;&#28508;&#22312;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#21307;&#23398;&#27010;&#24565;&#23545;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#35299;&#32544;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#26041;&#27861;&#19982;&#21307;&#23398;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#21457;&#29616;&#30142;&#30149;&#26032;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#23558;&#21307;&#23398;&#27010;&#24565;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#24471;&#30340;&#26102;&#38388;&#28508;&#22312;&#36807;&#31243;&#21487;&#20197;&#29992;&#20110;&#36827;&#19968;&#27493;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#20020;&#24202;&#20551;&#35774;&#27979;&#35797;&#65292;&#21253;&#25324;&#26597;&#25214;&#30456;&#20284;&#24739;&#32773;&#21644;&#23558;&#30142;&#30149;&#32858;&#31867;&#20026;&#26032;&#30340;&#20122;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#22312;&#32447;&#30417;&#27979;&#21644;&#39044;&#27979;&#22810;&#21464;&#37327;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a deep generative time series approach using latent temporal processes for modeling and holistically analyzing complex disease trajectories. We aim to find meaningful temporal latent representations of an underlying generative process that explain the observed disease trajectories in an interpretable and comprehensive way. To enhance the interpretability of these latent temporal processes, we develop a semi-supervised approach for disentangling the latent space using established medical concepts. By combining the generative approach with medical knowledge, we leverage the ability to discover novel aspects of the disease while integrating medical concepts into the model. We show that the learned temporal latent processes can be utilized for further data analysis and clinical hypothesis testing, including finding similar patients and clustering the disease into new sub-types. Moreover, our method enables personalized online monitoring and prediction of multivari
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36890;&#20449;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#30693;&#35782;&#33976;&#39311;&#65288;CC-BAKD&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#28151;&#21512;&#26426;&#21046;&#23558;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#19982;&#21387;&#32553;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22312;&#23398;&#20064;&#32773;&#19982;&#25945;&#24072;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#26102;&#20851;&#20110;&#25209;&#27425;&#36873;&#25321;&#21644;&#25209;&#27425;&#32534;&#30721;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.08053</link><description>&lt;p&gt;
&#36890;&#20449;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Communication-Constrained Bayesian Active Knowledge Distillation. (arXiv:2311.08053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36890;&#20449;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#30693;&#35782;&#33976;&#39311;&#65288;CC-BAKD&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#28151;&#21512;&#26426;&#21046;&#23558;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#19982;&#21387;&#32553;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22312;&#23398;&#20064;&#32773;&#19982;&#25945;&#24072;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#26102;&#20851;&#20110;&#25209;&#27425;&#36873;&#25321;&#21644;&#25209;&#27425;&#32534;&#30721;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#37325;&#20256;&#65288;ARQ&#65289;&#21327;&#35758;&#26088;&#22312;&#30830;&#20445;&#25509;&#25910;&#26041;&#27491;&#30830;&#25509;&#25910;&#21040;&#21457;&#23556;&#26041;&#30340;&#25152;&#26377;&#20998;&#32452;&#12290;&#24403;&#21457;&#23556;&#26041;&#26159;&#19968;&#20010;&#23398;&#20064;&#32773;&#19982;&#19968;&#20010;&#25945;&#24072;&#36827;&#34892;&#36890;&#20449;&#26102;&#65292;&#36825;&#20010;&#30446;&#26631;&#19982;&#23398;&#20064;&#32773;&#30340;&#23454;&#38469;&#30446;&#26631;&#30456;&#20914;&#31361;&#65292;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#20174;&#25945;&#24072;&#37027;&#37324;&#33719;&#21462;&#26368;&#30456;&#20851;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#20174;&#20027;&#21160;&#23398;&#20064;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#26412;&#25991;&#35299;&#20915;&#20197;&#19979;&#20851;&#38190;&#21327;&#35758;&#35774;&#35745;&#38382;&#39064;&#65306;(i)&#20027;&#21160;&#25209;&#27425;&#36873;&#25321;&#65306;&#24212;&#35813;&#21457;&#36865;&#21738;&#20010;&#25209;&#27425;&#30340;&#36755;&#20837;&#32473;&#25945;&#24072;&#20197;&#33719;&#21462;&#26368;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#23569;&#36890;&#20449;&#36718;&#27425;&#30340;&#25968;&#37327;&#65311;(ii)&#25209;&#27425;&#32534;&#30721;&#65306;&#26159;&#21542;&#21487;&#20197;&#32452;&#21512;&#25968;&#25454;&#28857;&#30340;&#25209;&#27425;&#20197;&#20943;&#23569;&#27599;&#20010;&#36890;&#20449;&#36718;&#27425;&#25152;&#38656;&#30340;&#36890;&#20449;&#36164;&#28304;&#65311;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#36890;&#20449;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#30693;&#35782;&#33976;&#39311;&#65288;CC-BAKD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#32447;&#24615;&#28151;&#21512;&#26426;&#21046;&#23558;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#19982;&#21387;&#32553;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional retransmission (ARQ) protocols are designed with the goal of ensuring the correct reception of all the individual transmitter's packets at the receiver. When the transmitter is a learner communicating with a teacher, this goal is at odds with the actual aim of the learner, which is that of eliciting the most relevant label information from the teacher. Taking an active learning perspective, this paper addresses the following key protocol design questions: (i) Active batch selection: Which batch of inputs should be sent to the teacher to acquire the most useful information and thus reduce the number of required communication rounds? (ii) Batch encoding: Can batches of data points be combined to reduce the communication resources required at each communication round? Specifically, this work introduces Communication-Constrained Bayesian Active Knowledge Distillation (CC-BAKD), a novel protocol that integrates Bayesian active learning with compression via a linear mix-up mecha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#30721;&#23545;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#32473;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#25552;&#20379;&#26377;&#38169;&#35823;&#21644;&#26080;&#38169;&#35823;&#29256;&#26412;&#30340;&#20195;&#30721;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#28431;&#27934;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#20219;&#21153;&#30456;&#27604;&#20110;&#32473;&#20986;&#20195;&#30721;&#29255;&#27573;&#24182;&#21028;&#26029;&#26159;&#21542;&#23384;&#22312;&#38169;&#35823;&#21450;&#20854;&#20301;&#32622;&#35201;&#23481;&#26131;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2311.07957</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20195;&#30721;&#23545;&#20998;&#31867;&#26356;&#22909;&#22320;&#26816;&#27979;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Language Models are Better Bug Detector Through Code-Pair Classification. (arXiv:2311.07957v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#30721;&#23545;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#32473;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#25552;&#20379;&#26377;&#38169;&#35823;&#21644;&#26080;&#38169;&#35823;&#29256;&#26412;&#30340;&#20195;&#30721;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#28431;&#27934;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#20219;&#21153;&#30456;&#27604;&#20110;&#32473;&#20986;&#20195;&#30721;&#29255;&#27573;&#24182;&#21028;&#26029;&#26159;&#21542;&#23384;&#22312;&#38169;&#35823;&#21450;&#20854;&#20301;&#32622;&#35201;&#23481;&#26131;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3.5&#21644;CodeLlama&#26159;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#21644;&#29702;&#35299;&#30340;&#24378;&#22823;&#27169;&#22411;&#12290;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#19988;&#38656;&#35201;&#19968;&#20010;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#25216;&#26415;&#20351;&#27169;&#22411;&#33021;&#22815;&#21482;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#26469;&#23398;&#20064;&#19979;&#28216;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#28431;&#27934;&#26816;&#27979;&#21644;&#20462;&#22797;&#26041;&#38754;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20195;&#30721;&#23545;&#20998;&#31867;&#20219;&#21153;&#65292;&#20854;&#20013;&#27169;&#22411;&#21516;&#26102;&#33719;&#21462;&#26377;&#38169;&#35823;&#21644;&#26080;&#38169;&#35823;&#29256;&#26412;&#30340;&#20195;&#30721;&#65292;&#24182;&#26631;&#35782;&#20986;&#26377;&#38169;&#35823;&#30340;&#29256;&#26412;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#28431;&#27934;&#26816;&#27979;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#26368;&#24378;&#22823;&#30340;LLMs&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#36890;&#24120;&#21487;&#20197;&#20174;&#20195;&#30721;&#30340;&#38750;&#38169;&#35823;&#29256;&#26412;&#20013;&#36873;&#25321;&#20986;&#38169;&#35823;&#29256;&#26412;&#65292;&#24182;&#19988;&#20195;&#30721;&#23545;&#20998;&#31867;&#20219;&#21153;&#30456;&#27604;&#20110;&#32473;&#20986;&#20195;&#30721;&#29255;&#27573;&#24182;&#20915;&#23450;&#26159;&#21542;&#23384;&#22312;&#38169;&#35823;&#21450;&#20854;&#20301;&#32622;&#35201;&#23481;&#26131;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-3.5 and CodeLlama are powerful models for code generation and understanding. Fine-tuning these models comes with a high computational cost and requires a large labeled dataset. Alternatively, in-context learning techniques allow models to learn downstream tasks with only a few examples. Recently, researchers have shown how in-context learning performs well in bug detection and repair. In this paper, we propose code-pair classification task in which both the buggy and non-buggy versions are given to the model, and the model identifies the buggy ones. We evaluate our task in real-world dataset of bug detection and two most powerful LLMs. Our experiments indicate that an LLM can often pick the buggy from the non-buggy version of the code, and the code-pair classification task is much easier compared to be given a snippet and deciding if and where a bug exists.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20840;&#38754;&#20998;&#26512;&#20102;&#20351;&#29992;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#29978;&#33267;&#21482;&#38656;&#26368;&#23567;&#30340;&#29305;&#24449;&#20540;&#20462;&#25913;&#12290;&#35813;&#25915;&#20987;&#36824;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2311.07550</link><description>&lt;p&gt;
Tabdoor&#65306;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#34920;&#26684;&#25968;&#25454;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#21518;&#38376;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data. (arXiv:2311.07550v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07550
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20840;&#38754;&#20998;&#26512;&#20102;&#20351;&#29992;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#29978;&#33267;&#21482;&#38656;&#26368;&#23567;&#30340;&#29305;&#24449;&#20540;&#20462;&#25913;&#12290;&#35813;&#25915;&#20987;&#36824;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#19982;&#36825;&#20123;&#21457;&#23637;&#21516;&#26102;&#65292;&#19982;DNN&#35757;&#32451;&#30456;&#20851;&#30340;&#28431;&#27934;&#65292;&#22914;&#21518;&#38376;&#25915;&#20987;&#65292;&#26159;&#19968;&#20010;&#37325;&#22823;&#20851;&#20999;&#12290;&#36825;&#20123;&#25915;&#20987;&#28041;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#24494;&#22937;&#22320;&#25554;&#20837;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#20801;&#35768;&#25805;&#32437;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;DNNs&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#20351;&#29992;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#29305;&#21035;&#20851;&#27880;&#36716;&#25442;&#22120;&#12290;&#37492;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#23884;&#20837;&#21518;&#38376;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#31995;&#32479;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#21363;&#20351;&#21482;&#26377;&#26368;&#23567;&#30340;&#29305;&#24449;&#20540;&#20462;&#25913;&#12290;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#65292;&#22914;XGBoost&#21644;DeepFM&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20960;&#20046;&#34920;&#26126;&#21518;&#38376;&#25915;&#20987;&#21487;&#20197;&#23436;&#32654;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) have shown great promise in various domains. Alongside these developments, vulnerabilities associated with DNN training, such as backdoor attacks, are a significant concern. These attacks involve the subtle insertion of triggers during model training, allowing for manipulated predictions.More recently, DNNs for tabular data have gained increasing attention due to the rise of transformer models.  Our research presents a comprehensive analysis of backdoor attacks on tabular data using DNNs, particularly focusing on transformers. Given the inherent complexities of tabular data, we explore the challenges of embedding backdoors. Through systematic experimentation across benchmark datasets, we uncover that transformer-based DNNs for tabular data are highly susceptible to backdoor attacks, even with minimal feature value alterations. We also verify that our attack can be generalized to other models, like XGBoost and DeepFM. Our results indicate nearly perfect attac
&lt;/p&gt;</description></item><item><title>GateLoop&#26159;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#24207;&#21015;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#32473;Attention&#12290;</title><link>http://arxiv.org/abs/2311.01927</link><description>&lt;p&gt;
GateLoop: &#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01927
&lt;/p&gt;
&lt;p&gt;
GateLoop&#26159;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#24207;&#21015;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#32473;Attention&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#36882;&#24402;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#24314;&#27169;&#38271;&#24207;&#21015;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#29616;&#26377;&#27169;&#22411;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#20854;&#28508;&#21147;&#12290;&#22312;&#36825;&#19968;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GateLoop&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#30784;&#24615;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#25511;&#21046;&#30340;&#29366;&#24577;&#36716;&#25442;&#26469;&#25512;&#24191;&#32447;&#24615;&#36882;&#24402;&#27169;&#22411;&#65292;&#22914;S4&#12289;S5&#12289;LRU&#21644;RetNet&#12290;&#21033;&#29992;&#36825;&#19968;&#29702;&#35770;&#36827;&#27493;&#65292;GateLoop&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#22312;&#23454;&#35777;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20302;&#25104;&#26412;&#30340;$O(l)$&#36882;&#24402;&#27169;&#24335;&#21644;&#39640;&#24230;&#20248;&#21270;&#30340;&#20851;&#32852;&#25195;&#25551;&#23454;&#29616;&#30340;&#39640;&#25928;$O(l \log_{2} l)$&#24182;&#34892;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;$O(l^2)$&#30340;&#20195;&#29702;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#23545;Transformer&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#26550;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#21521;Attention&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#12290;&#32780;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#26080;&#20851;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Recurrence has proven to be a powerful tool for modeling long sequences efficiently. In this work, we show that existing models fail to take full advantage of its potential. Motivated by this finding, we develop GateLoop, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet, by employing data-controlled state transitions. Utilizing this theoretical advance, GateLoop empirically outperforms existing models for auto-regressive language modeling. Our method comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$ parallel mode making use of highly optimized associative scan implementations. Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing remarkable implications for Transformer and recently proposed architectures. Specifically, we prove that our approach can be interpreted as providing data-controlled relative-positional information to Attention. While many existing models solely rely on da
&lt;/p&gt;</description></item><item><title>GOPlan&#26159;&#19968;&#20010;&#20351;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#20808;&#39564;&#31574;&#30053;&#21644;&#20351;&#29992;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#65292;&#29992;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.20025</link><description>&lt;p&gt;
GOPlan:&#36890;&#36807;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models. (arXiv:2310.20025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20025
&lt;/p&gt;
&lt;p&gt;
GOPlan&#26159;&#19968;&#20010;&#20351;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#20808;&#39564;&#31574;&#30053;&#21644;&#20351;&#29992;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#65292;&#29992;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;GCRL&#65289;&#20026;&#20174;&#22810;&#26679;&#21270;&#21644;&#22810;&#20219;&#21153;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#36890;&#29992;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#33539;&#20363;&#12290;&#23613;&#31649;&#36817;&#26399;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20027;&#23548;&#30340;&#31163;&#32447;GCRL&#26041;&#27861;&#20173;&#28982;&#21463;&#38480;&#20110;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#24212;&#23545;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;Goal-conditioned Offline Planning&#65288;GOPlan&#65289;&#65292;&#21253;&#25324;&#65288;1&#65289;&#39044;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#22810;&#30446;&#26631;&#25968;&#25454;&#38598;&#20013;&#22810;&#27169;&#24577;&#21160;&#20316;&#20998;&#24067;&#30340;&#20808;&#39564;&#31574;&#30053;&#65307;&#65288;2&#65289;&#21033;&#29992;&#35268;&#21010;&#30340;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#20026;&#24494;&#35843;&#31574;&#30053;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20808;&#39564;&#31574;&#30053;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;&#26126;&#26174;&#27169;&#24335;&#20998;&#31163;&#30340;&#24102;&#20248;&#21183;&#26435;&#37325;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#21160;&#20316;&#30340;&#32570;&#28857;&#12290;&#20026;&#36827;&#19968;&#27493;&#20248;&#21270;&#31574;&#30053;&#65292;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34394;&#26500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline goal-conditioned RL (GCRL) offers a feasible paradigm to learn general-purpose policies from diverse and multi-task offline datasets. Despite notable recent progress, the predominant offline GCRL methods have been restricted to model-free approaches, constraining their capacity to tackle limited data budgets and unseen goal generalization. In this work, we propose a novel two-stage model-based framework, Goal-conditioned Offline Planning (GOPlan), including (1) pretraining a prior policy capable of capturing multi-modal action distribution within the multi-goal dataset; (2) employing the reanalysis method with planning to generate imagined trajectories for funetuning policies. Specifically, the prior policy is based on an advantage-weighted Conditioned Generative Adversarial Networks that exhibits distinct mode separation to overcome the pitfalls of out-of-distribution (OOD) actions. For further policy optimization, the reanalysis method generates high-quality imaginary data by
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22768;&#38899;&#20998;&#31867;&#21644;&#23450;&#20301;&#32593;&#32476;&#20013;&#24341;&#20837;&#29305;&#24449;&#32858;&#21512;&#25216;&#26415;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#21306;&#20998;&#30452;&#25509;&#21644;&#38388;&#25509;&#22768;&#38899;&#20449;&#21495;&#30340;SSL&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2310.19063</link><description>&lt;p&gt;
&#21512;&#24182;&#29305;&#24449;&#22312;&#32852;&#21512;&#22768;&#38899;&#20998;&#31867;&#21644;&#23450;&#20301;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Feature Aggregation in Joint Sound Classification and Localization Neural Networks. (arXiv:2310.19063v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22768;&#38899;&#20998;&#31867;&#21644;&#23450;&#20301;&#32593;&#32476;&#20013;&#24341;&#20837;&#29305;&#24449;&#32858;&#21512;&#25216;&#26415;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#21306;&#20998;&#30452;&#25509;&#21644;&#38388;&#25509;&#22768;&#38899;&#20449;&#21495;&#30340;SSL&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#32852;&#21512;&#22768;&#38899;&#20449;&#21495;&#20998;&#31867;&#21644;&#23450;&#20301;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#22768;&#28304;&#23450;&#20301;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#22312;&#20854;&#26550;&#26500;&#20013;&#32570;&#20047;&#29305;&#24449;&#32858;&#21512;&#12290;&#29305;&#24449;&#32858;&#21512;&#36890;&#36807;&#20351;&#26469;&#33258;&#19981;&#21516;&#29305;&#24449;&#23610;&#24230;&#30340;&#20449;&#24687;&#25972;&#21512;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20174;&#32780;&#25913;&#21892;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#21644;&#19981;&#21464;&#24615;&#12290;&#36825;&#22312;SSL&#32593;&#32476;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;SSL&#32593;&#32476;&#24517;&#39035;&#21306;&#20998;&#30452;&#25509;&#21644;&#38388;&#25509;&#22768;&#38899;&#20449;&#21495;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#31070;&#32463;&#32593;&#32476;&#20013;&#20511;&#37492;&#20102;&#29305;&#24449;&#32858;&#21512;&#25216;&#26415;&#65292;&#23558;&#20854;&#24212;&#29992;&#22312;&#20449;&#21495;&#26816;&#27979;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23610;&#24230;&#32534;&#30721;&#32593;&#32476;&#65288;SEN&#65289;&#29992;&#20110;&#29305;&#24449;&#32858;&#21512;&#65292;&#20197;&#32534;&#30721;&#26469;&#33258;&#19981;&#21516;&#23610;&#24230;&#30340;&#29305;&#24449;&#65292;&#21387;&#32553;&#32593;&#32476;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#32858;&#21512;&#12290;&#20026;&#20102;&#35780;&#20272;&#29305;&#24449;&#32858;&#21512;&#22312;SSL&#32593;&#32476;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#38598;&#25104;&#20102;&#20197;&#19979;&#35745;&#31639;&#26426;&#35270;&#35273;&#29305;&#24449;&#32858;&#21512;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study addresses the application of deep learning techniques in joint sound signal classification and localization networks. Current state-of-the-art sound source localization deep learning networks lack feature aggregation within their architecture. Feature aggregation enhances model performance by enabling the consolidation of information from different feature scales, thereby improving feature robustness and invariance. This is particularly important in SSL networks, which must differentiate direct and indirect acoustic signals. To address this gap, we adapt feature aggregation techniques from computer vision neural networks to signal detection neural networks. Additionally, we propose the Scale Encoding Network (SEN) for feature aggregation to encode features from various scales, compressing the network for more computationally efficient aggregation. To evaluate the efficacy of feature aggregation in SSL networks, we integrated the following computer vision feature aggregation 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#29305;&#23450;&#23454;&#20363;&#19978;&#36827;&#34892;&#39640;&#25928;&#24494;&#35843;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#22810;&#34892;&#20026;&#35774;&#32622;&#21644;&#31163;&#25955;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.14526</link><description>&lt;p&gt;
&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#23454;&#29616;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Zero Shot Learning in Restless Multi-armed Bandits. (arXiv:2310.14526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14526
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#29305;&#23450;&#23454;&#20363;&#19978;&#36827;&#34892;&#39640;&#25928;&#24494;&#35843;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#22810;&#34892;&#20026;&#35774;&#32622;&#21644;&#31163;&#25955;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#19968;&#31867;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#8212;&#8212;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;RMABs&#65289;&#65292;&#35813;&#38382;&#39064;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#22312;&#32447;&#24191;&#21578;&#21644;&#21453;&#30423;&#29454;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#20808;&#21069;&#30340;RMAB&#30740;&#31350;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#36830;&#32493;&#29366;&#24577;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#24403;&#36172;&#21338;&#26426;&#30340;&#20837;&#36873;&#21644;&#36864;&#20986;&#19981;&#26029;&#21457;&#29983;&#26102;&#65292;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PreFeRMAB&#65289;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#23545;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#24191;&#27867;RMAB&#38382;&#39064;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#27604;&#20174;&#22836;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#22320;&#23545;&#29305;&#23450;&#23454;&#20363;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#22810;&#34892;&#20026;&#35774;&#32622;&#21644;&#31163;&#25955;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#12290;&#20026;&#20102;&#23454;&#29616;&#24555;&#36895;&#27867;&#21270;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#19968;&#31574;&#30053;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#29305;&#24449;&#20449;&#24687;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restless multi-arm bandits (RMABs), a class of resource allocation problems with broad application in areas such as healthcare, online advertising, and anti-poaching, have recently been studied from a multi-agent reinforcement learning perspective. Prior RMAB research suffers from several limitations, e.g., it fails to adequately address continuous states, and requires retraining from scratch when arms opt-in and opt-out over time, a common challenge in many real world applications. We address these limitations by developing a neural network-based pre-trained model (PreFeRMAB) that has general zero-shot ability on a wide range of previously unseen RMABs, and which can be fine-tuned on specific instances in a more sample-efficient way than retraining from scratch. Our model also accommodates general multi-action settings and discrete or continuous state spaces. To enable fast generalization, we learn a novel single policy network model that utilizes feature information and employs a tra
&lt;/p&gt;</description></item><item><title>GraphMaker&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.13833</link><description>&lt;p&gt;
GraphMaker: &#25193;&#25955;&#27169;&#22411;&#33021;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?. (arXiv:2310.13833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13833
&lt;/p&gt;
&lt;p&gt;
GraphMaker&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20855;&#26377;&#33410;&#28857;&#23646;&#24615;&#30340;&#22823;&#35268;&#27169;&#22270;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#21019;&#24314;&#19982;&#30495;&#23454;&#19990;&#30028;&#31034;&#20363;&#31867;&#20284;&#30340;&#21512;&#25104;&#12289;&#23500;&#23646;&#24615;&#22270;&#23545;&#20110;&#20849;&#20139;&#22270;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#21644;&#24320;&#21457;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#24403;&#21407;&#22987;&#25968;&#25454;&#38480;&#21046;&#34987;&#20849;&#20139;&#26102;&#12290;&#20256;&#32479;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#20123;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26368;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#27809;&#26377;&#23646;&#24615;&#21644;&#36739;&#23567;&#30340;&#20998;&#23376;&#22270;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#26041;&#38754;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#22797;&#26434;&#30340;&#23646;&#24615;-&#32467;&#26500;&#30456;&#20851;&#24615;&#21644;&#22270;&#30340;&#22823;&#35268;&#27169;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#65306;GraphMaker&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#32467;&#26500;&#29983;&#25104;&#36807;&#31243;&#30340;&#32452;&#21512;&#65292;&#21457;&#29616;&#24322;&#27493;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#20869;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale graphs with node attributes are increasingly common in various real-world applications. Creating synthetic, attribute-rich graphs that mirror real-world examples is crucial, especially for sharing graph data for analysis and developing learning models when original data is restricted to be shared. Traditional graph generation methods are limited in their capacity to handle these complex structures. Recent advances in diffusion models have shown potential in generating graph structures without attributes and smaller molecular graphs. However, these models face challenges in generating large attributed graphs due to the complex attribute-structure correlations and the large size of these graphs. This paper introduces a novel diffusion model, GraphMaker, specifically designed for generating large attributed graphs. We explore various combinations of node attribute and graph structure generation processes, finding that an asynchronous approach more effectively captures the intr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28151;&#21512;&#27169;&#22411;&#22522;&#20110;&#21518;&#32487;&#29305;&#24449;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#19981;&#21516;&#36716;&#31227;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30693;&#35782;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2310.10818</link><description>&lt;p&gt;
&#21033;&#29992;&#28151;&#21512;&#27169;&#22411;&#22522;&#20110;&#21518;&#32487;&#29305;&#24449;&#24378;&#21270;&#23398;&#20064;&#36328;&#20219;&#21153;&#20256;&#36882;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning. (arXiv:2310.10818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28151;&#21512;&#27169;&#22411;&#22522;&#20110;&#21518;&#32487;&#29305;&#24449;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#19981;&#21516;&#36716;&#31227;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30693;&#35782;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22797;&#26434;&#21644;&#22823;&#35268;&#27169;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#26679;&#26412;&#25928;&#29575;&#23545;&#20110;&#24320;&#21457;&#23454;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#23558;&#26469;&#33258;&#20808;&#21069;&#32463;&#39564;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#27867;&#21270;&#21040;&#19979;&#28216;&#20219;&#21153;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21518;&#32487;&#29305;&#24449;&#65288;SF&#65289;RL&#31639;&#27861;&#33021;&#22815;&#22312;&#20855;&#26377;&#19981;&#21516;&#22870;&#21169;&#20294;&#30456;&#21516;&#36716;&#31227;&#21160;&#21147;&#23398;&#30340;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#30693;&#35782;&#27867;&#21270;&#12290;&#26368;&#36817;&#25552;&#20986;&#32467;&#21512;&#27169;&#22411;&#22522;&#20110;&#65288;MB&#65289;&#26041;&#27861;&#21644;SF&#31639;&#27861;&#21487;&#20197;&#32531;&#35299;&#22266;&#23450;&#36716;&#31227;&#21160;&#21147;&#23398;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#25506;&#32034;&#26041;&#27861;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21478;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#12290;&#23558;&#28151;&#21512;&#27169;&#22411;&#22522;&#20110;&#21518;&#32487;&#29305;&#24449;&#65288;MB-SF&#65289;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#20004;&#20010;&#24605;&#24819;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36328;&#20219;&#21153;&#26679;&#26412;&#39640;&#25928;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30693;&#35782;&#20256;&#36882;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample efficiency is central to developing practical reinforcement learning (RL) for complex and large-scale decision-making problems. The ability to transfer and generalize knowledge gained from previous experiences to downstream tasks can significantly improve sample efficiency. Recent research indicates that successor feature (SF) RL algorithms enable knowledge generalization between tasks with different rewards but identical transition dynamics. It has recently been hypothesized that combining model-based (MB) methods with SF algorithms can alleviate the limitation of fixed transition dynamics. Furthermore, uncertainty-aware exploration is widely recognized as another appealing approach for improving sample efficiency. Putting together two ideas of hybrid model-based successor feature (MB-SF) and uncertainty leads to an approach to the problem of sample efficient uncertainty-aware knowledge transfer across tasks with different transition dynamics or/and reward functions. In this pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#35782;&#21035;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#20026;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20005;&#35880;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10705</link><description>&lt;p&gt;
&#29992;&#20110;&#35782;&#21035;&#21322;&#23548;&#20307;&#26230;&#22278;&#22320;&#22270;&#20013;&#32570;&#38519;&#27169;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65306;&#19968;&#39033;&#35843;&#26597;&#12289;&#23454;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations. (arXiv:2310.10705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#35782;&#21035;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#20026;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20005;&#35880;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#35782;&#21035;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#23398;&#12290;&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;ML&#22312;&#26230;&#22278;&#32570;&#38519;&#35782;&#21035;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#32570;&#20047;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#26412;&#25991;&#35797;&#22270;&#24357;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#36890;&#36807;&#25972;&#21512;&#29616;&#26377;&#25991;&#29486;&#65292;&#28145;&#20837;&#20998;&#26512;&#21508;&#31181;ML&#31639;&#27861;&#22312;&#26230;&#22278;&#32570;&#38519;&#26816;&#27979;&#39046;&#22495;&#30340;&#20248;&#21183;&#12289;&#23616;&#38480;&#24615;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#23398;&#20998;&#31867;&#20307;&#31995;&#65292;&#35814;&#32454;&#20998;&#31867;&#20102;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#30340;&#23376;&#25216;&#26415;&#21010;&#20998;&#12290;&#36825;&#20010;&#20998;&#31867;&#20307;&#31995;&#20174;&#24191;&#27867;&#30340;&#26041;&#27861;&#23398;&#31867;&#21035;&#24320;&#22987;&#65292;&#21040;&#20855;&#20307;&#30340;&#23376;&#25216;&#26415;&#32467;&#26463;&#12290;&#23427;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#19981;&#21516;&#31639;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#25216;&#26415;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#37319;&#29992;&#20005;&#35880;&#30340;&#23454;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;&#26469;&#39564;&#35777;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper offers a comprehensive review of methodologies utilizing machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing. Despite the growing body of research demonstrating the effectiveness of ML in wafer defect identification, there is a noticeable absence of comprehensive reviews on this subject. This survey attempts to fill this void by amalgamating available literature and providing an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in the realm of wafer defect detection. An innovative taxonomy of methodologies that we present provides a detailed classification of algorithms into more refined categories and techniques. This taxonomy follows a four-tier structure, starting from broad methodology categories and ending with specific sub-techniques. It aids researchers in comprehending the complex relationships between different algorithms and their techniques. We employ a rigorous em
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25286;&#20998;&#23398;&#20064;&#30340;&#34987;&#21160;&#25512;&#29702;&#25915;&#20987;&#26694;&#26550;SDAR&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#26469;&#25512;&#26029;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#29305;&#24449;&#21644;&#26631;&#31614;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#20027;&#21160;&#25915;&#20987;&#30456;&#24403;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10483</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#23545;&#25286;&#20998;&#23398;&#20064;&#36827;&#34892;&#34989;&#20987;&#30340;&#34987;&#21160;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Passive Inference Attacks on Split Learning via Adversarial Regularization. (arXiv:2310.10483v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25286;&#20998;&#23398;&#20064;&#30340;&#34987;&#21160;&#25512;&#29702;&#25915;&#20987;&#26694;&#26550;SDAR&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#26469;&#25512;&#26029;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#29305;&#24449;&#21644;&#26631;&#31614;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#20027;&#21160;&#25915;&#20987;&#30456;&#24403;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25286;&#20998;&#23398;&#20064;(SL)&#24050;&#25104;&#20026;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#31181;&#23454;&#29992;&#19988;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#34429;&#28982;&#20197;&#21069;&#25915;&#20987;SL&#30340;&#23581;&#35797;&#24448;&#24448;&#20381;&#36182;&#20110;&#36807;&#20110;&#24378;&#30828;&#30340;&#20551;&#35774;&#25110;&#32773;&#38024;&#23545;&#26131;&#21463;&#25915;&#20987;&#30340;&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#35797;&#22270;&#24320;&#21457;&#26356;&#21152;&#23454;&#29992;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SDAR&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#25317;&#26377;&#35802;&#23454;&#20294;&#22909;&#22855;&#30340;&#26381;&#21153;&#22120;&#30340;SL&#30340;&#26032;&#25915;&#20987;&#26694;&#26550;&#12290;SDAR&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#23458;&#25143;&#31471;&#31169;&#26377;&#27169;&#22411;&#30340;&#21487;&#35299;&#30721;&#27169;&#25311;&#22120;&#65292;&#22312;&#22522;&#26412;SL&#19979;&#21487;&#20197;&#26377;&#25928;&#22320;&#25512;&#26029;&#20986;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#29305;&#24449;&#65292;&#24182;&#22312;U&#22411;SL&#19979;&#25512;&#26029;&#20986;&#29305;&#24449;&#21644;&#26631;&#31614;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#23454;&#38469;&#30340;&#22330;&#26223;&#20013;&#65292;&#29616;&#26377;&#30340;&#34987;&#21160;&#25915;&#20987;&#38590;&#20197;&#26377;&#25928;&#22320;&#37325;&#24314;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#25968;&#25454;&#26102;&#65292;SDAR&#22987;&#32456;&#23454;&#29616;&#20102;&#19982;&#20027;&#21160;&#25915;&#20987;&#30456;&#24403;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;&#22312;CIFAR-10&#19978;&#65292;&#22312;&#28145;&#24230;&#25286;&#20998;&#27700;&#24179;&#20026;7&#30340;&#24773;&#20917;&#19979;&#65292;SDAR&#36798;&#21040;&#20102;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split Learning (SL) has emerged as a practical and efficient alternative to traditional federated learning. While previous attempts to attack SL have often relied on overly strong assumptions or targeted easily exploitable models, we seek to develop more practical attacks. We introduce SDAR, a novel attack framework against SL with an honest-but-curious server. SDAR leverages auxiliary data and adversarial regularization to learn a decodable simulator of the client's private model, which can effectively infer the client's private features under the vanilla SL, and both features and labels under the U-shaped SL. We perform extensive experiments in both configurations to validate the effectiveness of our proposed attacks. Notably, in challenging but practical scenarios where existing passive attacks struggle to reconstruct the client's private data effectively, SDAR consistently achieves attack performance comparable to active attacks. On CIFAR-10, at the deep split level of 7, SDAR achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;S&#24418;&#29942;&#39048;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#24341;&#20837;&#31163;&#25955;&#20613;&#31435;&#21494;&#21464;&#25442;&#65288;DFT&#65289;&#36755;&#20986;&#23618;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#20855;&#26377;&#31232;&#30095;&#26631;&#31614;&#32452;&#21512;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.10443</link><description>&lt;p&gt;
&#39535;&#26381;S&#24418;&#29942;&#39048;&#65306;&#21487;&#35777;&#26126;Argmaxable&#30340;&#31232;&#30095;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Taming the Sigmoid Bottleneck: Provably Argmaxable Sparse Multi-Label Classification. (arXiv:2310.10443v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;S&#24418;&#29942;&#39048;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#24341;&#20837;&#31163;&#25955;&#20613;&#31435;&#21494;&#21464;&#25442;&#65288;DFT&#65289;&#36755;&#20986;&#23618;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#20855;&#26377;&#31232;&#30095;&#26631;&#31614;&#32452;&#21512;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
S&#24418;&#36755;&#20986;&#23618;&#24191;&#27867;&#29992;&#20110;&#22810;&#26631;&#31614;&#20998;&#31867;&#65288;MLC&#65289;&#20219;&#21153;&#20013;&#65292;&#20854;&#20013;&#27599;&#20010;&#36755;&#20837;&#21487;&#20197;&#34987;&#20998;&#37197;&#22810;&#20010;&#26631;&#31614;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;MLC&#20219;&#21153;&#20013;&#65292;&#21487;&#33021;&#30340;&#26631;&#31614;&#25968;&#37327;&#22312;&#25104;&#21315;&#19978;&#19975;&#65292;&#24448;&#24448;&#36229;&#36807;&#36755;&#20837;&#29305;&#24449;&#30340;&#25968;&#37327;&#65292;&#23548;&#33268;&#20302;&#31209;&#36755;&#20986;&#23618;&#12290;&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#65292;&#24050;&#30693;&#36825;&#26679;&#30340;&#20302;&#31209;&#36755;&#20986;&#23618;&#26159;&#19968;&#20010;&#29942;&#39048;&#65292;&#21487;&#33021;&#23548;&#33268;&#26080;&#27861;Argmax&#30340;&#31867;&#21035;&#65306;&#21363;&#20219;&#20309;&#36755;&#20837;&#37117;&#26080;&#27861;&#39044;&#27979;&#30340;&#31867;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#23545;&#20110;MLC&#20219;&#21153;&#65292;&#31867;&#20284;&#30340;S&#24418;&#29942;&#39048;&#20250;&#23548;&#33268;&#25351;&#25968;&#32423;&#25968;&#37327;&#30340;&#26080;&#27861;Argmax&#30340;&#26631;&#31614;&#32452;&#21512;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#26816;&#27979;&#36825;&#20123;&#26080;&#27861;Argmax&#30340;&#36755;&#20986;&#65292;&#24182;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;MLC&#25968;&#25454;&#38598;&#20013;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#23384;&#22312;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#31163;&#25955;&#20613;&#31435;&#21494;&#21464;&#25442;&#65288;DFT&#65289;&#36755;&#20986;&#23618;&#22312;&#23454;&#36341;&#20013;&#38450;&#27490;&#65292;&#35813;&#36755;&#20986;&#23618;&#20445;&#35777;&#25152;&#26377;&#31232;&#30095;&#30340;&#24102;&#26377;&#26368;&#22810;k&#20010;&#27963;&#36291;&#26631;&#31614;&#30340;&#26631;&#31614;&#32452;&#21512;&#37117;&#26159;&#21487;Argmax&#30340;&#12290;&#25105;&#20204;&#30340;DFT&#23618;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#65292;&#26356;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sigmoid output layers are widely used in multi-label classification (MLC) tasks, in which multiple labels can be assigned to any input. In many practical MLC tasks, the number of possible labels is in the thousands, often exceeding the number of input features and resulting in a low-rank output layer. In multi-class classification, it is known that such a low-rank output layer is a bottleneck that can result in unargmaxable classes: classes which cannot be predicted for any input. In this paper, we show that for MLC tasks, the analogous sigmoid bottleneck results in exponentially many unargmaxable label combinations. We explain how to detect these unargmaxable outputs and demonstrate their presence in three widely used MLC datasets. We then show that they can be prevented in practice by introducing a Discrete Fourier Transform (DFT) output layer, which guarantees that all sparse label combinations with up to $k$ active labels are argmaxable. Our DFT layer trains faster and is more para
&lt;/p&gt;</description></item><item><title>Observatory&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#26694;&#26550;&#26469;&#20998;&#26512;&#20851;&#31995;&#34920;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.07736</link><description>&lt;p&gt;
Observatory: &#21051;&#30011;&#20851;&#31995;&#34920;&#23884;&#20837;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Observatory: Characterizing Embeddings of Relational Tables. (arXiv:2310.07736v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07736
&lt;/p&gt;
&lt;p&gt;
Observatory&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#26694;&#26550;&#26469;&#20998;&#26512;&#20851;&#31995;&#34920;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#38376;&#30340;&#34920;&#23884;&#20837;&#27169;&#22411;&#22312;&#35768;&#22810;&#34920;&#26684;&#25968;&#25454;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#37117;&#28212;&#26395;&#22312;&#35768;&#22810;&#26032;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65307;&#20294;&#26159;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#20197;&#21450;&#23427;&#20204;&#29983;&#25104;&#30340;&#34920;&#26684;&#34920;&#31034;&#30340;&#29702;&#35299;&#26377;&#38480;&#65292;&#23548;&#33268;&#22312;&#23547;&#25214;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#30340;&#36807;&#31243;&#20013;&#20381;&#36182;&#20110;&#35797;&#38169;&#12290;&#36843;&#20999;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#20943;&#23569;&#19979;&#28216;&#20351;&#29992;&#20013;&#30340;&#20302;&#25928;&#29575;&#21644;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Observatory&#30340;&#27491;&#24335;&#26694;&#26550;&#65292;&#20197;&#31995;&#32479;&#22320;&#20998;&#26512;&#20851;&#31995;&#34920;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;&#22312;&#20851;&#31995;&#25968;&#25454;&#27169;&#22411;&#30340;&#19981;&#21464;&#24615;&#21644;&#20851;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#32479;&#35745;&#32771;&#34385;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20843;&#20010;&#21407;&#22987;&#23646;&#24615;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#24230;&#37327;&#26469;&#23450;&#37327;&#22320;&#21051;&#30011;&#36825;&#20123;&#23646;&#24615;&#30340;&#34920;&#26684;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models and specialized table embedding models have recently demonstrated strong performance on many tasks over tabular data. Researchers and practitioners are keen to leverage these models in many new application contexts; but limited understanding of the strengths and weaknesses of these models, and the table representations they generate, makes the process of finding a suitable model for a given task reliant on trial and error. There is an urgent need to gain a comprehensive understanding of these models to minimize inefficiency and failures in downstream usage.  To address this need, we propose Observatory, a formal framework to systematically analyze embedding representations of relational tables. Motivated both by invariants of the relational data model and by statistical considerations regarding data distributions, we define eight primitive properties, and corresponding measures to quantitatively characterize table embeddings for these properties. Based on these properti
&lt;/p&gt;</description></item><item><title>BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07276</link><description>&lt;p&gt;
BioT5&#65306;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#36328;&#27169;&#24577;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07276
&lt;/p&gt;
&lt;p&gt;
BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#29983;&#29289;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#23637;&#21033;&#29992;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#25972;&#21512;&#26469;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#29983;&#25104;&#26080;&#25928;&#30340;&#20998;&#23376;SMILES&#12289;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#21033;&#29992;&#19981;&#36275;&#20197;&#21450;&#23545;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#31561;&#37327;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;BioT5&#65292;&#23427;&#36890;&#36807;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#29983;&#29289;&#23398;&#20013;&#30340;&#36328;&#27169;&#24577;&#25972;&#21512;&#12290;BioT5&#21033;&#29992;SELFIES&#36827;&#34892;100%&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#24182;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#29983;&#29289;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#29289;&#23454;&#20307;&#21608;&#22260;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;BioT5&#21306;&#20998;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20449;&#24687;&#12290;&#22312;&#24494;&#35843;&#21518;&#65292;BioT5&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36807;&#21442;&#25968;&#21270;&#26684;&#23376;&#65292;&#21033;&#29992;&#26684;&#23376;&#20989;&#25968;&#26368;&#23567;&#21270;&#31639;&#27861;&#36827;&#34892;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#20197;&#20811;&#26381;&#26684;&#23376;&#25805;&#20316;&#22120;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19977;&#20010;&#28508;&#22312;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2310.06639</link><description>&lt;p&gt;
&#36890;&#36807;&#26684;&#23376;&#36807;&#21442;&#25968;&#21270;&#33539;&#24335;&#36827;&#34892;&#26684;&#23376;&#25805;&#20316;&#22120;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
The Lattice Overparametrization Paradigm for the Machine Learning of Lattice Operators. (arXiv:2310.06639v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36807;&#21442;&#25968;&#21270;&#26684;&#23376;&#65292;&#21033;&#29992;&#26684;&#23376;&#20989;&#25968;&#26368;&#23567;&#21270;&#31639;&#27861;&#36827;&#34892;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#20197;&#20811;&#26381;&#26684;&#23376;&#25805;&#20316;&#22120;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19977;&#20010;&#28508;&#22312;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26684;&#23376;&#25805;&#20316;&#22120;&#30340;&#26426;&#22120;&#23398;&#20064;&#23384;&#22312;&#19977;&#20010;&#28508;&#22312;&#29942;&#39048;&#12290;&#20174;&#32479;&#35745;&#35282;&#24230;&#26469;&#30475;&#65292;&#38656;&#35201;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;&#20808;&#39564;&#20449;&#24687;&#30340;&#21463;&#38480;&#25805;&#20316;&#22120;&#31867;&#65292;&#20855;&#26377;&#20302;&#20559;&#24046;&#21644;&#19982;&#26679;&#26412;&#22823;&#23567;&#30456;&#23545;&#36739;&#20302;&#30340;&#22797;&#26434;&#24615;&#12290;&#20174;&#35745;&#31639;&#35282;&#24230;&#26469;&#30475;&#65292;&#24212;&#35813;&#26377;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#22312;&#35813;&#31867;&#19978;&#26368;&#23567;&#21270;&#32463;&#39564;&#35823;&#24046;&#12290;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#38656;&#35201;&#25512;&#23548;&#20986;&#23398;&#20064;&#21040;&#30340;&#25805;&#20316;&#22120;&#30340;&#23646;&#24615;&#65292;&#20197;&#20415;&#20174;&#29702;&#35770;&#19978;&#29702;&#35299;&#20854;&#34892;&#20026;&#12290;&#32479;&#35745;&#29942;&#39048;&#21487;&#20197;&#36890;&#36807;&#26377;&#20851;&#26684;&#23376;&#25805;&#20316;&#22120;&#34920;&#31034;&#30340;&#20016;&#23500;&#25991;&#29486;&#20811;&#26381;&#65292;&#20294;&#27809;&#26377;&#36890;&#29992;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#20854;&#20013;&#36890;&#36807;&#26684;&#23376;&#20013;&#30340;&#20803;&#32032;&#36827;&#34892;&#36807;&#21442;&#25968;&#21270;&#65292;&#23558;&#26684;&#23376;&#20989;&#25968;&#26368;&#23567;&#21270;&#31639;&#27861;&#24212;&#29992;&#20110;&#23398;&#20064;&#12290;&#25105;&#20204;&#23558;&#38543;&#26426;&#26684;&#23376;&#19979;&#38477;&#31639;&#27861;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#25805;&#20316;&#22120;&#30340;&#21463;&#38480;&#31867;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The machine learning of lattice operators has three possible bottlenecks. From a statistical standpoint, it is necessary to design a constrained class of operators based on prior information with low bias, and low complexity relative to the sample size. From a computational perspective, there should be an efficient algorithm to minimize an empirical error over the class. From an understanding point of view, the properties of the learned operator need to be derived, so its behavior can be theoretically understood. The statistical bottleneck can be overcome due to the rich literature about the representation of lattice operators, but there is no general learning algorithm for them. In this paper, we discuss a learning paradigm in which, by overparametrizing a class via elements in a lattice, an algorithm for minimizing functions in a lattice is applied to learn. We present the stochastic lattice descent algorithm as a general algorithm to learn on constrained classes of operators as long
&lt;/p&gt;</description></item><item><title>LARA&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#25239;&#36807;&#25311;&#21512;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20877;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#23558;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20984;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21453;&#24605;&#27169;&#22359;&#20197;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#65292;&#21516;&#26102;&#25968;&#23398;&#35777;&#26126;&#20102;&#22312;&#24494;&#35843;&#21518;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05668</link><description>&lt;p&gt;
LARA&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#25239;&#36807;&#25311;&#21512;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20877;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection. (arXiv:2310.05668v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05668
&lt;/p&gt;
&lt;p&gt;
LARA&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#25239;&#36807;&#25311;&#21512;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20877;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#23558;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20984;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21453;&#24605;&#27169;&#22359;&#20197;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#65292;&#21516;&#26102;&#25968;&#23398;&#35777;&#26126;&#20102;&#22312;&#24494;&#35843;&#21518;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22823;&#37096;&#20998;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#37117;&#20551;&#35774;&#27491;&#24120;&#27169;&#24335;&#22987;&#32456;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;Web&#26381;&#21153;&#30340;&#27491;&#24120;&#27169;&#24335;&#32463;&#24120;&#21457;&#29983;&#21095;&#28872;&#21464;&#21270;&#12290;&#22312;&#36825;&#31181;&#21464;&#21270;&#20043;&#21518;&#65292;&#20351;&#29992;&#26087;&#20998;&#24067;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#24050;&#32463;&#36807;&#26102;&#12290;&#27599;&#27425;&#37117;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#26159;&#26114;&#36149;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#27491;&#24120;&#27169;&#24335;&#21464;&#21270;&#24320;&#22987;&#26102;&#65292;&#26032;&#20998;&#24067;&#30340;&#35266;&#23519;&#25968;&#25454;&#19981;&#36275;&#12290;&#29992;&#26377;&#38480;&#30340;&#25968;&#25454;&#23545;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#25239;&#36807;&#25311;&#21512;&#30340;&#20877;&#35757;&#32451;&#26041;&#27861;&#65288;LARA&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;&#28145;&#24230;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65288;VAEs&#65289;&#12290;&#26412;&#24037;&#20316;&#26088;&#22312;&#25552;&#20986;&#19977;&#20010;&#26032;&#39062;&#30340;&#36129;&#29486;&#65306;1&#65289;&#23558;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20984;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#20197;&#24555;&#36895;&#25910;&#25947;&#20197;&#21450;&#38450;&#27490;&#36807;&#25311;&#21512;&#65307;2&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#21453;&#24605;&#27169;&#22359;&#65292;&#21487;&#20197;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#32780;&#26080;&#38656;&#20648;&#23384;&#23427;&#20204;&#65307;3&#65289;&#25968;&#23398;&#35777;&#26126;&#20102;&#22312;&#24494;&#35843;&#21518;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of current anomaly detection models assume that the normal pattern remains same all the time. However, the normal patterns of Web services change dramatically and frequently. The model trained on old-distribution data is outdated after such changes. Retraining the whole model every time is expensive. Besides, at the beginning of normal pattern changes, there is not enough observation data from the new distribution. Retraining a large neural network model with limited data is vulnerable to overfitting. Thus, we propose a Light and Anti-overfitting Retraining Approach (LARA) for deep variational auto-encoder based time series anomaly detection methods (VAEs). This work aims to make three novel contributions: 1) the retraining process is formulated as a convex problem and can converge at a fast rate as well as prevent overfitting; 2) designing a ruminate block, which leverages the historical data without the need to store them; 3) mathematically proving that when fine-tuning the late
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24615;&#33021;GPU&#24179;&#21488;&#65292;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#19982;&#25163;&#26415;&#26426;&#22120;&#20154;&#30340;&#21327;&#20316;&#12290;&#36890;&#36807;&#25552;&#39640;&#27169;&#25311;&#22120;&#25928;&#29575;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#26131;&#33719;&#21462;&#24615;&#65292;&#20026;&#25163;&#26415;&#33258;&#21160;&#21270;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.04676</link><description>&lt;p&gt;
&#12298;&#22806;&#31185;&#20581;&#36523;&#25151;&#65306;&#22522;&#20110;&#39640;&#24615;&#33021;GPU&#30340;&#25163;&#26415;&#26426;&#22120;&#20154;&#22686;&#24378;&#23398;&#20064;&#24179;&#21488;&#12299;
&lt;/p&gt;
&lt;p&gt;
Surgical Gym: A high-performance GPU-based platform for reinforcement learning with surgical robots. (arXiv:2310.04676v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04676
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24615;&#33021;GPU&#24179;&#21488;&#65292;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#19982;&#25163;&#26415;&#26426;&#22120;&#20154;&#30340;&#21327;&#20316;&#12290;&#36890;&#36807;&#25552;&#39640;&#27169;&#25311;&#22120;&#25928;&#29575;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#26131;&#33719;&#21462;&#24615;&#65292;&#20026;&#25163;&#26415;&#33258;&#21160;&#21270;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#20351;&#24471;&#25163;&#26415;&#36234;&#26469;&#36234;&#31934;&#30830;&#12289;&#39640;&#25928;&#21644;&#24494;&#21019;&#65292;&#24320;&#21551;&#20102;&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#24178;&#39044;&#30340;&#26032;&#26102;&#20195;&#12290;&#36825;&#20351;&#24471;&#21307;&#29983;&#22312;&#19982;&#26426;&#22120;&#20154;&#21327;&#20316;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#36890;&#36807;&#26356;&#23567;&#30340;&#20999;&#21475;&#36827;&#34892;&#20256;&#32479;&#25110;&#24494;&#21019;&#25163;&#26415;&#65292;&#20174;&#32780;&#25913;&#21892;&#25163;&#26415;&#32467;&#26524;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#20351;&#26426;&#22120;&#20154;&#25163;&#26415;&#26356;&#21152;&#33258;&#20027;&#65292;&#36825;&#26377;&#28508;&#21147;&#20943;&#23569;&#25163;&#26415;&#32467;&#26524;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#24182;&#21457;&#30151;&#29575;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20026;&#25163;&#26415;&#33258;&#21160;&#21270;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#22823;&#37327;&#30340;&#25968;&#25454;&#33719;&#21462;&#65292;&#22240;&#20026;&#22312;&#25104;&#21151;&#23436;&#25104;&#20219;&#21153;&#26041;&#38754;&#32570;&#20047;&#20808;&#39564;&#30693;&#35782;&#12290;&#30001;&#20110;&#27169;&#25311;&#25968;&#25454;&#25910;&#38598;&#30340;&#23494;&#38598;&#24615;&#36136;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#33268;&#21147;&#20110;&#20351;&#29616;&#26377;&#31639;&#27861;&#26356;&#21152;&#39640;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#25552;&#39640;&#27169;&#25311;&#22120;&#30340;&#25928;&#29575;&#65292;&#20351;&#35757;&#32451;&#25968;&#25454;&#27604;&#20197;&#21069;&#26356;&#26131;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in robot-assisted surgery have resulted in progressively more precise, efficient, and minimally invasive procedures, sparking a new era of robotic surgical intervention. This enables doctors, in collaborative interaction with robots, to perform traditional or minimally invasive surgeries with improved outcomes through smaller incisions. Recent efforts are working toward making robotic surgery more autonomous which has the potential to reduce variability of surgical outcomes and reduce complication rates. Deep reinforcement learning methodologies offer scalable solutions for surgical automation, but their effectiveness relies on extensive data acquisition due to the absence of prior knowledge in successfully accomplishing tasks. Due to the intensive nature of simulated data collection, previous works have focused on making existing algorithms more efficient. In this work, we focus on making the simulator more efficient, making training data much more accessible than prev
&lt;/p&gt;</description></item><item><title>Cadenza&#39033;&#30446;&#32452;&#32455;&#20102;ICASSP SP Cadenza Challenge&#65292;&#26088;&#22312;&#36890;&#36807;&#38899;&#20048;&#20998;&#35299;/&#28151;&#38899;&#26469;&#25552;&#21319;&#21161;&#21548;&#22120;&#38899;&#36136;&#65292;&#22788;&#29702;&#36807;&#31243;&#32771;&#34385;&#38899;&#20048;&#12289;&#22686;&#30410;&#21644;&#21548;&#21147;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.03480</link><description>&lt;p&gt;
Cadenza ICASSP 2024&#22823;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Cadenza ICASSP 2024 Grand Challenge. (arXiv:2310.03480v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03480
&lt;/p&gt;
&lt;p&gt;
Cadenza&#39033;&#30446;&#32452;&#32455;&#20102;ICASSP SP Cadenza Challenge&#65292;&#26088;&#22312;&#36890;&#36807;&#38899;&#20048;&#20998;&#35299;/&#28151;&#38899;&#26469;&#25552;&#21319;&#21161;&#21548;&#22120;&#38899;&#36136;&#65292;&#22788;&#29702;&#36807;&#31243;&#32771;&#34385;&#38899;&#20048;&#12289;&#22686;&#30410;&#21644;&#21548;&#21147;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cadenza&#39033;&#30446;&#26088;&#22312;&#25552;&#39640;&#21548;&#21147;&#21463;&#25439;&#20154;&#32676;&#30340;&#38899;&#20048;&#38899;&#36136;&#12290;&#20316;&#20026;&#35813;&#39033;&#30446;&#30340;&#19968;&#37096;&#20998;&#65292;&#35813;&#39033;&#30446;&#32452;&#32455;&#20102;ICASSP SP Cadenza Challenge&#65306;&#38754;&#21521;&#21161;&#21548;&#22120;&#30340;&#38899;&#20048;&#20998;&#35299;/&#28151;&#38899;&#12290;&#35813;&#25361;&#25112;&#21487;&#20197;&#36890;&#36807;&#23558;&#38899;&#20048;&#20998;&#35299;&#25104;&#20154;&#22768;&#12289;&#36125;&#26031;&#12289;&#40723;&#21644;&#20854;&#20182;&#32452;&#25104;&#37096;&#20998;&#26469;&#35299;&#20915;&#12290;&#28982;&#21518;&#21487;&#20197;&#20197;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#26234;&#33021;&#22320;&#36827;&#34892;&#28151;&#38899;&#65292;&#20197;&#25552;&#39640;&#38899;&#39057;&#36136;&#37327;&#12290;&#21478;&#22806;&#65292;&#36824;&#21487;&#20197;&#20351;&#29992;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#12290;&#22788;&#29702;&#36807;&#31243;&#38656;&#35201;&#32771;&#34385;&#38899;&#20048;&#26412;&#36523;&#12289;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#22686;&#30410;&#20197;&#21450;&#21548;&#20247;&#30340;&#21548;&#21147;&#25439;&#22833;&#12290;&#25552;&#20132;&#30340;&#20316;&#21697;&#23558;&#20351;&#29992;Hearing Aid Audio Quality Index&#65288;HAAQI&#65289;&#36825;&#19968;&#20837;&#20405;&#24335;&#23458;&#35266;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#35813;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Cadenza project aims to enhance the audio quality of music for individuals with hearing loss. As part of this, the project is organizing the ICASSP SP Cadenza Challenge: Music Demixing/Remixing for Hearing Aids. The challenge can be tackled by decomposing the music at the hearing aid microphones into vocals, bass, drums, and other components. These can then be intelligently remixed in a personalized manner to improve audio quality. Alternatively, an end-to-end approach could be used. Processes need to consider the music itself, the gain applied to each component, and the listener's hearing loss. The submitted entries will be evaluated using the intrusive objective metric, the Hearing Aid Audio Quality Index (HAAQI). This paper outlines the challenge.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32763;&#35793;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25105;&#20204;&#25104;&#21151;&#32469;&#36807;&#20102;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;AI&#23433;&#20840;&#24615;&#20013;&#30340;&#34180;&#24369;&#29615;&#33410;&#12290;</title><link>http://arxiv.org/abs/2310.02446</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#36234;&#29425; GPT-4
&lt;/p&gt;
&lt;p&gt;
Low-Resource Languages Jailbreak GPT-4. (arXiv:2310.02446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02446
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32763;&#35793;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25105;&#20204;&#25104;&#21151;&#32469;&#36807;&#20102;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;AI&#23433;&#20840;&#24615;&#20013;&#30340;&#34180;&#24369;&#29615;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#22521;&#35757;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32418;&#38431;&#27979;&#35797;&#26159;&#20943;&#23569;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#23558;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#32763;&#35793;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25104;&#21151;&#32469;&#36807;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#25581;&#31034;&#20102;&#36825;&#20123;&#23433;&#20840;&#26426;&#21046;&#30340;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#22312;AdvBenchmark&#20013;&#65292;GPT-4&#38024;&#23545;&#19981;&#23433;&#20840;&#30340;&#32763;&#35793;&#36755;&#20837;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#19988;79%&#30340;&#26102;&#38388;&#20869;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26041;&#26696;&#65292;&#20351;&#29992;&#25143;&#23454;&#29616;&#20854;&#26377;&#23475;&#30446;&#26631;&#65292;&#36825;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#30456;&#24403;&#12290;&#20854;&#20182;&#39640;/&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#26174;&#33879;&#36739;&#20302;&#65292;&#36825;&#34920;&#26126;&#36328;&#35821;&#35328;&#28431;&#27934;&#20027;&#35201;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20197;&#21069;&#65292;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26377;&#38480;&#35757;&#32451;&#20027;&#35201;&#24433;&#21709;&#37027;&#20123;&#20351;&#29992;&#36825;&#20123;&#35821;&#35328;&#30340;&#20154;&#65292;&#36896;&#25104;&#25216;&#26415;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#36716;&#21464;&#65306;
&lt;/p&gt;
&lt;p&gt;
AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4's safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift:
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;Time-LLM&#65292;&#19968;&#20010;&#37325;&#26032;&#32534;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#19968;&#33324;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.01728</link><description>&lt;p&gt;
Time-LLM: &#36890;&#36807;&#37325;&#26032;&#32534;&#31243;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. (arXiv:2310.01728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01728
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;Time-LLM&#65292;&#19968;&#20010;&#37325;&#26032;&#32534;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#19968;&#33324;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#35768;&#22810;&#23454;&#38469;&#21160;&#24577;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#19981;&#21516;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#22823;&#22411;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#32780;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22411;&#36890;&#24120;&#26159;&#19987;&#38376;&#21270;&#30340;&#65292;&#38656;&#35201;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#24212;&#29992;&#35774;&#35745;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;NLP&#21644;CV&#39046;&#22495;&#20013;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#21457;&#23637;&#21463;&#21040;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38480;&#21046;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#30340;&#24207;&#21015;&#26631;&#35760;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#20197;&#21033;&#29992;&#36825;&#20123;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Time-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#26032;&#32534;&#31243;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#37325;&#29992;LLMs&#26469;&#36827;&#34892;&#19968;&#33324;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#25345;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;ReLU&#35270;&#20026;&#20174;R&#25237;&#24433;&#21040;&#38750;&#36127;&#21322;&#32447;R+&#30340;&#25805;&#20316;&#65292;&#25105;&#20204;&#23558;&#20854;&#36890;&#36807;&#29992;&#20984;&#38181;&#30340;&#24191;&#20041;&#25237;&#24433;&#31639;&#23376;&#26367;&#20195;&#65292;&#25193;&#23637;&#20026;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#21644;&#22810;&#20010;&#36755;&#20986;&#30340;&#22810;&#21464;&#37327;&#25237;&#24433;&#21333;&#20803; (MPU)&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;ReLU&#28608;&#27963;&#30340;FNN&#12290;</title><link>http://arxiv.org/abs/2309.17194</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#21464;&#37327;&#25237;&#24433;&#36827;&#34892;&#24191;&#20041;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
Generalized Activation via Multivariate Projection. (arXiv:2309.17194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17194
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;ReLU&#35270;&#20026;&#20174;R&#25237;&#24433;&#21040;&#38750;&#36127;&#21322;&#32447;R+&#30340;&#25805;&#20316;&#65292;&#25105;&#20204;&#23558;&#20854;&#36890;&#36807;&#29992;&#20984;&#38181;&#30340;&#24191;&#20041;&#25237;&#24433;&#31639;&#23376;&#26367;&#20195;&#65292;&#25193;&#23637;&#20026;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#21644;&#22810;&#20010;&#36755;&#20986;&#30340;&#22810;&#21464;&#37327;&#25237;&#24433;&#21333;&#20803; (MPU)&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;ReLU&#28608;&#27963;&#30340;FNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;Rectified Linear Unit (ReLU)&#24120;&#22240;&#20854;&#31616;&#21333;&#21644;&#26377;&#25928;&#32780;&#21463;&#38738;&#30544;&#12290;&#21463;&#27973;&#23618;&#21069;&#21521;&#31070;&#32463;&#32593;&#32476; (FNN) &#21644;&#21333;&#27425;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477; (PGD) &#31639;&#27861;&#20043;&#38388;&#32467;&#26500;&#30456;&#20284;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;ReLU&#35270;&#20026;&#20174;R&#25237;&#24433;&#21040;&#38750;&#36127;&#21322;&#32447;R+&#30340;&#25805;&#20316;&#12290;&#22312;&#36825;&#20010;&#35299;&#37322;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#29992;&#20984;&#38181;&#30340;&#24191;&#20041;&#25237;&#24433;&#31639;&#23376;&#26367;&#20195;ReLU&#65292;&#22914;&#20108;&#38454;&#38181; (SOC) &#25237;&#24433;&#65292;&#20174;&#32780;&#23558;&#20854;&#33258;&#28982;&#22320;&#25193;&#23637;&#20026;&#22810;&#21464;&#37327;&#25237;&#24433;&#21333;&#20803; (MPU)&#65292;&#36825;&#26159;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#21644;&#22810;&#20010;&#36755;&#20986;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#25968;&#23398;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;SOC&#25237;&#24433;&#28608;&#27963;&#30340;FNN&#22312;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20351;&#29992;ReLU&#30340;FNN&#12290;&#36890;&#36807;&#23545;&#24191;&#27867;&#37319;&#29992;&#30340;&#26550;&#26500;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Activation functions are essential to introduce nonlinearity into neural networks, with the Rectified Linear Unit (ReLU) often favored for its simplicity and effectiveness. Motivated by the structural similarity between a shallow Feedforward Neural Network (FNN) and a single iteration of the Projected Gradient Descent (PGD) algorithm, a standard approach for solving constrained optimization problems, we consider ReLU as a projection from R onto the nonnegative half-line R+. Building on this interpretation, we extend ReLU by substituting it with a generalized projection operator onto a convex cone, such as the Second-Order Cone (SOC) projection, thereby naturally extending it to a Multivariate Projection Unit (MPU), an activation function with multiple inputs and multiple outputs. We further provide a mathematical proof establishing that FNNs activated by SOC projections outperform those utilizing ReLU in terms of expressive power. Experimental evaluations on widely-adopted architecture
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;184&#26465;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#20986;&#20102;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16742</link><description>&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#39118;&#38505;&#30340;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients. (arXiv:2309.16742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;184&#26465;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#20986;&#20102;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#65292;&#23588;&#20854;&#26159;2&#22411;&#31958;&#23615;&#30149;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#20581;&#24247;&#38382;&#39064;&#12290;&#19982;&#31958;&#23615;&#30149;&#30456;&#20851;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#20854;&#24182;&#21457;&#30151;&#30340;&#21457;&#23637;&#12290;&#31958;&#23615;&#30149;&#32958;&#30149;&#26159;&#31958;&#23615;&#30149;&#30340;&#19968;&#31181;&#24930;&#24615;&#24182;&#21457;&#30151;&#65292;&#19981;&#21033;&#22320;&#24433;&#21709;&#32958;&#33039;&#65292;&#23548;&#33268;&#32958;&#33039;&#25439;&#20260;&#12290;&#35786;&#26029;&#31958;&#23615;&#30149;&#32958;&#30149;&#28041;&#21450;&#32771;&#34385;&#21508;&#31181;&#26631;&#20934;&#20043;&#19968;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#23615;&#28082;&#20013;&#30333;&#34507;&#30333;&#30340;&#30149;&#29702;&#23398;&#30149;&#29702;&#23398;&#25968;&#37327;&#65292;&#31216;&#20026;&#30333;&#34507;&#30333;&#23615;&#12290;&#22240;&#27492;&#65292;&#23545;&#31958;&#23615;&#30149;&#24739;&#32773;&#23615;&#28082;&#20013;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#20855;&#26377;&#21450;&#26102;&#39044;&#38450;&#25514;&#26045;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#24739;&#26377;&#30333;&#34507;&#30333;&#23615;&#30340;&#39118;&#38505;&#12290;&#25152;&#36873;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#21253;&#25324;&#26420;&#32032;&#36125;&#21494;&#26031;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;AdaBoost&#65292;XGBoost&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12290;&#25105;&#20204;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#21253;&#25324;184&#26465;&#31958;&#23615;&#30149;&#24182;&#21457;&#30151;&#39118;&#38505;&#22240;&#32032;&#30340;&#26465;&#30446;&#34987;&#29992;&#26469;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diabetes, especially T2DM, continues to be a significant health problem. One of the major concerns associated with diabetes is the development of its complications. Diabetic nephropathy, one of the chronic complication of diabetes, adversely affects the kidneys, leading to kidney damage. Diagnosing diabetic nephropathy involves considering various criteria, one of which is the presence of a pathologically significant quantity of albumin in urine, known as albuminuria. Thus, early prediction of albuminuria in diabetic patients holds the potential for timely preventive measures. This study aimed to develop a supervised learning model to predict the risk of developing albuminuria in T2DM patients. The selected supervised learning algorithms included Na\"ive Bayes, Support Vector Machine (SVM), decision tree, random forest, AdaBoost, XGBoost, and Multi-Layer Perceptron (MLP). Our private dataset, comprising 184 entries of diabetes complications risk factors, was used to train the algorithm
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#65292;&#24403;&#28857;&#20987;&#25968;&#25454;&#19981;&#33021;&#23436;&#20840;&#25311;&#21512;&#26102;&#65292;&#26080;&#27861;&#24674;&#22797;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#22270;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.15560</link><description>&lt;p&gt;
&#35782;&#21035;&#24615;&#24456;&#37325;&#35201;&#65306;&#25581;&#31034;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#38544;&#34255;&#30340;&#21487;&#24674;&#22797;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank. (arXiv:2309.15560v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15560
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#65292;&#24403;&#28857;&#20987;&#25968;&#25454;&#19981;&#33021;&#23436;&#20840;&#25311;&#21512;&#26102;&#65292;&#26080;&#27861;&#24674;&#22797;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#22270;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;(Unbiased Learning to Rank, ULTR)&#22312;&#20174;&#26377;&#20559;&#28857;&#20987;&#26085;&#24535;&#35757;&#32451;&#26080;&#20559;&#25490;&#21517;&#27169;&#22411;&#30340;&#29616;&#20195;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20851;&#38190;&#22312;&#20110;&#26126;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#34892;&#20026;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#22522;&#20110;&#26816;&#39564;&#20551;&#35774;&#23545;&#28857;&#20987;&#25968;&#25454;&#36827;&#34892;&#25311;&#21512;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#21482;&#35201;&#28857;&#20987;&#23436;&#20840;&#25311;&#21512;&#65292;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#21487;&#20197;&#24674;&#22797;&#20986;&#30495;&#23454;&#28508;&#22312;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#24182;&#38750;&#24635;&#26159;&#33021;&#22815;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20174;&#32780;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22238;&#31572;&#30495;&#23454;&#30456;&#20851;&#24615;&#26159;&#21542;&#33021;&#22815;&#20174;&#28857;&#20987;&#25968;&#25454;&#24674;&#22797;&#20986;&#26469;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;ULTR&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#19968;&#20010;&#25490;&#21517;&#27169;&#22411;&#23450;&#20041;&#20026;&#21487;&#35782;&#21035;&#30340;&#65292;&#22914;&#26524;&#23427;&#21487;&#20197;&#24674;&#22797;&#20986;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#26368;&#22810;&#21482;&#26377;&#19968;&#20010;&#32553;&#25918;&#21464;&#25442;&#65292;&#36825;&#23545;&#20110;&#25104;&#23545;&#25490;&#21517;&#30446;&#26631;&#26469;&#35828;&#24050;&#36275;&#22815;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#31561;&#20215;&#30340;&#21487;&#35782;&#21035;&#26465;&#20214;&#65292;&#21487;&#20197;&#26032;&#39062;&#22320;&#34920;&#36798;&#20026;&#19968;&#20010;&#22270;&#36830;&#36890;&#24615;&#27979;&#35797;&#38382;&#39064;&#65306;&#24403;&#19988;&#20165;&#24403;&#19968;&#20010;&#22270;&#65288;&#21363;&#21487;&#35782;&#21035;&#24615;&#22270;&#65289;&#36830;&#36890;&#26102;&#65292;&#35813;&#25490;&#21517;&#27169;&#22411;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Unbiased Learning to Rank (ULTR) is widespread in modern systems for training unbiased ranking models from biased click logs. The key is to explicitly model a generation process for user behavior and fit click data based on examination hypothesis. Previous research found empirically that the true latent relevance can be recovered in most cases as long as the clicks are perfectly fitted. However, we demonstrate that this is not always achievable, resulting in a significant reduction in ranking performance. In this work, we aim to answer if or when the true relevance can be recovered from click data, which is a foundation issue for ULTR field. We first define a ranking model as identifiable if it can recover the true relevance up to a scaling transformation, which is enough for pairwise ranking objective. Then we explore an equivalent condition for identifiability that can be novely expressed as a graph connectivity test problem: if and only if a graph (namely identifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;TVLARS&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21487;&#37197;&#32622;&#30340;&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#27604;LARS&#21644;LAMB&#37117;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2309.14053</link><description>&lt;p&gt;
&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#25209;&#37327;&#35757;&#32451;&#27867;&#21270;&#24615;&#33021;&#30340;LARS&#20877;&#23457;&#35270;
&lt;/p&gt;
&lt;p&gt;
Revisiting LARS for Large Batch Training Generalization of Neural Networks. (arXiv:2309.14053v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;TVLARS&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21487;&#37197;&#32622;&#30340;&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#27604;LARS&#21644;LAMB&#37117;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#20351;&#29992;&#36880;&#23618;&#33258;&#36866;&#24212;&#32553;&#25918;&#27604;(LARS)&#26469;&#25506;&#32034;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;&#20855;&#26377;&#28909;&#36523;&#38454;&#27573;&#30340;LARS&#31639;&#27861;&#30001;&#20110;&#20887;&#20313;&#30340;&#27604;&#20363;&#32553;&#25918;&#23548;&#33268;&#22312;&#26089;&#26399;&#38519;&#20837;&#23574;&#38160;&#30340;&#26497;&#23567;&#21270;&#22120;&#12290;&#27492;&#22806;&#65292;&#21518;&#26399;&#22266;&#23450;&#30340;&#38497;&#23789;&#19979;&#38477;&#38480;&#21046;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#36941;&#21382;&#26089;&#26399;&#23574;&#38160;&#30340;&#26497;&#23567;&#21270;&#22120;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;Time Varying LARS (TVLARS)&#65292;&#23427;&#29992;&#21487;&#37197;&#32622;&#30340;&#31867;&#20284;sigmoid&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#22312;&#21021;&#22987;&#38454;&#27573;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;TVLARS&#22312;&#26089;&#26399;&#20419;&#36827;&#20102;&#26799;&#24230;&#25506;&#32034;&#65292;&#36229;&#36234;&#20102;&#23574;&#38160;&#30340;&#20248;&#21270;&#22120;&#65292;&#24182;&#36880;&#28176;&#36807;&#28193;&#21040;LARS&#20197;&#23454;&#29616;&#21518;&#26399;&#30340;&#31283;&#20581;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#22987;&#32456;&#20248;&#20110;LARS&#21644;LAMB&#65292;&#20998;&#31867;&#22330;&#26223;&#20013;&#30340;&#25913;&#36827;&#36798;&#21040;2\%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25152;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26696;&#20363;&#20013;&#65292;TVLARS&#37117;&#32988;&#36807;&#20102;LARS&#21644;LAMB&#65292;&#24182;&#19988;&#24615;&#33021;&#25552;&#21319;&#20102;
&lt;/p&gt;
&lt;p&gt;
This paper explores Large Batch Training techniques using layer-wise adaptive scaling ratio (LARS) across diverse settings, uncovering insights. LARS algorithms with warm-up tend to be trapped in sharp minimizers early on due to redundant ratio scaling. Additionally, a fixed steep decline in the latter phase restricts deep neural networks from effectively navigating early-phase sharp minimizers. Building on these findings, we propose Time Varying LARS (TVLARS), a novel algorithm that replaces warm-up with a configurable sigmoid-like function for robust training in the initial phase. TVLARS promotes gradient exploration early on, surpassing sharp optimizers and gradually transitioning to LARS for robustness in later phases. Extensive experiments demonstrate that TVLARS consistently outperforms LARS and LAMB in most cases, with up to 2\% improvement in classification scenarios. Notably, in all self-supervised learning cases, TVLARS dominates LARS and LAMB with performance improvements of
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#21327;&#21516;&#36215;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#23398;&#29983;&#22312;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26377;&#31526;&#21495;&#20108;&#20998;&#22270;&#20840;&#38754;&#24314;&#27169;&#23398;&#29983;&#22238;&#31572;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#22686;&#24378;&#20102;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13500</link><description>&lt;p&gt;
&#22312;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#38382;&#39064;&#19978;&#22686;&#24378;&#23398;&#29983;&#34920;&#29616;&#39044;&#27979;&#30340;SGNN-LLM&#21327;&#21516;
&lt;/p&gt;
&lt;p&gt;
Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy. (arXiv:2309.13500v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13500
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#21327;&#21516;&#36215;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#23398;&#29983;&#22312;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26377;&#31526;&#21495;&#20108;&#20998;&#22270;&#20840;&#38754;&#24314;&#27169;&#23398;&#29983;&#22238;&#31572;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#22686;&#24378;&#20102;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#29983;&#20869;&#23481;&#21019;&#20316;&#65292;&#23398;&#20064;&#32773;&#21512;&#20316;&#20855;&#26377;&#21487;&#25193;&#23637;&#25945;&#32946;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#23398;&#29983;&#22312;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#33267;&#20851;&#37325;&#35201;&#65292;&#30001;&#20110;&#23398;&#29983;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#22122;&#22768;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#33719;&#23398;&#29983;&#21644;&#38382;&#39064;&#20132;&#20114;&#30340;&#22797;&#26434;&#32593;&#32476;&#65292;&#20294;&#22312;&#20919;&#21551;&#21160;&#26465;&#20214;&#19979;&#65292;&#20854;&#20013;&#23398;&#29983;&#23545;&#38382;&#39064;&#30340;&#26377;&#38480;&#21442;&#19982;&#23548;&#33268;&#25968;&#25454;&#31232;&#30095;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#65292;&#23558;&#25972;&#21512;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#21327;&#21516;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26377;&#31526;&#21495;&#20108;&#20998;&#22270;&#20840;&#38754;&#24314;&#27169;&#23398;&#29983;&#22238;&#31572;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#22686;&#24378;&#20102;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;LLM&#30340;&#36129;&#29486;&#22312;&#20110;&#29983;&#25104;&#22522;&#30784;&#38382;&#39064;&#23884;&#20837;&#65292;&#29305;&#21035;&#26159;&#35777;&#26126;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learnersourcing offers great potential for scalable education through student content creation. However, predicting student performance on learnersourced questions, which is essential for personalizing the learning experience, is challenging due to the inherent noise in student-generated data. Moreover, while conventional graph-based methods can capture the complex network of student and question interactions, they often fall short under cold start conditions where limited student engagement with questions yields sparse data. To address both challenges, we introduce an innovative strategy that synergizes the potential of integrating Signed Graph Neural Networks (SGNNs) and Large Language Model (LLM) embeddings. Our methodology employs a signed bipartite graph to comprehensively model student answers, complemented by a contrastive learning framework that enhances noise resilience. Furthermore, LLM's contribution lies in generating foundational question embeddings, proving especially adv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#23548;&#40657;&#30418;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.13340</link><description>&lt;p&gt;
&#38754;&#21521;&#40657;&#30418;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;LLM&#24341;&#23548;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards LLM-guided Causal Explainability for Black-box Text Classifiers. (arXiv:2309.13340v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#23548;&#40657;&#30418;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22823;&#19988;&#26356;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#27604;&#22914;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#20687;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#36825;&#26679;&#30340;&#27169;&#22411;&#36136;&#37327;&#65292;&#23613;&#31649;&#38750;&#24120;&#20196;&#20154;&#21521;&#24448;&#65292;&#20294;&#21464;&#24471;&#36234;&#26469;&#36234;&#38590;&#20197;&#35299;&#20915;&#12290;&#20363;&#22914;&#65292;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#26159;&#35774;&#35745;&#20026;&#40657;&#30418;&#12290;&#23613;&#31649;&#26631;&#20934;&#30340;&#35299;&#37322;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#19968;&#23450;&#31243;&#24230;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#65292;&#23545;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#26377;&#38480;&#12290;&#22240;&#26524;&#35299;&#37322;&#33021;&#21147;&#26159;&#26356;&#29702;&#24819;&#30340;&#30446;&#26631;&#65292;&#20294;&#22312;NLP&#39046;&#22495;&#21364;&#26497;&#20855;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26377;&#24456;&#22810;&#12290;&#21463;&#21040;&#26368;&#36817;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#19987;&#23478;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#26368;&#26032;&#30340;LLMs&#30340;&#25351;&#23548;&#21644;&#29702;&#35299;&#33021;&#21147;&#65292;&#36890;&#36807;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#23454;&#29616;&#40657;&#30418;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#27969;&#31243;&#65292;
&lt;/p&gt;
&lt;p&gt;
With the advent of larger and more complex deep learning models, such as in Natural Language Processing (NLP), model qualities like explainability and interpretability, albeit highly desirable, are becoming harder challenges to tackle and solve. For example, state-of-the-art models in text classification are black-box by design. Although standard explanation methods provide some degree of explainability, these are mostly correlation-based methods and do not provide much insight into the model. The alternative of causal explainability is more desirable to achieve but extremely challenging in NLP due to a variety of reasons. Inspired by recent endeavors to utilize Large Language Models (LLMs) as experts, in this work, we aim to leverage the instruction-following and textual understanding capabilities of recent state-of-the-art LLMs to facilitate causal explainability via counterfactual explanation generation for black-box text classifiers. To do this, we propose a three-step pipeline via
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#21512;&#25104;&#23569;&#37327;&#31034;&#33539;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;ICL&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#24378;&#38544;&#31169;&#32423;&#21035;&#19979;&#33021;&#22815;&#21462;&#24471;&#31454;&#20105;&#24615;&#33021;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#39046;&#22495;&#30340;&#38544;&#31169;&#20445;&#25252;&#19979;ICL&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11765</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#24046;&#20998;&#38544;&#31169;&#24369;&#30417;&#30563;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation. (arXiv:2309.11765v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#21512;&#25104;&#23569;&#37327;&#31034;&#33539;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;ICL&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#24378;&#38544;&#31169;&#32423;&#21035;&#19979;&#33021;&#22815;&#21462;&#24471;&#31454;&#20105;&#24615;&#33021;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#39046;&#22495;&#30340;&#38544;&#31169;&#20445;&#25252;&#19979;ICL&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#24773;&#26223;&#20250;&#24102;&#26469;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;LLM&#21487;&#33021;&#27844;&#28431;&#25110;&#22797;&#36848;&#22312;&#25552;&#31034;&#20013;&#23637;&#31034;&#30340;&#31169;&#26377;&#31034;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#31169;&#26377;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#20855;&#26377;&#24418;&#24335;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#21512;&#25104;&#23569;&#37327;&#31034;&#33539;&#65292;&#24182;&#22312;&#23454;&#35777;&#19978;&#35777;&#26126;&#23427;&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#30340;ICL&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#38750;&#31169;&#26377;ICL&#21644;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#24378;&#38544;&#31169;&#32423;&#21035;&#19979;&#36798;&#21040;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;ICL&#22312;&#24191;&#27867;&#24212;&#29992;&#39046;&#22495;&#25171;&#24320;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of in-context learning (ICL) with large language models (LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak or regurgitate the private examples demonstrated in the prompt. We propose a novel algorithm that generates synthetic few-shot demonstrations from the private dataset with formal differential privacy (DP) guarantees, and show empirically that it can achieve effective ICL. We conduct extensive experiments on standard benchmarks and compare our algorithm with non-private ICL and zero-shot solutions. Our results demonstrate that our algorithm can achieve competitive performance with strong privacy levels. These results open up new possibilities for ICL with privacy protection for a broad range of applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#65292;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36229;&#21442;&#25968;&#35843;&#20248;&#21644;&#30828;&#20214;&#22788;&#29702;&#22120;&#30340;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#21462;&#24471;&#20102;&#30495;&#23454;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06782</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#21315;&#20806;&#32423;&#25968;&#25454;&#38598;&#29992;&#20110;&#31890;&#23376;&#27969;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Scalable neural network models and terascale datasets for particle-flow reconstruction. (arXiv:2309.06782v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#65292;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36229;&#21442;&#25968;&#35843;&#20248;&#21644;&#30828;&#20214;&#22788;&#29702;&#22120;&#30340;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#21462;&#24471;&#20102;&#30495;&#23454;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#22522;&#20110;&#39640;&#24230;&#31890;&#24230;&#25506;&#27979;&#22120;&#27169;&#25311;&#30340;&#23436;&#25972;&#20107;&#20214;&#37325;&#24314;&#65292;&#30740;&#31350;&#20102;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#31890;&#23376;&#27969;&#65288;PF&#65289;&#37325;&#24314;&#21487;&#36890;&#36807;&#36319;&#36394;&#21644;&#37327;&#33021;&#22120;&#22242;&#31751;&#25110;&#20987;&#20013;&#26469;&#26500;&#24314;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#20869;&#26680;&#30340;&#21464;&#25442;&#22120;&#65292;&#24182;&#35777;&#26126;&#20004;&#32773;&#37117;&#36991;&#20813;&#20102;&#20108;&#27425;&#20869;&#23384;&#20998;&#37197;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#30495;&#23454;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25152;&#24471;&#27169;&#22411;&#22312;&#30828;&#20214;&#22788;&#29702;&#22120;&#19978;&#20855;&#26377;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#25903;&#25345;NVIDIA, AMD&#21644;&#33521;&#29305;&#23572; Habana&#21345;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#21487;&#20197;&#22312;&#30001;&#36319;&#36394;&#21644;&#37327;&#33021;&#22120;&#20987;&#20013;&#32452;&#25104;&#30340;&#39640;&#31890;&#24230;&#36755;&#20837;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33719;&#24471;&#19982;&#22522;&#20934;&#30456;&#31454;&#20105;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;&#26377;&#20851;&#22797;&#29616;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24050;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study scalable machine learning models for full event reconstruction in high-energy electron-positron collisions based on a highly granular detector simulation. Particle-flow (PF) reconstruction can be formulated as a supervised learning task using tracks and calorimeter clusters or hits. We compare a graph neural network and kernel-based transformer and demonstrate that both avoid quadratic memory allocation and computational cost while achieving realistic PF reconstruction. We show that hyperparameter tuning on a supercomputer significantly improves the physics performance of the models. We also demonstrate that the resulting model is highly portable across hardware processors, supporting Nvidia, AMD, and Intel Habana cards. Finally, we demonstrate that the model can be trained on highly granular inputs consisting of tracks and calorimeter hits, resulting in a competitive physics performance with the baseline. Datasets and software to reproduce the studies are published following 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#30740;&#31350;&#20102;&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#23545;&#23398;&#20064;&#24615;&#33021;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.06054</link><description>&lt;p&gt;
&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#65306;&#23545;&#21512;&#25104;&#20219;&#21153;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
How does representation impact in-context learning: A exploration on a synthetic task. (arXiv:2309.06054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#30740;&#31350;&#20102;&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#23545;&#23398;&#20064;&#24615;&#33021;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21363;&#20174;&#19978;&#19979;&#25991;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#26159;Transformer&#30340;&#19968;&#39033;&#24341;&#20154;&#27880;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#39537;&#21160;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#23578;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#35282;&#24230;&#36827;&#34892;&#35843;&#26597;&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#34920;&#31034;&#26356;&#21152;&#22797;&#26434;&#65292;&#34920;&#31034;&#21487;&#20197;&#21463;&#21040;&#27169;&#22411;&#26435;&#37325;&#21644;&#19978;&#19979;&#25991;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#19978;&#36848;&#20004;&#20010;&#27010;&#24565;&#26041;&#38754;&#30340;&#34920;&#31034;&#20998;&#21035;&#31216;&#20026;&#26435;&#37325;&#20869;&#37096;&#25104;&#20998;&#21644;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20004;&#20010;&#25104;&#20998;&#22914;&#20309;&#24433;&#21709;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21512;&#25104;&#20219;&#21153;&#65292;&#20174;&#32780;&#21487;&#20197;&#35774;&#35745;&#20004;&#20010;&#25506;&#38024;&#65292;&#21363;&#26435;&#37325;&#20869;&#37096;&#25506;&#38024;&#21644;&#19978;&#19979;&#25991;&#25506;&#38024;&#65292;&#20998;&#21035;&#35780;&#20272;&#36825;&#20004;&#20010;&#25104;&#20998;&#12290;&#25105;&#20204;&#35777;&#26126;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#30340;&#22909;&#22351;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#39640;&#24230;&#30456;&#20851;&#65292;&#36825;&#34920;&#26126;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32416;&#32544;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, i.e., learning from in-context samples, is an impressive ability of Transformer. However, the mechanism driving the in-context learning is not yet fully understood. In this study, we aim to investigate from an underexplored perspective of representation learning. The representation is more complex for in-context learning senario, where the representation can be impacted by both model weights and in-context samples. We refer the above two conceptually aspects of representation as in-weight component and in-context component, respectively. To study how the two components affect in-context learning capabilities, we construct a novel synthetic task, making it possible to device two probes, in-weights probe and in-context probe, to evaluate the two components, respectively. We demonstrate that the goodness of in-context component is highly related to the in-context learning performance, which indicates the entanglement between in-context learning and representation lear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;RL-EA&#65289;&#65292;&#35813;&#31639;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#23545;&#21508;&#31181;RL-EA&#30340;&#32467;&#26500;&#12289;&#25805;&#20316;&#31526;&#21644;&#25628;&#32034;&#27169;&#24335;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.13420</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65306;&#35843;&#26597;&#21644;&#30740;&#31350;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and Research Opportunities. (arXiv:2308.13420v2 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;RL-EA&#65289;&#65292;&#35813;&#31639;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#23545;&#21508;&#31181;RL-EA&#30340;&#32467;&#26500;&#12289;&#25805;&#20316;&#31526;&#21644;&#25628;&#32034;&#27169;&#24335;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#26159;&#19968;&#31867;&#22522;&#20110;&#33258;&#28982;&#36827;&#21270;&#21407;&#29702;&#30340;&#38543;&#26426;&#25628;&#32034;&#26041;&#27861;&#65292;&#22240;&#20854;&#22312;&#21508;&#31181;&#23454;&#38469;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#32780;&#24191;&#21463;&#36190;&#35465;&#12290;&#23613;&#31649;&#20840;&#29699;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#36827;&#21270;&#31639;&#27861;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#23398;&#32773;&#31215;&#26497;&#25506;&#32034;&#25913;&#36827;&#31639;&#27861;&#32467;&#26500;&#12289;&#25805;&#20316;&#31526;&#12289;&#25628;&#32034;&#27169;&#24335;&#31561;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#20248;&#21270;&#24615;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#36827;&#21270;&#31639;&#27861;&#26694;&#26550;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#36229;&#36234;&#24615;&#33021;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#21040;&#36827;&#21270;&#31639;&#27861;&#20013;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#34987;&#31216;&#20026;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;RL-EA&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;RL-EA&#20013;&#19981;&#21516;&#32467;&#26500;&#12289;&#25805;&#20316;&#31526;&#21644;&#25628;&#32034;&#27169;&#24335;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary algorithms (EA), a class of stochastic search methods based on the principles of natural evolution, have received widespread acclaim for their exceptional performance in various real-world optimization problems. While researchers worldwide have proposed a wide variety of EAs, certain limitations remain, such as slow convergence speed and poor generalization capabilities. Consequently, numerous scholars actively explore improvements to algorithmic structures, operators, search patterns, etc., to enhance their optimization performance. Reinforcement learning (RL) integrated as a component in the EA framework has demonstrated superior performance in recent years. This paper presents a comprehensive survey on integrating reinforcement learning into the evolutionary algorithm, referred to as reinforcement learning-assisted evolutionary algorithm (RL-EA). We begin with the conceptual outlines of reinforcement learning and the evolutionary algorithm. We then provide a taxonomy of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Laplace-LoRA&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26469;&#22686;&#24378;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13111</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20302;&#31209;&#36866;&#24212;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bayesian low-rank adaptation for large language models. (arXiv:2308.13111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Laplace-LoRA&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26469;&#22686;&#24378;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#65288;PEFT&#65289;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#39640;&#25928;&#24494;&#35843;&#30340;&#26032;&#33539;&#24335;&#65292;&#20854;&#20013;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;LLMs&#24448;&#24448;&#21464;&#24471;&#36807;&#20110;&#33258;&#20449;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#20855;&#26377;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#21487;&#20316;&#20026;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#24182;&#22686;&#24378;&#26657;&#20934;&#33021;&#21147;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Laplace-LoRA&#65292;&#19968;&#31181;&#30452;&#35266;&#32780;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#23427;&#23558;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#24212;&#29992;&#20110;LoRA&#21442;&#25968;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;LLMs&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) has emerged as a new paradigm for cost-efficient fine-tuning of large language models (LLMs), with low-rank adaptation (LoRA) being a widely adopted choice. However, fine-tuned LLMs often become overconfident especially on when fine-tuned on smaller datasets. Bayesian methods, with their inherent ability to estimate uncertainty, serve as potent tools to mitigate overconfidence and enhance calibration. In this work, we introduce Laplace-LoRA, a straightforward yet effective Bayesian method, which applies the Laplace approximation to the LoRA parameters and, considerably boosts the calibration of fine-tuned LLMs.
&lt;/p&gt;</description></item><item><title>HypBO&#26159;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#20154;&#31867;&#30693;&#35782;&#24341;&#23548;&#36125;&#21494;&#26031;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26679;&#26412;&#31181;&#23376;&#26469;&#26356;&#24555;&#22320;&#25214;&#21040;&#26377;&#24076;&#26395;&#30340;&#21270;&#23398;&#31354;&#38388;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.11787</link><description>&lt;p&gt;
HypBO: &#19987;&#23478;&#24341;&#23548;&#19979;&#30340;&#21270;&#23398;&#23478;&#21442;&#19982;&#30340;&#36125;&#21494;&#26031;&#25628;&#32034;&#26032;&#26448;&#26009;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials. (arXiv:2308.11787v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11787
&lt;/p&gt;
&lt;p&gt;
HypBO&#26159;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#20154;&#31867;&#30693;&#35782;&#24341;&#23548;&#36125;&#21494;&#26031;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26679;&#26412;&#31181;&#23376;&#26469;&#26356;&#24555;&#22320;&#25214;&#21040;&#26377;&#24076;&#26395;&#30340;&#21270;&#23398;&#31354;&#38388;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#21270;&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#35299;&#20915;&#26448;&#26009;&#21457;&#29616;&#31561;&#38590;&#20197;&#35299;&#20915;&#30340;&#22810;&#21464;&#37327;&#31185;&#23398;&#38382;&#39064;&#65292;&#20294;&#21487;&#29992;&#30340;&#25628;&#32034;&#31354;&#38388;&#21487;&#33021;&#38750;&#24120;&#24222;&#22823;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#26679;&#26412;&#39640;&#25928;&#20248;&#21270;&#24341;&#25806;&#65292;&#22312;&#27809;&#26377;&#30446;&#26631;&#20989;&#25968;&#25110;&#23646;&#24615;&#30340;&#35299;&#26512;&#24418;&#24335;&#34987;&#30693;&#36947;&#30340;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#19987;&#23478;&#20154;&#31867;&#30693;&#35782;&#20197;&#20551;&#35774;&#30340;&#24418;&#24335;&#65292;&#26356;&#24555;&#22320;&#23558;&#36125;&#21494;&#26031;&#25628;&#32034;&#24341;&#23548;&#21040;&#26377;&#24076;&#26395;&#30340;&#21270;&#23398;&#31354;&#38388;&#21306;&#22495;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#20174;&#29616;&#26377;&#23454;&#39564;&#27979;&#37327;&#24471;&#21040;&#30340;&#28508;&#22312;&#20998;&#24067;&#65292;&#36825;&#23545;&#20110;&#26032;&#30340;&#26410;&#24320;&#21457;&#30340;&#31185;&#23398;&#20219;&#21153;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#26679;&#30340;&#20998;&#24067;&#26080;&#27861;&#25429;&#25417;&#31934;&#32454;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;HypBO&#65292;&#21033;&#29992;&#19987;&#23478;&#20154;&#31867;&#20551;&#35774;&#29983;&#25104;&#25913;&#36827;&#30340;&#26679;&#26412;&#31181;&#23376;&#12290;&#19981;&#22826;&#26377;&#24076;&#26395;&#30340;&#31181;&#23376;&#33258;&#21160;&#25240;&#25187;&#65292;&#32780;&#26377;&#24076;&#26395;&#30340;&#31181;&#23376;&#29992;&#20110;&#22686;&#21152;&#20195;&#29702;&#27169;&#22411;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20855;&#20449;&#24687;&#30340;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotics and automation offer massive accelerations for solving intractable, multivariate scientific problems such as materials discovery, but the available search spaces can be dauntingly large. Bayesian optimization (BO) has emerged as a popular sample-efficient optimization engine, thriving in tasks where no analytic form of the target function/property is known. Here we exploit expert human knowledge in the form of hypotheses to direct Bayesian searches more quickly to promising regions of chemical space. Previous methods have used underlying distributions derived from existing experimental measurements, which is unfeasible for new, unexplored scientific tasks. Also, such distributions cannot capture intricate hypotheses. Our proposed method, which we call HypBO, uses expert human hypotheses to generate an improved seed of samples. Unpromising seeds are automatically discounted, while promising seeds are used to augment the surrogate model data, thus achieving better-informed sampl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21457;&#29616;&#39640;&#38454;&#25277;&#35937;&#26469;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#24182;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.08334</link><description>&lt;p&gt;
&#36890;&#36807;&#21457;&#29616;&#39640;&#38454;&#25277;&#35937;&#26469;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning Logic Programs by Discovering Higher-Order Abstractions. (arXiv:2308.08334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21457;&#29616;&#39640;&#38454;&#25277;&#35937;&#26469;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#24182;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#26032;&#39062;&#30340;&#25277;&#35937;&#23545;&#20110;&#20154;&#31867;&#32423;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21457;&#29616;&#39640;&#38454;&#25277;&#35937;&#65288;&#20363;&#22914;map&#12289;filter&#21644;fold&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65292;&#21363;&#20174;&#31034;&#20363;&#21644;&#32972;&#26223;&#30693;&#35782;&#20013;&#24402;&#32435;&#36923;&#36753;&#31243;&#24207;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#38454;&#37325;&#26500;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#25277;&#35937;&#26469;&#21387;&#32553;&#36923;&#36753;&#31243;&#24207;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#22312;STEVIE&#20013;&#65292;&#23427;&#23558;&#39640;&#38454;&#37325;&#26500;&#38382;&#39064;&#24314;&#27169;&#20026;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#31243;&#24207;&#21512;&#25104;&#21644;&#35270;&#35273;&#25512;&#29702;&#65292;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#27809;&#26377;&#37325;&#26500;&#30456;&#27604;&#65292;STEVIE&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;27%&#24182;&#23558;&#23398;&#20064;&#26102;&#38388;&#20943;&#23569;47%&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;STEVIE&#21487;&#20197;&#21457;&#29616;&#36866;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering novel abstractions is important for human-level AI. We introduce an approach to discover higher-order abstractions, such as map, filter, and fold. We focus on inductive logic programming, which induces logic programs from examples and background knowledge. We introduce the higher-order refactoring problem, where the goal is to compress a logic program by introducing higher-order abstractions. We implement our approach in STEVIE, which formulates the higher-order refactoring problem as a constraint optimisation problem. Our experimental results on multiple domains, including program synthesis and visual reasoning, show that, compared to no refactoring, STEVIE can improve predictive accuracies by 27% and reduce learning times by 47%. We also show that STEVIE can discover abstractions that transfer to different domains
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#21018;&#20307;&#35282;&#33394;&#20223;&#30495;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#33258;&#36866;&#24212;&#21508;&#31181;&#29615;&#22659;&#21464;&#21270;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.07491</link><description>&lt;p&gt;
&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#23545;&#21333;&#21018;&#20307;&#35282;&#33394;&#30340;&#33258;&#36866;&#24212;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Adaptive Tracking of a Single-Rigid-Body Character in Various Environments. (arXiv:2308.07491v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#21018;&#20307;&#35282;&#33394;&#20223;&#30495;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#33258;&#36866;&#24212;&#21508;&#31181;&#29615;&#22659;&#21464;&#21270;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;DeepMimic&#30340;&#24341;&#20837;&#20197;&#26469;&#65292;&#21518;&#32493;&#30740;&#31350;&#19968;&#30452;&#33268;&#21147;&#20110;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#25193;&#23637;&#27169;&#25311;&#21160;&#20316;&#30340;&#33539;&#30068;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26367;&#20195;&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#21333;&#21018;&#20307;&#35282;&#33394;&#20223;&#30495;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#21033;&#29992;&#36136;&#24515;&#21160;&#21147;&#23398;&#27169;&#22411;&#65288;CDM&#65289;&#23558;&#20840;&#36523;&#35282;&#33394;&#34920;&#31034;&#20026;&#21333;&#21018;&#20307;&#65288;SRB&#65289;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#36319;&#36394;&#21442;&#32771;&#21160;&#20316;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#26410;&#35266;&#27979;&#29615;&#22659;&#21464;&#21270;&#21644;&#25511;&#21046;&#22120;&#36716;&#25442;&#30340;&#31574;&#30053;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23398;&#20064;&#12290;&#30001;&#20110;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#38477;&#32500;&#65292;&#23398;&#20064;&#36807;&#31243;&#20855;&#26377;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#26368;&#32456;&#30340;&#20840;&#36523;&#21160;&#20316;&#20197;&#29289;&#29702;&#21512;&#29702;&#30340;&#26041;&#24335;&#22522;&#20110;&#27169;&#25311;SRB&#35282;&#33394;&#30340;&#29366;&#24577;&#36827;&#34892;&#36816;&#21160;&#29983;&#25104;&#12290;SRB&#20223;&#30495;&#34987;&#21046;&#23450;&#20026;&#19968;&#20010;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65292;&#31574;&#30053;&#36755;&#20986;&#19968;&#20010;&#21160;&#20316;&#65292;&#20801;&#35768;&#35282;&#33394;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the introduction of DeepMimic [Peng et al. 2018], subsequent research has focused on expanding the repertoire of simulated motions across various scenarios. In this study, we propose an alternative approach for this goal, a deep reinforcement learning method based on the simulation of a single-rigid-body character. Using the centroidal dynamics model (CDM) to express the full-body character as a single rigid body (SRB) and training a policy to track a reference motion, we can obtain a policy that is capable of adapting to various unobserved environmental changes and controller transitions without requiring any additional learning. Due to the reduced dimension of state and action space, the learning process is sample-efficient. The final full-body motion is kinematically generated in a physically plausible way, based on the state of the simulated SRB character. The SRB simulation is formulated as a quadratic programming (QP) problem, and the policy outputs an action that allows th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#35780;&#20998;&#26469;&#33719;&#24471;&#20154;&#31867;&#25351;&#23548;&#65292;&#35813;&#26041;&#27861;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#26679;&#26412;&#36712;&#36857;&#30340;&#35780;&#20272;&#26469;&#36827;&#34892;&#23398;&#20064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#21644;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.16348</link><description>&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rating-based Reinforcement Learning. (arXiv:2307.16348v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#35780;&#20998;&#26469;&#33719;&#24471;&#20154;&#31867;&#25351;&#23548;&#65292;&#35813;&#26041;&#27861;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#26679;&#26412;&#36712;&#36857;&#30340;&#35780;&#20272;&#26469;&#36827;&#34892;&#23398;&#20064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#21644;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35780;&#20998;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#35780;&#20998;&#26469;&#33719;&#24471;&#20154;&#31867;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#20559;&#22909;&#21644;&#22522;&#20110;&#25490;&#21517;&#30340;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#20154;&#31867;&#23545;&#26679;&#26412;&#36712;&#36857;&#30340;&#35780;&#20272;&#32780;&#19981;&#26159;&#23545;&#26679;&#26412;&#23545;&#30340;&#30456;&#23545;&#27604;&#36739;&#12290;&#22522;&#20110;&#35780;&#20998;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#19968;&#20010;&#26032;&#30340;&#20154;&#31867;&#35780;&#20998;&#39044;&#27979;&#27169;&#22411;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#31867;&#25439;&#22833;&#20989;&#25968;&#19978;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#35780;&#20998;&#21644;&#30495;&#23454;&#20154;&#31867;&#35780;&#20998;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#26032;&#30340;&#22522;&#20110;&#35780;&#20998;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops a novel rating-based reinforcement learning approach that uses human ratings to obtain human guidance in reinforcement learning. Different from the existing preference-based and ranking-based reinforcement learning paradigms, based on human relative preferences over sample pairs, the proposed rating-based reinforcement learning approach is based on human evaluation of individual trajectories without relative comparisons between sample pairs. The rating-based reinforcement learning approach builds on a new prediction model for human ratings and a novel multi-class loss function. We conduct several experimental studies based on synthetic ratings and real human ratings to evaluate the effectiveness and benefits of the new rating-based reinforcement learning approach.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28548;&#28165;softmax&#20989;&#25968;&#19982;Boltzmann&#31639;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#23618;&#20855;&#26377;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#33258;&#27880;&#24847;&#21147;&#20855;&#22791;&#23436;&#20840;&#25429;&#33719;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#65292;&#21333;&#23618;Transformer&#23545;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#21333;&#23618;&#33258;&#27880;&#24847;&#21147;Transformer&#26159;&#32039;&#20945;&#22495;&#19978;&#36830;&#32493;&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.14023</link><description>&lt;p&gt;
&#21333;&#23618;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#20351;&#29992;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#26159;&#21542;&#26159;&#36890;&#29992;&#36924;&#36817;&#22120;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?. (arXiv:2307.14023v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14023
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28548;&#28165;softmax&#20989;&#25968;&#19982;Boltzmann&#31639;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#23618;&#20855;&#26377;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#33258;&#27880;&#24847;&#21147;&#20855;&#22791;&#23436;&#20840;&#25429;&#33719;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#65292;&#21333;&#23618;Transformer&#23545;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#21333;&#23618;&#33258;&#27880;&#24847;&#21147;Transformer&#26159;&#32039;&#20945;&#22495;&#19978;&#36830;&#32493;&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20851;&#20110;Transformer&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#30340;&#20998;&#26512;&#35201;&#27714;&#36807;&#28145;&#30340;&#23618;&#25968;&#26469;&#23454;&#29616;&#25968;&#25454;&#30340;&#35760;&#24518;&#65292;&#23548;&#33268;&#19982;&#23454;&#38469;&#20351;&#29992;&#30340;Transformer&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#23558;softmax&#20989;&#25968;&#35299;&#37322;&#20026;hardmax&#20989;&#25968;&#30340;&#36924;&#36817;&#12290;&#36890;&#36807;&#28548;&#28165;softmax&#20989;&#25968;&#19982;Boltzmann&#31639;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#23618;&#20855;&#26377;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#33258;&#27880;&#24847;&#21147;&#20855;&#22791;&#23436;&#20840;&#25429;&#33719;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21333;&#23618;Transformer&#23545;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#24182;&#19988;&#30001;&#20004;&#20010;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26500;&#25104;&#30340;&#21333;&#23618;&#33258;&#27880;&#24847;&#21147;Transformer&#26159;&#32039;&#20945;&#22495;&#19978;&#36830;&#32493;&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice. This is primarily due to the interpretation of the softmax function as an approximation of the hardmax function. By clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence. As a consequence, we show that single-layer Transformer has a memorization capacity for finite samples, and that Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous functions on a compact domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;AI&#20195;&#29702;&#26041;&#27861;REX&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#22870;&#21169;&#23618;&#21644;&#31867;&#20284;&#20110;UCB&#20998;&#25968;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;AI&#20195;&#29702;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#31163;&#32447;&#34892;&#20026;&#21033;&#29992;&#21644;&#19982;&#22522;&#30784;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.08962</link><description>&lt;p&gt;
REX: &#24555;&#36895;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#22686;&#24378;&#22411;AI&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
REX: Rapid Exploration and eXploitation for AI Agents. (arXiv:2307.08962v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;AI&#20195;&#29702;&#26041;&#27861;REX&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#22870;&#21169;&#23618;&#21644;&#31867;&#20284;&#20110;UCB&#20998;&#25968;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;AI&#20195;&#29702;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#31163;&#32447;&#34892;&#20026;&#21033;&#29992;&#21644;&#19982;&#22522;&#30784;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;AI&#20195;&#29702;&#26041;&#27861;&#65292;&#31216;&#20026;REX&#12290;&#29616;&#26377;&#30340;AutoGPT&#39118;&#26684;&#25216;&#26415;&#23384;&#22312;&#19968;&#20123;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#22914;&#23545;&#20110;&#20915;&#31574;&#30340;&#31934;&#30830;&#25551;&#36848;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#20197;&#21450;&#32570;&#20047;&#31867;&#20284;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;(Reinforcement Learning&#65292;RL)&#20013;&#30340;&#23581;&#35797;&#21644;&#22833;&#36133;&#31243;&#24207;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#12290;REX&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#22870;&#21169;&#23618;&#65292;&#24182;&#38598;&#25104;&#20102;&#31867;&#20284;&#20110;&#19978;&#38480;&#32622;&#20449;&#30028;&#38480;(UCB)&#20998;&#25968;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;AI&#20195;&#29702;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#26159;&#21487;&#20197;&#21033;&#29992;&#26469;&#33258;&#26085;&#24535;&#30340;&#31163;&#32447;&#34892;&#20026;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#24494;&#35843;&#12290;&#36890;&#36807;&#19982;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;&#24605;&#32500;&#38142;(CoT)&#21644;&#35268;&#21010;&#25512;&#29702;(RAP)&#65289;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#22522;&#20110;REX&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#36229;&#36807;&#20102;&#36825;&#20123;&#29616;&#26377;&#25216;&#26415;&#25152;&#21462;&#24471;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an enhanced approach for Rapid Exploration and eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional Reinforcement Learning (RL). REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance. This approach has the advantage of enabling the utilization of offline behaviors from logs and allowing seamless integration with existing foundation models while it does not require any model fine-tuning. Through comparative analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP), REX-based methods demonstrate comparable performance and, in certain cases, even surpass the results achieved by these existing techniqu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SelSync&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#21516;&#27493;&#65292;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26102;&#38388;&#24320;&#38144;&#12290;&#35813;&#26041;&#27861;&#26681;&#25454;&#27599;&#19968;&#27493;&#30340;&#37325;&#35201;&#24615;&#21160;&#24577;&#36873;&#25321;&#26159;&#21542;&#36827;&#34892;&#36890;&#20449;&#65292;&#36798;&#21040;&#20102;&#19982;&#25209;&#37327;&#21516;&#27493;&#24182;&#34892;&#65288;BSP&#65289;&#30456;&#21516;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07950</link><description>&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#24615;&#21516;&#27493;&#21152;&#36895;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Accelerating Distributed ML Training via Selective Synchronization. (arXiv:2307.07950v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SelSync&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#21516;&#27493;&#65292;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26102;&#38388;&#24320;&#38144;&#12290;&#35813;&#26041;&#27861;&#26681;&#25454;&#27599;&#19968;&#27493;&#30340;&#37325;&#35201;&#24615;&#21160;&#24577;&#36873;&#25321;&#26159;&#21542;&#36827;&#34892;&#36890;&#20449;&#65292;&#36798;&#21040;&#20102;&#19982;&#25209;&#37327;&#21516;&#27493;&#24182;&#34892;&#65288;BSP&#65289;&#30456;&#21516;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21516;&#26102;&#22312;&#22810;&#20010;&#24037;&#20316;&#32773;&#19978;&#21551;&#21160;&#65292;&#24182;&#20351;&#29992;&#25209;&#37327;&#21516;&#27493;&#24182;&#34892;&#65288;BSP&#65289;&#35757;&#32451;&#20013;&#30340;&#27599;&#20010;&#27493;&#39588;&#32858;&#21512;&#20854;&#26412;&#22320;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32858;&#21512;&#30340;&#36890;&#20449;&#25104;&#26412;&#36739;&#39640;&#65292;BSP&#26080;&#27861;&#32447;&#24615;&#25193;&#23637;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#24320;&#38144;&#65292;FedAvg&#21644;SSP&#31561;&#26367;&#20195;&#26041;&#26696;&#35201;&#20040;&#38477;&#20302;&#21516;&#27493;&#39057;&#29575;&#65292;&#35201;&#20040;&#23436;&#20840;&#28040;&#38500;&#21516;&#27493;&#65292;&#36890;&#24120;&#20197;&#38477;&#20302;&#26368;&#32456;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelSync&#30340;&#23454;&#29992;&#12289;&#20302;&#24320;&#38144;&#30340;DNN&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#26681;&#25454;&#27599;&#19968;&#27493;&#30340;&#37325;&#35201;&#24615;&#21160;&#24577;&#36873;&#25321;&#26159;&#21542;&#36827;&#34892;&#36890;&#20449;&#12290;&#20316;&#20026;\texttt{SelSync}&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22312;\textit {&#21322;&#21516;&#27493;}&#35757;&#32451;&#29615;&#22659;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#26368;&#22810;14&#65285;&#30340;&#21516;&#26102;&#65292;&#36798;&#21040;&#19982;BSP&#30456;&#21516;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In distributed training, deep neural networks (DNNs) are launched over multiple workers concurrently and aggregate their local updates on each step in bulk-synchronous parallel (BSP) training. However, BSP does not linearly scale-out due to high communication cost of aggregation. To mitigate this overhead, alternatives like Federated Averaging (FedAvg) and Stale-Synchronous Parallel (SSP) either reduce synchronization frequency or eliminate it altogether, usually at the cost of lower final accuracy. In this paper, we present \texttt{SelSync}, a practical, low-overhead method for DNN training that dynamically chooses to incur or avoid communication at each step either by calling the aggregation op or applying local updates based on their significance. We propose various optimizations as part of \texttt{SelSync} to improve convergence in the context of \textit{semi-synchronous} training. Our system converges to the same or better accuracy than BSP while reducing training time by up to 14
&lt;/p&gt;</description></item><item><title>UTrans&#26159;&#19968;&#31181;&#32479;&#19968;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#33021;&#26816;&#27979;&#21487;&#36716;&#31227;&#21464;&#37327;&#21644;&#28304;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00238</link><description>&lt;p&gt;
&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#30340;&#32479;&#19968;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unified Transfer Learning Models for High-Dimensional Linear Regression. (arXiv:2307.00238v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00238
&lt;/p&gt;
&lt;p&gt;
UTrans&#26159;&#19968;&#31181;&#32479;&#19968;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#33021;&#26816;&#27979;&#21487;&#36716;&#31227;&#21464;&#37327;&#21644;&#28304;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#24403;&#30446;&#26631;&#25968;&#25454;&#31232;&#32570;&#32780;&#28304;&#25968;&#25454;&#20805;&#36275;&#65292;&#25110;&#32773;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#36716;&#31227;&#23398;&#20064;&#22312;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#32479;&#19968;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;UTrans&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26816;&#27979;&#21487;&#36716;&#31227;&#21464;&#37327;&#21644;&#28304;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20272;&#35745;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#30028;&#38480;&#20302;&#20110;&#20165;&#26377;&#30446;&#26631;&#25968;&#25454;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#20551;&#35774;&#26816;&#39564;&#25552;&#20986;&#20102;&#19968;&#31181;&#28304;&#25968;&#25454;&#26816;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#25490;&#38500;&#19981;&#21487;&#36716;&#31227;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;UTrans&#19982;&#29616;&#26377;&#31639;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;UTrans&#22312;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#32654;&#22269;&#20195;&#38469;&#27969;&#21160;&#25968;&#25454;&#65292;&#24182;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#19982;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning plays a key role in modern data analysis when: (1) the target data are scarce but the source data are sufficient; (2) the distributions of the source and target data are heterogeneous. This paper develops an interpretable unified transfer learning model, termed as UTrans, which can detect both transferable variables and source data. More specifically, we establish the estimation error bounds and prove that our bounds are lower than those with target data only. Besides, we propose a source detection algorithm based on hypothesis testing to exclude the nontransferable data. We evaluate and compare UTrans to the existing algorithms in multiple experiments. It is shown that UTrans attains much lower estimation and prediction errors than the existing methods, while preserving interpretability. We finally apply it to the US intergenerational mobility data and compare our proposed algorithms to the classical machine learning algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#26469;&#39044;&#27979;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniXcoder&#20248;&#20110;CodeBERT&#12290;</title><link>http://arxiv.org/abs/2307.00012</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#39044;&#27979;&#26131;&#20986;&#38169;&#27979;&#35797;&#20462;&#22797;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Black-Box Prediction of Flaky Test Fix Categories Using Language Models. (arXiv:2307.00012v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#26469;&#39044;&#27979;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniXcoder&#20248;&#20110;CodeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26131;&#20986;&#38169;&#27979;&#35797;&#20250;&#22312;&#30456;&#21516;&#36719;&#20214;&#29256;&#26412;&#30340;&#27979;&#35797;&#19979;&#38750;&#30830;&#23450;&#24615;&#22320;&#36890;&#36807;&#25110;&#22833;&#36133;&#65292;&#24341;&#36215;&#28151;&#20081;&#24182;&#28010;&#36153;&#24320;&#21457;&#32773;&#26102;&#38388;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#34987;&#29992;&#20110;&#39044;&#27979;&#26131;&#20986;&#38169;&#24615;&#21450;&#20854;&#26681;&#26412;&#21407;&#22240;&#65292;&#20294;&#22312;&#25552;&#20379;&#20462;&#22797;&#25903;&#25345;&#26041;&#38754;&#20173;&#26377;&#36739;&#23569;&#24037;&#20316;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;13&#20010;&#20462;&#22797;&#31867;&#21035;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#34429;&#28982;&#22312;&#24403;&#21069;&#38454;&#27573;&#20934;&#30830;&#39044;&#27979;&#20462;&#22797;&#26412;&#36523;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#20294;&#36825;&#20123;&#31867;&#21035;&#25552;&#20379;&#20102;&#20851;&#20110;&#38656;&#35201;&#26816;&#26597;&#30340;&#27979;&#35797;&#20195;&#30721;&#37096;&#20998;&#30340;&#31934;&#30830;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;CodeBERT&#21644;UniXcoder&#65292;&#20854;&#36755;&#20986;&#32463;&#36807;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#25110;&#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;Few Shot Learning&#65288;FSL&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniXcoder&#22312;&#27491;&#30830;&#39044;&#27979;&#22823;&#22810;&#25968;&#20462;&#22797;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;CodeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;
Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting developer time. While machine learning models have been used to predict flakiness and its root causes, there is less work on providing support to fix the problem. To address this gap, we propose a framework that automatically generates labeled datasets for 13 fix categories and train models to predict the fix category of a flaky test by analyzing the test code only. Though it is unrealistic at this stage to accurately predict the fix itself, the categories provide precise guidance about what part of the test code to look at. Our approach is based on language models, namely CodeBERT and UniXcoder, whose output is fine-tuned with a Feed Forward Neural Network (FNN) or a Siamese Network-based Few Shot Learning (FSL). Our experimental results show that UniXcoder outperforms CodeBERT, in correctly predicting most of the categories of fixes a dev
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.15749</link><description>&lt;p&gt;
&#20309;&#21435;&#20309;&#20174;&#65306;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#30340;&#25968;&#23383;&#30828;&#20214;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15749
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#22312;&#28085;&#30422;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#31454;&#20105;&#21147;&#65307;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#25928;&#29575;&#20026;&#20195;&#20215;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#33021;&#21147;&#12290;&#29983;&#29289;&#33041;&#30340;&#21151;&#32791;&#25928;&#29575;&#36229;&#36807;&#20219;&#20309;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65307;&#22240;&#27492;&#65292;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#35797;&#22270;&#27169;&#20223;&#33041;&#37096;&#25805;&#20316;&#65292;&#20363;&#22914;&#22522;&#20110;&#33033;&#20914;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;DL&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#33041;&#37096;&#26377;&#35832;&#22914;&#39640;&#25928;&#30340;&#20449;&#24687;&#20256;&#36755;&#12289;&#23494;&#38598;&#30340;&#31070;&#32463;&#20803;&#36830;&#25509;&#21644;&#35745;&#31639;&#19982;&#23384;&#20648;&#30340;&#20849;&#21516;&#20301;&#32622;&#31561;&#20248;&#21183;&#65292;&#20294;&#21487;&#29992;&#30340;&#29983;&#29289;&#22522;&#24213;&#20005;&#37325;&#38480;&#21046;&#20102;&#29983;&#29289;&#22823;&#33041;&#30340;&#36827;&#21270;&#12290;&#30005;&#23376;&#30828;&#20214;&#27809;&#26377;&#30456;&#21516;&#30340;&#32422;&#26463;&#65307;&#22240;&#27492;&#65292;&#34429;&#28982;&#24314;&#27169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21487;&#33021;&#25581;&#31034;&#20102;&#19968;&#20010;&#35868;&#39064;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning models scale, they become increasingly competitive from domains spanning computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms the one of any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, po
&lt;/p&gt;</description></item><item><title>SPRINT &#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#37325;&#26631;&#35760;&#21450;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#39044;&#35757;&#32451;&#25152;&#38656;&#30340;&#20154;&#21147;&#65292;&#21516;&#26102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#26356;&#20016;&#23500;&#30340;&#25216;&#33021;&#24211;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.11886</link><description>&lt;p&gt;
SPRINT&#65306;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196; relabeling &#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#31574;&#30053;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling. (arXiv:2306.11886v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11886
&lt;/p&gt;
&lt;p&gt;
SPRINT &#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#37325;&#26631;&#35760;&#21450;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#39044;&#35757;&#32451;&#25152;&#38656;&#30340;&#20154;&#21147;&#65292;&#21516;&#26102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#26356;&#20016;&#23500;&#30340;&#25216;&#33021;&#24211;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#26426;&#22120;&#20154;&#31574;&#30053;&#24182;&#36171;&#20104;&#20016;&#23500;&#30340;&#25216;&#33021;&#38598;&#21512;&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#19979;&#28216;&#20219;&#21153;&#30340;&#23398;&#20064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23450;&#20041;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20294;&#36825;&#38656;&#35201;&#20154;&#20026;&#22320;&#27880;&#37322;&#25968;&#21313;&#19975;&#20010;&#25351;&#20196;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SPRINT&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#22823;&#22823;&#20943;&#23569;&#39044;&#35757;&#32451;&#22810;&#26679;&#30340;&#25216;&#33021;&#25152;&#38656;&#30340;&#20154;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#26680;&#24515;&#24819;&#27861;&#26469;&#33258;&#21160;&#25193;&#23637;&#22522;&#30784;&#39044;&#35757;&#32451;&#20219;&#21153;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#25351;&#20196;&#37325;&#26631;&#35760;&#21644;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20132;&#21449;&#36712;&#36857;&#25216;&#33021;&#38142;&#25509;&#12290;&#22240;&#27492;&#65292;SPRINT &#39044;&#35757;&#32451;&#21487;&#20197;&#20026;&#26426;&#22120;&#20154;&#35013;&#22791;&#26356;&#20016;&#23500;&#30340;&#25216;&#33021;&#24211;&#12290;&#22312;&#23478;&#24237;&#27169;&#25311;&#22120;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#21416;&#25151;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPRINT &#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training robot policies with a rich set of skills can substantially accelerate the learning of downstream tasks. Prior works have defined pre-training tasks via natural language instructions, but doing so requires tedious human annotation of hundreds of thousands of instructions. Thus, we propose SPRINT, a scalable offline policy pre-training approach which substantially reduces the human effort needed for pre-training a diverse set of skills. Our method uses two core ideas to automatically expand a base set of pre-training tasks: instruction relabeling via large language models and cross-trajectory skill chaining through offline reinforcement learning. As a result, SPRINT pre-training equips robots with a much richer repertoire of skills. Experimental results in a household simulator and on a real robot kitchen manipulation task show that SPRINT leads to substantially faster learning of new long-horizon tasks than previous pre-training approaches. Website at https://clvrai.com/spr
&lt;/p&gt;</description></item><item><title>AdaStop&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#32452;&#24207;&#21015;&#27979;&#35797;&#30340;&#26032;&#32479;&#35745;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#27604;&#36739;&#22810;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;&#23454;&#39564;&#32467;&#26524;&#21487;&#22797;&#21046;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10882</link><description>&lt;p&gt;
AdaStop&#65306;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#27604;&#36739;&#30340;&#39640;&#25928;&#21487;&#38752;&#24207;&#21015;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AdaStop: sequential testing for efficient and reliable comparisons of Deep RL Agents. (arXiv:2306.10882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10882
&lt;/p&gt;
&lt;p&gt;
AdaStop&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#32452;&#24207;&#21015;&#27979;&#35797;&#30340;&#26032;&#32479;&#35745;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#27604;&#36739;&#22810;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;&#23454;&#39564;&#32467;&#26524;&#21487;&#22797;&#21046;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#39564;&#32467;&#26524;&#30340;&#21487;&#22797;&#29616;&#24615;&#21463;&#21040;&#36136;&#30097;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#21487;&#22797;&#29616;&#24615;&#21361;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#21487;&#38752;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#22810;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#30001;&#20110;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#19968;&#27425;&#25191;&#34892;&#24615;&#33021;&#26159;&#38543;&#26426;&#30340;&#65292;&#25152;&#20197;&#38656;&#35201;&#36827;&#34892;&#29420;&#31435;&#30340;&#22810;&#27425;&#25191;&#34892;&#26469;&#31934;&#30830;&#35780;&#20272;&#23427;&#12290;&#24403;&#27604;&#36739;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26102;&#65292;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#38656;&#35201;&#36827;&#34892;&#22810;&#23569;&#27425;&#25191;&#34892;&#65292;&#24182;&#19988;&#22914;&#20309;&#30830;&#20445;&#36825;&#26679;&#27604;&#36739;&#30340;&#32467;&#26524;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#30340;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#20351;&#29992;&#23569;&#20110;5&#20010;&#29420;&#31435;&#25191;&#34892;&#26469;&#27604;&#36739;&#31639;&#27861;&#65306;&#25105;&#20204;&#35748;&#20026;&#36825;&#36890;&#24120;&#26159;&#19981;&#22815;&#30340;&#12290;&#32780;&#19988;&#65292;&#24403;&#21516;&#26102;&#27604;&#36739;&#20960;&#20010;&#31639;&#27861;&#26102;&#65292;&#27599;&#20010;&#27604;&#36739;&#30340;&#35823;&#24046;&#37117;&#20250;&#32047;&#31215;&#65292;&#24517;&#39035;&#37319;&#29992;&#22810;&#37325;&#27979;&#35797;&#31243;&#24207;&#26469;&#32771;&#34385;&#36825;&#20123;&#35823;&#24046;&#65292;&#20197;&#32500;&#25345;&#20302;&#35823;&#24046;&#20445;&#35777;&#12290;&#20026;&#20102;&#20197;&#32479;&#35745;&#23398;&#19978;&#30340;&#21487;&#38752;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AdaStop&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#32452;&#24207;&#21015;&#27979;&#35797;&#30340;&#26032;&#32479;&#35745;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reproducibility of many experimental results in Deep Reinforcement Learning (RL) is under question. To solve this reproducibility crisis, we propose a theoretically sound methodology to compare multiple Deep RL algorithms. The performance of one execution of a Deep RL algorithm is random so that independent executions are needed to assess it precisely. When comparing several RL algorithms, a major question is how many executions must be made and how can we assure that the results of such a comparison is theoretically sound. Researchers in Deep RL often use less than 5 independent executions to compare algorithms: we claim that this is not enough in general. Moreover, when comparing several algorithms at once, the error of each comparison accumulates and must be taken into account with a multiple tests procedure to preserve low error guarantees. To address this problem in a statistically sound way, we introduce AdaStop, a new statistical test based on multiple group sequential tests
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#29255;&#23494;&#25991;&#19978;&#30340;&#21516;&#24577;&#21152;&#23494;&#25968;&#25454;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22312;&#22823;&#23610;&#23544;&#21644;&#22810;&#36890;&#36947;&#22270;&#20687;&#19978;&#35780;&#20272;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#24182;&#31616;&#21270;&#20102;&#22810;&#36335;&#22797;&#29992;&#22270;&#20687;&#26684;&#24335;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#27491;&#21017;&#21270;&#29616;&#26377;&#27169;&#22411;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09189</link><description>&lt;p&gt;
&#22312;&#36890;&#36807;&#20998;&#29255;&#23494;&#25991;&#19978;&#30340;&#21516;&#24577;&#21152;&#23494;&#25968;&#25454;&#19978;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
High-Resolution Convolutional Neural Networks on Homomorphically Encrypted Data via Sharding Ciphertexts. (arXiv:2306.09189v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09189
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#29255;&#23494;&#25991;&#19978;&#30340;&#21516;&#24577;&#21152;&#23494;&#25968;&#25454;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22312;&#22823;&#23610;&#23544;&#21644;&#22810;&#36890;&#36947;&#22270;&#20687;&#19978;&#35780;&#20272;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#24182;&#31616;&#21270;&#20102;&#22810;&#36335;&#22797;&#29992;&#22270;&#20687;&#26684;&#24335;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#27491;&#21017;&#21270;&#29616;&#26377;&#27169;&#22411;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#20351;&#29992;&#22522;&#20110;&#21097;&#20313;&#25968;&#31995;&#32479;Cheon-Kim-Kim-Song (RNS-CKKS) &#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#30340;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#19978;&#65292;&#23545;&#21253;&#25324;ResNet-20&#22312;&#20869;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(DCNNs)&#36827;&#34892;&#20102;&#31169;&#23494;&#35780;&#20272;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#22312;&#22270;&#20687;&#19978;&#35780;&#20272;DCNNs&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#26356;&#22823;&#23610;&#23544;&#21644;&#26356;&#22810;&#36890;&#36947;&#30340;&#22270;&#20687;&#65292;&#36229;&#20986;&#20102;&#21333;&#20010;&#23494;&#25991;&#25152;&#33021;&#23384;&#20648;&#30340;&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31616;&#21270;&#21644;&#25913;&#36827;&#20102;&#26368;&#36817;&#24341;&#20837;&#30340;&#22810;&#36335;&#22797;&#29992;&#22270;&#20687;&#26684;&#24335;&#30340;&#25928;&#29575;&#65292;&#35777;&#26126;&#20102;&#21516;&#24577;&#35780;&#20272;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#30340;&#34892;&#20027;&#24207;&#30697;&#38453;&#25171;&#21253;&#65292;&#24182;&#23548;&#33268;&#21152;&#23494;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;$4.6-6.5\times$&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#29616;&#26377;DCNN&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#23545;&#39640;&#20998;&#36776;&#29575;&#30340;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#21516;&#24577;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;$80.2\%$&#30340;top-1&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23454;&#29616;&#20102;&#23545;CNN&#36827;&#34892;&#21516;&#24577;&#35780;&#20272;&#30340;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;
Recently, Deep Convolutional Neural Networks (DCNNs) including the ResNet-20 architecture have been privately evaluated on encrypted, low-resolution data with the Residue-Number-System Cheon-Kim-Kim-Song (RNS-CKKS) homomorphic encryption scheme. We extend methods for evaluating DCNNs on images with larger dimensions and many channels, beyond what can be stored in single ciphertexts. Additionally, we simplify and improve the efficiency of the recently introduced multiplexed image format, demonstrating that homomorphic evaluation can work with standard, row-major matrix packing and results in encrypted inference time speedups by $4.6-6.5\times$. We also show how existing DCNN models can be regularized during the training process to further improve efficiency and accuracy. These techniques are applied to homomorphically evaluate a DCNN with high accuracy on the high-resolution ImageNet dataset, achieving $80.2\%$ top-1 accuracy. We also achieve an accuracy of homomorphically evaluated CNN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ECG&#20449;&#21495;&#21512;&#25104;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;DiffECG&#65292;&#33021;&#22815;&#28085;&#30422;&#19977;&#31181;&#24773;&#24418;&#65292;&#24182;&#19988;&#26159;ECG&#21512;&#25104;&#30340;&#31532;&#19968;&#20010;&#24191;&#20041;&#26465;&#20214;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20248;&#20110;&#20854;&#20182;ECG&#29983;&#25104;&#27169;&#22411;&#24182;&#21487;&#25552;&#39640;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01875</link><description>&lt;p&gt;
DiffECG&#65306;ECG&#20449;&#21495;&#21512;&#25104;&#30340;&#19968;&#33324;&#21270;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffECG: A Generalized Probabilistic Diffusion Model for ECG Signals Synthesis. (arXiv:2306.01875v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ECG&#20449;&#21495;&#21512;&#25104;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;DiffECG&#65292;&#33021;&#22815;&#28085;&#30422;&#19977;&#31181;&#24773;&#24418;&#65292;&#24182;&#19988;&#26159;ECG&#21512;&#25104;&#30340;&#31532;&#19968;&#20010;&#24191;&#20041;&#26465;&#20214;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20248;&#20110;&#20854;&#20182;ECG&#29983;&#25104;&#27169;&#22411;&#24182;&#21487;&#25552;&#39640;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;ECG&#20449;&#21495;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#20013;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25968;&#25454;&#22686;&#24378;&#35299;&#20915;&#26041;&#26696;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;ECG&#21512;&#25104;&#26041;&#27861;,&#35206;&#30422;&#20102;&#19977;&#31181;&#24773;&#24418;&#65306;&#24515;&#36339;&#29983;&#25104;&#12289;&#37096;&#20998;&#20449;&#21495;&#23436;&#25104;&#21644;&#23436;&#25972;&#24515;&#36339;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;ECG&#21512;&#25104;&#30340;&#31532;&#19968;&#20010;&#24191;&#20041;&#26465;&#20214;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#21508;&#31181;ECG&#30456;&#20851;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;ECG&#29983;&#25104;&#27169;&#22411;&#24182;&#21487;&#20197;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep generative models have gained attention as a promising data augmentation solution for heart disease detection using deep learning approaches applied to ECG signals. In this paper, we introduce a novel approach based on denoising diffusion probabilistic models for ECG synthesis that covers three scenarios: heartbeat generation, partial signal completion, and full heartbeat forecasting. Our approach represents the first generalized conditional approach for ECG synthesis, and our experimental results demonstrate its effectiveness for various ECG-related tasks. Moreover, we show that our approach outperforms other state-of-the-art ECG generative models and can enhance the performance of state-of-the-art classifiers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Weisfeiler-Leman (WL)&#31639;&#27861;&#30340;&#23616;&#37096;&#29256;&#26412;&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#35745;&#25968;&#38382;&#39064;&#24182;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#65292;&#20063;&#32473;&#20986;&#20102;&#19968;&#20123;&#26102;&#38388;&#21644;&#31354;&#38388;&#25928;&#29575;&#26356;&#39640;&#30340;$k-$WL&#21464;&#20307;&#21644;&#20998;&#35010;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.19659</link><description>&lt;p&gt;
&#21033;&#29992;&#23616;&#37096;&#21270;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Expressivity of Graph Neural Networks using Localization. (arXiv:2305.19659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Weisfeiler-Leman (WL)&#31639;&#27861;&#30340;&#23616;&#37096;&#29256;&#26412;&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#35745;&#25968;&#38382;&#39064;&#24182;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#65292;&#20063;&#32473;&#20986;&#20102;&#19968;&#20123;&#26102;&#38388;&#21644;&#31354;&#38388;&#25928;&#29575;&#26356;&#39640;&#30340;$k-$WL&#21464;&#20307;&#21644;&#20998;&#35010;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Weisfeiler-Leman (WL)&#31639;&#27861;&#30340;&#23616;&#37096;&#29256;&#26412;&#65292;&#26088;&#22312;&#22686;&#21152;&#34920;&#36798;&#33021;&#21147;&#24182;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#23376;&#22270;&#35745;&#25968;&#38382;&#39064;&#65292;&#24182;&#20026;&#20219;&#24847;$k$&#32473;&#20986;$k-$WL&#30340;&#23616;&#37096;&#29256;&#26412;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;Local $k-$WL&#30340;&#20316;&#29992;&#65292;&#24182;&#35777;&#26126;&#20854;&#27604;$k-$WL&#26356;&#20855;&#34920;&#29616;&#21147;&#65292;&#24182;&#19988;&#33267;&#22810;&#19982;$(k+1)-$WL&#19968;&#26679;&#20855;&#26377;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20123;&#27169;&#24335;&#30340;&#29305;&#24449;&#65292;&#22914;&#26524;&#20004;&#20010;&#22270;&#26159;Local $k-$WL&#31561;&#20215;&#30340;&#65292;&#21017;&#23427;&#20204;&#30340;&#23376;&#22270;&#21644;&#35825;&#23548;&#23376;&#22270;&#30340;&#35745;&#25968;&#26159;&#19981;&#21464;&#30340;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;$k-$WL&#30340;&#20004;&#20010;&#21464;&#20307;&#65306;&#23618;$k-$WL&#21644;&#36882;&#24402;$k-$WL&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#25928;&#29575;&#27604;&#22312;&#25972;&#20010;&#22270;&#19978;&#24212;&#29992;$k-$WL&#26356;&#39640;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35010;&#25216;&#26415;&#65292;&#20351;&#29992;$1-$WL&#21363;&#21487;&#20445;&#35777;&#25152;&#26377;&#22823;&#23567;&#19981;&#36229;&#36807;4&#30340;&#35825;&#23548;&#23376;&#22270;&#30340;&#20934;&#30830;&#35745;&#25968;&#12290;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;$k&gt;1$&#36827;&#19968;&#27493;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#36824;&#23558;Local $k-$WL&#30340;&#34920;&#29616;&#21147;&#19982;&#20854;&#20182;GNN&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose localized versions of Weisfeiler-Leman (WL) algorithms in an effort to both increase the expressivity, as well as decrease the computational overhead. We focus on the specific problem of subgraph counting and give localized versions of $k-$WL for any $k$. We analyze the power of Local $k-$WL and prove that it is more expressive than $k-$WL and at most as expressive as $(k+1)-$WL. We give a characterization of patterns whose count as a subgraph and induced subgraph are invariant if two graphs are Local $k-$WL equivalent. We also introduce two variants of $k-$WL: Layer $k-$WL and recursive $k-$WL. These methods are more time and space efficient than applying $k-$WL on the whole graph. We also propose a fragmentation technique that guarantees the exact count of all induced subgraphs of size at most 4 using just $1-$WL. The same idea can be extended further for larger patterns using $k&gt;1$. We also compare the expressive power of Local $k-$WL with other GNN hierarc
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20998;&#26512;&#20102;&#26032;&#25552;&#20986;&#30340;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#22312;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#32531;&#35299;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16573</link><description>&lt;p&gt;
&#25506;&#32034;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#30340;&#26435;&#37325;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Exploring Weight Balancing on Long-Tailed Recognition Problem. (arXiv:2305.16573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16573
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20998;&#26512;&#20102;&#26032;&#25552;&#20986;&#30340;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#22312;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#32531;&#35299;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#25968;&#25454;&#20013;&#30340;&#35782;&#21035;&#38382;&#39064;&#26368;&#36817;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#20998;&#24067;&#36890;&#24120;&#26159;&#25351;&#25968;&#20998;&#24067;&#65292;&#38500;&#38750;&#26377;&#24847;&#22320;&#35843;&#25972;&#26679;&#26412;&#25968;&#37327;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33879;&#21517;&#30340;&#32463;&#20856;&#27491;&#21017;&#21270;&#25216;&#26415;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#12290;&#23613;&#31649;&#20854;&#31616;&#21333;&#24615;&#65292;&#20294;&#24050;&#30693;&#20854;&#23545;&#29616;&#26377;&#21508;&#31181;&#19981;&#21516;&#26041;&#27861;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#20026;&#20160;&#20040;&#36825;&#31181;&#26041;&#27861;&#23545;&#38271;&#23614;&#25968;&#25454;&#26377;&#25928;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#20851;&#27880;&#20102;&#31070;&#32463;&#23849;&#28291;&#21644;&#27599;&#20010;&#35757;&#32451;&#38454;&#27573;&#30340;&#22278;&#38181;&#25928;&#24212;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#20998;&#35299;&#20026;&#30001;&#26435;&#20540;&#34928;&#20943;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#24341;&#36215;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#20013;Fisher&#21028;&#21035;&#27604;&#30340;&#22686;&#21152;&#20197;&#21450;&#30001;&#26435;&#37325;&#34928;&#20943;&#21644;&#31867;&#24179;&#34913;&#27491;&#21017;&#21270;&#24341;&#36215;&#30340;&#38544;&#24335;&#36923;&#36753;&#35843;&#25972;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#25104;&#21151;&#32531;&#35299;&#20102;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#38271;&#23614;&#25968;&#25454;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognition problems in long-tailed data, where the sample size per class is heavily skewed, have recently gained importance because the distribution of the sample size per class in a dataset is generally exponential unless the sample size is intentionally adjusted. Various approaches have been devised to address these problems. Recently, weight balancing, which combines well-known classical regularization techniques with two-stage training, has been proposed. Despite its simplicity, it is known for its high performance against existing methods devised in various ways. However, there is a lack of understanding as to why this approach is effective for long-tailed data. In this study, we analyze the method focusing on neural collapse and cone effect at each training stage and find that it can be decomposed into the increase in Fisher's discriminant ratio of the feature extractor caused by weight decay and cross entropy loss and implicit logit adjustment caused by weight decay and class-b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20004;&#38454;&#27573;&#27169;&#24335;&#32467;&#21512;&#39044;&#20808;&#35745;&#31639;&#22270;&#20687;&#23884;&#20837;&#21644;&#21442;&#32771;&#25991;&#26412;-&#20505;&#36873;&#39033;&#19977;&#20803;&#32452;&#20132;&#20114;&#36873;&#25321;&#30340;&#26041;&#24335;&#36827;&#34892;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20505;&#36873;&#38598;&#37325;&#25490;&#24207;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.16304</link><description>&lt;p&gt;
&#20855;&#26377;&#21452;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20505;&#36873;&#38598;&#37325;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder. (arXiv:2305.16304v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20004;&#38454;&#27573;&#27169;&#24335;&#32467;&#21512;&#39044;&#20808;&#35745;&#31639;&#22270;&#20687;&#23884;&#20837;&#21644;&#21442;&#32771;&#25991;&#26412;-&#20505;&#36873;&#39033;&#19977;&#20803;&#32452;&#20132;&#20114;&#36873;&#25321;&#30340;&#26041;&#24335;&#36827;&#34892;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20505;&#36873;&#38598;&#37325;&#25490;&#24207;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#26088;&#22312;&#25214;&#21040;&#26368;&#21305;&#37197;&#32473;&#23450;&#22810;&#27169;&#24577;&#29992;&#25143;&#26597;&#35810;(&#21253;&#25324;&#21442;&#32771;&#22270;&#20687;&#21644;&#25991;&#26412;&#23545;)&#30340;&#22270;&#20687;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#39044;&#20808;&#35745;&#31639;&#25972;&#20010;&#35821;&#26009;&#24211;&#30340;&#22270;&#20687;&#23884;&#20837;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#23558;&#36825;&#20123;&#23884;&#20837;&#19982;&#32463;&#36807;&#26597;&#35810;&#25991;&#26412;&#20462;&#25913;&#30340;&#21442;&#32771;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#20165;&#36890;&#36807;&#30701;&#25991;&#26412;&#25551;&#36848;&#24341;&#23548;&#20462;&#25913;&#21442;&#32771;&#22270;&#20687;&#23884;&#20837;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#29420;&#31435;&#20110;&#28508;&#22312;&#30340;&#20505;&#36873;&#39033;&#12290;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26159;&#20801;&#35768;&#26597;&#35810;&#21644;&#27599;&#20010;&#21487;&#33021;&#30340;&#20505;&#36873;&#39033;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#21363;&#21442;&#32771;&#25991;&#26412;-&#20505;&#36873;&#39033;&#19977;&#20803;&#32452;&#65292;&#24182;&#20174;&#25972;&#20010;&#38598;&#21512;&#20013;&#36873;&#25321;&#26368;&#20339;&#21305;&#37197;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#26356;&#20855;&#26377;&#21028;&#21035;&#24615;&#65292;&#20294;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#30001;&#20110;&#19981;&#33021;&#39044;&#20808;&#35745;&#31639;&#20505;&#36873;&#23884;&#20837;&#65292;&#22240;&#27492;&#35745;&#31639;&#25104;&#26412;&#26159;&#31105;&#27490;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20004;&#38454;&#27573;&#27169;&#24335;&#32467;&#21512;&#36825;&#20004;&#20010;&#26041;&#26696;&#30340;&#20248;&#28857;
&lt;/p&gt;
&lt;p&gt;
Composed image retrieval aims to find an image that best matches a given multi-modal user query consisting of a reference image and text pair. Existing methods commonly pre-compute image embeddings over the entire corpus and compare these to a reference image embedding modified by the query text at test time. Such a pipeline is very efficient at test time since fast vector distances can be used to evaluate candidates, but modifying the reference image embedding guided only by a short textual description can be difficult, especially independent of potential candidates. An alternative approach is to allow interactions between the query and every possible candidate, i.e., reference-text-candidate triplets, and pick the best from the entire set. Though this approach is more discriminative, for large-scale datasets the computational cost is prohibitive since pre-computation of candidate embeddings is no longer possible. We propose to combine the merits of both schemes using a two-stage mode
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#23545;&#20449;&#21495;&#20316;&#20986;&#21709;&#24212;&#30340;&#33021;&#21147;&#65292;&#20026;&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#20316;&#20026;&#20154;&#24037;&#24418;&#24577;&#21457;&#29983;&#27169;&#22411;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#19988;&#20026;&#23558;&#21160;&#24577;&#34892;&#20026;&#23884;&#20837;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2305.12971</link><description>&lt;p&gt;
&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#33021;&#22815;&#23545;&#20449;&#21495;&#20316;&#20986;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Neural Cellular Automata Can Respond to Signals. (arXiv:2305.12971v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12971
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#23545;&#20449;&#21495;&#20316;&#20986;&#21709;&#24212;&#30340;&#33021;&#21147;&#65292;&#20026;&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#20316;&#20026;&#20154;&#24037;&#24418;&#24577;&#21457;&#29983;&#27169;&#22411;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#19988;&#20026;&#23558;&#21160;&#24577;&#34892;&#20026;&#23884;&#20837;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#65288;NCAs&#65289;&#26159;&#19968;&#31181;&#24418;&#24577;&#21457;&#29983;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#21333;&#20010;&#31181;&#23376;&#32454;&#32990;&#29983;&#38271;&#20986;&#20108;&#32500;&#20154;&#24037;&#29983;&#29289;&#20307;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;NCAs&#21487;&#20197;&#34987;&#35757;&#32451;&#25104;&#23545;&#20449;&#21495;&#20316;&#20986;&#21709;&#24212;&#12290;&#20351;&#29992;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#20449;&#21495;&#65306;&#20869;&#37096;&#65288;&#22522;&#22240;&#32534;&#30721;&#65289;&#20449;&#21495;&#21644;&#22806;&#37096;&#65288;&#29615;&#22659;&#65289;&#20449;&#21495;&#12290;&#20449;&#21495;&#34987;&#21576;&#29616;&#32473;&#21333;&#20010;&#20687;&#32032;&#22312;&#19968;&#20010;&#26102;&#38388;&#27493;&#38271;&#20869;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;NCAs&#33021;&#22815;&#26681;&#25454;&#20869;&#37096;&#20449;&#21495;&#21457;&#23637;&#25104;&#22810;&#31181;&#19981;&#21516;&#24418;&#24577;&#65292;&#24182;&#19988;&#33021;&#22815;&#26681;&#25454;&#22806;&#37096;&#20449;&#21495;&#25913;&#21464;&#39068;&#33394;&#12290;&#24635;&#20307;&#19978;&#65292;&#36825;&#20123;&#32467;&#26524;&#20026;NCAs&#20316;&#20026;&#20154;&#24037;&#24418;&#24577;&#21457;&#29983;&#27169;&#22411;&#30340;&#21457;&#23637;&#36129;&#29486;&#20102;&#65292;&#24182;&#20026;&#23558;&#21160;&#24577;&#34892;&#20026;&#23884;&#20837;NCA&#27169;&#22411;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#36890;&#36807;GitHub&#21487;&#20197;&#33719;&#24471;&#20195;&#30721;&#21644;&#30446;&#26631;&#22270;&#20687;&#65306;https://github.com/jstovold/ALIFE2023
&lt;/p&gt;
&lt;p&gt;
Neural Cellular Automata (NCAs) are a model of morphogenesis, capable of growing two-dimensional artificial organisms from a single seed cell. In this paper, we show that NCAs can be trained to respond to signals. Two types of signal are used: internal (genomically-coded) signals, and external (environmental) signals. Signals are presented to a single pixel for a single timestep.  Results show NCAs are able to grow into multiple distinct forms based on internal signals, and are able to change colour based on external signals. Overall these contribute to the development of NCAs as a model of artificial morphogenesis, and pave the way for future developments embedding dynamic behaviour into the NCA model.  Code and target images are available through GitHub: https://github.com/jstovold/ALIFE2023
&lt;/p&gt;</description></item><item><title>GraVAC&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#35843;&#25972;&#21387;&#32553;&#22240;&#23376;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36827;&#23637;&#21644;&#35780;&#20272;&#19982;&#21387;&#32553;&#30456;&#20851;&#30340;&#26799;&#24230;&#20449;&#24687;&#25439;&#22833;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;GraVAC&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#27169;&#22411;&#25110;&#20854;&#36229;&#21442;&#25968;&#30340;&#20808;&#21069;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#19982;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#27861;&#30456;&#21516;&#25110;&#26356;&#22909;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#22312;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#65292;&#30456;&#23545;&#20110;&#38745;&#24577;&#21387;&#32553;&#23545;&#24212;&#29289;&#65292;GraVAC&#21487;&#20197;&#23558;&#36890;&#20449;&#20943;&#23569;&#39640;&#36798;87&#65285;&#21644;75&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.12201</link><description>&lt;p&gt;
GraVAC&#65306;&#36866;&#24212;&#24615;&#21387;&#32553;&#29992;&#20110;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
GraVAC: Adaptive Compression for Communication-Efficient Distributed DL Training. (arXiv:2305.12201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12201
&lt;/p&gt;
&lt;p&gt;
GraVAC&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#35843;&#25972;&#21387;&#32553;&#22240;&#23376;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36827;&#23637;&#21644;&#35780;&#20272;&#19982;&#21387;&#32553;&#30456;&#20851;&#30340;&#26799;&#24230;&#20449;&#24687;&#25439;&#22833;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;GraVAC&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#27169;&#22411;&#25110;&#20854;&#36229;&#21442;&#25968;&#30340;&#20808;&#21069;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#19982;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#27861;&#30456;&#21516;&#25110;&#26356;&#22909;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#22312;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#65292;&#30456;&#23545;&#20110;&#38745;&#24577;&#21387;&#32553;&#23545;&#24212;&#29289;&#65292;GraVAC&#21487;&#20197;&#23558;&#36890;&#20449;&#20943;&#23569;&#39640;&#36798;87&#65285;&#21644;75&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#65288;DDP&#65289;&#35757;&#32451;&#36890;&#36807;&#22810;&#20010;&#35774;&#22791;&#22312;&#25968;&#25454;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#24182;&#32858;&#21512;&#26356;&#26032;&#26469;&#25552;&#39640;&#24212;&#29992;&#31243;&#24207;&#30340;&#25972;&#20307;&#21534;&#21520;&#37327;&#12290;&#27599;&#27425;&#36845;&#20195;&#30340;&#21608;&#26399;&#24615;&#21516;&#27493;&#20135;&#29983;&#20102;&#30456;&#24403;&#22823;&#30340;&#24320;&#38144;&#65292;&#21463;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#22823;&#21644;&#22797;&#26434;&#30340;&#24433;&#21709;&#26356;&#21152;&#20005;&#37325;&#12290;&#23613;&#31649;&#35768;&#22810;&#26799;&#24230;&#21387;&#32553;&#25216;&#26415;&#25552;&#20986;&#20102;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#30340;&#26041;&#24335;&#65292;&#20294;&#26368;&#20339;&#21387;&#32553;&#22240;&#23376;&#23548;&#33268;&#26368;&#22823;&#36895;&#24230;&#25552;&#21319;&#25110;&#26368;&#23567;&#25968;&#25454;&#20132;&#25442;&#30340;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24335;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#19982;&#21387;&#32553;&#36136;&#37327;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#32467;&#26500;&#12289;&#30828;&#20214;&#12289;&#32593;&#32476;&#25299;&#25169;&#21644;&#24102;&#23485;&#26377;&#20851;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GraVAC&#65292;&#19968;&#20010;&#21160;&#24577;&#35843;&#25972;&#21387;&#32553;&#22240;&#23376;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36827;&#23637;&#21644;&#35780;&#20272;&#19982;&#21387;&#32553;&#30456;&#20851;&#30340;&#26799;&#24230;&#20449;&#24687;&#25439;&#22833;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;GraVAC&#20197;&#22312;&#32447;&#30340;&#40657;&#30418;&#26041;&#24335;&#24037;&#20316;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#27169;&#22411;&#25110;&#20854;&#36229;&#21442;&#25968;&#30340;&#20808;&#21069;&#20551;&#35774;&#65292;&#21516;&#26102;&#23454;&#29616;&#19982;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#27861;&#30456;&#21516;&#25110;&#26356;&#22909;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#23545;&#20110;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#65292;&#30456;&#23545;&#20110;&#20854;&#38745;&#24577;&#21387;&#32553;&#23545;&#24212;&#29289;&#65292;GraVAC&#23558;&#36890;&#20449;&#20943;&#23569;&#20102;&#39640;&#36798;87&#65285;&#21644;75&#65285;&#65292;&#32780;&#25910;&#25947;&#31934;&#24230;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed data-parallel (DDP) training improves overall application throughput as multiple devices train on a subset of data and aggregate updates to produce a globally shared model. The periodic synchronization at each iteration incurs considerable overhead, exacerbated by the increasing size and complexity of state-of-the-art neural networks. Although many gradient compression techniques propose to reduce communication cost, the ideal compression factor that leads to maximum speedup or minimum data exchange remains an open-ended problem since it varies with the quality of compression, model size and structure, hardware, network topology and bandwidth. We propose GraVAC, a framework to dynamically adjust compression factor throughout training by evaluating model progress and assessing gradient information loss associated with compression. GraVAC works in an online, black-box manner without any prior assumptions about a model or its hyperparameters, while achieving the same or better
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#25913;&#21464;&#21518;&#30340;&#20132;&#36890;&#26631;&#24535;&#23545;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#26292;&#38706;&#20110;&#19981;&#22826;&#21487;&#33021;&#30340;&#26465;&#20214;&#19979;&#20462;&#25913;&#21518;&#30340;&#20132;&#36890;&#26631;&#24535;&#26102;&#65292;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.05499</link><description>&lt;p&gt;
&#23454;&#38469;&#20132;&#36890;&#26631;&#24535;&#25913;&#21464;&#23545;YOLOv7-&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effects of Real-Life Traffic Sign Alteration on YOLOv7- an Object Recognition Model. (arXiv:2305.05499v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#25913;&#21464;&#21518;&#30340;&#20132;&#36890;&#26631;&#24535;&#23545;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#26292;&#38706;&#20110;&#19981;&#22826;&#21487;&#33021;&#30340;&#26465;&#20214;&#19979;&#20462;&#25913;&#21518;&#30340;&#20132;&#36890;&#26631;&#24535;&#26102;&#65292;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#29289;&#20307;&#35782;&#21035;(OR)&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#22914;&#26426;&#22330;&#23433;&#20840;&#21644;&#37038;&#20214;&#20998;&#25315;&#12290;&#36825;&#20123;&#27169;&#22411;&#24050;&#25104;&#20026;AI&#33021;&#21147;&#30340;&#26631;&#24535;&#65292;&#24182;&#25903;&#25345;&#30528;&#22269;&#23478;&#37038;&#25919;&#36816;&#33829;&#31561;&#37325;&#35201;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;OR&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#29616;&#23454;&#22330;&#26223;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#20132;&#36890;&#26631;&#24535;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#25913;&#21464;&#21518;&#30340;&#20132;&#36890;&#26631;&#24535;&#23545;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#20351;&#29992;&#20844;&#24320;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#20132;&#36890;&#26631;&#24535;&#20462;&#25913;&#65292;&#21253;&#25324;&#22823;&#23567;&#12289;&#24418;&#29366;&#12289;&#39068;&#33394;&#12289;&#21487;&#35265;&#24615;&#21644;&#35282;&#24230;&#30340;&#25913;&#21464;&#65292;&#24182;&#20998;&#26512;&#36825;&#20123;&#20462;&#25913;&#23545;YOLOv7 (You Only Look Once)&#27169;&#22411;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26292;&#38706;&#20110;&#19981;&#22826;&#21487;&#33021;&#30340;&#26465;&#20214;&#19979;&#20462;&#25913;&#21518;&#30340;&#20132;&#36890;&#26631;&#24535;&#26102;&#65292;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of Image Processing has led to the widespread use of Object Recognition (OR) models in various applications, such as airport security and mail sorting. These models have become essential in signifying the capabilities of AI and supporting vital services like national postal operations. However, the performance of OR models can be impeded by real-life scenarios, such as traffic sign alteration. Therefore, this research investigates the effects of altered traffic signs on the accuracy and performance of object recognition models. To this end, a publicly available dataset was used to create different types of traffic sign alterations, including changes to size, shape, color, visibility, and angles. The impact of these alterations on the YOLOv7 (You Only Look Once) model's detection and classification abilities were analyzed. It reveals that the accuracy of object detection models decreases significantly when exposed to modified traffic signs under unlikely conditions. This
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#25105;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#36739;&#23567;&#30340;&#28176;&#36817;&#37319;&#26679;&#26041;&#24046;&#65292;&#36866;&#29992;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#37319;&#26679;&#21644;&#37051;&#22495;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2305.05097</link><description>&lt;p&gt;
&#36890;&#29992;&#22270;&#19978;&#30340;&#33258;&#25105;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208; - &#36890;&#36807;&#38750;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#38142;&#23454;&#29616;&#26368;&#23567;&#37319;&#26679;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
Self-Repellent Random Walks on General Graphs - Achieving Minimal Sampling Variance via Nonlinear Markov Chains. (arXiv:2305.05097v1 [math.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#25105;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#36739;&#23567;&#30340;&#28176;&#36817;&#37319;&#26679;&#26041;&#24046;&#65292;&#36866;&#29992;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#37319;&#26679;&#21644;&#37051;&#22495;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#20363;&#22914;&#19968;&#33324;&#30340;&#26080;&#21521;&#22270;&#65292;&#20854;&#20013;&#38543;&#26426;&#28216;&#36208;&#35774;&#35745;&#25104;&#36890;&#36807;&#37319;&#26679;&#21644;&#37051;&#22495;&#25506;&#32034;&#26469;&#36924;&#36817;&#32593;&#32476;&#25299;&#25169;&#19978;&#30340;&#30446;&#26631;&#37327;&#65292;&#20197;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599; (MCMC) &#31243;&#24207;&#30340;&#24418;&#24335;&#36827;&#34892;&#12290;&#23545;&#20110;&#20219;&#20309;&#30456;&#24212;&#20110;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#25105;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208; (SRRW)&#65292;&#23427;&#19981;&#22826;&#21487;&#33021;&#36716;&#31227;&#21040;&#36807;&#21435;&#34987;&#39640;&#24230;&#35775;&#38382;&#30340;&#33410;&#28857;&#65292;&#32780;&#26356;&#21487;&#33021;&#36716;&#31227;&#21040;&#24456;&#23569;&#34987;&#35775;&#38382;&#30340;&#33410;&#28857;&#12290;&#23545;&#20110;&#19968;&#31867;&#30001;&#27491;&#23454;&#25968; {\alpha} &#21442;&#25968;&#21270;&#30340; SRRW&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#36807;&#31243;&#30340;&#32463;&#39564;&#20998;&#24067;&#20960;&#20046;&#32943;&#23450;&#25910;&#25947;&#20110;&#24213;&#23618;&#39532;&#23572;&#21487;&#22827;&#38142;&#20869;&#26680;&#30340;&#30446;&#26631; (&#24179;&#31283;) &#20998;&#24067;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#24182;&#25512;&#23548;&#20986;&#25152;&#24471;&#21040;&#30340;&#28176;&#36817;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#31934;&#30830;&#24418;&#24335;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#26126;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#25490;&#26021;&#20316;&#29992; (&#36739;&#22823;&#30340; {\alpha}) &#30340; SRRW &#19968;&#23450;&#27604;&#20855;&#26377;&#36739;&#24369;&#30340;&#25490;&#26021;&#20316;&#29992; (&#36739;&#23567;&#30340; {\alpha}) &#30340; SRRW &#23454;&#29616;&#26356;&#23567;&#30340;&#28176;&#36817;&#37319;&#26679;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider random walks on discrete state spaces, such as general undirected graphs, where the random walkers are designed to approximate a target quantity over the network topology via sampling and neighborhood exploration in the form of Markov chain Monte Carlo (MCMC) procedures. Given any Markov chain corresponding to a target probability distribution, we design a self-repellent random walk (SRRW) which is less likely to transition to nodes that were highly visited in the past, and more likely to transition to seldom visited nodes. For a class of SRRWs parameterized by a positive real {\alpha}, we prove that the empirical distribution of the process converges almost surely to the the target (stationary) distribution of the underlying Markov chain kernel. We then provide a central limit theorem and derive the exact form of the arising asymptotic co-variance matrix, which allows us to show that the SRRW with a stronger repellence (larger {\alpha}) always achieves a smaller asymptotic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26494;&#24347;&#30340;&#39640;&#25928;&#23454;&#29992;&#30340;&#20219;&#24847;&#26102;&#21051;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#21407;&#20687;&#30340;&#31526;&#21495;&#19979;&#36817;&#20284;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25913;&#36827;&#21644;&#26356;&#39640;&#30340;&#21387;&#32553;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03686</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#21407;&#20687;&#36817;&#20284;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Preimage Approximation for Neural Networks. (arXiv:2305.03686v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26494;&#24347;&#30340;&#39640;&#25928;&#23454;&#29992;&#30340;&#20219;&#24847;&#26102;&#21051;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#21407;&#20687;&#30340;&#31526;&#21495;&#19979;&#36817;&#20284;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25913;&#36827;&#21644;&#26356;&#39640;&#30340;&#21387;&#32553;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#20027;&#35201;&#20851;&#27880;&#23616;&#37096;&#40065;&#26834;&#24615;&#65292;&#28982;&#32780;&#65292;&#36890;&#24120;&#38656;&#35201;&#30693;&#36947;&#32473;&#23450;&#23646;&#24615;&#26159;&#21542;&#22312;&#25972;&#20010;&#36755;&#20837;&#22495;&#20869;&#20840;&#23616;&#25104;&#31435;&#65292;&#22914;&#26524;&#19981;&#25104;&#31435;&#65292;&#21017;&#38656;&#35201;&#30693;&#36947;&#23646;&#24615;&#25104;&#31435;&#30340;&#36755;&#20837;&#27604;&#20363;&#26159;&#22810;&#23569;&#12290;&#23613;&#31649;&#31934;&#30830;&#30340;&#21407;&#20687;&#29983;&#25104;&#21487;&#20197;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#20215;&#34920;&#31034;&#65292;&#20294;&#22312;&#35268;&#27169;&#19978;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26494;&#24347;&#30340;&#39640;&#25928;&#23454;&#29992;&#30340;&#20219;&#24847;&#26102;&#21051;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#21407;&#20687;&#30340;&#31526;&#21495;&#19979;&#36817;&#20284;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#23558;&#36755;&#20837;&#21306;&#22495;&#21010;&#20998;&#20026;&#23376;&#21306;&#22495;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#26494;&#24347;&#36793;&#30028;&#21464;&#24471;&#26356;&#32039;&#65292;&#36845;&#20195;&#22320;&#26368;&#23567;&#21270;&#20307;&#31215;&#36924;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37319;&#29992;&#37319;&#26679;&#21644;&#21487;&#24494;&#20307;&#31215;&#36924;&#36817;&#26469;&#20248;&#20808;&#21010;&#20998;&#21306;&#22495;&#65292;&#24182;&#20248;&#21270;&#26494;&#24347;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24555;&#30340;&#25913;&#36827;&#21644;&#26356;&#39640;&#30340;&#21387;&#32553;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network verification mainly focuses on local robustness properties. However, often it is important to know whether a given property holds globally for the whole input domain, and if not then for what proportion of the input the property is true. While exact preimage generation can construct an equivalent representation of neural networks that can aid such (quantitative) global robustness verification, it is intractable at scale. In this work, we propose an efficient and practical anytime algorithm for generating symbolic under-approximations of the preimage of neural networks based on linear relaxation. Our algorithm iteratively minimizes the volume approximation error by partitioning the input region into subregions, where the neural network relaxation bounds become tighter. We further employ sampling and differentiable approximations to the volume in order to prioritize regions to split and optimize the parameters of the relaxation, leading to faster improvement and more compa
&lt;/p&gt;</description></item><item><title>AutoColor &#26159;&#31532;&#19968;&#20010;&#23398;&#20064;&#27491;&#30830;&#29031;&#26126;&#22810;&#33394;&#20840;&#24687;&#22270;&#25152;&#38656;&#20809;&#28304;&#21151;&#29575;&#30340;&#26041;&#27861;&#65292;&#23558;&#20248;&#21270;&#22810;&#33394;&#20840;&#24687;&#22270;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#20174;&#36229;&#36807;1000&#20010;&#38477;&#33267;70&#20010;&#36845;&#20195;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2305.01611</link><description>&lt;p&gt;
AutoColor: &#38024;&#23545;&#22810;&#33394;&#20840;&#24687;&#22270;&#30340;&#23398;&#20064;&#20809;&#21151;&#29575;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
AutoColor: Learned Light Power Control for Multi-Color Holograms. (arXiv:2305.01611v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01611
&lt;/p&gt;
&lt;p&gt;
AutoColor &#26159;&#31532;&#19968;&#20010;&#23398;&#20064;&#27491;&#30830;&#29031;&#26126;&#22810;&#33394;&#20840;&#24687;&#22270;&#25152;&#38656;&#20809;&#28304;&#21151;&#29575;&#30340;&#26041;&#27861;&#65292;&#23558;&#20248;&#21270;&#22810;&#33394;&#20840;&#24687;&#22270;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#20174;&#36229;&#36807;1000&#20010;&#38477;&#33267;70&#20010;&#36845;&#20195;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33394;&#20840;&#24687;&#22270;&#38656;&#35201;&#22810;&#20010;&#20809;&#28304;&#21516;&#26102;&#29031;&#23556;&#25165;&#33021;&#24418;&#25104;&#22270;&#24418;&#12290;&#36825;&#31181;&#20840;&#24687;&#22270;&#21487;&#20197;&#27604;&#20256;&#32479;&#21333;&#33394;&#20840;&#24687;&#22270;&#26356;&#22909;&#22320;&#21033;&#29992;&#20809;&#28304;&#65292;&#24182;&#25552;&#39640;&#20840;&#24687;&#26174;&#31034;&#30340;&#21160;&#24577;&#33539;&#22260;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;\projectname&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#29031;&#26126;&#22810;&#33394;&#20840;&#24687;&#22270;&#25152;&#38656;&#30340;&#26368;&#20339;&#20809;&#28304;&#21151;&#29575;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#21644;&#20854;&#28145;&#24230;&#20449;&#24687;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#22810;&#33394;&#20840;&#24687;&#22270;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#27969;&#34892;&#30340;&#27969;&#31243;&#32467;&#21512;&#29983;&#25104;&#27169;&#22411;&#12289;&#22823;&#35821;&#35328;&#21644;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#26469;&#29983;&#25104;&#36825;&#20123;&#21512;&#25104;&#22270;&#20687;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#38598;&#35757;&#32451;&#25105;&#20204;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23454;&#39564;&#24615;&#22320;&#35777;&#26126;&#65292;\projectname&#21487;&#20197;&#23558;&#20248;&#21270;&#22810;&#33394;&#20840;&#24687;&#22270;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#20174;&#36229;&#36807;1000&#20010;&#38477;&#33267;70&#20010;&#36845;&#20195;&#27493;&#39588;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-color holograms rely on simultaneous illumination from multiple light sources. These multi-color holograms could utilize light sources better than conventional single-color holograms and can improve the dynamic range of holographic displays. In this letter, we introduce \projectname, the first learned method for estimating the optimal light source powers required for illuminating multi-color holograms. For this purpose, we establish the first multi-color hologram dataset using synthetic images and their depth information. We generate these synthetic images using a trending pipeline combining generative, large language, and monocular depth estimation models. Finally, we train our learned model using our dataset and experimentally demonstrate that \projectname significantly decreases the number of steps required to optimize multi-color holograms from $&gt;1000$ to $70$ iteration steps without compromising image quality.
&lt;/p&gt;</description></item><item><title>&#21487;&#34892;&#24615;&#31574;&#30053;&#36845;&#20195; (FPI) &#26159;&#19968;&#20010;&#38388;&#25509;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#19978;&#19968;&#20010;&#31574;&#30053;&#30340;&#21487;&#34892;&#22495;&#26469;&#36845;&#20195;&#22320;&#38480;&#21046;&#24403;&#21069;&#31574;&#30053;&#12290;&#21487;&#34892;&#24615;&#31574;&#30053;&#25913;&#36827;&#26159;&#20854;&#26680;&#24515;&#65292;&#23427;&#22312;&#21487;&#34892;&#22495;&#20869;&#26368;&#22823;&#21270;&#22238;&#25253;&#65292;&#22312;&#21487;&#34892;&#22495;&#22806;&#26368;&#23567;&#21270;&#32422;&#26463;&#34928;&#20943;&#20989;&#25968; (CDF).</title><link>http://arxiv.org/abs/2304.08845</link><description>&lt;p&gt;
&#21487;&#34892;&#24615;&#31574;&#30053;&#36845;&#20195;
&lt;/p&gt;
&lt;p&gt;
Feasible Policy Iteration. (arXiv:2304.08845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08845
&lt;/p&gt;
&lt;p&gt;
&#21487;&#34892;&#24615;&#31574;&#30053;&#36845;&#20195; (FPI) &#26159;&#19968;&#20010;&#38388;&#25509;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#19978;&#19968;&#20010;&#31574;&#30053;&#30340;&#21487;&#34892;&#22495;&#26469;&#36845;&#20195;&#22320;&#38480;&#21046;&#24403;&#21069;&#31574;&#30053;&#12290;&#21487;&#34892;&#24615;&#31574;&#30053;&#25913;&#36827;&#26159;&#20854;&#26680;&#24515;&#65292;&#23427;&#22312;&#21487;&#34892;&#22495;&#20869;&#26368;&#22823;&#21270;&#22238;&#25253;&#65292;&#22312;&#21487;&#34892;&#22495;&#22806;&#26368;&#23567;&#21270;&#32422;&#26463;&#34928;&#20943;&#20989;&#25968; (CDF).
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#22312;&#23433;&#20840;&#32422;&#26463;&#19979;&#35299;&#20915;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340; $\textit{&#30452;&#25509;}$ &#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20250;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#19968;&#30452;&#20351;&#29992;&#21407;&#22987;&#32422;&#26463;&#12290;&#23427;&#20204;&#25110;&#32773;&#32570;&#20047;&#31574;&#30053;&#36845;&#20195;&#26399;&#38388;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#25110;&#32773;&#36973;&#36935;&#19981;&#21487;&#34892;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;&#21487;&#34892;&#24615;&#31574;&#30053;&#36845;&#20195;&#65288;FPI&#65289;&#30340; $\textit{&#38388;&#25509;}$ &#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#26368;&#21518;&#19968;&#20010;&#31574;&#30053;&#30340;&#21487;&#34892;&#22495;&#26469;&#36845;&#20195;&#22320;&#38480;&#21046;&#24403;&#21069;&#31574;&#30053;&#12290;&#21487;&#34892;&#22495;&#30001;&#19968;&#20010;&#21483;&#20570;&#32422;&#26463;&#34928;&#20943;&#20989;&#25968;&#65288;CDF&#65289;&#30340;&#21487;&#34892;&#24615;&#20989;&#25968;&#34920;&#31034;&#12290;FPI &#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#21483;&#20570;&#21487;&#34892;&#24615;&#31574;&#30053;&#25913;&#36827;&#30340;&#21306;&#22495;&#24615;&#31574;&#30053;&#26356;&#26032;&#35268;&#21017;&#65292;&#23427;&#22312;&#21487;&#34892;&#22495;&#20869;&#26368;&#22823;&#21270;&#22238;&#25253;&#65292;&#22312;&#21487;&#34892;&#22495;&#22806;&#26368;&#23567;&#21270; CDF&#12290;&#36825;&#20010;&#26356;&#26032;&#35268;&#21017;&#24635;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#30830;&#20445;&#21487;&#34892;&#22495;&#21333;&#35843;&#22320;&#25193;&#23637;&#65292;&#29366;&#24577;&#20540;&#20989;&#25968;&#21333;&#35843;&#22320;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning (RL) aims to solve an optimal control problem under safety constraints. Existing $\textit{direct}$ safe RL methods use the original constraint throughout the learning process. They either lack theoretical guarantees of the policy during iteration or suffer from infeasibility problems. To address this issue, we propose an $\textit{indirect}$ safe RL method called feasible policy iteration (FPI) that iteratively uses the feasible region of the last policy to constrain the current policy. The feasible region is represented by a feasibility function called constraint decay function (CDF). The core of FPI is a region-wise policy update rule called feasible policy improvement, which maximizes the return under the constraint of the CDF inside the feasible region and minimizes the CDF outside the feasible region. This update rule is always feasible and ensures that the feasible region monotonically expands and the state-value function monotonically increases inside 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04934</link><description>&lt;p&gt;
&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#31616;&#21270;&#26426;&#22120;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25968;&#25454;&#31649;&#21046;&#35201;&#27714;&#26426;&#22120;&#21453;&#23398;&#20064;&#65288;MU&#65289;&#65306;&#20174;&#27169;&#22411;&#20013;&#31227;&#38500;&#25351;&#23450;&#26679;&#20363;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21097;&#20313;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26469;&#36827;&#34892;&#31934;&#30830;&#21453;&#23398;&#20064;&#65292;&#20294;&#26159;&#20854;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#20102;&#36817;&#20284;&#20294;&#39640;&#25928;&#30340;&#21453;&#23398;&#20064;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;&#38500;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;MU&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35270;&#35282;&#25512;&#36827;MU&#65306;&#36890;&#36807;&#26435;&#20540;&#20462;&#21098;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#31232;&#30095;&#24615;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#32553;&#23567;&#36817;&#20284;&#38388;&#38553;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#26377;&#20102;&#36825;&#20010;&#35748;&#35782;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20004;&#20010;&#26032;&#30340;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#20803;&#26041;&#26696;&#65292;&#31216;&#20026;&#8220;&#20808;&#20462;&#21098;&#65292;&#28982;&#21518;&#21453;&#23398;&#20064;&#8221;&#21644;&#8220;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#21644;&#25552;&#35758;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#22987;&#32456;&#26377;&#30410;&#20110;MU&#65292;&#21253;&#25324;&#25353;&#31867;&#25968;&#25454;&#25830;&#38500;&#12289;&#38543;&#26426;&#25968;&#25454;&#25830;&#38500;&#21644;&#21518;&#38376;&#25968;&#25454;&#20266;&#36896;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31435;&#26041;&#20307;&#30340;3D&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26469;&#37325;&#24314;CBCT&#24182;&#35299;&#20915;&#20102;&#23384;&#20648;&#25972;&#20010;&#27491;&#24358;&#22270;&#30340;&#20869;&#23384;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#25972;&#20010;CBCT volume&#20998;&#25104;&#22810;&#20010;&#23567;&#31435;&#26041;&#20307;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#24182;&#22312;&#35270;&#35273;&#21644;&#23450;&#37327;&#35780;&#20215;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.12861</link><description>&lt;p&gt;
&#38754;&#21521;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#38181;&#26463;CT&#37325;&#24314;&#30340;&#22522;&#20110;&#31435;&#26041;&#20307;&#30340;3D&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cube-Based 3D Denoising Diffusion Probabilistic Model for Cone Beam Computed Tomography Reconstruction with Incomplete Data. (arXiv:2303.12861v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31435;&#26041;&#20307;&#30340;3D&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26469;&#37325;&#24314;CBCT&#24182;&#35299;&#20915;&#20102;&#23384;&#20648;&#25972;&#20010;&#27491;&#24358;&#22270;&#30340;&#20869;&#23384;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#25972;&#20010;CBCT volume&#20998;&#25104;&#22810;&#20010;&#23567;&#31435;&#26041;&#20307;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#24182;&#22312;&#35270;&#35273;&#21644;&#23450;&#37327;&#35780;&#20215;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#26029;&#23618;&#25668;&#24433;&#65288;CT&#65289;&#37325;&#24314;&#20013;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#35270;&#22270;CT&#37325;&#24314;&#20013;&#12290;&#28982;&#32780;&#65292;&#23558;DL&#24212;&#29992;&#20110;&#31232;&#30095;&#35270;&#22270;&#38181;&#26463;CT&#65288;CBCT&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31435;&#26041;&#20307;&#30340;3D&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26469;&#37325;&#24314;CBCT&#65292;&#24182;&#35299;&#20915;&#20102;&#23384;&#20648;&#25972;&#20010;&#27491;&#24358;&#22270;&#30340;&#20869;&#23384;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25972;&#20010;CBCT volume&#20998;&#25104;&#22810;&#20010;&#23567;&#31435;&#26041;&#20307;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35270;&#35273;&#21644;&#23450;&#37327;&#35780;&#20215;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has been extensively researched in the field of computed tomography (CT) reconstruction with incomplete data, particularly in sparse-view CT reconstruction. However, applying DL to sparse-view cone beam CT (CBCT) remains challenging. Many models learn the mapping from sparse-view CT images to ground truth but struggle to achieve satisfactory performance in terms of global artifact removal. Incorporating sinogram data and utilizing dual-domain information can enhance anti-artifact performance, but this requires storing the entire sinogram in memory. This presents a memory issue for high-resolution CBCT sinograms, limiting further research and application. In this paper, we propose a cube-based 3D denoising diffusion probabilistic model (DDPM) for CBCT reconstruction using down-sampled data. A DDPM network, trained on cubes extracted from paired fully sampled sinograms and down-sampled sinograms, is employed to inpaint down-sampled sinograms. Our method divides the ent
&lt;/p&gt;</description></item><item><title>Magnushammer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21069;&#25552;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;PISA&#22522;&#20934;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#22823;&#24133;&#24230;&#36229;&#36234;&#20256;&#32479;&#31526;&#21495;&#31995;&#32479;&#65292;&#24182;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#35777;&#26126;&#29575;&#20174;57.0&#65285;&#25552;&#39640;&#21040;71.0&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.04488</link><description>&lt;p&gt;
Magnushammer: &#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21069;&#25552;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Magnushammer: A Transformer-based Approach to Premise Selection. (arXiv:2303.04488v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04488
&lt;/p&gt;
&lt;p&gt;
Magnushammer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21069;&#25552;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;PISA&#22522;&#20934;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#22823;&#24133;&#24230;&#36229;&#36234;&#20256;&#32479;&#31526;&#21495;&#31995;&#32479;&#65292;&#24182;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#35777;&#26126;&#29575;&#20174;57.0&#65285;&#25552;&#39640;&#21040;71.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#25552;&#36873;&#25321;&#26159;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#24120;&#24120;&#20351;&#29992;&#22797;&#26434;&#30340;&#31526;&#21495;&#26041;&#27861;&#65292;&#20381;&#36182;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#31243;&#24037;&#20316;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#31070;&#32463;&#36716;&#25442;&#22120;&#30340;Magnushammer&#26041;&#27861;&#21487;&#20197;&#22823;&#24133;&#24230;&#22320;&#36229;&#36234;&#20256;&#32479;&#30340;&#31526;&#21495;&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;PISA&#22522;&#20934;&#19978;&#30340;&#27979;&#35797;&#65292;Magnushammer&#30340;&#35777;&#26126;&#29575;&#36798;&#21040;&#20102;59.5&#65285;&#65292;&#32780;&#26368;&#25104;&#29087;&#21644;&#27969;&#34892;&#30340;&#22522;&#20110;&#31526;&#21495;&#30340;&#27714;&#35299;&#22120;Sledgehammer&#30340;&#35777;&#26126;&#29575;&#21482;&#26377;38.3&#65285;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;Magnushammer&#19982;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#24418;&#24335;&#35777;&#26126;&#22120;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#35777;&#26126;&#29575;&#20174;57.0&#65285;&#22823;&#24133;&#25552;&#39640;&#21040;71.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Premise selection is a fundamental problem of automated theorem proving. Previous works often use intricate symbolic methods, rely on domain knowledge, and require significant engineering effort to solve this task. In this work, we show that Magnushammer, a neural transformer-based approach, can outperform traditional symbolic systems by a large margin. Tested on the PISA benchmark, Magnushammer achieves $59.5\%$ proof rate compared to a $38.3\%$ proof rate of Sledgehammer, the most mature and popular symbolic-based solver. Furthermore, by combining Magnushammer with a neural formal prover based on a language model, we significantly improve the previous state-of-the-art proof rate from $57.0\%$ to $71.0\%$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#22411;&#26080;&#20851;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#23558;&#21508;&#20010;&#35299;&#37322;&#33021;&#21147;&#26041;&#38754;&#24635;&#32467;&#25104;&#26631;&#37327;&#65292;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#20915;&#31574;&#32773;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.12094</link><description>&lt;p&gt;
&#35780;&#20272;&#20351;&#29992;&#27169;&#22411;&#26080;&#20851;&#30340;&#24230;&#37327;&#26631;&#20934;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating explainability for machine learning predictions using model-agnostic metrics. (arXiv:2302.12094v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#22411;&#26080;&#20851;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#23558;&#21508;&#20010;&#35299;&#37322;&#33021;&#21147;&#26041;&#38754;&#24635;&#32467;&#25104;&#26631;&#37327;&#65292;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#20915;&#31574;&#32773;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#31649;&#29702;&#21644;&#30417;&#31649;&#26041;&#38754;&#30340;&#20247;&#22810;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#27491;&#22312;&#34987;&#25972;&#21512;&#21040;&#21508;&#20010;&#34892;&#19994;&#21644;&#39046;&#22495;&#65292;&#20915;&#31574;&#32773;&#38656;&#20840;&#38754;&#32454;&#33268;&#22320;&#20102;&#35299;&#36825;&#20123;&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;&#36825;&#20010;&#38656;&#27714;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#33021;&#22815;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#20449;&#20219;&#24230;&#20197;&#21450;&#24110;&#21161;&#27169;&#22411;&#22312;&#36947;&#24503;&#19978;&#36827;&#34892;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#37327;&#21270;AI&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20854;&#29305;&#24449;&#36827;&#34892;&#26131;&#20110;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#23558;&#35299;&#37322;&#33021;&#21147;&#30340;&#19981;&#21516;&#26041;&#38754;&#24635;&#32467;&#20026;&#26631;&#37327;&#65292;&#25552;&#20379;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#20419;&#36827;&#20915;&#31574;&#32773;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in artificial intelligence (AI) technology have brought about a plethora of new challenges in terms of governance and regulation. AI systems are being integrated into various industries and sectors, creating a demand from decision-makers to possess a comprehensive and nuanced understanding of the capabilities and limitations of these systems. One critical aspect of this demand is the ability to explain the results of machine learning models, which is crucial to promoting transparency and trust in AI systems, as well as fundamental in helping machine learning models to be trained ethically. In this paper, we present novel metrics to quantify the degree of which AI model predictions can be easily explainable by its features. Our metrics summarize different aspects of explainability into scalars, providing a more comprehensive understanding of model predictions and facilitating communication between decision-makers and stakeholders, thereby increasing the overall transp
&lt;/p&gt;</description></item><item><title>&#27169;&#22359;&#21270;&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#19982;&#36335;&#30001;&#21644;&#23616;&#37096;&#26356;&#26032;&#27169;&#22359;&#20998;&#31163;&#65292;&#23454;&#29616;&#20102;&#31215;&#26497;&#36801;&#31227;&#21644;&#31995;&#32479;&#21270;&#30340;&#25512;&#24191;&#12290;</title><link>http://arxiv.org/abs/2302.11529</link><description>&lt;p&gt;
&#27169;&#22359;&#21270;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Modular Deep Learning. (arXiv:2302.11529v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11529
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22359;&#21270;&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#19982;&#36335;&#30001;&#21644;&#23616;&#37096;&#26356;&#26032;&#27169;&#22359;&#20998;&#31163;&#65292;&#23454;&#29616;&#20102;&#31215;&#26497;&#36801;&#31227;&#21644;&#31995;&#32479;&#21270;&#30340;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#36817;&#24180;&#26469;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#20027;&#35201;&#33539;&#24335;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#32463;&#36807;&#24494;&#35843;&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#24320;&#21457;&#33021;&#22815;&#19987;&#27880;&#20110;&#22810;&#20010;&#20219;&#21153;&#32780;&#19981;&#20250;&#20135;&#29983;&#36127;&#38754;&#24178;&#25200;&#65292;&#24182;&#19988;&#33021;&#22815;&#31995;&#32479;&#22320;&#25512;&#24191;&#21040;&#38750;&#30456;&#21516;&#20998;&#24067;&#20219;&#21153;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#27169;&#22359;&#21270;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#35745;&#31639;&#21333;&#20803;&#36890;&#24120;&#34987;&#23454;&#29616;&#20026;&#33258;&#20027;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22359;&#12290;&#20449;&#24687;&#34987;&#26377;&#26465;&#20214;&#22320;&#36335;&#30001;&#21040;&#19968;&#37096;&#20998;&#27169;&#22359;&#65292;&#28982;&#21518;&#36827;&#34892;&#27719;&#24635;&#12290;&#36825;&#20123;&#29305;&#24615;&#36890;&#36807;&#23558;&#35745;&#31639;&#19982;&#36335;&#30001;&#21644;&#23616;&#37096;&#26356;&#26032;&#27169;&#22359;&#20998;&#31163;&#65292;&#23454;&#29616;&#20102;&#31215;&#26497;&#36801;&#31227;&#21644;&#31995;&#32479;&#21270;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#27169;&#22359;&#21270;&#26550;&#26500;&#30340;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;&#22312;&#31185;&#23398;&#25991;&#29486;&#20013;&#29420;&#31435;&#28436;&#21270;&#30340;&#20960;&#20010;&#30740;&#31350;&#26041;&#21521;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#21508;&#31181;&#38468;&#21152;...
&lt;/p&gt;
&lt;p&gt;
Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various addi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;SBERT&#21644;CNN&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;Reddit&#29992;&#25143;&#30340;&#24086;&#23376;&#33258;&#21160;&#35782;&#21035;&#25233;&#37057;&#30151;&#24739;&#32773;&#12290;</title><link>http://arxiv.org/abs/2302.02759</link><description>&lt;p&gt;
&#29992;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;SBERT-CNN&#26816;&#27979;Reddit&#29992;&#25143;&#30340;&#25233;&#37057;&#30151;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Detecting Reddit Users with Depression Using a Hybrid Neural Network SBERT-CNN. (arXiv:2302.02759v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;SBERT&#21644;CNN&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;Reddit&#29992;&#25143;&#30340;&#24086;&#23376;&#33258;&#21160;&#35782;&#21035;&#25233;&#37057;&#30151;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#19968;&#31181;&#24191;&#27867;&#23384;&#22312;&#30340;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#65292;&#24433;&#21709;&#20102;&#20840;&#29699;&#20272;&#35745;3.8%&#30340;&#20154;&#21475;&#12290;&#23427;&#20063;&#26159;&#20840;&#29699;&#27531;&#30142;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#20010;&#20154;&#36234;&#26469;&#36234;&#21916;&#27426;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65288;&#22914;Reddit&#65289;&#26469;&#34920;&#36798;&#20182;&#20204;&#30340;&#22256;&#38590;&#21644;&#20581;&#24247;&#38382;&#39064;&#65288;&#22914;&#25233;&#37057;&#30151;&#65289;&#65292;&#24182;&#22312;&#22312;&#32447;&#31038;&#21306;&#23547;&#27714;&#20854;&#20182;&#29992;&#25143;&#30340;&#25903;&#25345;&#12290;&#36825;&#20026;&#36890;&#36807;&#20998;&#26512;&#25968;&#30334;&#19975;&#24086;&#23376;&#20197;&#23547;&#25214;&#28508;&#22312;&#30340;&#24178;&#39044;&#26426;&#20250;&#65292;&#33258;&#21160;&#35782;&#21035;&#20855;&#26377;&#25233;&#37057;&#30151;&#30340;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#26426;&#20250;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22240;&#20854;&#26131;&#29992;&#24615;&#12289;&#39640;&#25928;&#22788;&#29702;&#33021;&#21147;&#21644;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#30340;&#26368;&#26032;&#32467;&#26524;&#32780;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24320;&#22987;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#21477;&#23376;BERT&#65288;SBERT&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#36890;&#36807;&#20182;&#20204;&#22312;Reddit&#19978;&#30340;&#24086;&#23376;&#26816;&#27979;&#25233;&#37057;&#30151;&#24739;&#32773;&#12290;&#21477;&#23376;BERT&#29992;&#20110;&#23398;&#20064;&#21477;&#23376;&#30340;&#24847;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is a widespread mental health issue, affecting an estimated 3.8% of the global population. It is also one of the main contributors to disability worldwide. Recently it is becoming popular for individuals to use social media platforms (e.g., Reddit) to express their difficulties and health issues (e.g., depression) and seek support from other users in online communities. It opens great opportunities to automatically identify social media users with depression by parsing millions of posts for potential interventions. Deep learning methods have begun to dominate in the field of machine learning and natural language processing (NLP) because of their ease of use, efficient processing, and state-of-the-art results on many NLP tasks. In this work, we propose a hybrid deep learning model which combines a pretrained sentence BERT (SBERT) and convolutional neural network (CNN) to detect individuals with depression with their Reddit posts. The sentence BERT is used to learn the meaning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#34920;&#24449;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#24403;&#21069;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#30340;&#34920;&#31034;&#19981;&#23545;&#40784;&#30340;&#22256;&#22659;&#65292;&#24182;&#24314;&#35758;&#24212;&#23558;&#26426;&#22120;&#20154;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#20174;&#23454;&#29616;&#20219;&#21153;&#30446;&#26631;&#30340;&#35282;&#24230;&#36716;&#21521;&#19982;&#20154;&#31867;&#34920;&#24449;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.01928</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning Robot and Human Representations. (arXiv:2302.01928v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#34920;&#24449;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#24403;&#21069;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#30340;&#34920;&#31034;&#19981;&#23545;&#40784;&#30340;&#22256;&#22659;&#65292;&#24182;&#24314;&#35758;&#24212;&#23558;&#26426;&#22120;&#20154;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#20174;&#23454;&#29616;&#20219;&#21153;&#30446;&#26631;&#30340;&#35282;&#24230;&#36716;&#21521;&#19982;&#20154;&#31867;&#34920;&#24449;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#19990;&#30028;&#20013;&#34892;&#21160;&#65292;&#26426;&#22120;&#20154;&#20381;&#36182;&#20110;&#19968;&#20010;&#20984;&#26174;&#20219;&#21153;&#20851;&#38190;&#26041;&#38754;&#30340;&#34920;&#31034;&#65306;&#20363;&#22914;&#65292;&#20026;&#20102;&#25644;&#36816;&#21654;&#21857;&#26479;&#65292;&#26426;&#22120;&#20154;&#21487;&#33021;&#20250;&#32771;&#34385;&#21160;&#20316;&#25928;&#29575;&#25110;&#26479;&#23376;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#24076;&#26395;&#26426;&#22120;&#20154;&#20026;&#20154;&#31867;&#32780;&#34892;&#21160;&#65292;&#23427;&#20204;&#30340;&#34920;&#31034;&#19981;&#33021;&#21482;&#26159;&#21151;&#33021;&#24615;&#30340;&#65292;&#36824;&#24517;&#39035;&#21453;&#26144;&#20154;&#31867;&#20851;&#24515;&#30340;&#20107;&#29289;&#65292;&#21363;&#23427;&#20204;&#24517;&#39035;&#23545;&#40784;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#21069;&#30340;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#34920;&#31034;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#21363;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#34920;&#31034;&#19981;&#33021;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22240;&#20026;&#20154;&#31867;&#26159;&#26426;&#22120;&#20154;&#34920;&#29616;&#30340;&#26368;&#32456;&#35780;&#20272;&#32773;&#65292;&#25152;&#20197;&#25105;&#20204;&#24517;&#39035;&#26126;&#30830;&#22320;&#23558;&#25105;&#20204;&#30340;&#21162;&#21147;&#38598;&#20013;&#22312;&#19982;&#20154;&#31867;&#30340;&#34920;&#24449;&#23545;&#40784;&#19978;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#23398;&#20064;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20513;&#20174;&#23545;&#34920;&#24449;&#23545;&#40784;&#30446;&#26631;&#30340;&#23436;&#25104;&#31243;&#24230;&#30340;&#35282;&#24230;&#30740;&#31350;&#24403;&#21069;&#26426;&#22120;&#20154;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#19978;&#23450;&#20041;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#20851;&#38190;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
To act in the world, robots rely on a representation of salient task aspects: for example, to carry a coffee mug, a robot may consider movement efficiency or mug orientation in its behavior. However, if we want robots to act for and with people, their representations must not be just functional but also reflective of what humans care about, i.e. they must be aligned. We observe that current learning approaches suffer from representation misalignment, where the robot's learned representation does not capture the human's representation. We suggest that because humans are the ultimate evaluator of robot performance, we must explicitly focus our efforts on aligning learned representations with humans, in addition to learning the downstream task. We advocate that current representation learning approaches in robotics should be studied from the perspective of how well they accomplish the objective of representation alignment. We mathematically define the problem, identify its key desiderata,
&lt;/p&gt;</description></item><item><title>AutoPEFT&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;PEFT&#65288;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65289;&#37197;&#32622;&#25628;&#32034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#22320;&#25214;&#21040;&#26368;&#20339;&#30340;PEFT&#27169;&#22359;&#21644;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#22312;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#20013;&#65292;AutoPEFT&#34920;&#29616;&#20986;&#27604;&#25163;&#21160;&#35774;&#35745;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12132</link><description>&lt;p&gt;
AutoPEFT&#65306;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#33258;&#21160;&#37197;&#32622;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. (arXiv:2301.12132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12132
&lt;/p&gt;
&lt;p&gt;
AutoPEFT&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;PEFT&#65288;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65289;&#37197;&#32622;&#25628;&#32034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#22320;&#25214;&#21040;&#26368;&#20339;&#30340;PEFT&#27169;&#22359;&#21644;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#22312;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#20013;&#65292;AutoPEFT&#34920;&#29616;&#20986;&#27604;&#25163;&#21160;&#35774;&#35745;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19987;&#38376;&#30340;&#24494;&#35843;&#29992;&#20110;&#19979;&#28216;NLP&#20219;&#21153;&#65292;&#20294;&#36825;&#26679;&#30340;&#36807;&#31243;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#26368;&#36817;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#36890;&#36807;&#26356;&#26032;&#27604;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#65288;FFT&#65289;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;PEFT&#37197;&#32622;&#26041;&#38754;&#20570;&#20986;&#26126;&#26234;&#30340;&#35774;&#35745;&#36873;&#25321;&#26159;&#19981;&#23481;&#26131;&#30340;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#21487;&#35843;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#29978;&#33267;&#26159;PEFT&#27169;&#22359;&#25554;&#20837;&#30340;&#22270;&#23618;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;&#25163;&#21160;&#35774;&#35745;&#37197;&#32622;&#24456;&#21487;&#33021;&#22312;&#24615;&#33021;&#25928;&#29575;&#26435;&#34913;&#26041;&#38754;&#26159;&#27425;&#20248;&#30340;&#12290;&#21463;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoPEFT&#26469;&#33258;&#21160;&#36873;&#25321;PEFT&#37197;&#32622;&#65306;&#39318;&#20808;&#35774;&#35745;&#20855;&#26377;&#22810;&#20010;&#20195;&#34920;&#24615;PEFT&#27169;&#22359;&#30340;&#34920;&#36798;&#37197;&#32622;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#21518;&#20351;&#29992;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#20302;&#25104;&#26412;&#30340;&#35774;&#32622;&#65292;&#20174;&#32780;&#21457;&#29616;&#20248;&#21270;&#20219;&#21153;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#30340;Pareto&#20248;&#21270;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#38382;&#31572;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#19978;&#35780;&#20272;&#20102;AutoPEFT&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#25163;&#21160;&#35774;&#35745;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained language models are widely used in downstream NLP tasks via task-specific fine-tuning, but such procedures can be costly. Recently, Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task performance while updating a much smaller number of parameters compared to full model fine-tuning (FFT). However, it is non-trivial to make informed design choices on the PEFT configurations, such as their architecture, the number of tunable parameters, and even the layers in which the PEFT modules are inserted. Consequently, it is highly likely that the current, manually designed configurations are suboptimal in terms of their performance-efficiency trade-off. Inspired by advances in neural architecture search, we propose AutoPEFT for automatic PEFT configuration selection: we first design an expressive configuration search space with multiple representative PEFT modules as building blocks. Using multi-objective Bayesian optimisation in a low-cost setup, we then disc
&lt;/p&gt;</description></item><item><title>ScaDLES&#26159;&#19968;&#31181;&#29992;&#20110;&#36793;&#32536;&#31471;&#27969;&#24335;&#25968;&#25454;&#19978;&#30340;&#21487;&#25193;&#23637;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31995;&#32479;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2301.08897</link><description>&lt;p&gt;
ScaDLES: &#36793;&#32536;&#31471;&#27969;&#24335;&#25968;&#25454;&#19978;&#30340;&#21487;&#25193;&#23637;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ScaDLES: Scalable Deep Learning over Streaming data at the Edge. (arXiv:2301.08897v1 [cs.DC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08897
&lt;/p&gt;
&lt;p&gt;
ScaDLES&#26159;&#19968;&#31181;&#29992;&#20110;&#36793;&#32536;&#31471;&#27969;&#24335;&#25968;&#25454;&#19978;&#30340;&#21487;&#25193;&#23637;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31995;&#32479;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#65288;DDL&#65289;&#35757;&#32451;&#31995;&#32479;&#35774;&#35745;&#29992;&#20110;&#20113;&#21644;&#25968;&#25454;&#20013;&#24515;&#29615;&#22659;&#65292;&#20551;&#35774;&#20855;&#26377;&#22343;&#21248;&#35745;&#31639;&#36164;&#28304;&#12289;&#39640;&#32593;&#32476;&#24102;&#23485;&#12289;&#36275;&#22815;&#30340;&#20869;&#23384;&#21644;&#23384;&#20648;&#65292;&#20197;&#21450;&#22312;&#25152;&#26377;&#33410;&#28857;&#19978;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20551;&#35774;&#22312;&#36793;&#32536;&#31471;&#19981;&#19968;&#23450;&#36866;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#22312;&#32447;&#26041;&#24335;&#19979;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#12290;&#36793;&#32536;&#35745;&#31639;&#38754;&#20020;&#31995;&#32479;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#31995;&#32479;&#24322;&#36136;&#24615;&#24402;&#22240;&#20110;&#27599;&#20010;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#24102;&#23485;&#30340;&#24046;&#24322;&#65292;&#32780;&#32479;&#35745;&#24322;&#36136;&#24615;&#21017;&#26469;&#33258;&#20110;&#36793;&#32536;&#31471;&#30340;&#19981;&#24179;&#34913;&#21644;&#20559;&#26012;&#25968;&#25454;&#12290;&#22312;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#26102;&#65292;&#35774;&#22791;&#20043;&#38388;&#30340;&#19981;&#21516;&#27969;&#36895;&#20063;&#21487;&#20197;&#25104;&#20026;&#24322;&#36136;&#24615;&#30340;&#21478;&#19968;&#20010;&#26469;&#28304;&#12290;&#22914;&#26524;&#27969;&#36895;&#20302;&#20110;&#35757;&#32451;&#25209;&#37327;&#22823;&#23567;&#65292;&#35774;&#22791;&#38656;&#35201;&#31561;&#24453;&#36275;&#22815;&#30340;&#26679;&#26412;&#27969;&#20837;&#21518;&#25165;&#33021;&#25191;&#34892;&#19968;&#27425;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed deep learning (DDL) training systems are designed for cloud and data-center environments that assumes homogeneous compute resources, high network bandwidth, sufficient memory and storage, as well as independent and identically distributed (IID) data across all nodes. However, these assumptions don't necessarily apply on the edge, especially when training neural networks on streaming data in an online manner. Computing on the edge suffers from both systems and statistical heterogeneity. Systems heterogeneity is attributed to differences in compute resources and bandwidth specific to each device, while statistical heterogeneity comes from unbalanced and skewed data on the edge. Different streaming-rates among devices can be another source of heterogeneity when dealing with streaming data. If the streaming rate is lower than training batch-size, device needs to wait until enough samples have streamed in before performing a single iteration of stochastic gradient descent (SGD).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#38750;&#23616;&#37096;&#21608;&#38271;&#30340;&#20285;&#29595;&#25910;&#25947;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#19982;&#23616;&#37096;&#21508;&#21521;&#24322;&#24615;&#21608;&#38271;&#30340;&#20285;&#29595;&#25910;&#25947;&#12290;&#35813;&#24037;&#20316;&#23545;&#20110;&#29702;&#35299;&#23545;&#25239;&#24615;&#35757;&#32451;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#20197;&#21450;&#30456;&#20851;&#20248;&#21270;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2211.15223</link><description>&lt;p&gt;
&#19968;&#31181;&#22312;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#38750;&#23616;&#37096;&#21608;&#38271;&#30340;&#20285;&#29595;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Gamma-convergence of a nonlocal perimeter arising in adversarial machine learning. (arXiv:2211.15223v4 [math.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#38750;&#23616;&#37096;&#21608;&#38271;&#30340;&#20285;&#29595;&#25910;&#25947;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#19982;&#23616;&#37096;&#21508;&#21521;&#24322;&#24615;&#21608;&#38271;&#30340;&#20285;&#29595;&#25910;&#25947;&#12290;&#35813;&#24037;&#20316;&#23545;&#20110;&#29702;&#35299;&#23545;&#25239;&#24615;&#35757;&#32451;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#20197;&#21450;&#30456;&#20851;&#20248;&#21270;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#19968;&#31181;Minkowski&#31867;&#22411;&#30340;&#38750;&#23616;&#37096;&#21608;&#38271;&#23545;&#20110;&#19968;&#20010;&#23616;&#37096;&#21508;&#21521;&#24322;&#24615;&#21608;&#38271;&#30340;&#20285;&#29595;&#25910;&#25947;&#12290;&#36825;&#31181;&#38750;&#23616;&#37096;&#27169;&#22411;&#25551;&#36848;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#12290;&#33021;&#37327;&#30340;&#23454;&#36136;&#20381;&#36182;&#20110;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36825;&#20004;&#20010;&#20998;&#24067;&#27169;&#25311;&#20102;&#20851;&#32852;&#31867;&#21035;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20551;&#35774;&#23427;&#20204;&#20855;&#26377;&#26377;&#30028;&#30340;BV&#23494;&#24230;&#65292;&#20811;&#26381;&#20102;&#23545;&#20998;&#24067;&#30340;&#20856;&#22411;&#20005;&#26684;&#27491;&#21017;&#24615;&#20551;&#35774;&#12290;&#22312;&#32039;&#33268;&#24230;&#37327;&#31354;&#38388;&#33258;&#28982;&#25299;&#25169;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#19982;&#20004;&#20010;&#23494;&#24230;&#30340;&#21508;&#21521;&#24322;&#24615;&#20989;&#25968;&#30830;&#23450;&#30340;&#21152;&#26435;&#21608;&#38271;&#30340;&#20285;&#29595;&#25910;&#25947;&#12290;&#23613;&#31649;&#26159;&#23616;&#37096;&#30340;&#65292;&#36825;&#20010;&#38160;&#21033;&#30028;&#38754;&#26497;&#38480;&#21453;&#26144;&#20102;&#23545;&#25239;&#24615;&#25200;&#21160;&#19979;&#30340;&#20998;&#31867;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24212;&#29992;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#23548;&#20986;&#20102;&#20851;&#32852;&#24635;&#21464;&#24046;&#30340;&#20285;&#29595;&#25910;&#25947;&#65292;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#24182;&#35777;&#26126;&#20102;&#38750;&#23616;&#37096;&#21608;&#38271;&#30340;&#22270;&#31163;&#25955;&#30340;&#20285;&#29595;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we prove Gamma-convergence of a nonlocal perimeter of Minkowski type to a local anisotropic perimeter. The nonlocal model describes the regularizing effect of adversarial training in binary classifications. The energy essentially depends on the interaction between two distributions modelling likelihoods for the associated classes. We overcome typical strict regularity assumptions for the distributions by only assuming that they have bounded $BV$ densities. In the natural topology coming from compactness, we prove Gamma-convergence to a weighted perimeter with weight determined by an anisotropic function of the two densities. Despite being local, this sharp interface limit reflects classification stability with respect to adversarial perturbations. We further apply our results to deduce Gamma-convergence of the associated total variations, to study the asymptotics of adversarial training, and to prove Gamma-convergence of graph discretizations for the nonlocal perimeter.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26435;&#37325;&#20462;&#21098;&#12289;&#22836;&#37096;&#20462;&#21098;&#12289;&#20302;&#31209;&#36924;&#36817;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#22522;&#26412;&#30340;&#21387;&#32553;&#25216;&#26415;&#26159;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21387;&#32553;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.09949</link><description>&lt;p&gt;
&#23545;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#36827;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Compressing Transformer-based self-supervised models for speech processing. (arXiv:2211.09949v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26435;&#37325;&#20462;&#21098;&#12289;&#22836;&#37096;&#20462;&#21098;&#12289;&#20302;&#31209;&#36924;&#36817;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#22522;&#26412;&#30340;&#21387;&#32553;&#25216;&#26415;&#26159;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#26159;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#21508;&#31181;&#35774;&#22791;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#30446;&#21069;&#24050;&#26377;&#19968;&#20123;&#23396;&#31435;&#30340;&#23581;&#35797;&#26469;&#21387;&#32553;Transformer&#65292;&#20294;&#30740;&#31350;&#20013;&#30340;&#35774;&#32622;&#21644;&#25351;&#26631;&#21508;&#19981;&#30456;&#21516;&#12290;&#27492;&#21069;&#30340;&#24037;&#20316;&#24456;&#23569;&#28041;&#21450;&#19981;&#21516;&#21387;&#32553;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36825;&#20351;&#24471;&#27604;&#36739;&#21387;&#32553;&#25216;&#26415;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#36825;&#20123;&#23396;&#31435;&#32467;&#26524;&#25552;&#20379;&#32972;&#26223;&#65292;&#30740;&#31350;&#20960;&#31181;&#24120;&#29992;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#21253;&#25324;&#26435;&#37325;&#20462;&#21098;&#12289;&#22836;&#37096;&#20462;&#21098;&#12289;&#20302;&#31209;&#36924;&#36817;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#22312;&#19981;&#21516;&#21387;&#32553;&#29575;&#19979;&#30340;&#26435;&#34913;&#65292;&#21253;&#25324;&#22681;&#38047;&#26102;&#38388;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#20056;&#21152;&#25805;&#20316;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#36817;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#26412;&#30340;&#21387;&#32553;&#25216;&#26415;&#26159;&#24378;&#22823;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20960;&#31181;&#21387;&#32553;&#26041;&#27861;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of Transformers in self- supervised learning with applications to various downstream tasks, the computational cost of training and inference remains a major challenge for applying these models to a wide spectrum of devices. Several isolated attempts have been made to compress Transformers, but the settings and metrics are different across studies. Trade-off at various compression rates are also largely missing in prior work, making it difficult to compare compression techniques. In this work, we aim to provide context for the isolated results, studying several commonly used compression techniques, including weight pruning, head pruning, low-rank approximation, and knowledge distillation. We report trade- off at various compression rate, including wall-clock time, the number of parameters, and the number of multiply-accumulate operations. Our results show that compared to recent approaches, basic compression techniques are strong baselines. We further present several
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#20272;&#35745;&#26377;&#38480;&#25511;&#21046;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#65292;&#24182;&#36890;&#36807;&#35760;&#24405;&#31574;&#30053;&#30340;&#28151;&#21512;&#29305;&#24615;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#21644;&#26368;&#23567;&#21270;&#26465;&#20214;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23454;&#29616;&#29305;&#23450;&#30340;&#32479;&#35745;&#39118;&#38505;&#30028;&#38480;&#28041;&#21450;&#21040;&#28151;&#21512;&#29305;&#24615;&#30340;&#24378;&#24230;&#21644;&#26679;&#26412;&#25968;&#37327;&#20043;&#38388;&#24494;&#22937;&#32780;&#26377;&#36259;&#30340;&#26435;&#34913;&#12290;&#36824;&#20351;&#29992;&#36825;&#20123;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#24314;&#31435;&#20102;&#31163;&#32447;&#35780;&#20272;&#24658;&#23450;&#39532;&#23572;&#21487;&#22827;&#25511;&#21046;&#31574;&#30053;&#30340;&#30456;&#20851;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2211.07092</link><description>&lt;p&gt;
&#31163;&#32447;&#20272;&#35745;&#25511;&#21046;&#39532;&#23572;&#21487;&#22827;&#38142;&#65306;&#26368;&#23567;&#21270;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Offline Estimation of Controlled Markov Chains: Minimaxity and Sample Complexity. (arXiv:2211.07092v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#20272;&#35745;&#26377;&#38480;&#25511;&#21046;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#65292;&#24182;&#36890;&#36807;&#35760;&#24405;&#31574;&#30053;&#30340;&#28151;&#21512;&#29305;&#24615;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#21644;&#26368;&#23567;&#21270;&#26465;&#20214;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23454;&#29616;&#29305;&#23450;&#30340;&#32479;&#35745;&#39118;&#38505;&#30028;&#38480;&#28041;&#21450;&#21040;&#28151;&#21512;&#29305;&#24615;&#30340;&#24378;&#24230;&#21644;&#26679;&#26412;&#25968;&#37327;&#20043;&#38388;&#24494;&#22937;&#32780;&#26377;&#36259;&#30340;&#26435;&#34913;&#12290;&#36824;&#20351;&#29992;&#36825;&#20123;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#24314;&#31435;&#20102;&#31163;&#32447;&#35780;&#20272;&#24658;&#23450;&#39532;&#23572;&#21487;&#22827;&#25511;&#21046;&#31574;&#30053;&#30340;&#30456;&#20851;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#38480;&#25511;&#21046;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#30340;&#33258;&#28982;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#35774;&#32622;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20351;&#29992;&#25152;&#35859;&#30340;&#35760;&#24405;&#31574;&#30053;&#25910;&#38598;&#30340;&#12290;&#25105;&#20204;&#20026;&#20272;&#35745;&#22120;&#24320;&#21457;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#30028;&#38480;&#65292;&#24182;&#24314;&#31435;&#20102;&#26368;&#23567;&#21270;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#30028;&#38480;&#36890;&#36807;&#35760;&#24405;&#31574;&#30053;&#30340;&#28151;&#21512;&#29305;&#24615;&#26469;&#30830;&#23450;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23454;&#29616;&#29305;&#23450;&#30340;&#32479;&#35745;&#39118;&#38505;&#30028;&#38480;&#28041;&#21450;&#21040;&#28151;&#21512;&#29305;&#24615;&#30340;&#24378;&#24230;&#21644;&#26679;&#26412;&#25968;&#37327;&#20043;&#38388;&#24494;&#22937;&#32780;&#26377;&#36259;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#31034;&#20363;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#36951;&#20256;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#24369;&#36951;&#20256;&#38750;&#40784;&#27425;&#39532;&#23572;&#21487;&#22827;&#38142;&#21644;&#20855;&#26377;&#38750;&#24179;&#31283;&#39532;&#23572;&#21487;&#22827;&#12289;&#38454;&#27573;&#24615;&#21644;&#36138;&#23146;&#25511;&#21046;&#30340;&#25511;&#21046;&#39532;&#23572;&#21487;&#22827;&#38142;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#26469;&#24314;&#31435;&#31163;&#32447;&#35780;&#20272;&#24658;&#23450;&#39532;&#23572;&#21487;&#22827;&#25511;&#21046;&#31574;&#30053;&#30340;&#30456;&#20851;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study a natural nonparametric estimator of the transition probability matrices of a finite controlled Markov chain. We consider an offline setting with a fixed dataset, collected using a so-called logging policy. We develop sample complexity bounds for the estimator and establish conditions for minimaxity. Our statistical bounds depend on the logging policy through its mixing properties. We show that achieving a particular statistical risk bound involves a subtle and interesting trade-off between the strength of the mixing properties and the number of samples. We demonstrate the validity of our results under various examples, such as ergodic Markov chains, weakly ergodic inhomogeneous Markov chains, and controlled Markov chains with non-stationary Markov, episodic, and greedy controls. Lastly, we use these sample complexity bounds to establish concomitant ones for offline evaluation of stationary Markov control policies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#27979;&#22788;&#29702;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;P4O&#65289;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#33258;&#36523;&#24863;&#35273;&#29366;&#24577;&#26469;&#26368;&#23567;&#21270;&#24778;&#24322;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#32047;&#31215;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2211.06236</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#39044;&#27979;&#22788;&#29702;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Deep Reinforcement Learning with Predictive Processing Proximal Policy Optimization. (arXiv:2211.06236v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06236
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#27979;&#22788;&#29702;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;P4O&#65289;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#33258;&#36523;&#24863;&#35273;&#29366;&#24577;&#26469;&#26368;&#23567;&#21270;&#24778;&#24322;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#32047;&#31215;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#27493;&#24120;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#19988;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20173;&#28982;&#19981;&#39640;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#22823;&#33041;&#33021;&#22815;&#20351;&#29992;&#26377;&#38480;&#30340;&#36164;&#28304;&#26377;&#25928;&#22320;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#20511;&#37492;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#26469;&#25913;&#36827;&#24403;&#21069;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#39044;&#27979;&#22788;&#29702;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23427;&#35748;&#20026;&#20154;&#31867;&#22823;&#33041;&#20027;&#21160;&#23547;&#27714;&#26368;&#23567;&#21270;&#24778;&#24322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33021;&#22815;&#39044;&#27979;&#33258;&#36523;&#24863;&#35273;&#29366;&#24577;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#26368;&#23567;&#21270;&#24778;&#24322;&#65292;&#20174;&#32780;&#22312;&#32047;&#31215;&#22870;&#21169;&#19978;&#21462;&#24471;&#24040;&#22823;&#30340;&#25910;&#30410;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#27979;&#22788;&#29702;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;P4O&#65289;&#26234;&#33021;&#20307;&#65307;&#23427;&#26159;&#19968;&#20010;&#23558;&#39044;&#27979;&#22788;&#29702;&#24212;&#29992;&#21040;&#22522;&#20110;&#36882;&#24402;&#30340;PPO&#31639;&#27861;&#30340;&#28436;&#21592;&#25209;&#21028;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#23558;&#19990;&#30028;&#27169;&#22411;&#38598;&#25104;&#21040;&#20854;&#38544;&#34255;&#29366;&#24577;&#20013;&#12290;&#21363;&#20351;&#27809;&#26377;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;P4O&#19982;&#22522;&#32447;&#36882;&#24402;&#21464;&#20307;&#30456;&#27604;&#65292;&#20063;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in reinforcement learning (RL) often rely on massive compute resources and remain notoriously sample inefficient. In contrast, the human brain is able to efficiently learn effective control strategies using limited resources. This raises the question whether insights from neuroscience can be used to improve current RL methods. Predictive processing is a popular theoretical framework which maintains that the human brain is actively seeking to minimize surprise. We show that recurrent neural networks which predict their own sensory states can be leveraged to minimise surprise, yielding substantial gains in cumulative reward. Specifically, we present the Predictive Processing Proximal Policy Optimization (P4O) agent; an actor-critic reinforcement learning agent that applies predictive processing to a recurrent variant of the PPO algorithm by integrating a world model in its hidden state. Even without hyperparameter tuning, P4O significantly outperforms a baseline recurrent varian
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#35775;&#38382;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;3D&#28857;&#20113;&#20013;&#30340;&#23545;&#25239;&#28857;&#65292;&#25552;&#20379;&#20102;&#26080;&#30418;&#23376;&#25915;&#20987;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2210.14164</link><description>&lt;p&gt;
3D&#28857;&#20113;&#20998;&#31867;&#30340;&#26080;&#30418;&#23376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
No-Box Attacks on 3D Point Cloud Classification. (arXiv:2210.14164v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#35775;&#38382;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;3D&#28857;&#20113;&#20013;&#30340;&#23545;&#25239;&#28857;&#65292;&#25552;&#20379;&#20102;&#26080;&#30418;&#23376;&#25915;&#20987;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21508;&#31181;&#36755;&#20837;&#20449;&#21495;&#30340;&#20998;&#26512;&#65292;&#23545;&#25239;&#25915;&#20987;&#26500;&#25104;&#20102;&#20005;&#37325;&#25361;&#25112;&#12290;&#22312;3D&#28857;&#20113;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#35782;&#21035;&#22312;&#32593;&#32476;&#20915;&#31574;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#30340;&#28857;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#22312;&#29983;&#25104;&#29616;&#26377;&#30340;&#23545;&#25239;&#25915;&#20987;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#26174;&#33879;&#24615;&#22270;&#26041;&#27861;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#23545;&#25239;&#25915;&#20987;&#20250;&#26174;&#33879;&#24433;&#21709;&#32593;&#32476;&#20915;&#31574;&#30340;&#28857;&#12290;&#36890;&#24120;&#65292;&#35782;&#21035;&#23545;&#25239;&#28857;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#35775;&#38382;&#65292;&#20197;&#30830;&#23450;&#21738;&#20123;&#28857;&#23545;&#27169;&#22411;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#22312;&#19981;&#35775;&#38382;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#23545;&#25239;&#28857;&#65292;&#36825;&#34987;&#31216;&#20026;&#8220;&#26080;&#30418;&#23376;&#8221;&#25915;&#20987;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;14&#20010;&#28857;&#20113;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#26469;&#26816;&#26597;&#36825;&#20123;&#29305;&#24449;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#23545;&#25239;&#28857;&#65292;&#20197;&#21450;&#21738;&#20123;&#29305;&#24449;&#23545;&#39044;&#27979;&#26368;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks pose serious challenges for deep neural network (DNN)-based analysis of various input signals. In the case of 3D point clouds, methods have been developed to identify points that play a key role in network decision, and these become crucial in generating existing adversarial attacks. For example, a saliency map approach is a popular method for identifying adversarial drop points, whose removal would significantly impact the network decision. Generally, methods for identifying adversarial points rely on the access to the DNN model itself to determine which points are critically important for the model's decision. This paper aims to provide a novel viewpoint on this problem, where adversarial points can be predicted without access to the target DNN model, which is referred to as a ``no-box'' attack. To this end, we define 14 point cloud features and use multiple linear regression to examine whether these features can be used for adversarial point prediction, and which
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36229;&#24230;&#37327;&#26641;&#30340;&#26041;&#27861;&#65292;&#20197;&#36924;&#36817;&#21407;&#22987;&#31354;&#38388;&#20013;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#12290;&#36890;&#36807;&#22312;&#36229;&#24230;&#37327;&#31354;&#38388;&#19978;&#36827;&#34892;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#65292;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#20855;&#26377;&#26368;&#20248;&#26641;&#32467;&#26500;&#30340;&#26641;-&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2210.12288</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#26368;&#20248;&#36755;&#36816;&#22238;&#24402;&#30340;&#36229;&#24230;&#37327;&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning Ultrametric Trees for Optimal Transport Regression. (arXiv:2210.12288v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36229;&#24230;&#37327;&#26641;&#30340;&#26041;&#27861;&#65292;&#20197;&#36924;&#36817;&#21407;&#22987;&#31354;&#38388;&#20013;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#12290;&#36890;&#36807;&#22312;&#36229;&#24230;&#37327;&#31354;&#38388;&#19978;&#36827;&#34892;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#65292;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#20855;&#26377;&#26368;&#20248;&#26641;&#32467;&#26500;&#30340;&#26641;-&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#25552;&#20379;&#20102;&#19968;&#20010;&#24230;&#37327;&#37327;&#21270;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#24046;&#24322;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#25903;&#25745;&#22312;&#31163;&#25955;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;&#27979;&#24230;&#65292;&#25214;&#21040;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#26159;&#19982;&#31354;&#38388;&#22823;&#23567;&#21576;&#31435;&#26041;&#20851;&#31995;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#26641;&#19978;&#25903;&#25745;&#30340;&#27979;&#24230;&#21487;&#20197;&#36890;&#36807;&#38381;&#21512;&#24418;&#24335;&#30340;&#26368;&#20248;&#36755;&#36816;&#26469;&#35745;&#31639;&#65292;&#36825;&#26679;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#26159;&#32447;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#23545;&#20110;&#32473;&#23450;&#30340;&#31163;&#25955;&#24230;&#37327;&#31354;&#38388;&#26469;&#35828;&#20855;&#26377;&#26368;&#20248;&#26641;&#32467;&#26500;&#30340;&#26641;-&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#65292;&#20197;&#36924;&#36817;&#21407;&#22987;&#31354;&#38388;&#20013;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#12290;&#25105;&#20204;&#30340;&#19968;&#20010;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#36229;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#38382;&#39064;&#12290;&#36825;&#26679;&#20570;&#26377;&#21161;&#20110;&#25105;&#20204;&#36890;&#36807;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#22312;&#36229;&#24230;&#37327;&#30697;&#38453;&#31354;&#38388;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#36825;&#26159;&#19968;&#20010;&#28151;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23618;&#27425;&#26368;&#23567;&#29983;&#25104;&#26641;&#31639;&#27861;&#23558;&#21442;&#25968;&#25237;&#24433;&#21040;&#36229;&#24230;&#37327;&#31354;&#38388;&#65292;&#30456;&#24403;&#20110;&#23545;&#36229;&#24230;&#37327;&#30340;&#26368;&#25509;&#36817;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport provides a metric which quantifies the dissimilarity between probability measures. For measures supported in discrete metric spaces, finding the optimal transport distance has cubic time complexity in the size of the space. However, measures supported on trees admit a closed-form optimal transport that can be computed in linear time. In this paper, we aim to find an optimal tree structure for a given discrete metric space so that the tree-Wasserstein distance approximates the optimal transport distance in the original space. One of our key ideas is to cast the problem in ultrametric spaces. This helps us optimize over the space of ultrametric trees -- a mixed-discrete and continuous optimization problem -- via projected gradient decent over the space of ultrametric matrices. During optimization, we project the parameters to the ultrametric space via a hierarchical minimum spanning tree algorithm, equivalent to the closest projection to ultrametrics under the supremum 
&lt;/p&gt;</description></item><item><title>&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;ICL&#23545;&#22810;&#31181;&#25200;&#21160;&#31867;&#22411;&#20855;&#26377;&#25935;&#24863;&#24615;&#65292;&#26631;&#31614;&#20559;&#24046;&#23548;&#33268;&#36807;&#21435;&#30340;&#30740;&#31350;&#20302;&#20272;&#20102;ICL&#30340;&#25935;&#24863;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ICL&#30340;&#25935;&#24863;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21576;&#29616;&#36127;&#30456;&#20851;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;SenSel&#65292;&#23427;&#22312;&#25918;&#24323;&#25935;&#24863;&#39044;&#27979;&#20915;&#31574;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#24120;&#29992;&#22522;&#20934;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.07661</link><description>&lt;p&gt;
&#20851;&#20110;&#25935;&#24863;&#24615;&#19982;&#20934;&#30830;&#24615;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Relation between Sensitivity and Accuracy in In-context Learning. (arXiv:2209.07661v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07661
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;ICL&#23545;&#22810;&#31181;&#25200;&#21160;&#31867;&#22411;&#20855;&#26377;&#25935;&#24863;&#24615;&#65292;&#26631;&#31614;&#20559;&#24046;&#23548;&#33268;&#36807;&#21435;&#30340;&#30740;&#31350;&#20302;&#20272;&#20102;ICL&#30340;&#25935;&#24863;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ICL&#30340;&#25935;&#24863;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21576;&#29616;&#36127;&#30456;&#20851;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;SenSel&#65292;&#23427;&#22312;&#25918;&#24323;&#25935;&#24863;&#39044;&#27979;&#20915;&#31574;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#24120;&#29992;&#22522;&#20934;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064; (In-context learning, ICL) &#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24120;&#24120;&#21463;&#21040;&#25552;&#31034;&#30340;&#36807;&#24230;&#25935;&#24863;&#24615;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#19981;&#21487;&#38752;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;ICL&#23545;&#22810;&#31181;&#25200;&#21160;&#31867;&#22411;&#30340;&#25935;&#24863;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#31614;&#20559;&#24046;&#25513;&#30422;&#20102;&#30495;&#23454;&#30340;&#25935;&#24863;&#24615;&#65292;&#22240;&#27492;&#20043;&#21069;&#30340;&#30740;&#31350;&#21487;&#33021;&#22823;&#22823;&#20302;&#20272;&#20102;ICL&#30340;&#25935;&#24863;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ICL&#30340;&#25935;&#24863;&#24615;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#36127;&#30456;&#20851;&#20851;&#31995;&#65306;&#23545;&#25200;&#21160;&#25935;&#24863;&#30340;&#39044;&#27979;&#26356;&#19981;&#23481;&#26131;&#27491;&#30830;&#12290;&#22312;&#36825;&#20123;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SenSel&#30340;&#23569;&#26679;&#26412;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#23545;&#25935;&#24863;&#39044;&#27979;&#30340;&#20351;&#29992;&#12290;&#22312;&#21313;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SenSel&#22312;&#25918;&#24323;&#39044;&#27979;&#20915;&#31574;&#19978;&#22987;&#32456;&#20248;&#20110;&#20004;&#31181;&#24120;&#29992;&#30340;&#22522;&#20110;&#32622;&#20449;&#24230;&#21644;&#22522;&#20110;&#29109;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) suffers from oversensitivity to the prompt, making it unreliable in real-world scenarios. We study the sensitivity of ICL with respect to multiple perturbation types. First, we find that label bias obscures the true sensitivity, and therefore prior work may have significantly underestimated ICL sensitivity. Second, we observe a strong negative correlation between ICL sensitivity and accuracy: predictions sensitive to perturbations are less likely to be correct. Motivated by these findings, we propose \textsc{SenSel}, a few-shot selective prediction method that abstains from sensitive predictions. Experiments on ten classification datasets show that \textsc{SenSel} consistently outperforms two commonly used confidence-based and entropy-based baselines on abstention decisions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31890;&#23376;&#26041;&#27861;DeepParticle&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#29983;&#25104;Keller-Segel&#36235;&#21270;&#31995;&#32479;&#20013;&#30340;&#32858;&#38598;&#27169;&#24335;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#24182;&#36716;&#25442;&#28304;&#20998;&#24067;&#21040;&#30446;&#26631;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#29289;&#29702;&#21442;&#25968;&#21464;&#21270;&#19979;&#35299;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2209.00109</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#21644;&#29983;&#25104;&#22810;&#32500;Keller-Segel&#36235;&#21270;&#31995;&#32479;&#32858;&#38598;&#27169;&#24335;&#30340;DeepParticle&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A DeepParticle method for learning and generating aggregation patterns in multi-dimensional Keller-Segel chemotaxis systems. (arXiv:2209.00109v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31890;&#23376;&#26041;&#27861;DeepParticle&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#29983;&#25104;Keller-Segel&#36235;&#21270;&#31995;&#32479;&#20013;&#30340;&#32858;&#38598;&#27169;&#24335;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#24182;&#36716;&#25442;&#28304;&#20998;&#24067;&#21040;&#30446;&#26631;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#29289;&#29702;&#21442;&#25968;&#21464;&#21270;&#19979;&#35299;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20108;&#32500;&#21644;&#19977;&#32500;&#31354;&#38388;&#20013;Keller-Segel&#36235;&#21270;&#31995;&#32479;&#30340;&#32858;&#38598;&#27169;&#24335;&#21644;&#36817;&#22855;&#24322;&#35299;&#65292;&#28982;&#21518;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;DeepParticle&#65288;DP&#65289;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#21644;&#29983;&#25104;&#22312;&#29289;&#29702;&#21442;&#25968;&#21464;&#21270;&#19979;&#30340;&#35299;&#12290;KS&#35299;&#34987;&#36817;&#20284;&#20026;&#33258;&#36866;&#24212;&#20110;&#35299;&#30340;&#39640;&#26799;&#24230;&#37096;&#20998;&#30340;&#31890;&#23376;&#30340;&#32463;&#39564;&#24230;&#37327;&#12290;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#34920;&#31034;&#20174;&#32473;&#23450;&#21021;&#22987;&#65288;&#28304;&#65289;&#20998;&#24067;&#21040;&#26377;&#38480;&#26102;&#38388;T&#20043;&#21069;&#30340;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#36716;&#25442;&#65292;&#32780;&#19981;&#20551;&#35774;&#36716;&#25442;&#30340;&#21487;&#36870;&#24615;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#21270;&#36755;&#20837;&#21644;&#30446;&#26631;&#32463;&#39564;&#27979;&#24230;&#20043;&#38388;&#30340;&#31163;&#25955;2-Wasserstein&#36317;&#31163;&#26469;&#26356;&#26032;&#32593;&#32476;&#26435;&#37325;&#12290;&#20026;&#20102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#20998;&#32780;&#27835;&#20043;&#30340;&#31639;&#27861;&#65292;&#20197;&#22312;Wasserstein&#36317;&#31163;&#20013;&#25214;&#21040;&#26368;&#20248;&#36807;&#28193;&#30697;&#38453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a regularized interacting particle method for computing aggregation patterns and near singular solutions of a Keller-Segal (KS) chemotaxis system in two and three space dimensions, then further develop DeepParticle (DP) method to learn and generate solutions under variations of physical parameters. The KS solutions are approximated as empirical measures of particles which self-adapt to the high gradient part of solutions. We utilize the expressiveness of deep neural networks (DNNs) to represent the transform of samples from a given initial (source) distribution to a target distribution at finite time T prior to blowup without assuming invertibility of the transforms. In the training stage, we update the network weights by minimizing a discrete 2-Wasserstein distance between the input and target empirical measures. To reduce computational cost, we develop an iterative divide-and-conquer algorithm to find the optimal transition matrix in the Wasserstein distance. We present nume
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23618;&#32423;&#21069;&#21521;&#27169;&#22411;&#23454;&#29616;&#22810;&#26102;&#38388;&#27573;&#34920;&#31034;&#30340;&#36741;&#21161;&#20219;&#21153;HKSL&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#20010;&#34920;&#31034;&#21644;&#19981;&#21516;&#27493;&#38271;&#19979;&#30340;&#35780;&#35770;&#23478;&#65292;&#33021;&#22815;&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#26356;&#39640;&#30340;&#25110;&#26368;&#20248;&#30340;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2206.11396</link><description>&lt;p&gt;
&#20351;&#29992;&#23618;&#32423;&#21069;&#21521;&#27169;&#22411;&#23454;&#29616;&#22810;&#26102;&#38388;&#27573;&#34920;&#31034;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Horizon Representations with Hierarchical Forward Models for Reinforcement Learning. (arXiv:2206.11396v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11396
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23618;&#32423;&#21069;&#21521;&#27169;&#22411;&#23454;&#29616;&#22810;&#26102;&#38388;&#27573;&#34920;&#31034;&#30340;&#36741;&#21161;&#20219;&#21153;HKSL&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#20010;&#34920;&#31034;&#21644;&#19981;&#21516;&#27493;&#38271;&#19979;&#30340;&#35780;&#35770;&#23478;&#65292;&#33021;&#22815;&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#26356;&#39640;&#30340;&#25110;&#26368;&#20248;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#26469;&#35828;&#65292;&#20174;&#20687;&#32032;&#23398;&#20064;&#25511;&#21046;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#34920;&#31034;&#23398;&#20064;&#21644;&#31574;&#30053;&#23398;&#20064;&#26159;&#30456;&#20114;&#20132;&#32455;&#30340;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#36741;&#21161;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#19981;&#32771;&#34385;&#38382;&#39064;&#30340;&#26102;&#38388;&#26041;&#38754;&#65292;&#35201;&#20040;&#21482;&#32771;&#34385;&#21333;&#27493;&#36716;&#25442;&#65292;&#22914;&#26524;&#37325;&#35201;&#30340;&#29615;&#22659;&#21464;&#21270;&#38656;&#35201;&#22810;&#20010;&#27493;&#39588;&#25165;&#33021;&#34920;&#29616;&#20986;&#26469;&#65292;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Hierarchical $k$-Step Latent (HKSL)&#65292;&#19968;&#31181;&#36890;&#36807;&#23618;&#32423;&#21069;&#21521;&#27169;&#22411;&#23398;&#20064;&#22810;&#20010;&#34920;&#31034;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#36890;&#20449;&#20197;&#21450;&#19968;&#31995;&#21015;&#22312;&#19981;&#21516;&#27493;&#38271;&#19979;&#24037;&#20316;&#30340;$n$&#27493;&#35780;&#35770;&#23478;&#12290;&#25105;&#20204;&#22312;30&#20010;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#21644;&#19968;&#20010;&#25105;&#20204;&#21019;&#24314;&#30340;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;HKSL&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#26377;&#21644;&#26080;&#24178;&#25200;&#29289;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#20854;&#20182;&#20960;&#31181;&#26367;&#20195;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;HKSL&#35201;&#20040;&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#26356;&#39640;&#30340;&#25110;&#26368;&#20248;&#30340;&#22238;&#25253;&#65292;&#35201;&#20040;&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning control from pixels is difficult for reinforcement learning (RL) agents because representation learning and policy learning are intertwined. Previous approaches remedy this issue with auxiliary representation learning tasks, but they either do not consider the temporal aspect of the problem or only consider single-step transitions, which may cause learning inefficiencies if important environmental changes take many steps to manifest. We propose Hierarchical $k$-Step Latent (HKSL), an auxiliary task that learns multiple representations via a hierarchy of forward models that learn to communicate and an ensemble of $n$-step critics that all operate at varying magnitudes of step skipping. We evaluate HKSL in a suite of 30 robotic control tasks with and without distractors and a task of our creation. We find that HKSL either converges to higher or optimal episodic returns more quickly than several alternative representation learning approaches. Furthermore, we find that HKSL's repr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#21307;&#30103;&#26426;&#26500;&#38388;&#25968;&#25454;&#20849;&#20139;&#30340;&#38544;&#31169;&#38480;&#21046;&#21644;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#36890;&#20449;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#24615;&#12290;&#35813;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#35777;&#26126;&#20197;&#21450;&#22312;&#29616;&#23454;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#25311;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.05581</link><description>&lt;p&gt;
&#32852;&#37030;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Offline Reinforcement Learning. (arXiv:2206.05581v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#21307;&#30103;&#26426;&#26500;&#38388;&#25968;&#25454;&#20849;&#20139;&#30340;&#38544;&#31169;&#38480;&#21046;&#21644;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#36890;&#20449;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#24615;&#12290;&#35813;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#35777;&#26126;&#20197;&#21450;&#22312;&#29616;&#23454;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#25311;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35777;&#25454;&#25110;&#25968;&#25454;&#30340;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#23545;&#20110;&#20010;&#24615;&#21270;&#21307;&#30103;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#21463;&#30410;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#34429;&#28982;&#21307;&#30103;&#26426;&#26500;&#38388;&#26377;&#22823;&#37327;&#20581;&#24247;&#25968;&#25454;&#21487;&#29992;&#65292;&#20294;&#30001;&#20110;&#38544;&#31169;&#38480;&#21046;&#65292;&#23427;&#20204;&#26080;&#27861;&#20849;&#20139;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#31449;&#28857;&#23384;&#22312;&#24322;&#36136;&#24615;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#31163;&#32447;RL&#31639;&#27861;&#26159;&#24517;&#35201;&#30340;&#19988;&#26377;&#21069;&#36884;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31449;&#28857;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#27169;&#22411;&#65292;&#20801;&#35768;&#31449;&#28857;&#20043;&#38388;&#30340;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#25928;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#20998;&#26512;&#31449;&#28857;&#32423;&#29305;&#24449;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31163;&#32447;RL&#32852;&#37030;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#36890;&#20449;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#24615;&#65292;&#20165;&#38656;&#35201;&#36890;&#36807;&#20132;&#25442;&#25688;&#35201;&#32479;&#35745;&#37327;&#36827;&#34892;&#19968;&#36718;&#36890;&#20449;&#20132;&#20114;&#12290;&#25105;&#20204;&#20026;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#26080;&#38656;&#20551;&#35774;&#31449;&#28857;&#20043;&#38388;&#20855;&#26377;&#30456;&#21516;&#30340;&#36716;&#25442;&#21160;&#24577;&#12290;&#25105;&#20204;&#22312;&#29616;&#23454;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27169;&#25311;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evidence-based or data-driven dynamic treatment regimes are essential for personalized medicine, which can benefit from offline reinforcement learning (RL). Although massive healthcare data are available across medical institutions, they are prohibited from sharing due to privacy constraints. Besides, heterogeneity exists in different sites. As a result, federated offline RL algorithms are necessary and promising to deal with the problems. In this paper, we propose a multi-site Markov decision process model which allows both homogeneous and heterogeneous effects across sites. The proposed model makes the analysis of the site-level features possible. We design the first federated policy optimization algorithm for offline RL with sample complexity. The proposed algorithm is communication-efficient and privacy-preserving, which requires only a single round of communication interaction by exchanging summary statistics. We give a theoretical guarantee for the proposed algorithm without the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23500;&#26377;&#20915;&#31574;&#23454;&#38469;&#24847;&#20041;&#30340;&#32858;&#21512;&#20989;&#25968;&#31867;&#65292;&#20197;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#36890;&#36807;&#20998;&#26512;&#35889;&#39118;&#38505;&#24230;&#37327;&#12289;Choquet&#31215;&#20998;&#21644;Lorentz&#33539;&#25968;&#30340;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#39118;&#38505;&#24230;&#37327;&#36827;&#34892;&#20998;&#23618;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2206.03183</link><description>&lt;p&gt;
&#39118;&#38505;&#24230;&#37327;&#21644;&#19978;&#27010;&#29575;&#65306;&#36830;&#36143;&#24615;&#21644;&#20998;&#23618;
&lt;/p&gt;
&lt;p&gt;
Risk Measures and Upper Probabilities: Coherence and Stratification. (arXiv:2206.03183v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23500;&#26377;&#20915;&#31574;&#23454;&#38469;&#24847;&#20041;&#30340;&#32858;&#21512;&#20989;&#25968;&#31867;&#65292;&#20197;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#36890;&#36807;&#20998;&#26512;&#35889;&#39118;&#38505;&#24230;&#37327;&#12289;Choquet&#31215;&#20998;&#21644;Lorentz&#33539;&#25968;&#30340;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#39118;&#38505;&#24230;&#37327;&#36827;&#34892;&#20998;&#23618;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36890;&#24120;&#20551;&#35774;&#32463;&#20856;&#27010;&#29575;&#35770;&#65292;&#36825;&#24847;&#21619;&#30528;&#32858;&#21512;&#24314;&#31435;&#22312;&#26399;&#26395;&#20043;&#19978;&#12290;&#29616;&#22312;&#26377;&#22810;&#20010;&#29702;&#30001;&#26469;&#23547;&#25214;&#27604;&#32463;&#20856;&#27010;&#29575;&#35770;&#26356;&#20016;&#23500;&#30340;&#26367;&#20195;&#25968;&#23398;&#22522;&#30784;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#19968;&#31181;&#24378;&#22823;&#32780;&#20016;&#23500;&#30340;&#26367;&#20195;&#32858;&#21512;&#20989;&#25968;&#31867;&#65292;&#34987;&#31216;&#20026;&#35889;&#39118;&#38505;&#24230;&#37327;&#12289;Choquet&#31215;&#20998;&#25110;Lorentz&#33539;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#34920;&#24449;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20010;&#35889;&#23478;&#26063;&#30340;&#29305;&#27530;&#20043;&#22788;&#12290;&#36890;&#36807;&#21033;&#29992;&#21487;&#37325;&#25490;&#19981;&#21464;Banach&#31354;&#38388;&#29702;&#35770;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#23545;&#25152;&#26377;&#36830;&#36143;&#39118;&#38505;&#24230;&#37327;&#36827;&#34892;&#20998;&#23618;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#31034;&#23427;&#20204;&#25152;&#24341;&#23548;&#30340;&#19978;&#27010;&#29575;&#12290;&#25105;&#20204;&#20174;&#23454;&#35777;&#35282;&#24230;&#28436;&#31034;&#20102;&#36825;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#26041;&#27861;&#22914;&#20309;&#24110;&#21161;&#35299;&#20915;&#23454;&#38469;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning typically presupposes classical probability theory which implies that aggregation is built upon expectation. There are now multiple reasons to motivate looking at richer alternatives to classical probability theory as a mathematical foundation for machine learning. We systematically examine a powerful and rich class of alternative aggregation functionals, known variously as spectral risk measures, Choquet integrals or Lorentz norms. We present a range of characterization results, and demonstrate what makes this spectral family so special. In doing so we arrive at a natural stratification of all coherent risk measures in terms of the upper probabilities that they induce by exploiting results from the theory of rearrangement invariant Banach spaces. We empirically demonstrate how this new approach to uncertainty helps tackling practical machine learning problems.
&lt;/p&gt;</description></item><item><title>AugLoss&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#25968;&#25454;&#22686;&#24378;&#21644;&#31283;&#20581;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#35757;&#32451;&#26102;&#30340;&#22122;&#22768;&#26631;&#27880;&#21644;&#27979;&#35797;&#26102;&#30340;&#29305;&#24449;&#20998;&#24067;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.02286</link><description>&lt;p&gt;
AugLoss&#65306;&#19968;&#31181;&#31283;&#20581;&#30340;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AugLoss: A Robust Augmentation-based Fine Tuning Methodology. (arXiv:2206.02286v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02286
&lt;/p&gt;
&lt;p&gt;
AugLoss&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#25968;&#25454;&#22686;&#24378;&#21644;&#31283;&#20581;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#35757;&#32451;&#26102;&#30340;&#22122;&#22768;&#26631;&#27880;&#21644;&#27979;&#35797;&#26102;&#30340;&#29305;&#24449;&#20998;&#24067;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#38754;&#20020;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#35757;&#32451;&#38454;&#27573;&#30340;&#22122;&#22768;&#26631;&#27880;&#21644;&#27979;&#35797;&#38454;&#27573;&#30340;&#29305;&#24449;&#20998;&#24067;&#36716;&#31227;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#19968;&#27425;&#21482;&#35299;&#20915;&#19968;&#20010;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#19978;&#12290;&#20363;&#22914;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#35843;&#30340;&#31283;&#20581;&#25439;&#22833;&#20989;&#25968;&#26469;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#65292;&#20197;&#21450;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#65288;&#20363;&#22914;AugMix&#65289;&#26469;&#35299;&#20915;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#12290;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AugLoss&#65292;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#25968;&#25454;&#22686;&#24378;&#21644;&#31283;&#20581;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#35757;&#32451;&#26102;&#30340;&#22122;&#22768;&#26631;&#27880;&#21644;&#27979;&#35797;&#26102;&#30340;&#29305;&#24449;&#20998;&#24067;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#30495;&#23454;&#25968;&#25454;&#38598;&#25439;&#22351;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#19982;&#20197;&#21069;&#26041;&#27861;&#30456;&#27604;AugLoss&#25152;&#21462;&#24471;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) models achieve great successes in many domains. However, DL models increasingly face safety and robustness concerns, including noisy labeling in the training stage and feature distribution shifts in the testing stage. Previous works made significant progress in addressing these problems, but the focus has largely been on developing solutions for only one problem at a time. For example, recent work has argued for the use of tunable robust loss functions to mitigate label noise, and data augmentation (e.g., AugMix) to combat distribution shifts. As a step towards addressing both problems simultaneously, we introduce AugLoss, a simple but effective methodology that achieves robustness against both train-time noisy labeling and test-time feature distribution shifts by unifying data augmentation and robust loss functions. We conduct comprehensive experiments in varied settings of real-world dataset corruption to showcase the gains achieved by AugLoss compared to previous 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;MiniDisc&#30340;&#26368;&#23567;&#33976;&#39311;&#35745;&#21010;&#65292;&#21487;&#20197;&#22312;&#26368;&#23569;&#19968;&#27425;&#23581;&#35797;&#20013;&#35843;&#24230;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2205.14570</link><description>&lt;p&gt;
MiniDisc: &#26368;&#23567;&#33976;&#39311;&#35745;&#21010;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
MiniDisc: Minimal Distillation Schedule for Language Model Compression. (arXiv:2205.14570v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;MiniDisc&#30340;&#26368;&#23567;&#33976;&#39311;&#35745;&#21010;&#65292;&#21487;&#20197;&#22312;&#26368;&#23569;&#19968;&#27425;&#23581;&#35797;&#20013;&#35843;&#24230;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#23481;&#37327;&#24046;&#36317;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#30340;&#25928;&#26524;&#19981;&#20339;&#65292;&#24341;&#20837;&#20102;&#25945;&#24072;&#21161;&#25163;&#36741;&#21161;&#33976;&#39311;&#26469;&#24357;&#34917;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25945;&#24072;&#21161;&#25163;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#23581;&#35797;&#25165;&#33021;&#35843;&#24230;&#20986;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#33976;&#39311;&#35745;&#21010;&#65288;MiniDisc&#65289;&#65292;&#21487;&#20197;&#22312;&#26368;&#23569;&#19968;&#27425;&#23581;&#35797;&#20013;&#35843;&#24230;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#12290;MiniDisc&#26159;&#22522;&#20110;&#25945;&#24072;&#21161;&#25163;&#30340;&#35268;&#27169;-&#24615;&#33021;&#30340;&#26435;&#34913;&#26469;&#24230;&#37327;&#25945;&#24072;&#21161;&#25163;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#21487;&#20197;&#22312;&#19981;&#23545;&#23398;&#29983;&#36827;&#34892;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#35843;&#24230;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have uncovered that language model distillation is less effective when facing a large capacity gap between the teacher and the student, and introduced teacher assistant-based distillation to bridge the gap. As a connection, the scale and the performance of the teacher assistant is of vital importance to bring the knowledge from the teacher to the student. However, existing teacher assistant-based methods require maximally many trials before scheduling an optimal teacher assistant. To this end, we propose a minimal distillation schedule (MiniDisc) for scheduling the optimal teacher assistant in minimally one trial. In particular, motivated by the finding that the performance of the student is positively correlated to the scale-performance tradeoff of the teacher assistant, MiniDisc is designed with a $\lambda$-tradeoff to measure the optimality of the teacher assistant without trial distillation to the student. MiniDisc then can schedule the optimal teacher assistant with
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#22120;&#65292;&#36890;&#36807;&#22312;&#36793;&#32536;&#36827;&#34892;&#33258;&#36866;&#24212;&#32858;&#21512;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#26102;&#38388;&#25928;&#29575;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2204.07767</link><description>&lt;p&gt;
&#36793;&#32536;&#36164;&#28304;&#24863;&#30693;&#30340;&#20302;&#25104;&#26412;&#36793;&#32536;&#32858;&#21512;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards cost-effective and resource-aware aggregation at Edge for Federated Learning. (arXiv:2204.07767v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#22120;&#65292;&#36890;&#36807;&#22312;&#36793;&#32536;&#36827;&#34892;&#33258;&#36866;&#24212;&#32858;&#21512;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#26102;&#38388;&#25928;&#29575;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#28304;&#22788;&#35745;&#31639;&#25968;&#25454;&#26469;&#35299;&#20915;&#38544;&#31169;&#21644;&#25968;&#25454;&#20256;&#36755;&#25104;&#26412;&#38382;&#39064;&#12290;&#23427;&#22312;&#36793;&#32536;&#21644;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#29305;&#21035;&#21463;&#27426;&#36814;&#65292;&#20854;&#20013;&#32852;&#37030;&#23398;&#20064;&#30340;&#32858;&#21512;&#26381;&#21153;&#22120;&#20301;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#25968;&#25454;&#20013;&#24515;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12290;&#29616;&#26377;&#22522;&#20110;&#20113;&#30340;&#32858;&#21512;&#35299;&#20915;&#26041;&#26696;&#22312;&#36793;&#32536;&#19978;&#26082;&#36164;&#28304;&#20302;&#25928;&#21448;&#26114;&#36149;&#65292;&#23548;&#33268;&#21487;&#25193;&#23637;&#24615;&#20302;&#21644;&#24310;&#36831;&#39640;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#29289;&#32852;&#32593;&#21644;&#36793;&#32536;&#24212;&#29992;&#38656;&#27714;&#21464;&#21270;&#19979;&#30340;&#20808;&#21069;&#21644;&#26032;&#30340;&#32858;&#21512;&#26041;&#27861;&#12290;&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#22120;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#31649;&#29702;&#25104;&#26412;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#12290;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#20113;&#30340;&#38745;&#24577;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#35774;&#35745;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#26368;&#22810;4&#20493;&#65292;&#26102;&#38388;&#25928;&#29575;&#25552;&#39640;&#20102;8&#20493;&#65292;&#24182;&#23558;&#25104;&#26412;&#38477;&#20302;&#20102;&#36229;&#36807;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning approach that addresses privacy and data transfer costs by computing data at the source. It's particularly popular for Edge and IoT applications where the aggregator server of FL is in resource-capped edge data centers for reducing communication costs. Existing cloud-based aggregator solutions are resource-inefficient and expensive at the Edge, leading to low scalability and high latency. To address these challenges, this study compares prior and new aggregation methodologies under the changing demands of IoT and Edge applications. This work is the first to propose an adaptive FL aggregator at the Edge, enabling users to manage the cost and efficiency trade-off. An extensive comparative analysis demonstrates that the design improves scalability by up to 4X, time efficiency by 8X, and reduces costs by more than 2X compared to extant cloud-based static methodologies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#20809;&#35889;&#20687;&#32032;&#35299;&#28151;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#35757;&#32451;&#27169;&#22411;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20687;&#32032;&#35299;&#28151;&#65292;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#22909;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.01327</link><description>&lt;p&gt;
&#20351;&#29992;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#20809;&#35889;&#20687;&#32032;&#35299;&#28151;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral Pixel Unmixing with Latent Dirichlet Variational Autoencoder. (arXiv:2203.01327v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#20809;&#35889;&#20687;&#32032;&#35299;&#28151;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#35757;&#32451;&#27169;&#22411;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20687;&#32032;&#35299;&#28151;&#65292;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#22909;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#20809;&#35889;&#20687;&#32032;&#35299;&#28151;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20551;&#35774;(1)&#20016;&#24230;&#21487;&#20197;&#32534;&#30721;&#20026;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#65292;(2)&#25104;&#20998;&#30340;&#20809;&#35889;&#21487;&#20197;&#34920;&#31034;&#20026;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#19979;&#35299;&#20915;&#20102;&#20016;&#24230;&#20272;&#35745;&#21644;&#25104;&#20998;&#25552;&#21462;&#38382;&#39064;&#65292;&#20854;&#20013;&#29380;&#21033;&#20811;&#38647;&#29942;&#39048;&#23618;&#24314;&#27169;&#20016;&#24230;&#65292;&#35299;&#30721;&#22120;&#25191;&#34892;&#25104;&#20998;&#25552;&#21462;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#27169;&#22411;&#20165;&#22312;&#21253;&#21547;&#24863;&#20852;&#36259;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#25104;&#20998;&#30340;&#32447;&#24615;&#32452;&#21512;&#20687;&#32032;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20174;&#32654;&#22269;&#22320;&#36136;&#35843;&#26597;&#23616;&#20809;&#35889;&#24211;&#20013;&#26816;&#32034;&#20986;&#25104;&#20998;(&#20809;&#35889;)&#12290;&#28982;&#21518;&#65292;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#21253;&#21547;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#25152;&#20351;&#29992;&#30340;&#19968;&#37096;&#20998;&#25104;&#20998;&#30340;&#8220;&#23454;&#38469;&#25968;&#25454;&#8221;&#19978;&#36827;&#34892;&#20687;&#32032;&#35299;&#28151;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#35780;&#20215;&#25351;&#26631;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for hyperspectral pixel {\it unmixing}. The proposed method assumes that (1) {\it abundances} can be encoded as Dirichlet distributions and (2) spectra of {\it endmembers} can be represented as multivariate Normal distributions. The method solves the problem of abundance estimation and endmember extraction within a variational autoencoder setting where a Dirichlet bottleneck layer models the abundances, and the decoder performs endmember extraction. The proposed method can also leverage transfer learning paradigm, where the model is only trained on synthetic data containing pixels that are linear combinations of one or more endmembers of interest. In this case, we retrieve endmembers (spectra) from the United States Geological Survey Spectral Library. The model thus trained can be subsequently used to perform pixel unmixing on "real data" that contains a subset of the endmembers used to generated the synthetic data. The model achieves state-of-the-art results on sev
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21367;&#31215;&#23618;&#30340;&#29305;&#24449;&#22270;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#36739;&#22823;&#30340;&#29305;&#24449;&#22270;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#25200;&#21160;&#30340;&#25269;&#25239;&#33021;&#21147;&#65292;&#36825;&#20026;&#35774;&#35745;&#40065;&#26834;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#20462;&#25913;&#26041;&#27861;&#26469;&#25913;&#36827;&#29616;&#26377;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2202.12435</link><description>&lt;p&gt;
&#20174;&#21367;&#31215;&#23618;&#30340;&#29305;&#24449;&#22270;&#29702;&#35299;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding Adversarial Robustness from Feature Maps of Convolutional Layers. (arXiv:2202.12435v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12435
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21367;&#31215;&#23618;&#30340;&#29305;&#24449;&#22270;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#36739;&#22823;&#30340;&#29305;&#24449;&#22270;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#25200;&#21160;&#30340;&#25269;&#25239;&#33021;&#21147;&#65292;&#36825;&#20026;&#35774;&#35745;&#40065;&#26834;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#20462;&#25913;&#26041;&#27861;&#26469;&#25913;&#36827;&#29616;&#26377;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#20027;&#35201;&#21462;&#20915;&#20110;&#20004;&#20010;&#22240;&#32032;&#65306;&#27169;&#22411;&#23481;&#37327;&#21644;&#25239;&#25200;&#21160;&#33021;&#21147;&#12290;&#26412;&#25991;&#20174;&#21367;&#31215;&#23618;&#30340;&#29305;&#24449;&#22270;&#30740;&#31350;&#20102;&#32593;&#32476;&#30340;&#25239;&#25200;&#21160;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#24179;&#22343;&#27744;&#21270;&#20043;&#21069;&#36739;&#22823;&#30340;&#21367;&#31215;&#29305;&#24449;&#22270;&#21487;&#20197;&#25552;&#39640;&#23545;&#25200;&#21160;&#30340;&#25269;&#25239;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#26368;&#22823;&#27744;&#21270;&#24182;&#38750;&#22914;&#27492;&#12290;&#36825;&#20026;&#40065;&#26834;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#35745;&#24102;&#26469;&#20102;&#26032;&#30340;&#21551;&#31034;&#65292;&#24182;&#20419;&#20351;&#25105;&#20204;&#23558;&#36825;&#20123;&#21457;&#29616;&#24212;&#29992;&#20110;&#25913;&#36827;&#29616;&#26377;&#26550;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;&#20462;&#25913;&#38750;&#24120;&#31616;&#21333;&#65292;&#21482;&#38656;&#35201;&#23545;&#36755;&#20837;&#36827;&#34892;&#19978;&#37319;&#26679;&#25110;&#36731;&#24494;&#20462;&#25913;&#19979;&#37319;&#26679;&#36816;&#31639;&#31526;&#30340;&#27493;&#24133;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;AlexNet&#12289;VGG&#12289;RestNet18&#21644;PreActResNet18&#12290;&#22312;&#21508;&#31181;&#25915;&#20987;&#21644;&#38450;&#24481;&#20013;&#65292;&#37117;&#33021;&#23454;&#29616;&#33258;&#28982;&#20934;&#30830;&#24230;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#38750;&#24179;&#20961;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adversarial robustness of a neural network mainly relies on two factors: model capacity and anti-perturbation ability. In this paper, we study the anti-perturbation ability of the network from the feature maps of convolutional layers. Our theoretical analysis discovers that larger convolutional feature maps before average pooling can contribute to better resistance to perturbations, but the conclusion is not true for max pooling. It brings new inspiration to the design of robust neural networks and urges us to apply these findings to improve existing architectures. The proposed modifications are very simple and only require upsampling the inputs or slightly modifying the stride configurations of downsampling operators. We verify our approaches on several benchmark neural network architectures, including AlexNet, VGG, RestNet18, and PreActResNet18. Non-trivial improvements in terms of both natural accuracy and adversarial robustness can be achieved under various attack and defense m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#21943;&#27880;&#26631;&#35760;&#30340;&#31890;&#23376;&#21464;&#21387;&#22120;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;JetClass&#36827;&#34892;&#35757;&#32451;&#12290;&#31890;&#23376;&#21464;&#21387;&#22120;&#36890;&#36807;&#22312;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#24341;&#20837;&#31890;&#23376;&#38388;&#30340;&#20132;&#20114;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#21464;&#21387;&#22120;&#26356;&#39640;&#30340;&#26631;&#35760;&#24615;&#33021;&#65292;&#24182;&#22312;&#20004;&#31181;&#24120;&#29992;&#30340;&#21943;&#27880;&#26631;&#35760;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2202.03772</link><description>&lt;p&gt;
&#31890;&#23376;&#21464;&#21387;&#22120;&#29992;&#20110;&#21943;&#27880;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Particle Transformer for Jet Tagging. (arXiv:2202.03772v3 [hep-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#21943;&#27880;&#26631;&#35760;&#30340;&#31890;&#23376;&#21464;&#21387;&#22120;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;JetClass&#36827;&#34892;&#35757;&#32451;&#12290;&#31890;&#23376;&#21464;&#21387;&#22120;&#36890;&#36807;&#22312;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#24341;&#20837;&#31890;&#23376;&#38388;&#30340;&#20132;&#20114;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#21464;&#21387;&#22120;&#26356;&#39640;&#30340;&#26631;&#35760;&#24615;&#33021;&#65292;&#24182;&#22312;&#20004;&#31181;&#24120;&#29992;&#30340;&#21943;&#27880;&#26631;&#35760;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21943;&#27880;&#26631;&#35760;&#26159;&#31890;&#23376;&#29289;&#29702;&#23398;&#20013;&#19968;&#20010;&#20851;&#38190;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25913;&#21464;&#20102;&#21943;&#27880;&#26631;&#35760;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#25552;&#21319;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JetClass&#65292;&#19968;&#20010;&#29992;&#20110;&#21943;&#27880;&#26631;&#35760;&#30340;&#26032;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#12290;JetClass&#25968;&#25454;&#38598;&#21253;&#21547;1&#20159;&#20010;&#21943;&#27880;&#65292;&#27604;&#29616;&#26377;&#20844;&#20849;&#25968;&#25454;&#38598;&#22823;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#24635;&#20849;&#27169;&#25311;&#20102;10&#31181;&#31867;&#22411;&#30340;&#21943;&#27880;&#65292;&#21253;&#25324;&#33267;&#20170;&#20026;&#27490;&#23578;&#26410;&#29992;&#20110;&#26631;&#35760;&#30340;&#20960;&#31181;&#31867;&#22411;&#12290;&#22522;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#29992;&#20110;&#21943;&#27880;&#26631;&#35760;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#31890;&#23376;&#21464;&#21387;&#22120;&#65288;ParT&#65289;&#12290;&#36890;&#36807;&#22312;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#24341;&#20837;&#37197;&#23545;&#31890;&#23376;&#20132;&#20114;&#65292;ParT&#23454;&#29616;&#20102;&#27604;&#31616;&#21333;&#21464;&#21387;&#22120;&#26356;&#39640;&#30340;&#26631;&#35760;&#24615;&#33021;&#65292;&#24182;&#22823;&#24133;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;ParticleNet&#12290;&#39044;&#35757;&#32451;&#30340;ParT&#27169;&#22411;&#22312;&#24494;&#35843;&#21518;&#36824;&#21487;&#20197;&#22823;&#24133;&#25552;&#21319;&#24191;&#27867;&#24212;&#29992;&#30340;&#20004;&#31181;&#21943;&#27880;&#26631;&#35760;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jet tagging is a critical yet challenging classification task in particle physics. While deep learning has transformed jet tagging and significantly improved performance, the lack of a large-scale public dataset impedes further enhancement. In this work, we present JetClass, a new comprehensive dataset for jet tagging. The JetClass dataset consists of 100 M jets, about two orders of magnitude larger than existing public datasets. A total of 10 types of jets are simulated, including several types unexplored for tagging so far. Based on the large dataset, we propose a new Transformer-based architecture for jet tagging, called Particle Transformer (ParT). By incorporating pairwise particle interactions in the attention mechanism, ParT achieves higher tagging performance than a plain Transformer and surpasses the previous state-of-the-art, ParticleNet, by a large margin. The pre-trained ParT models, once fine-tuned, also substantially enhance the performance on two widely adopted jet taggi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#21327;&#26041;&#24046;&#39033;&#21644;&#28155;&#21152;&#36845;&#20195;&#24402;&#19968;&#21270;&#23618;&#65292;&#21152;&#36895;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2109.00783</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26102;&#24207;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Computer Vision Self-supervised Learning Methods on Time Series. (arXiv:2109.00783v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.00783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#21327;&#26041;&#24046;&#39033;&#21644;&#28155;&#21152;&#36845;&#20195;&#24402;&#19968;&#21270;&#23618;&#65292;&#21152;&#36895;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#30446;&#21069;&#20027;&#27969;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#22823;&#22810;&#22522;&#20110;Siamese&#32593;&#32476;&#26550;&#26500;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#35774;&#32622;&#65292;&#20197;&#36991;&#20813;&#29305;&#24449;&#23849;&#28291;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36825;&#20123;&#35745;&#31639;&#26426;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#22312;&#19981;&#21516;&#27169;&#24577;&#65288;&#21363;&#26102;&#38388;&#24207;&#21015;&#65289;&#19978;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;UCR&#21644;UEA&#26723;&#26696;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#21516;&#26679;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26368;&#36817;&#25552;&#20986;&#30340;VICReg&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;VICReg&#20013;&#25552;&#20986;&#30340;&#19968;&#20010;&#8220;&#21327;&#26041;&#24046;&#8221;&#39033;&#65292;&#21516;&#26102;&#22312;&#26550;&#26500;&#30340;&#22836;&#37096;&#22686;&#21152;&#20102;&#19968;&#20010;&#36845;&#20195;&#24402;&#19968;&#21270;&#23618;&#65292;&#21152;&#36895;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has had great success in both computer vision. Most of the current mainstream computer vision SSL frameworks are based on Siamese network architecture. These approaches often rely on cleverly crafted loss functions and training setups to avoid feature collapse. In this study, we evaluate if those computer-vision SSL frameworks are also effective on a different modality (\textit{i.e.,} time series). The effectiveness is experimented and evaluated on the UCR and UEA archives, and we show that the computer vision SSL frameworks can be effective even for time series. In addition, we propose a new method that improves on the recently proposed VICReg method. Our method improves on a \textit{covariance} term proposed in VICReg, and in addition we augment the head of the architecture by an iterative normalization layer that accelerates the convergence of the model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31639;&#27861;&#20272;&#35745;&#21147;&#31995;&#25968;&#26469;&#29983;&#25104;&#27169;&#25311;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#29992;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26426;&#22120;&#20154;&#25243;&#23556;&#24314;&#27169;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#20351;&#29992;&#29289;&#29702;&#27169;&#22411;&#36827;&#34892;&#24314;&#27169;&#26102;&#23384;&#22312;&#30340;&#26410;&#30693;&#22240;&#32032;&#21644;&#29289;&#20307;&#21464;&#24418;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2105.12833</link><description>&lt;p&gt;
&#36890;&#36807;&#31639;&#27861;&#21147;&#31995;&#25968;&#20272;&#35745;&#29983;&#25104;&#27169;&#25311;&#25968;&#25454;&#65292;&#29992;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26426;&#22120;&#20154;&#25243;&#23556;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Simulated Data Generation Through Algorithmic Force Coefficient Estimation for AI-Based Robotic Projectile Launch Modeling. (arXiv:2105.12833v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.12833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31639;&#27861;&#20272;&#35745;&#21147;&#31995;&#25968;&#26469;&#29983;&#25104;&#27169;&#25311;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#29992;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26426;&#22120;&#20154;&#25243;&#23556;&#24314;&#27169;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#20351;&#29992;&#29289;&#29702;&#27169;&#22411;&#36827;&#34892;&#24314;&#27169;&#26102;&#23384;&#22312;&#30340;&#26410;&#30693;&#22240;&#32032;&#21644;&#29289;&#20307;&#21464;&#24418;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21018;&#24615;&#29289;&#20307;&#25243;&#23556;&#21644;&#25805;&#32437;&#24314;&#27169;&#24456;&#22797;&#26434;&#65292;&#32771;&#34385;&#21040;&#24433;&#21709;&#36712;&#36857;&#30340;&#21508;&#31181;&#21160;&#21147;&#23398;&#22240;&#32032;&#65292;&#20854;&#20013;&#35768;&#22810;&#21487;&#33021;&#26159;&#26410;&#30693;&#30340;&#12290;&#20351;&#29992;&#29289;&#29702;&#27169;&#22411;&#21487;&#33021;&#19981;&#20934;&#30830;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#32771;&#34385;&#26410;&#30693;&#22240;&#32032;&#21644;&#29289;&#20307;&#25243;&#23556;&#26102;&#30340;&#21464;&#24418;&#25928;&#24212;&#12290;&#27492;&#22806;&#65292;&#35201;&#24819;&#24471;&#21040;&#36825;&#20123;&#27169;&#22411;&#30340;&#21147;&#31995;&#25968;&#65292;&#23601;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#23454;&#39564;&#27979;&#35797;&#12290;&#26368;&#36817;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#23398;&#20064;&#30340;&#27169;&#22411;&#21644;&#31995;&#32479;&#24471;&#20197;&#20986;&#29616;&#12290;&#22312;&#26426;&#22120;&#20154;&#19978;&#35757;&#32451;&#19968;&#20010;&#39044;&#27979;&#25243;&#23556;&#27169;&#22411;&#26159;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#32771;&#34385;&#21040;&#26080;&#27861;&#27979;&#37327;&#30340;&#21160;&#21147;&#23398;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#26080;&#27861;&#25910;&#38598;&#22823;&#37327;&#23454;&#39564;&#25968;&#25454;&#20250;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20272;&#35745;&#21147;&#31995;&#25968;&#65292;&#21487;&#20197;&#21033;&#29992;&#24050;&#25509;&#21463;&#30340;&#29289;&#29702;&#27169;&#22411;&#29983;&#25104;&#20805;&#36275;&#30340;&#34917;&#20805;&#25968;&#25454;&#65292;&#20154;&#20026;&#22320;&#22686;&#21152;&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#65292;&#20174;&#32780;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling of non-rigid object launching and manipulation is complex considering the wide range of dynamics affecting trajectory, many of which may be unknown. Using physics models can be inaccurate because they cannot account for unknown factors and the effects of the deformation of the object as it is launched; moreover, deriving force coefficients for these models is not possible without extensive experimental testing. Recently, advancements in data-powered artificial intelligence methods have allowed learnable models and systems to emerge. It is desirable to train a model for launch prediction on a robot, as deep neural networks can account for immeasurable dynamics. However, the inability to collect large amounts of experimental data decreases performance of deep neural networks. Through estimating force coefficients, the accepted physics models can be leveraged to produce adequate supplemental data to artificially increase the size of the training set, yielding improved neural netw
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#21464;&#25442;&#30340;&#23545;&#25239;&#31034;&#20363;&#38450;&#24481;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#23545;&#25239;&#31034;&#20363;&#29983;&#25104;&#19968;&#38454;&#25200;&#21160;&#26469;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#27491;&#30830;&#32467;&#26524;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2103.04565</link><description>&lt;p&gt;
&#25552;&#39640;&#22522;&#20110;&#21464;&#25442;&#30340;&#23545;&#25239;&#31034;&#20363;&#38450;&#24481;&#26041;&#27861;&#30340;&#19968;&#38454;&#25200;&#21160;&#25269;&#25239;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Transformation-based Defenses against Adversarial Examples with First-order Perturbations. (arXiv:2103.04565v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.04565
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#21464;&#25442;&#30340;&#23545;&#25239;&#31034;&#20363;&#38450;&#24481;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#23545;&#25239;&#31034;&#20363;&#29983;&#25104;&#19968;&#38454;&#25200;&#21160;&#26469;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#27491;&#30830;&#32467;&#26524;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#36825;&#23545;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26234;&#33021;&#31995;&#32479;&#26500;&#25104;&#20102;&#28508;&#22312;&#23041;&#32961;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36890;&#36807;&#23545;&#23545;&#25239;&#31034;&#20363;&#29983;&#25104;&#38024;&#23545;&#38750;&#39044;&#27979;&#26631;&#31614;&#30340;&#23567;&#19968;&#38454;&#25200;&#21160;&#21487;&#20197;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#32467;&#26524;&#20026;&#27491;&#30830;&#32467;&#26524;&#30340;&#27010;&#29575;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25269;&#25239;&#23545;&#25239;&#25200;&#21160;&#20197;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#22312;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#33509;&#24178;&#31867;&#21035;&#26631;&#31614;&#65292;&#24182;&#20026;&#36825;&#20123;&#36873;&#23450;&#30340;&#26631;&#31614;&#29983;&#25104;&#23567;&#30340;&#19968;&#38454;&#25200;&#21160;&#12290;&#29983;&#25104;&#30340;&#25200;&#21160;&#21152;&#22312;&#19968;&#36215;&#21518;&#34987;&#38480;&#21046;&#22312;&#25351;&#23450;&#30340;&#31354;&#38388;&#20869;&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#25200;&#21160;&#34987;&#28155;&#21152;&#21040;&#23545;&#25239;&#31034;&#20363;&#20013;&#65292;&#20197;&#25269;&#28040;&#20854;&#20013;&#30340;&#23545;&#25239;&#25200;&#21160;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25512;&#26029;&#26102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have been successfully applied in various machine learning tasks. However, studies show that neural networks are susceptible to adversarial attacks. This exposes a potential threat to neural network-based intelligent systems. We observe that the probability of the correct result outputted by the neural network increases by applying small first-order perturbations generated for non-predicted class labels to adversarial examples. Based on this observation, we propose a method for counteracting adversarial perturbations to improve adversarial robustness. In the proposed method, we randomly select a number of class labels and generate small first-order perturbations for these selected labels. The generated perturbations are added together and then clamped onto a specified space. The obtained perturbation is finally added to the adversarial example to counteract the adversarial perturbation contained in the example. The proposed method is applied at inference time and d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#23545;&#20219;&#24847;&#21442;&#25968;&#32500;&#24230;&#19979;&#30340;&#20219;&#24847;&#22495;&#20869;&#27491;&#24577;&#20998;&#24067;&#36827;&#34892;&#31215;&#20998;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#27861;&#21521;&#21521;&#37327;&#20989;&#25968;&#30340;&#30456;&#20851;&#27010;&#29575;&#23494;&#24230;&#21644;&#32479;&#35745;&#25351;&#26631;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#21487;&#20197;&#23545;&#20219;&#24847;&#25968;&#37327;&#27491;&#24577;&#20998;&#24067;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#21644;&#32500;&#24230;&#38477;&#20302;&#21644;&#21487;&#35270;&#21270;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2012.14331</link><description>&lt;p&gt;
&#19968;&#31181;&#25972;&#21512;&#21644;&#20998;&#31867;&#27491;&#24577;&#20998;&#24067;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A method to integrate and classify normal distributions. (arXiv:2012.14331v8 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.14331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#23545;&#20219;&#24847;&#21442;&#25968;&#32500;&#24230;&#19979;&#30340;&#20219;&#24847;&#22495;&#20869;&#27491;&#24577;&#20998;&#24067;&#36827;&#34892;&#31215;&#20998;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#27861;&#21521;&#21521;&#37327;&#20989;&#25968;&#30340;&#30456;&#20851;&#27010;&#29575;&#23494;&#24230;&#21644;&#32479;&#35745;&#25351;&#26631;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#21487;&#20197;&#23545;&#20219;&#24847;&#25968;&#37327;&#27491;&#24577;&#20998;&#24067;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#21644;&#32500;&#24230;&#38477;&#20302;&#21644;&#21487;&#35270;&#21270;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#27491;&#24577;&#27010;&#29575;&#20998;&#24067;&#22312;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#20915;&#31574;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#35745;&#31639;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#38656;&#35201;&#22312;&#29305;&#23450;&#21306;&#22495;&#20869;&#23545;&#36825;&#20123;&#20998;&#24067;&#36827;&#34892;&#31215;&#20998;&#65292;&#36825;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#20013;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#12290;&#38500;&#20102;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#36890;&#29992;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#12289;&#26631;&#20934;&#25968;&#20540;&#26041;&#27861;&#25110;&#36719;&#20214;&#26469;&#35745;&#31639;&#36825;&#20123;&#31215;&#20998;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#25968;&#23398;&#32467;&#26524;&#21644;&#24320;&#28304;&#36719;&#20214;&#65292;&#21487;&#20197;&#25552;&#20379;&#20197;&#19979;&#20869;&#23481;&#65306;&#65288;i&#65289;&#20219;&#24847;&#21442;&#25968;&#32500;&#24230;&#19979;&#20219;&#24847;&#22495;&#20869;&#27861;&#21521;&#30340;&#27010;&#29575;&#65292;&#65288;ii&#65289;&#27861;&#21521;&#21521;&#37327;&#20989;&#25968;&#30340;&#27010;&#29575;&#23494;&#24230;&#12289;&#32047;&#31215;&#20998;&#24067;&#21644;&#36870;&#32047;&#31215;&#20998;&#24067;&#65292;&#65288;iii&#65289;&#20219;&#24847;&#25968;&#37327;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;&#20998;&#31867;&#35823;&#24046;&#12289;&#36125;&#21494;&#26031;&#26368;&#20248;&#36776;&#21035;&#25351;&#25968;&#20197;&#21450;&#20854;&#19982;&#24037;&#20316;&#29305;&#24449;&#26354;&#32447;&#30340;&#20851;&#31995;&#65292;&#65288;iv&#65289;&#27492;&#31867;&#38382;&#39064;&#30340;&#32500;&#24230;&#38477;&#20302;&#21644;&#21487;&#35270;&#21270;&#65292;&#20197;&#21450;&#65288;v&#65289;&#23545;&#20110;&#32473;&#23450;&#25968;&#25454;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#20855;&#20307;&#30340;&#20363;&#23376;&#65292;&#21253;&#25324;&#37329;&#34701;&#12289;&#29983;&#29289;&#21644;&#24515;&#29702;&#23398;&#26469;&#28436;&#31034;&#36825;&#20123;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Univariate and multivariate normal probability distributions are widely used when modeling decisions under uncertainty. Computing the performance of such models requires integrating these distributions over specific domains, which can vary widely across models. Besides some special cases, there exist no general analytical expressions, standard numerical methods or software for these integrals. Here we present mathematical results and open-source software that provide (i) the probability in any domain of a normal in any dimensions with any parameters, (ii) the probability density, cumulative distribution, and inverse cumulative distribution of any function of a normal vector, (iii) the classification errors among any number of normal distributions, the Bayes-optimal discriminability index and relation to the operating characteristic, (iv) dimension reduction and visualizations for such problems, and (v) tests for how reliably these methods may be used on given data. We demonstrate these
&lt;/p&gt;</description></item><item><title>&#36873;&#25321;&#21512;&#36866;&#30340;&#20803;&#23398;&#20064;&#22120;&#23545;&#20110;&#22810;&#35270;&#35282;&#22534;&#21472;&#20013;&#30340;&#35270;&#22270;&#36873;&#25321;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#36890;&#36807;&#23545;&#19971;&#31181;&#19981;&#21516;&#30340;&#31639;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#38750;&#36127;&#22871;&#32034;&#12289;&#38750;&#36127;&#33258;&#36866;&#24212;&#22871;&#32034;&#21644;&#38750;&#36127;&#24377;&#24615;&#32593;&#32476;&#34987;&#35748;&#20026;&#26159;&#26368;&#21512;&#36866;&#30340;&#20803;&#23398;&#20064;&#22120;&#12290;</title><link>http://arxiv.org/abs/2010.16271</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#22534;&#21472;&#20013;&#30340;&#35270;&#22270;&#36873;&#25321;&#65306;&#36873;&#25321;&#20803;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
View selection in multi-view stacking: Choosing the meta-learner. (arXiv:2010.16271v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.16271
&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#21512;&#36866;&#30340;&#20803;&#23398;&#20064;&#22120;&#23545;&#20110;&#22810;&#35270;&#35282;&#22534;&#21472;&#20013;&#30340;&#35270;&#22270;&#36873;&#25321;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#36890;&#36807;&#23545;&#19971;&#31181;&#19981;&#21516;&#30340;&#31639;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#38750;&#36127;&#22871;&#32034;&#12289;&#38750;&#36127;&#33258;&#36866;&#24212;&#22871;&#32034;&#21644;&#38750;&#36127;&#24377;&#24615;&#32593;&#32476;&#34987;&#35748;&#20026;&#26159;&#26368;&#21512;&#36866;&#30340;&#20803;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#22534;&#21472;&#26159;&#19968;&#31181;&#23558;&#26469;&#33258;&#19981;&#21516;&#35270;&#22270;&#65288;&#21363;&#19981;&#21516;&#30340;&#29305;&#24449;&#38598;&#65289;&#25551;&#36848;&#30456;&#21516;&#23545;&#35937;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#22522;&#23398;&#20064;&#31639;&#27861;&#20998;&#21035;&#22312;&#27599;&#20010;&#35270;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#32467;&#26524;&#30001;&#20803;&#23398;&#20064;&#31639;&#27861;&#32452;&#21512;&#12290;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#22534;&#21472;&#30340;&#32602;&#20998;&#36923;&#36753;&#22238;&#24402;&#65292;&#20316;&#20026;&#22810;&#35270;&#35282;&#22534;&#21472;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#35782;&#21035;&#23545;&#39044;&#27979;&#26368;&#37325;&#35201;&#30340;&#35270;&#22270;&#26041;&#38754;&#26159;&#26377;&#29992;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19971;&#31181;&#19981;&#21516;&#30340;&#31639;&#27861;&#20316;&#20026;&#20803;&#23398;&#20064;&#22120;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#20004;&#20010;&#30495;&#23454;&#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#23427;&#20204;&#30340;&#35270;&#22270;&#36873;&#25321;&#21644;&#20998;&#31867;&#24615;&#33021;&#65292;&#25193;&#23637;&#20102;&#36825;&#39033;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#35270;&#22270;&#36873;&#25321;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#23545;&#30740;&#31350;&#37117;&#24456;&#37325;&#35201;&#65292;&#37027;&#20040;&#38750;&#36127;&#22871;&#32034;&#12289;&#38750;&#36127;&#33258;&#36866;&#24212;&#22871;&#32034;&#21644;&#38750;&#36127;&#24377;&#24615;&#32593;&#32476;&#37117;&#26159;&#21512;&#36866;&#30340;&#20803;&#23398;&#20064;&#22120;&#12290;&#20855;&#20307;&#22312;&#36825;&#19977;&#31181;&#26041;&#27861;&#20013;&#35813;&#36873;&#25321;&#21738;&#19968;&#31181;&#21462;&#20915;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Multi-view stacking is a framework for combining information from different views (i.e. different feature sets) describing the same set of objects. In this framework, a base-learner algorithm is trained on each view separately, and their predictions are then combined by a meta-learner algorithm. In a previous study, stacked penalized logistic regression, a special case of multi-view stacking, has been shown to be useful in identifying which views are most important for prediction. In this article we expand this research by considering seven different algorithms to use as the meta-learner, and evaluating their view selection and classification performance in simulations and two applications on real gene-expression data sets. Our results suggest that if both view selection and classification accuracy are important to the research at hand, then the nonnegative lasso, nonnegative adaptive lasso and nonnegative elastic net are suitable meta-learners. Exactly which among these three is to be
&lt;/p&gt;</description></item><item><title>&#12298;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#30452;&#35266;&#25945;&#31243;&#12299;&#26159;&#19968;&#31687;&#20171;&#32461;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#25945;&#31243;&#65292;&#26088;&#22312;&#30452;&#35266;&#22320;&#35299;&#37322;GPR&#30340;&#22522;&#26412;&#27010;&#24565;&#12289;&#25552;&#20379;&#23454;&#29616;&#20195;&#30721;&#65292;&#24182;&#22238;&#39038;&#26368;&#20808;&#36827;&#30340;&#39640;&#26031;&#36807;&#31243;&#31639;&#27861;&#12290;&#36866;&#21512;&#26426;&#22120;&#23398;&#20064;&#21021;&#23398;&#32773;&#38405;&#35835;&#65292;&#24110;&#21161;&#20182;&#20204;&#28165;&#26224;&#29702;&#35299;GPR&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;</title><link>http://arxiv.org/abs/2009.10862</link><description>&lt;p&gt;
&#12298;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#30452;&#35266;&#25945;&#31243;&#12299;
&lt;/p&gt;
&lt;p&gt;
An Intuitive Tutorial to Gaussian Process Regression. (arXiv:2009.10862v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.10862
&lt;/p&gt;
&lt;p&gt;
&#12298;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#30452;&#35266;&#25945;&#31243;&#12299;&#26159;&#19968;&#31687;&#20171;&#32461;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#25945;&#31243;&#65292;&#26088;&#22312;&#30452;&#35266;&#22320;&#35299;&#37322;GPR&#30340;&#22522;&#26412;&#27010;&#24565;&#12289;&#25552;&#20379;&#23454;&#29616;&#20195;&#30721;&#65292;&#24182;&#22238;&#39038;&#26368;&#20808;&#36827;&#30340;&#39640;&#26031;&#36807;&#31243;&#31639;&#27861;&#12290;&#36866;&#21512;&#26426;&#22120;&#23398;&#20064;&#21021;&#23398;&#32773;&#38405;&#35835;&#65292;&#24110;&#21161;&#20182;&#20204;&#28165;&#26224;&#29702;&#35299;GPR&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#26088;&#22312;&#30452;&#35266;&#20171;&#32461;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#12290;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;GPR&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#25945;&#31243;&#20174;&#35299;&#37322;&#39640;&#26031;&#36807;&#31243;&#26500;&#24314;&#30340;&#22522;&#26412;&#27010;&#24565;&#24320;&#22987;&#65292;&#21253;&#25324;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#12289;&#26680;&#20989;&#25968;&#12289;&#38750;&#21442;&#25968;&#27169;&#22411;&#20197;&#21450;&#32852;&#21512;&#27010;&#29575;&#21644;&#26465;&#20214;&#27010;&#29575;&#12290;&#28982;&#21518;&#65292;&#23427;&#25552;&#20379;&#20102;&#23545;GPR&#30340;&#31616;&#26126;&#25551;&#36848;&#21644;&#26631;&#20934;GPR&#31639;&#27861;&#30340;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#25945;&#31243;&#36824;&#22238;&#39038;&#20102;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#39640;&#26031;&#36807;&#31243;&#31639;&#27861;&#30340;&#36719;&#20214;&#21253;&#12290;&#26412;&#25945;&#31243;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#21463;&#20247;&#65292;&#21253;&#25324;&#23545;&#26426;&#22120;&#23398;&#20064;&#19981;&#29087;&#24713;&#30340;&#20154;&#65292;&#20197;&#30830;&#20445;&#23545;GPR&#30340;&#22522;&#26412;&#21407;&#29702;&#26377;&#28165;&#26224;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This tutorial aims to provide an intuitive introduction to Gaussian process regression (GPR). GPR models have been widely used in machine learning applications due to their representation flexibility and inherent capability to quantify uncertainty over predictions. The tutorial starts with explaining the basic concepts that a Gaussian process is built on, including multivariate normal distribution, kernels, non-parametric models, and joint and conditional probability. It then provides a concise description of GPR and an implementation of a standard GPR algorithm. In addition, the tutorial reviews packages for implementing state-of-the-art Gaussian process algorithms. This tutorial is accessible to a broad audience, including those new to machine learning, ensuring a clear understanding of GPR fundamentals.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20803;&#26799;&#24230;&#26041;&#24335;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#26102;&#25915;&#20987;&#65292;&#36890;&#36807;&#24494;&#23567;&#30340;&#22270;&#25200;&#21160;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#26080;&#30417;&#30563;&#23884;&#20837;&#20013;&#20063;&#33021;&#20135;&#29983;&#36801;&#31227;&#25928;&#24212;&#12290;&#36825;&#20123;&#25915;&#20987;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#30446;&#26631;&#20998;&#31867;&#22120;&#30340;&#30693;&#35782;&#25110;&#35775;&#38382;&#26435;&#38480;&#12290;</title><link>http://arxiv.org/abs/1902.08412</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks on Graph Neural Networks via Meta Learning. (arXiv:1902.08412v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1902.08412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20803;&#26799;&#24230;&#26041;&#24335;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#26102;&#25915;&#20987;&#65292;&#36890;&#36807;&#24494;&#23567;&#30340;&#22270;&#25200;&#21160;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#26080;&#30417;&#30563;&#23884;&#20837;&#20013;&#20063;&#33021;&#20135;&#29983;&#36801;&#31227;&#25928;&#24212;&#12290;&#36825;&#20123;&#25915;&#20987;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#30446;&#26631;&#20998;&#31867;&#22120;&#30340;&#30693;&#35782;&#25110;&#35775;&#38382;&#26435;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#20219;&#21153;&#20013;&#24182;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#23545;&#33410;&#28857;&#20998;&#31867;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#35757;&#32451;&#26102;&#25915;&#20987;&#65292;&#22312;&#31163;&#25955;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#21407;&#21017;&#26159;&#20351;&#29992;&#20803;&#26799;&#24230;&#26469;&#35299;&#20915;&#35757;&#32451;&#26102;&#25915;&#20987;&#32972;&#21518;&#30340;&#21452;&#23618;&#38382;&#39064;&#65292;&#26412;&#36136;&#19978;&#23558;&#22270;&#35270;&#20026;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24494;&#23567;&#30340;&#22270;&#25200;&#21160;&#36890;&#24120;&#20250;&#23548;&#33268;&#22270;&#21367;&#31215;&#32593;&#32476;&#24615;&#33021;&#30340;&#22823;&#24133;&#19979;&#38477;&#65292;&#29978;&#33267;&#20256;&#36882;&#32473;&#26080;&#30417;&#30563;&#23884;&#20837;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#25152;&#21019;&#24314;&#30340;&#25200;&#21160;&#21487;&#20197;&#35823;&#23548;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#24615;&#33021;&#27604;&#24573;&#30053;&#25152;&#26377;&#20851;&#32852;&#20449;&#24687;&#30340;&#31616;&#21333;&#22522;&#32447;&#27169;&#22411;&#26356;&#24046;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#30446;&#26631;&#20998;&#31867;&#22120;&#30340;&#30693;&#35782;&#25110;&#35775;&#38382;&#26435;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.
&lt;/p&gt;</description></item></channel></rss>