<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#21363;&#20351;&#32570;&#20047;&#35270;&#35273;&#36755;&#20837;&#65292;&#21033;&#29992;&#25152;&#26377;&#32452;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24674;&#22797;&#22823;&#37096;&#20998;VLM&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#35821;&#35328;&#36890;&#36807;&#25552;&#20379;&#23545;&#20808;&#21069;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#35775;&#38382;&#26469;&#23545;&#23398;&#20064;&#26032;&#20219;&#21153;&#26377;&#36129;&#29486;</title><link>https://arxiv.org/abs/2403.19669</link><description>&lt;p&gt;
&#20998;&#26512;&#35821;&#35328;&#21644;&#35270;&#35273;&#22312;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Roles of Language and Vision in Learning from Limited Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19669
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#21363;&#20351;&#32570;&#20047;&#35270;&#35273;&#36755;&#20837;&#65292;&#21033;&#29992;&#25152;&#26377;&#32452;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24674;&#22797;&#22823;&#37096;&#20998;VLM&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#35821;&#35328;&#36890;&#36807;&#25552;&#20379;&#23545;&#20808;&#21069;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#35775;&#38382;&#26469;&#23545;&#23398;&#20064;&#26032;&#20219;&#21153;&#26377;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19669v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#35821;&#35328;&#26159;&#21542;&#26377;&#21161;&#20110;&#29702;&#35299;&#35270;&#35273;&#19990;&#30028;&#65311;&#23454;&#38469;&#35266;&#23519;&#19990;&#30028;&#38656;&#35201;&#30475;&#21040;&#23454;&#38469;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#29992;&#25991;&#23383;&#25551;&#36848;&#21527;&#65311;&#20851;&#20110;&#26234;&#33021;&#26412;&#36136;&#30340;&#36825;&#20123;&#22522;&#26412;&#38382;&#39064;&#24456;&#38590;&#22238;&#31572;&#65292;&#22240;&#20026;&#25105;&#20204;&#21482;&#26377;&#19968;&#20010;&#26234;&#33021;&#31995;&#32479;&#30340;&#20363;&#23376;&#8212;&#8212;&#20154;&#31867;&#8212;&#8212;&#20197;&#21450;&#26377;&#38480;&#30340;&#29420;&#31435;&#35821;&#35328;&#25110;&#35270;&#35273;&#30340;&#26696;&#20363;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#22797;&#26434;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#25506;&#32034;&#35821;&#35328;&#21644;&#35270;&#35273;&#23545;&#20110;&#23398;&#20064;&#19990;&#30028;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#20174;&#36825;&#20123;&#27169;&#22411;&#30340;&#35748;&#30693;&#26550;&#26500;&#20013;&#20999;&#38500;&#32452;&#20214;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21033;&#29992;&#25152;&#26377;&#32452;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#24674;&#22797;&#20102;&#22823;&#37096;&#20998;VLM&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#23427;&#32570;&#20047;&#35270;&#35273;&#36755;&#20837;&#65292;&#32780;&#35821;&#35328;&#20284;&#20046;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#23545;&#20808;&#21069;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#35775;&#38382;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19669v1 Announce Type: cross  Abstract: Does language help make sense of the visual world? How important is it to actually see the world rather than having it described with words? These basic questions about the nature of intelligence have been difficult to answer because we only had one example of an intelligent system -- humans -- and limited access to cases that isolated language or vision. However, the development of sophisticated Vision-Language Models (VLMs) by artificial intelligence researchers offers us new opportunities to explore the contributions that language and vision make to learning about the world. We ablate components from the cognitive architecture of these models to identify their contributions to learning new tasks from limited data. We find that a language model leveraging all components recovers a majority of a VLM's performance, despite its lack of visual input, and that language seems to allow this by providing access to prior knowledge and reasoni
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#32423;Actor-Critic&#26694;&#26550;&#21644;&#22810;&#32423;&#33945;&#29305;&#21345;&#32599;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#26412;&#30740;&#31350;&#25104;&#21151;&#35299;&#20915;&#20102;&#24179;&#22343;&#22870;&#21169;MDPs&#20840;&#23616;&#25910;&#25947;&#20013;&#23545;&#28151;&#21512;&#26102;&#38388;&#39044;&#27979;&#30340;&#20381;&#36182;&#24615;&#65292;&#23637;&#29616;&#20986;&#26368;&#20005;&#26684;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.11925</link><description>&lt;p&gt;
&#22312;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#24615;&#32780;&#26080;&#38656;&#28151;&#21512;&#26102;&#38388;&#39044;&#27979;&#65306;&#22522;&#20110;&#22810;&#32423;Actor-Critic&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Global Optimality without Mixing Time Oracles in Average-reward RL via Multi-level Actor-Critic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11925
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#32423;Actor-Critic&#26694;&#26550;&#21644;&#22810;&#32423;&#33945;&#29305;&#21345;&#32599;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#26412;&#30740;&#31350;&#25104;&#21151;&#35299;&#20915;&#20102;&#24179;&#22343;&#22870;&#21169;MDPs&#20840;&#23616;&#25910;&#25947;&#20013;&#23545;&#28151;&#21512;&#26102;&#38388;&#39044;&#27979;&#30340;&#20381;&#36182;&#24615;&#65292;&#23637;&#29616;&#20986;&#26368;&#20005;&#26684;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#20110;&#28151;&#21512;&#26102;&#38388;&#30340;&#39044;&#27979;&#30340;oracle&#30693;&#35782;&#35201;&#27714;&#65292;&#21363;&#24230;&#37327;&#39532;&#23572;&#21487;&#22827;&#38142;&#22312;&#22266;&#23450;&#31574;&#30053;&#19979;&#36798;&#21040;&#20854;&#31283;&#24577;&#20998;&#24067;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#23545;&#20110;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#29699;&#25910;&#25947;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#32423;Actor-Critic&#65288;MAC&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#22810;&#32423;&#33945;&#29305;&#21345;&#27931;&#65288;MLMC&#65289;&#26799;&#24230;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#28151;&#21512;&#26102;&#38388;&#30693;&#35782;&#30340;&#20381;&#36182;&#24615;&#30340;&#26377;&#25928;&#20943;&#36731;&#65292;&#36825;&#26159;&#24179;&#22343;&#22870;&#21169;MDPs&#20840;&#23616;&#25910;&#25947;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#26368;&#20005;&#26684;&#30340;$\mathcal{O}$&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11925v1 Announce Type: new  Abstract: In the context of average-reward reinforcement learning, the requirement for oracle knowledge of the mixing time, a measure of the duration a Markov chain under a fixed policy needs to achieve its stationary distribution-poses a significant challenge for the global convergence of policy gradient methods. This requirement is particularly problematic due to the difficulty and expense of estimating mixing time in environments with large state spaces, leading to the necessity of impractically long trajectories for effective gradient estimation in practical applications. To address this limitation, we consider the Multi-level Actor-Critic (MAC) framework, which incorporates a Multi-level Monte Carlo (MLMC) gradient estimator. With our approach, we effectively alleviate the dependency on mixing time knowledge, a first for average-reward MDPs global convergence. Furthermore, our approach exhibits the tightest-available dependence of $\mathcal{O
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XIAI&#65289;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#27880;&#24847;&#26426;&#21046;&#26159;&#20027;&#35201;&#26032;&#20852;IAI&#65292;&#21516;&#26102;&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#23616;&#24314;&#27169;&#12289;&#26368;&#20339;&#23454;&#36341;&#20197;&#21450;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.11894</link><description>&lt;p&gt;
&#20174;&#21487;&#35299;&#37322;&#21040;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#29616;&#23454;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11894
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XIAI&#65289;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#27880;&#24847;&#26426;&#21046;&#26159;&#20027;&#35201;&#26032;&#20852;IAI&#65292;&#21516;&#26102;&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#23616;&#24314;&#27169;&#12289;&#26368;&#20339;&#23454;&#36341;&#20197;&#21450;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36890;&#36807;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65292;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#21307;&#30103;&#20445;&#20581;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;DL&#30340;NLP&#26041;&#27861;&#26085;&#30410;&#22797;&#26434;&#65292;&#38656;&#35201;&#36879;&#26126;&#30340;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#25110;&#33267;&#23569;&#26159;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#36827;&#34892;&#21487;&#38752;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#26412;&#25991;&#23545;&#21307;&#30103;&#20581;&#24247;NLP&#20013;&#30340;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;DL&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#33539;&#22260;&#23457;&#26597;&#12290;&#24341;&#20837;&#20102;&#26415;&#35821;&#8220;XIAI&#8221;&#65288;eXplainable&#21644;Interpretable Artificial Intelligence&#65289;&#20197;&#21306;&#20998;XAI&#21644;IAI&#12290;&#26041;&#27861;&#26681;&#25454;&#20854;&#21151;&#33021;&#65288;&#27169;&#22411;&#12289;&#36755;&#20837;&#12289;&#36755;&#20986;&#20026;&#22522;&#30784;&#65289;&#21644;&#33539;&#22260;&#65288;&#23616;&#37096;&#12289;&#20840;&#23616;&#65289;&#36827;&#19968;&#27493;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#27880;&#24847;&#26426;&#21046;&#26159;&#26368;&#20027;&#35201;&#30340;&#26032;&#20852;IAI&#12290;&#27492;&#22806;&#65292;IAI&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#23545;&#25239;XAI&#12290;&#30830;&#23450;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22823;&#22810;&#25968;XIAI&#19981;&#25506;&#32034;&#8220;&#20840;&#23616;&#8221;&#24314;&#27169;&#36807;&#31243;&#65292;&#32570;&#20047;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#19988;&#38656;&#35201;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11894v1 Announce Type: cross  Abstract: Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks. Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI. Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms were the most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The major challenges identified are that most XIAI do not explore "global" modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks. Importan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CardioGenAI&#65292;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#23569;&#33647;&#29289;&#30340;hERG&#27963;&#24615;&#24182;&#20445;&#30041;&#33647;&#29702;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07632</link><description>&lt;p&gt;
CardioGenAI&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#29992;&#20110;&#20943;&#23569;hERG&#27602;&#24615;&#30340;&#33647;&#29289;&#20877;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs for Reduced hERG Liability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07632
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CardioGenAI&#65292;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#23569;&#33647;&#29289;&#30340;hERG&#27963;&#24615;&#24182;&#20445;&#30041;&#33647;&#29702;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#35825;&#23548;&#30340;&#24515;&#33039;&#27602;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#19981;&#33391;&#21453;&#24212;&#65292;&#21253;&#25324;&#36890;&#36807;&#38459;&#28382;&#30005;&#21387;&#38376;&#25511;&#30340;hERG&#38078;&#31163;&#23376;&#36890;&#36947;&#23548;&#33268;&#29983;&#21629;&#23041;&#32961;&#30340;&#24515;&#24459;&#22833;&#24120;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26089;&#26399;&#38454;&#27573;&#37492;&#23450;hERG&#27963;&#24615;&#21270;&#21512;&#29289;&#30340;&#20808;&#36827;&#26041;&#27861;&#65292;&#20197;&#21450;&#20248;&#21270;&#21830;&#19994;&#21270;&#33647;&#29289;&#20197;&#20943;&#23569;hERG&#27963;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CardioGenAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20877;&#35774;&#35745;&#24320;&#21457;&#20013;&#21644;&#24050;&#19978;&#24066;&#33647;&#29289;&#65292;&#20197;&#20943;&#23569;hERG&#27963;&#24615;&#21516;&#26102;&#20445;&#30041;&#20854;&#33647;&#29702;&#27963;&#24615;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#29992;&#20110;&#39044;&#27979;hERG&#36890;&#36947;&#27963;&#24615;&#30340;&#26368;&#26032;&#21028;&#21035;&#27169;&#22411;&#65292;&#20197;&#21450;&#38048;&#31163;&#23376;&#36890;&#36947;NaV1.5&#21644;&#38041;&#31163;&#23376;&#36890;&#36947;CaV1.2&#27963;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35843;&#33410;&#30001;hERG&#36890;&#36947;&#38459;&#28382;&#24341;&#36215;&#30340;&#24515;&#24459;&#22833;&#24120;&#28508;&#22312;&#24433;&#21709;&#20013;&#20855;&#26377;&#28508;&#22312;&#24847;&#20041;&#12290;&#36825;&#20123;&#27169;&#22411;&#36824;&#21487;&#20197;se
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07632v1 Announce Type: new  Abstract: Drug-induced cardiotoxicity is a major health concern which can lead to serious adverse effects including life-threatening cardiac arrhythmias via the blockade of the voltage-gated hERG potassium ion channel. It is therefore of tremendous interest to develop advanced methods to identify hERG-active compounds in early stages of drug development, as well as to optimize commercially available drugs for reduced hERG activity. In this work, we present CardioGenAI, a machine learning-based framework for re-engineering both developmental and marketed drugs for reduced hERG activity while preserving their pharmacological activity. The framework incorporates novel state-of-the-art discriminative models for predicting hERG channel activity, as well as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to their potential implications in modulating the arrhythmogenic potential induced by hERG channel blockade. These models can also se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#35774;&#22791;&#30828;&#20214;&#21644;&#36719;&#20214;&#37197;&#32622;&#30340;&#19981;&#21516;&#23548;&#33268;&#30340;&#31995;&#32479;&#35825;&#23548;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#24322;&#36136;&#24615;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#21517;&#20026;HeteroSwitch&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.04207</link><description>&lt;p&gt;
HeteroSwitch&#65306;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#31995;&#32479;&#35825;&#23548;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#34920;&#24449;&#19982;&#35843;&#25511;
&lt;/p&gt;
&lt;p&gt;
HeteroSwitch: Characterizing and Taming System-Induced Data Heterogeneity in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#35774;&#22791;&#30828;&#20214;&#21644;&#36719;&#20214;&#37197;&#32622;&#30340;&#19981;&#21516;&#23548;&#33268;&#30340;&#31995;&#32479;&#35825;&#23548;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#24322;&#36136;&#24615;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#21517;&#20026;HeteroSwitch&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#29992;&#25143;&#35774;&#22791;&#21327;&#20316;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35774;&#22791;&#19978;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#22312;FL&#20013;&#65292;&#21442;&#19982;&#30340;&#29992;&#25143;&#35774;&#22791;&#22312;&#30828;&#20214;&#21644;&#36719;&#20214;&#37197;&#32622;&#26041;&#38754;&#39640;&#24230;&#30862;&#29255;&#21270;&#12290;&#36825;&#31181;&#30862;&#29255;&#21270;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#21363;\textit{&#31995;&#32479;&#35825;&#23548;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;}&#65292;&#22240;&#20026;&#27599;&#20010;&#35774;&#22791;&#26681;&#25454;&#20854;&#30828;&#20214;&#21644;&#36719;&#20214;&#37197;&#32622;&#29983;&#25104;&#19981;&#21516;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#34920;&#24449;&#20102;&#31995;&#32479;&#35825;&#23548;&#25968;&#25454;&#24322;&#36136;&#24615;&#23545;FL&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#36328;&#20379;&#24212;&#21830;&#21644;&#24615;&#33021;&#23618;&#32423;&#21464;&#21270;&#30340;&#24322;&#26500;&#35774;&#22791;&#25910;&#38598;&#20102;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;\textit{&#31995;&#32479;&#35825;&#23548;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;}&#23545;&#20934;&#30830;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#24694;&#21270;&#20102;FL&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HeteroSwitch&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04207v1 Announce Type: new  Abstract: Federated Learning (FL) is a practical approach to train deep learning models collaboratively across user-end devices, protecting user privacy by retaining raw data on-device. In FL, participating user-end devices are highly fragmented in terms of hardware and software configurations. Such fragmentation introduces a new type of data heterogeneity in FL, namely \textit{system-induced data heterogeneity}, as each device generates distinct data depending on its hardware and software configurations. In this paper, we first characterize the impact of system-induced data heterogeneity on FL model performance. We collect a dataset using heterogeneous devices with variations across vendors and performance tiers. By using this dataset, we demonstrate that \textit{system-induced data heterogeneity} negatively impacts accuracy, and deteriorates fairness and domain generalization problems in FL. To address these challenges, we propose HeteroSwitch, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;RNNs&#21644;Transformer&#22312;&#22788;&#29702;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#33021;&#21147;&#24046;&#36317;&#65292;&#21457;&#29616;RNNs&#23384;&#22312;&#20851;&#38190;&#29942;&#39048;&#65292;&#21363;&#26080;&#27861;&#23436;&#32654;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#23548;&#33268;&#26080;&#27861;&#20687;Transformer&#37027;&#26679;&#36731;&#26494;&#35299;&#20915;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.18510</link><description>&lt;p&gt;
RNNs&#36824;&#19981;&#26159;Transformer&#65306;&#22312;&#19978;&#19979;&#25991;&#26816;&#32034;&#20013;&#30340;&#20851;&#38190;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;RNNs&#21644;Transformer&#22312;&#22788;&#29702;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#33021;&#21147;&#24046;&#36317;&#65292;&#21457;&#29616;RNNs&#23384;&#22312;&#20851;&#38190;&#29942;&#39048;&#65292;&#21363;&#26080;&#27861;&#23436;&#32654;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#23548;&#33268;&#26080;&#27861;&#20687;Transformer&#37027;&#26679;&#36731;&#26494;&#35299;&#20915;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#21644;Transformer&#22312;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#31034;&#33021;&#21147;&#24046;&#36317;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;RNNs&#26159;&#21542;&#33021;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#65292;&#36890;&#36807;Chain-of-Thought (CoT)&#25552;&#31034;&#65292;&#19982;Transformer&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#26174;&#31034;CoT&#21487;&#20197;&#25913;&#36827;RNNs&#65292;&#20294;&#26080;&#27861;&#24357;&#34917;&#19982;Transformer&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20851;&#38190;&#29942;&#39048;&#22312;&#20110;RNNs&#26080;&#27861;&#23436;&#20840;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#21363;&#20351;&#32463;&#36807;CoT&#30340;&#22686;&#24378;&#65306;&#23545;&#20110;&#20960;&#20010;&#26126;&#30830;&#25110;&#38544;&#24335;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#65292;&#22914;&#32852;&#24819;&#21484;&#22238;&#21644;&#30830;&#23450;&#22270;&#26159;&#21542;&#20026;&#26641;&#65292;&#25105;&#20204;&#35777;&#26126;RNNs&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#20197;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#65292;&#32780;Transformer&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#37319;&#29992;&#22686;&#24378;RNNs&#19978;&#19979;&#25991;&#26816;&#32034;&#33021;&#21147;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18510v1 Announce Type: cross  Abstract: This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, inclu
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#36807;&#21435;10&#24180;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#21508;&#31181;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30528;&#37325;&#27604;&#36739;&#20102;&#26368;&#26032;&#30340;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.17045</link><description>&lt;p&gt;
&#23545;&#21508;&#31867;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24615;&#33021;&#30340;&#30740;&#31350;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Investigation into the Performances of the State-of-the-art Machine Learning Approaches for Various Cyber-attack Detection: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17045
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#36807;&#21435;10&#24180;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#21508;&#31181;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30528;&#37325;&#27604;&#36739;&#20102;&#26368;&#26032;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20445;&#25252;&#35745;&#31639;&#26426;&#21644;&#20449;&#24687;&#31995;&#32479;&#20813;&#21463;&#25915;&#20987;&#32773;&#21033;&#29992;&#31995;&#32479;&#20013;&#30340;&#28431;&#27934;&#36827;&#34892;&#32593;&#32476;&#29359;&#32618;&#30340;&#20405;&#23475;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#28431;&#27934;&#20197;&#25552;&#39640;&#20449;&#24687;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#26041;&#27861;&#12290;&#22312;&#25152;&#26377;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#26159;&#23454;&#29616;&#31995;&#32479;&#23433;&#20840;&#30340;&#25928;&#26524;&#26368;&#22909;&#30340;&#26041;&#27861;&#65292;&#20854;&#33021;&#21147;&#33539;&#22260;&#20174;&#26089;&#26399;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#21040;&#23454;&#26102;&#26816;&#27979;&#31995;&#32479;&#20013;&#27491;&#22312;&#36827;&#34892;&#30340;&#22949;&#21327;&#12290;&#30001;&#20110;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#32593;&#32476;&#25915;&#20987;&#65292;&#27599;&#31181;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37117;&#20381;&#36182;&#20110;&#19981;&#21516;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36825;&#20063;&#24433;&#21709;&#20102;&#23427;&#20204;&#23545;&#29305;&#23450;&#31867;&#22411;&#32593;&#32476;&#25915;&#20987;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36807;&#21435;10&#24180;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#27599;&#19968;&#20010;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#37325;&#28857;&#25918;&#22312;&#26368;&#36817;&#30340;&#24037;&#20316;&#19978;&#20197;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17045v1 Announce Type: cross  Abstract: To secure computers and information systems from attackers taking advantage of vulnerabilities in the system to commit cybercrime, several methods have been proposed for real-time detection of vulnerabilities to improve security around information systems. Of all the proposed methods, machine learning had been the most effective method in securing a system with capabilities ranging from early detection of software vulnerabilities to real-time detection of ongoing compromise in a system. As there are different types of cyberattacks, each of the existing state-of-the-art machine learning models depends on different algorithms for training which also impact their suitability for detection of a particular type of cyberattack. In this research, we analyzed each of the current state-of-theart machine learning models for different types of cyberattack detection from the past 10 years with a major emphasis on the most recent works for comparat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#23433;&#20840;&#25506;&#32034;&#20934;&#21017;&#65292;&#32467;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#20934;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.15347</link><description>&lt;p&gt;
&#20449;&#24687;&#35770;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Safe Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15347
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#23433;&#20840;&#25506;&#32034;&#20934;&#21017;&#65292;&#32467;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#19981;&#35780;&#20272;&#36829;&#21453;&#20808;&#39564;&#26410;&#30693;&#65288;&#23433;&#20840;&#65289;&#32422;&#26463;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#26410;&#30693;&#20989;&#25968;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#22312;&#26410;&#30693;&#20989;&#25968;&#19978;&#25918;&#32622;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#65292;&#24182;&#19988;&#20165;&#20801;&#35768;&#22312;&#39640;&#27010;&#29575;&#23433;&#20840;&#21306;&#22495;&#20869;&#36827;&#34892;&#35780;&#20272;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#22495;&#30340;&#31163;&#25955;&#21270;&#65292;&#24182;&#19988;&#19981;&#33021;&#30452;&#25509;&#25193;&#23637;&#21040;&#36830;&#32493;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21033;&#29992;&#32422;&#26463;&#30340;&#35268;&#21017;&#20551;&#35774;&#30340;&#26041;&#24335;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#23433;&#20840;&#25506;&#32034;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#30452;&#25509;&#21033;&#29992;GP&#21518;&#39564;&#26469;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#30340;&#23433;&#20840;&#21442;&#25968;&#36827;&#34892;&#35780;&#20272;&#12290;&#23558;&#36825;&#19968;&#25506;&#32034;&#20934;&#21017;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#32467;&#21512;&#36215;&#26469;&#65292;&#20135;&#29983;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15347v1 Announce Type: cross  Abstract: We consider a sequential decision making task, where the goal is to optimize an unknown function without evaluating parameters that violate an a~priori unknown (safety) constraint. A common approach is to place a Gaussian process prior on the unknown functions and allow evaluations only in regions that are safe with high probability. Most current methods rely on a discretization of the domain and cannot be directly extended to the continuous case. Moreover, the way in which they exploit regularity assumptions about the constraint introduces an additional critical hyperparameter. In this paper, we propose an information-theoretic safe exploration criterion that directly exploits the GP posterior to identify the most informative safe parameters to evaluate. The combination of this exploration criterion with a well known Bayesian optimization acquisition function yields a novel safe Bayesian optimization selection criterion. Our approach 
&lt;/p&gt;</description></item><item><title>&#23558;&#26102;&#38388;&#21160;&#20316;&#25277;&#35937;&#35270;&#20026;&#24207;&#21015;&#21387;&#32553;&#38382;&#39064;&#65292;&#20351;&#29992;Primitive Sequence Encoding (PRISE)&#26041;&#27861;&#32467;&#21512;&#36830;&#32493;&#21160;&#20316;&#37327;&#21270;&#19982;BPE&#26469;&#23398;&#20064;&#24378;&#22823;&#30340;&#21160;&#20316;&#25277;&#35937;&#65292;&#24182;&#22312;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;</title><link>https://arxiv.org/abs/2402.10450</link><description>&lt;p&gt;
PRISE&#65306;&#23558;&#26102;&#38388;&#21160;&#20316;&#25277;&#35937;&#35270;&#20026;&#24207;&#21015;&#21387;&#32553;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10450
&lt;/p&gt;
&lt;p&gt;
&#23558;&#26102;&#38388;&#21160;&#20316;&#25277;&#35937;&#35270;&#20026;&#24207;&#21015;&#21387;&#32553;&#38382;&#39064;&#65292;&#20351;&#29992;Primitive Sequence Encoding (PRISE)&#26041;&#27861;&#32467;&#21512;&#36830;&#32493;&#21160;&#20316;&#37327;&#21270;&#19982;BPE&#26469;&#23398;&#20064;&#24378;&#22823;&#30340;&#21160;&#20316;&#25277;&#35937;&#65292;&#24182;&#22312;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#21160;&#20316;&#25277;&#35937;&#20197;&#21450;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26159;&#24207;&#36143;&#20915;&#31574;&#20013;&#30340;&#24378;&#22823;&#30693;&#35782;&#20849;&#20139;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#28857;&#65292;&#23558;&#35825;&#23548;&#26102;&#38388;&#21160;&#20316;&#25277;&#35937;&#35270;&#20026;&#24207;&#21015;&#21387;&#32553;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;LLM&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#19968;&#20010;&#24494;&#22937;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998; -- &#36755;&#20837;&#26631;&#35760;&#21270;&#36890;&#36807;&#23383;&#33410;&#23545;&#32534;&#30721;&#65288;BPE&#65289; -- &#24102;&#21040;&#20102;&#36830;&#32493;&#25511;&#21046;&#39046;&#22495;&#20013;&#23398;&#20064;&#21487;&#21464;&#26102;&#38388;&#36328;&#24230;&#25216;&#33021;&#30340; seemingly distant &#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;Primitive Sequence Encoding&#65288;PRISE&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#36830;&#32493;&#21160;&#20316;&#37327;&#21270;&#19982;BPE&#30456;&#32467;&#21512;&#65292;&#23398;&#20064;&#24378;&#22823;&#30340;&#21160;&#20316;&#25277;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;PRISE&#20174;&#19968;&#32452;&#26426;&#22120;&#20154;&#25805;&#20316;&#28436;&#31034;&#20013;&#21457;&#29616;&#30340;&#39640;&#32423;&#25216;&#33021;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#20197;&#21450;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312; https: &#25918;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10450v1 Announce Type: new  Abstract: Temporal action abstractions, along with belief state representations, are a powerful knowledge sharing mechanism for sequential decision making. In this work, we propose a novel view that treats inducing temporal action abstractions as a sequence compression problem. To do so, we bring a subtle but critical component of LLM training pipelines -- input tokenization via byte pair encoding (BPE) -- to the seemingly distant task of learning skills of variable time span in continuous control domains. We introduce an approach called Primitive Sequence Encoding (PRISE) that combines continuous action quantization with BPE to learn powerful action abstractions. We empirically show that high-level skills discovered by PRISE from a multitask set of robotic manipulation demonstrations significantly boost the performance of both multitask imitation learning as well as few-shot imitation learning on unseen tasks. Our code will be released at https:/
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21463;&#22823;&#33041;&#32467;&#26500;&#21551;&#21457;&#30340;&#26032;&#30340;&#35745;&#31639;&#26041;&#27861;&#8212;&#8212;&#26368;&#36817;&#37051;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#24212;&#20851;&#31995;&#21644;&#26126;&#30830;&#30340;&#26500;&#36896;&#65292;&#21487;&#20197;&#34920;&#31034;&#28145;&#24230;&#20026;2&#30340;&#38408;&#20540;&#30005;&#36335;&#31561;&#19981;&#21516;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.08751</link><description>&lt;p&gt;
&#31070;&#32463;&#30005;&#36335;&#30340;&#26368;&#36817;&#37051;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nearest Neighbor Representations of Neural Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08751
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21463;&#22823;&#33041;&#32467;&#26500;&#21551;&#21457;&#30340;&#26032;&#30340;&#35745;&#31639;&#26041;&#27861;&#8212;&#8212;&#26368;&#36817;&#37051;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#24212;&#20851;&#31995;&#21644;&#26126;&#30830;&#30340;&#26500;&#36896;&#65292;&#21487;&#20197;&#34920;&#31034;&#28145;&#24230;&#20026;2&#30340;&#38408;&#20540;&#30005;&#36335;&#31561;&#19981;&#21516;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25104;&#21151;&#22320;&#25429;&#25417;&#21040;&#20102;&#20154;&#33041;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#21463;&#21040;&#22823;&#33041;&#32467;&#26500;&#30340;&#21551;&#21457;&#65292;&#26368;&#36817;&#37051;&#65288;NN&#65289;&#34920;&#31034;&#27861;&#26159;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;NN&#34920;&#31034;&#27861;&#19982;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#24314;&#31435;&#20102;&#26356;&#29282;&#22266;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#34429;&#28982;&#24050;&#30693;&#22914;&#20309;&#20351;&#29992;NN&#34920;&#31034;&#27861;&#34920;&#31034;&#21333;&#20010;&#31070;&#32463;&#20803;&#65292;&#20294;&#23545;&#20110;&#23567;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23578;&#26080;&#32467;&#26524;&#12290;&#20855;&#20307;&#22320;&#65292;&#38024;&#23545;&#28145;&#24230;&#20026;2&#30340;&#38408;&#20540;&#30005;&#36335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20854;NN&#34920;&#31034;&#30340;&#26126;&#30830;&#26500;&#36896;&#65292;&#24182;&#32473;&#20986;&#20102;&#34920;&#31034;&#25152;&#38656;&#20301;&#25968;&#30340;&#26126;&#30830;&#30028;&#38480;&#12290;&#31034;&#20363;&#20989;&#25968;&#21253;&#25324;&#20984;&#22810;&#38754;&#20307;&#30340;NN&#34920;&#31034;&#65288;&#38408;&#20540;&#38376;&#30340;AND&#65289;&#12289;IP2&#12289;&#38408;&#20540;&#38376;&#30340;OR&#20197;&#21450;&#32447;&#24615;&#25110;&#31934;&#30830;&#20915;&#31574;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08751v1 Announce Type: cross Abstract: Neural networks successfully capture the computational power of the human brain for many tasks. Similarly inspired by the brain architecture, Nearest Neighbor (NN) representations is a novel approach of computation. We establish a firmer correspondence between NN representations and neural networks. Although it was known how to represent a single neuron using NN representations, there were no results even for small depth neural networks. Specifically, for depth-2 threshold circuits, we provide explicit constructions for their NN representation with an explicit bound on the number of bits to represent it. Example functions include NN representations of convex polytopes (AND of threshold gates), IP2, OR of threshold gates, and linear or exact decision lists.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#37051;&#65288;NN&#65289;&#34920;&#31034;&#29992;&#20110;&#34920;&#31034;&#31070;&#32463;&#20803;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#33879;&#21517;&#30340;&#38408;&#20540;&#20989;&#25968;&#21487;&#20197;&#29992;&#22810;&#39033;&#24335;&#35268;&#27169;&#30340;&#38170;&#28857;&#21644;&#23545;&#25968;&#20998;&#36776;&#29575;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.08748</link><description>&lt;p&gt;
&#31070;&#32463;&#20803;&#30340;&#26368;&#36817;&#37051;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Nearest Neighbor Representations of Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08748
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#37051;&#65288;NN&#65289;&#34920;&#31034;&#29992;&#20110;&#34920;&#31034;&#31070;&#32463;&#20803;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#33879;&#21517;&#30340;&#38408;&#20540;&#20989;&#25968;&#21487;&#20197;&#29992;&#22810;&#39033;&#24335;&#35268;&#27169;&#30340;&#38170;&#28857;&#21644;&#23545;&#25968;&#20998;&#36776;&#29575;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#37051;&#65288;NN&#65289;&#34920;&#31034;&#26159;&#19968;&#31181;&#21463;&#21040;&#22823;&#33041;&#21551;&#21457;&#30340;&#26032;&#20852;&#35745;&#31639;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;NN&#34920;&#31034;&#26469;&#34920;&#31034;&#31070;&#32463;&#20803;&#65288;&#38408;&#20540;&#20989;&#25968;&#65289;&#30340;&#22797;&#26434;&#24615;&#12290;&#24050;&#30693;&#65292;&#20004;&#20010;&#38170;&#28857;&#65288;NN&#35745;&#31639;&#30340;&#28857;&#65289;&#36275;&#20197;&#34920;&#31034;&#38408;&#20540;&#20989;&#25968;&#30340;NN&#34920;&#31034;&#65292;&#28982;&#32780;&#20998;&#36776;&#29575;&#65288;&#38170;&#28857;&#26465;&#30446;&#25152;&#38656;&#30340;&#26368;&#22823;&#27604;&#29305;&#25968;&#65289;&#26159;$O(n\log{n})$&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#38170;&#28857;&#25968;&#21644;&#38408;&#20540;&#20989;&#25968;&#30340;NN&#34920;&#31034;&#30340;&#20998;&#36776;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#33879;&#21517;&#30340;&#38408;&#20540;&#20989;&#25968;EQUALITY&#65292;COMPARISON&#21644;ODD-MAX-BIT&#21487;&#20197;&#36890;&#36807;$n$&#21644;$O(\log{n})$&#30340;&#20998;&#36776;&#29575;&#20197;&#22810;&#39033;&#24335;&#35268;&#27169;&#30340;&#38170;&#28857;&#34920;&#31034;&#65292;&#36825;&#20123;&#20989;&#25968;&#38656;&#35201;2&#25110;3&#20010;&#38170;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08748v1 Announce Type: cross Abstract: The Nearest Neighbor (NN) Representation is an emerging computational model that is inspired by the brain. We study the complexity of representing a neuron (threshold function) using the NN representations. It is known that two anchors (the points to which NN is computed) are sufficient for a NN representation of a threshold function, however, the resolution (the maximum number of bits required for the entries of an anchor) is $O(n\log{n})$. In this work, the trade-off between the number of anchors and the resolution of a NN representation of threshold functions is investigated. We prove that the well-known threshold functions EQUALITY, COMPARISON, and ODD-MAX-BIT, which require 2 or 3 anchors and resolution of $O(n)$, can be represented by polynomially large number of anchors in $n$ and $O(\log{n})$ resolution. We conjecture that for all threshold functions, there are NN representations with polynomially large size and logarithmic reso
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#26041;&#27861;Riemann-Lebesgue Forest (RLF)&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#21010;&#20998;&#20989;&#25968;&#30340;&#20540;&#22495;&#20026;&#22810;&#20010;&#21306;&#38388;&#26469;&#36924;&#36817;&#21487;&#27979;&#20989;&#25968;&#30340;&#24605;&#24819;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#23398;&#20064;&#31639;&#27861;Riemann-Lebesgue Tree&#12290;&#36890;&#36807;Hoeffding&#20998;&#35299;&#21644;Stein&#26041;&#27861;&#25512;&#23548;&#20102;RLF&#22312;&#19981;&#21516;&#21442;&#25968;&#35774;&#32622;&#19979;&#30340;&#28176;&#36817;&#24615;&#33021;&#65292;&#24182;&#22312;&#20223;&#30495;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;RLF&#19982;&#21407;&#22987;&#38543;&#26426;&#26862;&#26519;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04550</link><description>&lt;p&gt;
Riemann-Lebesgue Forest&#22238;&#24402;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Riemann-Lebesgue Forest for Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04550
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#26041;&#27861;Riemann-Lebesgue Forest (RLF)&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#21010;&#20998;&#20989;&#25968;&#30340;&#20540;&#22495;&#20026;&#22810;&#20010;&#21306;&#38388;&#26469;&#36924;&#36817;&#21487;&#27979;&#20989;&#25968;&#30340;&#24605;&#24819;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#23398;&#20064;&#31639;&#27861;Riemann-Lebesgue Tree&#12290;&#36890;&#36807;Hoeffding&#20998;&#35299;&#21644;Stein&#26041;&#27861;&#25512;&#23548;&#20102;RLF&#22312;&#19981;&#21516;&#21442;&#25968;&#35774;&#32622;&#19979;&#30340;&#28176;&#36817;&#24615;&#33021;&#65292;&#24182;&#22312;&#20223;&#30495;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;RLF&#19982;&#21407;&#22987;&#38543;&#26426;&#26862;&#26519;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#31216;&#20026;Riemann-Lebesgue Forest (RLF)&#12290;RLF&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#23558;&#20989;&#25968;&#30340;&#20540;&#22495;&#21010;&#20998;&#20026;&#20960;&#20010;&#21306;&#38388;&#26469;&#27169;&#25311;&#21487;&#27979;&#20989;&#25968;&#30340;&#36924;&#36817;&#26041;&#24335;&#12290;&#22522;&#20110;&#36825;&#20010;&#24605;&#24819;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;Riemann-Lebesgue Tree&#65292;&#23427;&#22312;&#27599;&#20010;&#38750;&#21494;&#33410;&#28857;&#19978;&#26377;&#26426;&#20250;&#20174;&#21709;&#24212;Y&#25110;&#29305;&#24449;&#31354;&#38388;X&#20013;&#30340;&#26041;&#21521;&#36827;&#34892;&#20999;&#21106;&#12290;&#25105;&#20204;&#36890;&#36807;Hoeffding&#20998;&#35299;&#21644;Stein&#26041;&#27861;&#26469;&#25512;&#23548;&#19981;&#21516;&#21442;&#25968;&#35774;&#32622;&#19979;RLF&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;&#24403;&#24213;&#23618;&#20989;&#25968;Y=f(X)&#36981;&#24490;&#21152;&#27861;&#22238;&#24402;&#27169;&#22411;&#26102;&#65292;RLF&#19982;Scornet&#31561;&#20154;&#30340;&#35770;&#35777;&#65288;2014&#24180;&#65289;&#20445;&#25345;&#19968;&#33268;&#12290;&#36890;&#36807;&#22312;&#20223;&#30495;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;RLF&#19982;&#21407;&#22987;&#38543;&#26426;&#26862;&#26519;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel ensemble method called Riemann-Lebesgue Forest (RLF) for regression. The core idea of RLF is to mimic the way how a measurable function can be approximated by partitioning its range into a few intervals. With this idea in mind, we develop a new tree learner named Riemann-Lebesgue Tree which has a chance to split the node from response $Y$ or a direction in feature space $\mathbf{X}$ at each non-terminal node. We generalize the asymptotic performance of RLF under different parameter settings mainly through Hoeffding decomposition \cite{Vaart} and Stein's method \cite{Chen2010NormalAB}. When the underlying function $Y=f(\mathbf{X})$ follows an additive regression model, RLF is consistent with the argument from \cite{Scornet2014ConsistencyOR}. The competitive performance of RLF against original random forest \cite{Breiman2001RandomF} is demonstrated by experiments in simulation data and real world datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#38543;&#26426;&#24615;&#30340;&#26080;&#38480;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#26694;&#26550;&#20013;&#25972;&#21512;&#37096;&#20998;&#38543;&#26426;&#24615;&#65292;&#25913;&#21892;&#29616;&#26377;&#26550;&#26500;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#28789;&#27963;&#30340;&#32593;&#32476;&#35774;&#35745;&#37197;&#32622;&#65292;&#21516;&#26102;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03495</link><description>&lt;p&gt;
&#37096;&#20998;&#38543;&#26426;&#30340;&#26080;&#38480;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Partially Stochastic Infinitely Deep Bayesian Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#38543;&#26426;&#24615;&#30340;&#26080;&#38480;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#26694;&#26550;&#20013;&#25972;&#21512;&#37096;&#20998;&#38543;&#26426;&#24615;&#65292;&#25913;&#21892;&#29616;&#26377;&#26550;&#26500;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#28789;&#27963;&#30340;&#32593;&#32476;&#35774;&#35745;&#37197;&#32622;&#65292;&#21516;&#26102;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#38543;&#26426;&#30340;&#26080;&#38480;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#37096;&#20998;&#38543;&#26426;&#24615;&#25972;&#21512;&#21040;&#26080;&#38480;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#30340;&#26032;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#26550;&#26500;&#26088;&#22312;&#25913;&#21892;&#29616;&#26377;&#26550;&#26500;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#38480;&#21046;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#37096;&#20998;&#38543;&#26426;&#24615;&#22312;&#26080;&#38480;&#28145;&#24230;&#26497;&#38480;&#19979;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#20840;&#38543;&#26426;&#24615;&#30340;&#22909;&#22788;&#65292;&#22914;&#40065;&#26834;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#20869;&#23384;&#25928;&#29575;&#65292;&#21516;&#26102;&#25913;&#21892;&#20102;&#23427;&#20204;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#26550;&#26500;&#37197;&#32622;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#35774;&#35745;&#30340;&#28789;&#27963;&#24615;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#26435;&#37325;&#21010;&#20998;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#30830;&#31435;&#25105;&#20204;&#30340;&#32593;&#32476;&#23478;&#26063;&#31526;&#21512;&#36890;&#29992;&#26465;&#20214;&#20998;&#24067;&#36817;&#20284;&#22120;&#30340;&#25968;&#23398;&#20445;&#35777;&#65292;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Partially Stochastic Infinitely Deep Bayesian Neural Networks, a novel family of architectures that integrates partial stochasticity into the framework of infinitely deep neural networks. Our new class of architectures is designed to improve the limitations of existing architectures around computational efficiency at training and inference time. To do this, we leverage the advantages of partial stochasticity in the infinite-depth limit which include the benefits of full stochasticity e.g. robustness, uncertainty quantification, and memory efficiency, whilst improving their limitations around computational efficiency at training and inference time. We present a variety of architectural configurations, offering flexibility in network design including different methods for weight partition. We also provide mathematical guarantees on the expressivity of our models by establishing that our network family qualifies as Universal Conditional Distribution Approximators
&lt;/p&gt;</description></item><item><title>Taylor&#35270;&#39057;&#26159;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#26684;&#24335;&#65292;&#29992;&#20110;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#21160;&#20316;&#25552;&#21462;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;Taylor&#23637;&#24320;&#36817;&#20284;&#35745;&#31639;&#38544;&#21547;&#30340;&#21160;&#20316;&#25552;&#21462;&#20989;&#25968;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21160;&#20316;&#25552;&#21462;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03019</link><description>&lt;p&gt;
&#29992;&#20110;&#21160;&#20316;&#35782;&#21035;&#30340;Taylor&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Taylor Videos for Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03019
&lt;/p&gt;
&lt;p&gt;
Taylor&#35270;&#39057;&#26159;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#26684;&#24335;&#65292;&#29992;&#20110;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#21160;&#20316;&#25552;&#21462;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;Taylor&#23637;&#24320;&#36817;&#20284;&#35745;&#31639;&#38544;&#21547;&#30340;&#21160;&#20316;&#25552;&#21462;&#20989;&#25968;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21160;&#20316;&#25552;&#21462;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#39057;&#20013;&#26377;&#25928;&#22320;&#25552;&#21462;&#21160;&#20316;&#26159;&#21160;&#20316;&#35782;&#21035;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21160;&#20316;(i)&#27809;&#26377;&#26126;&#30830;&#30340;&#24418;&#24335;&#65292;(ii)&#25317;&#26377;&#35832;&#22914;&#20301;&#31227;&#12289;&#36895;&#24230;&#21644;&#21152;&#36895;&#24230;&#31561;&#21508;&#31181;&#27010;&#24565;&#65292;(iii)&#36890;&#24120;&#20250;&#21463;&#21040;&#19981;&#31283;&#23450;&#20687;&#32032;&#24341;&#36215;&#30340;&#22122;&#22768;&#30340;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Taylor&#35270;&#39057;&#65292;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#26684;&#24335;&#65292;&#23427;&#31361;&#20986;&#26174;&#31034;&#20102;&#27599;&#20010;&#24103;&#20013;&#30340;&#20027;&#35201;&#21160;&#20316;&#65288;&#20363;&#22914;&#25381;&#25163;&#65289;&#34987;&#31216;&#20026;Taylor&#24103;&#12290;Taylor&#35270;&#39057;&#30340;&#21629;&#21517;&#26469;&#28304;&#20110;Taylor&#32423;&#25968;&#65292;&#23427;&#20351;&#29992;&#37325;&#35201;&#30340;&#39033;&#26469;&#36817;&#20284;&#32473;&#23450;&#28857;&#19978;&#30340;&#20989;&#25968;&#12290;&#22312;&#35270;&#39057;&#30340;&#24773;&#22659;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#38544;&#21547;&#30340;&#21160;&#20316;&#25552;&#21462;&#20989;&#25968;&#65292;&#26088;&#22312;&#20174;&#35270;&#39057;&#26102;&#38388;&#22359;&#20013;&#25552;&#21462;&#21160;&#20316;&#12290;&#22312;&#36825;&#20010;&#22359;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24103;&#12289;&#24046;&#20998;&#24103;&#21644;&#39640;&#38454;&#24046;&#20998;&#24103;&#36827;&#34892;Taylor&#23637;&#24320;&#65292;&#20197;&#36817;&#20284;&#35745;&#31639;&#36215;&#22987;&#24103;&#19978;&#30340;&#36825;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Taylor&#32423;&#25968;&#20013;&#39640;&#38454;&#39033;&#30340;&#27714;&#21644;&#32473;&#25105;&#20204;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Effectively extracting motions from video is a critical and long-standing problem for action recognition. This problem is very challenging because motions (i) do not have an explicit form, (ii) have various concepts such as displacement, velocity, and acceleration, and (iii) often contain noise caused by unstable pixels. Addressing these challenges, we propose the Taylor video, a new video format that highlights the dominate motions (e.g., a waving hand) in each of its frames named the Taylor frame. Taylor video is named after Taylor series, which approximates a function at a given point using important terms. In the scenario of videos, we define an implicit motion-extraction function which aims to extract motions from video temporal block. In this block, using the frames, the difference frames, and higher-order difference frames, we perform Taylor expansion to approximate this function at the starting frame. We show the summation of the higher-order terms in the Taylor series gives us
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35299;&#32806;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#38450;&#27490;&#20266;&#35013;&#30340;&#31243;&#24207;&#19981;&#20844;&#24179;&#65292;&#24182;&#24378;&#35843;&#20102;&#28385;&#36275;&#31243;&#24207;&#20844;&#24179;&#35201;&#27714;&#30340;&#37325;&#35201;&#24615;</title><link>https://arxiv.org/abs/2311.14688</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#32806;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#26469;&#23454;&#29616;&#31243;&#24207;&#20844;&#24179;
&lt;/p&gt;
&lt;p&gt;
Procedural Fairness Through Decoupling Objectionable Data Generating Components
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14688
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35299;&#32806;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#38450;&#27490;&#20266;&#35013;&#30340;&#31243;&#24207;&#19981;&#20844;&#24179;&#65292;&#24182;&#24378;&#35843;&#20102;&#28385;&#36275;&#31243;&#24207;&#20844;&#24179;&#35201;&#27714;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25581;&#31034;&#24182;&#35299;&#20915;&#20102;&#32463;&#24120;&#34987;&#24573;&#35270;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#21363;&#20266;&#35013;&#30340;&#31243;&#24207;&#19981;&#20844;&#24179;&#65292;&#21363;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#20013;&#31435;&#65288;&#21363;&#19981;&#25104;&#38382;&#39064;&#30340;&#65289;&#26041;&#38754;&#30340;&#21487;&#33021;&#26080;&#24847;&#30340;&#25913;&#21464;&#65292;&#21644;/&#25110;&#23545;&#26368;&#19981;&#21033;&#21033;&#30410;&#20010;&#20307;&#30340;&#23454;&#29616;&#27809;&#26377;&#31243;&#24207;&#20445;&#35777;&#12290;&#21463;&#32422;&#32752;&#183;&#32599;&#23572;&#26031;&#23545;&#32431;&#31243;&#24207;&#20844;&#27491;&#30340;&#20513;&#23548;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#33258;&#21160;&#20915;&#31574;&#35270;&#20026;&#31038;&#20250;&#21046;&#24230;&#30340;&#32553;&#24433;&#65292;&#24182;&#32771;&#34385;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#26412;&#36523;&#22914;&#20309;&#28385;&#36275;&#31243;&#24207;&#20844;&#24179;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#32771;&#28857;&#21644;&#30456;&#20851;&#30340;&#20215;&#20540;&#23454;&#20363;&#21270;&#35268;&#21017;&#65292;&#23558;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#19982;&#20013;&#31435;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#35299;&#32806;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#38450;&#27490;&#20266;&#35013;&#30340;&#31243;&#24207;&#19981;&#20844;&#24179;&#30340;&#24517;&#35201;&#24615;&#65292;&#19981;&#20165;&#24341;&#36215;&#20102;&#25105;&#20204;&#21147;&#22270;&#32531;&#35299;&#30340;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#30340;&#27880;&#24847;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14688v2 Announce Type: replace-cross  Abstract: We reveal and address the frequently overlooked yet important issue of disguised procedural unfairness, namely, the potentially inadvertent alterations on the behavior of neutral (i.e., not problematic) aspects of data generating process, and/or the lack of procedural assurance of the greatest benefit of the least advantaged individuals. Inspired by John Rawls's advocacy for pure procedural justice, we view automated decision-making as a microcosm of social institutions, and consider how the data generating process itself can satisfy the requirements of procedural fairness. We propose a framework that decouples the objectionable data generating components from the neutral ones by utilizing reference points and the associated value instantiation rule. Our findings highlight the necessity of preventing disguised procedural unfairness, drawing attention not only to the objectionable data generating components that we aim to mitiga
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31526;&#21495;&#26816;&#32034;&#8221;&#30340;&#26041;&#27861;&#26469;&#39640;&#25928;&#21387;&#32553;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#65288;DCT&#65289;&#31995;&#25968;&#30340;&#31526;&#21495;&#20449;&#24687;&#12290;&#36890;&#36807;&#25490;&#38500;&#31526;&#21495;&#20449;&#24687;&#24182;&#22312;&#35299;&#30721;&#22120;&#20013;&#34917;&#20805;&#65292;&#35813;&#26041;&#27861;&#22312;&#31526;&#21495;&#20301;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2209.10712</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#31526;&#21495;&#26816;&#32034;&#22312;&#22522;&#20110;DCT&#30340;&#22270;&#20687;&#32534;&#30721;&#20013;&#21387;&#32553;&#31526;&#21495;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Compressing Sign Information in DCT-based Image Coding via Deep Sign Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.10712
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31526;&#21495;&#26816;&#32034;&#8221;&#30340;&#26041;&#27861;&#26469;&#39640;&#25928;&#21387;&#32553;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#65288;DCT&#65289;&#31995;&#25968;&#30340;&#31526;&#21495;&#20449;&#24687;&#12290;&#36890;&#36807;&#25490;&#38500;&#31526;&#21495;&#20449;&#24687;&#24182;&#22312;&#35299;&#30721;&#22120;&#20013;&#34917;&#20805;&#65292;&#35813;&#26041;&#27861;&#22312;&#31526;&#21495;&#20301;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31526;&#21495;&#30340;&#31561;&#27010;&#29575;&#29305;&#24615;&#65292;&#21387;&#32553;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#65288;DCT&#65289;&#31995;&#25968;&#30340;&#31526;&#21495;&#20449;&#24687;&#26159;&#22270;&#20687;&#32534;&#30721;&#26041;&#26696;&#20013;&#19968;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31526;&#21495;&#26816;&#32034;&#8221;&#30340;&#31526;&#21495;&#20449;&#24687;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21463;&#21040;&#30456;&#20301;&#24674;&#22797;&#30340;&#21551;&#21457;&#65292;&#30456;&#20301;&#24674;&#22797;&#26159;&#20174;&#20854;&#24133;&#24230;&#20013;&#25214;&#21040;&#31163;&#25955;&#20613;&#37324;&#21494;&#21464;&#25442;&#31995;&#25968;&#30340;&#30456;&#20301;&#20449;&#24687;&#30340;&#32463;&#20856;&#20449;&#21495;&#24674;&#22797;&#38382;&#39064;&#12290;&#22312;&#32534;&#30721;&#22120;&#20013;&#65292;&#25152;&#26377;DCT&#31995;&#25968;&#30340;&#31526;&#21495;&#20449;&#24687;&#34987;&#25490;&#38500;&#22312;&#27604;&#29305;&#27969;&#20043;&#22806;&#65292;&#24182;&#19988;&#36890;&#36807;&#25105;&#20204;&#30340;&#31526;&#21495;&#26816;&#32034;&#26041;&#27861;&#22312;&#35299;&#30721;&#22120;&#20013;&#34917;&#20805;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31526;&#21495;&#20301;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;Python&#35821;&#35328;&#23454;&#29616;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;https://github.com/ctsutake/dsr&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compressing the sign information of discrete cosine transform (DCT) coefficients is an intractable problem in image coding schemes due to the equiprobable characteristics of the signs. To overcome this difficulty, we propose an efficient compression method for the sign information called "sign retrieval." This method is inspired by phase retrieval, which is a classical signal restoration problem of finding the phase information of discrete Fourier transform coefficients from their magnitudes. The sign information of all DCT coefficients is excluded from a bitstream at the encoder and is complemented at the decoder through our sign retrieval method. We show through experiments that our method outperforms previous ones in terms of the bit amount for the signs and computation cost. Our method, implemented in Python language, is available from https://github.com/ctsutake/dsr.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#24555;&#36895;&#20805;&#30005;&#30005;&#21160;&#36710;&#20013;&#24515;&#30340;&#21160;&#24577;&#23450;&#20215;&#31454;&#20105;&#12290;&#36890;&#36807;&#39044;&#27979;&#24615;&#36141;&#20080;&#30005;&#21147;&#38656;&#27714;&#21644;&#35774;&#23450;&#31454;&#20105;&#24615;&#20215;&#26684;&#31574;&#30053;&#65292;&#20805;&#30005;&#31449;&#21487;&#20197;&#22312;&#31454;&#20105;&#20013;&#36827;&#34892;&#26377;&#25928;&#23450;&#20215;&#12290;</title><link>http://arxiv.org/abs/2401.15108</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#31454;&#20105;&#20013;&#20026;&#24555;&#36895;&#20805;&#30005;&#30005;&#21160;&#36710;&#20013;&#24515;&#30340;&#21160;&#24577;&#23450;&#20215;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Deep Reinforcement Learning for Dynamic Pricing by Fast-charging Electric Vehicle Hubs in ccompetition. (arXiv:2401.15108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#24555;&#36895;&#20805;&#30005;&#30005;&#21160;&#36710;&#20013;&#24515;&#30340;&#21160;&#24577;&#23450;&#20215;&#31454;&#20105;&#12290;&#36890;&#36807;&#39044;&#27979;&#24615;&#36141;&#20080;&#30005;&#21147;&#38656;&#27714;&#21644;&#35774;&#23450;&#31454;&#20105;&#24615;&#20215;&#26684;&#31574;&#30053;&#65292;&#20805;&#30005;&#31449;&#21487;&#20197;&#22312;&#31454;&#20105;&#20013;&#36827;&#34892;&#26377;&#25928;&#23450;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#20805;&#30005;&#31449;&#23558;&#25104;&#20026;&#20840;&#29699;&#26032;&#24314;&#20132;&#36890;&#30005;&#27668;&#21270;&#22522;&#30784;&#35774;&#26045;&#30340;&#19968;&#37096;&#20998;&#12290;&#36825;&#20123;&#20805;&#30005;&#31449;&#23558;&#25215;&#36733;&#35768;&#22810;&#30452;&#27969;&#24555;&#36895;&#20805;&#30005;&#35774;&#22791;&#65292;&#20165;&#21487;&#20379;&#30005;&#21160;&#36710;&#36742;&#20805;&#30005;&#20351;&#29992;&#12290;&#31867;&#20284;&#20110;&#27773;&#27833;&#21152;&#27833;&#31449;&#65292;&#21516;&#19968;&#22320;&#21306;&#30340;&#24555;&#36895;&#20805;&#30005;&#31449;&#23558;&#26681;&#25454;&#31454;&#20105;&#35843;&#25972;&#20215;&#26684;&#20197;&#21560;&#24341;&#21516;&#19968;&#32676;&#30005;&#21160;&#36710;&#20027;&#12290;&#36825;&#20123;&#20805;&#30005;&#31449;&#23558;&#19982;&#30005;&#21147;&#32593;&#32476;&#36827;&#34892;&#20132;&#20114;&#65292;&#36890;&#36807;&#39044;&#27979;&#24615;&#36141;&#20080;&#22312;&#21069;&#19968;&#22825;&#30005;&#21147;&#24066;&#22330;&#19978;&#30340;&#30005;&#21147;&#38656;&#27714;&#65292;&#24182;&#22312;&#23454;&#26102;&#24066;&#22330;&#19978;&#28385;&#36275;&#24046;&#39069;&#38656;&#27714;&#12290;&#20805;&#30005;&#31449;&#21487;&#33021;&#37197;&#22791;&#34917;&#20805;&#30005;&#27744;&#20648;&#33021;&#31995;&#32479;&#29992;&#20110;&#22871;&#21033;&#12290;&#26412;&#25991;&#38024;&#23545;&#20805;&#30005;&#31449;&#31454;&#20105;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#27493;&#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#24577;&#23450;&#20215;&#26041;&#27861;&#12290;&#39318;&#20808;&#36890;&#36807;&#27714;&#35299;&#38543;&#26426;&#30340;&#21069;&#19968;&#22825;&#30005;&#21147;&#38656;&#27714;&#27169;&#22411;&#24471;&#21040;&#32435;&#20837;&#25215;&#35834;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#28216;&#25103;&#24314;&#27169;&#20026;&#31454;&#20105;&#26469;&#24471;&#21040;&#20805;&#30005;&#31449;&#30340;&#20215;&#26684;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast-charging hubs for electric vehicles will soon become part of the newly built infrastructure for transportation electrification across the world. These hubs are expected to host many DC fast-charging stations and will admit EVs only for charging. Like the gasoline refueling stations, fast-charging hubs in a neighborhood will dynamically vary their prices to compete for the same pool of EV owners. These hubs will interact with the electric power network by making purchase commitments for a significant part of their power needs in the day-ahead (DA) electricity market and meeting the difference from the real-time (RT) market. Hubs may have supplemental battery storage systems (BSS), which they will use for arbitrage. In this paper, we develop a two-step data-driven dynamic pricing methodology for hubs in price competition. We first obtain the DA commitment by solving a stochastic DA commitment model. Thereafter we obtain the hub pricing strategies by modeling the game as a competitiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#27963;&#21160;&#32500;&#24230;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20986;&#29616;&#39057;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09556</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#30340;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;: &#23398;&#20064;&#20943;&#23569;&#27169;&#22411;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Deep learning enhanced mixed integer optimization: Learning to reduce model dimensionality. (arXiv:2401.09556v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#27963;&#21160;&#32500;&#24230;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20986;&#29616;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#20013;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;(ANN)&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22312;&#36817;&#20284;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#26631;&#31614;&#20998;&#31867;&#26469;&#32771;&#34385;&#22810;&#20010;&#27963;&#21160;&#32500;&#24230;&#12290;&#20026;&#20102;&#25552;&#39640;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#20248;&#65292;&#20197;&#26368;&#22823;&#21270;&#26679;&#26412;&#32423;&#20934;&#30830;&#24615;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#22320;&#39044;&#27979;&#25152;&#26377;&#27963;&#21160;&#32500;&#24230;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20986;&#29616;&#39057;&#29575;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#25551;&#36848;&#20010;&#24615;&#21270;&#21307;&#23398;&#20379;&#24212;&#38142;&#20013;&#30340;&#38271;&#26399;&#25237;&#36164;&#35268;&#21010;&#21644;&#20013;&#26399;&#25112;&#26415;&#35268;&#21010;&#30340;&#22522;&#20110;&#27969;&#30340;&#35774;&#26045;&#20301;&#32622;&#20998;&#37197;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;(MILP)&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a framework to address the computational complexity inherent in Mixed-Integer Programming (MIP) models by harnessing the potential of deep learning. We compare the effectiveness of (a) feed-forward neural networks (ANN) and (b) convolutional neural networks (CNN) in approximating the active dimensions within MIP problems. We utilize multi-label classification to account for more than one active dimension. To enhance the framework's performance, we employ Bayesian optimization for hyperparameter tuning, aiming to maximize sample-level accuracy. The primary objective is to train the neural networks to predict all active dimensions accurately, thereby maximizing the occurrence of global optimum solutions. We apply this framework to a flow-based facility location allocation Mixed-Integer Linear Programming (MILP) formulation that describes long-term investment planning and medium-term tactical planning in a personalized medicine supply chain for cell therapy manufactur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#31163;&#25506;&#32034;&#21644;&#21033;&#29992;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#20048;&#35266;&#19982;&#24754;&#35266;&#28436;&#21592;&#30340;&#21452;&#28436;&#21592;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#24179;&#34913;&#21644;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2312.15965</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65306;&#25506;&#32034;&#19982;&#21033;&#29992;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcemen Learning with Decoupling Exploration and Utilization. (arXiv:2312.15965v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#31163;&#25506;&#32034;&#21644;&#21033;&#29992;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#20048;&#35266;&#19982;&#24754;&#35266;&#28436;&#21592;&#30340;&#21452;&#28436;&#21592;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#24179;&#34913;&#21644;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36807;&#20110;&#20445;&#23432;&#65292;&#23545;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#22788;&#29702;&#38480;&#21046;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#23548;&#33268;&#31639;&#27861;&#21482;&#36866;&#24212;&#26576;&#20010;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#27425;&#20248;&#35299;&#12290;&#21516;&#26679;&#65292;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20043;&#21069;&#30340;&#24809;&#32602;&#24615;&#24754;&#35266;&#20027;&#20041;&#20063;&#21093;&#22842;&#20102;&#27169;&#22411;&#30340;&#25506;&#32034;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20048;&#35266;&#19982;&#24754;&#35266;&#28436;&#21592;&#24378;&#21270;&#23398;&#20064;&#65288;OPARL&#65289;&#12290;OPARL&#37319;&#29992;&#29420;&#29305;&#30340;&#21452;&#28436;&#21592;&#26041;&#27861;&#65306;&#20048;&#35266;&#28436;&#21592;&#19987;&#27880;&#20110;&#25506;&#32034;&#65292;&#24754;&#35266;&#28436;&#21592;&#19987;&#27880;&#20110;&#21033;&#29992;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;&#25506;&#32034;&#21644;&#21033;&#29992;&#31574;&#30053;&#12290;&#36825;&#31181;&#32452;&#21512;&#22312;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#20419;&#36827;&#20102;&#26356;&#24179;&#34913;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24754;&#35266;&#30340;&#21033;&#29992;&#31574;&#30053;&#20248;&#21270;&#30528;&#37325;&#20110;&#20135;&#29983;&#39640;&#22870;&#21169;&#21160;&#20316;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network(DNN) generalization is limited by the over-reliance of current offline reinforcement learning techniques on conservative processing of existing datasets. This method frequently results in algorithms that settle for suboptimal solutions that only adjust to a certain dataset. Similarly, in online reinforcement learning, the previously imposed punitive pessimism also deprives the model of its exploratory potential. Our research proposes a novel framework, Optimistic and Pessimistic Actor Reinforcement Learning (OPARL). OPARL employs a unique dual-actor approach: an optimistic actor dedicated to exploration and a pessimistic actor focused on utilization, thereby effectively differentiating between exploration and utilization strategies. This unique combination in reinforcement learning methods fosters a more balanced and efficient approach. It enables the optimization of policies that focus on actions yielding high rewards through pessimistic utilization strategies, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26143;&#31995;&#30446;&#24405;&#36827;&#34892;&#22330;&#32423;&#21035;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#33021;&#22815;&#40065;&#26834;&#22320;&#25512;&#26029;&#20986;&#23431;&#23449;&#23398;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#35266;&#27979;&#21463;&#21040;&#30340;&#31995;&#32479;&#25928;&#24212;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.15234</link><description>&lt;p&gt;
&#20351;&#29992;&#26143;&#31995;&#30446;&#24405;&#36827;&#34892;&#22330;&#32423;&#21035;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#65306;&#31995;&#32479;&#25928;&#24212;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Field-level simulation-based inference with galaxy catalogs: the impact of systematic effects. (arXiv:2310.15234v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26143;&#31995;&#30446;&#24405;&#36827;&#34892;&#22330;&#32423;&#21035;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#33021;&#22815;&#40065;&#26834;&#22320;&#25512;&#26029;&#20986;&#23431;&#23449;&#23398;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#35266;&#27979;&#21463;&#21040;&#30340;&#31995;&#32479;&#25928;&#24212;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#31181;&#20174;&#26143;&#31995;&#32418;&#31227;&#35843;&#26597;&#20013;&#38480;&#21046;&#23431;&#23449;&#23398;&#21442;&#25968;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22330;&#32423;&#21035;&#30340;&#26080;&#20284;&#28982;&#25512;&#26029;&#65292;&#32780;&#19981;&#23545;&#23610;&#24230;&#36827;&#34892;&#21098;&#20999;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24503;&#26705;&#33922;&#31561;&#20154;&#65288;2023&#24180;&#65289;&#24320;&#21457;&#20102;&#33021;&#22815;&#20934;&#30830;&#25512;&#26029;&#20986;&#36890;&#36807;&#20165;&#21253;&#21547;&#26143;&#31995;&#20301;&#32622;&#21644;&#24452;&#21521;&#36895;&#24230;&#30340;&#30446;&#24405;&#26469;&#30830;&#23450;$\Omega_{\rm m}$&#20540;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#23545;&#22825;&#20307;&#29289;&#29702;&#21644;&#20122;&#32593;&#26684;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#35266;&#27979;&#21463;&#21040;&#35768;&#22810;&#25928;&#24212;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;1&#65289;&#25513;&#34109;&#25928;&#24212;&#65292;2&#65289;&#29305;&#24322;&#36895;&#24230;&#21644;&#24452;&#21521;&#36317;&#31163;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#21450;3&#65289;&#19981;&#21516;&#30340;&#26143;&#31995;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#35266;&#27979;&#21482;&#20801;&#35768;&#25105;&#20204;&#27979;&#37327;&#32418;&#31227;&#65292;&#32416;&#32544;&#20102;&#26143;&#31995;&#30340;&#24452;&#21521;&#20301;&#32622;&#21644;&#36895;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;CAMELS&#39033;&#30446;&#20013;&#19981;&#21516;&#20195;&#30721;&#36816;&#34892;&#30340;&#26368;&#26032;&#27700;&#21160;&#21147;&#23398;&#27169;&#25311;&#29983;&#25104;&#30340;&#26143;&#31995;&#30446;&#24405;&#26469;&#35757;&#32451;&#21644;&#27979;&#35797;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#25311;&#32771;&#34385;&#20102;&#36825;&#20123;&#35266;&#27979;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been recently shown that a powerful way to constrain cosmological parameters from galaxy redshift surveys is to train graph neural networks to perform field-level likelihood-free inference without imposing cuts on scale. In particular, de Santi et al. (2023) developed models that could accurately infer the value of $\Omega_{\rm m}$ from catalogs that only contain the positions and radial velocities of galaxies that are robust to uncertainties in astrophysics and subgrid models. However, observations are affected by many effects, including 1) masking, 2) uncertainties in peculiar velocities and radial distances, and 3) different galaxy selections. Moreover, observations only allow us to measure redshift, intertwining galaxies' radial positions and velocities. In this paper we train and test our models on galaxy catalogs, created from thousands of state-of-the-art hydrodynamic simulations run with different codes from the CAMELS project, that incorporate these observational effect
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Cosmos&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#24314;&#27169;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#21644;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#22330;&#26223;&#19978;&#30340;&#39640;&#24615;&#33021;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12690</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#19978;&#30340;&#32452;&#21512;&#24335;&#19990;&#30028;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic Grounding for Compositional World Models. (arXiv:2310.12690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Cosmos&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#24314;&#27169;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#21644;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#22330;&#26223;&#19978;&#30340;&#39640;&#24615;&#33021;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Cosmos&#65292;&#19968;&#20010;&#38024;&#23545;&#32452;&#21512;&#27867;&#21270;&#65288;CG&#65289;&#35774;&#35745;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#24314;&#27169;&#26694;&#26550;&#65292;&#21363;&#22312;&#36890;&#36807;&#24050;&#30693;&#30340;&#35270;&#35273;&#8220;&#21407;&#23376;&#8221;&#32452;&#21512;&#33719;&#24471;&#30340;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#22330;&#26223;&#19978;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;Cosmos&#30340;&#26680;&#24515;&#27934;&#23519;&#21147;&#26159;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#24037;&#20855;&#65306;&#65288;i&#65289;&#31070;&#32463;&#31526;&#21495;&#21270;&#22330;&#26223;&#32534;&#30721;&#65292;&#20351;&#29992;&#31070;&#32463;&#32534;&#30721;&#22120;&#35745;&#31639;&#27599;&#20010;&#22330;&#26223;&#20013;&#30340;&#23454;&#20307;&#30340;&#23454;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#25551;&#36848;&#23454;&#20307;&#23646;&#24615;&#30340;&#21487;&#32452;&#21512;&#31526;&#21495;&#21521;&#37327;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#31070;&#32463;&#31526;&#21495;&#21270;&#27880;&#24847;&#26426;&#21046;&#65292;&#23558;&#36825;&#20123;&#23454;&#20307;&#19982;&#23398;&#20064;&#21040;&#30340;&#20132;&#20114;&#35268;&#21017;&#32465;&#23450;&#36215;&#26469;&#12290;Cosmos&#26159;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#65307;&#27492;&#22806;&#65292;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#38656;&#35201;&#25163;&#21160;&#23558;&#34920;&#31034;&#26144;&#23556;&#20026;&#31526;&#21495;&#19981;&#21516;&#65292;&#23427;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#35745;&#31639;&#23454;&#20307;&#30340;&#31526;&#21495;&#23646;&#24615;&#12290;&#36890;&#36807;&#23545;&#24050;&#24314;&#31435;&#30340;blocks&#22330;&#26223;&#36827;&#34892;&#20004;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;CG&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;Cosmos&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Cosmos, a framework for object-centric world modeling that is designed for compositional generalization (CG), i.e., high performance on unseen input scenes obtained through the composition of known visual "atoms." The central insight behind Cosmos is the use of a novel form of neurosymbolic grounding. Specifically, the framework introduces two new tools: (i) neurosymbolic scene encodings, which represent each entity in a scene using a real vector computed using a neural encoder, as well as a vector of composable symbols describing attributes of the entity, and (ii) a neurosymbolic attention mechanism that binds these entities to learned rules of interaction. Cosmos is end-to-end differentiable; also, unlike traditional neurosymbolic methods that require representations to be manually mapped to symbols, it computes an entity's symbolic attributes using vision-language foundation models. Through an evaluation that considers two different forms of CG on an established blocks-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#31639;&#26415;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#20559;&#32622;&#20197;&#21450;Attention Bias Calibration&#65288;ABC&#65289;&#26469;&#23454;&#29616;&#23545;&#20110;&#38271;&#38271;&#24230;&#30340;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.11984</link><description>&lt;p&gt;
&#20174;&#25554;&#20540;&#21040;&#22806;&#25512;&#65306;&#31639;&#26415;Transformer&#30340;&#23436;&#25972;&#38271;&#24230;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers. (arXiv:2310.11984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#31639;&#26415;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#20559;&#32622;&#20197;&#21450;Attention Bias Calibration&#65288;ABC&#65289;&#26469;&#23454;&#29616;&#23545;&#20110;&#38271;&#38271;&#24230;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#25552;&#20986;&#20197;&#26469;&#65292;Transformer&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#31639;&#27861;&#20219;&#21153;&#20013;&#65292;&#38271;&#24230;&#27867;&#21270;&#20173;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#31639;&#26415;&#31639;&#27861;&#65288;&#22914;&#21152;&#27861;&#21644;&#20056;&#27861;&#65289;&#26041;&#38754;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#21644;&#27880;&#24847;&#21147;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23454;&#29616;&#26368;&#20339;&#38271;&#24230;&#27867;&#21270;&#30340;&#20960;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#30446;&#26631;&#25351;&#21521;&#20559;&#32622;&#26469;&#27867;&#21270;&#21040;&#38271;&#38271;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Attention Bias Calibration&#65288;ABC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26657;&#20934;&#38454;&#27573;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#36866;&#24403;&#30340;&#27880;&#24847;&#21147;&#20559;&#32622;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#26426;&#21046;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;ABC&#65292;Transformer&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#20123;&#31639;&#26415;&#20219;&#21153;&#19978;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#23436;&#32654;&#38271;&#24230;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its introduction, the transformer model has demonstrated outstanding performance across various tasks. However, there are still unresolved issues regarding length generalization, particularly in algorithmic tasks. In this paper, we investigate the inherent capabilities of transformer models in learning arithmetic algorithms, such as addition and multiplication. Through experiments and attention analysis, we identify a number of crucial factors for achieving optimal length generalization. We show that transformer models are able to generalize to long lengths with the help of targeted attention biasing. We then introduce Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn the proper attention biases, which we link to mechanisms in relative position encoding. We demonstrate that using ABC, the transformer model can achieve unprecedented perfect length generalization on certain arithmetic tasks.
&lt;/p&gt;</description></item><item><title>JEI-DNN&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#36864;&#20986;&#21644;&#25512;&#26029;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#36827;&#34892;&#37096;&#20998;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#27599;&#27425;&#25512;&#26029;&#25152;&#38656;&#22823;&#37327;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09163</link><description>&lt;p&gt;
&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23398;&#20064;&#36864;&#20986;&#21644;&#25512;&#26029;&#65306;JEI-DNN
&lt;/p&gt;
&lt;p&gt;
Jointly-Learned Exit and Inference for a Dynamic Neural Network : JEI-DNN. (arXiv:2310.09163v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09163
&lt;/p&gt;
&lt;p&gt;
JEI-DNN&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#36864;&#20986;&#21644;&#25512;&#26029;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#36827;&#34892;&#37096;&#20998;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#27599;&#27425;&#25512;&#26029;&#25152;&#38656;&#22823;&#37327;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#32467;&#21512;&#24494;&#35843;&#24050;&#25104;&#20026;&#20027;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#36890;&#24120;&#21463;&#21040;&#27599;&#27425;&#25512;&#26029;&#25152;&#38656;&#30340;&#24040;&#22823;&#36164;&#28304;&#30340;&#38480;&#21046;&#12290;&#26089;&#26399;&#36864;&#20986;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#65288;EDNN&#65289;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#20174;&#20013;&#38388;&#23618;&#36827;&#34892;&#37096;&#20998;&#39044;&#27979;&#65288;&#21363;&#26089;&#26399;&#36864;&#20986;&#65289;&#26469;&#32469;&#36807;&#36825;&#20010;&#38382;&#39064;&#12290;&#35757;&#32451;EDNN&#26550;&#26500;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#21253;&#25324;&#20004;&#20010;&#30456;&#20114;&#20132;&#32455;&#30340;&#32452;&#20214;&#65306;&#25511;&#21046;&#26089;&#26399;&#36864;&#20986;&#20915;&#31574;&#30340;&#38376;&#25511;&#26426;&#21046;&#65288;GM&#65289;&#21644;&#25191;&#34892;&#20013;&#38388;&#34920;&#31034;&#25512;&#26029;&#30340;&#20013;&#38388;&#25512;&#26029;&#27169;&#22359;&#65288;IMs&#65289;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#38376;&#25511;&#26426;&#21046;&#30340;&#38408;&#20540;&#32622;&#20449;&#24230;&#24230;&#37327;&#65292;&#24182;&#21162;&#21147;&#25913;&#36827;&#22522;&#26412;&#30340;&#39592;&#24178;&#32593;&#32476;&#21644;&#25512;&#26029;&#27169;&#22359;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#26377;&#20004;&#20010;&#22522;&#26412;&#32570;&#28857;&#65306;1&#65289;&#38376;&#25511;&#26426;&#21046;&#21644;&#20013;&#38388;&#25512;&#26029;&#27169;&#22359;&#19981;&#33021;&#20849;&#21516;&#23398;&#20064;&#21644;&#20248;&#21270;&#65292;2&#65289;GM&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;IMS&#21644;&#39592;&#24178;&#32593;&#32476;&#30340;&#36136;&#37327;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained models, coupled with fine-tuning, are slowly becoming established as the dominant architecture in machine learning. Even though these models offer impressive performance, their practical application is often limited by the prohibitive amount of resources required for every inference. Early-exiting dynamic neural networks (EDNN) circumvent this issue by allowing a model to make some of its predictions from intermediate layers (i.e., early-exit). Training an EDNN architecture is challenging as it consists of two intertwined components: the gating mechanism (GM) that controls early-exiting decisions and the intermediate inference modules (IMs) that perform inference from intermediate representations. As a result, most existing approaches rely on thresholding confidence metrics for the gating mechanism and strive to improve the underlying backbone network and the inference modules. Although successful, this approach has two fundamental shortcomings: 1) the GMs and the IMs 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24230;&#37327;&#24544;&#23454;&#24615;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#23631;&#34109;&#20196;&#29260;&#20316;&#20026;&#35774;&#35745;&#20351;&#20854;&#25104;&#20026;&#20998;&#24067;&#20869;&#65292;&#20197;&#35299;&#20915;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26102;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07819</link><description>&lt;p&gt;
&#21487;&#24230;&#37327;&#24544;&#23454;&#24615;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Faithfulness Measurable Masked Language Models. (arXiv:2310.07819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24230;&#37327;&#24544;&#23454;&#24615;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#23631;&#34109;&#20196;&#29260;&#20316;&#20026;&#35774;&#35745;&#20351;&#20854;&#25104;&#20026;&#20998;&#24067;&#20869;&#65292;&#20197;&#35299;&#20915;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26102;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#37325;&#35201;&#24615;&#24230;&#37327;&#26469;&#34920;&#36798;&#21738;&#20123;&#20196;&#29260;&#23545;&#20110;&#39044;&#27979;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#35299;&#37322;&#20855;&#26377;&#35828;&#26381;&#21147;&#65292;&#20294;&#24448;&#24448;&#26159;&#38169;&#35823;&#30340;&#12290;&#22240;&#27492;&#65292;&#27979;&#37327;&#23427;&#20204;&#30340;&#24544;&#23454;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#31181;&#24230;&#37327;&#26631;&#20934;&#26159;&#22914;&#26524;&#20196;&#29260;&#30830;&#23454;&#24456;&#37325;&#35201;&#65292;&#37027;&#20040;&#23631;&#34109;&#23427;&#20204;&#24212;&#35813;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#21464;&#24046;&#12290;&#28982;&#32780;&#65292;&#20196;&#29260;&#23631;&#34109;&#20250;&#24341;&#20837;&#21306;&#22495;&#22806;&#38382;&#39064;&#65292;&#32780;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#35745;&#31639;&#19978;&#24456;&#26114;&#36149;&#24182;&#19988;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20854;&#20182;&#25351;&#26631;&#30340;&#36866;&#29992;&#33539;&#22260;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22266;&#26377;&#30340;&#24544;&#23454;&#24615;&#21487;&#24230;&#37327;&#27169;&#22411;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#23558;&#23631;&#34109;&#20196;&#29260;&#20316;&#20026;&#35774;&#35745;&#20351;&#20854;&#25104;&#20026;&#20998;&#24067;&#20869;&#12290;&#36825;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#26041;&#27861;&#23436;&#20840;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach to explain NLP models, is to use importance measures that express which tokens are important for a prediction. Unfortunately, such explanations are often wrong despite being persuasive. Therefore, it is essential to measure their faithfulness. One such metric is if tokens are truly important, then masking them should result in worse model performance. However, token masking introduces out-of-distribution issues and existing solutions are computationally expensive and employ proxy-models. Furthermore, other metrics are very limited in scope. In this work, we propose an inherently faithfulness measurable model that addresses these challenges. This is achieved by using a novel fine-tuning method that incorporates masking, such that masking tokens become in-distribution by design. This differs from existing approaches, which are completely model-agnostic but are inapplicable in practice. We demonstrate the generality of our approach by applying it to various tasks and val
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12559</link><description>&lt;p&gt;
&#36890;&#36807;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#36827;&#34892;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Invariant Learning via Probability of Sufficient and Necessary Causes. (arXiv:2309.12559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37326;&#22806;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#26410;&#30693;&#30340;&#12289;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#27979;&#35797;&#20998;&#24067;&#65292;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26368;&#36817;&#20174;&#22240;&#26524;&#24615;&#24341;&#21457;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;OOD&#27867;&#21270;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#32780;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#23646;&#24615;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#19968;&#20010;&#24517;&#35201;&#20294;&#19981;&#20805;&#20998;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#23545;&#20110;&#20998;&#24067;&#36716;&#25442;&#26159;&#19981;&#21464;&#30340;&#65292;&#20294;&#21487;&#33021;&#27809;&#26377;&#25152;&#38656;&#30340;&#20934;&#30830;&#24230;&#12290;&#30456;&#21453;&#65292;&#19968;&#20010;&#20805;&#20998;&#20294;&#19981;&#24517;&#35201;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#20542;&#21521;&#20110;&#24456;&#22909;&#22320;&#36866;&#24212;&#29305;&#23450;&#25968;&#25454;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#25429;&#25417;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#32463;&#20856;&#27010;&#24565;&#8212;&#8212;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#65292;&#23427;&#25351;&#31034;&#20102;&#19968;&#20010;&#22240;&#32032;&#26159;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#30340;&#27010;&#29575;&#12290;&#20026;&#20102;&#23558;PNS&#19982;OOD&#27867;&#21270;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#32959;&#30244;&#20307;&#31215;&#30340;&#26080;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#26469;&#35299;&#20915;&#32959;&#30244;&#21306;&#22495;&#20307;&#31215;&#21464;&#21270;&#22833;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10153</link><description>&lt;p&gt;
&#20445;&#25345;&#32959;&#30244;&#20307;&#31215;&#30340;&#26080;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Preserving Tumor Volumes for Unsupervised Medical Image Registration. (arXiv:2309.10153v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10153
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#32959;&#30244;&#20307;&#31215;&#30340;&#26080;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#26469;&#35299;&#20915;&#32959;&#30244;&#21306;&#22495;&#20307;&#31215;&#21464;&#21270;&#22833;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#26159;&#20272;&#35745;&#22270;&#20687;&#38388;&#31354;&#38388;&#23545;&#24212;&#20851;&#31995;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#37117;&#20381;&#36182;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#29983;&#25104;&#21464;&#24418;&#22330;&#65292;&#36825;&#24448;&#24448;&#23548;&#33268;&#19981;&#30456;&#20284;&#21306;&#22495;&#30340;&#20307;&#31215;&#21464;&#21270;&#22833;&#34913;&#65292;&#29305;&#21035;&#26159;&#22312;&#32959;&#30244;&#21306;&#22495;&#12290;&#36825;&#20123;&#25913;&#21464;&#20250;&#26174;&#33879;&#25913;&#21464;&#32959;&#30244;&#22823;&#23567;&#21644;&#24213;&#23618;&#35299;&#21078;&#32467;&#26500;&#65292;&#38480;&#21046;&#20102;&#22270;&#20687;&#37197;&#20934;&#22312;&#20020;&#24202;&#35786;&#26029;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20197;&#32959;&#30244;&#20026;&#32422;&#26463;&#26465;&#20214;&#30340;&#22270;&#20687;&#37197;&#20934;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31574;&#30053;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#37197;&#20934;&#26469;&#36890;&#36807;&#20307;&#31215;&#21464;&#21270;&#35782;&#21035;&#28508;&#22312;&#30340;&#32959;&#30244;&#21306;&#22495;&#65292;&#29983;&#25104;&#30456;&#24212;&#30340;&#36719;&#32959;&#30244;&#25513;&#33180;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20445;&#25345;&#20307;&#31215;&#30340;&#37197;&#20934;&#26041;&#27861;&#65292;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#20307;&#31215;&#20445;&#25345;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image registration is a critical task that estimates the spatial correspondence between pairs of images. However, current traditional and deep-learning-based methods rely on similarity measures to generate a deforming field, which often results in disproportionate volume changes in dissimilar regions, especially in tumor regions. These changes can significantly alter the tumor size and underlying anatomy, which limits the practical use of image registration in clinical diagnosis. To address this issue, we have formulated image registration with tumors as a constraint problem that preserves tumor volumes while maximizing image similarity in other normal regions. Our proposed strategy involves a two-stage process. In the first stage, we use similarity-based registration to identify potential tumor regions by their volume change, generating a soft tumor mask accordingly. In the second stage, we propose a volume-preserving registration with a novel adaptive volume-preserving loss t
&lt;/p&gt;</description></item><item><title>&#27491;&#21017;&#34920;&#36798;&#24335;&#25512;&#29702;&#25361;&#25112;&#26159;&#19968;&#20010;&#20197;&#25214;&#21040;&#26368;&#23567;&#27491;&#21017;&#34920;&#36798;&#24335;&#20026;&#30446;&#26631;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#24212;&#29992;&#24191;&#27867;&#12289;&#38590;&#24230;&#21487;&#35843;&#12289;&#36866;&#29992;&#20110;&#20195;&#30721;/&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.07899</link><description>&lt;p&gt;
&#27491;&#21017;&#34920;&#36798;&#24335;&#25512;&#29702;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Regular Expression Inference Challenge. (arXiv:2308.07899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07899
&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#34920;&#36798;&#24335;&#25512;&#29702;&#25361;&#25112;&#26159;&#19968;&#20010;&#20197;&#25214;&#21040;&#26368;&#23567;&#27491;&#21017;&#34920;&#36798;&#24335;&#20026;&#30446;&#26631;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#24212;&#29992;&#24191;&#27867;&#12289;&#38590;&#24230;&#21487;&#35843;&#12289;&#36866;&#29992;&#20110;&#20195;&#30721;/&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#23558;&#27491;&#21017;&#34920;&#36798;&#24335;&#25512;&#29702;&#65288;REI&#65289;&#20316;&#20026;&#20195;&#30721;/&#35821;&#35328;&#24314;&#27169;&#20197;&#21450;&#26356;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#25361;&#25112;&#12290;REI&#26159;&#19968;&#20010;&#26377;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#31243;&#24207;&#21512;&#25104;&#20219;&#21153;&#65292;&#23427;&#25552;&#20986;&#20102;&#20174;&#31034;&#20363;&#20013;&#25214;&#21040;&#26368;&#23567;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#20004;&#20010;&#26377;&#38480;&#23383;&#31526;&#20018;&#38598;&#21512;P&#21644;N&#20197;&#21450;&#19968;&#20010;&#25104;&#26412;&#20989;&#25968;cost(&#183;)&#65292;&#20219;&#21153;&#26159;&#29983;&#25104;&#19968;&#20010;&#25509;&#21463;P&#20013;&#25152;&#26377;&#23383;&#31526;&#20018;&#24182;&#25298;&#32477;N&#20013;&#25152;&#26377;&#23383;&#31526;&#20018;&#30340;&#34920;&#36798;&#24335;r&#65292;&#32780;&#19981;&#23384;&#22312;&#20854;&#20182;&#34920;&#36798;&#24335;r'&#65292;&#20351;&#24471;cost(r')&lt;cost(r)&#12290;REI&#20316;&#20026;&#19968;&#20010;&#25361;&#25112;&#38382;&#39064;&#20855;&#26377;&#20197;&#19979;&#20248;&#21183;&#65306;&#65288;i&#65289;&#27491;&#21017;&#34920;&#36798;&#24335;&#26159;&#20247;&#25152;&#21608;&#30693;&#12289;&#24191;&#27867;&#20351;&#29992;&#30340;&#65292;&#26159;&#20195;&#30721;&#30340;&#33258;&#28982;&#29702;&#24819;&#21270;&#65307;&#65288;ii&#65289;REI&#30340;&#28176;&#36817;&#26368;&#22351;&#24773;&#20917;&#22797;&#26434;&#24615;&#24050;&#34987;&#20805;&#20998;&#29702;&#35299;&#65307;&#65288;iii&#65289;REI&#20855;&#26377;&#19968;&#23567;&#37096;&#20998;&#26131;&#20110;&#29702;&#35299;&#30340;&#21442;&#25968;&#65288;&#20363;&#22914;P&#25110;N&#30340;&#22522;&#25968;&#12289;&#31034;&#20363;&#30340;&#23383;&#31526;&#20018;&#38271;&#24230;&#25110;&#25104;&#26412;&#20989;&#25968;&#65289;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;REI&#30340;&#38590;&#24230;&#65307;&#65288;iv&#65289;&#23545;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;M&#27169;&#22411;&#32780;&#35328;&#65292;REI&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose \emph{regular expression inference (REI)} as a challenge for code/language modelling, and the wider machine learning community. REI is a supervised machine learning (ML) and program synthesis task, and poses the problem of finding minimal regular expressions from examples: Given two finite sets of strings $P$ and $N$ and a cost function $\text{cost}(\cdot)$, the task is to generate an expression $r$ that accepts all strings in $P$ and rejects all strings in $N$, while no other such expression $r'$ exists with $\text{cost}(r')&lt;\text{cost}(r)$.  REI has advantages as a challenge problem: (i) regular expressions are well-known, widely used, and a natural idealisation of code; (ii) REI's asymptotic worst-case complexity is well understood; (iii) REI has a small number of easy to understand parameters (e.g.~$P$ or $N$ cardinality, string lengths of examples, or the cost function); this lets us easily finetune REI-hardness; (iv) REI is an unsolved problem for deep learning based M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33021;&#21147;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01222</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#65306;&#26368;&#26032;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Calibration in Deep Learning: A Survey of the State-of-the-Art. (arXiv:2308.01222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33021;&#21147;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#21487;&#38752;&#12289;&#40065;&#26834;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#30340;&#26657;&#20934;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#20135;&#29983;&#19981;&#21487;&#38752;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#29702;&#24819;&#30340;&#28145;&#24230;&#27169;&#22411;&#19981;&#20165;&#24212;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#36824;&#24212;&#20855;&#26377;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#20351;&#29992;&#19981;&#21516;&#26426;&#21046;&#36827;&#34892;&#28145;&#24230;&#27169;&#22411;&#26657;&#20934;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#26032;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#25191;&#34892;&#27169;&#22411;&#26657;&#20934;&#30340;&#21407;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#26657;&#20934;&#30340;&#23450;&#20041;&#24320;&#22987;&#65292;&#35299;&#37322;&#20102;&#27169;&#22411;&#26657;&#20934;&#19981;&#20934;&#30830;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#20197;&#34913;&#37327;&#27169;&#22411;&#26657;&#20934;&#24615;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19968;&#20123;&#26657;&#20934;&#26041;&#27861;&#30340;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibrat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20313;&#24358;&#30456;&#20284;&#24230;&#20381;&#36182;&#30340;&#28201;&#24230;&#35843;&#25972;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#36827;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#20248;&#21270;&#20102;&#26679;&#26412;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2308.01140</link><description>&lt;p&gt;
DySTreSS: &#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#21160;&#24577;&#28201;&#24230;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning. (arXiv:2308.01140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20313;&#24358;&#30456;&#20284;&#24230;&#20381;&#36182;&#30340;&#28201;&#24230;&#35843;&#25972;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#36827;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#20248;&#21270;&#20102;&#26679;&#26412;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#31639;&#27861;&#20013;&#65292;&#35832;&#22914;SimCLR&#12289;MoCo&#31561;&#65292;&#24179;&#34913;&#20004;&#20010;&#35821;&#20041;&#30456;&#20284;&#26679;&#26412;&#20043;&#38388;&#30340;&#21560;&#24341;&#21147;&#21644;&#20004;&#20010;&#19981;&#21516;&#31867;&#21035;&#26679;&#26412;&#20043;&#38388;&#30340;&#25490;&#26021;&#21147;&#30340;&#20219;&#21153;&#20027;&#35201;&#21463;&#21040;&#30828;&#36127;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#24050;&#32463;&#35777;&#26126;InfoNCE&#25439;&#22833;&#21487;&#20197;&#26681;&#25454;&#22256;&#38590;&#31243;&#24230;&#26045;&#21152;&#24809;&#32602;&#65292;&#20294;&#28201;&#24230;&#36229;&#21442;&#25968;&#26159;&#35843;&#33410;&#24809;&#32602;&#21644;&#22343;&#21248;&#24615;&#19982;&#23481;&#24525;&#24230;&#20043;&#38388;&#26435;&#34913;&#30340;&#20851;&#38190;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30528;&#30524;&#20110;&#25913;&#36827;SSL&#20013;InfoNCE&#25439;&#22833;&#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#28201;&#24230;&#36229;&#21442;&#25968;&#20540;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20313;&#24358;&#30456;&#20284;&#24230;&#20381;&#36182;&#30340;&#28201;&#24230;&#35843;&#25972;&#20989;&#25968;&#65292;&#20197;&#26377;&#25928;&#20248;&#21270;&#29305;&#24449;&#31354;&#38388;&#20013;&#26679;&#26412;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#22343;&#21248;&#24615;&#21644;&#23481;&#24525;&#24230;&#24230;&#37327;&#65292;&#20197;&#30740;&#31350;&#20313;&#24358;&#30456;&#20284;&#24230;&#31354;&#38388;&#20013;&#26356;&#22909;&#20248;&#21270;&#30340;&#26368;&#20339;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#23616;&#37096;&#21644;&#20840;&#23616;&#34892;&#20026;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contemporary self-supervised contrastive algorithms like SimCLR, MoCo, etc., the task of balancing attraction between two semantically similar samples and repulsion between two samples from different classes is primarily affected by the presence of hard negative samples. While the InfoNCE loss has been shown to impose penalties based on hardness, the temperature hyper-parameter is the key to regulating the penalties and the trade-off between uniformity and tolerance. In this work, we focus our attention to improve the performance of InfoNCE loss in SSL by studying the effect of temperature hyper-parameter values. We propose a cosine similarity-dependent temperature scaling function to effectively optimize the distribution of the samples in the feature space. We further analyze the uniformity and tolerance metrics to investigate the optimal regions in the cosine similarity space for better optimization. Additionally, we offer a comprehensive examination of the behavior of local and g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#32467;&#26500;&#20013;&#65292;&#21487;&#20197;&#28789;&#27963;&#12289;&#39640;&#25928;&#22320;&#27169;&#25311;&#20855;&#26377;&#38750;&#24179;&#31283;&#30456;&#20851;&#24615;&#30340;&#26497;&#31471;&#20107;&#20214;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26102;&#38388;&#25928;&#29575;&#21644;&#24615;&#33021;&#19978;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#35768;&#22810;&#20855;&#26377;&#24179;&#31283;&#30456;&#20851;&#24615;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.08079</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#33258;&#21160; &#32534;&#30721;&#22120;&#23454;&#29616;&#28789;&#27963;&#39640;&#25928;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Flexible and efficient spatial extremes emulation via variational autoencoders. (arXiv:2307.08079v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#32467;&#26500;&#20013;&#65292;&#21487;&#20197;&#28789;&#27963;&#12289;&#39640;&#25928;&#22320;&#27169;&#25311;&#20855;&#26377;&#38750;&#24179;&#31283;&#30456;&#20851;&#24615;&#30340;&#26497;&#31471;&#20107;&#20214;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26102;&#38388;&#25928;&#29575;&#21644;&#24615;&#33021;&#19978;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#35768;&#22810;&#20855;&#26377;&#24179;&#31283;&#30456;&#20851;&#24615;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#36807;&#31243;&#20855;&#26377;&#22797;&#26434;&#30340;&#23614;&#20381;&#36182;&#32467;&#26500;&#65292;&#36825;&#31181;&#32467;&#26500;&#26080;&#27861;&#20351;&#29992;&#20256;&#32479;&#30340;&#39640;&#26031;&#36807;&#31243;&#26469;&#25551;&#36848;&#12290;&#26356;&#28789;&#27963;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292; &#22914;&#39640;&#26031;&#23610;&#24230;&#28151;&#21512;&#27169;&#22411;&#21644;&#21333;&#31449;&#28857;&#35843;&#33410;&#27169;&#22411;&#65292;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#26497;&#31471;&#20381;&#36182;&#24615;&#36136;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#25311;&#21512;&#21644;&#27169;&#25311;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292;&#20855;&#26377;&#28789;&#27963;&#21644;&#38750;&#24179;&#31283;&#30340;&#30456;&#20851;&#24615;&#23646;&#24615;&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120; (extVAE) &#30340;&#32534;&#30721;-&#35299;&#30721;&#32467;&#26500;&#20013;&#12290; extVAE &#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#26102;&#31354;&#27169;&#25311;&#22120;&#65292;&#23545;&#28508;&#22312;&#30340;&#26426;&#21046;&#27169;&#22411;&#36755;&#20986;&#29366;&#24577;&#30340;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20135;&#29983;&#20855;&#26377;&#19982;&#36755;&#20837;&#30456;&#21516;&#23646;&#24615;&#30340;&#36755;&#20986;&#65292;&#23588;&#20854;&#26159;&#22312;&#23614;&#37096;&#21306;&#22495;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;extVAE&#27604;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26356;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377; &#24179;&#31283;&#30456;&#20851;&#24615;&#32467;&#26500;&#30340;&#35768;&#22810;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#20013;&#34920;&#29616; &#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world processes have complex tail dependence structures that cannot be characterized using classical Gaussian processes. More flexible spatial extremes models such as Gaussian scale mixtures and single-station conditioning models exhibit appealing extremal dependence properties but are often exceedingly prohibitive to fit and simulate from. In this paper, we develop a new spatial extremes model that has flexible and non-stationary dependence properties, and we integrate it in the encoding-decoding structure of a variational autoencoder (extVAE). The extVAE can be used as a spatio-temporal emulator that characterizes the distribution of potential mechanistic model output states and produces outputs that have the same properties as the inputs, especially in the tail. Through extensive simulation studies, we show that our extVAE is vastly more time-efficient than traditional Bayesian inference while also outperforming many spatial extremes models with a stationary dependence str
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#29256;&#26435;&#20445;&#25252;&#25968;&#23383;&#27700;&#21360;&#26041;&#27861;DiffusionShield&#65292;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#20405;&#26435;&#65292;&#31616;&#21333;&#26131;&#23398;&#65292;&#38590;&#20197;&#26816;&#27979;&#65292;&#21487;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#34892;&#21508;&#19994;&#12290;</title><link>http://arxiv.org/abs/2306.04642</link><description>&lt;p&gt;
DiffusionShield&#65306;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#29256;&#26435;&#20445;&#25252;&#25968;&#23383;&#27700;&#21360;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models. (arXiv:2306.04642v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#29256;&#26435;&#20445;&#25252;&#25968;&#23383;&#27700;&#21360;&#26041;&#27861;DiffusionShield&#65292;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#20405;&#26435;&#65292;&#31616;&#21333;&#26131;&#23398;&#65292;&#38590;&#20197;&#26816;&#27979;&#65292;&#21487;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#34892;&#21508;&#19994;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;(GDMs)&#22312;&#23398;&#20064;&#21644;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#23637;&#31034;&#20102;&#20854;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#30340;GDMs&#31038;&#21306;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#24182;&#20419;&#36827;&#20102;GDMs&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26080;&#38480;&#21046;&#30340;&#25193;&#25955;&#24341;&#36215;&#20102;&#26377;&#20851;&#29256;&#26435;&#20445;&#25252;&#30340;&#20005;&#37325;&#20851;&#27880;&#12290;&#20363;&#22914;&#65292;&#33402;&#26415;&#23478;(&#21253;&#25324;&#30011;&#23478;&#21644;&#25668;&#24433;&#24072;)&#36234;&#26469;&#36234;&#25285;&#24515;GDMs&#21487;&#20197;&#27627;&#19981;&#36153;&#21147;&#22320;&#22797;&#21046;&#20182;&#20204;&#29420;&#29305;&#30340;&#21019;&#24847;&#20316;&#21697;&#32780;&#26080;&#38656;&#25480;&#26435;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;GDMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#27700;&#21360;&#26041;&#26696;&#8212;&#8212;DiffusionShield&#12290;&#36890;&#36807;&#23558;&#25152;&#26377;&#26435;&#20449;&#24687;&#32534;&#30721;&#25104;&#19968;&#20010;&#19981;&#21487;&#23519;&#35273;&#30340;&#27700;&#21360;&#24182;&#23558;&#20854;&#27880;&#20837;&#22270;&#20687;&#20013;&#65292;DiffusionShield&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;GDMs&#20405;&#26435;&#12290;&#23427;&#30340;&#27700;&#21360;&#21487;&#20197;&#34987;GDMs&#36731;&#26494;&#22320;&#23398;&#20064;&#24182;&#37325;&#29616;&#22312;&#23427;&#20204;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#12290;&#36890;&#36807;&#20174;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#26816;&#27979;&#27700;&#21360;&#65292;&#21487;&#20197;&#25581;&#38706;&#29256;&#26435;&#20405;&#26435;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Generative Diffusion Models (GDMs) have showcased their remarkable capabilities in learning and generating images. A large community of GDMs has naturally emerged, further promoting the diversified applications of GDMs in various fields. However, this unrestricted proliferation has raised serious concerns about copyright protection. For example, artists including painters and photographers are becoming increasingly concerned that GDMs could effortlessly replicate their unique creative works without authorization. In response to these challenges, we introduce a novel watermarking scheme, DiffusionShield, tailored for GDMs. DiffusionShield protects images from copyright infringement by GDMs through encoding the ownership information into an imperceptible watermark and injecting it into the images. Its watermark can be easily learned by GDMs and will be reproduced in their generated images. By detecting the watermark from generated images, copyright infringement can be exposed w
&lt;/p&gt;</description></item><item><title>Phylo2Vec&#26159;&#19968;&#31181;&#26032;&#30340;&#20108;&#21449;&#26641;&#31616;&#26126;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36731;&#26494;&#37319;&#26679;&#20108;&#21449;&#26641;&#65292;&#24182;&#20197;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#36941;&#21382;&#26641;&#31354;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#34507;&#30333;&#36136;&#31867;&#21035;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12693</link><description>&lt;p&gt;
Phylo2Vec: &#19968;&#31181;&#20108;&#21449;&#26641;&#30340;&#21521;&#37327;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Phylo2Vec: a vector representation for binary trees. (arXiv:2304.12693v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12693
&lt;/p&gt;
&lt;p&gt;
Phylo2Vec&#26159;&#19968;&#31181;&#26032;&#30340;&#20108;&#21449;&#26641;&#31616;&#26126;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36731;&#26494;&#37319;&#26679;&#20108;&#21449;&#26641;&#65292;&#24182;&#20197;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#36941;&#21382;&#26641;&#31354;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#34507;&#30333;&#36136;&#31867;&#21035;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29983;&#29289;&#25968;&#25454;&#25512;&#26029;&#24471;&#21040;&#30340;&#20108;&#21449;&#36827;&#21270;&#26641;&#23545;&#20110;&#29702;&#35299;&#29983;&#29289;&#20043;&#38388;&#20849;&#20139;&#30340;&#36827;&#21270;&#21382;&#21490;&#33267;&#20851;&#37325;&#35201;&#12290;&#26681;&#25454;&#26368;&#22823;&#20284;&#28982;&#31561;&#26576;&#20010;&#26368;&#20248;&#24615;&#20934;&#21017;&#25512;&#26029;&#20986;&#26641;&#20013;&#28508;&#22312;&#33410;&#28857;&#30340;&#20301;&#32622;&#26159;NP-hard&#38382;&#39064;&#65292;&#36825;&#25512;&#21160;&#20102;&#22823;&#37327;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#36890;&#24120;&#32570;&#20047;&#19968;&#31181;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#26469;&#22343;&#21248;&#37319;&#26679;&#38543;&#26426;&#26641;&#25110;&#26377;&#25928;&#22320;&#25506;&#32034;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#26641;&#31354;&#38388;&#65292;&#36825;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31561;&#20248;&#21270;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Phylo2Vec&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#31616;&#26126;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#36827;&#21270;&#26641;&#12290;Phylo2Vec&#23558;&#20219;&#20309;&#20855;&#26377;n&#20010;&#21494;&#23376;&#30340;&#20108;&#21449;&#26641;&#26144;&#23556;&#21040;&#38271;&#24230;&#20026;n&#30340;&#25972;&#25968;&#21521;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Phylo2Vec&#22312;&#31354;&#38388;&#20013;&#26082;&#26159;&#33391;&#23450;&#30340;&#21448;&#26159;&#21452;&#23556;&#30340;&#12290;Phylo2Vec&#30340;&#20248;&#28857;&#26159;&#65306;i&#65289;&#36731;&#26494;&#22343;&#21248;&#37319;&#26679;&#20108;&#21449;&#26641;&#65307;ii&#65289;&#20197;&#38750;&#24120;&#22823;&#25110;&#23567;&#30340;&#27493;&#38271;&#31995;&#32479;&#22320;&#36941;&#21382;&#26641;&#31354;&#38388;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#20351;&#29992;Phylo2Vec&#26500;&#24314;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20174;&#27688;&#22522;&#37240;&#24207;&#21015;&#39044;&#27979;&#34507;&#30333;&#36136;&#31867;&#21035;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Phylo2Vec&#26174;&#33879;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Binary phylogenetic trees inferred from biological data are central to understanding the shared evolutionary history of organisms. Inferring the placement of latent nodes in a tree by any optimality criterion (e.g., maximum likelihood) is an NP-hard problem, propelling the development of myriad heuristic approaches. Yet, these heuristics often lack a systematic means of uniformly sampling random trees or effectively exploring a tree space that grows factorially, which are crucial to optimisation problems such as machine learning. Accordingly, we present Phylo2Vec, a new parsimonious representation of a phylogenetic tree. Phylo2Vec maps any binary tree with $n$ leaves to an integer vector of length $n$. We prove that Phylo2Vec is both well-defined and bijective to the space of phylogenetic trees. The advantages of Phylo2Vec are twofold: i) easy uniform sampling of binary trees and ii) systematic ability to traverse tree space in very large or small jumps. As a proof of concept, we use P
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36816;&#34892;&#21464;&#21270;&#22312;&#23454;&#38469;&#20013;&#26356;&#23569;&#36935;&#21040;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#32479;&#35745;&#20551;&#35774;&#24182;&#35777;&#26126;&#26041;&#24046;&#20027;&#35201;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#39640;&#25935;&#24863;&#24615;&#25152;&#23548;&#33268;&#12290;</title><link>http://arxiv.org/abs/2304.01910</link><description>&lt;p&gt;
&#26657;&#20934;&#28151;&#20081;&#65306;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36816;&#34892;&#21464;&#21270;&#22312;&#26080;&#24847;&#20013;&#19988;&#26080;&#23475;
&lt;/p&gt;
&lt;p&gt;
Calibrated Chaos: Variance Between Runs of Neural Network Training is Harmless and Inevitable. (arXiv:2304.01910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01910
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36816;&#34892;&#21464;&#21270;&#22312;&#23454;&#38469;&#20013;&#26356;&#23569;&#36935;&#21040;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#32479;&#35745;&#20551;&#35774;&#24182;&#35777;&#26126;&#26041;&#24046;&#20027;&#35201;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#39640;&#25935;&#24863;&#24615;&#25152;&#23548;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#22312;&#37325;&#22797;&#27979;&#35797;&#26102;&#20250;&#26377;&#26174;&#33879;&#30340;&#27979;&#35797;&#38598;&#24615;&#33021;&#24046;&#24322;&#65292;&#24433;&#21709;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#27604;&#36739;&#21644;&#35757;&#32451;&#30340;&#21487;&#37325;&#22797;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#19979;&#27604;&#36739;&#26469;&#35299;&#37322;&#36825;&#31181;&#21464;&#21270;&#65306;&#65288;1&#65289;&#23613;&#31649;&#22312;&#27979;&#35797;&#38598;&#19978;&#26377;&#26174;&#33879;&#30340;&#26041;&#24046;&#65292;&#20294;&#26631;&#20934;&#30340;CIFAR-10&#19982;ImageNet&#35757;&#32451;&#22312;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#34920;&#29616;&#21364;&#38750;&#24120;&#19968;&#33268;&#65292;&#36825;&#34920;&#26126;&#26041;&#24046;&#19981;&#20687;&#20043;&#21069;&#24819;&#35937;&#30340;&#37027;&#20040;&#20005;&#37325;&#12290;&#65288;2&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#32479;&#35745;&#20551;&#35774;&#65292;&#20197;&#32039;&#23494;&#36817;&#20284;&#27979;&#35797;&#38598;&#20934;&#30830;&#24615;&#20998;&#24067;&#32467;&#26500;&#12290;&#65288;3&#65289;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#20197;&#19979;&#20004;&#20010;&#24847;&#20041;&#19978;&#65292;&#27979;&#35797;&#38598;&#26041;&#24046;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26041;&#24046;&#20027;&#35201;&#26159;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#39640;&#25935;&#24863;&#24615;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#38543;&#26426;&#28304;&#65288;&#22914;&#25968;&#25454;&#25490;&#24207;&#21644;&#25193;&#20805;&#65289;&#25152;&#23548;&#33268;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26041;&#24046;&#26159;&#39057;&#29575;&#26497;&#38480;&#30340;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#26469;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typical neural network trainings have substantial variance in test-set performance between repeated runs, impeding hyperparameter comparison and training reproducibility. We present the following results towards understanding this variation. (1) Despite having significant variance on their test-sets, we demonstrate that standard CIFAR-10 and ImageNet trainings have very little variance in their performance on the test-distributions from which those test-sets are sampled, suggesting that variance is less of a practical issue than previously thought. (2) We present a simplifying statistical assumption which closely approximates the structure of the test-set accuracy distribution. (3) We argue that test-set variance is inevitable in the following two senses. First, we show that variance is largely caused by high sensitivity of the training process to initial conditions, rather than by specific sources of randomness like the data order and augmentations. Second, we prove that variance is u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#35843;&#25972;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#20272;&#35745;&#30340;&#32479;&#35745;&#24615;&#33021;&#65292;&#20445;&#25345;&#26679;&#26412;&#22806;&#24615;&#33021;&#20445;&#35777;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.15579</link><description>&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#35843;&#25972;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning. (arXiv:2303.15579v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#35843;&#25972;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#20272;&#35745;&#30340;&#32479;&#35745;&#24615;&#33021;&#65292;&#20445;&#25345;&#26679;&#26412;&#22806;&#24615;&#33021;&#20445;&#35777;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#32479;&#35745;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#35843;&#25972;&#30340;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;&#8212;&#8212;&#22522;&#20110;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;&#65288;WDRO&#65289;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#12290;&#36825;&#31181;&#36716;&#25442;&#23558;&#25552;&#39640;WDRO&#30340;&#32479;&#35745;&#24615;&#33021;&#65292;&#22240;&#20026;&#35843;&#25972;&#21518;&#30340;WDRO&#20272;&#35745;&#22120;&#28176;&#36827;&#26080;&#20559;&#24182;&#19988;&#22343;&#26041;&#35823;&#24046;&#36235;&#36817;&#20110;&#38646;&#12290;&#35843;&#25972;&#21518;&#30340;WDRO&#19981;&#20250;&#21066;&#24369;WDRO&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35843;&#25972;WDRO&#20272;&#35745;&#22120;&#30340;&#23384;&#22312;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#32473;&#20986;&#20102;&#35745;&#31639;&#35843;&#25972;WDRO&#20272;&#35745;&#22120;&#30340;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#23637;&#31034;&#22914;&#20309;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#24320;&#21457;&#35843;&#25972;WDRO&#20272;&#35745;&#22120;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35843;&#25972;&#21518;&#30340;&#20272;&#35745;&#22120;&#27604;&#32463;&#20856;&#20272;&#35745;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an adjusted Wasserstein distributionally robust estimator -- based on a nonlinear transformation of the Wasserstein distributionally robust (WDRO) estimator in statistical learning. This transformation will improve the statistical performance of WDRO because the adjusted WDRO estimator is asymptotically unbiased and has an asymptotically smaller mean squared error. The adjusted WDRO will not mitigate the out-of-sample performance guarantee of WDRO. Sufficient conditions for the existence of the adjusted WDRO estimator are presented, and the procedure for the computation of the adjusted WDRO estimator is given. Specifically, we will show how the adjusted WDRO estimator is developed in the generalized linear model. Numerical experiments demonstrate the favorable practical performance of the adjusted estimator over the classic one.
&lt;/p&gt;</description></item></channel></rss>