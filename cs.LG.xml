<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20004;&#20010;&#21313;&#24180;&#30340;&#24222;&#22823;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#22522;&#20110;&#24615;&#21035;&#30340;&#34880;&#21387;&#21464;&#21270;&#19981;&#26174;&#33879;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#20551;&#35774;&#65307;&#21516;&#26102;&#65292;&#33298;&#24352;&#21387;&#38543;&#30528;&#24180;&#40836;&#22686;&#38271;&#32780;&#25345;&#32493;&#22686;&#21152;&#65292;&#32780;&#25910;&#32553;&#21387;&#22312;&#22235;&#21313;&#22810;&#23681;&#30340;&#24180;&#40836;&#32452;&#26174;&#31034;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#23792;&#20540;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01598</link><description>&lt;p&gt;
&#20174;&#20004;&#20010;&#21313;&#24180;&#30340;&#34880;&#21387;&#25968;&#25454;&#20013;&#23398;&#20064;&#65306;&#36328;&#36234;7500&#19975;&#24739;&#32773;&#23601;&#35786;&#30340;&#19981;&#21516;&#20154;&#32676;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Learning from Two Decades of Blood Pressure Data: Demography-Specific Patterns Across 75 Million Patient Encounters
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01598
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20004;&#20010;&#21313;&#24180;&#30340;&#24222;&#22823;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#22522;&#20110;&#24615;&#21035;&#30340;&#34880;&#21387;&#21464;&#21270;&#19981;&#26174;&#33879;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#20551;&#35774;&#65307;&#21516;&#26102;&#65292;&#33298;&#24352;&#21387;&#38543;&#30528;&#24180;&#40836;&#22686;&#38271;&#32780;&#25345;&#32493;&#22686;&#21152;&#65292;&#32780;&#25910;&#32553;&#21387;&#22312;&#22235;&#21313;&#22810;&#23681;&#30340;&#24180;&#40836;&#32452;&#26174;&#31034;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#23792;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#34880;&#21387;&#20173;&#28982;&#26159;&#20840;&#29699;&#20851;&#27880;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#20854;&#24739;&#30149;&#29575;&#19981;&#26029;&#19978;&#21319;&#65292;&#22240;&#27492;&#38656;&#35201;&#26377;&#25928;&#30340;&#30417;&#27979;&#21644;&#29702;&#35299;&#34880;&#21387;&#21160;&#24577;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#20174;&#34880;&#21387;&#27979;&#37327;&#20013;&#33719;&#24471;&#30340;&#22823;&#37327;&#20449;&#24687;&#65292;&#36825;&#26159;&#20102;&#35299;&#39640;&#34880;&#21387;&#36235;&#21183;&#30340;&#37325;&#35201;&#36884;&#24452;&#12290;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25253;&#36947;&#20102;&#34880;&#21387;&#21464;&#21270;&#19982;&#21508;&#31181;&#22240;&#32032;&#30340;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20221;&#28085;&#30422;&#20102;&#20004;&#20010;&#21313;&#24180;&#30340;7500&#19975;&#35760;&#24405;&#30340;&#24222;&#22823;&#25968;&#25454;&#38598;&#65292;&#20026;&#25506;&#32034;&#21644;&#20998;&#26512;&#19981;&#21516;&#20154;&#32676;&#29305;&#24449;&#65292;&#22914;&#24180;&#40836;&#12289;&#31181;&#26063;&#21644;&#24615;&#21035;&#20043;&#38388;&#30340;&#34880;&#21387;&#21464;&#21270;&#25552;&#20379;&#20102;&#29420;&#29305;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#24615;&#21035;&#30340;&#34880;&#21387;&#21464;&#21270;&#22312;&#32479;&#35745;&#19978;&#24182;&#19981;&#26174;&#33879;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#20551;&#35774;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#33298;&#24352;&#21387;&#65288;SBP&#65289;&#38543;&#30528;&#24180;&#40836;&#30340;&#22686;&#38271;&#32780;&#25345;&#32493;&#22686;&#21152;&#65292;&#32780;&#33298;&#24352;&#21387;&#65288;DBP&#65289;&#22312;&#22235;&#21313;&#22810;&#23681;&#30340;&#24180;&#40836;&#32452;&#26174;&#31034;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#23792;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#25581;&#31034;&#20102;&#20998;&#24067;&#27169;&#24335;&#20013;&#30340;&#19968;&#20123;&#26377;&#36259;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypertension remains a global health concern with a rising prevalence, necessitating effective monitoring and understanding of blood pressure (BP) dynamics. This study delves into the wealth of information derived from BP measurement, a crucial approach in informing our understanding of hypertensive trends. Numerous studies have reported on the relationship between BP variation and various factors. In this research, we leveraged an extensive dataset comprising 75 million records spanning two decades, offering a unique opportunity to explore and analyze BP variations across demographic features such as age, race, and gender. Our findings revealed that gender-based BP variation was not statistically significant, challenging conventional assumptions. Interestingly, systolic blood pressure (SBP) consistently increased with age, while diastolic blood pressure (DBP) displayed a distinctive peak in the forties age group. Moreover, our analysis uncovered intriguing similarities in the distribu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#20174;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#38750;&#21487;&#20998;&#35299;&#24615;&#33021;&#24230;&#37327;&#30340;&#22810;&#31867;&#23398;&#20064;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#20998;&#21035;&#36866;&#29992;&#20110;&#21333;&#35843;&#20984;&#24615;&#21644;&#32447;&#24615;&#27604;&#29575;&#20004;&#31867;&#24615;&#33021;&#24230;&#37327;&#65292;&#24182;&#22522;&#20110;&#31867;&#26465;&#20214;&#22122;&#22768;&#27169;&#22411;&#36827;&#34892;&#22122;&#22768;&#26657;&#27491;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01055</link><description>&lt;p&gt;
&#20174;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#38750;&#21487;&#20998;&#35299;&#24615;&#33021;&#24230;&#37327;&#30340;&#22810;&#31867;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multiclass Learning from Noisy Labels for Non-decomposable Performance Measures
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#20174;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#38750;&#21487;&#20998;&#35299;&#24615;&#33021;&#24230;&#37327;&#30340;&#22810;&#31867;&#23398;&#20064;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#20998;&#21035;&#36866;&#29992;&#20110;&#21333;&#35843;&#20984;&#24615;&#21644;&#32447;&#24615;&#27604;&#29575;&#20004;&#31867;&#24615;&#33021;&#24230;&#37327;&#65292;&#24182;&#22522;&#20110;&#31867;&#26465;&#20214;&#22122;&#22768;&#27169;&#22411;&#36827;&#34892;&#22122;&#22768;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23398;&#20064;&#20174;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#24471;&#21040;&#33391;&#22909;&#20998;&#31867;&#22120;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#20174;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#26631;&#20934;&#30340;&#22522;&#20110;&#25439;&#22833;&#30340;&#24615;&#33021;&#24230;&#37327;&#19978;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#38656;&#35201;&#20351;&#29992;&#38750;&#21487;&#20998;&#35299;&#24615;&#33021;&#24230;&#37327;&#65292;&#36825;&#20123;&#24230;&#37327;&#19981;&#33021;&#34920;&#31034;&#20026;&#21333;&#20010;&#31034;&#20363;&#19978;&#30340;&#25439;&#22833;&#30340;&#26399;&#26395;&#25110;&#24635;&#21644;&#65307;&#20854;&#20013;&#21253;&#25324;&#31867;&#19981;&#24179;&#34913;&#35774;&#32622;&#20013;&#30340;H-mean&#65292;Q-mean&#21644;G-mean&#65292;&#20197;&#21450;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;Micro F1&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20004;&#31867;&#24191;&#27867;&#30340;&#22810;&#31867;&#38750;&#21487;&#20998;&#35299;&#24615;&#33021;&#24230;&#37327;&#65292;&#21363;&#21333;&#35843;&#20984;&#24615;&#21644;&#32447;&#24615;&#27604;&#29575;&#65292;&#23427;&#20204;&#21253;&#25324;&#19978;&#36848;&#25152;&#26377;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;Narasimhan&#31561;&#20154;&#30340;Frank-Wolfe&#21644;Bisection&#31639;&#27861;(2015)&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#24191;&#27867;&#30740;&#31350;&#30340;&#31867;&#26465;&#20214;&#22122;&#22768;&#27169;&#22411;&#23478;&#26063;&#19979;&#24320;&#21457;&#20102;&#31639;&#27861;&#30340;&#22122;&#22768;&#26657;&#27491;&#29256;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36951;&#25022;(&#36229;&#39069;&#39118;&#38505;)&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been much interest in recent years in learning good classifiers from data with noisy labels. Most work on learning from noisy labels has focused on standard loss-based performance measures. However, many machine learning problems require using non-decomposable performance measures which cannot be expressed as the expectation or sum of a loss on individual examples; these include for example the H-mean, Q-mean and G-mean in class imbalance settings, and the Micro $F_1$ in information retrieval. In this paper, we design algorithms to learn from noisy labels for two broad classes of multiclass non-decomposable performance measures, namely, monotonic convex and ratio-of-linear, which encompass all the above examples. Our work builds on the Frank-Wolfe and Bisection based methods of Narasimhan et al. (2015). In both cases, we develop noise-corrected versions of the algorithms under the widely studied family of class-conditional noise models. We provide regret (excess risk) bounds 
&lt;/p&gt;</description></item><item><title>ILPO-Net&#26159;&#19968;&#31181;&#22788;&#29702;&#20219;&#24847;&#24418;&#29366;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#36816;&#31639;&#23545;&#23616;&#37096;&#31354;&#38388;&#27169;&#24335;&#26041;&#21521;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#22312;&#21508;&#31181;&#20307;&#31215;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.19612</link><description>&lt;p&gt;
ILPO-NET&#65306;&#29992;&#20110;&#19977;&#32500;&#20013;&#20219;&#24847;&#20307;&#31215;&#27169;&#24335;&#19981;&#21464;&#35782;&#21035;&#30340;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ILPO-NET: Network for the invariant recognition of arbitrary volumetric patterns in 3D
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19612
&lt;/p&gt;
&lt;p&gt;
ILPO-Net&#26159;&#19968;&#31181;&#22788;&#29702;&#20219;&#24847;&#24418;&#29366;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#36816;&#31639;&#23545;&#23616;&#37096;&#31354;&#38388;&#27169;&#24335;&#26041;&#21521;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#22312;&#21508;&#31181;&#20307;&#31215;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31354;&#38388;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#26377;&#25928;&#35782;&#21035;&#31354;&#38388;&#27169;&#24335;&#24182;&#23398;&#20064;&#20854;&#23618;&#27425;&#32467;&#26500;&#33267;&#20851;&#37325;&#35201;&#12290;&#20307;&#31215;&#25968;&#25454;&#24212;&#29992;&#23547;&#27714;&#30830;&#20445;&#23545;&#20301;&#31227;&#21644;&#27169;&#24335;&#26059;&#36716;&#22343;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#25216;&#26415;&#12290;ILPO-Net&#65288;Invariant to Local Patterns Orientation Network&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;Wigner&#30697;&#38453;&#23637;&#24320;&#65292;&#22312;&#21367;&#31215;&#25805;&#20316;&#20013;&#22788;&#29702;&#20219;&#24847;&#24418;&#29366;&#30340;&#27169;&#24335;&#65292;&#20174;&#32780;&#26412;&#36136;&#19978;&#23545;&#23616;&#37096;&#31354;&#38388;&#27169;&#24335;&#26041;&#21521;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#26080;&#32541;&#38598;&#25104;&#20102;&#26032;&#30340;&#21367;&#31215;&#36816;&#31639;&#31526;&#65292;&#22312;&#21508;&#31181;&#20307;&#31215;&#25968;&#25454;&#38598;&#65288;&#22914;MedMNIST&#21644;CATH&#65289;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#27604;&#22522;&#20934;&#32447;&#26356;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#20943;&#23569; - &#22312;MedMNIST&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#20102;&#39640;&#36798;1000&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19612v1 Announce Type: cross  Abstract: Effective recognition of spatial patterns and learning their hierarchy is crucial in modern spatial data analysis. Volumetric data applications seek techniques ensuring invariance not only to shifts but also to pattern rotations. While traditional methods can readily achieve translational invariance, rotational invariance possesses multiple challenges and remains an active area of research. Here, we present ILPO-Net (Invariant to Local Patterns Orientation Network), a novel approach that handles arbitrarily shaped patterns with the convolutional operation inherently invariant to local spatial pattern orientations using the Wigner matrix expansions. Our architecture seamlessly integrates the new convolution operator and, when benchmarked on diverse volumetric datasets such as MedMNIST and CATH, demonstrates superior performance over the baselines with significantly reduced parameter counts - up to 1000 times fewer in the case of MedMNIS
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35774;&#35745;&#20102;&#19968;&#20010;&#27010;&#29575;&#26080;&#30417;&#30563;&#27169;&#22411;&#65292;&#29992;&#20110;&#25512;&#26029;&#21307;&#38498;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#22810;&#20010;&#20219;&#24847;&#38271;&#24230;&#24207;&#21015;&#65292;&#21487;&#20197;&#25429;&#25417;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21407;&#22987;&#25968;&#25454;&#19978;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.19011</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#29575;&#27169;&#22411;&#39034;&#24207;&#25512;&#26029;&#21307;&#38498;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Sequential Inference of Hospitalization ElectronicHealth Records Using Probabilistic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19011
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35774;&#35745;&#20102;&#19968;&#20010;&#27010;&#29575;&#26080;&#30417;&#30563;&#27169;&#22411;&#65292;&#29992;&#20110;&#25512;&#26029;&#21307;&#38498;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#22810;&#20010;&#20219;&#24847;&#38271;&#24230;&#24207;&#21015;&#65292;&#21487;&#20197;&#25429;&#25417;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21407;&#22987;&#25968;&#25454;&#19978;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#30340;&#21307;&#38498;&#29615;&#22659;&#20013;&#65292;&#20915;&#31574;&#25903;&#25345;&#21487;&#20197;&#25104;&#20026;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#22312;&#36825;&#31181;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#22522;&#20110;&#25968;&#25454;&#30340;&#25512;&#26029;&#26410;&#26469;&#32467;&#26524;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38271;&#24207;&#21015;&#65288;&#22914;&#23454;&#39564;&#23460;&#26816;&#27979;&#21644;&#33647;&#29289;&#65289;&#32463;&#24120;&#26356;&#26032;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#21307;&#38498;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#20013;&#30340;&#22810;&#20010;&#20219;&#24847;&#38271;&#24230;&#24207;&#21015;&#30340;&#27010;&#29575;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#28508;&#22312;&#21464;&#37327;&#32467;&#26500;&#65292;&#25429;&#25417;&#20102;&#33647;&#29289;&#12289;&#35786;&#26029;&#12289;&#23454;&#39564;&#23460;&#26816;&#27979;&#12289;&#31070;&#32463;&#35780;&#20272;&#21644;&#33647;&#29289;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#23427;&#21487;&#20197;&#22312;&#21407;&#22987;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#25439;&#22833;&#36716;&#25442;&#25110;&#26102;&#38388;&#20998;&#31665;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19011v1 Announce Type: cross  Abstract: In the dynamic hospital setting, decision support can be a valuable tool for improving patient outcomes. Data-driven inference of future outcomes is challenging in this dynamic setting, where long sequences such as laboratory tests and medications are updated frequently. This is due in part to heterogeneity of data types and mixed-sequence types contained in variable length sequences. In this work we design a probabilistic unsupervised model for multiple arbitrary-length sequences contained in hospitalization Electronic Health Record (EHR) data. The model uses a latent variable structure and captures complex relationships between medications, diagnoses, laboratory tests, neurological assessments, and medications. It can be trained on original data, without requiring any lossy transformations or time binning. Inference algorithms are derived that use partial data to infer properties of the complete sequences, including their length and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#29702;&#35770;&#33719;&#21462;&#20989;&#25968;&#65292;&#29992;&#20110;&#24179;&#34913;&#22312;&#36830;&#32493;&#30340;&#20248;&#21270;&#20219;&#21153;&#20013;&#33719;&#24471;&#26368;&#20248;&#20540;&#25110;&#35299;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.09570</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20445;&#30495;&#24230;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21450;&#36328;&#20219;&#21153;&#21487;&#36716;&#31227;&#30340;&#26368;&#22823;&#20540;&#29109;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#29702;&#35770;&#33719;&#21462;&#20989;&#25968;&#65292;&#29992;&#20110;&#24179;&#34913;&#22312;&#36830;&#32493;&#30340;&#20248;&#21270;&#20219;&#21153;&#20013;&#33719;&#24471;&#26368;&#20248;&#20540;&#25110;&#35299;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#35774;&#35745;&#32773;&#38754;&#20020;&#19968;&#31995;&#21015;&#20248;&#21270;&#20219;&#21153;&#65292;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#26114;&#36149;&#35780;&#20272;&#30340;&#40657;&#30418;&#20989;&#25968;&#24418;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#33719;&#21462;&#20989;&#25968;&#65292;&#29992;&#20110;&#24179;&#34913;&#38656;&#35201;&#33719;&#21462;&#19981;&#21516;&#20219;&#21153;&#30340;&#26368;&#20248;&#20540;&#25110;&#35299;&#30340;&#20449;&#24687;&#21644;&#36890;&#36807;&#21442;&#25968;&#30340;&#36716;&#31227;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09570v1 Announce Type: new  Abstract: In many applications, ranging from logistics to engineering, a designer is faced with a sequence of optimization tasks for which the objectives are in the form of black-box functions that are costly to evaluate. For example, the designer may need to tune the hyperparameters of neural network models for different learning tasks over time. Rather than evaluating the objective function for each candidate solution, the designer may have access to approximations of the objective functions, for which higher-fidelity evaluations entail a larger cost. Existing multi-fidelity black-box optimization strategies select candidate solutions and fidelity levels with the goal of maximizing the information accrued about the optimal value or solution for the current task. Assuming that successive optimization tasks are related, this paper introduces a novel information-theoretic acquisition function that balances the need to acquire information about the 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;OpenFOAM&#21644;SmartSim&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#21487;&#20280;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#24320;&#21457;CFD+ML&#31639;&#27861;&#65292;&#36890;&#36807;SmartSim&#23558;OpenFOAM&#30340;&#19981;&#21516;&#37096;&#20998;&#26377;&#25928;&#22320;&#19982;ML&#32806;&#21512;&#65292;&#21253;&#25324;&#39044;&#22788;&#29702;/&#21518;&#22788;&#29702;&#24212;&#29992;&#31243;&#24207;&#12289;&#27714;&#35299;&#22120;&#12289;&#20989;&#25968;&#23545;&#35937;&#21644;&#32593;&#26684;&#36816;&#21160;&#27714;&#35299;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.16196</link><description>&lt;p&gt;
&#32467;&#21512; OpenFOAM &#21644; SmartSim &#30340;&#26426;&#22120;&#23398;&#20064;&#19982;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Combining Machine Learning with Computational Fluid Dynamics using OpenFOAM and SmartSim
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16196
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;OpenFOAM&#21644;SmartSim&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#21487;&#20280;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#24320;&#21457;CFD+ML&#31639;&#27861;&#65292;&#36890;&#36807;SmartSim&#23558;OpenFOAM&#30340;&#19981;&#21516;&#37096;&#20998;&#26377;&#25928;&#22320;&#19982;ML&#32806;&#21512;&#65292;&#21253;&#25324;&#39044;&#22788;&#29702;/&#21518;&#22788;&#29702;&#24212;&#29992;&#31243;&#24207;&#12289;&#27714;&#35299;&#22120;&#12289;&#20989;&#25968;&#23545;&#35937;&#21644;&#32593;&#26684;&#36816;&#21160;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#19982;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;CFD&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#25913;&#36827;&#25216;&#26415;&#21644;&#33258;&#28982;&#31995;&#32479;&#30340;&#27169;&#25311;&#25171;&#24320;&#20102;&#35768;&#22810;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;CFD+ML&#31639;&#27861;&#38656;&#35201;&#22312;&#24322;&#26500;&#30828;&#20214;&#19978;&#20132;&#25442;&#25968;&#25454;&#12289;&#21516;&#27493;&#21644;&#35745;&#31639;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#30340;&#23454;&#29616;&#24322;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#21487;&#20280;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#24320;&#28304;&#36719;&#20214;OpenFOAM&#21644;SmartSim&#24320;&#21457;CFD+ML&#31639;&#27861;&#12290;SmartSim&#25552;&#20379;&#20102;&#19968;&#20010;&#32534;&#25490;&#22120;&#65292;&#22823;&#22823;&#31616;&#21270;&#20102;&#32534;&#31243;CFD+ML&#31639;&#27861;&#30340;&#36807;&#31243;&#65292;&#20197;&#21450;&#19968;&#20010;Redis&#25968;&#25454;&#24211;&#65292;&#30830;&#20445;ML&#21644;CFD&#23458;&#25143;&#31471;&#20043;&#38388;&#39640;&#24230;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#20132;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;SmartSim&#23558;OpenFOAM&#30340;&#19981;&#21516;&#37096;&#20998;&#26377;&#25928;&#22320;&#19982;ML&#32806;&#21512;&#65292;&#21253;&#25324;&#39044;&#22788;&#29702;/&#21518;&#22788;&#29702;&#24212;&#29992;&#31243;&#24207;&#12289;&#27714;&#35299;&#22120;&#12289;&#20989;&#25968;&#23545;&#35937;&#21644;&#32593;&#26684;&#36816;&#21160;&#27714;&#35299;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;OpenFOAM&#23376;&#27169;&#22359;&#65292;&#20854;&#20013;&#21253;&#21547;&#21487;&#29992;&#20316;&#36215;&#22987;&#28857;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16196v1 Announce Type: new  Abstract: Combining machine learning (ML) with computational fluid dynamics (CFD) opens many possibilities for improving simulations of technical and natural systems. However, CFD+ML algorithms require exchange of data, synchronization, and calculation on heterogeneous hardware, making their implementation for large-scale problems exceptionally challenging.   We provide an effective and scalable solution to developing CFD+ML algorithms using open source software OpenFOAM and SmartSim. SmartSim provides an Orchestrator that significantly simplifies the programming of CFD+ML algorithms and a Redis database that ensures highly scalable data exchange between ML and CFD clients. We show how to leverage SmartSim to effectively couple different segments of OpenFOAM with ML, including pre/post-processing applications, solvers, function objects, and mesh motion solvers. We additionally provide an OpenFOAM sub-module with examples that can be used as starti
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#65292;&#38024;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.14081</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#36816;&#21160;&#20195;&#30721;&#30340;&#38543;&#26426;&#36807;&#31243;&#27169;&#22411;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Learning of Noisy Time Series Collections Using Stochastic Process Models with Motion Codes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14081
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#65292;&#38024;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#39044;&#27979;&#38382;&#39064;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20855;&#26377;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#38271;&#24230;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24773;&#20917;&#20173;&#20855;&#25361;&#25112;&#24615;&#12290;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#23454;&#20363;&#21487;&#20197;&#30475;&#20316;&#26159;&#22024;&#26434;&#21160;&#24577;&#27169;&#22411;&#30340;&#19968;&#20010;&#26679;&#26412;&#23454;&#29616;&#65292;&#20854;&#29305;&#28857;&#26159;&#36830;&#32493;&#38543;&#26426;&#36807;&#31243;&#12290;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#65292;&#25968;&#25454;&#26159;&#28151;&#21512;&#30340;&#65292;&#30001;&#22810;&#20010;&#38543;&#26426;&#36807;&#31243;&#24314;&#27169;&#30340;&#20960;&#31181;&#31867;&#22411;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#24207;&#21015;&#32452;&#25104;&#65292;&#20351;&#24471;&#39044;&#27979;&#21644;&#20998;&#31867;&#20219;&#21153;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#19981;&#26159;&#31616;&#21333;&#22320;&#23558;&#25968;&#25454;&#22238;&#24402;&#21040;&#27599;&#31181;&#26102;&#38388;&#24207;&#21015;&#31867;&#22411;&#65292;&#32780;&#26159;&#37319;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#20026;&#27599;&#31181;&#31867;&#22411;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#33258;&#21160;&#20998;&#37197;&#19968;&#20010;&#31216;&#20026;&#20854;&#36816;&#21160;&#20195;&#30721;&#30340;&#31614;&#21517;&#21521;&#37327;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#20998;&#37197;&#30340;&#36816;&#21160;&#20195;&#30721;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25512;&#26029;&#20986;&#30456;&#20851;&#24615;&#30340;&#31232;&#30095;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14081v1 Announce Type: cross  Abstract: While time series classification and forecasting problems have been extensively studied, the cases of noisy time series data with arbitrary time sequence lengths have remained challenging. Each time series instance can be thought of as a sample realization of a noisy dynamical model, which is characterized by a continuous stochastic process. For many applications, the data are mixed and consist of several types of noisy time series sequences modeled by multiple stochastic processes, making the forecasting and classification tasks even more challenging. Instead of regressing data naively and individually to each time series type, we take a latent variable model approach using a mixtured Gaussian processes with learned spectral kernels. More specifically, we auto-assign each type of noisy time series data a signature vector called its motion code. Then, conditioned on each assigned motion code, we infer a sparse approximation of the corr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#26032;&#25552;&#20986;&#30340;&#36873;&#25321;&#24615;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;Mamba&#20855;&#26377;&#19982;transformers&#31867;&#20284;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#23545;&#20110;&#28041;&#21450;&#36739;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;ICL&#20219;&#21153;&#65292;Mamba&#21487;&#20197;&#25104;&#20026;transformers&#30340;&#39640;&#25928;&#26367;&#20195;&#21697;&#12290;</title><link>https://arxiv.org/abs/2402.03170</link><description>&lt;p&gt;
Mamba&#33021;&#21542;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Mamba Capable of In-Context Learning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#26032;&#25552;&#20986;&#30340;&#36873;&#25321;&#24615;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;Mamba&#20855;&#26377;&#19982;transformers&#31867;&#20284;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#23545;&#20110;&#28041;&#21450;&#36739;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;ICL&#20219;&#21153;&#65292;Mamba&#21487;&#20197;&#25104;&#20026;transformers&#30340;&#39640;&#25928;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#32463;&#39564;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#26032;&#25552;&#20986;&#30340;&#36873;&#25321;&#24615;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;Mamba&#20855;&#26377;&#19982;transformers&#31867;&#20284;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#31616;&#21333;&#20989;&#25968;&#36924;&#36817;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#30340;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;Mamba&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20004;&#31867;&#20219;&#21153;&#20013;&#65292;Mamba&#22312;ICL&#26041;&#38754;&#30340;&#24615;&#33021;&#19982;transformer&#27169;&#22411;&#30456;&#21305;&#37197;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#65292;&#31867;&#20284;transformers&#65292;Mamba&#20284;&#20046;&#36890;&#36807;&#36880;&#27493;&#20248;&#21270;&#20854;&#20869;&#37096;&#34920;&#31034;&#26469;&#35299;&#20915;ICL&#38382;&#39064;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#28041;&#21450;&#36739;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;ICL&#20219;&#21153;&#65292;Mamba&#21487;&#20197;&#25104;&#20026;transformers&#30340;&#39640;&#25928;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides empirical evidence that Mamba, a newly proposed selective structured state space model, has similar in-context learning (ICL) capabilities as transformers. We evaluated Mamba on tasks involving simple function approximation as well as more complex natural language processing problems. Our results demonstrate that across both categories of tasks, Mamba matches the performance of transformer models for ICL. Further analysis reveals that like transformers, Mamba appears to solve ICL problems by incrementally optimizing its internal representations. Overall, our work suggests that Mamba can be an efficient alternative to transformers for ICL tasks involving longer input sequences.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#26524;&#26041;&#38754;&#30340;&#26368;&#26032;&#26041;&#27861;&#21644;&#36827;&#23637;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#19981;&#21516;&#21387;&#32553;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.01799</link><description>&lt;p&gt;
&#26356;&#24555;&#26356;&#36731;&#30340;LLMs&#65306;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#26524;&#26041;&#38754;&#30340;&#26368;&#26032;&#26041;&#27861;&#21644;&#36827;&#23637;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#19981;&#21516;&#21387;&#32553;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLMs&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#25512;&#29702;&#36807;&#31243;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#65292;&#23427;&#20204;&#30340;&#26222;&#21450;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#26368;&#36817;&#22312;&#27169;&#22411;&#21387;&#32553;&#21644;&#31995;&#32479;&#32423;&#20248;&#21270;&#26041;&#27861;&#26041;&#38754;&#30340;&#36827;&#23637;&#26088;&#22312;&#22686;&#24378;LLM&#25512;&#29702;&#25928;&#26524;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#27010;&#36848;&#65292;&#24378;&#35843;&#20102;&#26368;&#36817;&#30340;&#21457;&#23637;&#12290;&#36890;&#36807;&#23545;LLaMA(/2)-7B&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#21387;&#32553;&#25216;&#26415;&#65292;&#20026;&#22312;&#32479;&#19968;&#29615;&#22659;&#20013;&#39640;&#25928;&#37096;&#32626;LLM&#25552;&#20379;&#20102;&#23454;&#36341;&#35265;&#35299;&#12290;&#23545;LLaMA(/2)-7B&#30340;&#23454;&#35777;&#20998;&#26512;&#31361;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22522;&#20110;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#25913;&#21892;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#22312;https://github.com/nyunAI/Faster-LLM-Survey&#21457;&#24067;&#20102;&#29992;&#20110;&#22797;&#29616;&#26412;&#25991;&#32467;&#26524;&#30340;&#20195;&#30721;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive performance of LLMs, their widespread adoption faces challenges due to substantial computational and memory requirements during inference. Recent advancements in model compression and system-level optimization methods aim to enhance LLM inference. This survey offers an overview of these methods, emphasizing recent developments. Through experiments on LLaMA(/2)-7B, we evaluate various compression techniques, providing practical insights for efficient LLM deployment in a unified setting. The empirical analysis on LLaMA(/2)-7B highlights the effectiveness of these methods. Drawing from survey insights, we identify current limitations and discuss potential future directions to improve LLM inference efficiency. We release the codebase to reproduce the results presented in this paper at https://github.com/nyunAI/Faster-LLM-Survey
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#35760;&#24405;&#65292;&#25105;&#20204;&#30340;ReAlnet&#27169;&#22411;&#19982;&#20154;&#33041;&#27963;&#21160;&#30456;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20379;&#20102;&#26356;&#31867;&#20284;&#20154;&#31867;&#22823;&#33041;&#35270;&#35273;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.17231</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#31070;&#32463;&#34920;&#31034;&#23545;&#40784;&#23454;&#29616;&#26356;&#31867;&#20284;&#20154;&#31867;&#22823;&#33041;&#35270;&#35273;&#30340;ReAlnet&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ReAlnet: Achieving More Human Brain-Like Vision via Human Neural Representational Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17231
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#35760;&#24405;&#65292;&#25105;&#20204;&#30340;ReAlnet&#27169;&#22411;&#19982;&#20154;&#33041;&#27963;&#21160;&#30456;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20379;&#20102;&#26356;&#31867;&#20284;&#20154;&#31867;&#22823;&#33041;&#35270;&#35273;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#29289;&#20307;&#35782;&#21035;&#27169;&#22411;&#22312;&#27169;&#25311;&#20154;&#33041;&#35270;&#35273;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#21033;&#29992;&#31070;&#32463;&#25968;&#25454;&#26469;&#27169;&#20223;&#22823;&#33041;&#22788;&#29702;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#38750;&#20154;&#31867;&#23454;&#39564;&#23545;&#35937;&#30340;&#20405;&#20837;&#24615;&#31070;&#32463;&#35760;&#24405;&#65292;&#36825;&#22312;&#25105;&#20204;&#23545;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#21644;&#24320;&#21457;&#26356;&#31867;&#20284;&#20154;&#31867;&#22823;&#33041;&#35270;&#35273;&#27169;&#22411;&#30340;&#29702;&#35299;&#19978;&#23384;&#22312;&#30528;&#37325;&#35201;&#30340;&#32570;&#21475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#8220;Re(presentational)Al(ignment)net&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#35760;&#24405;&#20026;&#22522;&#30784;&#30340;&#19982;&#20154;&#33041;&#27963;&#21160;&#30456;&#23545;&#40784;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#19982;&#20154;&#33041;&#34920;&#31034;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#22270;&#20687;&#21040;&#33041;&#22810;&#23618;&#32534;&#30721;&#23545;&#40784;&#26694;&#26550;&#19981;&#20165;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#22810;&#20010;&#23618;&#27425;&#65292;&#26631;&#24535;&#30528;&#31070;&#32463;&#23545;&#40784;&#26041;&#38754;&#30340;&#37325;&#22823;&#31361;&#30772;&#65292;&#32780;&#19988;&#36824;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#21644;&#27169;&#20223;&#20154;&#33041;&#30340;&#35270;&#35273;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable strides made in artificial intelligence, current object recognition models still lag behind in emulating the mechanism of visual information processing in human brains. Recent studies have highlighted the potential of using neural data to mimic brain processing; however, these often reply on invasive neural recordings from non-human subjects, leaving a critical gap in our understanding of human visual perception and the development of more human brain-like vision models. Addressing this gap, we present, for the first time, "Re(presentational)Al(ignment)net", a vision model aligned with human brain activity based on non-invasive EEG recordings, demonstrating a significantly higher similarity to human brain representations. Our innovative image-to-brain multi-layer encoding alignment framework not only optimizes multiple layers of the model, marking a substantial leap in neural alignment, but also enables the model to efficiently learn and mimic human brain's visua
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#22270;&#20449;&#21495;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#38544;&#24335;&#21453;&#39304;&#25968;&#25454;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21019;&#26032;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#25193;&#25955;&#36807;&#31243;&#23548;&#33268;&#30340;&#20010;&#24615;&#21270;&#20449;&#24687;&#20002;&#22833;&#21644;&#22270;&#24418;&#32467;&#26500;&#19981;&#19968;&#33268;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.08744</link><description>&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#30340;&#22270;&#20449;&#21495;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Signal Diffusion Model for Collaborative Filtering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#22270;&#20449;&#21495;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#38544;&#24335;&#21453;&#39304;&#25968;&#25454;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21019;&#26032;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#25193;&#25955;&#36807;&#31243;&#23548;&#33268;&#30340;&#20010;&#24615;&#21270;&#20449;&#24687;&#20002;&#22833;&#21644;&#22270;&#24418;&#32467;&#26500;&#19981;&#19968;&#33268;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#33539;&#24335;&#26159;&#22522;&#20110;&#21382;&#21490;&#35266;&#23519;&#37325;&#24314;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#12290;&#36825;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#26368;&#36817;&#21457;&#23637;&#30340;&#25193;&#25955;&#27169;&#22411;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#32570;&#20047;&#23545;&#38544;&#24335;&#21453;&#39304;&#25968;&#25454;&#24314;&#27169;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#29305;&#21035;&#26159;&#65292;&#26631;&#20934;&#25193;&#25955;&#36807;&#31243;&#30340;&#21508;&#21521;&#21516;&#24615;&#29305;&#24615;&#26410;&#33021;&#32771;&#34385;&#29289;&#21697;&#20043;&#38388;&#30340;&#24322;&#36136;&#20381;&#36182;&#20851;&#31995;&#65292;&#23548;&#33268;&#19982;&#20132;&#20114;&#31354;&#38388;&#30340;&#22270;&#24418;&#32467;&#26500;&#19981;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#38543;&#26426;&#22122;&#22768;&#30772;&#22351;&#20102;&#20132;&#20114;&#21521;&#37327;&#20013;&#30340;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#23548;&#33268;&#21453;&#21521;&#37325;&#24314;&#22256;&#38590;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#25913;&#36827;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#22270;&#20449;&#21495;&#25193;&#25955;&#27169;&#22411;&#65288;&#31216;&#20026;GiffCF&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08744v2 Announce Type: replace-cross  Abstract: Collaborative filtering is a critical technique in recommender systems. Among various methods, an increasingly popular paradigm is to reconstruct user-item interactions based on the historical observations. This can be viewed as a conditional generative task, where recently developed diffusion model demonstrates great potential. However, existing studies on diffusion models lack effective solutions for modeling implicit feedback data. Particularly, the isotropic nature of the standard diffusion process fails to account for the heterogeneous dependencies among items, leading to a misalignment with the graphical structure of the interaction space. Meanwhile, random noise destroying personalized information in interaction vectors, causing difficulty in reverse reconstruction. In this paper, we make novel adaptions of diffusion model and propose Graph Signal Diffusion Model for Collaborative Filtering (named GiffCF). To better repr
&lt;/p&gt;</description></item><item><title>LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21487;&#33021;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#26356;&#38590;&#20197;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#21487;&#33021;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2309.13788</link><description>&lt;p&gt;
&#33021;&#22815;&#26816;&#27979;&#21040;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can LLM-Generated Misinformation Be Detected?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13788
&lt;/p&gt;
&lt;p&gt;
LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21487;&#33021;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#26356;&#38590;&#20197;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#21487;&#33021;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;LLMs&#65288;&#22914;ChatGPT&#65289;&#21487;&#33021;&#34987;&#21033;&#29992;&#26469;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#36825;&#32473;&#22312;&#32447;&#23433;&#20840;&#21644;&#20844;&#20247;&#20449;&#20219;&#24102;&#26469;&#20102;&#20005;&#37325;&#20851;&#20999;&#12290;&#19968;&#20010;&#22522;&#26412;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#65306;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#26159;&#21542;&#20250;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#36896;&#25104;&#26356;&#22823;&#21361;&#23475;?&#25105;&#20204;&#25552;&#20986;&#20174;&#26816;&#27979;&#38590;&#24230;&#30340;&#35282;&#24230;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21033;&#29992;LLMs&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#28508;&#22312;&#30495;&#23454;&#19990;&#30028;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#30340;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#30456;&#27604;&#65292;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#23545;&#20154;&#31867;&#21644;&#26816;&#27979;&#22120;&#26469;&#35828;&#26356;&#38590;&#26816;&#27979;&#65292;&#36825;&#34920;&#26126;&#23427;&#21487;&#33021;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#28508;&#22312;&#22320;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13788v3 Announce Type: replace-cross  Abstract: The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery
&lt;/p&gt;</description></item><item><title>LLMCheckup&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65292;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19982;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#65292;&#29983;&#25104;&#33258;&#25105;&#35299;&#37322;&#24182;&#25552;&#20379;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2401.12576</link><description>&lt;p&gt;
LLMCheckup&#65306;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#24335;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools. (arXiv:2401.12576v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12576
&lt;/p&gt;
&lt;p&gt;
LLMCheckup&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65292;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19982;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#65292;&#29983;&#25104;&#33258;&#25105;&#35299;&#37322;&#24182;&#25552;&#20379;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20197;&#23545;&#35805;&#24418;&#24335;&#36827;&#34892;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#24050;&#32463;&#35777;&#26126;&#22312;&#22686;&#24378;&#29992;&#25143;&#29702;&#35299;&#26041;&#38754;&#20855;&#26377;&#25928;&#26524;&#65292;&#22240;&#20026;&#19968;&#27425;&#24615;&#35299;&#37322;&#26377;&#26102;&#26080;&#27861;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#32473;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;&#23545;&#35805;&#30340;&#35299;&#37322;&#26041;&#26696;&#38656;&#35201;&#35768;&#22810;&#20381;&#36182;&#39033;&#65292;&#24182;&#19988;&#19981;&#23481;&#26131;&#36716;&#31227;&#21040;&#23427;&#20204;&#26410;&#35774;&#35745;&#30340;&#20219;&#21153;&#19978;&#12290;&#36890;&#36807;LLMCheckup&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#24037;&#20855;&#65292;&#20801;&#35768;&#29992;&#25143;&#19982;&#20219;&#20309;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#23545;&#35805;&#20197;&#20102;&#35299;&#20854;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;LLMs&#33021;&#22815;&#33258;&#34892;&#29983;&#25104;&#25152;&#26377;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#19982;&#19968;&#31995;&#21015;&#21487;&#35299;&#37322;&#24615;AI&#65288;XAI&#65289;&#24037;&#20855;&#65288;&#20363;&#22914;&#29305;&#24449;&#24402;&#22240;&#12289;&#22522;&#20110;&#23884;&#20837;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#21453;&#20107;&#23454;&#21644;&#22522;&#20110;&#29702;&#30001;&#29983;&#25104;&#30340;&#25552;&#31034;&#31574;&#30053;&#65289;&#36830;&#25509;&#65292;&#20197;&#23436;&#25104;&#24847;&#22270;&#35782;&#21035;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;LLM&#65288;&#33258;&#25105;&#65289;&#35299;&#37322;&#20197;&#20132;&#20114;&#23545;&#35805;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#25903;&#25345;&#21518;&#32493;&#38382;&#39064;&#21644;&#29983;&#25104;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability tools that offer explanations in the form of a dialogue have demonstrated their efficacy in enhancing users' understanding, as one-off explanations may occasionally fall short in providing sufficient information to the user. Current solutions for dialogue-based explanations, however, require many dependencies and are not easily transferable to tasks they were not designed for. With LLMCheckup, we present an easily accessible tool that allows users to chat with any state-of-the-art large language model (LLM) about its behavior. We enable LLMs to generate all explanations by themselves and take care of intent recognition without fine-tuning, by connecting them with a broad spectrum of Explainable AI (XAI) tools, e.g. feature attributions, embedding-based similarity, and prompting strategies for counterfactual and rationale generation. LLM (self-)explanations are presented as an interactive dialogue that supports follow-up questions and generates suggestions. LLMCheckup p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#22521;&#20859;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#26412;&#22320;&#21644;&#36328;&#39046;&#22495;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#23545;&#26410;&#30693;&#20998;&#24067;&#39046;&#22495;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20808;&#39564;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#20998;&#31163;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.17300</link><description>&lt;p&gt;
&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36890;&#36807;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#25913;&#21892;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Intrusion Detection with Domain-Invariant Representation Learning in Latent Space. (arXiv:2312.17300v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#22521;&#20859;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#26412;&#22320;&#21644;&#36328;&#39046;&#22495;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#23545;&#26410;&#30693;&#20998;&#24067;&#39046;&#22495;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20808;&#39564;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#20998;&#31163;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#32858;&#28966;&#20110;&#21033;&#29992;&#26469;&#33258;&#20855;&#26377;&#20016;&#23500;&#35757;&#32451;&#25968;&#25454;&#21644;&#26631;&#31614;&#30340;&#22810;&#20010;&#30456;&#20851;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#23545;&#26410;&#30693;&#20998;&#24067;&#65288;IN&#65289;&#21644;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#39046;&#22495;&#30340;&#25512;&#29702;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20174;&#36328;&#36234;&#22810;&#20010;&#39046;&#22495;&#30340;&#29305;&#24449;&#20013;&#22521;&#20859;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#65292;&#21253;&#25324;&#26412;&#22320;&#21644;&#36328;&#39046;&#22495;&#65292;&#20197;&#22686;&#24378;&#23545;IN&#21644;OOD&#39046;&#22495;&#30340;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#26368;&#23567;&#21270;&#20808;&#39564;&#19982;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#20998;&#31163;&#28508;&#22312;&#31354;&#38388;&#65292;&#26377;&#25928;&#28040;&#38500;&#34394;&#20551;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#32508;&#21512;&#32780;&#35328;&#65292;&#32852;&#21512;&#20248;&#21270;&#23558;&#20419;&#36827;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#20998;&#31867;&#25351;&#26631;&#35780;&#20272;&#27169;&#22411;&#22312;&#22810;&#20010;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#33021;&#65292;&#23545;&#27604;&#20102;&#29616;&#20195;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization focuses on leveraging knowledge from multiple related domains with ample training data and labels to enhance inference on unseen in-distribution (IN) and out-of-distribution (OOD) domains. In our study, we introduce a two-phase representation learning technique using multi-task learning. This approach aims to cultivate a latent space from features spanning multiple domains, encompassing both native and cross-domains, to amplify generalization to IN and OOD territories. Additionally, we attempt to disentangle the latent space by minimizing the mutual information between the prior and latent space, effectively de-correlating spurious feature correlations. Collectively, the joint optimization will facilitate domain-invariant feature learning. We assess the model's efficacy across multiple cybersecurity datasets, using standard classification metrics on both unseen IN and OOD sets, and juxtapose the results with contemporary domain generalization methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#22270;&#32467;&#21512;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2311.12399</link><description>&lt;p&gt;
&#22270;&#36935;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#36827;&#23637;&#19982;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Graph Meets Large Language Model: Progress and Future Directions. (arXiv:2311.12399v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.12399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#22270;&#32467;&#21512;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22312;&#34920;&#31034;&#21644;&#20998;&#26512;&#35832;&#22914;&#24341;&#29992;&#32593;&#32476;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#29983;&#29289;&#25968;&#25454;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#24182;&#19988;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22270;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;&#36229;&#36234;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#23558;LLMs&#19982;&#22270;&#32467;&#21512;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#26681;&#25454;LLMs&#22312;&#22270;&#30456;&#20851;&#20219;&#21153;&#20013;&#25198;&#28436;&#30340;&#35282;&#33394;(&#21363;&#22686;&#24378;&#22120;&#12289;&#39044;&#27979;&#22120;&#21644;&#23545;&#40784;&#32452;&#20214;)&#65292;&#23558;&#29616;&#26377;&#26041;&#27861;&#32452;&#32455;&#20026;&#19977;&#20010;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#20998;&#31867;&#27861;&#19977;&#20010;&#31867;&#21035;&#20013;&#30340;&#20195;&#34920;&#24615;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Recently, Large Language Models (LLMs), which have achieved tremendous success in various domains, have also been leveraged in graph-related tasks to surpass traditional Graph Neural Networks (GNNs) based methods and yield state-of-the-art performance. In this survey, we first present a comprehensive review and analysis of existing methods that integrate LLMs with graphs. First of all, we propose a new taxonomy, which organizes existing methods into three categories based on the role (i.e., enhancer, predictor, and alignment component) played by LLMs in graph-related tasks. Then we systematically survey the representative methods along the three categories of the taxonomy. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. The relevant papers are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#20248;&#20808;&#20307;&#39564;&#20013;&#32487;&#65292;&#20195;&#29702;&#20043;&#38388;&#20849;&#20139;&#26377;&#38480;&#25968;&#37327;&#30340;&#35757;&#32451;&#32463;&#39564;&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#22522;&#20934;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00865</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#20998;&#20139;&#20307;&#39564;&#25552;&#21319;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning. (arXiv:2311.00865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#20248;&#20808;&#20307;&#39564;&#20013;&#32487;&#65292;&#20195;&#29702;&#20043;&#38388;&#20849;&#20139;&#26377;&#38480;&#25968;&#37327;&#30340;&#35757;&#32451;&#32463;&#39564;&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#22522;&#20934;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#20248;&#20808;&#20307;&#39564;&#20013;&#32487;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#20998;&#20139;&#35757;&#32451;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#26377;&#38480;&#30340;&#36716;&#25442;&#19982;&#20854;&#20182;&#20195;&#29702;&#20849;&#20139;&#12290;&#20854;&#32972;&#21518;&#30340;&#30452;&#35273;&#26159;&#65292;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#30340;&#23569;&#37327;&#30456;&#20851;&#32463;&#39564;&#21487;&#20197;&#24110;&#21161;&#27599;&#20010;&#20195;&#29702;&#23398;&#20064;&#12290;&#19982;&#35768;&#22810;&#20854;&#20182;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#22522;&#26412;&#21435;&#20013;&#24515;&#21270;&#30340;&#35757;&#32451;&#65292;&#21482;&#38656;&#35201;&#20195;&#29702;&#20043;&#38388;&#30340;&#26377;&#38480;&#36890;&#20449;&#28192;&#36947;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#26080;&#20849;&#20139;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#21644;&#26368;&#20808;&#36827;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#20165;&#20998;&#20139;&#23569;&#37327;&#39640;&#24230;&#30456;&#20851;&#30340;&#32463;&#39564;&#20248;&#20110;&#20195;&#29702;&#20043;&#38388;&#30340;&#25152;&#26377;&#32463;&#39564;&#20849;&#20139;&#65292;&#32780;&#19988;&#36873;&#25321;&#24615;&#20307;&#39564;&#20849;&#20139;&#30340;&#24615;&#33021;&#25552;&#21319;&#22312;&#21508;&#31181;&#36229;&#21442;&#25968;&#21644;DQN&#21464;&#20307;&#20013;&#22343;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#21442;&#32771;&#23454;&#29616;&#21487;&#22312;https://github.com/mgerstgrasser/super&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel multi-agent RL approach, Selective Multi-Agent Prioritized Experience Relay, in which agents share with other agents a limited number of transitions they observe during training. The intuition behind this is that even a small number of relevant experiences from other agents could help each agent learn. Unlike many other multi-agent RL algorithms, this approach allows for largely decentralized training, requiring only a limited communication channel between agents. We show that our approach outperforms baseline no-sharing decentralized training and state-of-the art multi-agent RL algorithms. Further, sharing only a small number of highly relevant experiences outperforms sharing all experiences between agents, and the performance uplift from selective experience sharing is robust across a range of hyperparameters and DQN variants. A reference implementation of our algorithm is available at https://github.com/mgerstgrasser/super.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#36807;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#37327;&#36716;&#25442;&#20844;&#24335;&#26799;&#24230;&#30340;&#39640;&#25928;&#20272;&#35745;&#22120;&#65292;&#20811;&#26381;&#20102;&#24402;&#19968;&#21270;&#27969;&#35774;&#35745;&#22312;&#35299;&#26512;&#36870;&#21464;&#25442;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#36825;&#20351;&#24471;&#20219;&#20309;&#20445;&#25345;&#32500;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#37117;&#21487;&#20197;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#65292;&#24182;&#22312;&#20998;&#23376;&#29983;&#25104;&#21644;&#21453;&#38382;&#39064;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.16624</link><description>&lt;p&gt;
&#33258;&#30001;&#24418;&#24335;&#27969;&#21160;&#65306;&#20351;&#20219;&#20309;&#26550;&#26500;&#25104;&#20026;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Free-form Flows: Make Any Architecture a Normalizing Flow. (arXiv:2310.16624v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#36807;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#37327;&#36716;&#25442;&#20844;&#24335;&#26799;&#24230;&#30340;&#39640;&#25928;&#20272;&#35745;&#22120;&#65292;&#20811;&#26381;&#20102;&#24402;&#19968;&#21270;&#27969;&#35774;&#35745;&#22312;&#35299;&#26512;&#36870;&#21464;&#25442;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#36825;&#20351;&#24471;&#20219;&#20309;&#20445;&#25345;&#32500;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#37117;&#21487;&#20197;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#65292;&#24182;&#22312;&#20998;&#23376;&#29983;&#25104;&#21644;&#21453;&#38382;&#39064;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#27969;&#26159;&#30452;&#25509;&#26368;&#22823;&#21270;&#21487;&#33021;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#20197;&#21069;&#65292;&#24402;&#19968;&#21270;&#27969;&#30340;&#35774;&#35745;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#23545;&#35299;&#26512;&#36870;&#21464;&#25442;&#30340;&#38656;&#35201;&#38480;&#21046;&#12290;&#36890;&#36807;&#20351;&#29992;&#23545;&#21464;&#37327;&#36716;&#25442;&#20844;&#24335;&#30340;&#26799;&#24230;&#30340;&#39640;&#25928;&#20272;&#35745;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#36825;&#20351;&#24471;&#20219;&#20309;&#20445;&#25345;&#32500;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#37117;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#23558;&#37325;&#28857;&#25918;&#22312;&#31934;&#30830;&#35843;&#25972;&#24402;&#32435;&#20559;&#35265;&#20197;&#36866;&#24212;&#25163;&#22836;&#30340;&#20219;&#21153;&#19978;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#20998;&#23376;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21033;&#29992;$E(n)$-&#31561;&#21464;&#32593;&#32476;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#21453;&#38382;&#39064;&#22522;&#20934;&#27979;&#35797;&#20013;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#37319;&#29992;&#29616;&#25104;&#30340;ResNet&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing Flows are generative models that directly maximize the likelihood. Previously, the design of normalizing flows was largely constrained by the need for analytical invertibility. We overcome this constraint by a training procedure that uses an efficient estimator for the gradient of the change of variables formula. This enables any dimension-preserving neural network to serve as a generative model through maximum likelihood training. Our approach allows placing the emphasis on tailoring inductive biases precisely to the task at hand. Specifically, we achieve excellent results in molecule generation benchmarks utilizing $E(n)$-equivariant networks. Moreover, our method is competitive in an inverse problem benchmark, while employing off-the-shelf ResNet architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#21270;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#65288;NS-POSGs&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#34701;&#21512;&#24863;&#30693;&#26426;&#21046;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#24207;&#21015;&#20915;&#31574;&#20013;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#21482;&#26377;&#37096;&#20998;&#35266;&#27979;&#20449;&#24687;&#30340;&#26234;&#33021;&#20307;&#21644;&#19968;&#31181;&#23436;&#20840;&#35266;&#27979;&#30340;&#26234;&#33021;&#20307;&#30340;&#21333;&#26041;&#38754;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#35745;&#31639;NS-POSGs&#20540;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11566</link><description>&lt;p&gt;
&#20855;&#26377;&#31070;&#32463;&#24863;&#30693;&#26426;&#21046;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Stochastic Games with Neural Perception Mechanisms. (arXiv:2310.11566v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#21270;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#65288;NS-POSGs&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#34701;&#21512;&#24863;&#30693;&#26426;&#21046;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#24207;&#21015;&#20915;&#31574;&#20013;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#21482;&#26377;&#37096;&#20998;&#35266;&#27979;&#20449;&#24687;&#30340;&#26234;&#33021;&#20307;&#21644;&#19968;&#31181;&#23436;&#20840;&#35266;&#27979;&#30340;&#26234;&#33021;&#20307;&#30340;&#21333;&#26041;&#38754;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#35745;&#31639;NS-POSGs&#20540;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21338;&#24328;&#26159;&#19968;&#20010;&#20026;&#22810;&#26234;&#33021;&#20307;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#24207;&#21015;&#20915;&#31574;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#22312;&#29616;&#23454;&#20013;&#65292;&#26234;&#33021;&#20307;&#23545;&#29615;&#22659;&#21482;&#26377;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#36825;&#20351;&#24471;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#21333;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#26234;&#33021;&#20307;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#22312;&#36830;&#32493;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#24863;&#30693;&#29615;&#22659;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#21270;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#65288;NS-POSGs&#65289;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#36830;&#32493;&#31354;&#38388;&#24182;&#21457;&#38543;&#26426;&#21338;&#24328;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#26126;&#30830;&#22320;&#34701;&#20837;&#20102;&#24863;&#30693;&#26426;&#21046;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21333;&#26041;&#38754;&#30340;&#35774;&#32622;&#65292;&#21253;&#21547;&#20102;&#19968;&#20010;&#20855;&#26377;&#31163;&#25955;&#12289;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#35266;&#27979;&#21644;&#19968;&#20010;&#20855;&#26377;&#36830;&#32493;&#35266;&#27979;&#30340;&#20805;&#20998;&#20102;&#35299;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#36793;NS-HSVI&#30340;&#22522;&#20110;&#28857;&#30340;&#26041;&#27861;&#65292;&#29992;&#26469;&#36817;&#20284;&#35745;&#31639;&#21333;&#26041;&#38754;NS-POSGs&#30340;&#20540;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic games are a well established model for multi-agent sequential decision making under uncertainty. In reality, though, agents have only partial observability of their environment, which makes the problem computationally challenging, even in the single-agent setting of partially observable Markov decision processes. Furthermore, in practice, agents increasingly perceive their environment using data-driven approaches such as neural networks trained on continuous data. To tackle this problem, we propose the model of neuro-symbolic partially-observable stochastic games (NS-POSGs), a variant of continuous-space concurrent stochastic games that explicitly incorporates perception mechanisms. We focus on a one-sided setting, comprising a partially-informed agent with discrete, data-driven observations and a fully-informed agent with continuous observations. We present a new point-based method, called one-sided NS-HSVI, for approximating values of one-sided NS-POSGs and implement it ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#26041;&#27861;&#65292;&#20998;&#21035;&#36890;&#36807;&#26367;&#25442;Lai&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.03722</link><description>&lt;p&gt;
&#26410;&#30693;&#26041;&#24046;&#19979;&#30340;&#39640;&#26031;&#22343;&#20540;&#30340;&#20219;&#24847;&#26377;&#25928;T&#26816;&#39564;&#21644;&#32622;&#20449;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance. (arXiv:2310.03722v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#26041;&#27861;&#65292;&#20998;&#21035;&#36890;&#36807;&#26367;&#25442;Lai&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;1976&#24180;&#65292;Lai&#26500;&#36896;&#20102;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#22343;&#20540;$\mu$&#30340;&#39640;&#26031;&#20998;&#24067;&#30340;&#32622;&#20449;&#24207;&#21015;&#65292;&#35813;&#20998;&#24067;&#30340;&#26041;&#24046;$\sigma$&#26159;&#26410;&#30693;&#30340;&#12290;&#20182;&#20351;&#29992;&#20102;&#20851;&#20110;$\sigma$&#30340;&#19981;&#36866;&#24403;&#65288;&#21491;Haar&#65289;&#28151;&#21512;&#21644;&#20851;&#20110;$\mu$&#30340;&#19981;&#36866;&#24403;&#65288;&#24179;&#22374;&#65289;&#28151;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#20182;&#26500;&#24314;&#30340;&#32454;&#33410;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#24191;&#20041;&#30340;&#19981;&#21487;&#31215;&#20998;&#38789;&#21644;&#25193;&#23637;&#30340;&#32500;&#23572;&#19981;&#31561;&#24335;&#12290;&#23613;&#31649;&#36825;&#30830;&#23454;&#20135;&#29983;&#20102;&#19968;&#20010;&#39034;&#24207;T&#26816;&#39564;&#65292;&#20294;&#30001;&#20110;&#20182;&#30340;&#38789;&#19981;&#21487;&#31215;&#20998;&#65292;&#23427;&#24182;&#27809;&#26377;&#20135;&#29983;&#19968;&#20010;&#8220;e-process&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#30456;&#21516;&#30340;&#35774;&#32622;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#65306;&#19968;&#20010;&#26159;&#22312;&#32553;&#20943;&#28388;&#27874;&#22120;&#20013;&#30340;&#27979;&#35797;&#38789;&#65292;&#21478;&#19968;&#20010;&#26159;&#22312;&#35268;&#33539;&#25968;&#25454;&#28388;&#27874;&#22120;&#20013;&#30340;&#8220;e-process&#8221;&#12290;&#36825;&#20123;&#20998;&#21035;&#26159;&#36890;&#36807;&#23558;Lai&#30340;&#24179;&#22374;&#28151;&#21512;&#26367;&#25442;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#24182;&#23558;&#23545;$\sigma$&#30340;&#21491;Haar&#28151;&#21512;&#26367;&#25442;&#20026;&#22312;&#38646;&#31354;&#38388;&#19979;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#23601;&#20687;&#22312;&#36890;&#29992;&#25512;&#26029;&#20013;&#19968;&#26679;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 1976, Lai constructed a nontrivial confidence sequence for the mean $\mu$ of a Gaussian distribution with unknown variance $\sigma$. Curiously, he employed both an improper (right Haar) mixture over $\sigma$ and an improper (flat) mixture over $\mu$. Here, we elaborate carefully on the details of his construction, which use generalized nonintegrable martingales and an extended Ville's inequality. While this does yield a sequential t-test, it does not yield an ``e-process'' (due to the nonintegrability of his martingale). In this paper, we develop two new e-processes and confidence sequences for the same setting: one is a test martingale in a reduced filtration, while the other is an e-process in the canonical data filtration. These are respectively obtained by swapping Lai's flat mixture for a Gaussian mixture, and swapping the right Haar mixture over $\sigma$ with the maximum likelihood estimate under the null, as done in universal inference. We also analyze the width of resulting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#20250;&#23548;&#33268;&#23545;&#26550;&#26500;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#32780;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23558;Transformers&#19982;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#21040;&#24456;&#23567;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#30340;&#24615;&#33021;&#19982;S4&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#22312;PathX-256&#20219;&#21153;&#19978;&#25913;&#36827;&#20102;SSMs&#30340;&#26368;&#20339;&#32467;&#26524;20&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02980</link><description>&lt;p&gt;
&#27704;&#36828;&#19981;&#35201;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65306;&#20844;&#27491;&#27604;&#36739;&#38271;&#24207;&#21015;&#27169;&#22411;&#38656;&#35201;&#25968;&#25454;&#39537;&#21160;&#30340;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors. (arXiv:2310.02980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#20250;&#23548;&#33268;&#23545;&#26550;&#26500;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#32780;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23558;Transformers&#19982;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#21040;&#24456;&#23567;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#30340;&#24615;&#33021;&#19982;S4&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#22312;PathX-256&#20219;&#21153;&#19978;&#25913;&#36827;&#20102;SSMs&#30340;&#26368;&#20339;&#32467;&#26524;20&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30446;&#26631;&#65292;&#24182;&#23548;&#33268;&#20102;&#19968;&#20123;&#26550;&#26500;&#65292;&#22914;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#27604;Transformers&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#24615;&#36827;&#23637;&#20027;&#35201;&#26159;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#24182;&#36890;&#36807;&#39044;&#27979;&#36755;&#20837;&#24207;&#21015;&#30340;&#30446;&#26631;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#30340;&#22522;&#20934;&#27979;&#35797;&#65288;&#20363;&#22914;Long Range Arena&#65289;&#19978;&#23637;&#31034;&#20986;&#26469;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#26426;&#21021;&#22987;&#21270;&#23548;&#33268;&#23545;&#26550;&#26500;&#20043;&#38388;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#24182;&#19988;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#65288;&#20165;&#20351;&#29992;&#19979;&#28216;&#20219;&#21153;&#25968;&#25454;&#65289;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;Transformers&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#20043;&#38388;&#24471;&#21040;&#24456;&#23567;&#30340;&#24046;&#36317;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#19982;S4&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#25105;&#20204;&#22312;PathX-256&#20219;&#21153;&#19978;&#23558;SSMs&#30340;&#26368;&#20339;&#25253;&#21578;&#32467;&#26524;&#25552;&#39640;&#20102;20&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using $\textit{only the downstream task data}$, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P-ROCKET&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#36827;&#34892;&#21098;&#26525;&#12290;</title><link>http://arxiv.org/abs/2309.08499</link><description>&lt;p&gt;
P-ROCKET: &#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
P-ROCKET: Pruning Random Convolution Kernels for Time Series Classification. (arXiv:2309.08499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P-ROCKET&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#36827;&#34892;&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;ROCKET&#21644;MINIROCKET&#22240;&#20854;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;ROCKET&#21644;MINIROCKET&#21033;&#29992;&#26080;&#38656;&#35757;&#32451;&#30340;&#38543;&#26426;&#19968;&#32500;&#21367;&#31215;&#26680;&#65292;&#21487;&#20197;&#24555;&#36895;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#39640;&#25928;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20840;&#38754;&#25429;&#25417;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#26469;&#35828;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36827;&#21270;&#31639;&#27861;S-ROCKET&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#21098;&#26525;&#20887;&#20313;&#30340;&#21367;&#31215;&#26680;&#12290;&#28982;&#32780;&#65292;&#36827;&#21270;&#31639;&#27861;&#26412;&#36523;&#30340;&#29305;&#24615;&#23548;&#33268;&#22312;S-ROCKET&#20013;&#35780;&#20272;&#21367;&#31215;&#26680;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#20013;&#65292;&#19982;&#30452;&#25509;&#35780;&#20272;&#20855;&#26377;&#38750;&#26174;&#33879;&#24046;&#24322;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;S-ROCKET&#19981;&#21516;&#65292;&#25105;&#20204;&#20174;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#36890;&#36807;&#28040;&#38500;&#24207;&#21015;&#20013;&#30340;&#30456;&#20851;&#36830;&#25509;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, two time series classification models, ROCKET and MINIROCKET, have attracted much attention for their low training cost and state-of-the-art accuracy. Utilizing random 1-D convolutional kernels without training, ROCKET and MINIROCKET can rapidly extract features from time series data, allowing for the efficient fitting of linear classifiers. However, to comprehensively capture useful features, a large number of random kernels are required, which is incompatible for resource-constrained devices. Therefore, a heuristic evolutionary algorithm named S-ROCKET is devised to recognize and prune redundant kernels. Nevertheless, the inherent nature of evolutionary algorithms renders the evaluation of kernels within S-ROCKET an unacceptable time-consuming process. In this paper, diverging from S-ROCKET, which directly evaluates random kernels with nonsignificant differences, we remove kernels from a feature selection perspective by eliminating associating connections in the sequ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;&#65292;&#22312;&#20505;&#36873;&#20154;&#31579;&#36873;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#20505;&#36873;&#20154;&#27744;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#65292;&#31867;&#20154;&#31579;&#36873;&#32773;&#21487;&#33021;&#23545;&#21463;&#20445;&#25252;&#12289;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#19982;&#19968;&#23478;&#22823;&#20844;&#21496;&#21512;&#20316;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#28508;&#22312;&#30340;&#33258;&#21160;&#21270;&#25307;&#32856;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.15398</link><description>&lt;p&gt;
&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Initial Screening Order Problem. (arXiv:2307.15398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;&#65292;&#22312;&#20505;&#36873;&#20154;&#31579;&#36873;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#20505;&#36873;&#20154;&#27744;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#65292;&#31867;&#20154;&#31579;&#36873;&#32773;&#21487;&#33021;&#23545;&#21463;&#20445;&#25252;&#12289;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#19982;&#19968;&#23478;&#22823;&#20844;&#21496;&#21512;&#20316;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#28508;&#22312;&#30340;&#33258;&#21160;&#21270;&#25307;&#32856;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;&#65292;&#36825;&#26159;&#20505;&#36873;&#20154;&#31579;&#36873;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#23427;&#28041;&#21450;&#19968;&#20010;&#31867;&#20284;&#20154;&#31867;&#30340;&#31579;&#36873;&#32773;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#30340;&#20505;&#36873;&#20154;&#27744;&#20013;&#25214;&#21040;&#21069;k&#20010;&#36866;&#21512;&#30340;&#20505;&#36873;&#20154;&#65292;&#32780;&#19981;&#26159;&#26368;&#22909;&#30340;k&#20010;&#36866;&#21512;&#30340;&#20505;&#36873;&#20154;&#12290;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#34920;&#31034;&#31867;&#20154;&#31579;&#36873;&#32773;&#22312;&#31579;&#36873;&#20043;&#21069;&#22914;&#20309;&#23433;&#25490;&#20505;&#36873;&#20154;&#27744;&#12290;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#30340;&#36873;&#25321;&#23545;&#25152;&#36873;&#30340;k&#20010;&#20505;&#36873;&#20154;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20505;&#36873;&#20154;&#27744;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#65292;&#30007;&#24615;&#20505;&#36873;&#20154;&#22810;&#20110;&#22899;&#24615;&#20505;&#36873;&#20154;&#65289;&#65292;&#31867;&#20154;&#31579;&#36873;&#32773;&#21487;&#33021;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#23545;&#21463;&#20445;&#25252;&#30340;&#12289;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20135;&#29983;&#19981;&#24179;&#31561;&#30340;&#21162;&#21147;&#12290;&#20854;&#20182;&#20844;&#24179;&#24615;&#32467;&#26524;&#20063;&#22312;&#31867;&#20154;&#31579;&#36873;&#32773;&#19979;&#24471;&#21040;&#35777;&#26126;&#12290;&#36825;&#39033;&#30740;&#31350;&#26159;&#19982;&#19968;&#23478;&#22823;&#20844;&#21496;&#21512;&#20316;&#30340;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20102;&#35299;&#20854;&#28508;&#22312;&#33258;&#21160;&#21270;&#30340;&#25307;&#32856;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present the initial screening order problem, a crucial step within candidate screening. It involves a human-like screener with an objective to find the first k suitable candidates rather than the best k suitable candidates in a candidate pool given an initial screening order. The initial screening order represents the way in which the human-like screener arranges the candidate pool prior to screening. The choice of initial screening order has considerable effects on the selected set of k candidates. We prove that under an unbalanced candidate pool (e.g., having more male than female candidates), the human-like screener can suffer from uneven efforts that hinder its decision-making over the protected, under-represented group relative to the non-protected, over-represented group. Other fairness results are proven under the human-like screener. This research is based on a collaboration with a large company to better understand its hiring process for potential automation. 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#31995;&#32479;&#22312;&#38544;&#34109;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#22270;&#20687;&#25200;&#21160;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08939</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#23454;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks. (arXiv:2307.08939v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08939
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#31995;&#32479;&#22312;&#38544;&#34109;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#22270;&#20687;&#25200;&#21160;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#39550;&#39542;&#21592;&#36741;&#21161;&#21151;&#33021;&#65292;&#29992;&#20110;&#20445;&#25345;&#26399;&#26395;&#36895;&#24230;&#21644;&#19982;&#21069;&#26041;&#36710;&#36742;&#30340;&#23433;&#20840;&#36317;&#31163;&#12290;&#26412;&#25991;&#35780;&#20272;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;ACC&#31995;&#32479;&#22312;&#38544;&#34109;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#65292;&#35813;&#25915;&#20987;&#20250;&#23545;&#25668;&#20687;&#26426;&#25968;&#25454;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#25200;&#21160;&#65292;&#20197;&#23548;&#33268;&#21069;&#26041;&#30896;&#25758;&#20107;&#25925;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#65292;&#29992;&#20110;&#36873;&#25321;&#35302;&#21457;&#25915;&#20987;&#26368;&#20851;&#38190;&#30340;&#26102;&#38388;&#28857;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#36816;&#34892;&#26102;&#29983;&#25104;&#36866;&#24212;&#24615;&#22270;&#20687;&#25200;&#21160;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#39550;&#39542;&#25968;&#25454;&#38598;&#21644;&#36924;&#30495;&#30340;&#20223;&#30495;&#24179;&#21488;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#20223;&#30495;&#24179;&#21488;&#20351;&#29992;&#20102;&#26469;&#33258;&#29983;&#20135;ACC&#31995;&#32479;&#30340;&#25511;&#21046;&#36719;&#20214;&#21644;&#29289;&#29702;&#19990;&#30028;&#39550;&#39542;&#27169;&#25311;&#22120;&#65292;&#24182;&#32771;&#34385;&#20102;&#39550;&#39542;&#21592;&#30340;&#24178;&#39044;&#20197;&#21450;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#21644;&#21069;&#21521;&#30896;&#25758;&#35686;&#31034;&#65288;FCW&#65289;&#31561;&#23433;&#20840;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive Cruise Control (ACC) is a widely used driver assistance feature for maintaining desired speed and safe distance to the leading vehicles. This paper evaluates the security of the deep neural network (DNN) based ACC systems under stealthy perception attacks that strategically inject perturbations into camera data to cause forward collisions. We present a combined knowledge-and-data-driven approach to design a context-aware strategy for the selection of the most critical times for triggering the attacks and a novel optimization-based method for the adaptive generation of image perturbations at run-time. We evaluate the effectiveness of the proposed attack using an actual driving dataset and a realistic simulation platform with the control software from a production ACC system and a physical-world driving simulator while considering interventions by the driver and safety features such as Automatic Emergency Braking (AEB) and Forward Collision Warning (FCW). Experimental results sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#39640;&#25928;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;GPU&#19978;&#30452;&#25509;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#20197;INR&#26684;&#24335;&#65292;&#20943;&#23569;&#20102;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#39640;&#24230;&#24182;&#34892;&#21270;&#21644;&#23454;&#26102;&#25191;&#34892;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16699</link><description>&lt;p&gt;
&#24555;&#36895;-INR: &#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#25928;&#29575;&#39640;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Rapid-INR: Storage Efficient CPU-free DNN Training Using Implicit Neural Representation. (arXiv:2306.16699v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#39640;&#25928;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;GPU&#19978;&#30452;&#25509;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#20197;INR&#26684;&#24335;&#65292;&#20943;&#23569;&#20102;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#39640;&#24230;&#24182;&#34892;&#21270;&#21644;&#23454;&#26102;&#25191;&#34892;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;(INR)&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#31034;&#22797;&#26434;&#30340;&#24418;&#29366;&#25110;&#23545;&#35937;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#23450;&#20041;&#23427;&#20204;&#30340;&#20960;&#20309;&#24418;&#29366;&#25110;&#34920;&#38754;&#32467;&#26500;&#12290;&#30456;&#21453;&#65292;INR&#23558;&#23545;&#35937;&#34920;&#31034;&#20026;&#36830;&#32493;&#20989;&#25968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#29992;&#20316;INR&#36827;&#34892;&#22270;&#20687;&#21387;&#32553;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#19982;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;JPEG&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;INR&#22312;&#22270;&#20687;&#21387;&#32553;&#20043;&#22806;&#36824;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#28508;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Rapid-INR&#65292;&#19968;&#31181;&#21033;&#29992;INR&#23545;&#22270;&#20687;&#36827;&#34892;&#32534;&#30721;&#21644;&#21387;&#32553;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#21152;&#36895;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;GPU&#19978;&#30452;&#25509;&#20197;INR&#26684;&#24335;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;CPU&#21644;GPU&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#20174;INR&#21040;RGB&#26684;&#24335;&#30340;&#35299;&#30721;&#36807;&#31243;&#39640;&#24230;&#24182;&#34892;&#21270;&#24182;&#23454;&#26102;&#25191;&#34892;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21387;&#32553;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#22270;&#20687;&#21387;&#32553;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Representation (INR) is an innovative approach for representing complex shapes or objects without explicitly defining their geometry or surface structure. Instead, INR represents objects as continuous functions. Previous research has demonstrated the effectiveness of using neural networks as INR for image compression, showcasing comparable performance to traditional methods such as JPEG. However, INR holds potential for various applications beyond image compression. This paper introduces Rapid-INR, a novel approach that utilizes INR for encoding and compressing images, thereby accelerating neural network training in computer vision tasks. Our methodology involves storing the whole dataset directly in INR format on a GPU, mitigating the significant data communication overhead between the CPU and GPU during training. Additionally, the decoding process from INR to RGB format is highly parallelized and executed on-the-fly. To further enhance compression, we propose iterativ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25104;&#21151;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#29983;&#25104;&#24314;&#27169;&#30340;&#20248;&#24322;&#24615;&#36136;&#19982;&#39640;&#25928;&#33258;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#12290;&#20316;&#32773;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#35774;&#35745;&#20102;&#28040;&#38500;&#36845;&#20195;&#30340;&#20272;&#35745;&#22120;&#24182;&#25552;&#20986;&#20102;&#31283;&#23450;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#19968;&#31995;&#21015;&#38750;&#32422;&#26463;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01843</link><description>&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Maximum Likelihood Training of Autoencoders. (arXiv:2306.01843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25104;&#21151;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#29983;&#25104;&#24314;&#27169;&#30340;&#20248;&#24322;&#24615;&#36136;&#19982;&#39640;&#25928;&#33258;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#12290;&#20316;&#32773;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#35774;&#35745;&#20102;&#28040;&#38500;&#36845;&#20195;&#30340;&#20272;&#35745;&#22120;&#24182;&#25552;&#20986;&#20102;&#31283;&#23450;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#19968;&#31995;&#21015;&#38750;&#32422;&#26463;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#20855;&#26377;&#20248;&#24322;&#30340;&#32479;&#35745;&#24615;&#36136;&#65292;&#23588;&#20854;&#26159;&#22312;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#20013;&#38750;&#24120;&#27969;&#34892;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#27969;&#24418;&#20551;&#35774;&#65292;&#29983;&#25104;&#33258;&#32534;&#30721;&#22120;&#26377;&#26395;&#27604;&#24402;&#19968;&#21270;&#27969;&#26356;&#39640;&#25928;&#12290;&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#20102;&#38750;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#30340;&#25104;&#21151;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#65292;&#23558;&#36825;&#20004;&#31181;&#33539;&#24335;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35782;&#21035;&#24182;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#29616;&#26377;&#30340;&#33258;&#30001;&#26684;&#24335;&#32593;&#32476;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#36807;&#20110;&#32531;&#24930;&#65292;&#20381;&#36182;&#20110;&#36845;&#20195;&#26041;&#26696;&#65292;&#20854;&#25104;&#26412;&#38543;&#28508;&#22312;&#32500;&#24230;&#21576;&#32447;&#24615;&#22686;&#38271;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#20272;&#35745;&#22120;&#65292;&#28040;&#38500;&#20102;&#36845;&#20195;&#65292;&#20174;&#32780;&#20351;&#25104;&#26412;&#20445;&#25345;&#19981;&#21464;&#65288;&#27599;&#20010;&#25209;&#27425;&#30340;&#36816;&#34892;&#26102;&#38388;&#22823;&#32422;&#26159;&#26222;&#36890;&#33258;&#32534;&#30721;&#22120;&#30340;&#20004;&#20493;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#26420;&#32032;&#22320;&#23558;&#26368;&#22823;&#20284;&#28982;&#24212;&#29992;&#20110;&#33258;&#32534;&#30721;&#22120;&#21487;&#33021;&#23548;&#33268;&#21457;&#25955;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#24819;&#27861;&#26469;&#25512;&#21160;&#31283;&#23450;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#19968;&#31995;&#21015;&#38750;&#32422;&#26463;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#29983;&#25104;&#22270;&#20687;&#12289;&#25554;&#20540;&#21644;&#21464;&#25442;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum likelihood training has favorable statistical properties and is popular for generative modeling, especially with normalizing flows. On the other hand, generative autoencoders promise to be more efficient than normalizing flows due to the manifold hypothesis. In this work, we introduce successful maximum likelihood training of unconstrained autoencoders for the first time, bringing the two paradigms together. To do so, we identify and overcome two challenges: Firstly, existing maximum likelihood estimators for free-form networks are unacceptably slow, relying on iteration schemes whose cost scales linearly with latent dimension. We introduce an improved estimator which eliminates iteration, resulting in constant cost (roughly double the runtime per batch of a vanilla autoencoder). Secondly, we demonstrate that naively applying maximum likelihood to autoencoders can lead to divergent solutions and use this insight to motivate a stable maximum likelihood training objective. We per
&lt;/p&gt;</description></item><item><title>LANISTR&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#21487;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.16556</link><description>&lt;p&gt;
LANISTR&#65306;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LANISTR: Multimodal Learning from Structured and Unstructured Data. (arXiv:2305.16556v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16556
&lt;/p&gt;
&lt;p&gt;
LANISTR&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#21487;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#24050;&#32463;&#22312;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#65289;&#26041;&#38754;&#23637;&#29616;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20294;&#26159;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#26368;&#24120;&#35265;&#30340;&#24773;&#20917;&#26159;&#32467;&#26500;&#21270;&#65288;&#21253;&#25324;&#34920;&#26684;&#21644;&#26102;&#38388;&#24207;&#21015;&#65289;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32467;&#21512;&#65292;&#20294;&#36825;&#19968;&#39046;&#22495;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LANISTR&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#35821;&#35328;&#12289;&#22270;&#20687;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#22810;&#27169;&#24577;&#36974;&#32617;&#25439;&#22833;&#65292;&#20351;&#24471;LANISTR&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#36328;&#27169;&#24577;&#20851;&#31995;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#22788;&#29702;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;MIMIC-IV&#21644;Amazon Product Review&#19978;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#65292;LANISTR&#20998;&#21035;&#36798;&#21040;&#20102;6.47%&#65288;AUROC&#65289;&#21644;&#39640;&#36798;17.69%&#65288;&#20934;&#30830;&#24230;&#65289;&#30340;&#32477;&#23545;&#25552;&#21319;&#65292;&#24182;&#26174;&#31034;&#20986;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large-scale pretraining has shown impressive performance gains for unstructured data including language, image, audio, and video. Yet, the scenario most prominent in real-world applications is the existence of combination of structured (including tabular and time-series) and unstructured data, and this has so far been understudied. Towards this end, we propose LANISTR, a novel attention-based framework to learn from LANguage, Image, and STRuctured data. We introduce a new multimodal fusion module with a similarity-based multimodal masking loss that enables LANISTR to learn cross-modal relations from large-scale multimodal data with missing modalities during training and test time. On two publicly available challenging datasets, MIMIC-IV and Amazon Product Review, LANISTR achieves absolute improvements of 6.47% (AUROC) and up to 17.69% (accuracy), respectively, compared to the state-of-the-art multimodal models while showing superior generalization capabilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#30340;&#25925;&#38556;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#23545;ONNX&#30456;&#20851;&#30340;&#36716;&#25442;&#22120;&#36827;&#34892;&#20102;&#39318;&#27425;&#25925;&#38556;&#20998;&#26512;&#65292;&#24182;&#35814;&#32454;&#25253;&#21578;&#20102;&#25925;&#38556;&#30340;&#30151;&#29366;&#65292;&#21407;&#22240;&#21644;&#20301;&#32622;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.17708</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#20013;&#30340;&#25925;&#38556;&#21644;&#39118;&#38505;&#20998;&#26512;&#65306;&#20197;ONNX&#29983;&#24577;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Analysis of Failures and Risks in Deep Learning Model Converters: A Case Study in the ONNX Ecosystem. (arXiv:2303.17708v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#30340;&#25925;&#38556;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#23545;ONNX&#30456;&#20851;&#30340;&#36716;&#25442;&#22120;&#36827;&#34892;&#20102;&#39318;&#27425;&#25925;&#38556;&#20998;&#26512;&#65292;&#24182;&#35814;&#32454;&#25253;&#21578;&#20102;&#25925;&#38556;&#30340;&#30151;&#29366;&#65292;&#21407;&#22240;&#21644;&#20301;&#32622;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24037;&#31243;&#24072;&#24320;&#21457;&#65292;&#20248;&#21270;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#20182;&#20204;&#22312;&#21508;&#31181;&#24320;&#21457;&#26694;&#26550;&#20013;&#20351;&#29992;&#21644;&#37325;&#26032;&#20351;&#29992;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#36816;&#34892;&#26102;&#29615;&#22659;&#20013;&#37096;&#32626;&#23427;&#20204;&#12290;&#22312;&#36825;&#20010;&#22810;&#26679;&#21270;&#30340;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#24037;&#31243;&#24072;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#23558;&#27169;&#22411;&#20174;&#26694;&#26550;&#31227;&#21160;&#21040;&#36816;&#34892;&#26102;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#36716;&#25442;&#22120;&#20013;&#30340;&#38169;&#35823;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#24182;&#30772;&#22351;&#37096;&#32626;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#30340;&#25925;&#38556;&#39057;&#29575;&#21644;&#25925;&#38556;&#27169;&#24335;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#38024;&#23545;ONNX (Open Neural Network eXchange)&#30456;&#20851;&#30340;&#27169;&#22411;&#36716;&#25442;&#22120;&#36827;&#34892;&#20102;&#39318;&#27425;&#25925;&#38556;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;ONNX&#36716;&#25442;&#22120;&#22312;&#20004;&#20010;&#37325;&#35201;&#30340;DL&#26694;&#26550;PyTorch&#21644;TensorFlow&#20013;&#30340;&#36807;&#21435;&#25925;&#38556;&#12290;&#36824;&#25253;&#21578;&#20102;&#25925;&#38556;&#65288;N=200&#20010;&#38382;&#39064;&#65289;&#30340;&#30151;&#29366;&#65292;&#21407;&#22240;&#21644;&#20301;&#32622;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#36716;&#25442;8,797&#20010;&#27169;&#22411;&#65288;&#30495;&#23454;&#19990;&#30028;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;&#23454;&#20363;&#65289;&#26469;&#35780;&#20272;&#24403;&#20170;&#30340;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software engineers develop, fine-tune, and deploy deep learning (DL) models. They use and re-use models in a variety of development frameworks and deploy them on a range of runtime environments. In this diverse ecosystem, engineers use DL model converters to move models from frameworks to runtime environments. However, errors in converters can compromise model quality and disrupt deployment. The failure frequency and failure modes of DL model converters are unknown.  In this paper, we conduct the first failure analysis on DL model converters. Specifically, we characterize failures in model converters associated with ONNX (Open Neural Network eXchange). We analyze past failures in the ONNX converters in two major DL frameworks, PyTorch and TensorFlow. The symptoms, causes, and locations of failures (for N=200 issues), and trends over time are also reported. We also evaluate present-day failures by converting 8,797 models, both real-world and synthetically generated instances. The consis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20302;&#24310;&#36831;&#33258;&#36866;&#24212;&#32534;&#30721;&#33033;&#20914;&#26694;&#26550;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#32534;&#30721;&#22120;&#28789;&#27963;&#24615;&#12289;&#24310;&#36831;&#21644;&#33021;&#37327;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#24322;&#24615;&#33021;&#21644;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2211.11760</link><description>&lt;p&gt;
&#19968;&#20010;&#20302;&#24310;&#36831;&#33258;&#36866;&#24212;&#32534;&#30721;&#33033;&#20914;&#26694;&#26550;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Low Latency Adaptive Coding Spiking Framework for Deep Reinforcement Learning. (arXiv:2211.11760v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20302;&#24310;&#36831;&#33258;&#36866;&#24212;&#32534;&#30721;&#33033;&#20914;&#26694;&#26550;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#32534;&#30721;&#22120;&#28789;&#27963;&#24615;&#12289;&#24310;&#36831;&#21644;&#33021;&#37327;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#24322;&#24615;&#33021;&#21644;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20302;&#21151;&#32791;&#21644;&#20107;&#20214;&#39537;&#21160;&#29305;&#24615;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#34987;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#28982;&#32780;&#65292;&#22266;&#23450;&#32534;&#30721;&#26041;&#27861;&#23548;&#33268;&#30340;&#33033;&#20914;&#24378;&#21270;&#23398;&#20064;&#65288;SRL&#65289;&#20173;&#28982;&#38754;&#20020;&#39640;&#24310;&#36831;&#21644;&#36739;&#24046;&#30340;&#28789;&#27963;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#30697;&#38453;&#20056;&#27861;&#23545;&#33033;&#20914;&#36827;&#34892;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#25552;&#39640;&#32534;&#30721;&#22120;&#30340;&#28789;&#27963;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#24310;&#36831;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#30452;&#25509;&#35757;&#32451;&#26041;&#27861;&#35757;&#32451;SNNs&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#32467;&#26500;&#29992;&#20110;&#22312;&#32447;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#25317;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#31639;&#27861;&#21644;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24310;&#36831;&#26497;&#20302;&#65288;&#20165;&#20026;&#20854;&#20182;SRL&#26041;&#27861;&#30340;0.8%&#65289;&#19988;&#20855;&#26377;&#26497;&#39640;&#30340;&#33021;&#37327;&#25928;&#29575;&#65288;&#39640;&#36798;DNNs&#30340;5&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, spiking neural networks (SNNs) have been used in reinforcement learning (RL) due to their low power consumption and event-driven features. However, spiking reinforcement learning (SRL), which suffers from fixed coding methods, still faces the problems of high latency and poor versatility. In this paper, we use learnable matrix multiplication to encode and decode spikes, improving the flexibility of the coders and thus reducing latency. Meanwhile, we train the SNNs using the direct training method and use two different structures for online and offline RL algorithms, which gives our model a wider range of applications. Extensive experiments have revealed that our method achieves optimal performance with ultra-low latency (as low as 0.8% of other SRL methods) and excellent energy efficiency (up to 5X the DNNs) in different algorithms and different environments.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;&#21560;&#25910;&#27604;&#20363;&#32553;&#25918;&#22270;&#21644;&#39532;&#23572;&#21487;&#22827;&#26102;&#38388;&#25195;&#25551;&#25913;&#36827;&#20102;InfoMap&#31639;&#27861;&#65292;&#26816;&#27979;&#32593;&#32476;&#19978;&#23494;&#38598;&#36830;&#25509;&#30340;&#33410;&#28857;&#31038;&#21306;&#65292;&#27492;&#26041;&#27861;&#36866;&#24212;&#33410;&#28857;&#20855;&#26377;&#19981;&#21516;&#31227;&#38500;&#29575;&#30340;&#24773;&#20917;&#65292;&#31038;&#21306;&#32467;&#26500;&#19982;&#19981;&#32771;&#34385;&#33410;&#28857;&#21560;&#25910;&#29575;&#30340;&#26041;&#27861;&#21487;&#33021;&#26377;&#26174;&#33879;&#19981;&#21516;&#65292;&#24182;&#23545;&#26131;&#24863;-&#24863;&#26579;-&#24674;&#22797;&#65288;SI&#65289;&#27169;&#22411;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2112.10953</link><description>&lt;p&gt;
&#20351;&#29992;&#21560;&#25910;&#27604;&#20363;&#32553;&#25918;&#22270;&#30340;InfoMap&#31639;&#27861;&#22312;&#21560;&#25910;&#38543;&#26426;&#28459;&#27493;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An adaptation of InfoMap to absorbing random walks using absorption-scaled graphs. (arXiv:2112.10953v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.10953
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#21560;&#25910;&#27604;&#20363;&#32553;&#25918;&#22270;&#21644;&#39532;&#23572;&#21487;&#22827;&#26102;&#38388;&#25195;&#25551;&#25913;&#36827;&#20102;InfoMap&#31639;&#27861;&#65292;&#26816;&#27979;&#32593;&#32476;&#19978;&#23494;&#38598;&#36830;&#25509;&#30340;&#33410;&#28857;&#31038;&#21306;&#65292;&#27492;&#26041;&#27861;&#36866;&#24212;&#33410;&#28857;&#20855;&#26377;&#19981;&#21516;&#31227;&#38500;&#29575;&#30340;&#24773;&#20917;&#65292;&#31038;&#21306;&#32467;&#26500;&#19982;&#19981;&#32771;&#34385;&#33410;&#28857;&#21560;&#25910;&#29575;&#30340;&#26041;&#27861;&#21487;&#33021;&#26377;&#26174;&#33879;&#19981;&#21516;&#65292;&#24182;&#23545;&#26131;&#24863;-&#24863;&#26579;-&#24674;&#22797;&#65288;SI&#65289;&#27169;&#22411;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
InfoMap&#31639;&#27861;&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#19978;&#23494;&#38598;&#36830;&#25509;&#30340;&#8220;&#31038;&#21306;&#8221;&#33410;&#28857;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#26412;&#25991;&#23558;&#20854;&#24212;&#29992;&#19982;&#21560;&#25910;&#38543;&#26426;&#28459;&#27493;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#21560;&#25910;&#27604;&#20363;&#32553;&#25918;&#22270;&#21644;&#39532;&#23572;&#21487;&#22827;&#26102;&#38388;&#25195;&#25551;&#26469;&#36866;&#24212;&#33410;&#28857;&#20855;&#26377;&#19981;&#21516;&#31227;&#38500;&#29575;&#30340;&#24773;&#20917;&#12290;&#25913;&#36827;&#21518;&#30340;InfoMap&#31639;&#27861;&#26816;&#27979;&#21040;&#30340;&#31038;&#21306;&#32467;&#26500;&#21487;&#33021;&#19982;&#19981;&#32771;&#34385;&#33410;&#28857;&#21560;&#25910;&#29575;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#24182;&#23545;&#26131;&#24863;-&#24863;&#26579;-&#24674;&#22797;&#65288;SI&#65289;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
InfoMap is a popular approach for detecting densely connected "communities" of nodes in networks. To detect such communities, InfoMap uses random walks and ideas from information theory. Motivated by the dynamics of disease spread on networks, whose nodes may have heterogeneous disease-removal rates, we adapt InfoMap to absorbing random walks. To do this, we use absorption-scaled graphs, in which the edge weights are scaled according to absorption rates, along with Markov time sweeping. One of our adaptations of InfoMap converges to the standard version of InfoMap in the limit in which the node-absorption rates approach $0$. The community structure that we obtain using our adaptations of InfoMap can differ markedly from the community structure that one detects using methods that do not take node-absorption rates into account. Additionally, we demonstrate that the community structure that is induced by local dynamics can have important implications for susceptible-infected-recovered (SI
&lt;/p&gt;</description></item></channel></rss>