<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#39318;&#27425;&#23558;Fisher&#21512;&#24182;&#26041;&#27861;&#24212;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;&#20013;&#65292;&#36890;&#36807;&#21512;&#24182;&#22810;&#20010;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#25913;&#21892;&#25972;&#20307;&#24615;&#33021;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23454;&#38469;&#25361;&#25112;&#65292;&#20855;&#26377;&#25512;&#21160;&#26368;&#26032;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05476</link><description>&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#20013;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#30340;Fisher&#21152;&#26435;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Fisher-Weighted Merge of Contrastive Learning Models in Sequential Recommendation. (arXiv:2307.05476v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23558;Fisher&#21512;&#24182;&#26041;&#27861;&#24212;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;&#20013;&#65292;&#36890;&#36807;&#21512;&#24182;&#22810;&#20010;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#25913;&#21892;&#25972;&#20307;&#24615;&#33021;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23454;&#38469;&#25361;&#25112;&#65292;&#20855;&#26377;&#25512;&#21160;&#26368;&#26032;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#24179;&#21488;&#21644;&#26381;&#21153;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#20026;&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#35782;&#21035;&#30456;&#20851;&#29289;&#21697;&#30340;&#24517;&#22791;&#24037;&#20855;&#12290;&#24207;&#21015;&#25512;&#33616;&#30340;&#39046;&#22495;&#26088;&#22312;&#25429;&#25417;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#21160;&#24577;&#20559;&#22909;&#65292;&#24050;&#25552;&#20986;&#20102;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#24212;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30001;&#20110;&#26377;&#38480;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#32780;&#23548;&#33268;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;Fisher&#21512;&#24182;&#26041;&#27861;&#24212;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;&#20013;&#65292;&#35299;&#20915;&#24182;&#35299;&#20915;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#21512;&#24182;&#22810;&#20010;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#30830;&#20445;&#40065;&#26834;&#24494;&#35843;&#65292;&#20174;&#32780;&#25913;&#21892;&#25972;&#20307;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#24207;&#21015;&#23398;&#20064;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#25512;&#21160;&#26368;&#26032;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Along with the exponential growth of online platforms and services, recommendation systems have become essential for identifying relevant items based on user preferences. The domain of sequential recommendation aims to capture evolving user preferences over time. To address dynamic preference, various contrastive learning methods have been proposed to target data sparsity, a challenge in recommendation systems due to the limited user-item interactions. In this paper, we are the first to apply the Fisher-Merging method to Sequential Recommendation, addressing and resolving practical challenges associated with it. This approach ensures robust fine-tuning by merging the parameters of multiple models, resulting in improved overall performance. Through extensive experiments, we demonstrate the effectiveness of our proposed methods, highlighting their potential to advance the state-of-the-art in sequential learning and recommendation systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AdaptiveRec&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#25361;&#25112;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21892;&#23884;&#20837;&#36136;&#37327;&#21644;&#20943;&#36731;&#35823;&#21028;&#38382;&#39064;&#26469;&#25552;&#39640;&#25928;&#26524;&#65292;&#24182;&#22312;&#21508;&#31181;&#25512;&#33616;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#36866;&#29992;&#24615;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.05469</link><description>&lt;p&gt;
AdaptiveRec&#65306;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#33258;&#36866;&#24212;&#26500;&#24314;&#23545;&#27604;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
AdaptiveRec: Adaptively Construct Pairs for Contrastive Learning in Sequential Recommendation. (arXiv:2307.05469v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AdaptiveRec&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#25361;&#25112;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21892;&#23884;&#20837;&#36136;&#37327;&#21644;&#20943;&#36731;&#35823;&#21028;&#38382;&#39064;&#26469;&#25552;&#39640;&#25928;&#26524;&#65292;&#24182;&#22312;&#21508;&#31181;&#25512;&#33616;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#36866;&#29992;&#24615;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#23545;&#27604;&#23398;&#20064;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#35299;&#20915;&#20102;&#35823;&#21028;&#30340;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#38480;&#21046;&#20102;&#25512;&#33616;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#20808;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#29289;&#21697;&#23884;&#20837;&#30340;&#36136;&#37327;&#65292;&#24182;&#20943;&#36731;&#20102;&#23558;&#30456;&#20284;&#23454;&#20363;&#38169;&#35823;&#22320;&#24402;&#31867;&#20026;&#19981;&#30456;&#20284;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#31995;&#32479;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#28789;&#27963;&#24615;&#21644;&#36866;&#29992;&#24615;&#36827;&#19968;&#27493;&#20984;&#26174;&#20102;&#23427;&#22312;&#22686;&#24378;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a solution to the challenges faced by contrastive learning in sequential recommendation systems. In particular, it addresses the issue of false negative, which limits the effectiveness of recommendation algorithms. By introducing an advanced approach to contrastive learning, the proposed method improves the quality of item embeddings and mitigates the problem of falsely categorizing similar instances as dissimilar. Experimental results demonstrate performance enhancements compared to existing systems. The flexibility and applicability of the proposed approach across various recommendation scenarios further highlight its value in enhancing sequential recommendation systems.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24418;&#24577;&#23398;&#24863;&#30693;&#26694;&#26550; M2C&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#27979;&#35797;&#26469;&#35780;&#20272; NLP &#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#29305;&#24449;&#19979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#33521;&#35821;&#20013;&#65292;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#26031;&#29926;&#24076;&#37324;&#35821;&#30340;&#26102;&#38388;&#34920;&#36798;&#21644;&#33452;&#20848;&#35821;&#30340;&#21512;&#25104;&#25152;&#26377;&#26684;&#31561;&#29305;&#23450;&#31867;&#22411;&#29305;&#24449;&#19978;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#33021;&#22815;&#35299;&#20915;&#36825;&#20123;&#30450;&#28857;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.05454</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#31867;&#22411;&#29305;&#24449;&#22686;&#24378;&#36328;&#35821;&#35328;&#34892;&#20026;&#27979;&#35797;&#30340; NLP &#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features. (arXiv:2307.05454v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05454
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24418;&#24577;&#23398;&#24863;&#30693;&#26694;&#26550; M2C&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#27979;&#35797;&#26469;&#35780;&#20272; NLP &#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#29305;&#24449;&#19979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#33521;&#35821;&#20013;&#65292;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#26031;&#29926;&#24076;&#37324;&#35821;&#30340;&#26102;&#38388;&#34920;&#36798;&#21644;&#33452;&#20848;&#35821;&#30340;&#21512;&#25104;&#25152;&#26377;&#26684;&#31561;&#29305;&#23450;&#31867;&#22411;&#29305;&#24449;&#19978;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#33021;&#22815;&#35299;&#20915;&#36825;&#20123;&#30450;&#28857;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#38754;&#21521;&#19990;&#30028;&#21508;&#35821;&#35328;&#30340; NLP &#31995;&#32479;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#29702;&#35299;&#23427;&#20204;&#22312;&#19982;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30456;&#20851;&#30340;&#31867;&#22411;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; M2C&#65292;&#19968;&#20010;&#23545; NLP &#27169;&#22411;&#36827;&#34892;&#34892;&#20026;&#27979;&#35797;&#30340;&#24418;&#24577;&#23398;&#24863;&#30693;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992; M2C &#29983;&#25104;&#27979;&#35797;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#22312;12&#31181;&#31867;&#22411;&#22810;&#26679;&#30340;&#35821;&#35328;&#20013;&#38024;&#23545;&#29305;&#23450;&#35821;&#35328;&#29305;&#24449;&#34920;&#29616;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#29983;&#25104;&#30340;&#27979;&#35797;&#19978;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#34429;&#28982;&#27169;&#22411;&#22312;&#33521;&#35821;&#19978;&#30340;&#22823;&#22810;&#25968;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#26031;&#29926;&#24076;&#37324;&#35821;&#30340;&#26102;&#38388;&#34920;&#36798;&#21644;&#33452;&#20848;&#35821;&#30340;&#21512;&#25104;&#25152;&#26377;&#26684;&#31561;&#29305;&#23450;&#31867;&#22411;&#29305;&#24449;&#30340;&#27867;&#21270;&#22833;&#36133;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20419;&#36827;&#20102;&#24320;&#21457;&#33021;&#22815;&#35299;&#20915;&#36825;&#20123;&#30450;&#28857;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A challenge towards developing NLP systems for the world's languages is understanding how they generalize to typological differences relevant for real-world applications. To this end, we propose M2C, a morphologically-aware framework for behavioral testing of NLP models. We use M2C to generate tests that probe models' behavior in light of specific linguistic features in 12 typologically diverse languages. We evaluate state-of-the-art language models on the generated tests. While models excel at most tests in English, we highlight generalization failures to specific typological characteristics such as temporal expressions in Swahili and compounding possessives in Finish. Our findings motivate the development of models that address these blind spots.
&lt;/p&gt;</description></item><item><title>ISLTranslate&#26159;&#19968;&#20010;&#21253;&#21547;31k&#20010;ISL-&#33521;&#35821;&#21477;&#23376;/&#30701;&#35821;&#23545;&#30340;&#26368;&#22823;&#36830;&#32493;&#21360;&#24230;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#24110;&#21161;&#24320;&#21457;&#25163;&#35821;&#32763;&#35793;&#31995;&#32479;&#65292;&#35299;&#20915;&#21360;&#24230;&#25163;&#35821;&#36164;&#28304;&#21294;&#20047;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05440</link><description>&lt;p&gt;
ISLTranslate: &#32763;&#35793;&#21360;&#24230;&#25163;&#35821;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ISLTranslate: Dataset for Translating Indian Sign Language. (arXiv:2307.05440v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05440
&lt;/p&gt;
&lt;p&gt;
ISLTranslate&#26159;&#19968;&#20010;&#21253;&#21547;31k&#20010;ISL-&#33521;&#35821;&#21477;&#23376;/&#30701;&#35821;&#23545;&#30340;&#26368;&#22823;&#36830;&#32493;&#21360;&#24230;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#24110;&#21161;&#24320;&#21457;&#25163;&#35821;&#32763;&#35793;&#31995;&#32479;&#65292;&#35299;&#20915;&#21360;&#24230;&#25163;&#35821;&#36164;&#28304;&#21294;&#20047;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#26159;&#20840;&#29699;&#35768;&#22810;&#21548;&#38556;&#20154;&#22763;&#30340;&#20027;&#35201;&#36890;&#20449;&#26041;&#24335;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#24357;&#34917;&#21548;&#38556;&#31038;&#21306;&#19982;&#20854;&#20182;&#20154;&#32676;&#20043;&#38388;&#30340;&#27807;&#36890;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#20960;&#20010;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#24320;&#21457;&#32479;&#35745;&#25163;&#35821;&#32763;&#35793;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#21360;&#24230;&#25163;&#35821;&#30340;&#36164;&#28304;&#21294;&#20047;&#12290;&#26412;&#36164;&#28304;&#35770;&#25991;&#20171;&#32461;&#20102;ISLTranslate&#65292;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#21360;&#24230;&#25163;&#35821;&#65288;ISL&#65289;&#30340;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;31k&#20010;ISL-&#33521;&#35821;&#21477;&#23376;/&#30701;&#35821;&#23545;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#36830;&#32493;&#21360;&#24230;&#25163;&#35821;&#26368;&#22823;&#30340;&#32763;&#35793;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#20026;&#20102;&#39564;&#35777;&#29616;&#26377;&#30340;&#31471;&#21040;&#31471;&#25163;&#35821;&#21040;&#21475;&#35821;&#32763;&#35793;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;ISL&#32763;&#35793;&#23545;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign languages are the primary means of communication for many hard-of-hearing people worldwide. Recently, to bridge the communication gap between the hard-of-hearing community and the rest of the population, several sign language translation datasets have been proposed to enable the development of statistical sign language translation systems. However, there is a dearth of sign language resources for the Indian sign language. This resource paper introduces ISLTranslate, a translation dataset for continuous Indian Sign Language (ISL) consisting of 31k ISL-English sentence/phrase pairs. To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language. We provide a detailed analysis of the dataset. To validate the performance of existing end-to-end Sign language to spoken language translation systems, we benchmark the created dataset with a transformer-based model for ISL translation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22478;&#24066;&#37319;&#26679;&#30340;&#31616;&#21333;&#21152;&#22122;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#21463;&#32422;&#26463;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#37319;&#26679;&#22120;&#65292;&#35813;&#26041;&#26696;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#23454;&#35777;&#24615;&#33021;&#26041;&#38754;&#37117;&#26377;&#26126;&#26174;&#25552;&#21319;&#65292;&#24182;&#19988;&#34987;&#35777;&#26126;&#26159;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#30340;&#26377;&#25928;&#31163;&#25955;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.05439</link><description>&lt;p&gt;
&#21463;&#38480;&#25193;&#25955;&#27169;&#22411;&#30340;&#22478;&#24066;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Metropolis Sampling for Constrained Diffusion Models. (arXiv:2307.05439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22478;&#24066;&#37319;&#26679;&#30340;&#31616;&#21333;&#21152;&#22122;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#21463;&#32422;&#26463;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#37319;&#26679;&#22120;&#65292;&#35813;&#26041;&#26696;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#23454;&#35777;&#24615;&#33021;&#26041;&#38754;&#37117;&#26377;&#26126;&#26174;&#25552;&#21319;&#65292;&#24182;&#19988;&#34987;&#35777;&#26126;&#26159;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#30340;&#26377;&#25928;&#31163;&#25955;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#24314;&#27169;&#30340;&#20027;&#35201;&#33539;&#24335;&#12290;&#23427;&#20204;&#23545;&#40654;&#26364;&#27969;&#24418;&#30340;&#25193;&#23637;&#20351;&#24471;&#23427;&#20204;&#33021;&#22815;&#22312;&#33258;&#28982;&#31185;&#23398;&#20013;&#30340;&#19968;&#31995;&#21015;&#38382;&#39064;&#19978;&#24471;&#21040;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#20013;&#65292;&#36825;&#20123;&#27969;&#24418;&#30001;&#19968;&#32452;&#32422;&#26463;&#23450;&#20041;&#65292;&#24182;&#19988;&#19981;&#34987;&#29616;&#26377;&#30340;&#65288;&#40654;&#26364;&#65289;&#25193;&#25955;&#27169;&#22411;&#26041;&#27861;&#25152;&#35206;&#30422;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23545;&#25968;&#38556;&#30861;&#26041;&#27861;&#25110;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#30340;&#26032;&#22411;&#21152;&#22122;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#32422;&#26463;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#65292;&#30456;&#20851;&#30340;&#37319;&#26679;&#22120;&#35745;&#31639;&#36127;&#25285;&#36739;&#37325;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22478;&#24066;&#37319;&#26679;&#30340;&#26367;&#20195;&#31616;&#21333;&#21152;&#22122;&#26041;&#26696;&#65292;&#19982;&#26089;&#26399;&#30340;&#37319;&#26679;&#22120;&#30456;&#27604;&#65292;&#35745;&#31639;&#25928;&#29575;&#21644;&#23454;&#35777;&#24615;&#33021;&#37117;&#26377;&#24456;&#22823;&#25552;&#21319;&#12290;&#22312;&#29420;&#31435;&#30340;&#20852;&#36259;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#26032;&#36807;&#31243;&#23545;&#24212;&#20110;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#30340;&#26377;&#25928;&#31163;&#25955;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models have recently emerged as the predominant paradigm for generative modelling. Their extension to Riemannian manifolds has facilitated their application to an array of problems in the natural sciences. Yet, in many practical settings, such manifolds are defined by a set of constraints and are not covered by the existing (Riemannian) diffusion model methodology. Recent work has attempted to address this issue by employing novel noising processes based on logarithmic barrier methods or reflected Brownian motions. However, the associated samplers are computationally burdensome as the complexity of the constraints increases. In this paper, we introduce an alternative simple noising scheme based on Metropolis sampling that affords substantial gains in computational efficiency and empirical performance compared to the earlier samplers. Of independent interest, we prove that this new process corresponds to a valid discretisation of the reflected Brownian motion. We dem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#26234;&#33021;&#25163;&#34920;&#25903;&#20184;&#30340;&#23433;&#20840;&#24615;&#65292;&#21253;&#25324;&#26500;&#24314;&#20102;&#19968;&#20010;&#36229;&#36234;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#35748;&#35777;&#31995;&#32479;&#21644;&#24320;&#21457;&#19968;&#20010;&#29983;&#25104;&#21512;&#25104;&#25163;&#21183;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#29992;&#25143;&#22312;&#35748;&#35777;&#31995;&#32479;&#20013;&#25552;&#20379;&#25163;&#21183;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05437</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#25552;&#39640;&#26234;&#33021;&#25163;&#34920;&#25903;&#20184;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Security of Smartwatch Payment with Deep Learning. (arXiv:2307.05437v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#26234;&#33021;&#25163;&#34920;&#25903;&#20184;&#30340;&#23433;&#20840;&#24615;&#65292;&#21253;&#25324;&#26500;&#24314;&#20102;&#19968;&#20010;&#36229;&#36234;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#35748;&#35777;&#31995;&#32479;&#21644;&#24320;&#21457;&#19968;&#20010;&#29983;&#25104;&#21512;&#25104;&#25163;&#21183;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#29992;&#25143;&#22312;&#35748;&#35777;&#31995;&#32479;&#20013;&#25552;&#20379;&#25163;&#21183;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#25163;&#34920;&#36827;&#34892;&#38750;&#25509;&#35302;&#24335;&#25903;&#20184;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#35813;&#25903;&#20184;&#26041;&#24335;&#32570;&#20047;&#20256;&#32479;&#30340;&#29983;&#29289;&#35782;&#21035;&#23433;&#20840;&#25514;&#26045;&#65292;&#22914;&#38754;&#37096;&#25110;&#25351;&#32441;&#35782;&#21035;&#12290;&#22312;2022&#24180;&#65292;Sturgess&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WatchAuth&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807;&#21521;&#25903;&#20184;&#32456;&#31471;&#20280;&#25163;&#30340;&#29289;&#29702;&#25163;&#21183;&#26469;&#35748;&#35777;&#26234;&#33021;&#25163;&#34920;&#25903;&#20184;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#35813;&#31995;&#32479;&#38656;&#35201;&#29992;&#25143;&#32463;&#21382;&#32321;&#37325;&#30340;&#27880;&#20876;&#36807;&#31243;&#25165;&#33021;&#36798;&#21040;&#21487;&#25509;&#21463;&#30340;&#38169;&#35823;&#29575;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#26159;&#21542;&#21487;&#20197;&#20943;&#23569;&#29992;&#25143;&#22312;&#26234;&#33021;&#25163;&#34920;&#25903;&#20184;&#35748;&#35777;&#31995;&#32479;&#20013;&#24517;&#39035;&#25552;&#20379;&#30340;&#25163;&#21183;&#25968;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#35748;&#35777;&#31995;&#32479;&#65292;&#21253;&#25324;&#22312;&#30446;&#26631;&#29992;&#25143;&#25552;&#20379;&#26377;&#38480;&#25163;&#21183;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#30340;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#26469;&#29983;&#25104;&#29992;&#25143;&#29305;&#23450;&#30340;&#21512;&#25104;&#25163;&#21183;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#36825;&#20123;&#25163;&#21183;&#21487;&#20197;&#25913;&#21892;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making contactless payments using a smartwatch is increasingly popular, but this payment medium lacks traditional biometric security measures such as facial or fingerprint recognition. In 2022, Sturgess et al. proposed WatchAuth, a system for authenticating smartwatch payments using the physical gesture of reaching towards a payment terminal. While effective, the system requires the user to undergo a burdensome enrolment period to achieve acceptable error levels. In this dissertation, we explore whether applications of deep learning can reduce the number of gestures a user must provide to enrol into an authentication system for smartwatch payment. We firstly construct a deep-learned authentication system that outperforms the current state-of-the-art, including in a scenario where the target user has provided a limited number of gestures. We then develop a regularised autoencoder model for generating synthetic user-specific gestures. We show that using these gestures in training improve
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#23545;&#22810;&#65288;OvO&#65289;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#36229;&#36807;&#19977;&#20010;&#27169;&#24577;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05435</link><description>&lt;p&gt;
One-Versus-Others Attention: &#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
One-Versus-Others Attention: Scalable Multimodal Integration. (arXiv:2307.05435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05435
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#23545;&#22810;&#65288;OvO&#65289;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#36229;&#36807;&#19977;&#20010;&#27169;&#24577;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#23398;&#20064;&#27169;&#22411;&#22312;&#38382;&#39064;&#22238;&#31572;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#21508;&#31181;&#20219;&#21153;&#19978;&#36229;&#36234;&#21333;&#27169;&#24577;&#26041;&#27861;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20165;&#20851;&#27880;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#65292;&#20854;&#20013;&#27169;&#24577;&#25968;&#36890;&#24120;&#23569;&#20110;&#22235;&#20010;&#65288;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#20182;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#39046;&#22495;&#65292;&#25968;&#25454;&#36755;&#20837;&#21487;&#33021;&#21253;&#25324;X&#23556;&#32447;&#12289;PET&#25195;&#25551;&#12289;MRI&#12289;&#36951;&#20256;&#31579;&#26597;&#12289;&#20020;&#24202;&#31508;&#35760;&#31561;&#65292;&#36825;&#23601;&#38656;&#35201;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20381;&#36182;&#20110;&#20004;&#20004;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#65292;&#20294;&#23545;&#20110;&#36229;&#36807;&#19977;&#20010;&#27169;&#24577;&#30340;&#24212;&#29992;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20250;&#24456;&#22909;&#22320;&#25193;&#23637;&#12290;&#23545;&#20110;$n$&#20010;&#27169;&#24577;&#65292;&#35745;&#31639;&#27880;&#24847;&#21147;&#23558;&#23548;&#33268;$n \choose 2$&#30340;&#22797;&#26434;&#24230;&#65292;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#20013;&#31435;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21363;&#19968;&#23545;&#22810;&#65288;OvO&#65289;&#27880;&#24847;&#21147;&#65292;&#35813;&#26426;&#21046;&#38543;&#30528;&#27169;&#24577;&#25968;&#37327;&#32447;&#24615;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning models have become increasingly important as they surpass single-modality approaches on diverse tasks ranging from question-answering to autonomous driving. Despite the importance of multimodal learning, existing efforts focus on NLP applications, where the number of modalities is typically less than four (audio, video, text, images). However, data inputs in other domains, such as the medical field, may include X-rays, PET scans, MRIs, genetic screening, clinical notes, and more, creating a need for both efficient and accurate information fusion. Many state-of-the-art models rely on pairwise cross-modal attention, which does not scale well for applications with more than three modalities. For $n$ modalities, computing attention will result in $n \choose 2$ operations, potentially requiring considerable amounts of computational resources. To address this, we propose a new domain-neutral attention mechanism, One-Versus-Others (OvO) attention, that scales linearly with
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26446;&#23545;&#31216;&#23558;&#24322;&#26500;&#25968;&#25454;&#20013;&#30340;PDEs&#34920;&#31034;&#36827;&#34892;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;&#19981;&#21464;&#20219;&#21153;&#30340;&#24615;&#33021;&#24182;&#25913;&#36827;&#20102;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#26102;&#38388;&#25512;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05432</link><description>&lt;p&gt;
&#21033;&#29992;&#26446;&#23545;&#31216;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning with Lie Symmetries for Partial Differential Equations. (arXiv:2307.05432v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26446;&#23545;&#31216;&#23558;&#24322;&#26500;&#25968;&#25454;&#20013;&#30340;PDEs&#34920;&#31034;&#36827;&#34892;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;&#19981;&#21464;&#20219;&#21153;&#30340;&#24615;&#33021;&#24182;&#25913;&#36827;&#20102;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#26102;&#38388;&#25512;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#26041;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#20026;&#25968;&#20540;&#27714;&#35299;&#22120;&#25552;&#20379;&#20102;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#33021;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20135;&#29983;&#24191;&#27867;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#26045;&#32852;&#21512;&#23884;&#20837;&#26041;&#27861;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26694;&#26550;&#65292;&#20174;&#24322;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;PDEs&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#30340;&#34920;&#31034;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#22312;&#19981;&#21464;&#20219;&#21153;&#65288;&#22914;&#22238;&#24402;PDE&#30340;&#31995;&#25968;&#65289;&#19978;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#25552;&#39640;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#26102;&#38388;&#25512;&#36827;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#22312;&#26410;&#26469;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning for differential equations paves the way for computationally efficient alternatives to numerical solvers, with potentially broad impacts in science and engineering. Though current algorithms typically require simulated training data tailored to a given setting, one may instead wish to learn useful information from heterogeneous sources, or from real dynamical systems observations that are messy or incomplete. In this work, we learn general-purpose representations of PDEs from heterogeneous data by implementing joint embedding methods for self-supervised learning (SSL), a framework for unsupervised representation learning that has had notable success in computer vision. Our representation outperforms baseline approaches to invariant tasks, such as regressing the coefficients of a PDE, while also improving the time-stepping performance of neural solvers. We hope that our proposed methodology will prove useful in the eventual development of general-purpose foundation mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#26080;&#38480;&#32500;&#24314;&#27169;&#65292;&#24182;&#24341;&#20837;&#20960;&#20309;&#20808;&#39564;&#20197;&#22788;&#29702;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#24102;&#26377;&#23545;&#31216;&#24615;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#23545;&#31216;&#32676;&#21464;&#25442;&#30340;&#20960;&#20309;&#39640;&#26031;&#36807;&#31243;&#21644;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#24471;&#20998;&#65292;&#29983;&#25104;&#20989;&#25968;&#27169;&#22411;&#20063;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05431</link><description>&lt;p&gt;
&#20960;&#20309;&#31070;&#32463;&#25193;&#25955;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Geometric Neural Diffusion Processes. (arXiv:2307.05431v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#26080;&#38480;&#32500;&#24314;&#27169;&#65292;&#24182;&#24341;&#20837;&#20960;&#20309;&#20808;&#39564;&#20197;&#22788;&#29702;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#24102;&#26377;&#23545;&#31216;&#24615;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#23545;&#31216;&#32676;&#21464;&#25442;&#30340;&#20960;&#20309;&#39640;&#26031;&#36807;&#31243;&#21644;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#24471;&#20998;&#65292;&#29983;&#25104;&#20989;&#25968;&#27169;&#22411;&#20063;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#29983;&#25104;&#24314;&#27169;&#33539;&#24335;&#12290;&#26368;&#36817;&#23558;&#20854;&#25193;&#23637;&#21040;&#26080;&#38480;&#32500;&#27431;&#27663;&#31354;&#38388;&#20351;&#24471;&#21487;&#20197;&#23545;&#38543;&#26426;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#31185;&#23398;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#37117;&#28041;&#21450;&#23545;&#31216;&#24615;&#21644;&#23384;&#22312;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#25193;&#23637;&#21040;&#26080;&#38480;&#32500;&#24314;&#27169;&#20013;&#24341;&#20837;&#19968;&#31995;&#21015;&#20960;&#20309;&#20808;&#39564;&#12290;&#25105;&#20204;&#36890;&#36807; a) &#26500;&#24314;&#19968;&#20010;&#22122;&#22768;&#36807;&#31243;&#65292;&#20854;&#26497;&#38480;&#20998;&#24067;&#26159;&#22312;&#24863;&#20852;&#36259;&#30340;&#23545;&#31216;&#32676;&#19979;&#21464;&#25442;&#30340;&#20960;&#20309;&#39640;&#26031;&#36807;&#31243;&#65292;&#24182; b) &#20351;&#29992;&#23545;&#36825;&#20010;&#32676;&#20855;&#26377;&#31561;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;&#24471;&#20998;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#29983;&#25104;&#20989;&#25968;&#27169;&#22411;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110; Langevin &#30340;&#26465;&#20214;&#37319;&#26679;&#22120;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23481;&#37327;&#24615;&#65292;&#20197;&#36866;&#24212;&#22797;&#26434;&#30340;&#26631;&#37327;&#21644;&#21521;&#37327;&#22330;&#65292;&#36825;&#20123;&#22330;&#23384;&#22312;&#20110;&#27431;&#27663;&#31354;&#38388;&#21644;&#29699;&#24418;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models have proven to be a flexible and effective paradigm for generative modelling. Their recent extension to infinite dimensional Euclidean spaces has allowed for the modelling of stochastic processes. However, many problems in the natural sciences incorporate symmetries and involve data living in non-Euclidean spaces. In this work, we extend the framework of diffusion models to incorporate a series of geometric priors in infinite-dimension modelling. We do so by a) constructing a noising process which admits, as limiting distribution, a geometric Gaussian process that transforms under the symmetry group of interest, and b) approximating the score with a neural network that is equivariant w.r.t. this group. We show that with these conditions, the generative functional model admits the same symmetry. We demonstrate scalability and capacity of the model, using a novel Langevin-based conditional sampler, to fit complex scalar and vector fields, with Euclidean and sph
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;BOLD-fMRI&#35745;&#31639;&#20154;&#31867;&#36830;&#25509;&#32452;&#21457;&#23637;&#38431;&#21015;&#20013;&#21628;&#21560;&#20307;&#31215;&#19982;&#26102;&#38388;&#65288;RTV&#65289;&#21644;&#21628;&#21560;&#21464;&#24322;&#65288;RV&#65289;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#21487;&#20197;&#20174;&#38745;&#24687;&#29366;&#24577;&#30340;BOLD&#20449;&#21495;&#20013;&#25429;&#25417;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24182;&#37325;&#24314;&#30495;&#23454;&#30340;&#21628;&#21560;&#26102;&#24207;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;fMRI&#30740;&#31350;&#30340;&#25104;&#26412;&#65292;&#24182;&#20943;&#36731;&#21442;&#19982;&#32773;&#30340;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2307.05426</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;BOLD-fMRI&#35745;&#31639;&#20154;&#31867;&#36830;&#25509;&#32452;&#21457;&#23637;&#38431;&#21015;&#20013;&#30340;&#21628;&#21560;&#20307;&#31215;&#19982;&#26102;&#38388;&#65288;RTV&#65289;&#21644;&#21628;&#21560;&#21464;&#24322;&#65288;RV&#65289;
&lt;/p&gt;
&lt;p&gt;
Using BOLD-fMRI to Compute the Respiration Volume per Time (RTV) and Respiration Variation (RV) with Convolutional Neural Networks (CNN) in the Human Connectome Development Cohort. (arXiv:2307.05426v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;BOLD-fMRI&#35745;&#31639;&#20154;&#31867;&#36830;&#25509;&#32452;&#21457;&#23637;&#38431;&#21015;&#20013;&#21628;&#21560;&#20307;&#31215;&#19982;&#26102;&#38388;&#65288;RTV&#65289;&#21644;&#21628;&#21560;&#21464;&#24322;&#65288;RV&#65289;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#21487;&#20197;&#20174;&#38745;&#24687;&#29366;&#24577;&#30340;BOLD&#20449;&#21495;&#20013;&#25429;&#25417;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24182;&#37325;&#24314;&#30495;&#23454;&#30340;&#21628;&#21560;&#26102;&#24207;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;fMRI&#30740;&#31350;&#30340;&#25104;&#26412;&#65292;&#24182;&#20943;&#36731;&#21442;&#19982;&#32773;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;fMRI&#30740;&#31350;&#20013;&#65292;&#21628;&#21560;&#20449;&#21495;&#19981;&#21487;&#29992;&#25110;&#36136;&#37327;&#19981;&#21487;&#25509;&#21463;&#12290;&#22240;&#27492;&#65292;&#26080;&#27861;&#30452;&#25509;&#20174;BOLD&#20449;&#21495;&#20013;&#21435;&#38500;&#20302;&#39057;&#21628;&#21560;&#21464;&#21270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#32500;CNN&#27169;&#22411;&#65292;&#29992;&#20110;&#37325;&#24314;&#20004;&#20010;&#21628;&#21560;&#27979;&#37327;&#25351;&#26631;&#65292;RV&#21644;RVT&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#21487;&#20197;&#20174;&#38745;&#24687;&#29366;&#24577;&#30340;BOLD&#20449;&#21495;&#20013;&#25429;&#25417;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24182;&#37325;&#24314;&#30495;&#23454;&#30340;RV&#21644;RVT&#26102;&#24207;&#12290;&#39044;&#35745;&#36825;&#31181;&#26041;&#27861;&#30340;&#24212;&#29992;&#23558;&#38477;&#20302;fMRI&#30740;&#31350;&#30340;&#25104;&#26412;&#65292;&#20943;&#23569;&#22797;&#26434;&#24615;&#65292;&#24182;&#20943;&#36731;&#21442;&#19982;&#32773;&#30340;&#36127;&#25285;&#65292;&#22240;&#20026;&#20182;&#20204;&#19981;&#38656;&#35201;&#20329;&#25140;&#21628;&#21560;&#36125;&#27931;&#26031;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many fMRI studies, respiratory signals are unavailable or do not have acceptable quality. Consequently, the direct removal of low-frequency respiratory variations from BOLD signals is not possible. This study proposes a one-dimensional CNN model for reconstruction of two respiratory measures, RV and RVT. Results show that a CNN can capture informative features from resting BOLD signals and reconstruct realistic RV and RVT timeseries. It is expected that application of the proposed method will lower the cost of fMRI studies, reduce complexity, and decrease the burden on participants as they will not be required to wear a respiratory bellows.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#40657;&#30418;&#24773;&#20917;&#19979;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21518;&#38376;&#25915;&#20987;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#35302;&#21457;&#22120;&#21644;&#33391;&#24615;&#29305;&#24449;&#23545;&#30830;&#23450;&#21518;&#38376;&#32593;&#32476;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#22120;&#36827;&#34892;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.05422</link><description>&lt;p&gt;
&#40657;&#30418;DNN&#21518;&#38376;&#26816;&#27979;&#30340;&#35302;&#21457;&#22120;&#21644;&#33391;&#24615;&#29305;&#24449;&#30340;&#24046;&#24322;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection. (arXiv:2307.05422v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#40657;&#30418;&#24773;&#20917;&#19979;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21518;&#38376;&#25915;&#20987;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#35302;&#21457;&#22120;&#21644;&#33391;&#24615;&#29305;&#24449;&#23545;&#30830;&#23450;&#21518;&#38376;&#32593;&#32476;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#22120;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#40657;&#30418;&#24773;&#20917;&#19979;&#65292;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#30340;&#24605;&#36335;&#26159;&#65292;&#19982;&#20854;&#20182;&#33391;&#24615;&#29305;&#24449;&#30456;&#27604;&#65292;&#19982;&#35302;&#21457;&#22120;&#30456;&#20851;&#30340;&#29305;&#24449;&#23545;&#30830;&#23450;&#21518;&#38376;&#32593;&#32476;&#36755;&#20986;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;&#20026;&#20102;&#23450;&#37327;&#34913;&#37327;&#35302;&#21457;&#22120;&#21644;&#33391;&#24615;&#29305;&#24449;&#23545;&#30830;&#23450;&#21518;&#38376;&#32593;&#32476;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20116;&#20010;&#24230;&#37327;&#25351;&#26631;&#12290;&#20026;&#20102;&#35745;&#31639;&#32473;&#23450;&#36755;&#20837;&#30340;&#20116;&#20010;&#24230;&#37327;&#20540;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23558;&#36755;&#20837;&#30340;&#37096;&#20998;&#20869;&#23481;&#27880;&#20837;&#21040;&#24178;&#20928;&#30340;&#39564;&#35777;&#26679;&#26412;&#20013;&#29983;&#25104;&#20960;&#20010;&#21512;&#25104;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#30456;&#24212;&#21512;&#25104;&#26679;&#26412;&#30340;&#36755;&#20986;&#26631;&#31614;&#35745;&#31639;&#20986;&#20116;&#20010;&#24230;&#37327;&#25351;&#26631;&#12290;&#26412;&#25991;&#30340;&#19968;&#20010;&#36129;&#29486;&#26159;&#20351;&#29992;&#20102;&#19968;&#20010;&#23567;&#22411;&#30340;&#24178;&#20928;&#39564;&#35777;&#25968;&#25454;&#38598;&#12290;&#22312;&#35745;&#31639;&#20986;&#20116;&#20010;&#24230;&#37327;&#20540;&#21518;&#65292;&#25105;&#20204;&#20174;&#39564;&#35777;&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20986;&#20116;&#20010;&#26032;&#39062;&#24615;&#26816;&#27979;&#22120;&#12290;&#19968;&#20010;&#20803;&#26032;&#39062;&#24615;&#26816;&#27979;&#22120;&#23558;&#20116;&#20010;&#35757;&#32451;&#22909;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#22120;&#30340;&#36755;&#20986;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a data-efficient detection method for deep neural networks against backdoor attacks under a black-box scenario. The proposed approach is motivated by the intuition that features corresponding to triggers have a higher influence in determining the backdoored network output than any other benign features. To quantitatively measure the effects of triggers and benign features on determining the backdoored network output, we introduce five metrics. To calculate the five-metric values for a given input, we first generate several synthetic samples by injecting the input's partial contents into clean validation samples. Then, the five metrics are computed by using the output labels of the corresponding synthetic samples. One contribution of this work is the use of a tiny clean validation dataset. Having the computed five metrics, five novelty detectors are trained from the validation dataset. A meta novelty detector fuses the output of the five trained novelty detectors to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#25552;&#20379;&#30340;&#20998;&#25968;&#26469;&#25913;&#36827;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#39304;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20998;&#25968;&#20195;&#26367;&#25104;&#23545;&#20559;&#22909;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#20998;&#25968;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.05405</link><description>&lt;p&gt;
&#25552;&#21319;&#33258;&#36866;&#24212;&#23398;&#20064;&#20998;&#25968;&#26469;&#22686;&#21152;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#39304;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores. (arXiv:2307.05405v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#25552;&#20379;&#30340;&#20998;&#25968;&#26469;&#25913;&#36827;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#39304;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20998;&#25968;&#20195;&#26367;&#25104;&#23545;&#20559;&#22909;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#20998;&#25968;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#22312;&#23398;&#20064;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#22823;&#37327;&#30340;&#20132;&#20114;&#21453;&#39304;&#65292;&#36825;&#19968;&#36807;&#31243;&#21487;&#33021;&#38656;&#35201;&#20154;&#24037;&#21442;&#19982;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#25552;&#20379;&#30340;&#20998;&#25968;&#32780;&#19981;&#26159;&#25104;&#23545;&#20559;&#22909;&#65292;&#26469;&#25552;&#39640;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#39304;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#20998;&#25968;&#21487;&#20197;&#20135;&#29983;&#27604;&#25104;&#23545;&#20559;&#22909;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#35201;&#27714;&#25945;&#24072;&#19982;&#20195;&#29702;&#20132;&#20114;&#35780;&#20998;&#20840;&#38754;&#30340;&#36712;&#36857;&#26469;&#35757;&#32451;&#34892;&#20026;&#31574;&#30053;&#12290;&#20026;&#20102;&#36991;&#20813;&#20154;&#31867;&#32473;&#20986;&#30340;&#19981;&#31283;&#23450;&#20998;&#25968;&#23545;&#35757;&#32451;&#36807;&#31243;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#12290;&#36825;&#20351;&#24471;&#23398;&#20064;&#33539;&#24335;&#23545;&#20110;&#19981;&#23436;&#32654;&#25110;&#19981;&#21487;&#38752;&#30340;&#20998;&#25968;&#19981;&#25935;&#24863;&#12290;&#25105;&#20204;&#23545;&#26426;&#22120;&#20154;&#36816;&#21160;&#21644;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#20998;&#25968;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive reinforcement learning has shown promise in learning complex robotic tasks. However, the process can be human-intensive due to the requirement of large amount of interactive feedback. This paper presents a new method that uses scores provided by humans, instead of pairwise preferences, to improve the feedback efficiency of interactive reinforcement learning. Our key insight is that scores can yield significantly more data than pairwise preferences. Specifically, we require a teacher to interactively score the full trajectories of an agent to train a behavioral policy in a sparse reward environment. To avoid unstable scores given by human negatively impact the training process, we propose an adaptive learning scheme. This enables the learning paradigm to be insensitive to imperfect or unreliable scores. We extensively evaluate our method on robotic locomotion and manipulation tasks. The results show that the proposed method can efficiently learn near-optimal policies by adap
&lt;/p&gt;</description></item><item><title>&#22312;&#25991;&#26723;&#22788;&#29702;&#24179;&#21488;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#21333;&#29420;&#21576;&#29616;&#26102;&#35757;&#32451;&#39640;&#24615;&#33021;&#20998;&#31867;&#22120;&#65292;&#26080;&#38656;&#20351;&#29992;&#35760;&#24518;&#32531;&#20914;&#21306;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#21442;&#32771;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05399</link><description>&lt;p&gt;
&#22312;&#25991;&#26723;&#22788;&#29702;&#24179;&#21488;&#20013;&#29992;&#20110;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#30340;&#39046;&#22495;&#26080;&#20851;&#31070;&#32463;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Neural Architecture for Class Incremental Continual Learning in Document Processing Platform. (arXiv:2307.05399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05399
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26723;&#22788;&#29702;&#24179;&#21488;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#21333;&#29420;&#21576;&#29616;&#26102;&#35757;&#32451;&#39640;&#24615;&#33021;&#20998;&#31867;&#22120;&#65292;&#26080;&#38656;&#20351;&#29992;&#35760;&#24518;&#32531;&#20914;&#21306;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#21442;&#32771;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#29983;&#20135;&#37096;&#32626;&#35201;&#27714;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#23545;&#22810;&#20010;&#20219;&#21153;&#39640;&#25928;&#21487;&#29992;&#12290;&#29305;&#21035;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#20197;&#27969;&#24335;&#26041;&#24335;&#21040;&#36798;&#65292;&#24182;&#19988;&#27599;&#20010;&#31867;&#21035;&#21333;&#29420;&#21576;&#29616;&#12290;&#26368;&#36817;&#30340;&#38543;&#26426;&#26799;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#25110;&#32773;&#23384;&#22312;&#35832;&#22914;&#20869;&#23384;&#32531;&#20914;&#21306;&#30340;&#38480;&#21046;&#65292;&#19981;&#33021;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#20840;&#21487;&#24494;&#26550;&#26500;&#65292;&#21487;&#20197;&#22312;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#21333;&#29420;&#21576;&#29616;&#26102;&#35757;&#32451;&#39640;&#24615;&#33021;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#21644;&#22312;&#32447;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#22312;&#27809;&#26377;&#35760;&#24518;&#32531;&#20914;&#21306;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;SOTA&#32467;&#26524;&#65292;&#24182;&#26126;&#26174;&#20248;&#20110;&#21442;&#32771;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Production deployments in complex systems require ML architectures to be highly efficient and usable against multiple tasks. Particularly demanding are classification problems in which data arrives in a streaming fashion and each class is presented separately. Recent methods with stochastic gradient learning have been shown to struggle in such setups or have limitations like memory buffers, and being restricted to specific domains that disable its usage in real-world scenarios. For this reason, we present a fully differentiable architecture based on the Mixture of Experts model, that enables the training of high-performance classifiers when examples from each class are presented separately. We conducted exhaustive experiments that proved its applicability in various domains and ability to learn online in production environments. The proposed technique achieves SOTA results without a memory buffer and clearly outperforms the reference methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Simplicial&#20449;&#24687;&#20256;&#36882;&#65288;SMP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#25429;&#25417;&#20998;&#23376;&#20013;&#38544;&#34255;&#30340;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#22312;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.05392</link><description>&lt;p&gt;
&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#30340;Simplicial&#20449;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Simplicial Message Passing for Chemical Property Prediction. (arXiv:2307.05392v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05392
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Simplicial&#20449;&#24687;&#20256;&#36882;&#65288;SMP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#25429;&#25417;&#20998;&#23376;&#20013;&#38544;&#34255;&#30340;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#22312;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#20026;&#22788;&#29702;&#20998;&#23376;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#65292;&#24182;&#22312;&#20419;&#36827;&#21457;&#29616;&#21644;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#26448;&#26009;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#30340;MPNN&#26041;&#27861;&#22312;&#25429;&#25417;&#20998;&#23376;&#32467;&#26500;&#20013;&#38544;&#34255;&#30340;&#24378;&#25299;&#25169;&#20449;&#24687;&#65288;&#22914;&#38750;&#21516;&#26500;&#22270;&#65289;&#26041;&#38754;&#20063;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Simplicial&#20449;&#24687;&#20256;&#36882;&#65288;SMP&#65289;&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20998;&#23376;&#20013;&#30340;&#25299;&#25169;&#20449;&#24687;&#65292;&#20174;&#32780;&#31361;&#30772;&#20102;&#20256;&#32479;&#20449;&#24687;&#20256;&#36882;&#33539;&#24335;&#30340;&#23616;&#38480;&#12290;&#22312;SMP&#20013;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24191;&#20041;&#20449;&#24687;&#20256;&#36882;&#26694;&#26550;&#65292;&#29992;&#20110;&#32858;&#38598;&#20219;&#24847;&#27425;&#24207;&#30340;simplicial&#22797;&#21512;&#20307;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#35814;&#32454;&#38416;&#36848;&#20102;&#19968;&#20010;&#20998;&#23618;&#32467;&#26500;&#65292;&#20801;&#35768;&#19981;&#21516;&#27425;&#24207;&#30340;simplices&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#27969;&#12290;&#25105;&#20204;&#23558;SMP&#26694;&#26550;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#65292;&#24182;&#21462;&#24471;&#20102;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, message-passing Neural networks (MPNN) provide a promising tool for dealing with molecular graphs and have achieved remarkable success in facilitating the discovery and materials design with desired properties. However, the classical MPNN methods also suffer from a limitation in capturing the strong topological information hidden in molecular structures, such as nonisomorphic graphs. To address this problem, this work proposes a Simplicial Message Passing (SMP) framework to better capture the topological information from molecules, which can break through the limitation within the vanilla message-passing paradigm. In SMP, a generalized message-passing framework is established for aggregating the information from arbitrary-order simplicial complex, and a hierarchical structure is elaborated to allow information exchange between different order simplices. We apply the SMP framework within deep learning architectures for quantum-chemical properties prediction and achieve state-o
&lt;/p&gt;</description></item><item><title>CrysMMNet&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#24314;&#27169;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#26469;&#39044;&#27979;&#26230;&#20307;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05390</link><description>&lt;p&gt;
CrysMMNet: &#22810;&#27169;&#24577;&#34920;&#31034;&#29992;&#20110;&#26230;&#20307;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CrysMMNet: Multimodal Representation for Crystal Property Prediction. (arXiv:2307.05390v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05390
&lt;/p&gt;
&lt;p&gt;
CrysMMNet&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#24314;&#27169;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#26469;&#39044;&#27979;&#26230;&#20307;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#24555;&#36895;&#20934;&#30830;&#39044;&#27979;&#19981;&#21516;&#26230;&#20307;&#24615;&#36136;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20381;&#36182;&#20110;&#26230;&#20307;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#24577;&#65292;&#21363;&#26230;&#20307;&#22270;&#32467;&#26500;&#65292;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#24314;&#31435;&#30456;&#37051;&#21407;&#23376;&#20043;&#38388;&#30340;&#36793;&#65292;&#24182;&#24212;&#29992;GNN&#23398;&#20064;&#26448;&#26009;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#25104;&#21151;&#22320;&#32534;&#30721;&#20102;&#21407;&#23376;&#21608;&#22260;&#30340;&#23616;&#37096;&#21270;&#23398;&#35821;&#20041;&#65292;&#20294;&#26410;&#33021;&#25429;&#25417;&#21040;&#24433;&#21709;&#19981;&#21516;&#26230;&#20307;&#23646;&#24615;&#30340;&#37325;&#35201;&#30340;&#20840;&#23616;&#21608;&#26399;&#32467;&#26500;&#20449;&#24687;&#65292;&#22914;&#31354;&#38388;&#32676;&#25968;&#12289;&#26230;&#20307;&#23545;&#31216;&#24615;&#12289;&#26059;&#36716;&#20449;&#24687;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26448;&#26009;&#30340;&#25991;&#26412;&#25551;&#36848;&#23558;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#24314;&#27169;&#20026;&#22270;&#32467;&#26500;&#65292;&#24182;&#23398;&#20064;&#26230;&#20307;&#26448;&#26009;&#26356;&#21152;&#31283;&#20581;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25972;&#29702;&#20102;&#19968;&#20010;&#21253;&#21547;&#27599;&#20010;&#26448;&#26009;&#25551;&#36848;&#30340;&#26230;&#20307;&#26448;&#26009;&#25968;&#25454;&#24211;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CrysMMNet&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning models have emerged as a powerful tool for fast and accurate prediction of different crystalline properties. Exiting state-of-the-art models rely on a single modality of crystal data i.e. crystal graph structure, where they construct multi-graph by establishing edges between nearby atoms in 3D space and apply GNN to learn materials representation. Thereby, they encode local chemical semantics around the atoms successfully but fail to capture important global periodic structural information like space group number, crystal symmetry, rotational information, etc, which influence different crystal properties. In this work, we leverage textual descriptions of materials to model global structural information into graph structure and learn a more robust and enriched representation of crystalline materials. To this effect, we first curate a textual dataset for crystalline material databases containing descriptions of each material. Further, we propose CrysMMNet, a simple multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26680;&#25216;&#26415;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#21442;&#25968;&#36739;&#23569;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#20998;&#21106;PPG&#20449;&#21495;&#30340;&#36136;&#37327;&#21644;&#20266;&#24433;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05385</link><description>&lt;p&gt;
&#23398;&#20064;&#26680;&#25216;&#26415;&#29992;&#20110;&#21487;&#35299;&#37322;&#21644;&#39640;&#25928;&#30340;PPG&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#21644;&#20266;&#24433;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation. (arXiv:2307.05385v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26680;&#25216;&#26415;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#21442;&#25968;&#36739;&#23569;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#20998;&#21106;PPG&#20449;&#21495;&#30340;&#36136;&#37327;&#21644;&#20266;&#24433;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#23481;&#25239;(PPG)&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#26041;&#27861;&#26469;&#25345;&#32493;&#30417;&#27979;&#21508;&#31181;&#24515;&#34880;&#31649;&#21442;&#25968;&#12290;PPG&#20449;&#21495;&#30001;&#21487;&#31359;&#25140;&#35774;&#22791;&#20135;&#29983;&#65292;&#24120;&#24120;&#21253;&#21547;&#30001;&#22806;&#37096;&#22240;&#32032;(&#22914;&#20154;&#20307;&#36816;&#21160;)&#24341;&#36215;&#30340;&#22823;&#22411;&#20266;&#24433;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#29983;&#29702;&#21442;&#25968;&#36827;&#34892;&#31283;&#20581;&#21644;&#20934;&#30830;&#30340;&#25552;&#21462;&#65292;&#20449;&#21495;&#30340;&#25439;&#22351;&#21306;&#22495;&#38656;&#35201;&#34987;&#27491;&#30830;&#22320;&#35782;&#21035;&#21644;&#22788;&#29702;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20381;&#38752;&#25163;&#24037;&#29305;&#24449;&#26816;&#27979;&#22120;&#25110;&#20449;&#21495;&#24230;&#37327;&#65292;&#32467;&#26524;&#24615;&#33021;&#19981;&#20339;&#65292;&#25110;&#20381;&#38752;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#35745;&#31639;&#21644;&#20869;&#23384;&#23494;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#23567;&#32452;&#21487;&#35299;&#37322;&#30340;&#21367;&#31215;&#26680;&#65292;&#20854;&#24615;&#33021;&#19982;&#29616;&#26377;&#25216;&#26415;DNN&#26041;&#27861;&#30456;&#20284;&#65292;&#29978;&#33267;&#26356;&#22909;&#65292;&#32780;&#21442;&#25968;&#25968;&#37327;&#27604;DNN&#26041;&#27861;&#23569;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#39033;&#24037;&#20316;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#31283;&#20581;&#21644;&#21487;&#35299;&#37322;&#30340;PPG&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#21644;&#20266;&#24433;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmography (PPG) provides a low-cost, non-invasive method to continuously monitor various cardiovascular parameters. PPG signals are generated by wearable devices and frequently contain large artifacts caused by external factors, such as motion of the human subject. In order to ensure robust and accurate extraction of physiological parameters, corrupted areas of the signal need to be identified and handled appropriately. Previous methodology relied either on handcrafted feature detectors or signal metrics which yield sub-optimal performance, or relied on machine learning techniques such as deep neural networks (DNN) which lack interpretability and are computationally and memory intensive. In this work, we present a novel method to learn a small set of interpretable convolutional kernels that has performance similar to -- and often better than -- the state-of-the-art DNN approach with several orders of magnitude fewer parameters. This work allows for efficient, robust, and int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#23884;&#22871;&#26500;&#25104;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#40065;&#26834;&#29305;&#24449;&#23398;&#20064;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#30697;&#38453;&#27714;&#36870;&#25110;&#23567;&#25209;&#37327;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.05384</link><description>&lt;p&gt;
&#38543;&#26426;&#23884;&#22871;&#26500;&#25104;&#30340;&#21452;&#23618;&#20248;&#21270;&#29992;&#20110;&#40065;&#26834;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stochastic Nested Compositional Bi-level Optimization for Robust Feature Learning. (arXiv:2307.05384v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#23884;&#22871;&#26500;&#25104;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#40065;&#26834;&#29305;&#24449;&#23398;&#20064;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#30697;&#38453;&#27714;&#36870;&#25110;&#23567;&#25209;&#37327;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;&#35299;&#20915;&#23884;&#22871;&#26500;&#25104;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#12290;&#36825;&#20123;&#38382;&#39064;&#28041;&#21450;&#21040;&#19978;&#23618;&#30340;$T$&#20010;&#28508;&#22312;&#38750;&#20984;&#24179;&#28369;&#20989;&#25968;&#30340;&#23884;&#22871;&#26500;&#36896;&#65292;&#20197;&#21450;&#19979;&#23618;&#30340;&#24179;&#28369;&#19988;&#24378;&#20984;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#30697;&#38453;&#27714;&#36870;&#25110;&#23567;&#25209;&#37327;&#36755;&#20837;&#65292;&#24182;&#19988;&#21487;&#20197;&#20197;&#36817;&#20284;$\tilde{O}_T(1/\epsilon^{2})$&#30340;&#39044;&#31639;&#22797;&#26434;&#24230;&#23454;&#29616;$\epsilon$-&#31283;&#23450;&#35299;&#65292;&#20551;&#35774;&#33021;&#22815;&#24471;&#21040;&#19978;&#23618;&#32452;&#25104;&#20013;&#30340;&#20010;&#20307;&#20989;&#25968;&#21644;&#19979;&#23618;&#20989;&#25968;&#30340;&#38543;&#26426;&#19968;&#38454;&#35834;&#22467;&#23572;&#65292;&#36825;&#20123;&#19968;&#38454;&#35834;&#22467;&#23572;&#26159;&#26080;&#20559;&#19988;&#20855;&#26377;&#26377;&#30028;&#30697;&#12290;&#36825;&#37324;&#65292;$\tilde{O}_T$&#21487;&#20197;&#38544;&#34255;&#22810;&#39033;&#23545;&#25968;&#31995;&#25968;&#21644;&#24120;&#25968;&#65292;&#20381;&#36182;&#20110;$T$&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop and analyze stochastic approximation algorithms for solving nested compositional bi-level optimization problems. These problems involve a nested composition of $T$ potentially non-convex smooth functions in the upper-level, and a smooth and strongly convex function in the lower-level. Our proposed algorithm does not rely on matrix inversions or mini-batches and can achieve an $\epsilon$-stationary solution with an oracle complexity of approximately $\tilde{O}_T(1/\epsilon^{2})$, assuming the availability of stochastic first-order oracles for the individual functions in the composition and the lower-level, which are unbiased and have bounded moments. Here, $\tilde{O}_T$ hides polylog factors and constants that depend on $T$. The key challenge we address in establishing this result relates to handling three distinct sources of bias in the stochastic gradients. The first source arises from the compositional nature of the upper-level, the second stems from the bi-level structure
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#36873;&#25321;&#30340;&#30382;&#32932;&#30005;&#21453;&#24212;&#20449;&#21495;&#29305;&#24449;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#20154;&#31867;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20154;&#31867;&#24773;&#32490;&#35782;&#21035;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.05383</link><description>&lt;p&gt;
&#22522;&#20110;&#30382;&#32932;&#30005;&#21453;&#24212;&#20449;&#21495;&#29305;&#24449;&#36873;&#25321;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#20154;&#31867;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Human Emotion Recognition Based On Galvanic Skin Response signal Feature Selection and SVM. (arXiv:2307.05383v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#36873;&#25321;&#30340;&#30382;&#32932;&#30005;&#21453;&#24212;&#20449;&#21495;&#29305;&#24449;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#20154;&#31867;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20154;&#31867;&#24773;&#32490;&#35782;&#21035;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#36873;&#25321;&#30340;&#30382;&#32932;&#30005;&#21453;&#24212;&#65288;GSR&#65289;&#20449;&#21495;&#29305;&#24449;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#30340;&#26032;&#22411;&#20154;&#31867;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#12290;&#36890;&#36807; e-Health Sensor Platform V2.0 &#33719;&#21462;&#20102;&#30382;&#32932;&#30005;&#21453;&#24212;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#23567;&#27874;&#20989;&#25968;&#23545;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#21644;&#24402;&#19968;&#21270;&#20197;&#28040;&#38500;&#20010;&#20307;&#24046;&#24322;&#12290;&#20174;&#24402;&#19968;&#21270;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#20102;30&#20010;&#29305;&#24449;&#65292;&#20294;&#30452;&#25509;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#20250;&#23548;&#33268;&#35782;&#21035;&#29575;&#36739;&#20302;&#12290;&#20026;&#20102;&#33719;&#24471;&#20248;&#21270;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21327;&#26041;&#24046;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#21253;&#21547;&#20248;&#21270;&#29305;&#24449;&#30340;SVM&#23454;&#29616;&#20102;&#20154;&#31867;&#24773;&#32490;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#33391;&#22909;&#30340;&#20154;&#31867;&#24773;&#32490;&#35782;&#21035;&#65292;&#35782;&#21035;&#20934;&#30830;&#29575;&#36229;&#36807;66.67%&#12290;
&lt;/p&gt;
&lt;p&gt;
A novel human emotion recognition method based on automatically selected Galvanic Skin Response (GSR) signal features and SVM is proposed in this paper. GSR signals were acquired by e-Health Sensor Platform V2.0. Then, the data is de-noised by wavelet function and normalized to get rid of the individual difference. 30 features are extracted from the normalized data, however, directly using of these features will lead to a low recognition rate. In order to gain the optimized features, a covariance based feature selection is employed in our method. Finally, a SVM with input of the optimized features is utilized to achieve the human emotion recognition. The experimental results indicate that the proposed method leads to good human emotion recognition, and the recognition accuracy is more than 66.67%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STATENet&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#22312;&#26102;&#31354;&#21644;&#27169;&#22411;&#23618;&#38754;&#19978;&#35299;&#20915;&#20102;&#26032;&#29983;&#20799;&#30315;&#30187;&#26816;&#27979;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#33021;&#26174;&#33879;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05382</link><description>&lt;p&gt;
&#20445;&#25252;&#26410;&#26469;: &#22522;&#20110;&#26102;&#31354;&#24314;&#27169;&#30340;&#26032;&#29983;&#20799;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Protecting the Future: Neonatal Seizure Detection with Spatial-Temporal Modeling. (arXiv:2307.05382v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STATENet&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#22312;&#26102;&#31354;&#21644;&#27169;&#22411;&#23618;&#38754;&#19978;&#35299;&#20915;&#20102;&#26032;&#29983;&#20799;&#30315;&#30187;&#26816;&#27979;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#33021;&#26174;&#33879;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26032;&#29983;&#20799;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65292;&#21450;&#26102;&#26816;&#27979;&#20855;&#26377;&#30005;&#33041;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#26032;&#29983;&#20799;&#30340;&#30315;&#30187;&#21457;&#20316;&#26159;&#19968;&#39033;&#24120;&#35265;&#20294;&#33021;&#25327;&#25937;&#29983;&#21629;&#30340;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#23454;&#26102;&#30417;&#27979;&#38656;&#35201;&#20154;&#21147;&#22823;&#37327;&#25237;&#20837;&#65292;&#22240;&#27492;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#26032;&#29983;&#20799;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#38024;&#23545;&#25104;&#20154;&#30315;&#30187;&#30417;&#27979;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#36890;&#24120;&#20250;&#22240;&#20026;&#20197;&#19979;&#21407;&#22240;&#32780;&#22833;&#36133;&#65306;&#65288;i&#65289;&#20154;&#33041;&#20013;&#30315;&#30187;&#21457;&#20316;&#36215;&#22987;&#20301;&#32622;&#30340;&#21160;&#24577;&#21464;&#21270;&#65307;&#65288;ii&#65289;&#26032;&#29983;&#20799;&#33041;&#30005;&#22270;&#30340;&#19981;&#21516;&#30005;&#26497;&#37197;&#32622;&#20197;&#21450;&#65288;iii&#65289;&#19981;&#21516;&#21463;&#35797;&#23545;&#35937;&#20043;&#38388;&#30340;&#24040;&#22823;&#20998;&#24067;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STATENet&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#22312;&#26102;&#31354;&#21644;&#27169;&#22411;&#23618;&#38754;&#19978;&#35299;&#20915;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#23545;&#30495;&#23454;&#30340;&#22823;&#35268;&#27169;&#26032;&#29983;&#20799;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#30315;&#30187;&#26816;&#27979;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
A timely detection of seizures for newborn infants with electroencephalogram (EEG) has been a common yet life-saving practice in the Neonatal Intensive Care Unit (NICU). However, it requires great human efforts for real-time monitoring, which calls for automated solutions to neonatal seizure detection. Moreover, the current automated methods focusing on adult epilepsy monitoring often fail due to (i) dynamic seizure onset location in human brains; (ii) different montages on neonates and (iii) huge distribution shift among different subjects. In this paper, we propose a deep learning framework, namely STATENet, to address the exclusive challenges with exquisite designs at the temporal, spatial and model levels. The experiments over the real-world large-scale neonatal EEG dataset illustrate that our framework achieves significantly better seizure detection performance.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;GPU&#20248;&#21270;&#20013;&#29983;&#25104;&#26230;&#20307;&#26448;&#26009;&#30340;&#22270;&#24418;&#34920;&#31034;&#30340;&#39640;&#25928;&#24037;&#20855;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;Pytorch&#20860;&#23481;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#23454;&#26102;&#29983;&#25104;&#22270;&#65292;&#25105;&#20204;&#30340;&#24037;&#20855;&#33021;&#22815;&#26356;&#26032;&#32467;&#26500;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#22788;&#29702;&#26356;&#26032;&#21518;&#30340;&#22270;&#12290;</title><link>http://arxiv.org/abs/2307.05380</link><description>&lt;p&gt;
&#20248;&#21270;&#30340;&#26230;&#20307;&#23398;&#22270;&#29983;&#25104;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
Optimized Crystallographic Graph Generation for Material Science. (arXiv:2307.05380v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05380
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;GPU&#20248;&#21270;&#20013;&#29983;&#25104;&#26230;&#20307;&#26448;&#26009;&#30340;&#22270;&#24418;&#34920;&#31034;&#30340;&#39640;&#25928;&#24037;&#20855;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;Pytorch&#20860;&#23481;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#23454;&#26102;&#29983;&#25104;&#22270;&#65292;&#25105;&#20204;&#30340;&#24037;&#20855;&#33021;&#22815;&#26356;&#26032;&#32467;&#26500;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#22788;&#29702;&#26356;&#26032;&#21518;&#30340;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24212;&#29992;&#20110;&#21270;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#21457;&#29616;&#20013;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26230;&#20307;&#26448;&#26009;&#32780;&#35328;&#65292;&#20174;&#20960;&#20309;&#20449;&#24687;&#29983;&#25104;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#26230;&#20307;&#30340;&#21608;&#26399;&#24615;&#38656;&#35201;&#22312;&#22823;&#35268;&#27169;&#24182;&#34892;&#29615;&#22659;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#23454;&#26102;&#22788;&#29702;&#12290;&#20026;&#20102;&#35757;&#32451;&#22522;&#20110;&#22270;&#30340;&#26448;&#26009;&#21457;&#29616;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;GPU&#20248;&#21270;&#20013;&#29983;&#25104;&#25130;&#26029;&#22270;&#21644;k&#26368;&#36817;&#37051;&#22270;&#30340;&#39640;&#25928;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;Pytorch&#20860;&#23481;&#30340;pyMatGraph&#26694;&#26550;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35757;&#32451;&#26399;&#38388;&#23454;&#26102;&#29983;&#25104;&#22270;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#21487;&#20197;&#26356;&#26032;&#19968;&#20010;&#32467;&#26500;&#30340;&#22270;&#65292;&#20351;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#26356;&#26032;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#22312;GPU&#19978;&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#26102;&#22788;&#29702;&#26356;&#26032;&#21518;&#30340;&#22270;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110;https://github.com/aklipf/mat
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are widely used in machine learning applied to chemistry, and in particular for material science discovery. For crystalline materials, however, generating graph-based representation from geometrical information for neural networks is not a trivial task. The periodicity of crystalline needs efficient implementations to be processed in real-time under a massively parallel environment. With the aim of training graph-based generative models of new material discovery, we propose an efficient tool to generate cutoff graphs and k-nearest-neighbours graphs of periodic structures within GPU optimization. We provide pyMatGraph a Pytorch-compatible framework to generate graphs in real-time during the training of neural network architecture. Our tool can update a graph of a structure, making generative models able to update the geometry and process the updated graph during the forward propagation on the GPU side. Our code is publicly available at https://github.com/aklipf/mat
&lt;/p&gt;</description></item><item><title>M$^2$Hub&#26159;&#19968;&#20010;&#25512;&#21160;&#26426;&#22120;&#23398;&#20064;&#22312;&#26448;&#26009;&#21457;&#29616;&#20013;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#38598;&#25104;&#24179;&#21488;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#26448;&#26009;&#21457;&#29616;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12289;&#35780;&#20272;&#21644;&#22522;&#20934;&#32467;&#26524;&#30340;&#20415;&#25463;&#35775;&#38382;&#65292;&#28085;&#30422;&#34394;&#25311;&#31579;&#36873;&#12289;&#36870;&#21521;&#35774;&#35745;&#21644;&#20998;&#23376;&#27169;&#25311;&#31561;&#20851;&#38190;&#38454;&#27573;&#30340;&#22810;&#31181;&#26448;&#26009;&#31867;&#22411;&#21644;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05378</link><description>&lt;p&gt;
M$^2$Hub&#65306;&#37322;&#25918;&#26426;&#22120;&#23398;&#20064;&#22312;&#26448;&#26009;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
M$^2$Hub: Unlocking the Potential of Machine Learning for Materials Discovery. (arXiv:2307.05378v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05378
&lt;/p&gt;
&lt;p&gt;
M$^2$Hub&#26159;&#19968;&#20010;&#25512;&#21160;&#26426;&#22120;&#23398;&#20064;&#22312;&#26448;&#26009;&#21457;&#29616;&#20013;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#38598;&#25104;&#24179;&#21488;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#26448;&#26009;&#21457;&#29616;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12289;&#35780;&#20272;&#21644;&#22522;&#20934;&#32467;&#26524;&#30340;&#20415;&#25463;&#35775;&#38382;&#65292;&#28085;&#30422;&#34394;&#25311;&#31579;&#36873;&#12289;&#36870;&#21521;&#35774;&#35745;&#21644;&#20998;&#23376;&#27169;&#25311;&#31561;&#20851;&#38190;&#38454;&#27573;&#30340;&#22810;&#31181;&#26448;&#26009;&#31867;&#22411;&#21644;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;M$^2$Hub&#65292;&#36825;&#26159;&#19968;&#20010;&#25512;&#21160;&#26426;&#22120;&#23398;&#20064;&#22312;&#26448;&#26009;&#21457;&#29616;&#20013;&#30340;&#24037;&#20855;&#21253;&#12290;&#26426;&#22120;&#23398;&#20064;&#22312;&#24314;&#27169;&#20998;&#23376;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#29983;&#29289;&#20998;&#23376;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#24314;&#27169;&#26448;&#26009;&#32467;&#26500;&#26041;&#38754;&#30340;&#21457;&#23637;&#28382;&#21518;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#19968;&#20010;&#38598;&#25104;&#24179;&#21488;&#65292;&#33021;&#22815;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#26448;&#26009;&#21457;&#29616;&#20219;&#21153;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;M$^2$Hub&#23558;&#23454;&#29616;&#23545;&#26448;&#26009;&#21457;&#29616;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12289;&#35780;&#20272;&#21644;&#22522;&#20934;&#32467;&#26524;&#30340;&#20415;&#25463;&#35775;&#38382;&#65292;&#28085;&#30422;&#25972;&#20010;&#24037;&#20316;&#27969;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;M$^2$Hub&#30340;&#31532;&#19968;&#20010;&#29256;&#26412;&#37325;&#28857;&#20851;&#27880;&#26448;&#26009;&#21457;&#29616;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#38454;&#27573;&#65306;&#34394;&#25311;&#31579;&#36873;&#12289;&#36870;&#21521;&#35774;&#35745;&#21644;&#20998;&#23376;&#27169;&#25311;&#65292;&#21253;&#25324;&#21253;&#21547;56&#20010;&#20219;&#21153;&#30340;6&#31181;&#26448;&#26009;&#31867;&#22411;&#30340;9&#20010;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;8&#31181;&#26448;&#26009;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;2&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26448;&#26009;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce M$^2$Hub, a toolkit for advancing machine learning in materials discovery. Machine learning has achieved remarkable progress in modeling molecular structures, especially biomolecules for drug discovery. However, the development of machine learning approaches for modeling materials structures lag behind, which is partly due to the lack of an integrated platform that enables access to diverse tasks for materials discovery. To bridge this gap, M$^2$Hub will enable easy access to materials discovery tasks, datasets, machine learning methods, evaluations, and benchmark results that cover the entire workflow. Specifically, the first release of M$^2$Hub focuses on three key stages in materials discovery: virtual screening, inverse design, and molecular simulation, including 9 datasets that covers 6 types of materials with 56 tasks across 8 types of material properties. We further provide 2 synthetic datasets for the purpose of generative tasks on materials. In addition to random 
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#25552;&#39640;&#30456;&#24178;&#31995;&#32479;&#20013;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#19968;&#20010;"&#21333;&#19968;"&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#65292;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#22312;&#21457;&#23556;&#21151;&#29575;&#12289;&#31526;&#21495;&#36895;&#29575;&#25110;&#20256;&#36755;&#36317;&#31163;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#23558;Q&#22240;&#23376;&#25552;&#39640;&#26368;&#39640;&#36798;4 dB&#12290;</title><link>http://arxiv.org/abs/2307.05374</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#20197;&#25552;&#39640;&#30456;&#24178;&#20809;&#31995;&#32479;&#20013;&#31070;&#32463;&#32593;&#32476;&#22343;&#34913;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning to Enhance Generazability of Neural Network Equalizers in Coherent Optical Systems. (arXiv:2307.05374v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05374
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#25552;&#39640;&#30456;&#24178;&#31995;&#32479;&#20013;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#19968;&#20010;"&#21333;&#19968;"&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#65292;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#22312;&#21457;&#23556;&#21151;&#29575;&#12289;&#31526;&#21495;&#36895;&#29575;&#25110;&#20256;&#36755;&#36317;&#31163;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#23558;Q&#22240;&#23376;&#25552;&#39640;&#26368;&#39640;&#36798;4 dB&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#25552;&#39640;&#30456;&#24178;&#31995;&#32479;&#20013;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#30340;&#28789;&#27963;&#24615;&#12290;&#19982;&#24120;&#35268;&#25968;&#23383;&#26102;&#38047;&#24674;&#22797;&#26041;&#27861;&#30456;&#27604;&#65292;&#19968;&#20010;"&#21333;&#19968;"&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#22312;&#21457;&#23556;&#21151;&#29575;&#12289;&#31526;&#21495;&#36895;&#29575;&#25110;&#20256;&#36755;&#36317;&#31163;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#23558;Q&#22240;&#23376;&#25552;&#39640;&#26368;&#39640;&#36798;4 dB&#12290;
&lt;/p&gt;
&lt;p&gt;
For the first time, multi-task learning is proposed to improve the flexibility of NN-based equalizers in coherent systems. A "single" NN-based equalizer improves Q-factor by up to 4 dB compared to CDC, without re-training, even with variations in launch power, symbol rate, or transmission distance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;SSNet&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;EEG&#12289;EOG&#21644;EMG&#20449;&#21495;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#20840;&#36830;&#25509;&#23618;&#36827;&#34892;&#20998;&#31867;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#65292;&#24182;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#21644;Kappa&#31995;&#25968;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05373</link><description>&lt;p&gt;
&#36890;&#36807;SSNet&#20174;EEG&#12289;EOG&#21644;EMG&#20449;&#21495;&#20013;&#20998;&#31867;&#30561;&#30496;&#38454;&#27573; (arXiv:2307.05373v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
Classification of sleep stages from EEG, EOG and EMG signals by SSNet. (arXiv:2307.05373v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;SSNet&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;EEG&#12289;EOG&#21644;EMG&#20449;&#21495;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#20840;&#36830;&#25509;&#23618;&#36827;&#34892;&#20998;&#31867;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#65292;&#24182;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#21644;Kappa&#31995;&#25968;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#22312;&#35786;&#26029;&#30561;&#30496;&#30456;&#20851;&#30142;&#30149;&#65292;&#21253;&#25324;&#30561;&#30496;&#21628;&#21560;&#38556;&#30861;(SDB)&#30142;&#30149;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21629;&#21517;&#20026;SSNet&#65292;&#23427;&#21253;&#25324;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#12290;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20174;&#30005;&#30524;&#22270;&#65288;EOG&#65289;&#12289;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#21644;&#32908;&#30005;&#22270;&#65288;EMG&#65289;&#20449;&#21495;&#30340;&#32452;&#21512;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#22240;&#20026;&#27599;&#20010;&#20449;&#21495;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#12290;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20135;&#29983;&#30340;&#29305;&#24449;&#34987;&#36830;&#25509;&#36215;&#26469;&#20256;&#36882;&#21040;&#20840;&#36830;&#25509;&#23618;&#29992;&#20110;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20351;&#29992;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;Sleep-EDF Expanded dataset&#21644;ISRUC-Sleep dataset&#36827;&#34892;&#35780;&#20272;&#12290;&#30561;&#30496;-EDF Expanded&#25968;&#25454;&#38598;&#30340;&#19977;&#31181;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;Kappa&#31995;&#25968;&#20998;&#21035;&#20026;96.36%&#21644;93.40%&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification of sleep stages plays an essential role in diagnosing sleep-related diseases including Sleep Disorder Breathing (SDB) disease. In this study, we propose an end-to-end deep learning architecture, named SSNet, which comprises of two deep learning networks based on Convolutional Neuron Networks (CNN) and Long Short Term Memory (LSTM). Both deep learning networks extract features from the combination of Electrooculogram (EOG), Electroencephalogram (EEG), and Electromyogram (EMG) signals, as each signal has distinct features that help in the classification of sleep stages. The features produced by the two-deep learning networks are concatenated to pass to the fully connected layer for the classification. The performance of our proposed model is evaluated by using two public datasets Sleep-EDF Expanded dataset and ISRUC-Sleep dataset. The accuracy and Kappa coefficient are 96.36% and 93.40% respectively, for classifying three classes of sleep stages using Sleep-EDF Expanded da
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#36861;&#36394;&#21487;&#25240;&#21472;&#26234;&#33021;&#32442;&#32455;&#21697;&#65292;&#36890;&#36807;&#32467;&#21512;&#25240;&#21472;&#32455;&#29289;&#32467;&#26500;&#21644;&#30005;&#23481;&#20256;&#24863;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#26816;&#27979;&#32467;&#26500;&#36816;&#21160;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#30005;&#23481;&#20449;&#21495;&#20013;&#20934;&#30830;&#37325;&#26500;&#20986;&#29255;&#27573;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;</title><link>http://arxiv.org/abs/2307.05370</link><description>&lt;p&gt;
Capafoldable: &#20855;&#26377;&#30005;&#23481;&#20256;&#24863;&#33021;&#21147;&#30340;&#33258;&#36861;&#36394;&#21487;&#25240;&#21472;&#26234;&#33021;&#32442;&#32455;&#21697;
&lt;/p&gt;
&lt;p&gt;
Capafoldable: self-tracking foldable smart textiles with capacitive sensing. (arXiv:2307.05370v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05370
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#36861;&#36394;&#21487;&#25240;&#21472;&#26234;&#33021;&#32442;&#32455;&#21697;&#65292;&#36890;&#36807;&#32467;&#21512;&#25240;&#21472;&#32455;&#29289;&#32467;&#26500;&#21644;&#30005;&#23481;&#20256;&#24863;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#26816;&#27979;&#32467;&#26500;&#36816;&#21160;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#30005;&#23481;&#20449;&#21495;&#20013;&#20934;&#30830;&#37325;&#26500;&#20986;&#29255;&#27573;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25240;&#21472;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;&#32467;&#26500;&#25216;&#26415;&#65292;&#21487;&#20197;&#20351;&#24179;&#38754;&#26448;&#26009;&#20855;&#26377;&#36816;&#21160;&#25110;&#19977;&#32500;&#21147;&#23398;&#29305;&#24615;&#12290;&#22522;&#20110;&#32442;&#32455;&#21697;&#30340;&#30005;&#23481;&#20256;&#24863;&#24050;&#32463;&#34987;&#35777;&#26126;&#23545;&#23548;&#30005;&#32442;&#32455;&#21697;&#30340;&#20960;&#20309;&#24418;&#21464;&#21644;&#30456;&#23545;&#36816;&#21160;&#38750;&#24120;&#25935;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#36861;&#36394;&#21487;&#25240;&#21472;&#26234;&#33021;&#32442;&#32455;&#21697;&#65292;&#23558;&#25240;&#21472;&#32455;&#29289;&#32467;&#26500;&#21644;&#30005;&#23481;&#20256;&#24863;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#20256;&#24863;&#30005;&#36335;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#26816;&#27979;&#32467;&#26500;&#36816;&#21160;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#20004;&#31181;&#25240;&#21472;&#27169;&#24335;&#65292;&#25163;&#39118;&#29748;&#21644;&#40831;&#24418;&#65292;&#27599;&#31181;&#27169;&#24335;&#20013;&#37117;&#26377;&#20004;&#31181;&#24067;&#23616;&#30340;&#30005;&#23481;&#20256;&#24863;&#22120;&#65292;&#20197;&#28909;&#31896;&#38468;&#30340;&#23548;&#30005;&#32442;&#32455;&#21697;&#29255;&#30340;&#24418;&#24335;&#23384;&#22312;&#12290;&#22312;&#25163;&#21160;&#31227;&#21160;&#25240;&#21472;&#27169;&#24335;&#30340;&#29255;&#27573;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#21644;&#37325;&#26500;&#29255;&#27573;&#30340;&#35270;&#35273;&#36319;&#36394;&#24418;&#29366;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#30005;&#23481;&#20449;&#21495;&#20013;&#37325;&#26500;&#23450;&#20041;&#29255;&#27573;&#24418;&#29366;&#30340;&#20960;&#20309;&#21407;&#35821;&#65292;R-squared&#20540;&#21487;&#36798;95&#65285;&#65292;22.5cm&#38271;&#29255;&#27573;&#30340;&#36861;&#36394;&#35823;&#24046;&#20026;1cm&#12290;
&lt;/p&gt;
&lt;p&gt;
Folding is an unique structural technique to enable planer materials with motion or 3D mechanical properties. Textile-based capacitive sensing has shown to be sensitive to the geometry deformation and relative motion of conductive textiles. In this work, we propose a novel self-tracking foldable smart textile by combining folded fabric structures and capacitive sensing to detect the structural motions using state-of-the-art sensing circuits and deep learning technologies. We created two folding patterns, Accordion and Chevron, each with two layouts of capacitive sensors in the form of thermobonded conductive textile patches. In an experiment of manually moving patches of the folding patterns, we developed deep neural network to learn and reconstruct the vision-tracked shape of the patches. Through our approach, the geometry primitives defining the patch shape can be reconstructed from the capacitive signals with R-squared value of up to 95\% and tracking error of 1cm for 22.5cm long pa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#36807;&#20013;&#23376;&#21644;X&#23556;&#32447;&#21453;&#23556;&#29575;&#26354;&#32447;&#30830;&#23450;&#22810;&#23618;&#34180;&#33180;&#29289;&#29702;&#21442;&#25968;&#30340;&#30456;&#20301;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#33021;&#22815;&#25913;&#21892;&#35757;&#32451;&#36807;&#31243;&#24182;&#35299;&#20915;&#38382;&#39064;&#30340;&#27424;&#23450;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2307.05364</link><description>&lt;p&gt;
&#20013;&#23376;&#21644;X&#23556;&#32447;&#21453;&#23556;&#29575;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#65306;&#34701;&#20837;&#20808;&#39564;&#30693;&#35782;&#35299;&#20915;&#30456;&#20301;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Neural network analysis of neutron and X-ray reflectivity data: Incorporating prior knowledge for tackling the phase problem. (arXiv:2307.05364v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#36807;&#20013;&#23376;&#21644;X&#23556;&#32447;&#21453;&#23556;&#29575;&#26354;&#32447;&#30830;&#23450;&#22810;&#23618;&#34180;&#33180;&#29289;&#29702;&#21442;&#25968;&#30340;&#30456;&#20301;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#33021;&#22815;&#25913;&#21892;&#35757;&#32451;&#36807;&#31243;&#24182;&#35299;&#20915;&#38382;&#39064;&#30340;&#27424;&#23450;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#30456;&#20301;&#20449;&#24687;&#65292;&#20174;&#27979;&#37327;&#30340;&#20013;&#23376;&#21644;X&#23556;&#32447;&#21453;&#23556;&#29575;&#26354;&#32447;&#20013;&#30830;&#23450;&#22810;&#23618;&#34180;&#33180;&#30340;&#29289;&#29702;&#21442;&#25968;&#65292;&#22312;&#22522;&#26412;&#27700;&#24179;&#19978;&#26159;&#19968;&#20010;&#27424;&#23450;&#21453;&#38382;&#39064;&#12290;&#36825;&#20010;&#25152;&#35859;&#30340;&#30456;&#20301;&#38382;&#39064;&#38480;&#21046;&#20102;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#38480;&#21046;&#20102;&#20808;&#21069;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20013;&#32771;&#34385;&#30340;&#21442;&#25968;&#33539;&#22260;&#21644;&#25968;&#37327;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#23545;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#22823;&#21442;&#25968;&#31354;&#38388;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#24773;&#26223;&#19979;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20855;&#26377;&#31665;&#27169;&#22411;&#21442;&#25968;&#21270;&#21644;&#29289;&#29702;&#21551;&#21457;&#24335;&#29305;&#27530;&#21442;&#25968;&#21270;&#30340;&#22810;&#23618;&#32467;&#26500;&#30340;&#25955;&#23556;&#38271;&#24230;&#23494;&#24230;&#21078;&#38754;&#12290;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#30340;&#36755;&#20837;&#65292;&#25105;&#20204;&#21487;&#20197;&#25913;&#21892;&#35757;&#32451;&#21160;&#21147;&#23398;&#24182;&#35299;&#20915;&#38382;&#39064;&#30340;&#27424;&#23450;&#29305;&#24615;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22686;&#21152;&#21442;&#25968;&#31354;&#38388;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the lack of phase information, determining the physical parameters of multilayer thin films from measured neutron and X-ray reflectivity curves is, on a fundamental level, an underdetermined inverse problem. This so-called phase problem poses limitations on standard neural networks, constraining the range and number of considered parameters in previous machine learning solutions. To overcome this, we present an approach that utilizes prior knowledge to regularize the training process over larger parameter spaces. We demonstrate the effectiveness of our method in various scenarios, including multilayer structures with box model parameterization and a physics-inspired special parameterization of the scattering length density profile for a multilayer structure. By leveraging the input of prior knowledge, we can improve the training dynamics and address the underdetermined ("ill-posed") nature of the problem. In contrast to previous methods, our approach scales favorably when increa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SleepEGAN&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22686;&#24378;&#30340;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#19981;&#24179;&#34913;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;GAN&#26550;&#26500;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#24182;&#37319;&#29992;&#26080;&#25104;&#26412;&#30340;&#38598;&#25104;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#20010;&#20307;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05362</link><description>&lt;p&gt;
SleepEGAN:&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#22411;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#19981;&#24179;&#34913;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SleepEGAN: A GAN-enhanced Ensemble Deep Learning Model for Imbalanced Classification of Sleep Stages. (arXiv:2307.05362v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SleepEGAN&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22686;&#24378;&#30340;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#19981;&#24179;&#34913;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;GAN&#26550;&#26500;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#24182;&#37319;&#29992;&#26080;&#25104;&#26412;&#30340;&#38598;&#25104;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#20010;&#20307;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#21160;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#20869;&#37096;&#29305;&#24449;&#36716;&#25442;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30561;&#30496;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#20010;&#20307;&#24322;&#36136;&#24615;&#24448;&#24448;&#20250;&#26174;&#33879;&#24433;&#21709;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;SleepEGAN&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22686;&#24378;&#30340;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#19981;&#24179;&#34913;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#12290;&#20026;&#20102;&#32531;&#35299;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;EEG&#20449;&#21495;&#29305;&#24449;&#30340;&#26032;GAN&#26550;&#26500;&#65288;&#31216;&#20026;EGAN&#65289;&#65292;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#23569;&#25968;&#31867;&#21035;&#30340;&#29983;&#25104;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#25104;&#26412;&#30340;&#38598;&#25104;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#30001;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#24341;&#36215;&#30340;&#27169;&#22411;&#20272;&#35745;&#26041;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have played an important role in automatic sleep stage classification because of their strong representation and in-model feature transformation abilities. However, class imbalance and individual heterogeneity which typically exist in raw EEG signals of sleep data can significantly affect the classification performance of any machine learning algorithms. To solve these two problems, this paper develops a generative adversarial network (GAN)-powered ensemble deep learning model, named SleepEGAN, for the imbalanced classification of sleep stages. To alleviate class imbalance, we propose a new GAN (called EGAN) architecture adapted to the features of EEG signals for data augmentation. The generated samples for the minority classes are used in the training process. In addition, we design a cost-free ensemble learning strategy to reduce the model estimation variance caused by the heterogeneity between the validation and test sets, so as to enhance the accuracy and robus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#20302;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;sEMG&#30340;&#32908;&#32905;&#21147;&#21644;&#20851;&#33410;&#36816;&#21160;&#23398;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#23558;&#25289;&#26684;&#26391;&#26085;&#36816;&#21160;&#26041;&#31243;&#21644;&#21453;&#21160;&#21147;&#23398;&#32908;&#32905;&#27169;&#22411;&#25972;&#21512;&#21040;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#65292;&#23454;&#29616;&#23545;&#23567;&#26679;&#26412;&#25968;&#25454;&#30340;&#32467;&#26500;&#21270;&#29305;&#24449;&#35299;&#30721;&#21644;&#22806;&#25512;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2307.05361</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#30340;&#20302;&#26679;&#26412;&#23398;&#20064;&#23545;&#22522;&#20110;sEMG&#30340;&#32908;&#32905;&#21147;&#21644;&#20851;&#33410;&#36816;&#21160;&#23398;&#36827;&#34892;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
A Physics-Informed Low-Shot Learning For sEMG-Based Estimation of Muscle Force and Joint Kinematics. (arXiv:2307.05361v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#20302;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;sEMG&#30340;&#32908;&#32905;&#21147;&#21644;&#20851;&#33410;&#36816;&#21160;&#23398;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#23558;&#25289;&#26684;&#26391;&#26085;&#36816;&#21160;&#26041;&#31243;&#21644;&#21453;&#21160;&#21147;&#23398;&#32908;&#32905;&#27169;&#22411;&#25972;&#21512;&#21040;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#65292;&#23454;&#29616;&#23545;&#23567;&#26679;&#26412;&#25968;&#25454;&#30340;&#32467;&#26500;&#21270;&#29305;&#24449;&#35299;&#30721;&#21644;&#22806;&#25512;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#34920;&#38754;&#32908;&#30005;&#22270;&#65288;sEMG&#65289;&#30340;&#32908;&#32905;&#21147;&#21644;&#20851;&#33410;&#36816;&#21160;&#23398;&#20272;&#35745;&#65292;&#26159;&#23454;&#26102;&#29983;&#29289;&#21147;&#23398;&#20998;&#26512;&#20013;&#31070;&#32463;&#32908;&#32905;&#21050;&#28608;&#12289;&#32908;&#32905;&#21160;&#21147;&#23398;&#21644;&#21160;&#21147;&#23398;&#20043;&#38388;&#21160;&#24577;&#30456;&#20114;&#20316;&#29992;&#30340;&#20851;&#38190;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#23427;&#20204;&#26377;&#28508;&#21147;&#20197;&#20840;&#33258;&#21160;&#21644;&#21487;&#37325;&#22797;&#30340;&#26041;&#24335;&#25913;&#36827;&#29983;&#29289;&#21147;&#23398;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#29983;&#29289;&#21147;&#23398;&#20998;&#26512;&#30340;&#23567;&#26679;&#26412;&#24615;&#36136;&#21644;&#29289;&#29702;&#21487;&#35299;&#37322;&#24615;&#38480;&#21046;&#20102;DNN&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#20449;&#24687;&#20302;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;sEMG&#30340;&#32908;&#32905;&#21147;&#21644;&#20851;&#33410;&#36816;&#21160;&#23398;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#23558;&#25289;&#26684;&#26391;&#26085;&#36816;&#21160;&#26041;&#31243;&#21644;&#21453;&#21160;&#21147;&#23398;&#32908;&#32905;&#27169;&#22411;&#26080;&#32541;&#22320;&#25972;&#21512;&#21040;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#23545;&#23567;&#26679;&#26412;&#25968;&#25454;&#30340;&#32467;&#26500;&#21270;&#29305;&#24449;&#35299;&#30721;&#21644;&#22806;&#25512;&#20272;&#35745;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24341;&#20837;&#25289;&#26684;&#26391;&#26085;&#36816;&#21160;&#26041;&#31243;&#21040;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#20197;&#38480;&#21046;&#39640;&#23618;&#32467;&#26500;&#21270;&#35299;&#30721;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Muscle force and joint kinematics estimation from surface electromyography (sEMG) are essential for real-time biomechanical analysis of the dynamic interplay among neural muscle stimulation, muscle dynamics, and kinetics. Recent advances in deep neural networks (DNNs) have shown the potential to improve biomechanical analysis in a fully automated and reproducible manner. However, the small sample nature and physical interpretability of biomechanical analysis limit the applications of DNNs. This paper presents a novel physics-informed low-shot learning method for sEMG-based estimation of muscle force and joint kinematics. This method seamlessly integrates Lagrange's equation of motion and inverse dynamic muscle model into the generative adversarial network (GAN) framework for structured feature decoding and extrapolated estimation from the small sample data. Specifically, Lagrange's equation of motion is introduced into the generative model to restrain the structured decoding of the hig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;FedDure&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#21644;&#32454;&#35843;&#33410;&#22120;&#23545;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#36827;&#34892;&#35268;&#33539;&#65292;&#20197;&#21450;&#23398;&#20064;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2307.05358</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#35843;&#33410;&#22120;&#35299;&#20915;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators. (arXiv:2307.05358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;FedDure&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#21644;&#32454;&#35843;&#33410;&#22120;&#23545;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#36827;&#34892;&#35268;&#33539;&#65292;&#20197;&#21450;&#23398;&#20064;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20174;&#20998;&#25955;&#24322;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#30001;&#20110;&#20998;&#25955;&#23458;&#25143;&#31471;&#19978;&#26631;&#31614;&#31232;&#32570;&#65292;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;FSSL&#65289;&#20986;&#29616;&#20197;&#20174;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#20013;&#35757;&#32451;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FSSL&#26041;&#27861;&#20551;&#35774;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#26631;&#31614;&#25968;&#25454;&#29420;&#31435;&#19988;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#23458;&#25143;&#31471;&#20869;&#37096;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#20855;&#26377;&#19968;&#33268;&#30340;&#31867;&#21035;&#20998;&#24067;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;FSSL&#30340;&#26356;&#23454;&#38469;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#21363;&#25968;&#25454;&#20998;&#24067;&#19981;&#20165;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#19981;&#21516;&#65292;&#22312;&#23458;&#25143;&#31471;&#20869;&#37096;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#20063;&#19981;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;FSSL&#26694;&#26550;&#65292;FedDure&#12290;FedDure&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#65288;C-reg&#65289;&#21644;&#32454;&#35843;&#33410;&#22120;&#65288;F-reg&#65289;&#35299;&#38500;&#20102;&#20197;&#21069;&#30340;&#20551;&#35774;&#65306;C-reg&#36890;&#36807;&#36319;&#36394;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#23398;&#20064;&#25928;&#26524;&#26469;&#35268;&#33539;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#65307;F-reg&#23398;&#20064;&#19968;&#20010;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#20197;&#36866;&#24212;&#23458;&#25143;&#31471;&#20869;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has become a popular method to learn from decentralized heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train models from a small fraction of labeled data due to label scarcity on decentralized clients. Existing FSSL methods assume independent and identically distributed (IID) labeled data across clients and consistent class distribution between labeled and unlabeled data within a client. This work studies a more practical and challenging scenario of FSSL, where data distribution is different not only across clients but also within a client between labeled and unlabeled data. To address this challenge, we propose a novel FSSL framework with dual regulators, FedDure.} FedDure lifts the previous assumption with a coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg regularizes the updating of the local model by tracking the learning effect on labeled data distribution; F-reg learns an adaptive weighting scheme tailored f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;VisText&#65292;&#19968;&#20010;&#20016;&#23500;&#35821;&#20041;&#30340;&#22270;&#34920;&#26631;&#39064;&#35780;&#27979;&#22522;&#20934;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;12,441&#20010;&#22270;&#34920;&#21644;&#26631;&#39064;&#23545;&#65292;&#25551;&#36848;&#20102;&#22270;&#34920;&#30340;&#26500;&#36896;&#12289;&#25253;&#21578;&#20102;&#20851;&#38190;&#32479;&#35745;&#25968;&#25454;&#65292;&#24182;&#35782;&#21035;&#20102;&#24863;&#30693;&#21644;&#35748;&#30693;&#29616;&#35937;&#12290;&#36890;&#36807;&#22312;&#22270;&#34920;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#19978;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#24182;&#24212;&#29992;&#39044;&#22788;&#29702;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;VisText&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.05356</link><description>&lt;p&gt;
VisText&#65306;&#19968;&#20010;&#20016;&#23500;&#35821;&#20041;&#30340;&#22270;&#34920;&#26631;&#39064;&#35780;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
VisText: A Benchmark for Semantically Rich Chart Captioning. (arXiv:2307.05356v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;VisText&#65292;&#19968;&#20010;&#20016;&#23500;&#35821;&#20041;&#30340;&#22270;&#34920;&#26631;&#39064;&#35780;&#27979;&#22522;&#20934;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;12,441&#20010;&#22270;&#34920;&#21644;&#26631;&#39064;&#23545;&#65292;&#25551;&#36848;&#20102;&#22270;&#34920;&#30340;&#26500;&#36896;&#12289;&#25253;&#21578;&#20102;&#20851;&#38190;&#32479;&#35745;&#25968;&#25454;&#65292;&#24182;&#35782;&#21035;&#20102;&#24863;&#30693;&#21644;&#35748;&#30693;&#29616;&#35937;&#12290;&#36890;&#36807;&#22312;&#22270;&#34920;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#19978;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#24182;&#24212;&#29992;&#39044;&#22788;&#29702;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;VisText&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25551;&#36848;&#25110;&#35299;&#37322;&#22270;&#34920;&#30340;&#26631;&#39064;&#26377;&#21161;&#20110;&#25552;&#39640;&#23545;&#22270;&#34920;&#25968;&#25454;&#30340;&#22238;&#24518;&#21644;&#29702;&#35299;&#65292;&#24182;&#20026;&#35270;&#35273;&#38556;&#30861;&#20154;&#22763;&#25552;&#20379;&#26356;&#26131;&#25509;&#35302;&#30340;&#23186;&#20171;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#33258;&#21160;&#29983;&#25104;&#36825;&#31867;&#26631;&#39064;&#30340;&#26041;&#27861;&#38590;&#20197;&#34920;&#36798;&#22270;&#34920;&#30340;&#24863;&#30693;&#25110;&#35748;&#30693;&#29305;&#24449;&#65288;&#22914;&#22797;&#26434;&#30340;&#36235;&#21183;&#21644;&#27169;&#24335;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;VisText&#65306;&#19968;&#20010;&#30001;12,441&#20010;&#22270;&#34920;&#21644;&#26631;&#39064;&#23545;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#25551;&#36848;&#20102;&#22270;&#34920;&#30340;&#26500;&#36896;&#65292;&#25253;&#21578;&#20102;&#20851;&#38190;&#32479;&#35745;&#25968;&#25454;&#65292;&#24182;&#35782;&#21035;&#20102;&#24863;&#30693;&#21644;&#35748;&#30693;&#29616;&#35937;&#12290;&#22312;VisText&#20013;&#65292;&#19968;&#20010;&#22270;&#34920;&#26377;&#19977;&#31181;&#34920;&#31034;&#24418;&#24335;&#65306;&#20809;&#26629;&#21270;&#22270;&#20687;&#12289;&#25903;&#25345;&#25968;&#25454;&#34920;&#26684;&#21644;&#22330;&#26223;&#22270;&#8212;&#8212;&#31867;&#20284;&#20110;Web&#39029;&#38754;&#30340;&#25991;&#26723;&#23545;&#35937;&#27169;&#22411;&#65288;DOM&#65289;&#30340;&#22270;&#34920;&#21487;&#35270;&#20803;&#32032;&#30340;&#20998;&#23618;&#34920;&#31034;&#12290;&#20026;&#20102;&#35780;&#20272;VisText&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#22312;&#22270;&#34920;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#19978;&#23545;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#24212;&#29992;&#20102;&#39044;&#22788;&#29702;&#26469;&#20135;&#29983;&#20256;&#36798;&#35821;&#20041;&#20869;&#23481;&#21464;&#21270;&#30340;&#26631;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Captions that describe or explain charts help improve recall and comprehension of the depicted data and provide a more accessible medium for people with visual disabilities. However, current approaches for automatically generating such captions struggle to articulate the perceptual or cognitive features that are the hallmark of charts (e.g., complex trends and patterns). In response, we introduce VisText: a dataset of 12,441 pairs of charts and captions that describe the charts' construction, report key statistics, and identify perceptual and cognitive phenomena. In VisText, a chart is available as three representations: a rasterized image, a backing data table, and a scene graph -- a hierarchical representation of a chart's visual elements akin to a web page's Document Object Model (DOM). To evaluate the impact of VisText, we fine-tune state-of-the-art language models on our chart captioning task and apply prefix-tuning to produce captions that vary the semantic content they convey. O
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27169;&#31946;&#20102;&#21518;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#19982;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#30028;&#38480;&#65292;&#36890;&#36807;&#20174;&#28789;&#27963;&#30340;&#40657;&#30418;&#27169;&#22411;&#24320;&#22987;&#65292;&#36880;&#28176;&#24341;&#20837;&#21487;&#35299;&#37322;&#27169;&#22411;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23545;&#26679;&#26412;&#30340;&#36335;&#30001;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.05350</link><description>&lt;p&gt;
&#36335;&#30001;&#12289;&#35299;&#37322;&#12289;&#37325;&#22797;&#65306;&#27169;&#31946;&#21518;&#35299;&#37322;&#24615;&#19982;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Route, Interpret, Repeat: Blurring the line between post hoc explainability and interpretable models. (arXiv:2307.05350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27169;&#31946;&#20102;&#21518;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#19982;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#30028;&#38480;&#65292;&#36890;&#36807;&#20174;&#28789;&#27963;&#30340;&#40657;&#30418;&#27169;&#22411;&#24320;&#22987;&#65292;&#36880;&#28176;&#24341;&#20837;&#21487;&#35299;&#37322;&#27169;&#22411;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23545;&#26679;&#26412;&#30340;&#36335;&#30001;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#26041;&#27861;&#35201;&#20040;&#36873;&#25321;&#19968;&#20010;&#28789;&#27963;&#30340;&#40657;&#30418;&#27169;&#22411;&#24182;&#22312;&#21518;&#26399;&#35299;&#37322;&#23427;&#65292;&#35201;&#20040;&#20174;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;&#40657;&#30418;&#27169;&#22411;&#28789;&#27963;&#20294;&#38590;&#20197;&#35299;&#37322;&#65292;&#32780;&#21487;&#35299;&#37322;&#27169;&#22411;&#35774;&#35745;&#20026;&#21487;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#21487;&#35299;&#37322;&#27169;&#22411;&#38656;&#35201;&#28145;&#21402;&#30340;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#65292;&#32780;&#24471;&#21040;&#30340;&#27169;&#22411;&#24448;&#24448;&#19981;&#22815;&#28789;&#27963;&#65292;&#21487;&#33021;&#24615;&#33021;&#19981;&#21450;&#20854;&#40657;&#30418;&#27169;&#22411;&#30340;&#31561;&#20215;&#29289;&#12290;&#26412;&#25991;&#26088;&#22312;&#27169;&#31946;&#21518;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#19982;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#25105;&#20204;&#25552;&#35758;&#20174;&#19968;&#20010;&#28789;&#27963;&#30340;&#40657;&#30418;&#27169;&#22411;&#24320;&#22987;&#65292;&#24182;&#36880;&#28176;&#12300;&#38613;&#21051;&#12301;&#20986;&#19968;&#31181;&#28151;&#21512;&#20102;&#21487;&#35299;&#37322;&#27169;&#22411;&#21644;&#19968;&#20010;&#12300;&#27531;&#24046;&#32593;&#32476;&#12301;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#36890;&#36807;&#21487;&#35299;&#37322;&#27169;&#22411;&#12300;&#36335;&#30001;&#12301;&#19968;&#37096;&#20998;&#26679;&#26412;&#65292;&#21097;&#20313;&#30340;&#26679;&#26412;&#21017;&#36890;&#36807;&#28789;&#27963;&#30340;&#27531;&#24046;&#32593;&#32476;&#36827;&#34892;&#36335;&#30001;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#38454;&#36923;&#36753;&#65288;FOL&#65289;&#20316;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current approach to ML model design is either to choose a flexible Blackbox model and explain it post hoc or to start with an interpretable model. Blackbox models are flexible but difficult to explain, whereas interpretable models are designed to be explainable. However, developing interpretable models necessitates extensive ML knowledge, and the resulting models tend to be less flexible, offering potentially subpar performance compared to their Blackbox equivalents. This paper aims to blur the distinction between a post hoc explanation of a BlackBox and constructing interpretable models. We propose beginning with a flexible BlackBox model and gradually \emph{carving out} a mixture of interpretable models and a \emph{residual network}. Our design identifies a subset of samples and \emph{routes} them through the interpretable models. The remaining samples are routed through a flexible residual network. We adopt First Order Logic (FOL) as the interpretable model's backbone, which pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#24773;&#22659;&#36172;&#21338;&#20013;&#30340;&#26368;&#26174;&#33879;&#21464;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#35745;&#31639;&#26174;&#33879;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#23616;&#37096;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05341</link><description>&lt;p&gt;
&#36319;&#36394;&#38750;&#21442;&#25968;&#24773;&#22659;&#36172;&#21338;&#20013;&#26368;&#26174;&#33879;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Tracking Most Significant Shifts in Nonparametric Contextual Bandits. (arXiv:2307.05341v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05341
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#24773;&#22659;&#36172;&#21338;&#20013;&#30340;&#26368;&#26174;&#33879;&#21464;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#35745;&#31639;&#26174;&#33879;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#23616;&#37096;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#24773;&#22659;&#36172;&#21338;&#65292;&#20854;&#20013;Lipschitz&#22343;&#20540;&#22870;&#21169;&#20989;&#25968;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#36825;&#20010;&#36739;&#23569;&#34987;&#29702;&#35299;&#30340;&#24773;&#22659;&#19979;&#24314;&#31435;&#20102;&#21160;&#24577;&#36951;&#25022;&#29575;&#30340;&#26497;&#23567;&#26497;&#22823;&#20540;&#65292;&#36825;&#20123;&#20540;&#19982;&#21464;&#21270;&#25968;&#37327;L&#21644;&#24635;&#21464;&#24046;V&#26377;&#20851;&#65292;&#20004;&#32773;&#37117;&#21487;&#20197;&#25429;&#25417;&#21040;&#19978;&#19979;&#25991;&#31354;&#38388;&#30340;&#25152;&#26377;&#20998;&#24067;&#21464;&#21270;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#30446;&#21069;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#24773;&#22659;&#19979;&#26159;&#27425;&#20248;&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#31181;&#24773;&#22659;&#19979;&#30340;&#36866;&#24212;&#24615;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#30693;&#36947;L&#25110;V&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26497;&#23567;&#26497;&#22823;&#20540;&#12290;&#38750;&#24120;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;X_t&#22788;&#65292;&#36172;&#21338;&#38382;&#39064;&#22312;&#19978;&#19979;&#25991;&#31354;&#38388;&#20854;&#20182;&#37096;&#20998;&#20013;&#30340;&#22870;&#21169;&#21464;&#21270;&#19981;&#24212;&#35813;&#20135;&#29983;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#21270;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32463;&#39564;&#26174;&#33879;&#21464;&#21270;&#65292;&#26356;&#22909;&#22320;&#32771;&#34385;&#20102;&#23616;&#37096;&#24615;&#65292;&#22240;&#27492;&#27604;L&#21644;V&#35745;&#25968;&#26356;&#23569;&#12290;&#27492;&#22806;&#65292;&#31867;&#20284;&#20110;&#26368;&#36817;&#22312;&#38750;&#24179;&#31283;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#24037;&#20316;&#65288;Suk&#21644;Kpotufe&#65292;2022&#65289;&#65292;&#32463;&#39564;&#26174;&#33879;&#21464;&#21270;&#21482;&#35745;&#31639;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study nonparametric contextual bandits where Lipschitz mean reward functions may change over time. We first establish the minimax dynamic regret rate in this less understood setting in terms of number of changes $L$ and total-variation $V$, both capturing all changes in distribution over context space, and argue that state-of-the-art procedures are suboptimal in this setting.  Next, we tend to the question of an adaptivity for this setting, i.e. achieving the minimax rate without knowledge of $L$ or $V$. Quite importantly, we posit that the bandit problem, viewed locally at a given context $X_t$, should not be affected by reward changes in other parts of context space $\cal X$. We therefore propose a notion of change, which we term experienced significant shifts, that better accounts for locality, and thus counts considerably less changes than $L$ and $V$. Furthermore, similar to recent work on non-stationary MAB (Suk &amp; Kpotufe, 2022), experienced significant shifts only count the m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#21487;&#31359;&#25140;&#35774;&#22791;&#25910;&#38598;&#30340;PPG&#20449;&#21495;&#20013;&#21435;&#22122;&#20197;&#20272;&#35745;&#24515;&#29575;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#37325;&#26500;&#21463;&#25439;&#20449;&#21495;&#37096;&#20998;&#21644;&#20445;&#30041;&#24178;&#20928;&#20449;&#21495;&#37096;&#20998;&#30340;&#26041;&#24335;&#36827;&#34892;&#21435;&#22122;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#24178;&#20928;PPG&#20449;&#21495;&#35757;&#32451;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;PPG&#20449;&#21495;&#24515;&#29575;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.05339</link><description>&lt;p&gt;
&#29992;&#20110;&#20174;&#21487;&#31359;&#25140;&#35774;&#22791;&#20013;&#21435;&#22122;&#20809;&#30005;&#23481;/&#23481;&#31215;&#25551;&#35760;&#20449;&#21495;&#20197;&#20272;&#35745;&#24515;&#29575;&#30340;&#33258;&#30417;&#30563;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Self-Supervised Algorithm for Denoising Photoplethysmography Signals for Heart Rate Estimation from Wearables. (arXiv:2307.05339v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#21487;&#31359;&#25140;&#35774;&#22791;&#25910;&#38598;&#30340;PPG&#20449;&#21495;&#20013;&#21435;&#22122;&#20197;&#20272;&#35745;&#24515;&#29575;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#37325;&#26500;&#21463;&#25439;&#20449;&#21495;&#37096;&#20998;&#21644;&#20445;&#30041;&#24178;&#20928;&#20449;&#21495;&#37096;&#20998;&#30340;&#26041;&#24335;&#36827;&#34892;&#21435;&#22122;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#24178;&#20928;PPG&#20449;&#21495;&#35757;&#32451;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;PPG&#20449;&#21495;&#24515;&#29575;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25163;&#34920;&#21644;&#20854;&#20182;&#21487;&#31359;&#25140;&#35774;&#22791;&#37197;&#22791;&#20102;&#20809;&#30005;&#23481;/&#23481;&#31215;&#25551;&#35760;&#65288;PPG&#65289;&#20256;&#24863;&#22120;&#65292;&#29992;&#20110;&#30417;&#27979;&#24515;&#29575;&#21644;&#20854;&#20182;&#24515;&#34880;&#31649;&#20581;&#24247;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#20174;&#36825;&#20123;&#35774;&#22791;&#25910;&#38598;&#30340;PPG&#20449;&#21495;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#21644;&#36816;&#21160;&#20266;&#36857;&#30340;&#24178;&#25200;&#65292;&#23548;&#33268;&#24515;&#29575;&#20272;&#35745;&#35823;&#24046;&#12290;&#20256;&#32479;&#30340;&#21435;&#22122;&#26041;&#27861;&#24448;&#24448;&#20250;&#20197;&#36807;&#28388;&#25110;&#37325;&#26500;&#30340;&#26041;&#24335;&#22788;&#29702;&#20449;&#21495;&#65292;&#36825;&#26679;&#20250;&#23548;&#33268;&#20449;&#21495;&#30340;&#24418;&#24577;&#20449;&#24687;&#20002;&#22833;&#65292;&#21363;&#20351;&#26159;&#23545;&#20110;&#21407;&#26412;&#24178;&#20928;&#30340;&#20449;&#21495;&#37096;&#20998;&#32780;&#35328;&#65292;&#20063;&#20250;&#26377;&#29992;&#30340;&#20449;&#24687;&#34987;&#20002;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#21435;&#22122;PPG&#20449;&#21495;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#37325;&#26500;&#21463;&#25439;&#20449;&#21495;&#30340;&#37096;&#20998;&#65292;&#21516;&#26102;&#20445;&#30041;PPG&#20449;&#21495;&#30340;&#24178;&#20928;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#26694;&#26550;&#22522;&#20110;&#33258;&#30417;&#30563;&#35757;&#32451;&#65292;&#21033;&#29992;&#22823;&#37327;&#30340;&#24178;&#20928;PPG&#20449;&#21495;&#25968;&#25454;&#24211;&#26469;&#35757;&#32451;&#21435;&#22122;&#33258;&#21160;&#32534;&#30721;&#22120;&#12290;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#65292;&#25105;&#20204;&#37325;&#26500;&#30340;&#20449;&#21495;&#25552;&#20379;&#20102;&#27604;&#39046;&#20808;&#30340;&#24515;&#29575;&#20272;&#35745;&#26041;&#27861;&#26356;&#22909;&#30340;PPG&#20449;&#21495;&#24515;&#29575;&#20272;&#35745;&#32467;&#26524;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Smart watches and other wearable devices are equipped with photoplethysmography (PPG) sensors for monitoring heart rate and other aspects of cardiovascular health. However, PPG signals collected from such devices are susceptible to corruption from noise and motion artifacts, which cause errors in heart rate estimation. Typical denoising approaches filter or reconstruct the signal in ways that eliminate much of the morphological information, even from the clean parts of the signal that would be useful to preserve. In this work, we develop an algorithm for denoising PPG signals that reconstructs the corrupted parts of the signal, while preserving the clean parts of the PPG signal. Our novel framework relies on self-supervised training, where we leverage a large database of clean PPG signals to train a denoising autoencoder. As we show, our reconstructed signals provide better estimates of heart rate from PPG signals than the leading heart rate estimation methods. Further experiments show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23646;&#24615;&#20844;&#24179;&#25439;&#22833;&#30340;CNN&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#24739;&#32773;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#20844;&#24179;&#39044;&#27979;&#30140;&#30171;&#29366;&#24577;&#65292;&#33268;&#21147;&#20110;&#20943;&#23569;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.05333</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#36827;&#34892;&#26080;&#20559;&#35265;&#30340;&#30140;&#30171;&#35780;&#20272;&#65306;&#22522;&#20110;&#22810;&#23646;&#24615;&#20844;&#24179;&#25439;&#22833;&#30340;CNN&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unbiased Pain Assessment through Wearables and EHR Data: Multi-attribute Fairness Loss-based CNN Approach. (arXiv:2307.05333v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23646;&#24615;&#20844;&#24179;&#25439;&#22833;&#30340;CNN&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#24739;&#32773;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#20844;&#24179;&#39044;&#27979;&#30140;&#30171;&#29366;&#24577;&#65292;&#33268;&#21147;&#20110;&#20943;&#23569;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#21270;&#30340;&#20581;&#24247;&#25968;&#25454;&#65288;&#29289;&#32852;&#32593;&#12289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#21644;&#20020;&#24202;&#35843;&#26597;&#65289;&#19982;&#21487;&#25193;&#23637;&#30340;&#36866;&#24212;&#24615;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#24050;&#32463;&#23454;&#29616;&#20102;&#23545;&#30140;&#30171;&#29366;&#24577;&#30340;&#36523;&#20307;&#12289;&#34892;&#20026;&#21644;&#24515;&#29702;&#31038;&#20132;&#25351;&#26631;&#30340;&#21457;&#29616;&#12290;&#23613;&#31649;&#20197;&#25216;&#26415;&#36827;&#27493;&#25913;&#21464;&#21307;&#30103;&#31995;&#32479;&#30340;&#28909;&#24773;&#21644;&#25215;&#35834;&#65292;&#20294;&#20020;&#24202;&#30140;&#30171;&#35780;&#20272;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#21463;&#21040;&#20102;&#38382;&#39064;&#26412;&#36523;&#30340;&#22810;&#26679;&#24615;&#21644;&#20010;&#24615;&#21270;&#20197;&#21450;&#20844;&#24179;&#24615;&#31561;&#20854;&#20182;&#25361;&#25112;&#30340;&#38459;&#30861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#65288;&#22914;&#26426;&#22120;&#23398;&#20064;&#25110;&#28145;&#24230;&#23398;&#20064;&#65289;&#27169;&#22411;&#26174;&#31034;&#20986;&#20559;&#35265;&#65292;&#24182;&#27495;&#35270;&#29305;&#23450;&#20154;&#32676;&#65288;&#22914;&#22522;&#20110;&#24615;&#21035;&#25110;&#31181;&#26063;&#65289;&#65292;&#36825;&#24341;&#36215;&#20102;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#23545;&#20154;&#24037;&#26234;&#33021;&#36866;&#24212;&#24615;&#30340;&#24576;&#30097;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23646;&#24615;&#20844;&#24179;&#25439;&#22833;&#30340;CNN&#27169;&#22411;&#65292;&#26088;&#22312;&#32771;&#34385;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#20219;&#20309;&#25935;&#24863;&#23646;&#24615;&#65292;&#24182;&#20844;&#24179;&#39044;&#27979;&#24739;&#32773;&#30340;&#30140;&#30171;&#29366;&#24577;&#65292;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The combination of diverse health data (IoT, EHR, and clinical surveys) and scalable-adaptable Artificial Intelligence (AI), has enabled the discovery of physical, behavioral, and psycho-social indicators of pain status. Despite the hype and promise to fundamentally alter the healthcare system with technological advancements, much AI adoption in clinical pain evaluation has been hampered by the heterogeneity of the problem itself and other challenges, such as personalization and fairness. Studies have revealed that many AI (i.e., machine learning or deep learning) models display biases and discriminate against specific population segments (such as those based on gender or ethnicity), which breeds skepticism among medical professionals about AI adaptability. In this paper, we propose a Multi-attribute Fairness Loss (MAFL) based CNN model that aims to account for any sensitive attributes included in the data and fairly predict patients' pain status while attempting to minimize the discre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36793;&#38469;&#20272;&#20540;&#23545;&#22269;&#38469;&#35937;&#26827;&#26827;&#30424;&#19978;&#30340;&#26827;&#23376;&#21644;&#26827;&#30424;&#36827;&#34892;&#35780;&#20215;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#39532;&#12289;&#35937;&#21644;&#20853;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.05330</link><description>&lt;p&gt;
&#26827;&#30424;&#19978;&#30340;&#26827;&#23376;&#20215;&#20540;&#12290; (arXiv:2307.05330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
The Value of Chess Squares. (arXiv:2307.05330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36793;&#38469;&#20272;&#20540;&#23545;&#22269;&#38469;&#35937;&#26827;&#26827;&#30424;&#19978;&#30340;&#26827;&#23376;&#21644;&#26827;&#30424;&#36827;&#34892;&#35780;&#20215;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#39532;&#12289;&#35937;&#21644;&#20853;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35780;&#20272;&#26827;&#30424;&#19978;&#26827;&#23376;&#30340;&#20215;&#20540;&#65292;&#24182;&#30830;&#23450;&#26827;&#23376;&#22312;&#26827;&#30424;&#19978;&#30340;&#25670;&#25918;&#20301;&#32622;&#12290;&#38543;&#30528;&#22269;&#38469;&#35937;&#26827;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#22269;&#38469;&#35937;&#26827;&#23616;&#38754;&#30340;&#20215;&#20540;&#12290;&#20256;&#32479;&#26041;&#27861;&#23545;&#26827;&#23376;&#36171;&#20104;&#22266;&#23450;&#30340;&#20215;&#20540;$(\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26827;&#23376;&#21644;&#26827;&#30424;&#26041;&#38754;&#30340;&#36793;&#38469;&#20272;&#20540;&#26469;&#25913;&#36827;&#36825;&#31181;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#39532;&#21644;&#35937;&#30340;&#20301;&#32622;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#20853;&#30340;&#20215;&#20540;&#30340;&#23453;&#36149;&#35265;&#35299;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23612;&#22982;&#20304;&#32500;&#22855;&#26159;&#20513;&#23548;&#20853;&#30340;&#32467;&#26500;&#21644;&#20215;&#20540;&#30340;&#20808;&#39537;&#20043;&#19968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Valuing chess squares and determining the placement of pieces on the board are the main objectives of our study. With the emergence of chess AI, it has become possible to accurately assess the worth of positions in a game of chess. The conventional approach assigns fixed values to pieces $(\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$. We enhance this analysis by introducing marginal valuations for both pieces and squares. We demonstrate our method by examining the positioning of Knights and Bishops, and also provide valuable insights into the valuation of pawns. Notably, Nimzowitsch was among the pioneers in advocating for the significance of Pawn structure and valuation. Finally, we conclude by suggesting potential avenues for future research.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#22312;&#31471;&#28857;&#35774;&#22791;&#19978;&#39044;&#27979;&#23567;&#20998;&#23376;&#28342;&#35299;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38745;&#24577;&#32593;&#31449;&#36816;&#34892;&#65292;&#21516;&#26102;&#20855;&#22791;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05318</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#22312;&#31471;&#28857;&#35774;&#22791;&#19978;&#39044;&#27979;&#23567;&#20998;&#23376;&#30340;&#28342;&#35299;&#24230;
&lt;/p&gt;
&lt;p&gt;
Predicting small molecules solubilities on endpoint devices using deep ensemble neural networks. (arXiv:2307.05318v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05318
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#22312;&#31471;&#28857;&#35774;&#22791;&#19978;&#39044;&#27979;&#23567;&#20998;&#23376;&#28342;&#35299;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38745;&#24577;&#32593;&#31449;&#36816;&#34892;&#65292;&#21516;&#26102;&#20855;&#22791;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#28342;&#35299;&#24230;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#20294;&#38590;&#20197;&#39044;&#27979;&#30340;&#24615;&#36136;&#12290;&#20351;&#29992;&#19968;&#32423;&#21407;&#29702;&#26041;&#27861;&#35745;&#31639;&#28342;&#35299;&#24230;&#38656;&#35201;&#32771;&#34385;&#29109;&#21644;&#28947;&#30340;&#31454;&#20105;&#25928;&#24212;&#65292;&#23548;&#33268;&#35745;&#31639;&#26102;&#38388;&#36739;&#38271;&#19988;&#20934;&#30830;&#24615;&#30456;&#23545;&#36739;&#24046;&#12290;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#22914;&#28145;&#24230;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#20219;&#20309;&#35745;&#31639;&#25216;&#26415;&#30340;&#26131;&#29992;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#23548;&#33268;&#32676;&#20307;&#36129;&#29486;&#26041;&#27861;&#30340;&#25345;&#32493;&#27969;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#20855;&#26377;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#22312;&#38745;&#24577;&#32593;&#31449;&#19978;&#36816;&#34892;&#65288;&#26080;&#38656;&#26381;&#21153;&#22120;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#35745;&#31639;&#38656;&#27714;&#36716;&#31227;&#21040;&#32593;&#31449;&#35775;&#38382;&#32773;&#36523;&#19978;&#65292;&#32780;&#19981;&#38656;&#35201;&#23433;&#35013;&#65292;&#28040;&#38500;&#20102;&#25903;&#20184;&#21644;&#32500;&#25252;&#26381;&#21153;&#22120;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28342;&#35299;&#24230;&#39044;&#27979;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21019;&#24314;&#24179;&#34913;&#28342;&#35299;&#24230;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aqueous solubility is a valuable yet challenging property to predict. Computing solubility using first-principles methods requires accounting for the competing effects of entropy and enthalpy, resulting in long computations for relatively poor accuracy. Data-driven approaches, such as deep learning, offer improved accuracy and computational efficiency but typically lack uncertainty quantification. Additionally, ease of use remains a concern for any computational technique, resulting in the sustained popularity of group-based contribution methods. In this work, we addressed these problems with a deep learning model with predictive uncertainty that runs on a static website (without a server). This approach moves computing needs onto the website visitor without requiring installation, removing the need to pay for and maintain servers. Our model achieves satisfactory results in solubility prediction. Furthermore, we demonstrate how to create molecular property prediction models that balanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21704;&#23494;&#23572;&#39039;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNN&#65289;&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#20174;&#36712;&#36857;&#20013;&#23398;&#20064;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#22312;&#22810;&#20010;&#31995;&#32479;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#33021;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#25552;&#21462;&#31526;&#21512;&#30495;&#23454;&#24773;&#20917;&#30340;&#23450;&#24459;&#12290;</title><link>http://arxiv.org/abs/2307.05299</link><description>&lt;p&gt;
&#29992;&#21704;&#23494;&#23572;&#39039;&#22270;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20174;&#36712;&#36857;&#20013;&#21457;&#29616;&#31526;&#21495;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Discovering Symbolic Laws Directly from Trajectories with Hamiltonian Graph Neural Networks. (arXiv:2307.05299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21704;&#23494;&#23572;&#39039;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNN&#65289;&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#20174;&#36712;&#36857;&#20013;&#23398;&#20064;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#22312;&#22810;&#20010;&#31995;&#32479;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#33021;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#25552;&#21462;&#31526;&#21512;&#30495;&#23454;&#24773;&#20917;&#30340;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#30001;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#65292;&#36825;&#20123;&#26041;&#31243;&#20381;&#36182;&#20110;&#33021;&#37327;&#21644;&#21147;&#31561;&#25277;&#35937;&#37327;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#37327;&#26159;&#22522;&#20110;&#20301;&#32622;&#21644;&#36895;&#24230;&#31561;&#21487;&#35266;&#27979;&#37327;&#30340;&#27867;&#20989;&#23548;&#20986;&#30340;&#12290;&#21457;&#29616;&#36825;&#20123;&#25511;&#21046;&#31526;&#21495;&#23450;&#24459;&#26159;&#29702;&#35299;&#33258;&#28982;&#30028;&#30456;&#20114;&#20316;&#29992;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21704;&#23494;&#23572;&#39039;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNN&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#36890;&#36807;&#36712;&#36857;&#30452;&#25509;&#23398;&#20064;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#29289;&#29702;&#24378;&#21270;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;HGNN&#22312;n&#20010;&#24377;&#31783;&#12289;n&#20010;&#25670;&#21160;&#31995;&#32479;&#12289;&#24341;&#21147;&#31995;&#32479;&#21644;&#20108;&#36827;&#21046;Lennard Jones&#31995;&#32479;&#19978;&#30340;&#24615;&#33021;&#65292;HGNN&#33021;&#22815;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#20986;&#19982;&#30495;&#23454;&#24773;&#20917;&#26497;&#20339;&#19968;&#33268;&#30340;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;HGNN&#22312;&#26356;&#22823;&#31995;&#32479;&#22823;&#23567;&#20197;&#21450;&#28151;&#21512;&#24377;&#31783;-&#25670;&#21160;&#31995;&#32479;&#19978;&#30340;&#25512;&#24191;&#33021;&#21147;&#65292;&#35813;&#31995;&#32479;&#26159;&#30001;&#20004;&#20010;&#29420;&#31435;&#35757;&#32451;&#30340;&#21407;&#22987;&#31995;&#32479;&#65288;&#24377;&#31783;&#21644;&#25670;&#21160;&#65289;&#30340;&#32452;&#21512;&#26500;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#23545;&#23398;&#21040;&#30340;&#23450;&#24459;&#36827;&#34892;&#20102;&#25512;&#23548;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The time evolution of physical systems is described by differential equations, which depend on abstract quantities like energy and force. Traditionally, these quantities are derived as functionals based on observables such as positions and velocities. Discovering these governing symbolic laws is the key to comprehending the interactions in nature. Here, we present a Hamiltonian graph neural network (HGNN), a physics-enforced GNN that learns the dynamics of systems directly from their trajectory. We demonstrate the performance of HGNN on n-springs, n-pendulums, gravitational systems, and binary Lennard Jones systems; HGNN learns the dynamics in excellent agreement with the ground truth from small amounts of data. We also evaluate the ability of HGNN to generalize to larger system sizes, and to hybrid spring-pendulum system that is a combination of two original systems (spring and pendulum) on which the models are trained independently. Finally, employing symbolic regression on the learn
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#31934;&#32454;&#35821;&#35328;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;WhyShift&#23454;&#39564;&#24179;&#21488;&#65292;&#24182;&#35752;&#35770;&#20102;$Y|X$-&#20559;&#31227;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.05284</link><description>&lt;p&gt;
&#20851;&#20110;&#38656;&#35201;&#25551;&#36848;&#20998;&#24067;&#20559;&#31227;&#30340;&#35821;&#35328;&#65306;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets. (arXiv:2307.05284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#31934;&#32454;&#35821;&#35328;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;WhyShift&#23454;&#39564;&#24179;&#21488;&#65292;&#24182;&#35752;&#35770;&#20102;$Y|X$-&#20559;&#31227;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#20998;&#24067;&#20559;&#31227;&#38656;&#35201;&#19981;&#21516;&#30340;&#31639;&#27861;&#21644;&#25805;&#20316;&#24178;&#39044;&#12290;&#26041;&#27861;&#30740;&#31350;&#24517;&#39035;&#20197;&#20854;&#25152;&#28041;&#21450;&#30340;&#20855;&#20307;&#20559;&#31227;&#20026;&#22522;&#30784;&#12290;&#23613;&#31649;&#26032;&#20852;&#30340;&#22522;&#20934;&#25968;&#25454;&#20026;&#23454;&#35777;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#38544;&#21547;&#22320;&#20851;&#27880;&#21327;&#21464;&#37327;&#20559;&#31227;&#65292;&#24182;&#19988;&#23454;&#35777;&#21457;&#29616;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#20559;&#31227;&#31867;&#22411;&#65292;&#20363;&#22914;&#65292;&#24403;$Y|X$&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#20043;&#21069;&#20851;&#20110;&#31639;&#27861;&#24615;&#33021;&#30340;&#35266;&#23519;&#21487;&#33021;&#26080;&#25928;&#12290;&#25105;&#20204;&#23545;5&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;86,000&#20010;&#27169;&#22411;&#37197;&#32622;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#19968;&#31181;&#31934;&#32454;&#30340;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#35821;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;WhyShift&#65292;&#19968;&#20010;&#30001;&#31574;&#21010;&#30340;&#30495;&#23454;&#19990;&#30028;&#20559;&#31227;&#27979;&#35797;&#24179;&#21488;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#23545;&#25105;&#20204;&#22522;&#20934;&#24615;&#33021;&#30340;&#20559;&#31227;&#31867;&#22411;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#30001;&#20110;$Y|X$-&#20559;&#31227;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#24456;&#24120;&#35265;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21463;&#21040;&#26368;&#22823;$Y|X$-&#20559;&#31227;&#24433;&#21709;&#30340;&#21327;&#21464;&#37327;&#21306;&#22495;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different distribution shifts require different algorithmic and operational interventions. Methodological research must be grounded by the specific shifts they address. Although nascent benchmarks provide a promising empirical foundation, they implicitly focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the $Y|X$ distribution changes. We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that $Y|X$-shifts are most prevalent. To encourage researchers to develop a refined language for distribution shifts, we build WhyShift, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over. Since $Y|X$-shifts are prevalent in tabular settings, we identify covariate regions that suffer the biggest $Y|X$-shifts and discuss implications for algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CareFall&#65292;&#19968;&#31181;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#33258;&#21160;&#36300;&#20498;&#26816;&#27979;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#38408;&#20540;&#30340;&#26041;&#27861;&#65292;&#20026;&#35299;&#20915;&#32769;&#24180;&#20154;&#36300;&#20498;&#38382;&#39064;&#25552;&#20379;&#20102;&#26234;&#33021;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.05275</link><description>&lt;p&gt;
CareFall: &#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#33258;&#21160;&#36300;&#20498;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CareFall: Automatic Fall Detection through Wearable Devices and AI Methods. (arXiv:2307.05275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CareFall&#65292;&#19968;&#31181;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#33258;&#21160;&#36300;&#20498;&#26816;&#27979;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#38408;&#20540;&#30340;&#26041;&#27861;&#65292;&#20026;&#35299;&#20915;&#32769;&#24180;&#20154;&#36300;&#20498;&#38382;&#39064;&#25552;&#20379;&#20102;&#26234;&#33021;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#21475;&#32769;&#40836;&#21270;&#23548;&#33268;&#20102;&#20840;&#29699;&#33539;&#22260;&#20869;&#36300;&#20498;&#20107;&#20214;&#30340;&#22686;&#22810;&#65292;&#23545;&#20840;&#29699;&#20844;&#20849;&#20581;&#24247;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CareFall&#65292;&#19968;&#31181;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#33258;&#21160;&#36300;&#20498;&#26816;&#27979;&#31995;&#32479;&#65288;FDS&#65289;&#12290;CareFall&#21033;&#29992;&#20174;&#26234;&#33021;&#25163;&#34920;&#25552;&#21462;&#30340;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#26102;&#38388;&#20449;&#21495;&#12290;&#37319;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#26041;&#27861;&#65306;&#19968;&#26159;&#22522;&#20110;&#38408;&#20540;&#30340;&#26041;&#27861;&#65292;&#20108;&#26159;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#24211;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#20449;&#24687;&#32467;&#21512;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#38408;&#20540;&#30340;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#20026;&#35774;&#35745;&#26234;&#33021;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32531;&#35299;&#32769;&#24180;&#20154;&#36300;&#20498;&#30340;&#36127;&#38754;&#21518;&#26524;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aging population has led to a growing number of falls in our society, affecting global public health worldwide. This paper presents CareFall, an automatic Fall Detection System (FDS) based on wearable devices and Artificial Intelligence (AI) methods. CareFall considers the accelerometer and gyroscope time signals extracted from a smartwatch. Two different approaches are used for feature extraction and classification: i) threshold-based, and ii) machine learning-based. Experimental results on two public databases show that the machine learning-based approach, which combines accelerometer and gyroscope information, outperforms the threshold-based approach in terms of accuracy, sensitivity, and specificity. This research contributes to the design of smart and user-friendly solutions to mitigate the negative consequences of falls among older people.
&lt;/p&gt;</description></item><item><title>U-CREAT&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#20107;&#20214;&#25552;&#21462;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#26816;&#32034;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2307.05260</link><description>&lt;p&gt;
U-CREAT: &#26080;&#30417;&#30563;&#20107;&#20214;&#25552;&#21462;&#30340;&#26080;&#30417;&#30563;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
U-CREAT: Unsupervised Case Retrieval using Events extrAcTion. (arXiv:2307.05260v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05260
&lt;/p&gt;
&lt;p&gt;
U-CREAT&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#20107;&#20214;&#25552;&#21462;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#26816;&#32034;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#39046;&#22495;&#65292;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#30340;&#20219;&#21153;&#26159;&#33258;&#21160;&#24341;&#29992;&#19982;&#32473;&#23450;&#26597;&#35810;&#26696;&#20363;&#30456;&#20851;&#65288;&#22522;&#20110;&#20107;&#23454;&#21644;&#20808;&#20363;&#65289;&#30340;&#20808;&#21069;&#27861;&#24459;&#26696;&#20363;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25512;&#21160;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#30740;&#31350;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#22522;&#20934;&#65288;&#20197;&#33521;&#25991;&#20026;&#20027;&#65289;&#29992;&#20110;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#20219;&#21153;&#65306;IL-PCR&#65288;&#21360;&#24230;&#27861;&#24459;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#65289;&#35821;&#26009;&#24211;&#12290;&#32771;&#34385;&#21040;&#26696;&#20363;&#30456;&#20851;&#24615;&#30340;&#22797;&#26434;&#24615;&#21644;&#27861;&#24459;&#25991;&#26723;&#30340;&#38271;&#24230;&#65292;BM25&#20173;&#28982;&#26159;&#25490;&#21517;&#24341;&#29992;&#20808;&#21069;&#25991;&#26723;&#30340;&#24378;&#22823;&#22522;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20107;&#20214;&#22312;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#26816;&#32034;&#26041;&#27861;&#30340;&#31649;&#36947;&#31995;&#32479;U-CREAT&#65288;&#26080;&#30417;&#30563;&#20107;&#20214;&#25552;&#21462;&#30340;&#26080;&#30417;&#30563;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#25552;&#20986;&#30340;&#26080;&#30417;&#30563;&#26816;&#32034;&#26041;&#27861;&#19982;BM25&#30456;&#27604;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#26816;&#32034;&#36895;&#24230;&#22823;&#22823;&#21152;&#24555;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#23454;&#26102;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#23427;&#36866;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#27861;&#24459;&#20307;&#31995;&#65288;&#21360;&#24230;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of Prior Case Retrieval (PCR) in the legal domain is about automatically citing relevant (based on facts and precedence) prior legal cases in a given query case. To further promote research in PCR, in this paper, we propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian Legal Prior Case Retrieval) corpus. Given the complex nature of case relevance and the long size of legal documents, BM25 remains a strong baseline for ranking the cited prior documents. In this work, we explore the role of events in legal case retrieval and propose an unsupervised retrieval method-based pipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find that the proposed unsupervised retrieval method significantly increases performance compared to BM25 and makes retrieval faster by a considerable margin, making it applicable to real-time case retrieval systems. Our proposed system is generic, we show that it generalizes across two different legal systems (India
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;MAP&#21644;MLE&#30340;&#25945;&#23398;&#26041;&#27861;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#26681;&#25454;&#35266;&#23519;&#32467;&#26524;&#25512;&#26029;&#38544;&#34255;&#30340;&#27010;&#24565;&#65292;&#25945;&#24072;&#35797;&#22270;&#25214;&#21040;&#26368;&#23567;&#30340;&#35266;&#23519;&#38598;&#21512;&#20197;&#20351;&#24471;&#23398;&#20064;&#32773;&#36820;&#22238;&#29305;&#23450;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2307.05252</link><description>&lt;p&gt;
&#22522;&#20110;MAP&#21644;MLE&#30340;&#25945;&#23398;
&lt;/p&gt;
&lt;p&gt;
MAP- and MLE-Based Teaching. (arXiv:2307.05252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;MAP&#21644;MLE&#30340;&#25945;&#23398;&#26041;&#27861;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#26681;&#25454;&#35266;&#23519;&#32467;&#26524;&#25512;&#26029;&#38544;&#34255;&#30340;&#27010;&#24565;&#65292;&#25945;&#24072;&#35797;&#22270;&#25214;&#21040;&#26368;&#23567;&#30340;&#35266;&#23519;&#38598;&#21512;&#20197;&#20351;&#24471;&#23398;&#20064;&#32773;&#36820;&#22238;&#29305;&#23450;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#19968;&#20010;&#23398;&#20064;&#32773;L&#35797;&#22270;&#20174;&#19968;&#31995;&#21015;&#35266;&#23519;&#20013;&#25512;&#26029;&#20986;&#19968;&#20010;&#38544;&#34255;&#30340;&#27010;&#24565;&#12290;&#22312;Ferri&#31561;&#20154;&#30340;&#24037;&#20316;[4]&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20551;&#35774;&#23398;&#20064;&#32773;&#30001;&#20808;&#39564;P(c)&#21644;&#26465;&#20214;&#27010;&#29575;P(z|c)&#21442;&#25968;&#21270;&#65292;&#20854;&#20013;c&#33539;&#22260;&#22312;&#32473;&#23450;&#31867;&#21035;C&#20013;&#30340;&#25152;&#26377;&#27010;&#24565;&#19978;&#65292;z&#33539;&#22260;&#22312;&#35266;&#23519;&#38598;&#21512;Z&#20013;&#30340;&#25152;&#26377;&#35266;&#23519;&#19978;&#12290;&#22914;&#26524;L&#23558;&#19968;&#32452;&#35266;&#23519;&#30475;&#20316;&#26159;&#38543;&#26426;&#26679;&#26412;&#65292;&#24182;&#36820;&#22238;&#20855;&#26377;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#30340;&#27010;&#24565;&#65288;&#30456;&#24212;&#22320;&#65292;&#36820;&#22238;&#26368;&#22823;&#21270;S&#30340;c&#26465;&#20214;&#27010;&#29575;&#30340;&#27010;&#24565;&#65289;&#65292;&#21017;L&#34987;&#31216;&#20026;MAP&#23398;&#20064;&#22120;&#65288;resp. MLE&#23398;&#20064;&#22120;&#65289;&#12290;&#26681;&#25454;L&#26159;&#21542;&#20551;&#35774;S&#26159;&#20174;&#26377;&#24207;&#25110;&#26080;&#24207;&#37319;&#26679;&#65288;resp. &#26377;&#26367;&#25442;&#25110;&#26080;&#26367;&#25442;&#37319;&#26679;&#65289;&#33719;&#24471;&#30340;&#65292;&#21487;&#20197;&#21306;&#20998;&#22235;&#31181;&#19981;&#21516;&#30340;&#37319;&#26679;&#27169;&#24335;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#30446;&#26631;&#27010;&#24565;c&#22312;C&#20013;&#65292;&#23545;&#20110;MAP&#23398;&#20064;&#22120;L&#26469;&#35828;&#65292;&#25945;&#24072;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#26368;&#23567;&#30340;&#35266;&#23519;&#38598;&#21512;&#65292;&#20351;&#24471;L&#36820;&#22238;c&#12290;&#36825;&#31181;&#26041;&#27861;&#33258;&#28982;&#22320;&#23548;&#33268;&#20102;&#21508;&#31181;MAP&#25110;MLE&#25945;&#23398;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imagine a learner L who tries to infer a hidden concept from a collection of observations. Building on the work [4] of Ferri et al., we assume the learner to be parameterized by priors P(c) and by c-conditional likelihoods P(z|c) where c ranges over all concepts in a given class C and z ranges over all observations in an observation set Z. L is called a MAP-learner (resp. an MLE-learner) if it thinks of a collection S of observations as a random sample and returns the concept with the maximum a-posteriori probability (resp. the concept which maximizes the c-conditional likelihood of S). Depending on whether L assumes that S is obtained from ordered or unordered sampling resp. from sampling with or without replacement, we can distinguish four different sampling modes. Given a target concept c in C, a teacher for a MAP-learner L aims at finding a smallest collection of observations that causes L to return c. This approach leads in a natural manner to various notions of a MAP- or MLE-teac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#36335;&#30001;&#21644;&#36328;&#23618;&#36830;&#25509;&#26469;&#35299;&#20915;&#22810;&#20013;&#24515;PET&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#22495;&#36716;&#31227;&#21644;&#20013;&#24515;&#24178;&#25200;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05249</link><description>&lt;p&gt;
DRMC: &#19968;&#31181;&#20855;&#26377;&#21160;&#24577;&#36335;&#30001;&#30340;&#36890;&#29992;&#27169;&#22411;&#29992;&#20110;&#22810;&#20013;&#24515;PET&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
DRMC: A Generalist Model with Dynamic Routing for Multi-Center PET Image Synthesis. (arXiv:2307.05249v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#36335;&#30001;&#21644;&#36328;&#23618;&#36830;&#25509;&#26469;&#35299;&#20915;&#22810;&#20013;&#24515;PET&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#22495;&#36716;&#31227;&#21644;&#20013;&#24515;&#24178;&#25200;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20013;&#24515;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;&#65288;PET&#65289;&#22270;&#20687;&#21512;&#25104;&#26088;&#22312;&#20174;&#22810;&#20010;&#19981;&#21516;&#20013;&#24515;&#24674;&#22797;&#20302;&#21058;&#37327;PET&#22270;&#20687;&#12290;&#30001;&#20110;&#19981;&#21516;&#20013;&#24515;&#20855;&#26377;&#19981;&#21516;&#30340;&#25104;&#20687;&#31995;&#32479;/&#21327;&#35758;&#21644;&#25968;&#25454;&#20998;&#24067;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#20173;&#28982;&#19981;&#22815;&#29702;&#24819;&#65292;&#20250;&#23548;&#33268;&#22495;&#36716;&#31227;&#38382;&#39064;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#36890;&#36807;&#20026;&#27599;&#20010;&#20013;&#24515;&#35757;&#32451;&#19987;&#38376;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#21442;&#25968;&#25928;&#29575;&#20302;&#65292;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#20013;&#24515;&#20043;&#38388;&#30340;&#20849;&#20139;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#20013;&#24515;&#20043;&#38388;&#20849;&#20139;&#26550;&#26500;&#21644;&#21442;&#25968;&#20197;&#21033;&#29992;&#20849;&#20139;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#27169;&#22411;&#21487;&#33021;&#20250;&#36935;&#21040;&#20013;&#24515;&#24178;&#25200;&#30340;&#38382;&#39064;&#65292;&#21363;&#30001;&#20110;&#38750;&#30456;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#19981;&#21516;&#20013;&#24515;&#30340;&#26799;&#24230;&#26041;&#21521;&#21487;&#33021;&#19981;&#19968;&#33268;&#29978;&#33267;&#30456;&#21453;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#24178;&#25200;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#36335;&#30001;&#31574;&#30053;&#21644;&#36328;&#23618;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-center positron emission tomography (PET) image synthesis aims at recovering low-dose PET images from multiple different centers. The generalizability of existing methods can still be suboptimal for a multi-center study due to domain shifts, which result from non-identical data distribution among centers with different imaging systems/protocols. While some approaches address domain shifts by training specialized models for each center, they are parameter inefficient and do not well exploit the shared knowledge across centers. To address this, we develop a generalist model that shares architecture and parameters across centers to utilize the shared knowledge. However, the generalist model can suffer from the center interference issue, \textit{i.e.} the gradient directions of different centers can be inconsistent or even opposite owing to the non-identical data distribution. To mitigate such interference, we introduce a novel dynamic routing strategy with cross-layer connections th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.05232</link><description>&lt;p&gt;
&#20174;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#21040;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey From Distributed Machine Learning to Distributed Deep Learning. (arXiv:2307.05232v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05232
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#21151;&#12290;&#36825;&#19968;&#25104;&#21151;&#24402;&#21151;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#30828;&#20214;&#21152;&#36895;&#30340;&#36827;&#27493;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#21644;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#31639;&#27861;&#24517;&#39035;&#29992;&#26356;&#22810;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20040;&#22823;&#37327;&#30340;&#25968;&#25454;&#21487;&#33021;&#38656;&#35201;&#28040;&#32791;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#22788;&#29702;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#21644;&#31639;&#27861;&#20998;&#24067;&#22312;&#22810;&#21488;&#26426;&#22120;&#19978;&#65292;&#21487;&#20197;&#23454;&#29616;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#34987;&#31216;&#20026;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#12290;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26041;&#38754;&#65292;&#24050;&#32463;&#25237;&#20837;&#20102;&#30456;&#24403;&#22810;&#30340;&#21162;&#21147;&#65292;&#30446;&#21069;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#32508;&#36848;&#65292;&#23545;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#29366;&#24577;&#36827;&#34892;&#20840;&#38754;&#24635;&#32467;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#31639;&#27861;&#20998;&#25104;&#20998;&#31867;&#21644;&#32858;&#31867;&#65288;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#65289;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20960;&#32452;&#12290;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#30740;&#31350;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence has achieved significant success in handling complex tasks in recent years. This success is due to advances in machine learning algorithms and hardware acceleration. In order to obtain more accurate results and solve more complex problems, algorithms must be trained with more data. This huge amount of data could be time-consuming to process and require a great deal of computation. This solution could be achieved by distributing the data and algorithm across several machines, which is known as distributed machine learning. There has been considerable effort put into distributed machine learning algorithms, and different methods have been proposed so far. In this article, we present a comprehensive summary of the current state-of-the-art in the field through the review of these algorithms. We divide this algorithms in classification and clustering (traditional machine learning), deep learning and deep reinforcement learning groups. Distributed deep learning has ga
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23454;&#20363;&#32423;&#25511;&#21046;&#20195;&#30721;&#30340;&#23545;&#35805;&#24341;&#23548;&#31639;&#27861;&#65292;&#29992;&#20110;&#25506;&#32034;&#23454;&#20363;&#29305;&#23450;&#30340;&#25552;&#31034;&#23545;&#20110;&#25511;&#21046;&#23545;&#35805;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#25552;&#31034;&#22522;&#32447;&#65292;&#24182;&#19988;&#19982;&#20165;&#20351;&#29992;&#24635;&#21442;&#25968;&#30340;&#24494;&#35843;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2307.05228</link><description>&lt;p&gt;
&#23646;&#24615;&#25511;&#21046;&#30340;&#23545;&#35805;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Attribute Controlled Dialogue Prompting. (arXiv:2307.05228v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23454;&#20363;&#32423;&#25511;&#21046;&#20195;&#30721;&#30340;&#23545;&#35805;&#24341;&#23548;&#31639;&#27861;&#65292;&#29992;&#20110;&#25506;&#32034;&#23454;&#20363;&#29305;&#23450;&#30340;&#25552;&#31034;&#23545;&#20110;&#25511;&#21046;&#23545;&#35805;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#25552;&#31034;&#22522;&#32447;&#65292;&#24182;&#19988;&#19982;&#20165;&#20351;&#29992;&#24635;&#21442;&#25968;&#30340;&#24494;&#35843;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#25552;&#31034;&#35843;&#25972;&#24050;&#25104;&#20026;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#31163;&#25955;&#25552;&#31034;&#21644;&#36830;&#32493;&#25552;&#31034;&#37117;&#20551;&#35774;&#20219;&#21153;&#20013;&#30340;&#25152;&#26377;&#25968;&#25454;&#26679;&#26412;&#20351;&#29992;&#30456;&#21516;&#30340;&#22266;&#23450;&#25552;&#31034;&#65292;&#24573;&#30053;&#20102;&#26576;&#20123;&#20219;&#21153;&#65288;&#22914;&#24320;&#25918;&#22495;&#23545;&#35805;&#29983;&#25104;&#65289;&#20013;&#36755;&#20837;&#30340;&#24040;&#22823;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#22522;&#20110;&#23454;&#20363;&#32423;&#25511;&#21046;&#20195;&#30721;&#30340;&#23545;&#35805;&#24341;&#23548;&#31639;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22522;&#20110;&#23454;&#20363;&#32423;&#25511;&#21046;&#20195;&#30721;&#32780;&#19981;&#26159;&#23545;&#35805;&#21382;&#21490;&#29983;&#25104;&#25552;&#31034;&#65292;&#20197;&#25506;&#32034;&#23454;&#20363;&#29305;&#23450;&#30340;&#25552;&#31034;&#23545;&#20110;&#25511;&#21046;&#23545;&#35805;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;&#22312;&#27969;&#34892;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#22312;&#33258;&#21160;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#26041;&#38754;&#37117;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25552;&#31034;&#22522;&#32447;&#65292;&#24182;&#19988;&#19982;&#20165;&#20351;&#29992;&#24635;&#21442;&#25968;&#30340;5%-6%&#30340;&#24494;&#35843;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks. However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation. In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation. Specifically, we generate prompts based on instance-level control code, rather than the conversation history, to explore their impact on controlled dialogue generation. Experiments on popular open-domain dialogue datasets, evaluated on both automated metrics and human evaluation, demonstrate that our method is superior to prompting baselines and comparable to fine-tuning with only 5%-6% of total parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#24212;&#29992;&#65292;&#20197;&#40723;&#21169;&#20849;&#20139;&#30456;&#21516;&#31867;&#21035;&#26631;&#31614;&#30340;&#33410;&#28857;&#33719;&#24471;&#26356;&#39640;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#24182;&#22312;&#22810;&#20010;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27604;&#26631;&#20934;&#22522;&#32447;&#27169;&#22411;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05217</link><description>&lt;p&gt;
&#22522;&#20110;&#21516;&#36136;&#24615;&#30340;&#26377;&#30417;&#30563;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Supervised Attention Using Homophily in Graph Neural Networks. (arXiv:2307.05217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#24212;&#29992;&#65292;&#20197;&#40723;&#21169;&#20849;&#20139;&#30456;&#21516;&#31867;&#21035;&#26631;&#31614;&#30340;&#33410;&#28857;&#33719;&#24471;&#26356;&#39640;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#24182;&#22312;&#22810;&#20010;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27604;&#26631;&#20934;&#22522;&#32447;&#27169;&#22411;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#22788;&#29702;&#22270;&#19978;&#23398;&#20064;&#38382;&#39064;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#22312;&#19981;&#21516;&#21464;&#31181;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GATs&#65289;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#22312;GAT&#27169;&#22411;&#20013;&#65292;&#27599;&#20010;&#33410;&#28857;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#20026;&#20854;&#37051;&#23621;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#20854;&#20182;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;GAT&#32858;&#21512;&#26469;&#33258;&#23646;&#20110;&#19981;&#21516;&#31867;&#21035;&#30340;&#33410;&#28857;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#20135;&#29983;&#30340;&#33410;&#28857;&#34920;&#31034;&#22312;&#19981;&#21516;&#31867;&#21035;&#26041;&#38754;&#19981;&#22815;&#26126;&#30830;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#23558;&#20854;&#32435;&#20837;&#21040;&#20219;&#20309;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#65292;&#20197;&#40723;&#21169;&#20849;&#20139;&#30456;&#21516;&#31867;&#21035;&#26631;&#31614;&#30340;&#33410;&#28857;&#20043;&#38388;&#33719;&#24471;&#26356;&#39640;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#26631;&#20934;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks have become the standard approach for dealing with learning problems on graphs. Among the different variants of graph neural networks, graph attention networks (GATs) have been applied with great success to different tasks. In the GAT model, each node assigns an importance score to its neighbors using an attention mechanism. However, similar to other graph neural networks, GATs aggregate messages from nodes that belong to different classes, and therefore produce node representations that are not well separated with respect to the different classes, which might hurt their performance. In this work, to alleviate this problem, we propose a new technique that can be incorporated into any graph attention model to encourage higher attention scores between nodes that share the same class label. We evaluate the proposed method on several node classification datasets demonstrating increased performance over standard baseline models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#35780;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#21442;&#25968;&#20998;&#24067;&#26469;&#35745;&#31639;&#20915;&#31574;&#28966;&#28857;&#27169;&#22411;&#30340;&#26356;&#26032;&#65292;&#20197;&#25193;&#22823;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05213</link><description>&lt;p&gt;
&#35780;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#20197;&#25193;&#22823;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Score Function Gradient Estimation to Widen the Applicability of Decision-Focused Learning. (arXiv:2307.05213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#35780;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#21442;&#25968;&#20998;&#24067;&#26469;&#35745;&#31639;&#20915;&#31574;&#28966;&#28857;&#27169;&#22411;&#30340;&#26356;&#26032;&#65292;&#20197;&#25193;&#22823;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#38382;&#39064;&#37117;&#21253;&#21547;&#38656;&#35201;&#22312;&#35299;&#20915;&#20043;&#21069;&#36827;&#34892;&#39044;&#27979;&#30340;&#26410;&#30693;&#21442;&#25968;&#12290;&#20026;&#20102;&#35757;&#32451;&#28041;&#21450;&#30340;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#36890;&#24120;&#37319;&#29992;&#30340;&#26041;&#27861;&#26159;&#19987;&#27880;&#20110;&#26368;&#22823;&#21270;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#24635;&#26159;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#25439;&#22833;&#30340;&#26368;&#23567;&#21270;&#12290;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#33539;&#24335;&#65292;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#30452;&#25509;&#26368;&#23567;&#21270;&#20219;&#21153;&#25439;&#22833;&#26469;&#35757;&#32451;ML&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;DFL&#26041;&#27861;&#21463;&#21040;&#23427;&#20204;&#23545;&#20248;&#21270;&#38382;&#39064;&#32467;&#26500;&#30340;&#20551;&#35774;&#65288;&#20363;&#22914;&#65292;&#38382;&#39064;&#26159;&#32447;&#24615;&#30340;&#65289;&#20197;&#21450;&#21482;&#33021;&#39044;&#27979;&#20986;&#29616;&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#30340;&#21442;&#25968;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30456;&#21453;&#22320;&#39044;&#27979;&#21442;&#25968;&#30340;&#20998;&#24067;&#65292;&#24182;&#37319;&#29992;&#35780;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#65288;SFGE&#65289;&#26469;&#35745;&#31639;&#20915;&#31574;&#28966;&#28857;&#27169;&#22411;&#30340;&#26356;&#26032;&#65292;&#20174;&#32780;&#25193;&#22823;DFL&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
Many real-world optimization problems contain unknown parameters that must be predicted prior to solving. To train the predictive machine learning (ML) models involved, the commonly adopted approach focuses on maximizing predictive accuracy. However, this approach does not always lead to the minimization of the downstream task loss. Decision-focused learning (DFL) is a recently proposed paradigm whose goal is to train the ML model by directly minimizing the task loss. However, state-of-the-art DFL methods are limited by the assumptions they make about the structure of the optimization problem (e.g., that the problem is linear) and by the fact that can only predict parameters that appear in the objective function. In this work, we address these limitations by instead predicting \textit{distributions} over parameters and adopting score function gradient estimation (SFGE) to compute decision-focused updates to the predictive model, thereby widening the applicability of DFL. Our experiment
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#65292;&#24182;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#25552;&#21319;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;</title><link>http://arxiv.org/abs/2307.05209</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#30340;&#19978;&#19979;&#25991;&#39044;&#35268;&#21010;&#20197;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning. (arXiv:2307.05209v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05209
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#65292;&#24182;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#25552;&#21319;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#20542;&#21521;&#20110;&#36807;&#25311;&#21512;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#19988;&#26080;&#27861;&#36866;&#24212;&#36731;&#24494;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;&#20026;&#20102;&#22312;&#36716;&#31227;&#21040;&#26410;&#35265;&#20219;&#21153;&#26102;&#21152;&#24555;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#65288;RM&#65289;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#22870;&#21169;&#26426;&#22120;&#26159;&#22522;&#20110;&#24403;&#21069;&#20219;&#21153;&#30340;&#22870;&#21169;&#21644;&#21160;&#24577;&#29983;&#25104;&#23376;&#20219;&#21153;&#30340;&#29366;&#24577;&#26426;&#25277;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20195;&#29702;&#25552;&#20379;&#20102;&#24403;&#21069;&#25277;&#35937;&#29366;&#24577;&#30340;&#31526;&#21495;&#34920;&#31034;&#65292;&#24182;&#22870;&#21169;&#23427;&#20204;&#36798;&#25104;&#36825;&#20123;&#36716;&#25442;&#12290;&#36825;&#20123;&#34920;&#31034;&#22312;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#20808;&#21069;&#36935;&#21040;&#30340;&#31526;&#21495;&#21644;&#36716;&#25442;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#36801;&#31227;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#34920;&#31034;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RM), state machine abstractions that induce subtasks based on the current task's rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Our empirical evaluation shows that our representations improve sample efficiency and few-shot transfer in a variety of domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#25298;&#32477;&#36873;&#39033;&#27169;&#22411;&#26469;&#22788;&#29702;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21452;&#35780;&#20998;&#36229;&#20986;&#20998;&#24067;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.05199</link><description>&lt;p&gt;
&#25298;&#32477;&#36873;&#39033;&#27169;&#22411;&#20013;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Reject option models comprising out-of-distribution detection. (arXiv:2307.05199v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#25298;&#32477;&#36873;&#39033;&#27169;&#22411;&#26469;&#22788;&#29702;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21452;&#35780;&#20998;&#36229;&#20986;&#20998;&#24067;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#30340;&#26368;&#20248;&#39044;&#27979;&#31574;&#30053;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#30340;&#25298;&#32477;&#36873;&#39033;&#27169;&#22411;&#65306;&#22522;&#20110;&#25104;&#26412;&#30340;&#27169;&#22411;&#12289;&#26377;&#30028;TPR-FPR&#27169;&#22411;&#21644;&#26377;&#30028;&#31934;&#30830;&#24230;-&#21484;&#22238;&#29575;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#25193;&#23637;&#20102;&#38750;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#25298;&#32477;&#36873;&#39033;&#27169;&#22411;&#65292;&#24182;&#23450;&#20041;&#20102;&#26368;&#20248;&#36229;&#20986;&#20998;&#24067;&#36873;&#25321;&#20998;&#31867;&#22120;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#20844;&#24335;&#19978;&#19981;&#21516;&#65292;&#20294;&#23427;&#20204;&#37117;&#20849;&#20139;&#19968;&#31867;&#26368;&#20248;&#31574;&#30053;&#12290;&#21463;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21452;&#35780;&#20998;&#36229;&#20986;&#20998;&#24067;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#20004;&#20010;&#36873;&#25321;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#22120;&#30340;&#19981;&#30830;&#23450;&#24230;&#35780;&#20998;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#36229;&#20986;&#20998;&#24067;/&#20869;&#37096;&#20998;&#24067;&#30340;&#21306;&#20998;&#65292;&#21478;&#19968;&#20010;&#19987;&#27880;&#20110;&#35823;&#20998;&#31867;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#19968;&#33268;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#31616;&#21333;&#31574;&#30053;&#30340;&#24615;&#33021;&#26356;&#20248;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal prediction strategy for out-of-distribution (OOD) setups is a fundamental question in machine learning. In this paper, we address this question and present several contributions. We propose three reject option models for OOD setups: the Cost-based model, the Bounded TPR-FPR model, and the Bounded Precision-Recall model. These models extend the standard reject option models used in non-OOD setups and define the notion of an optimal OOD selective classifier. We establish that all the proposed models, despite their different formulations, share a common class of optimal strategies. Motivated by the optimal strategy, we introduce double-score OOD methods that leverage uncertainty scores from two chosen OOD detectors: one focused on OOD/ID discrimination and the other on misclassification detection. The experimental results consistently demonstrate the superior performance of this simple strategy compared to state-of-the-art methods. Additionally, we propose novel evaluation met
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;$\beta$-&#20998;&#35299;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\beta$D-Bayes&#65292;&#19968;&#31181;&#33021;&#22815;&#23454;&#29616;&#24046;&#20998;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.05194</link><description>&lt;p&gt;
&#36890;&#36807;$\beta$-&#20998;&#35299;&#19968;&#21518;&#39564;&#37319;&#26679;&#23454;&#29616;&#24046;&#20998;&#35745;&#31639;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Statistical Inference through $\beta$-Divergence One Posterior Sampling. (arXiv:2307.05194v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05194
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;$\beta$-&#20998;&#35299;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\beta$D-Bayes&#65292;&#19968;&#31181;&#33021;&#22815;&#23454;&#29616;&#24046;&#20998;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#31169;&#23494;&#24615;&#30830;&#20445;&#20102;&#21253;&#21547;&#25935;&#24863;&#25968;&#25454;&#30340;&#32479;&#35745;&#20998;&#26512;&#32467;&#26524;&#21487;&#20197;&#22312;&#19981;&#25439;&#23475;&#20219;&#20309;&#20010;&#20307;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21457;&#24067;&#12290;&#23454;&#29616;&#36825;&#31181;&#20445;&#35777;&#36890;&#24120;&#38656;&#35201;&#22312;&#21442;&#25968;&#20272;&#35745;&#25110;&#20272;&#35745;&#36807;&#31243;&#20013;&#30452;&#25509;&#27880;&#20837;&#22122;&#38899;&#12290;&#32780;&#37319;&#26679;&#26469;&#33258;&#36125;&#21494;&#26031;&#21518;&#39564;&#20998;&#24067;&#24050;&#34987;&#35777;&#26126;&#26159;&#25351;&#25968;&#26426;&#21046;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#33268;&#19988;&#39640;&#25928;&#30340;&#31169;&#23494;&#20272;&#35745;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#30340;&#24212;&#29992;&#21463;&#21040;&#36739;&#24378;&#30340;&#36793;&#30028;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#20551;&#35774;&#23545;&#20110;&#22522;&#26412;&#27169;&#22411;&#65288;&#22914;&#31616;&#21333;&#30340;&#32447;&#24615;&#22238;&#24402;&#22120;&#65289;&#24182;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\beta$D-Bayes&#65292;&#19968;&#31181;&#20174;&#24191;&#20041;&#21518;&#39564;&#20013;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#30340;&#26041;&#26696;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#27169;&#22411;&#19982;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#30340;$\beta$-&#20998;&#35299;&#12290;&#36825;&#25552;&#20379;&#20102;&#31169;&#23494;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential privacy guarantees allow the results of a statistical analysis involving sensitive data to be released without compromising the privacy of any individual taking part. Achieving such guarantees generally requires the injection of noise, either directly into parameter estimates or into the estimation process. Instead of artificially introducing perturbations, sampling from Bayesian posterior distributions has been shown to be a special case of the exponential mechanism, producing consistent, and efficient private estimates without altering the data generative process. The application of current approaches has, however, been limited by their strong bounding assumptions which do not hold for basic models, such as simple linear regressors. To ameliorate this, we propose $\beta$D-Bayes, a posterior sampling scheme from a generalised posterior targeting the minimisation of the $\beta$-divergence between the model and the data generating process. This provides private estimation t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#25239;&#25200;&#21160;&#30340;DNN&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;MI&#25915;&#20987;&#36827;&#34892;&#32479;&#19968;&#24182;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22312;&#31616;&#21333;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.05193</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#25239;&#25200;&#21160;&#30340;DNN&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attacks on DNNs using Adversarial Perturbations. (arXiv:2307.05193v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05193
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#25239;&#25200;&#21160;&#30340;DNN&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;MI&#25915;&#20987;&#36827;&#34892;&#32479;&#19968;&#24182;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22312;&#31616;&#21333;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#25104;&#21592;&#25512;&#26029;&#65288;MI&#65289;&#25915;&#20987;&#26041;&#27861;&#26469;&#23457;&#35745;&#30446;&#26631;DNN&#12290;&#32473;&#23450;&#19968;&#32452;&#20027;&#39064;&#65292;MI&#25915;&#20987;&#21487;&#20197;&#21578;&#35785;&#25105;&#20204;&#30446;&#26631;DNN&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#20102;&#21738;&#20123;&#20027;&#39064;&#12290;&#26412;&#25991;&#20851;&#27880;&#21518;&#26399;&#35757;&#32451;MI&#25915;&#20987;&#65292;&#24378;&#35843;&#22312;&#20302;&#34394;&#20551;&#38451;&#24615;&#29575;&#65288;FPR&#65289;&#19979;&#30340;&#39640;&#32622;&#20449;&#24230;&#25104;&#21592;&#26816;&#27979;--&#30495;&#27491;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#12290;&#30446;&#21069;&#22312;&#36825;&#20010;&#31867;&#21035;&#20013;&#30340;&#24037;&#20316;--&#20284;&#28982;&#27604;&#25915;&#20987;&#65288;LiRA&#65289;&#21644;&#22686;&#24378;MI&#25915;&#20987;&#65288;EMIA&#65289;--&#21482;&#22312;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#65288;&#27604;&#22914;CIFAR-10&#21644;Imagenet&#65289;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#26356;&#31616;&#21333;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#65288;Fashion-MNIST&#19978;&#20004;&#31181;&#25915;&#20987;&#37117;&#26159;0%&#30340;TPR&#65292;MNIST&#19978;&#20998;&#21035;&#26159;1% FPR&#30340;2%&#21644;0% TPR&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#39318;&#20808;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#30340;&#26694;&#26550;&#65288;&#20934;&#22791;&#12289;&#25351;&#31034;&#21644;&#20915;&#31574;&#65289;&#26469;&#32479;&#19968;&#24403;&#21069;&#30340;MI&#25915;&#20987;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65306;&#65288;1&#65289;&#23545;&#25239;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;AMIA&#65289;&#26377;&#25928;&#22320;&#21033;&#29992;&#25104;&#21592;&#20998;&#31867;&#20013;&#30340;&#29305;&#24449;&#36827;&#34892;&#25915;&#20987;&#12290;(2) Emphasizing-on-Misclassified Samples Attack (EMSA) &#36890;&#36807;&#24378;&#35843;&#38169;&#35823;&#20998;&#31867;&#30340;&#26679;&#26412;&#26469;&#25552;&#39640;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several membership inference (MI) attacks have been proposed to audit a target DNN. Given a set of subjects, MI attacks tell which subjects the target DNN has seen during training. This work focuses on the post-training MI attacks emphasizing high confidence membership detection -- True Positive Rates (TPR) at low False Positive Rates (FPR). Current works in this category -- likelihood ratio attack (LiRA) and enhanced MI attack (EMIA) -- only perform well on complex datasets (e.g., CIFAR-10 and Imagenet) where the target DNN overfits its train set, but perform poorly on simpler datasets (0% TPR by both attacks on Fashion-MNIST, 2% and 0% TPR respectively by LiRA and EMIA on MNIST at 1% FPR). To address this, firstly, we unify current MI attacks by presenting a framework divided into three stages -- preparation, indication and decision. Secondly, we utilize the framework to propose two novel attacks: (1) Adversarial Membership Inference Attack (AMIA) efficiently utilizes the membership 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#26469;&#36845;&#20195;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36755;&#20986;&#21521;&#21518;&#35745;&#31639;&#31070;&#32463;&#20803;&#30340;&#29702;&#24819;&#24635;&#36755;&#20837;&#20540;&#65292;&#20197;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#36845;&#20195;&#26356;&#26032;&#21442;&#25968;&#21644;&#28608;&#27963;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.05189</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#36845;&#20195;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Using Linear Regression for Iteratively Training Neural Networks. (arXiv:2307.05189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05189
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#26469;&#36845;&#20195;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36755;&#20986;&#21521;&#21518;&#35745;&#31639;&#31070;&#32463;&#20803;&#30340;&#29702;&#24819;&#24635;&#36755;&#20837;&#20540;&#65292;&#20197;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#36845;&#20195;&#26356;&#26032;&#21442;&#25968;&#21644;&#28608;&#27963;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#32447;&#24615;&#22238;&#24402;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#65292;&#20316;&#20026;&#26631;&#20934;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#26159;&#25506;&#32034;&#24615;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#23558;&#25551;&#36848;&#21644;&#23454;&#39564;&#38480;&#21046;&#22312;&#65288;i&#65289;&#31616;&#21333;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#65288;ii&#65289;&#26631;&#37327;&#65288;&#21333;&#36755;&#20986;&#65289;&#22238;&#24402;&#38382;&#39064;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#21487;&#36870;&#28608;&#27963;&#20989;&#25968;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#26356;&#22823;&#12289;&#26356;&#22797;&#26434;&#30340;&#26550;&#26500;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#35266;&#23519;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#36755;&#20837;&#26159;&#21069;&#19968;&#23618;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#20197;&#21450;&#35813;&#23618;&#30340;&#21442;&#25968;&#65288;&#26435;&#37325;&#21644;&#20559;&#32622;&#65289;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#20174;&#36755;&#20986;&#21521;&#21518;&#35745;&#31639;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#29702;&#24819;&#24635;&#36755;&#20837;&#20540;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#23398;&#20064;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#22312;&#26356;&#26032;&#21442;&#25968;&#21644;&#28608;&#27963;&#20540;&#20043;&#38388;&#36845;&#20195;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#31639;&#27861;&#26469;&#23454;&#29616;&#36825;&#20010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple linear regression based approach for learning the weights and biases of a neural network, as an alternative to standard gradient based backpropagation. The present work is exploratory in nature, and we restrict the description and experiments to (i) simple feedforward neural networks, (ii) scalar (single output) regression problems, and (iii) invertible activation functions. However, the approach is intended to be extensible to larger, more complex architectures. The key idea is the observation that the input to every neuron in a neural network is a linear combination of the activations of neurons in the previous layer, as well as the parameters (weights and biases) of the layer. If we are able to compute the ideal total input values to every neuron by working backwards from the output, we can formulate the learning problem as a linear least squares problem which iterates between updating the parameters and the activation values. We present an explicit algorithm tha
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#20984;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#27714;&#35299;&#22120;&#36827;&#34892;&#21435;&#30456;&#20851;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23558;&#36830;&#32493;&#29305;&#24449;&#31354;&#38388;&#19982;&#21463;&#20445;&#25252;&#23646;&#24615;&#21435;&#30456;&#20851;&#12290;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21943;&#27880;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#26356;&#22909;&#22320;&#21435;&#30456;&#20851;&#22810;&#31867;&#36755;&#20986;&#29305;&#24449;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.05187</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#21435;&#30456;&#20851;
&lt;/p&gt;
&lt;p&gt;
Decorrelation using Optimal Transport. (arXiv:2307.05187v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05187
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#20984;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#27714;&#35299;&#22120;&#36827;&#34892;&#21435;&#30456;&#20851;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23558;&#36830;&#32493;&#29305;&#24449;&#31354;&#38388;&#19982;&#21463;&#20445;&#25252;&#23646;&#24615;&#21435;&#30456;&#20851;&#12290;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21943;&#27880;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#26356;&#22909;&#22320;&#21435;&#30456;&#20851;&#22810;&#31867;&#36755;&#20986;&#29305;&#24449;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20174;&#21463;&#20445;&#25252;&#23646;&#24615;&#20013;&#23558;&#29305;&#24449;&#31354;&#38388;&#21435;&#30456;&#20851;&#26159;&#20262;&#29702;&#23398;&#12289;&#20844;&#24179;&#24615;&#20197;&#21450;&#33258;&#28982;&#31185;&#23398;&#39046;&#22495;&#27963;&#36291;&#30740;&#31350;&#21644;&#23398;&#20064;&#30340;&#19968;&#20010;&#39046;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#21033;&#29992;&#20984;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#27714;&#35299;&#22120;&#65288;Cnots&#65289;&#36827;&#34892;&#21435;&#30456;&#20851;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#20351;&#36830;&#32493;&#29305;&#24449;&#31354;&#38388;&#19982;&#21463;&#20445;&#25252;&#23646;&#24615;&#21435;&#30456;&#20851;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#36827;&#34892;&#21943;&#27880;&#20998;&#31867;&#26102;&#65292;&#23427;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#20854;&#20013;&#20998;&#31867;&#22120;&#24471;&#20998;&#24076;&#26395;&#19982;&#21943;&#27880;&#30340;&#36136;&#37327;&#21435;&#30456;&#20851;&#12290;&#22312;&#20108;&#36827;&#21046;&#20998;&#31867;&#20013;&#23454;&#29616;&#30340;&#21435;&#30456;&#20851;&#31243;&#24230;&#25509;&#36817;&#20110;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#24403;&#36716;&#21521;&#22810;&#31867;&#36755;&#20986;&#26102;&#65292;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;&#30340;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#34920;&#26126;&#22312;&#21435;&#30456;&#20851;&#22810;&#32500;&#29305;&#24449;&#31354;&#38388;&#26041;&#38754;&#26377;&#23454;&#36136;&#24615;&#30340;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to decorrelate a feature space from protected attributes is an area of active research and study in ethics, fairness, and also natural sciences. We introduce a novel decorrelation method using Convex Neural Optimal Transport Solvers (Cnots), that is able to decorrelate continuous feature space against protected attributes with optimal transport. We demonstrate how well it performs in the context of jet classification in high energy physics, where classifier scores are desired to be decorrelated from the mass of a jet. The decorrelation achieved in binary classification approaches the levels achieved by the state-of-the-art using conditional normalising flows. When moving to multiclass outputs the optimal transport approach performs significantly better than the state-of-the-art, suggesting substantial gains at decorrelating multidimensional feature spaces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20272;&#35745;&#38085;&#37240;&#30005;&#27744;&#30340;SoH&#21644;RUL&#30340;&#26144;&#23556;&#30740;&#31350;&#12290;&#27491;&#30830;&#30340;&#20272;&#35745;&#36825;&#20004;&#20010;&#25351;&#26631;&#21487;&#20197;&#25552;&#39640;&#30005;&#27744;&#31995;&#32479;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#12289;&#21487;&#38752;&#24615;&#21644;&#23551;&#21629;&#12290;&#36825;&#23545;&#20110;&#30005;&#21160;&#27773;&#36710;&#12289;&#21487;&#20877;&#29983;&#33021;&#28304;&#31995;&#32479;&#31561;&#20381;&#36182;&#27492;&#30005;&#27744;&#25216;&#26415;&#30340;&#24212;&#29992;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2307.05163</link><description>&lt;p&gt;
&#21097;&#20313;&#23551;&#21629;&#20272;&#35745;&#20013;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26144;&#23556;&#30740;&#31350;&#65306;&#20197;&#38085;&#37240;&#30005;&#27744;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Mapping Study of Machine Learning Methods for Remaining Useful Life Estimation of Lead-Acid Batteries. (arXiv:2307.05163v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20272;&#35745;&#38085;&#37240;&#30005;&#27744;&#30340;SoH&#21644;RUL&#30340;&#26144;&#23556;&#30740;&#31350;&#12290;&#27491;&#30830;&#30340;&#20272;&#35745;&#36825;&#20004;&#20010;&#25351;&#26631;&#21487;&#20197;&#25552;&#39640;&#30005;&#27744;&#31995;&#32479;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#12289;&#21487;&#38752;&#24615;&#21644;&#23551;&#21629;&#12290;&#36825;&#23545;&#20110;&#30005;&#21160;&#27773;&#36710;&#12289;&#21487;&#20877;&#29983;&#33021;&#28304;&#31995;&#32479;&#31561;&#20381;&#36182;&#27492;&#30005;&#27744;&#25216;&#26415;&#30340;&#24212;&#29992;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#20648;&#23384;&#35299;&#20915;&#26041;&#26696;&#22312;&#29616;&#20195;&#22522;&#30784;&#35774;&#26045;&#20013;&#25198;&#28436;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#32780;&#38085;&#37240;&#30005;&#27744;&#21017;&#26159;&#21487;&#20805;&#30005;&#31867;&#21035;&#20013;&#26368;&#24120;&#29992;&#30340;&#19968;&#31181;&#12290;&#30001;&#20110;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#20250;&#36880;&#28176;&#36864;&#21270;&#65292;&#27491;&#30830;&#30830;&#23450;&#30005;&#27744;&#30340;&#20581;&#24247;&#29366;&#24577;&#65288;SoH&#65289;&#21644;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#23545;&#20110;&#25552;&#39640;&#39044;&#27979;&#24615;&#32500;&#25252;&#12289;&#21487;&#38752;&#24615;&#21644;&#30005;&#27744;&#31995;&#32479;&#30340;&#23551;&#21629;&#26377;&#30528;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;&#38500;&#20102;&#25552;&#39640;&#25104;&#26412;&#33410;&#32422;&#22806;&#65292;&#27491;&#30830;&#20272;&#35745;SoH&#36824;&#21487;&#20197;&#36890;&#36807;&#22238;&#25910;&#21033;&#29992;&#36864;&#24441;&#30005;&#27744;&#26469;&#20943;&#23569;&#27745;&#26579;&#12290;&#26412;&#25991;&#23545;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20272;&#35745;&#38085;&#37240;&#30005;&#27744;&#30340;SoH&#21644;RUL&#26041;&#38754;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#26144;&#23556;&#30740;&#31350;&#12290;&#36825;&#20004;&#20010;&#25351;&#26631;&#22312;&#30005;&#21160;&#27773;&#36710;&#12289;&#21487;&#20877;&#29983;&#33021;&#28304;&#31995;&#32479;&#31561;&#24191;&#27867;&#24212;&#29992;&#35813;&#30005;&#27744;&#25216;&#26415;&#30340;&#39046;&#22495;&#20013;&#30340;&#30005;&#27744;&#31649;&#29702;&#31995;&#32479;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29992;&#20110;&#20272;&#35745;SoH&#21644;RUL&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#31867;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy storage solutions play an increasingly important role in modern infrastructure and lead-acid batteries are among the most commonly used in the rechargeable category. Due to normal degradation over time, correctly determining the battery's State of Health (SoH) and Remaining Useful Life (RUL) contributes to enhancing predictive maintenance, reliability, and longevity of battery systems. Besides improving the cost savings, correct estimation of the SoH can lead to reduced pollution though reuse of retired batteries. This paper presents a mapping study of the state-of-the-art in machine learning methods for estimating the SoH and RUL of lead-acid batteries. These two indicators are critical in the battery management systems of electric vehicles, renewable energy systems, and other applications that rely heavily on this battery technology. In this study, we analyzed the types of machine learning algorithms employed for estimating SoH and RUL, and evaluated their performance in terms
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;LoRA&#30340;&#21442;&#25968;&#39640;&#25928;&#32454;&#35843;&#26041;&#27861;&#22312;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#20013;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;LoRA&#19982;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#32454;&#35843;&#25928;&#26524;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2307.05162</link><description>&lt;p&gt;
SuryaKiran&#22312;MEDIQA-Sum 2023&#20013;&#30340;&#24212;&#29992;&#65306;&#21033;&#29992;LoRA&#36827;&#34892;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization. (arXiv:2307.05162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;LoRA&#30340;&#21442;&#25968;&#39640;&#25928;&#32454;&#35843;&#26041;&#27861;&#22312;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#20013;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;LoRA&#19982;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#32454;&#35843;&#25928;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#25913;&#21892;&#29305;&#23450;&#39046;&#22495;&#29992;&#20363;&#30340;&#32467;&#26524;&#12290;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#32454;&#35843;&#32791;&#36153;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#24182;&#20855;&#26377;&#39640;&#23384;&#20648;&#38656;&#27714;&#20197;&#23384;&#20648;&#32454;&#35843;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21442;&#25968;&#39640;&#25928;&#32454;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#36890;&#36807;&#20445;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#22266;&#23450;&#22522;&#20934;&#24182;&#28155;&#21152;&#39069;&#22806;&#23618;&#26469;&#35299;&#20915;&#26102;&#38388;&#21644;&#36164;&#28304;&#25361;&#25112;&#65292;PEFT&#26041;&#27861;&#36827;&#34892;&#32454;&#35843;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;&#21517;&#20026;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;PEFT&#26041;&#27861;&#22312;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#20013;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;LoRA&#19982;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#32454;&#35843;&#30340;&#25928;&#26524;&#30456;&#24403;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#35299;&#20915;ImageCLEFmedical&#30340;Subtask A&#21644;B&#25152;&#36827;&#34892;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning Large Language Models helps improve the results for domain-specific use cases. End-to-end finetuning of large language models is time and resource intensive and has high storage requirements to store the finetuned version of the large language model. Parameter Efficient Fine Tuning (PEFT) methods address the time and resource challenges by keeping the large language model as a fixed base and add additional layers, which the PEFT methods finetune. This paper demonstrates the evaluation results for one such PEFT method Low Rank Adaptation (LoRA), for Clinical Dialogue Summarization. The evaluation results show that LoRA works at par with end-to-end finetuning for a large language model. The paper presents the evaluations done for solving both the Subtask A and B from ImageCLEFmedical {https://www.imageclef.org/2023/medical}
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35821;&#38899;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#19982;&#35821;&#38899;&#30456;&#20851;&#30340;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36866;&#24212;&#65292;&#24182;&#22312;&#22810;&#20010;MIR&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#38899;&#20048;&#25968;&#25454;&#19978;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;MIR&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05161</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#38899;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#38899;&#20048;&#20013;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effectiveness of Speech Self-supervised Learning for Music. (arXiv:2307.05161v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35821;&#38899;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#19982;&#35821;&#38899;&#30456;&#20851;&#30340;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36866;&#24212;&#65292;&#24182;&#22312;&#22810;&#20010;MIR&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#38899;&#20048;&#25968;&#25454;&#19978;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;MIR&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20013;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#22312;&#38899;&#20048;&#35760;&#24405;&#19978;&#39044;&#35757;&#32451;&#30340;SSL&#27169;&#22411;&#21487;&#33021;&#20027;&#35201;&#26159;&#38381;&#28304;&#30340;&#65292;&#20294;&#26368;&#36817;&#30340;&#35821;&#38899;&#27169;&#22411;&#22914;wav2vec2.0&#22312;&#38899;&#20048;&#24314;&#27169;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23558;&#35821;&#38899;SSL&#27169;&#22411;&#24212;&#29992;&#20110;&#38899;&#20048;&#35760;&#24405;&#30340;&#26377;&#25928;&#24615;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#20010;&#29420;&#29305;&#30340;&#19982;&#35821;&#38899;&#30456;&#20851;&#30340;&#27169;&#22411;&#22312;&#38899;&#20048;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36866;&#24212;&#65292;&#20998;&#21035;&#26159;data2vec1.0&#21644;Hubert&#65292;&#24182;&#23558;&#23427;&#20204;&#20998;&#21035;&#31216;&#20026;music2vec&#21644;musicHuBERT&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#37197;&#32622;&#19979;&#35757;&#32451;&#20102;12&#20010;&#20855;&#26377;95M&#21442;&#25968;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#31995;&#32479;&#35780;&#20272;&#20102;13&#20010;&#19981;&#21516;&#30340;MIR&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38899;&#20048;&#25968;&#25454;&#19978;&#35757;&#32451;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;MIR&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#20351;&#29992;&#35774;&#35745;&#20026;&#35821;&#38899;&#27169;&#22411;&#30340;&#35757;&#32451;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has shown promising results in various speech and natural language processing applications. However, its efficacy in music information retrieval (MIR) still remains largely unexplored. While previous SSL models pre-trained on music recordings may have been mostly closed-sourced, recent speech models such as wav2vec2.0 have shown promise in music modelling. Nevertheless, research exploring the effectiveness of applying speech SSL models to music recordings has been limited. We explore the music adaption of SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and refer to them as music2vec and musicHuBERT, respectively. We train $12$ SSL models with 95M parameters under various pre-training configurations and systematically evaluate the MIR task performances with 13 different MIR tasks. Our findings suggest that training with music data can generally improve performance on MIR tasks, even when models are trained using paradigms designed f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;CERN&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#20013;&#36873;&#25321;&#20013;&#24615;&#38271;&#23551;&#21629;&#31890;&#23376;&#34928;&#21464;&#30340;&#20107;&#20214;&#65292;&#24182;&#36890;&#36807;&#21152;&#36895;&#21345;&#36827;&#34892;&#21152;&#36895;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#21152;&#36895;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#24182;&#31526;&#21512;&#31532;&#20108;&#32423;&#35302;&#21457;&#30340;&#24310;&#36831;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.05152</link><description>&lt;p&gt;
&#22312;&#24378;&#23376;&#23545;&#25758;&#26426;&#19978;&#29992;&#20110;&#38271;&#23551;&#21629;&#31890;&#23376;&#35302;&#21457;&#30340;FPGA&#24555;&#36895;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Fast Neural Network Inference on FPGAs for Triggering on Long-Lived Particles at Colliders. (arXiv:2307.05152v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;CERN&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#20013;&#36873;&#25321;&#20013;&#24615;&#38271;&#23551;&#21629;&#31890;&#23376;&#34928;&#21464;&#30340;&#20107;&#20214;&#65292;&#24182;&#36890;&#36807;&#21152;&#36895;&#21345;&#36827;&#34892;&#21152;&#36895;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#21152;&#36895;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#24182;&#31526;&#21512;&#31532;&#20108;&#32423;&#35302;&#21457;&#30340;&#24310;&#36831;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#31890;&#23376;&#29289;&#29702;&#23398;&#38656;&#35201;&#19968;&#20010;&#22797;&#26434;&#30340;&#35302;&#21457;&#21644;&#37319;&#38598;&#31995;&#32479;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#20445;&#30041;&#24863;&#20852;&#36259;&#30340;&#30896;&#25758;&#20107;&#20214;&#20197;&#20379;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#21033;&#29992;FPGA&#21152;&#36895;&#21345;&#36827;&#34892;&#24322;&#26500;&#35745;&#31639;&#21487;&#33021;&#25104;&#20026;CERN&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#21363;&#23558;&#21040;&#26469;&#30340;&#39640;&#20142;&#24230;&#35745;&#21010;&#20013;&#35302;&#21457;&#31574;&#30053;&#30340;&#19968;&#31181;&#26032;&#36235;&#21183;&#25216;&#26415;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#22312;&#25506;&#27979;&#22120;&#20307;&#31215;&#20869;&#21457;&#29983;&#20013;&#24615;&#38271;&#23551;&#21629;&#31890;&#23376;&#34928;&#21464;&#30340;&#20107;&#20214;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#21830;&#19994;&#21487;&#33719;&#24471;&#30340;Xilinx FPGA&#21152;&#36895;&#21345;&#19978;&#36827;&#34892;&#21152;&#36895;&#30340;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;&#25512;&#29702;&#26102;&#38388;&#36824;&#19982;&#22522;&#20110;CPU&#21644;GPU&#30340;&#30828;&#20214;&#35774;&#32622;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26032;&#31639;&#27861;&#22312;&#32771;&#34385;&#30340;&#22522;&#20934;&#29289;&#29702;&#22330;&#26223;&#20013;&#26159;&#39640;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;FPGA&#21345;&#19978;&#21152;&#36895;&#26102;&#20934;&#30830;&#24615;&#27809;&#26377;&#38477;&#20302;&#12290;&#25152;&#26377;&#27979;&#35797;&#30340;&#26550;&#26500;&#37117;&#31526;&#21512;&#31532;&#20108;&#32423;&#35302;&#21457;&#30340;&#24310;&#36831;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimental particle physics demands a sophisticated trigger and acquisition system capable to efficiently retain the collisions of interest for further investigation. Heterogeneous computing with the employment of FPGA cards may emerge as a trending technology for the triggering strategy of the upcoming high-luminosity program of the Large Hadron Collider at CERN. In this context, we present two machine-learning algorithms for selecting events where neutral long-lived particles decay within the detector volume studying their accuracy and inference time when accelerated on commercially available Xilinx FPGA accelerator cards. The inference time is also confronted with a CPU- and GPU-based hardware setup. The proposed new algorithms are proven efficient for the considered benchmark physics scenario and their accuracy is found to not degrade when accelerated on the FPGA cards. The results indicate that all tested architectures fit within the latency requirements of a second-level trigge
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#36816;&#21160;&#21407;&#29702;&#27169;&#22411;&#65292;&#20855;&#22791;&#22810;&#31181;&#25805;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05141</link><description>&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#36816;&#21160;&#21407;&#29702;&#19982;&#36125;&#21494;&#26031;&#32858;&#21512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Deep Probabilistic Movement Primitives with a Bayesian Aggregator. (arXiv:2307.05141v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05141
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#36816;&#21160;&#21407;&#29702;&#27169;&#22411;&#65292;&#20855;&#22791;&#22810;&#31181;&#25805;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#21407;&#29702;&#26159;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#28436;&#31034;&#38598;&#21512;&#20013;&#22797;&#21046;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#20801;&#35768;&#26102;&#38388;&#35843;&#21046;&#36816;&#21160;&#65288;&#21152;&#36895;&#25110;&#20943;&#36895;&#22797;&#21046;&#36816;&#21160;&#65289;&#12289;&#28151;&#21512;&#65288;&#23558;&#20004;&#20010;&#36816;&#21160;&#21512;&#24182;&#20026;&#19968;&#20010;&#65289;&#12289;&#36890;&#36807;&#28857;&#35843;&#33410;&#65288;&#23558;&#36816;&#21160;&#32422;&#26463;&#21040;&#29305;&#23450;&#30340;&#36890;&#36807;&#28857;&#65289;&#21644;&#19978;&#19979;&#25991;&#35843;&#33410;&#65288;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#29983;&#25104;&#36816;&#21160;&#65292;&#20363;&#22914;&#29289;&#20307;&#30340;&#20301;&#32622;&#65289;&#23637;&#31034;&#20986;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36816;&#21160;&#21407;&#29702;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19968;&#20123;&#24418;&#24335;&#30340;&#36755;&#20837;&#35843;&#33410;&#25110;&#26102;&#38388;&#35843;&#21046;&#34920;&#36798;&#20013;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#25552;&#20986;&#19968;&#20010;&#21333;&#19968;&#32479;&#19968;&#30340;&#28145;&#24230;&#36816;&#21160;&#21407;&#29702;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20855;&#22791;&#25152;&#26377;&#20808;&#21069;&#30340;&#25805;&#20316;&#65292;&#36825;&#38480;&#21046;&#20102;&#31070;&#32463;&#36816;&#21160;&#21407;&#29702;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#36816;&#21160;&#21407;&#29702;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Movement primitives are trainable parametric models that reproduce robotic movements starting from a limited set of demonstrations. Previous works proposed simple linear models that exhibited high sample efficiency and generalization power by allowing temporal modulation of movements (reproducing movements faster or slower), blending (merging two movements into one), via-point conditioning (constraining a movement to meet some particular via-points) and context conditioning (generation of movements based on an observed variable, e.g., position of an object). Previous works have proposed neural network-based motor primitive models, having demonstrated their capacity to perform tasks with some forms of input conditioning or time-modulation representations. However, there has not been a single unified deep motor primitive's model proposed that is capable of all previous operations, limiting neural motor primitive's potential applications. This paper proposes a deep movement primitive arch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;TIAM&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#20869;&#23481;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#21253;&#25324;&#23545;&#35937;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.05134</link><description>&lt;p&gt;
TIAM -- &#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation. (arXiv:2307.05134v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;TIAM&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#20869;&#23481;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#21253;&#25324;&#23545;&#35937;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#30340;&#36827;&#23637;&#20351;&#24471;&#35780;&#20272;&#20854;&#36136;&#37327;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#28210;&#26579;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#32780;&#35328;&#65292;&#32771;&#34385;&#21040;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#37325;&#35201;&#20869;&#23481;&#20043;&#38388;&#30340;&#30456;&#20284;&#31243;&#24230;&#31561;&#39069;&#22806;&#22240;&#32032;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#29983;&#25104;&#30340;&#22270;&#20687;&#36890;&#24120;&#26159;&#20174;&#38543;&#26426;&#36215;&#22987;&#28857;&#24320;&#22987;&#30340;&#65292;&#20294;&#36890;&#24120;&#19981;&#32771;&#34385;&#36825;&#19968;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#25552;&#31034;&#20013;&#25351;&#23450;&#30340;&#20869;&#23481;&#19982;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#24615;&#12290;&#23427;&#20801;&#35768;&#25105;&#20204;&#26356;&#22909;&#22320;&#25551;&#36848;&#23545;&#40784;&#24615;&#65292;&#21253;&#25324;&#25351;&#23450;&#23545;&#35937;&#30340;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#25105;&#20204;&#23545;&#20960;&#20010;&#26368;&#36817;&#30340;T2I&#27169;&#22411;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#33719;&#24471;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#39069;&#22806;&#32467;&#26524;&#65292;&#21363;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#22823;&#24133;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress in the generation of synthetic images has made it crucial to assess their quality. While several metrics have been proposed to assess the rendering of images, it is crucial for Text-to-Image (T2I) models, which generate images based on a prompt, to consider additional aspects such as to which extent the generated image matches the important content of the prompt. Moreover, although the generated images usually result from a random starting point, the influence of this one is generally not considered. In this article, we propose a new metric based on prompt templates to study the alignment between the content specified in the prompt and the corresponding generated images. It allows us to better characterize the alignment in terms of the type of the specified objects, their number, and their color. We conducted a study on several recent T2I models about various aspects. An additional interesting result we obtained with our approach is that image quality can vary drastically 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#33258;&#21457;&#35821;&#38899;&#21512;&#25104;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#24335;&#35821;&#38899;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#33258;&#21457;&#35821;&#38899;&#36716;&#35821;&#38899;&#31995;&#32479;&#20013;&#26368;&#36866;&#21512;&#30340;SSL&#21644;SSL&#27169;&#22411;&#30340;&#21738;&#19968;&#23618;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25193;&#23637;&#20102;&#22522;&#20110;SSL&#30340;MOS&#39044;&#27979;&#26694;&#26550;&#65292;&#25104;&#21151;&#22320;&#22312;&#21512;&#25104;&#30340;&#33258;&#21457;&#35821;&#38899;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.05132</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#24335;&#35821;&#38899;&#34920;&#31034;&#22312;&#33258;&#21457;&#35821;&#38899;&#21512;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Use of Self-Supervised Speech Representations in Spontaneous Speech Synthesis. (arXiv:2307.05132v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05132
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#33258;&#21457;&#35821;&#38899;&#21512;&#25104;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#24335;&#35821;&#38899;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#33258;&#21457;&#35821;&#38899;&#36716;&#35821;&#38899;&#31995;&#32479;&#20013;&#26368;&#36866;&#21512;&#30340;SSL&#21644;SSL&#27169;&#22411;&#30340;&#21738;&#19968;&#23618;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25193;&#23637;&#20102;&#22522;&#20110;SSL&#30340;MOS&#39044;&#27979;&#26694;&#26550;&#65292;&#25104;&#21151;&#22320;&#22312;&#21512;&#25104;&#30340;&#33258;&#21457;&#35821;&#38899;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#20174;&#22823;&#37327;&#22810;&#26679;&#21270;&#12289;&#36136;&#37327;&#19981;&#19968;&#33268;&#30340;&#35821;&#38899;&#25968;&#25454;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#19968;&#31181;&#26377;&#25928;&#30340;&#35821;&#38899;&#34920;&#31034;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#36716;&#24405;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;SSL&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#20013;&#38388;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#20004;&#38454;&#27573;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#20013;&#65292;&#26080;&#35770;&#26159;&#38024;&#23545;&#26391;&#35835;&#35821;&#38899;&#36824;&#26159;&#33258;&#21457;&#35821;&#38899;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#21738;&#31181;SSL&#20197;&#21450;&#27599;&#20010;SSL&#27169;&#22411;&#20013;&#30340;&#21738;&#19968;&#23618;&#36866;&#21512;&#20110;&#33258;&#21457;&#35821;&#38899;&#30340;TTS&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#19981;&#36275;&#65292;&#25105;&#20204;&#25193;&#22823;&#20102;&#23545;&#33258;&#21457;&#35821;&#38899;TTS&#20013;SSL&#30340;&#27604;&#36739;&#33539;&#22260;&#65292;&#21253;&#25324;6&#31181;&#19981;&#21516;&#30340;SSL&#21644;&#27599;&#31181;SSL&#20013;&#30340;3&#23618;&#12290;&#27492;&#22806;&#65292;SSL&#22312;&#39044;&#27979;&#21512;&#25104;&#35821;&#38899;&#30340;&#20027;&#35266;&#36136;&#37327;&#35780;&#20998;&#65288;MOS&#65289;&#26041;&#38754;&#20063;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#36825;&#21482;&#22312;&#26391;&#35835;&#35821;&#38899;MOS&#39044;&#27979;&#20013;&#36827;&#34892;&#36807;&#30740;&#31350;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#20043;&#21069;&#29992;&#20110;&#35780;&#20998;&#26391;&#35835;&#35821;&#38899;&#21512;&#25104;&#30340;&#22522;&#20110;SSL&#30340;MOS&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#22312;&#21512;&#25104;&#30340;&#33258;&#21457;&#35821;&#38899;&#19978;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#25152;&#26377;&#23454;&#39564;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#33258;&#21457;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#37325;&#22797;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) speech representations learned from large amounts of diverse, mixed-quality speech data without transcriptions are gaining ground in many speech technology applications. Prior work has shown that SSL is an effective intermediate representation in two-stage text-to-speech (TTS) for both read and spontaneous speech. However, it is still not clear which SSL and which layer from each SSL model is most suited for spontaneous TTS. We address this shortcoming by extending the scope of comparison for SSL in spontaneous TTS to 6 different SSLs and 3 layers within each SSL. Furthermore, SSL has also shown potential in predicting the mean opinion scores (MOS) of synthesized speech, but this has only been done in read-speech MOS prediction. We extend an SSL-based MOS prediction framework previously developed for scoring read speech synthesis and evaluate its performance on synthesized spontaneous speech. All experiments are conducted twice on two different spontaneou
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28508;&#22312;ODE-LSTM&#26041;&#27861;&#22686;&#24378;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#26631;&#20934;RNN&#36827;&#34892;&#24314;&#27169;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#32570;&#22833;&#25968;&#25454;&#31561;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;ODE-RNN&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;ODE&#20316;&#20026;&#35299;&#30721;&#22120;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24314;&#27169;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05126</link><description>&lt;p&gt;
&#29992;&#28508;&#22312;ODE-LSTM&#26041;&#27861;&#22686;&#24378;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Enhancing Continuous Time Series Modelling with a Latent ODE-LSTM Approach. (arXiv:2307.05126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05126
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28508;&#22312;ODE-LSTM&#26041;&#27861;&#22686;&#24378;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#26631;&#20934;RNN&#36827;&#34892;&#24314;&#27169;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#32570;&#22833;&#25968;&#25454;&#31561;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;ODE-RNN&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;ODE&#20316;&#20026;&#35299;&#30721;&#22120;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24314;&#27169;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#19981;&#35268;&#21017;&#30340;&#37319;&#26679;&#29575;&#21644;&#39640;&#39057;&#37319;&#26679;&#31561;&#21160;&#24577;&#29305;&#24615;&#65292;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#65288;CTS&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#12290;&#30001;&#20110;&#20855;&#26377;&#19981;&#35268;&#21017;&#37319;&#26679;&#29575;&#30340;CTS&#38590;&#20197;&#20351;&#29992;&#26631;&#20934;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#36827;&#34892;&#24314;&#27169;&#65292;&#22240;&#27492;RNN&#34987;&#25512;&#24191;&#20026;&#20855;&#26377;&#30001;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODE&#65289;&#23450;&#20041;&#30340;&#36830;&#32493;&#26102;&#38388;&#38544;&#34255;&#21160;&#21147;&#23398;&#30340;ODE-RNN&#27169;&#22411;&#12290;&#21478;&#19968;&#31181;&#25552;&#20379;&#26356;&#22909;&#24314;&#27169;&#25928;&#26524;&#30340;&#26041;&#27861;&#26159;&#28508;&#22312;ODE&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#65292;&#22312;&#25152;&#26377;&#26102;&#38388;&#28857;&#19978;&#23450;&#20041;&#20102;&#19968;&#20010;&#28508;&#22312;&#29366;&#24577;&#12290;&#28508;&#22312;ODE&#27169;&#22411;&#20351;&#29992;&#26631;&#20934;RNN&#20316;&#20026;&#32534;&#30721;&#22120;&#21644;&#31070;&#32463;ODE&#20316;&#20026;&#35299;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;RNN&#32534;&#30721;&#22120;&#23384;&#22312;&#32570;&#22833;&#25968;&#25454;&#21644;&#19981;&#23436;&#20840;&#23450;&#20041;&#30340;&#28508;&#22312;&#21464;&#37327;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ODE-RNN&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;ODE-RNN&#27169;&#22411;&#12290;&#30001;&#20110;&#26799;&#24230;&#28040;&#22833;&#21644;&#29190;&#28856;&#38382;&#39064;&#65292;&#28508;&#22312;ODE&#27169;&#22411;&#21644;&#28508;&#22312;ODE-RNN&#27169;&#22411;&#37117;&#38590;&#20197;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their dynamic properties such as irregular sampling rate and high-frequency sampling, Continuous Time Series (CTS) are found in many applications. Since CTS with irregular sampling rate are difficult to model with standard Recurrent Neural Networks (RNNs), RNNs have been generalised to have continuous-time hidden dynamics defined by a Neural Ordinary Differential Equation (Neural ODE), leading to the ODE-RNN model. Another approach that provides a better modelling is that of the Latent ODE model, which constructs a continuous-time model where a latent state is defined at all times. The Latent ODE model uses a standard RNN as the encoder and a Neural ODE as the decoder. However, since the RNN encoder leads to difficulties with missing data and ill-defined latent variables, a Latent ODE-RNN model has recently been proposed that uses a ODE-RNN model as the encoder instead. Both the Latent ODE and Latent ODE-RNN models are difficult to train due to the vanishing and exploding gradie
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STA-GT&#30340;&#26032;&#39062;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20132;&#26131;&#27450;&#35784;&#26816;&#27979;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#31354;&#38388;-&#26102;&#38388;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#21512;&#24182;&#20840;&#23616;&#20449;&#24687;&#25913;&#36827;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.05121</link><description>&lt;p&gt;
&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#24863;&#30693;&#22270;&#36716;&#25442;&#22120;&#36827;&#34892;&#20132;&#26131;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transaction Fraud Detection via Spatial-Temporal-Aware Graph Transformer. (arXiv:2307.05121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05121
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STA-GT&#30340;&#26032;&#39062;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20132;&#26131;&#27450;&#35784;&#26816;&#27979;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#31354;&#38388;-&#26102;&#38388;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#21512;&#24182;&#20840;&#23616;&#20449;&#24687;&#25913;&#36827;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#33719;&#21462;&#20132;&#26131;&#30340;&#20449;&#24687;&#34920;&#31034;&#24182;&#36827;&#34892;&#27450;&#35784;&#20132;&#26131;&#30340;&#35782;&#21035;&#26159;&#30830;&#20445;&#37329;&#34701;&#23433;&#20840;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#20132;&#26131;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32467;&#26500;&#38480;&#21046;&#65292;&#23427;&#20204;&#22312;&#26377;&#25928;&#23398;&#20064;&#31354;&#38388;-&#26102;&#38388;&#20449;&#24687;&#26041;&#38754;&#36935;&#21040;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#24456;&#23569;&#26377;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20808;&#21069;&#26816;&#27979;&#22120;&#24847;&#35782;&#21040;&#21512;&#24182;&#20840;&#23616;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#35813;&#20840;&#23616;&#20449;&#24687;&#28085;&#30422;&#20102;&#30456;&#20284;&#30340;&#34892;&#20026;&#27169;&#24335;&#24182;&#20026;&#21028;&#21035;&#24615;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;&#31354;&#38388;-&#26102;&#38388;&#24863;&#30693;&#22270;&#36716;&#25442;&#22120;&#65288;STA-GT&#65289;&#65292;&#29992;&#20110;&#20132;&#26131;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26102;&#38388;&#32534;&#30721;&#31574;&#30053;&#26469;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#65292;&#22686;&#24378;&#20102;&#31354;&#38388;-&#26102;&#38388;&#20449;&#24687;&#24314;&#27169;&#24182;&#25913;&#36827;&#20102;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to obtain informative representations of transactions and then perform the identification of fraudulent transactions is a crucial part of ensuring financial security. Recent studies apply Graph Neural Networks (GNNs) to the transaction fraud detection problem. Nevertheless, they encounter challenges in effectively learning spatial-temporal information due to structural limitations. Moreover, few prior GNN-based detectors have recognized the significance of incorporating global information, which encompasses similar behavioral patterns and offers valuable insights for discriminative representation learning. Therefore, we propose a novel heterogeneous graph neural network called Spatial-Temporal-Aware Graph Transformer (STA-GT) for transaction fraud detection problems. Specifically, we design a temporal encoding strategy to capture temporal dependencies and incorporate it into the graph neural network framework, enhancing spatial-temporal information modeling and improving expressive
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#20449;&#20013;&#30340;&#20219;&#24847;&#20998;&#21306;&#27169;&#22411;&#20013;&#30340;$\ell_p$&#22238;&#24402;&#38382;&#39064;&#30340;&#38543;&#26426;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#32473;&#20986;&#20102;&#35813;&#38382;&#39064;&#30340;&#26174;&#33879;&#25913;&#36827;&#30340;&#30028;&#38480;&#12290;&#23545;&#20110;$p = 2$&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#26368;&#20248;$\tilde{\Theta}(sd^2 + sd/\epsilon)$&#27604;&#29305;&#30340;&#30028;&#38480;&#12290;&#23545;&#20110;$p \in (1,2)$&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;$\tilde{O}(sd^2/\epsilon + sd/\mathrm{poly}(\epsilon))$&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.05117</link><description>&lt;p&gt;
&#36890;&#20449;&#20013;&#30340;&#20219;&#24847;&#20998;&#21306;&#27169;&#22411;&#20013;&#30340;$\ell_p$&#22238;&#24402;&#38382;&#39064;&#30340;&#38543;&#26426;&#36890;&#20449;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
$\ell_p$-Regression in the Arbitrary Partition Model of Communication. (arXiv:2307.05117v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05117
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#20449;&#20013;&#30340;&#20219;&#24847;&#20998;&#21306;&#27169;&#22411;&#20013;&#30340;$\ell_p$&#22238;&#24402;&#38382;&#39064;&#30340;&#38543;&#26426;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#32473;&#20986;&#20102;&#35813;&#38382;&#39064;&#30340;&#26174;&#33879;&#25913;&#36827;&#30340;&#30028;&#38480;&#12290;&#23545;&#20110;$p = 2$&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#26368;&#20248;$\tilde{\Theta}(sd^2 + sd/\epsilon)$&#27604;&#29305;&#30340;&#30028;&#38480;&#12290;&#23545;&#20110;$p \in (1,2)$&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;$\tilde{O}(sd^2/\epsilon + sd/\mathrm{poly}(\epsilon))$&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#21327;&#35843;&#22120;&#27169;&#22411;&#20013;&#20998;&#24067;&#24335;$\ell_p$&#22238;&#24402;&#38382;&#39064;&#30340;&#38543;&#26426;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;$p\in(0,2]$&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#26377;&#19968;&#20010;&#21327;&#35843;&#22120;&#21644;$s$&#20010;&#26381;&#21153;&#22120;&#12290;&#31532;$i$&#20010;&#26381;&#21153;&#22120;&#25509;&#25910;&#21040;$A^i\in\{-M, -M+1, \ldots, M\}^{n\times d}$&#21644;$b^i\in\{-M, -M+1, \ldots, M\}^n$&#65292;&#21327;&#35843;&#22120;&#24819;&#35201;&#25214;&#21040;&#19968;&#20010;$(1+\epsilon)$-&#36817;&#20284;&#35299;&#26469;&#27714;&#35299;$\min_{x\in\mathbb{R}^n} \|(\sum_i A^i)x - (\sum_i b^i)\|_p$&#12290;&#36825;&#37324;&#30340;$M \leq \mathrm{poly}(nd)$&#12290;&#25968;&#25454;&#20197;&#21152;&#27861;&#30340;&#26041;&#24335;&#22312;&#26381;&#21153;&#22120;&#38388;&#20849;&#20139;&#65292;&#36825;&#20010;&#27169;&#22411;&#36890;&#24120;&#34987;&#31216;&#20026;&#20219;&#24847;&#20998;&#21306;&#27169;&#22411;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#35813;&#38382;&#39064;&#30340;&#26174;&#33879;&#25913;&#36827;&#30340;&#30028;&#38480;&#12290;&#23545;&#20110;$p=2$&#30340;&#24773;&#20917;&#65292;&#21363;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#31532;&#19968;&#20010;&#20248;&#21270;&#30028;&#38480;&#65306;$\tilde{\Theta}(sd^2 + sd/\epsilon)$&#27604;&#29305;&#12290;&#23545;&#20110;$p\in(1,2)$&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;$\tilde{O}(sd^2/\epsilon + sd/\mathrm{poly}(\epsilon))$&#30340;&#19978;&#30028;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;&#36275;&#22815;&#22823;&#30340;$d$&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#39033;&#21482;&#19982;$1/\epsilon$&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the randomized communication complexity of the distributed $\ell_p$-regression problem in the coordinator model, for $p\in (0,2]$. In this problem, there is a coordinator and $s$ servers. The $i$-th server receives $A^i\in\{-M, -M+1, \ldots, M\}^{n\times d}$ and $b^i\in\{-M, -M+1, \ldots, M\}^n$ and the coordinator would like to find a $(1+\epsilon)$-approximate solution to $\min_{x\in\mathbb{R}^n} \|(\sum_i A^i)x - (\sum_i b^i)\|_p$. Here $M \leq \mathrm{poly}(nd)$ for convenience. This model, where the data is additively shared across servers, is commonly referred to as the arbitrary partition model.  We obtain significantly improved bounds for this problem. For $p = 2$, i.e., least squares regression, we give the first optimal bound of $\tilde{\Theta}(sd^2 + sd/\epsilon)$ bits.  For $p \in (1,2)$,we obtain an $\tilde{O}(sd^2/\epsilon + sd/\mathrm{poly}(\epsilon))$ upper bound. Notably, for $d$ sufficiently large, our leading order term only depends linearly on $1/\epsilo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21512;&#35268;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#36873;&#25321;&#21464;&#37327;&#22312;&#36755;&#20837;&#25968;&#25454;&#24494;&#23567;&#25200;&#21160;&#19979;&#30340;&#19981;&#21464;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#20540;&#24310;&#25299;&#25216;&#26415;&#39640;&#25928;&#36924;&#36817;&#35299;&#20915;&#26041;&#26696;&#36335;&#24452;&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#21512;&#35268;&#21270;&#38598;&#21512;&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.05109</link><description>&lt;p&gt;
&#31232;&#30095;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21512;&#35268;&#21270;
&lt;/p&gt;
&lt;p&gt;
Conformalization of Sparse Generalized Linear Models. (arXiv:2307.05109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21512;&#35268;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#36873;&#25321;&#21464;&#37327;&#22312;&#36755;&#20837;&#25968;&#25454;&#24494;&#23567;&#25200;&#21160;&#19979;&#30340;&#19981;&#21464;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#20540;&#24310;&#25299;&#25216;&#26415;&#39640;&#25928;&#36924;&#36817;&#35299;&#20915;&#26041;&#26696;&#36335;&#24452;&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#21512;&#35268;&#21270;&#38598;&#21512;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#31995;&#21015;&#21487;&#35266;&#27979;&#21464;&#37327;{(x1&#65292;y1)&#65292;&#8230;&#65292;(xn&#65292;yn)}&#65292;&#21512;&#35268;&#21270;&#39044;&#27979;&#26041;&#27861;&#36890;&#36807;&#20165;&#20551;&#35774;&#25968;&#25454;&#30340;&#32852;&#21512;&#20998;&#24067;&#26159;&#32622;&#25442;&#19981;&#21464;&#30340;&#65292;&#20026;&#32473;&#23450;x_{n+1}&#20272;&#35745;y_{n+1}&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#36825;&#20010;&#32622;&#20449;&#21306;&#38388;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#26679;&#26412;&#37327;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;&#23613;&#31649;&#26377;&#21560;&#24341;&#21147;&#65292;&#22312;&#22823;&#22810;&#25968;&#22238;&#24402;&#38382;&#39064;&#20013;&#35745;&#31639;&#36825;&#26679;&#30340;&#32622;&#20449;&#21306;&#38388;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#26410;&#30693;&#21464;&#37327;y_{n+1}&#21487;&#20197;&#21462;&#26080;&#38480;&#22810;&#20010;&#21487;&#33021;&#30340;&#20505;&#36873;&#20540;&#65292;&#24182;&#19988;&#29983;&#25104;&#21512;&#35268;&#21270;&#38598;&#21512;&#38656;&#35201;&#20026;&#27599;&#20010;&#20505;&#36873;&#37325;&#26032;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20165;&#20351;&#29992;&#23376;&#38598;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#30340;&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#25968;&#20540;&#24310;&#25299;&#25216;&#26415;&#39640;&#25928;&#36924;&#36817;&#35299;&#20915;&#26041;&#26696;&#36335;&#24452;&#12290;&#25105;&#20204;&#21033;&#29992;&#30340;&#20851;&#38190;&#29305;&#24615;&#26159;&#25152;&#36873;&#21464;&#37327;&#38598;&#22312;&#36755;&#20837;&#25968;&#25454;&#30340;&#24494;&#23567;&#25200;&#21160;&#19979;&#26159;&#19981;&#21464;&#30340;&#12290;&#22240;&#27492;&#65292;&#21482;&#38656;&#35201;&#22312;&#21464;&#21270;&#28857;&#26522;&#20030;&#21644;&#37325;&#26032;&#25311;&#21512;&#27169;&#22411;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a sequence of observable variables $\{(x_1, y_1), \ldots, (x_n, y_n)\}$, the conformal prediction method estimates a confidence set for $y_{n+1}$ given $x_{n+1}$ that is valid for any finite sample size by merely assuming that the joint distribution of the data is permutation invariant. Although attractive, computing such a set is computationally infeasible in most regression problems. Indeed, in these cases, the unknown variable $y_{n+1}$ can take an infinite number of possible candidate values, and generating conformal sets requires retraining a predictive model for each candidate. In this paper, we focus on a sparse linear model with only a subset of variables for prediction and use numerical continuation techniques to approximate the solution path efficiently. The critical property we exploit is that the set of selected variables is invariant under a small perturbation of the input data. Therefore, it is sufficient to enumerate and refit the model only at the change points of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#20351;&#29992;&#25200;&#21160;&#20316;&#20026;&#35780;&#20272;&#20174;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#24402;&#22240;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;XAI&#25216;&#26415;&#30340;&#24212;&#29992;&#21644;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#25200;&#21160;&#20998;&#26512;&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;&#24402;&#22240;&#30340;&#36136;&#37327;&#65292;&#24182;&#20026;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05104</link><description>&lt;p&gt;
&#23545;&#25200;&#21160;&#20316;&#20026;&#26102;&#24207;XAI&#35780;&#20272;&#25216;&#26415;&#30340;&#28145;&#20837;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Deep Dive into Perturbations as Evaluation Technique for Time Series XAI. (arXiv:2307.05104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#20351;&#29992;&#25200;&#21160;&#20316;&#20026;&#35780;&#20272;&#20174;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#24402;&#22240;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;XAI&#25216;&#26415;&#30340;&#24212;&#29992;&#21644;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#25200;&#21160;&#20998;&#26512;&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;&#24402;&#22240;&#30340;&#36136;&#37327;&#65292;&#24182;&#20026;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#38656;&#27714;&#22686;&#21152;&#20102;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#37329;&#34701;&#12289;&#21307;&#30103;&#21644;&#27668;&#20505;&#31185;&#23398;&#39046;&#22495;&#65292;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;XAI&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#35299;&#37322;&#36136;&#37327;&#65292;&#22914;XAI&#25216;&#26415;&#25552;&#20379;&#30340;&#24402;&#22240;&#65292;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#25200;&#21160;&#26469;&#35780;&#20272;&#20174;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#24402;&#22240;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25200;&#21160;&#20998;&#26512;&#21253;&#25324;&#31995;&#32479;&#22320;&#20462;&#25913;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#35780;&#20272;&#23545;XAI&#26041;&#27861;&#29983;&#25104;&#30340;&#24402;&#22240;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;XAI&#25216;&#26415;&#65292;&#24182;&#22312;&#19977;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25200;&#21160;&#20998;&#26512;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;&#24402;&#22240;&#30340;&#36136;&#37327;&#65292;&#24182;&#27934;&#23519;&#21147;&#22320;&#25581;&#31034;&#20986;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence (XAI) has gained significant attention recently as the demand for transparency and interpretability of machine learning models has increased. In particular, XAI for time series data has become increasingly important in finance, healthcare, and climate science. However, evaluating the quality of explanations, such as attributions provided by XAI techniques, remains challenging. This paper provides an in-depth analysis of using perturbations to evaluate attributions extracted from time series models. A perturbation analysis involves systematically modifying the input data and evaluating the impact on the attributions generated by the XAI method. We apply this approach to several state-of-the-art XAI techniques and evaluate their performance on three time series classification datasets. Our results demonstrate that the perturbation analysis approach can effectively evaluate the quality of attributions and provide insights into the strengths and limitati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20219;&#20309;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#20013;&#26631;&#31614;&#30340;&#36136;&#37327;&#21644;&#38169;&#35823;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#38169;&#35823;&#30340;&#26631;&#31614;&#36136;&#37327;&#35780;&#20998;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#24110;&#21161;&#30830;&#23450;&#38656;&#35201;&#37325;&#28857;&#26816;&#26597;&#30340;&#25968;&#25454;&#65292;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;/&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.05080</link><description>&lt;p&gt;
&#36890;&#36807;&#20219;&#20309;&#27169;&#22411;&#20272;&#35745;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#20013;&#30340;&#26631;&#31614;&#36136;&#37327;&#21644;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Estimating label quality and errors in semantic segmentation data via any model. (arXiv:2307.05080v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05080
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20219;&#20309;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#20013;&#26631;&#31614;&#30340;&#36136;&#37327;&#21644;&#38169;&#35823;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#38169;&#35823;&#30340;&#26631;&#31614;&#36136;&#37327;&#35780;&#20998;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#24110;&#21161;&#30830;&#23450;&#38656;&#35201;&#37325;&#28857;&#26816;&#26597;&#30340;&#25968;&#25454;&#65292;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;/&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#30340;&#21171;&#21160;&#23494;&#38598;&#22411;&#27880;&#37322;&#36807;&#31243;&#24448;&#24448;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#65292;&#22240;&#20026;&#20154;&#20204;&#24456;&#38590;&#27491;&#30830;&#26631;&#35760;&#27599;&#20010;&#20687;&#32032;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#26816;&#27979;&#27492;&#31867;&#27880;&#37322;&#38169;&#35823;&#30340;&#31639;&#27861;&#65292;&#23588;&#20854;&#26159;&#35780;&#20998;&#26631;&#31614;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20351;&#24471;&#24471;&#20998;&#26368;&#20302;&#30340;&#22270;&#20687;&#26368;&#19981;&#21487;&#33021;&#34987;&#27491;&#30830;&#26631;&#35760;&#12290;&#36825;&#26377;&#21161;&#20110;&#20248;&#20808;&#32771;&#34385;&#35201;&#23457;&#26597;&#30340;&#25968;&#25454;&#65292;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;/&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#22312;&#25935;&#24863;&#24212;&#29992;&#65288;&#22914;&#21307;&#23398;&#25104;&#20687;&#21644;&#33258;&#21160;&#39550;&#39542;&#65289;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#24191;&#27867;&#36866;&#29992;&#65292;&#25105;&#20204;&#30340;&#26631;&#31614;&#36136;&#37327;&#35780;&#20998;&#20381;&#38752;&#35757;&#32451;&#30340;&#20998;&#21106;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;-&#20219;&#20309;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#36807;&#31243;&#37117;&#21487;&#20197;&#21033;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19982;DeepLabV3+&#25110;FPN&#20998;&#21106;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#30340;7&#31181;&#19981;&#21516;&#30340;&#26631;&#31614;&#36136;&#37327;&#35780;&#20998;&#26041;&#27861;&#65292;&#20197;&#22312;SYNTHIA&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#29256;&#26412;&#20013;&#26816;&#27979;&#27880;&#37322;&#38169;&#35823;&#12290;&#36890;&#36807;&#31934;&#30830;&#24230;-&#21484;&#22238;&#29575;&#35780;&#20272;&#25581;&#31034;&#20102;&#19968;&#20010;&#24471;&#20998;-
&lt;/p&gt;
&lt;p&gt;
The labor-intensive annotation process of semantic segmentation datasets is often prone to errors, since humans struggle to label every pixel correctly. We study algorithms to automatically detect such annotation errors, in particular methods to score label quality, such that the images with the lowest scores are least likely to be correctly labeled. This helps prioritize what data to review in order to ensure a high-quality training/evaluation dataset, which is critical in sensitive applications such as medical imaging and autonomous vehicles. Widely applicable, our label quality scores rely on probabilistic predictions from a trained segmentation model -- any model architecture and training procedure can be utilized. Here we study 7 different label quality scoring methods used in conjunction with a DeepLabV3+ or a FPN segmentation model to detect annotation errors in a version of the SYNTHIA dataset. Precision-recall evaluations reveal a score -- the soft-minimum of the model-estimat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#20551;&#35774;&#36923;&#36753;&#20840;&#30693;&#24615;&#30340;&#29702;&#24615;&#20915;&#31574;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#21487;&#29992;&#20110;&#35299;&#20915;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#23384;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26080;&#27861;&#23545;&#33258;&#36523;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05068</link><description>&lt;p&gt;
&#26377;&#30028;&#24402;&#32435;&#29702;&#24615;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Bounded Inductive Rationality. (arXiv:2307.05068v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#20551;&#35774;&#36923;&#36753;&#20840;&#30693;&#24615;&#30340;&#29702;&#24615;&#20915;&#31574;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#21487;&#29992;&#20110;&#35299;&#20915;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#23384;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26080;&#27861;&#23545;&#33258;&#36523;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24615;&#36873;&#25321;&#30340;&#20027;&#27969;&#29702;&#35770;&#20551;&#35774;&#36923;&#36753;&#20840;&#30693;&#24615;&#65292;&#21363;&#24403;&#38754;&#20020;&#20915;&#31574;&#38382;&#39064;&#26102;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#21487;&#20197;&#25191;&#34892;&#25152;&#26377;&#30456;&#20851;&#35745;&#31639;&#65292;&#24182;&#30830;&#23450;&#25152;&#26377;&#30456;&#20851;&#30340;&#36923;&#36753;/&#25968;&#23398;&#21629;&#39064;&#30340;&#30495;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20551;&#35774;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#20363;&#22914;&#25105;&#20204;&#23545;&#22278;&#21608;&#29575;&#30340;&#36828;&#31243;&#23567;&#25968;&#25552;&#20379;&#36172;&#27880;&#65292;&#25110;&#32773;&#26234;&#33021;&#20307;&#38754;&#20020;&#35745;&#31639;&#22797;&#26434;&#30340;&#35268;&#21010;&#38382;&#39064;&#26102;&#12290;&#27492;&#22806;&#65292;&#36923;&#36753;&#20840;&#30693;&#24615;&#30340;&#20551;&#35774;&#22312;&#29615;&#22659;&#20013;&#21253;&#21547;&#26234;&#33021;&#20307;&#33258;&#36523;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#20250;&#20135;&#29983;&#30683;&#30462;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#21338;&#24328;&#29702;&#35770;&#30740;&#31350;&#30340;&#25112;&#30053;&#20114;&#21160;&#26159;&#20915;&#31574;&#38382;&#39064;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#19968;&#20010;&#29702;&#24615;&#26234;&#33021;&#20307;&#30001;&#20854;&#29615;&#22659;&#65288;&#20854;&#20182;&#29609;&#23478;&#65289;&#39044;&#27979;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#20551;&#35774;&#36923;&#36753;&#20840;&#30693;&#24615;&#30340;&#29702;&#24615;&#20915;&#31574;&#29702;&#35770;&#12290;&#25105;&#20204;&#32771;&#34385;&#21453;&#22797;&#38754;&#20020;&#20915;&#31574;&#38382;&#39064;&#30340;&#26234;&#33021;&#20307;&#65288;&#21253;&#25324;&#23545;&#22278;&#21608;&#29575;&#23567;&#25968;&#36172;&#27880;&#30340;&#38382;&#39064;&#20197;&#21450;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#23545;&#25112;&#30340;&#28216;&#25103;&#65289;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;...
&lt;/p&gt;
&lt;p&gt;
The dominant theories of rational choice assume logical omniscience. That is, they assume that when facing a decision problem, an agent can perform all relevant computations and determine the truth value of all relevant logical/mathematical claims. This assumption is unrealistic when, for example, we offer bets on remote digits of pi or when an agent faces a computationally intractable planning problem. Furthermore, the assumption of logical omniscience creates contradictions in cases where the environment can contain descriptions of the agent itself. Importantly, strategic interactions as studied in game theory are decision problems in which a rational agent is predicted by its environment (the other players). In this paper, we develop a theory of rational decision making that does not assume logical omniscience. We consider agents who repeatedly face decision problems (including ones like betting on digits of pi or games against other agents). The main contribution of this paper is t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36827;&#34892;&#20102;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#25237;&#36164;&#32452;&#21512;&#35774;&#35745;&#26041;&#27861;&#65292;&#21253;&#25324;&#22343;&#20540;&#26041;&#24046;&#25237;&#36164;&#32452;&#21512;&#65288;MVP&#65289;&#12289;&#23618;&#27425;&#39118;&#38505;&#22343;&#34913;&#65288;HRP&#65289;&#22522;&#20110;&#25237;&#36164;&#32452;&#21512;&#21644;&#33258;&#32534;&#30721;&#22120;&#22522;&#20110;&#25237;&#36164;&#32452;&#21512;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#21360;&#24230;&#22269;&#23478;&#35777;&#21048;&#20132;&#26131;&#25152;&#65288;NSE&#65289;&#19978;&#30340;&#21313;&#20010;&#20027;&#39064;&#34892;&#19994;&#30340;&#32929;&#31080;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26679;&#26412;&#22806;&#25968;&#25454;&#19978;&#65292;MVP&#25237;&#36164;&#32452;&#21512;&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2307.05048</link><description>&lt;p&gt;
&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Portfolio Optimization: A Comparative Study. (arXiv:2307.05048v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36827;&#34892;&#20102;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#25237;&#36164;&#32452;&#21512;&#35774;&#35745;&#26041;&#27861;&#65292;&#21253;&#25324;&#22343;&#20540;&#26041;&#24046;&#25237;&#36164;&#32452;&#21512;&#65288;MVP&#65289;&#12289;&#23618;&#27425;&#39118;&#38505;&#22343;&#34913;&#65288;HRP&#65289;&#22522;&#20110;&#25237;&#36164;&#32452;&#21512;&#21644;&#33258;&#32534;&#30721;&#22120;&#22522;&#20110;&#25237;&#36164;&#32452;&#21512;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#21360;&#24230;&#22269;&#23478;&#35777;&#21048;&#20132;&#26131;&#25152;&#65288;NSE&#65289;&#19978;&#30340;&#21313;&#20010;&#20027;&#39064;&#34892;&#19994;&#30340;&#32929;&#31080;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26679;&#26412;&#22806;&#25968;&#25454;&#19978;&#65292;MVP&#25237;&#36164;&#32452;&#21512;&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#19968;&#30452;&#26159;&#37329;&#34701;&#30740;&#31350;&#30028;&#20851;&#27880;&#30340;&#39046;&#22495;&#12290;&#35774;&#35745;&#19968;&#20010;&#30408;&#21033;&#30340;&#25237;&#36164;&#32452;&#21512;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#21040;&#23545;&#26410;&#26469;&#32929;&#31080;&#25910;&#30410;&#21644;&#39118;&#38505;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#26412;&#31456;&#20171;&#32461;&#20102;&#19977;&#31181;&#25237;&#36164;&#32452;&#21512;&#35774;&#35745;&#26041;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#21253;&#25324;&#22343;&#20540;&#26041;&#24046;&#25237;&#36164;&#32452;&#21512;&#65288;MVP&#65289;&#12289;&#23618;&#27425;&#39118;&#38505;&#22343;&#34913;&#65288;HRP&#65289;&#22522;&#20110;&#25237;&#36164;&#32452;&#21512;&#21644;&#33258;&#32534;&#30721;&#22120;&#22522;&#20110;&#25237;&#36164;&#32452;&#21512;&#12290;&#36825;&#19977;&#31181;&#25237;&#36164;&#32452;&#21512;&#35774;&#35745;&#26041;&#27861;&#24212;&#29992;&#20110;&#20174;&#21360;&#24230;&#22269;&#23478;&#35777;&#21048;&#20132;&#26131;&#25152;&#65288;NSE&#65289;&#19978;&#36873;&#25321;&#30340;&#21313;&#20010;&#20027;&#39064;&#34892;&#19994;&#30340;&#32929;&#31080;&#30340;&#21382;&#21490;&#20215;&#26684;&#12290;&#25237;&#36164;&#32452;&#21512;&#26159;&#20351;&#29992;2018&#24180;1&#26376;1&#26085;&#21040;2021&#24180;12&#26376;31&#26085;&#30340;&#32929;&#31080;&#20215;&#26684;&#25968;&#25454;&#36827;&#34892;&#35774;&#35745;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#34920;&#29616;&#22312;2022&#24180;1&#26376;1&#26085;&#21040;2022&#24180;12&#26376;31&#26085;&#30340;&#26679;&#26412;&#22806;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23545;&#25237;&#36164;&#32452;&#21512;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35814;&#32454;&#32467;&#26524;&#20998;&#26512;&#12290;&#35266;&#23519;&#21040;MVP&#25237;&#36164;&#32452;&#21512;&#22312;&#26679;&#26412;&#22806;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#26368;&#22909;&#65292;&#39118;&#38505;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Portfolio optimization has been an area that has attracted considerable attention from the financial research community. Designing a profitable portfolio is a challenging task involving precise forecasting of future stock returns and risks. This chapter presents a comparative study of three portfolio design approaches, the mean-variance portfolio (MVP), hierarchical risk parity (HRP)-based portfolio, and autoencoder-based portfolio. These three approaches to portfolio design are applied to the historical prices of stocks chosen from ten thematic sectors listed on the National Stock Exchange (NSE) of India. The portfolios are designed using the stock price data from January 1, 2018, to December 31, 2021, and their performances are tested on the out-of-sample data from January 1, 2022, to December 31, 2022. Extensive results are analyzed on the performance of the portfolios. It is observed that the performance of the MVP portfolio is the best on the out-of-sample data for the risk-adjust
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#35843;&#26597;&#20102;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#25968;&#23383;&#31995;&#32479;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#36164;&#28304;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.05035</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#25968;&#23383;&#31995;&#32479;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Number Systems for Deep Neural Network Architectures: A Survey. (arXiv:2307.05035v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05035
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#35843;&#26597;&#20102;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#25968;&#23383;&#31995;&#32479;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#36164;&#28304;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24050;&#25104;&#20026;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#22312;&#33258;&#21160;&#39550;&#39542;&#12289;&#20581;&#24247;&#24212;&#29992;&#31561;&#26041;&#38754;&#65292;DNN&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#33021;&#27604;&#20154;&#31867;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#23558;DNN&#37096;&#32626;&#21040;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#20173;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#35745;&#31639;&#22797;&#26434;&#24615;&#12289;&#33021;&#28304;&#25928;&#29575;&#12289;&#24310;&#36831;&#21644;&#25104;&#26412;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#27491;&#22312;&#36861;&#27714;&#20960;&#20010;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#21152;&#36895;&#21644;&#39640;&#25928;&#22320;&#23454;&#29616;DNNs&#12290;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#26041;&#21521;&#26159;&#30830;&#23450;&#36866;&#29992;&#20110;DNN&#22788;&#29702;&#20013;&#28041;&#21450;&#30340;&#22823;&#37327;&#25968;&#25454;&#30340;&#36866;&#24403;&#25968;&#25454;&#34920;&#31034;&#12290;&#20351;&#29992;&#20256;&#32479;&#30340;&#25968;&#23383;&#31995;&#32479;&#34987;&#21457;&#29616;&#23545;DNNs&#26469;&#35828;&#19981;&#22815;&#20248;&#21270;&#12290;&#30456;&#21453;&#65292;&#35768;&#22810;&#30740;&#31350;&#23558;&#37325;&#28857;&#25918;&#22312;&#25506;&#32034;&#21512;&#36866;&#30340;&#25968;&#23383;&#31995;&#32479;&#19978;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#26356;&#26377;&#25928;&#34920;&#31034;&#30340;&#26367;&#20195;&#25968;&#23383;&#31995;&#32479;&#30340;&#20840;&#38754;&#35843;&#26597;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have become an enabling component for a myriad of artificial intelligence applications. DNNs have shown sometimes superior performance, even compared to humans, in cases such as self-driving, health applications, etc. Because of their computational complexity, deploying DNNs in resource-constrained devices still faces many challenges related to computing complexity, energy efficiency, latency, and cost. To this end, several research directions are being pursued by both academia and industry to accelerate and efficiently implement DNNs. One important direction is determining the appropriate data representation for the massive amount of data involved in DNN processing. Using conventional number systems has been found to be sub-optimal for DNNs. Alternatively, a great body of research focuses on exploring suitable number systems. This article aims to provide a comprehensive survey and discussion about alternative number systems for more efficient representation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#24320;&#28304;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#24037;&#20855;&#65292;&#20351;&#26222;&#36890;&#20154;&#21487;&#20197;&#30452;&#35266;&#22320;&#29702;&#35299;&#21644;&#25913;&#21892;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05029</link><description>&lt;p&gt;
FairLay-ML&#65306;&#25968;&#25454;&#39537;&#21160;&#31038;&#20250;&#20851;&#38190;&#31639;&#27861;&#20013;&#19981;&#20844;&#24179;&#30340;&#30452;&#35266;&#25913;&#21892;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FairLay-ML: Intuitive Remedies for Unfairness in Data-Driven Social-Critical Algorithms. (arXiv:2307.05029v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#24320;&#28304;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#24037;&#20855;&#65292;&#20351;&#26222;&#36890;&#20154;&#21487;&#20197;&#30452;&#35266;&#22320;&#29702;&#35299;&#21644;&#25913;&#21892;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#24320;&#28304;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#35299;&#37322;&#24037;&#20855;&#65292;&#20197;&#20102;&#35299;&#36825;&#20123;&#24037;&#20855;&#26159;&#21542;&#33021;&#22815;&#35753;&#26222;&#36890;&#20154;&#21487;&#35270;&#21270;&#12289;&#29702;&#35299;&#21644;&#24314;&#35758;&#30452;&#35266;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;ML&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#38024;&#23545;&#23569;&#25968;&#32676;&#20307;&#21463;&#20559;&#35265;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#25351;&#23548;&#37325;&#22823;&#30340;&#31038;&#20250;&#20915;&#31574;&#65292;&#36843;&#20999;&#38656;&#35201;&#30740;&#31350;&#23427;&#20204;&#22312;&#19981;&#20844;&#24179;&#26041;&#38754;&#30340;&#36923;&#36753;&#12290;&#30001;&#20110;&#36825;&#20010;&#38382;&#39064;&#23545;&#24191;&#22823;&#20844;&#20247;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#29702;&#35299;&#36825;&#20123;&#31639;&#27861;&#20013;&#19981;&#20844;&#24179;&#24615;&#36136;&#21644;&#28508;&#22312;&#30340;&#26435;&#34913;&#26159;&#38750;&#24120;&#20851;&#38190;&#30340;&#65292;&#19981;&#20165;&#20165;&#26159;&#31038;&#20250;&#27491;&#20041;&#39046;&#22495;&#30340;&#19987;&#23478;&#25110;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#38656;&#35201;&#29702;&#35299;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25968;&#23398;&#23450;&#20041;&#21644;&#24037;&#20855;&#19978;&#65292;&#20197;&#29702;&#35299;&#21644;&#35299;&#20915;&#19981;&#20844;&#24179;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20123;&#30452;&#25509;&#25552;&#21040;&#29992;&#25143;&#20132;&#20114;&#24037;&#20855;&#26159;&#26410;&#26469;&#24037;&#20316;&#25152;&#24517;&#38656;&#30340;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;FairLay-ML&#65292;&#19968;&#20010;&#27010;&#24565;&#35777;&#26126;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
This thesis explores open-sourced machine learning (ML) model explanation tools to understand whether these tools can allow a layman to visualize, understand, and suggest intuitive remedies to unfairness in ML-based decision-support systems. Machine learning models trained on datasets biased against minority groups are increasingly used to guide life-altering social decisions, prompting the urgent need to study their logic for unfairness. Due to this problem's impact on vast populations of the general public, it is critical for the layperson -- not just subject matter experts in social justice or machine learning experts -- to understand the nature of unfairness within these algorithms and the potential trade-offs. Existing research on fairness in machine learning focuses mostly on the mathematical definitions and tools to understand and remedy unfair models, with some directly citing user-interactive tools as necessary for future work. This thesis presents FairLay-ML, a proof-of-conce
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#34920;&#26126;&#65292;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#31574;&#30053;&#65288;&#22914;&#23398;&#20064;&#29575;&#34928;&#20943;&#12289;&#27169;&#22411;&#26435;&#37325;&#24179;&#22343;&#21644;&#25968;&#25454;&#22686;&#24378;&#65289;&#30340;&#31616;&#21333;&#22522;&#20934;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#27491;&#21017;&#21270;&#31574;&#30053;&#30340;&#32452;&#21512;&#22312;&#23398;&#20064;&#22024;&#26434;&#26631;&#31614;&#38382;&#39064;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05025</link><description>&lt;p&gt;
&#21457;&#25381;&#27491;&#21017;&#21270;&#31574;&#30053;&#22312;&#20855;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Potential of Regularization Strategies in Learning with Noisy Labels. (arXiv:2307.05025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05025
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#34920;&#26126;&#65292;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#31574;&#30053;&#65288;&#22914;&#23398;&#20064;&#29575;&#34928;&#20943;&#12289;&#27169;&#22411;&#26435;&#37325;&#24179;&#22343;&#21644;&#25968;&#25454;&#22686;&#24378;&#65289;&#30340;&#31616;&#21333;&#22522;&#20934;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#27491;&#21017;&#21270;&#31574;&#30053;&#30340;&#32452;&#21512;&#22312;&#23398;&#20064;&#22024;&#26434;&#26631;&#31614;&#38382;&#39064;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#23398;&#20064;&#22024;&#26434;&#26631;&#31614;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#26032;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#22024;&#26434;&#35757;&#32451;&#26631;&#31614;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#36827;&#34892;&#27867;&#21270;&#12290;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#21253;&#25324;&#22797;&#26434;&#30340;&#25216;&#26415;&#65292;&#22914;&#22122;&#22768;&#24314;&#27169;&#12289;&#26631;&#31614;&#26657;&#27491;&#21644;&#21327;&#21516;&#35757;&#32451;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#24120;&#29992;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#65288;&#22914;&#23398;&#20064;&#29575;&#34928;&#20943;&#12289;&#27169;&#22411;&#26435;&#37325;&#24179;&#22343;&#21644;&#25968;&#25454;&#22686;&#24378;&#65289;&#30340;&#31616;&#21333;&#22522;&#20934;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#37319;&#29992;&#27491;&#21017;&#21270;&#31574;&#30053;&#30340;&#32452;&#21512;&#27604;&#22797;&#26434;&#30340;&#31639;&#27861;&#26356;&#26377;&#25928;&#22320;&#24212;&#23545;&#23398;&#20064;&#22024;&#26434;&#26631;&#31614;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#19968;&#20123;&#27491;&#21017;&#21270;&#31574;&#30053;&#22312;&#20043;&#21069;&#30340;&#23398;&#20064;&#22024;&#26434;&#26631;&#31614;&#30740;&#31350;&#20013;&#24050;&#34987;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#40723;&#21169;&#37325;&#26032;&#35780;&#20272;&#23398;&#20064;&#22024;&#26434;&#26631;&#31614;&#30340;&#22522;&#20934;&#65292;&#24182;&#20419;&#20351;&#37325;&#26032;&#32771;&#34385;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, research on learning with noisy labels has focused on devising novel algorithms that can achieve robustness to noisy training labels while generalizing to clean data. These algorithms often incorporate sophisticated techniques, such as noise modeling, label correction, and co-training. In this study, we demonstrate that a simple baseline using cross-entropy loss, combined with widely used regularization strategies like learning rate decay, model weights average, and data augmentations, can outperform state-of-the-art methods. Our findings suggest that employing a combination of regularization strategies can be more effective than intricate algorithms in tackling the challenges of learning with noisy labels. While some of these regularization strategies have been utilized in previous noisy label learning research, their full potential has not been thoroughly explored. Our results encourage a reevaluation of benchmarks for learning with noisy labels and prompt reconsider
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#28608;&#27963;&#26144;&#23556;&#65288;FAM&#65289;&#30340;&#35299;&#37322;&#24037;&#20855;&#65292;&#21487;&#20197;&#35299;&#37322;&#27809;&#26377;FC&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#20998;&#31867;&#22120;&#65292;&#20351;&#20854;&#26356;&#21152;&#21487;&#35299;&#37322;&#12289;&#36879;&#26126;&#21644;&#21487;&#20449;&#12290;</title><link>http://arxiv.org/abs/2307.05017</link><description>&lt;p&gt;
&#29305;&#24449;&#28608;&#27963;&#26144;&#23556;&#65306;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#35270;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Feature Activation Map: Visual Explanation of Deep Learning Models for Image Classification. (arXiv:2307.05017v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#28608;&#27963;&#26144;&#23556;&#65288;FAM&#65289;&#30340;&#35299;&#37322;&#24037;&#20855;&#65292;&#21487;&#20197;&#35299;&#37322;&#27809;&#26377;FC&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#20998;&#31867;&#22120;&#65292;&#20351;&#20854;&#26356;&#21152;&#21487;&#35299;&#37322;&#12289;&#36879;&#26126;&#21644;&#21487;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#35270;&#21270;&#22270;&#20687;&#19978;&#30340;&#21028;&#21035;&#21306;&#22495;&#65292;&#21487;&#20197;&#29702;&#35299;&#21644;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20915;&#31574;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;CAM&#65289;&#30340;&#26041;&#27861;&#20316;&#20026;&#24378;&#22823;&#30340;&#35299;&#37322;&#24037;&#20855;&#65292;&#20351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#26356;&#21152;&#21487;&#35299;&#37322;&#12289;&#36879;&#26126;&#21644;&#21487;&#20449;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#22522;&#20110;CAM&#30340;&#26041;&#27861;&#65288;&#22914;CAM&#12289;Grad-CAM&#21644;Relevance-CAM&#65289;&#21482;&#33021;&#29992;&#20110;&#35299;&#37322;&#20855;&#26377;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#23618;&#20316;&#20026;&#20998;&#31867;&#22120;&#30340;CNN&#27169;&#22411;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27809;&#26377;FC&#23618;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#20363;&#22914;&#23567;&#26679;&#26412;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#12289;&#23545;&#27604;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#28608;&#27963;&#26144;&#23556;&#65288;FAM&#65289;&#30340;&#20107;&#21518;&#35299;&#37322;&#24037;&#20855;&#65292;&#21487;&#20197;&#35299;&#37322;&#27809;&#26377;FC&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#20998;&#31867;&#22120;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;FAM&#31639;&#27861;&#20013;&#65292;&#36890;&#36807;&#20004;&#20010;&#22270;&#20687;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24471;&#21040;&#36890;&#36947;&#26435;&#37325;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decisions made by convolutional neural networks(CNN) can be understood and explained by visualizing discriminative regions on images. To this end, Class Activation Map (CAM) based methods were proposed as powerful interpretation tools, making the prediction of deep learning models more explainable, transparent, and trustworthy. However, all the CAM-based methods (e.g., CAM, Grad-CAM, and Relevance-CAM) can only be used for interpreting CNN models with fully-connected (FC) layers as a classifier. It is worth noting that many deep learning models classify images without FC layers, e.g., few-shot learning image classification, contrastive learning image classification, and image retrieval tasks. In this work, a post-hoc interpretation tool named feature activation map (FAM) is proposed, which can interpret deep learning models without FC layers as a classifier. In the proposed FAM algorithm, the channel-wise contribution weights are derived from the similarity scores between two image emb
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#27979;&#35797;&#26102;&#22521;&#35757;&#65288;TTT&#65289;&#21040;&#35270;&#39057;&#27969;&#30340;&#35774;&#32622;&#20013;&#65292;&#25552;&#20986;&#20102;&#22312;&#32447;TTT&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#22266;&#23450;&#27169;&#22411;&#22522;&#32447;&#21644;&#31163;&#32447;TTT&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#37117;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#21253;&#25324;&#23454;&#20363;&#21644;&#20840;&#26223;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2307.05014</link><description>&lt;p&gt;
&#35270;&#39057;&#27969;&#19978;&#30340;&#27979;&#35797;&#26102;&#22521;&#35757;
&lt;/p&gt;
&lt;p&gt;
Test-Time Training on Video Streams. (arXiv:2307.05014v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05014
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#27979;&#35797;&#26102;&#22521;&#35757;&#65288;TTT&#65289;&#21040;&#35270;&#39057;&#27969;&#30340;&#35774;&#32622;&#20013;&#65292;&#25552;&#20986;&#20102;&#22312;&#32447;TTT&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#22266;&#23450;&#27169;&#22411;&#22522;&#32447;&#21644;&#31163;&#32447;TTT&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#37117;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#21253;&#25324;&#23454;&#20363;&#21644;&#20840;&#26223;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#23558;&#27979;&#35797;&#26102;&#22521;&#35757;&#65288;TTT&#65289;&#30830;&#23450;&#20026;&#19968;&#31181;&#22312;&#27979;&#35797;&#26102;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#27169;&#22411;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#22312;&#23545;&#27599;&#20010;&#27979;&#35797;&#23454;&#20363;&#36827;&#34892;&#39044;&#27979;&#20043;&#21069;&#65292;&#27169;&#22411;&#20250;&#20351;&#29992;&#33258;&#30417;&#30563;&#20219;&#21153;&#65288;&#20363;&#22914;&#20351;&#29992;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#22270;&#20687;&#37325;&#24314;&#65289;&#22312;&#21516;&#19968;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;TTT&#25193;&#23637;&#21040;&#27969;&#24335;&#35774;&#32622;&#20013;&#65292;&#20854;&#20013;&#22810;&#20010;&#27979;&#35797;&#23454;&#20363;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#20026;&#35270;&#39057;&#24103;&#65289;&#25353;&#26102;&#38388;&#39034;&#24207;&#21040;&#36798;&#12290;&#25105;&#20204;&#30340;&#25193;&#23637;&#26159;&#22312;&#32447;TTT&#65306;&#24403;&#21069;&#27169;&#22411;&#20174;&#19978;&#20010;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#28982;&#21518;&#22312;&#24403;&#21069;&#24103;&#21644;&#21069;&#20960;&#20010;&#24103;&#30340;&#23567;&#31383;&#21475;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#32447;TTT&#22312;&#22235;&#20010;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#22266;&#23450;&#27169;&#22411;&#22522;&#32447;&#65292;&#22312;&#19977;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#30456;&#23545;&#25913;&#36827;&#20998;&#21035;&#20026;45%&#21644;66%&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#32447;TTT&#20063;&#20248;&#20110;&#20854;&#31163;&#32447;&#29256;&#26412;&#65292;&#21518;&#32773;&#35775;&#38382;&#26356;&#22810;&#20449;&#24687;&#65292;&#21487;&#20197;&#35757;&#32451;&#25152;&#26377;&#24103;&#32780;&#19981;&#32771;&#34385;&#26102;&#38388;&#39034;&#24207;&#12290;&#36825;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has established test-time training (TTT) as a general framework to further improve a trained model at test time. Before making a prediction on each test instance, the model is trained on the same instance using a self-supervised task, such as image reconstruction with masked autoencoders. We extend TTT to the streaming setting, where multiple test instances - video frames in our case - arrive in temporal order. Our extension is online TTT: The current model is initialized from the previous model, then trained on the current frame and a small window of frames immediately before. Online TTT significantly outperforms the fixed-model baseline for four tasks, on three real-world datasets. The relative improvement is 45% and 66% for instance and panoptic segmentation. Surprisingly, online TTT also outperforms its offline variant that accesses more information, training on all frames from the entire test video regardless of temporal order. This differs from previous findings using 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LookAhead&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#25552;&#21069;&#35266;&#23519;&#38899;&#39057;&#36755;&#20837;&#30340;&#26410;&#26469;&#37096;&#20998;&#65292;&#20351;RNN-Transducers&#27169;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#26356;&#21152;&#19982;&#22768;&#23398;&#30456;&#31526;&#12290;&#35813;&#25216;&#26415;&#22312;&#20934;&#30830;&#29575;&#19978;&#30456;&#23545;&#38477;&#20302;&#20102;5%-20%&#12290;</title><link>http://arxiv.org/abs/2307.05006</link><description>&lt;p&gt;
&#29992;&#22768;&#23398;&#39044;&#27979;&#25913;&#36827;RNN-Transducers&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving RNN-Transducers with Acoustic LookAhead. (arXiv:2307.05006v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LookAhead&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#25552;&#21069;&#35266;&#23519;&#38899;&#39057;&#36755;&#20837;&#30340;&#26410;&#26469;&#37096;&#20998;&#65292;&#20351;RNN-Transducers&#27169;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#26356;&#21152;&#19982;&#22768;&#23398;&#30456;&#31526;&#12290;&#35813;&#25216;&#26415;&#22312;&#20934;&#30830;&#29575;&#19978;&#30456;&#23545;&#38477;&#20302;&#20102;5%-20%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RNN-Transducers&#65288;RNN-Ts&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#25509;&#21463;&#20316;&#20026;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35821;&#38899;&#36716;&#25991;&#26412;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#39640;&#20934;&#30830;&#29575;&#21644;&#27969;&#24335;&#22788;&#29702;&#33021;&#21147;&#12290;&#20256;&#32479;&#30340;RNN-T&#27169;&#22411;&#29420;&#31435;&#22320;&#32534;&#30721;&#36755;&#20837;&#38899;&#39057;&#21644;&#25991;&#26412;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#34180;&#22411;&#32852;&#21512;&#32593;&#32476;&#23558;&#20004;&#31181;&#32534;&#30721;&#32467;&#21512;&#36215;&#26469;&#12290;&#34429;&#28982;&#36825;&#31181;&#26550;&#26500;&#25552;&#20379;&#20102;SOTA&#30340;&#27969;&#24335;&#22788;&#29702;&#20934;&#30830;&#29575;&#65292;&#20294;&#20063;&#20351;&#27169;&#22411;&#23545;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#20559;&#35265;&#33030;&#24369;&#65292;&#36825;&#34920;&#29616;&#20026;&#22312;&#27809;&#26377;&#22768;&#23398;&#35777;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#25991;&#26412;&#36827;&#34892;&#22810;&#27493;&#24187;&#35273;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LookAhead&#25216;&#26415;&#65292;&#36890;&#36807;&#25552;&#21069;&#35266;&#23519;&#38899;&#39057;&#36755;&#20837;&#30340;&#26410;&#26469;&#37096;&#20998;&#65292;&#20351;&#25991;&#26412;&#34920;&#31034;&#26356;&#20855;&#26377;&#22768;&#23398;&#22522;&#30784;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#35780;&#20272;&#38598;&#19978;&#30456;&#23545;&#38169;&#35823;&#29575;&#26377;&#26174;&#33879;&#30340;5%-20%&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
RNN-Transducers (RNN-Ts) have gained widespread acceptance as an end-to-end model for speech to text conversion because of their high accuracy and streaming capabilities. A typical RNN-T independently encodes the input audio and the text context, and combines the two encodings by a thin joint network. While this architecture provides SOTA streaming accuracy, it also makes the model vulnerable to strong LM biasing which manifests as multi-step hallucination of text without acoustic evidence. In this paper we propose LookAhead that makes text representations more acoustically grounded by looking ahead into the future within the audio input. This technique yields a significant 5%-20% relative reduction in word error rate on both in-domain and out-of-domain evaluation sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25511;&#21046;&#19982;&#27010;&#29575;&#25512;&#29702;&#32467;&#21512;&#30340;&#26032;&#39062;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#12290;&#26234;&#33021;&#20307;&#36890;&#36807;&#25512;&#29702;&#25511;&#21046;&#20854;&#21160;&#20316;&#65292;&#24182;&#36890;&#36807;&#28040;&#24687;&#36827;&#34892;&#36890;&#20449;&#65292;&#20174;&#32780;&#23454;&#29616;&#21327;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.05004</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25511;&#21046;&#20316;&#20026;&#27010;&#29575;&#25512;&#29702;&#30340;&#26032;&#20852;&#36890;&#20449;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Control as Probabilistic Inference as an Emergent Communication Mechanism in Multi-Agent Reinforcement Learning. (arXiv:2307.05004v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25511;&#21046;&#19982;&#27010;&#29575;&#25512;&#29702;&#32467;&#21512;&#30340;&#26032;&#39062;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#12290;&#26234;&#33021;&#20307;&#36890;&#36807;&#25512;&#29702;&#25511;&#21046;&#20854;&#21160;&#20316;&#65292;&#24182;&#36890;&#36807;&#28040;&#24687;&#36827;&#34892;&#36890;&#20449;&#65292;&#20174;&#32780;&#23454;&#29616;&#21327;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#65292;&#23558;&#26032;&#20852;&#36890;&#20449;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20102;&#25972;&#21512;&#12290;&#26234;&#33021;&#20307;&#36890;&#36807;&#27010;&#29575;&#25512;&#29702;&#36827;&#34892;&#21160;&#20316;&#35268;&#21010;&#65292;&#31216;&#20026;&#25511;&#21046;&#20316;&#20026;&#25512;&#29702;&#65292;&#24182;&#20351;&#29992;&#28508;&#22312;&#21464;&#37327;&#21644;&#26681;&#25454;&#35268;&#21010;&#30340;&#21160;&#20316;&#36827;&#34892;&#20272;&#35745;&#30340;&#28040;&#24687;&#36827;&#34892;&#36890;&#20449;&#12290;&#36890;&#36807;&#36825;&#20123;&#28040;&#24687;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#21487;&#20197;&#21457;&#36865;&#20851;&#20110;&#20854;&#21160;&#20316;&#30340;&#20449;&#24687;&#65292;&#24182;&#20102;&#35299;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#21160;&#20316;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#26234;&#33021;&#20307;&#26681;&#25454;&#20272;&#35745;&#30340;&#28040;&#24687;&#26469;&#25913;&#21464;&#20854;&#21160;&#20316;&#65292;&#20197;&#23454;&#29616;&#21327;&#20316;&#20219;&#21153;&#12290;&#36825;&#31181;&#28040;&#24687;&#30340;&#25512;&#29702;&#21487;&#20197;&#34987;&#35270;&#20026;&#36890;&#20449;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;Metropolis-Hasting&#21629;&#21517;&#28216;&#25103;&#26469;&#36827;&#34892;&#24418;&#24335;&#21270;&#12290;&#36890;&#36807;&#22312;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#20986;&#30340;&#27010;&#29575;&#22270;&#27169;&#22411;&#21487;&#20197;&#25512;&#26029;&#20986;&#26377;&#24847;&#20041;&#30340;&#28040;&#24687;&#65292;&#20197;&#23454;&#29616;&#21327;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a generative probabilistic model integrating emergent communication and multi-agent reinforcement learning. The agents plan their actions by probabilistic inference, called control as inference, and communicate using messages that are latent variables and estimated based on the planned actions. Through these messages, each agent can send information about its actions and know information about the actions of another agent. Therefore, the agents change their actions according to the estimated messages to achieve cooperative tasks. This inference of messages can be considered as communication, and this procedure can be formulated by the Metropolis-Hasting naming game. Through experiments in the grid world environment, we show that the proposed PGM can infer meaningful messages to achieve the cooperative task.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#32447;&#22238;&#24402;&#23454;&#29616;&#36873;&#25321;&#24615;&#37319;&#26679;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#21482;&#26377;&#22122;&#22768;&#19987;&#23478;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#12290;&#31639;&#27861;&#19981;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#21363;&#21487;&#25104;&#21151;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#22238;&#24402;&#21644;&#26597;&#35810;&#27425;&#25968;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.04998</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#22238;&#24402;&#36827;&#34892;&#36873;&#25321;&#24615;&#37319;&#26679;&#21644;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Selective Sampling and Imitation Learning via Online Regression. (arXiv:2307.04998v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#32447;&#22238;&#24402;&#23454;&#29616;&#36873;&#25321;&#24615;&#37319;&#26679;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#21482;&#26377;&#22122;&#22768;&#19987;&#23478;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#12290;&#31639;&#27861;&#19981;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#21363;&#21487;&#25104;&#21151;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#22238;&#24402;&#21644;&#26597;&#35810;&#27425;&#25968;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#20027;&#21160;&#26597;&#35810;&#22024;&#26434;&#30340;&#19987;&#23478;&#26469;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#27169;&#20223;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#20551;&#35774;&#21487;&#20197;&#33719;&#24471;&#26080;&#22122;&#22768;&#30340;&#19987;&#23478;&#21453;&#39304;&#65292;&#32780;&#36825;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#23454;&#38469;&#19978;&#65292;&#24403;&#21482;&#33021;&#33719;&#24471;&#22024;&#26434;&#30340;&#19987;&#23478;&#21453;&#39304;&#26102;&#65292;&#20381;&#36182;&#32431;&#31163;&#32447;&#25968;&#25454;&#30340;&#31639;&#27861;&#65288;&#38750;&#20132;&#20114;&#24335;IL&#65289;&#34987;&#35777;&#26126;&#38656;&#35201;&#22823;&#37327;&#30340;&#26679;&#26412;&#25165;&#33021;&#25104;&#21151;&#12290;&#30456;&#21453;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;IL&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#37319;&#26679;&#26469;&#20027;&#21160;&#26597;&#35810;&#22024;&#26434;&#30340;&#19987;&#23478;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36890;&#29992;&#20989;&#25968;&#31867;&#21644;&#22810;&#20010;&#21160;&#20316;&#30340;&#26032;&#36873;&#25321;&#24615;&#37319;&#26679;&#31639;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#22238;&#24402;&#21644;&#26597;&#35810;&#27425;&#25968;&#30028;&#38480;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#20998;&#26512;&#25193;&#23637;&#21040;&#20102;&#20855;&#26377;&#22024;&#26434;&#19987;&#23478;&#21453;&#39304;&#30340;IL&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;IL&#31639;&#27861;&#26469;&#36827;&#34892;&#26377;&#38480;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of Imitation Learning (IL) by actively querying noisy expert for feedback. While imitation learning has been empirically successful, much of prior work assumes access to noiseless expert feedback which is not practical in many applications. In fact, when one only has access to noisy expert feedback, algorithms that rely on purely offline data (non-interactive IL) can be shown to need a prohibitively large number of samples to be successful. In contrast, in this work, we provide an interactive algorithm for IL that uses selective sampling to actively query the noisy expert for feedback. Our contributions are twofold: First, we provide a new selective sampling algorithm that works with general function classes and multiple actions, and obtains the best-known bounds for the regret and the number of queries. Next, we extend this analysis to the problem of IL with noisy expert feedback and provide a new IL algorithm that makes limited queries.  Our algorithm for sele
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#21478;&#19968;&#31181;&#20351;&#29992;XGBoost&#31639;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#31456;&#25512;&#33616;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#22312;&#19968;&#20010;&#22823;&#22411;&#36328;&#22269;&#37329;&#34701;&#26381;&#21153;&#20844;&#21496;&#30340;&#23458;&#25143;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.04996</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning. (arXiv:2307.04996v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#21478;&#19968;&#31181;&#20351;&#29992;XGBoost&#31639;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#31456;&#25512;&#33616;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#22312;&#19968;&#20010;&#22823;&#22411;&#36328;&#22269;&#37329;&#34701;&#26381;&#21153;&#20844;&#21496;&#30340;&#23458;&#25143;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25512;&#33616;&#22312;&#30452;&#25509;&#33829;&#38144;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#28608;&#21457;&#20102;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#24212;&#29992;&#26469;&#25552;&#21319;&#23458;&#25143;&#20307;&#39564;&#30340;&#30740;&#31350;&#21160;&#26426;&#12290;&#20363;&#22914;&#65292;&#22312;&#37329;&#34701;&#26381;&#21153;&#39046;&#22495;&#65292;&#20844;&#21496;&#21487;&#20197;&#36890;&#36807;&#21521;&#23458;&#25143;&#25552;&#20379;&#30456;&#20851;&#37329;&#34701;&#25991;&#31456;&#26469;&#22521;&#20859;&#20851;&#31995;&#65292;&#20419;&#36827;&#23458;&#25143;&#21442;&#19982;&#21644;&#20419;&#36827;&#30693;&#24773;&#30340;&#37329;&#34701;&#20915;&#31574;&#12290;&#23613;&#31649;&#19968;&#20123;&#26041;&#27861;&#19987;&#27880;&#20110;&#22522;&#20110;KG&#30340;&#25512;&#33616;&#31995;&#32479;&#20197;&#25913;&#36827;&#20869;&#23481;&#65292;&#20294;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;KG&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#36827;&#34892;&#20915;&#31574;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#20010;&#24615;&#21270;&#25991;&#31456;&#25512;&#33616;&#26041;&#27861;&#65292;&#29992;&#20110;&#19968;&#23478;&#22823;&#22411;&#36328;&#22269;&#37329;&#34701;&#26381;&#21153;&#20844;&#21496;&#30340;&#19968;&#32452;&#23458;&#25143;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#31532;&#20108;&#31181;&#26041;&#27861;&#20351;&#29992;XGBoost&#31639;&#27861;&#26469;&#21521;&#23458;&#25143;&#25512;&#33616;&#25991;&#31456;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21033;&#29992;&#20174;&#32467;&#26500;&#21270;&#65288;&#34920;&#26684;&#25968;&#25454;&#65289;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#65289;&#29983;&#25104;&#30340;KG&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized recommendations have a growing importance in direct marketing, which motivates research to enhance customer experiences by knowledge graph (KG) applications. For example, in financial services, companies may benefit from providing relevant financial articles to their customers to cultivate relationships, foster client engagement and promote informed financial decisions. While several approaches center on KG-based recommender systems for improved content, in this study we focus on interpretable KG-based recommender systems for decision making.To this end, we present two knowledge graph-based approaches for personalized article recommendations for a set of customers of a large multinational financial services company. The first approach employs Reinforcement Learning and the second approach uses the XGBoost algorithm for recommending articles to the customers. Both approaches make use of a KG generated from both structured (tabular data) and unstructured data (a large body o
&lt;/p&gt;</description></item><item><title>PowerFusion&#26159;&#19968;&#31181;&#24352;&#37327;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#32771;&#34385;&#35745;&#31639;&#21644;&#25968;&#25454;&#31227;&#21160;&#20248;&#21270;&#65292;&#29983;&#25104;&#39640;&#24615;&#33021;&#20869;&#23384;&#23494;&#38598;&#36816;&#31639;&#31526;&#30340;&#39640;&#25928;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2307.04995</link><description>&lt;p&gt;
PowerFusion: &#19968;&#31181;&#24102;&#26377;&#26174;&#24335;&#25968;&#25454;&#31227;&#21160;&#25551;&#36848;&#21644;&#25351;&#20196;&#32423;&#22270;&#24418;IR&#30340;&#24352;&#37327;&#32534;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
PowerFusion: A Tensor Compiler with Explicit Data Movement Description and Instruction-level Graph IR. (arXiv:2307.04995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04995
&lt;/p&gt;
&lt;p&gt;
PowerFusion&#26159;&#19968;&#31181;&#24352;&#37327;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#32771;&#34385;&#35745;&#31639;&#21644;&#25968;&#25454;&#31227;&#21160;&#20248;&#21270;&#65292;&#29983;&#25104;&#39640;&#24615;&#33021;&#20869;&#23384;&#23494;&#38598;&#36816;&#31639;&#31526;&#30340;&#39640;&#25928;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#21152;&#36895;DNN&#35745;&#31639;&#65292;&#25552;&#20986;&#20102;&#24352;&#37327;&#32534;&#35793;&#22120;&#20197;&#22312;&#19981;&#21516;&#30340;&#39046;&#22495;&#29305;&#23450;&#21152;&#36895;&#22120;&#19978;&#29983;&#25104;&#39640;&#25928;&#30340;&#20195;&#30721;&#12290;&#29616;&#26377;&#30340;&#24352;&#37327;&#32534;&#35793;&#22120;&#20027;&#35201;&#20851;&#27880;&#20248;&#21270;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21152;&#36895;&#22120;&#30340;&#35745;&#31639;&#24615;&#33021;&#36828;&#36828;&#36229;&#36807;&#20869;&#23384;&#24615;&#33021;&#65292;&#20869;&#23384;&#35775;&#38382;&#25104;&#20026;&#20851;&#38190;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#30446;&#21069;&#24352;&#37327;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#65288;IR&#65289;&#20013;&#23545;&#20869;&#23384;&#35775;&#38382;&#21644;&#25968;&#25454;&#20381;&#36182;&#30340;&#30452;&#25509;&#25551;&#36848;&#19981;&#36275;&#65292;&#32473;&#29983;&#25104;&#20869;&#23384;&#39640;&#25928;&#20195;&#30721;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IntelliGen&#65292;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#35745;&#31639;&#21644;&#25968;&#25454;&#31227;&#21160;&#20248;&#21270;&#26469;&#29983;&#25104;&#39640;&#24615;&#33021;&#20869;&#23384;&#23494;&#38598;&#36816;&#31639;&#31526;&#30340;&#24352;&#37327;&#32534;&#35793;&#22120;&#12290;IntelliGen&#20351;&#29992;GIR&#26469;&#34920;&#31034;DNN&#31243;&#24207;&#65292;&#20854;&#20013;&#21253;&#21547;&#25351;&#31034;&#20854;&#35745;&#31639;&#12289;&#25968;&#25454;&#31227;&#21160;&#21644;&#24182;&#34892;&#31574;&#30053;&#30340;&#22522;&#20803;&#12290;&#36825;&#20123;&#20449;&#24687;&#23558;&#34987;&#12290;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are of critical use in different domains. To accelerate DNN computation, tensor compilers are proposed to generate efficient code on different domain-specific accelerators. Existing tensor compilers mainly focus on optimizing computation efficiency. However, memory access is becoming a key performance bottleneck because the computational performance of accelerators is increasing much faster than memory performance. The lack of direct description of memory access and data dependence in current tensor compilers' intermediate representation (IR) brings significant challenges to generate memory-efficient code.  In this paper, we propose IntelliGen, a tensor compiler that can generate high-performance code for memory-intensive operators by considering both computation and data movement optimizations. IntelliGen represent a DNN program using GIR, which includes primitives indicating its computation, data movement, and parallel strategies. This information will be 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#35774;&#32622;&#20013;&#37327;&#21270;&#40657;&#27934;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#26377;&#29992;&#30340;&#39044;&#27979;&#21306;&#38388;&#25351;&#26631;&#65292;&#24182;&#35843;&#25972;&#21040;&#40657;&#27934;&#36136;&#37327;&#21450;&#20854;&#30456;&#20851;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04993</link><description>&lt;p&gt;
&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#23545;&#32500;&#37324;&#40657;&#27934;&#36136;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification of the Virial Black Hole Mass with Conformal Prediction. (arXiv:2307.04993v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04993
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#35774;&#32622;&#20013;&#37327;&#21270;&#40657;&#27934;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#26377;&#29992;&#30340;&#39044;&#27979;&#21306;&#38388;&#25351;&#26631;&#65292;&#24182;&#35843;&#25972;&#21040;&#40657;&#27934;&#36136;&#37327;&#21450;&#20854;&#30456;&#20851;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#27979;&#37327;&#40657;&#27934;&#36136;&#37327;&#23545;&#20110;&#29702;&#35299;&#40657;&#27934;&#21644;&#23487;&#20027;&#26143;&#31995;&#30340;&#20849;&#21516;&#28436;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#30452;&#25509;&#27979;&#37327;&#40657;&#27934;&#36136;&#37327;&#36890;&#24120;&#20165;&#38480;&#20110;&#26368;&#36817;&#30340;&#26143;&#31995;&#65292;&#32780;&#39640;&#32418;&#31227;&#30340;&#29289;&#20307;&#21017;&#20351;&#29992;&#38388;&#25509;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#21333;&#27425;&#32426;&#20803;&#30340;&#32500;&#37324;&#40657;&#27934;&#36136;&#37327;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#20854;&#20381;&#36182;&#20110;&#19968;&#20010;&#23567;&#26679;&#26412;&#30340;&#26412;&#22320;&#27963;&#21160;&#26143;&#31995;&#26680;&#30340;&#27604;&#20363;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#31526;&#21512;&#21270;&#20998;&#20301;&#25968;&#22238;&#24402;&#65288;CQR&#65289;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20197;&#37327;&#21270;&#40657;&#27934;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23558;CQR&#19982;&#21508;&#31181;&#39044;&#27979;&#21306;&#38388;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;CQR&#21487;&#20197;&#25552;&#20379;&#26356;&#26377;&#29992;&#30340;&#39044;&#27979;&#21306;&#38388;&#25351;&#26631;&#12290;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CQR&#26041;&#27861;&#33021;&#22815;&#35843;&#25972;&#21040;&#40657;&#27934;&#36136;&#37327;&#21450;&#20854;&#30456;&#20851;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise measurements of the black hole mass are essential to gain insight on the black hole and host galaxy co-evolution. A direct measure of the black hole mass is often restricted to nearest galaxies and instead, an indirect method using the single-epoch virial black hole mass estimation is used for objects at high redshifts. However, this method is subjected to biases and uncertainties as it is reliant on the scaling relation from a small sample of local active galactic nuclei. In this study, we propose the application of conformalised quantile regression (CQR) to quantify the uncertainties of the black hole predictions in a machine learning setting. We compare CQR with various prediction interval techniques and demonstrated that CQR can provide a more useful prediction interval indicator. In contrast to baseline approaches for prediction interval estimation, we show that the CQR method provides prediction intervals that adjust to the black hole mass and its related properties. That
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38480;&#21046;&#27169;&#22411;&#65292;&#21363;&#21333;&#35843;DBM&#65292;&#23427;&#20801;&#35768;&#27599;&#19968;&#23618;&#20855;&#26377;&#20219;&#24847;&#30340;&#33258;&#36830;&#25509;&#65292;&#20294;&#36890;&#36807;&#19968;&#31181;&#26041;&#24335;&#38480;&#21046;&#20102;&#26435;&#37325;&#65292;&#20197;&#20445;&#35777;&#23384;&#22312;&#21644;&#20840;&#23616;&#21807;&#19968;&#30340;&#22343;&#22330;&#19981;&#21160;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.04990</link><description>&lt;p&gt;
&#21333;&#35843;&#30340;&#28145;&#24230;Boltzmann&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Monotone deep Boltzmann machines. (arXiv:2307.04990v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04990
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38480;&#21046;&#27169;&#22411;&#65292;&#21363;&#21333;&#35843;DBM&#65292;&#23427;&#20801;&#35768;&#27599;&#19968;&#23618;&#20855;&#26377;&#20219;&#24847;&#30340;&#33258;&#36830;&#25509;&#65292;&#20294;&#36890;&#36807;&#19968;&#31181;&#26041;&#24335;&#38480;&#21046;&#20102;&#26435;&#37325;&#65292;&#20197;&#20445;&#35777;&#23384;&#22312;&#21644;&#20840;&#23616;&#21807;&#19968;&#30340;&#22343;&#22330;&#19981;&#21160;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;Boltzmann&#26426;&#22120;(DBMs)&#26159;&#26368;&#26089;&#30740;&#31350;&#30340;"&#28145;&#24230;"&#23398;&#20064;&#26041;&#27861;&#20043;&#19968;&#65292;&#23427;&#26159;&#30001;&#19968;&#20010;&#25551;&#36848;&#32593;&#32476;&#20013;&#25152;&#26377;&#21464;&#37327;/&#33410;&#28857;&#30340;&#21487;&#33021;&#24615;&#30340;&#25104;&#23545;&#33021;&#37327;&#20989;&#25968;&#25152;&#25511;&#21046;&#30340;&#22810;&#23618;&#27010;&#29575;&#27169;&#22411;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20026;&#20102;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25512;&#29702;&#65292;DBMs&#36890;&#24120;&#20250;&#21463;&#21040;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#36890;&#36807;"&#38480;&#21046;&#24615;" Boltzmann&#26426;&#22120;(RBM)&#26550;&#26500;&#65288;&#19981;&#20801;&#35768;&#23618;&#38388;&#36830;&#25509;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#36890;&#29992;&#30340;DBM&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26159;&#21542;&#23384;&#22312;&#20854;&#20182;&#21487;&#33021;&#30340;&#35774;&#35745;&#38480;&#21046;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#65288;&#36817;&#20284;&#65289;&#25512;&#29702;&#65311;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#38480;&#21046;&#27169;&#22411;&#65292;&#21363;&#21333;&#35843;DBM&#65292;&#23427;&#20801;&#35768;&#27599;&#19968;&#23618;&#20855;&#26377;&#20219;&#24847;&#30340;&#33258;&#36830;&#25509;&#65292;&#20294;&#36890;&#36807;&#19968;&#31181;&#26041;&#24335;&#38480;&#21046;&#20102;&#26435;&#37325;&#65292;&#20197;&#20445;&#35777;&#23384;&#22312;&#21644;&#20840;&#23616;&#21807;&#19968;&#30340;&#22343;&#22330;&#19981;&#21160;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#21333;&#35843;&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#30340;&#24037;&#20855;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Deep Boltzmann machines (DBMs), one of the first ``deep'' learning methods ever studied, are multi-layered probabilistic models governed by a pairwise energy function that describes the likelihood of all variables/nodes in the network. In practice, DBMs are often constrained, i.e., via the \emph{restricted} Boltzmann machine (RBM) architecture (which does not permit intra-layer connections), in order to allow for more efficient inference. In this work, we revisit the generic DBM approach, and ask the question: are there other possible restrictions to their design that would enable efficient (approximate) inference? In particular, we develop a new class of restricted model, the monotone DBM, which allows for arbitrary self-connection in each layer, but restricts the \emph{weights} in a manner that guarantees the existence and global uniqueness of a mean-field fixed point. To do this, we leverage tools from the recently-proposed monotone Deep Equilibrium model and show that a particular 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#20845;&#31181;&#22522;&#20934;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#21644;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#22522;&#20110; GFlowNets &#30340;&#26041;&#27861;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616; GFlowNets &#20855;&#26377;&#25429;&#25417;&#21508;&#31181;&#26377;&#29992;&#21644;&#22810;&#26679;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.04988</link><description>&lt;p&gt;
&#29992;&#20110;&#19979;&#28216;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Bayesian Causal Discovery Methods for Downstream Treatment Effect Estimation. (arXiv:2307.04988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04988
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#20845;&#31181;&#22522;&#20934;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#21644;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#22522;&#20110; GFlowNets &#30340;&#26041;&#27861;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616; GFlowNets &#20855;&#26377;&#25429;&#25417;&#21508;&#31181;&#26377;&#29992;&#21644;&#22810;&#26679;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#20013;&#22240;&#26524;&#24615;&#30340;&#23454;&#38469;&#24212;&#29992;&#34987;&#24191;&#27867;&#35748;&#21487;&#65292;&#22240;&#26524;&#21457;&#29616;&#21644;&#25512;&#29702;&#22312;&#26412;&#36136;&#19978;&#26159;&#30456;&#20114;&#20132;&#32455;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#35780;&#20272;&#20013;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#65292;&#23545;&#19979;&#28216;&#25512;&#29702;&#30340;&#37325;&#35270;&#31243;&#24230;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20845;&#31181;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110; GFlowNets &#30340;&#26032;&#26041;&#27861;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23454;&#26045;&#19968;&#20010;&#31283;&#20581;&#30340;&#35780;&#20272;&#36807;&#31243;&#65292;&#25105;&#20204;&#20026;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#36825;&#20123;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#32771;&#34385;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#22330;&#26223;&#20197;&#21450;&#20302;&#25968;&#25454;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GFlowNets &#20855;&#26377;&#26377;&#25928;&#25429;&#25417;&#21508;&#31181;&#26377;&#29992;&#21644;&#22810;&#26679;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The practical utility of causality in decision-making is widely recognized, with causal discovery and inference being inherently intertwined. Nevertheless, a notable gap exists in the evaluation of causal discovery methods, where insufficient emphasis is placed on downstream inference. To address this gap, we evaluate six established baseline causal discovery methods and a newly proposed method based on GFlowNets, on the downstream task of treatment effect estimation. Through the implementation of a robust evaluation procedure, we offer valuable insights into the efficacy of these causal discovery methods for treatment effect estimation, considering both synthetic and real-world scenarios, as well as low-data scenarios. Furthermore, the results of our study demonstrate that GFlowNets possess the capability to effectively capture a wide range of useful and diverse ATE modes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22870;&#21169;&#27169;&#22411;&#12289;PPO&#21644;&#36827;&#31243;&#30417;&#30563;&#31561;&#25216;&#26415;&#36335;&#24452;&#65292;&#25506;&#32034;&#22914;&#20309;&#35299;&#20915;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04964</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494; &#31532;&#19968;&#37096;&#20998;&#65306;PPO
&lt;/p&gt;
&lt;p&gt;
Secrets of RLHF in Large Language Models Part I: PPO. (arXiv:2307.04964v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22870;&#21169;&#27169;&#22411;&#12289;PPO&#21644;&#36827;&#31243;&#30417;&#30563;&#31561;&#25216;&#26415;&#36335;&#24452;&#65292;&#25506;&#32034;&#22914;&#20309;&#35299;&#20915;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#25512;&#21160;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#34013;&#22270;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#25104;&#20026;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#65288;&#26377;&#30410;&#12289;&#35802;&#23454;&#21644;&#26080;&#23475;&#65289;&#21161;&#25163;&#12290;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#24847;&#20041;&#65292;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#25104;&#20026;&#25903;&#25745;&#36825;&#19968;&#36861;&#27714;&#30340;&#20851;&#38190;&#25216;&#26415;&#33539;&#24335;&#12290;&#24403;&#21069;&#30340;&#25216;&#26415;&#36335;&#32447;&#36890;&#24120;&#21253;&#25324;&#29992;&#20110;&#34913;&#37327;&#20154;&#31867;&#20559;&#22909;&#30340;&#22870;&#21169;&#27169;&#22411;&#12289;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#27169;&#22411;&#36755;&#20986;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#20197;&#21450;&#29992;&#20110;&#25913;&#21892;&#36880;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#36827;&#31243;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22870;&#21169;&#35774;&#35745;&#12289;&#29615;&#22659;&#20132;&#20114;&#21644;&#20195;&#29702;&#35757;&#32451;&#30340;&#25361;&#25112;&#65292;&#20877;&#21152;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35797;&#39564;&#25104;&#26412;&#24040;&#22823;&#65292;&#23545;&#20110;AI&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#28608;&#21169;&#25216;&#26415;&#23545;&#40784;&#21644;LLMs&#30340;&#23433;&#20840;&#30528;&#38470;&#23384;&#22312;&#37325;&#22823;&#38556;&#30861;&#12290;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \textbf{reward models} to measure human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first re
&lt;/p&gt;</description></item><item><title>DyCL&#36890;&#36807;&#31243;&#24207;&#37325;&#20889;&#21644;&#22270;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;DL&#32534;&#35793;&#22120;&#22312;&#32534;&#35793;&#20855;&#26377;&#21160;&#24577;&#29305;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#25104;&#21151;&#32534;&#35793;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2307.04963</link><description>&lt;p&gt;
DyCL: &#36890;&#36807;&#31243;&#24207;&#37325;&#20889;&#21644;&#22270;&#20248;&#21270;&#23454;&#29616;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#32534;&#35793;
&lt;/p&gt;
&lt;p&gt;
DyCL: Dynamic Neural Network Compilation Via Program Rewriting and Graph Optimization. (arXiv:2307.04963v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04963
&lt;/p&gt;
&lt;p&gt;
DyCL&#36890;&#36807;&#31243;&#24207;&#37325;&#20889;&#21644;&#22270;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;DL&#32534;&#35793;&#22120;&#22312;&#32534;&#35793;&#20855;&#26377;&#21160;&#24577;&#29305;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#25104;&#21151;&#32534;&#35793;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DL&#32534;&#35793;&#22120;&#30340;&#20027;&#35201;&#21151;&#33021;&#26159;&#23558;&#20351;&#29992;&#39640;&#32423;DL&#26694;&#26550;&#65288;&#22914;PyTorch&#21644;TensorFlow&#65289;&#32534;&#20889;&#30340;DNN&#31243;&#24207;&#36716;&#25442;&#20026;&#21487;&#31227;&#26893;&#30340;&#21487;&#25191;&#34892;&#25991;&#20214;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DL&#32534;&#35793;&#22120;&#20381;&#36182;&#20110;&#36319;&#36394;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#28041;&#21450;&#21521;&#31070;&#32463;&#32593;&#32476;&#31243;&#24207;&#25552;&#20379;&#36816;&#34892;&#26102;&#36755;&#20837;&#65292;&#24182;&#36319;&#36394;&#31243;&#24207;&#25191;&#34892;&#36335;&#24452;&#20197;&#29983;&#25104;&#32534;&#35793;&#25152;&#38656;&#30340;&#35745;&#31639;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26426;&#21046;&#22312;&#22788;&#29702;&#20855;&#26377;&#26681;&#25454;&#36755;&#20837;&#21464;&#21270;&#30340;&#35745;&#31639;&#22270;&#30340;&#29616;&#20195;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#65288;DyNNs&#65289;&#26102;&#23384;&#22312;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;DL&#32534;&#35793;&#22120;&#22312;&#23558;DyNNs&#20934;&#30830;&#32534;&#35793;&#20026;&#21487;&#25191;&#34892;&#20195;&#30721;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\tool&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#20219;&#20309;&#29616;&#26377;&#30340;DL&#32534;&#35793;&#22120;&#25104;&#21151;&#32534;&#35793;DyNNs&#12290;\tool&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#32534;&#35793;&#26426;&#21046;&#26469;&#35299;&#20915;DyNNs&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#35813;&#26426;&#21046;&#37325;&#26032;&#20998;&#37197;&#21407;&#22987;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
DL compiler's primary function is to translate DNN programs written in high-level DL frameworks such as PyTorch and TensorFlow into portable executables. These executables can then be flexibly executed by the deployed host programs. However, existing DL compilers rely on a tracing mechanism, which involves feeding a runtime input to a neural network program and tracing the program execution paths to generate the computational graph necessary for compilation. Unfortunately, this mechanism falls short when dealing with modern dynamic neural networks (DyNNs) that possess varying computational graphs depending on the inputs. Consequently, conventional DL compilers struggle to accurately compile DyNNs into executable code. To address this limitation, we propose \tool, a general approach that enables any existing DL compiler to successfully compile DyNNs. \tool tackles the dynamic nature of DyNNs by introducing a compilation mechanism that redistributes the control and data flow of the origi
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25506;&#32034;&#12290;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36138;&#23146;&#35780;&#20272;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.04962</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#32593;&#32476;&#29702;&#35770;&#36827;&#34892;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04962
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25506;&#32034;&#12290;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36138;&#23146;&#35780;&#20272;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#22312;&#39537;&#21160;&#30340;&#25506;&#32034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#29992;&#36884;&#65292;&#21363;&#20351;&#27809;&#26377;&#39069;&#22806;&#30340;&#22806;&#22312;&#22870;&#21169;&#12290;&#24403;&#29615;&#22659;&#33258;&#28982;&#34920;&#31034;&#20026;&#22270;&#26102;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#24341;&#23548;&#25506;&#32034;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65306;&#20449;&#24687;&#24046;&#29702;&#35770;&#21644;&#21387;&#32553;&#36827;&#23637;&#29702;&#35770;&#65292;&#26469;&#28608;&#21169;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#25506;&#32034;&#12290;&#36825;&#20123;&#29702;&#35770;&#23558;&#22909;&#22855;&#24515;&#35270;&#20026;&#23545;&#29615;&#22659;&#20013;&#35775;&#38382;&#33410;&#28857;&#25152;&#24341;&#21457;&#30340;&#23376;&#22270;&#30340;&#25299;&#25169;&#29305;&#24449;&#36827;&#34892;&#20248;&#21270;&#30340;&#20869;&#22312;&#21160;&#26426;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25552;&#20986;&#30340;&#29305;&#24449;&#20316;&#20026;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#22870;&#21169;&#12290;&#22312;&#22810;&#20010;&#31867;&#21035;&#30340;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#20195;&#29702;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#21644;&#27604;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#38271;&#30340;&#25506;&#32034;&#24615;&#27493;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#20110;&#30456;&#20851;&#25299;&#25169;&#23646;&#24615;&#30340;&#36138;&#23146;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#20869;&#22312;&#21160;&#26426;&#20135;&#29983;&#30340;&#22870;&#21169;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#29983;&#25104;&#19978;&#25512;&#24191;&#33391;&#22909;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#33021;&#22815;&#22312;&#26356;&#22823;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intrinsically motivated exploration has proven useful for reinforcement learning, even without additional extrinsic rewards. When the environment is naturally represented as a graph, how to guide exploration best remains an open question. In this work, we propose a novel approach for exploring graph-structured data motivated by two theories of human curiosity: the information gap theory and the compression progress theory. The theories view curiosity as an intrinsic motivation to optimize for topological features of subgraphs induced by the visited nodes in the environment. We use these proposed features as rewards for graph neural-network-based reinforcement learning. On multiple classes of synthetically generated graphs, we find that trained agents generalize to larger environments and to longer exploratory walks than are seen during training. Our method computes more efficiently than the greedy evaluation of the relevant topological properties. The proposed intrinsic motivations bea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#26041;&#31243;&#20013;&#20351;&#29992;&#24191;&#20041;&#36816;&#31639;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38750;&#32047;&#31215;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.04957</link><description>&lt;p&gt;
&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Non-Cumulative Objective. (arXiv:2307.04957v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#26041;&#31243;&#20013;&#20351;&#29992;&#24191;&#20041;&#36816;&#31639;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38750;&#32047;&#31215;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30446;&#26631;&#20960;&#20046;&#24635;&#26159;&#23450;&#20041;&#20026;&#27839;&#36807;&#31243;&#20013;&#22870;&#21169;&#30340;\emph{&#32047;&#31215;}&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#36890;&#20449;&#21644;&#32593;&#32476;&#39046;&#22495;&#20013;&#65292;&#30446;&#26631;&#24182;&#19981;&#33258;&#28982;&#22320;&#34920;&#36798;&#20026;&#22870;&#21169;&#30340;&#27714;&#21644;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#21508;&#31181;&#38382;&#39064;&#20013;&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#20197;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#35768;&#22810;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#65306;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#26041;&#31243;&#12290;&#20026;&#20102;&#20248;&#21270;&#38750;&#32047;&#31215;&#30446;&#26631;&#65292;&#25105;&#20204;&#29992;&#19982;&#30446;&#26631;&#30456;&#23545;&#24212;&#30340;&#24191;&#20041;&#36816;&#31639;&#26367;&#25442;&#20102;&#36125;&#23572;&#26364;&#26356;&#26032;&#35268;&#21017;&#20013;&#30340;&#21407;&#22987;&#27714;&#21644;&#36816;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24191;&#20041;&#36816;&#31639;&#24418;&#24335;&#30340;&#36275;&#22815;&#26465;&#20214;&#20197;&#21450;&#23545;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning, the objective is almost always defined as a \emph{cumulative} function over the rewards along the process. However, there are many optimal control and reinforcement learning problems in various application fields, especially in communications and networking, where the objectives are not naturally expressed as summations of the rewards. In this paper, we recognize the prevalence of non-cumulative objectives in various problems, and propose a modification to existing algorithms for optimizing such objectives. Specifically, we dive into the fundamental building block for many optimal control and reinforcement learning algorithms: the Bellman optimality equation. To optimize a non-cumulative objective, we replace the original summation operation in the Bellman update rule with a generalized operation corresponding to the objective. Furthermore, we provide sufficient conditions on the form of the generalized operation as well as assumptions on the Markov decision 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#38544;&#39532;&#23572;&#21487;&#22827;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#30701;&#26399;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20132;&#36890;&#21464;&#37327;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#21442;&#25968;&#27169;&#22411;&#12290;&#36825;&#31181;&#27169;&#22411;&#32467;&#21512;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#25429;&#25417;&#20132;&#36890;&#31995;&#32479;&#30340;&#22797;&#26434;&#21160;&#24577;&#27169;&#24335;&#21644;&#38750;&#24179;&#31283;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04954</link><description>&lt;p&gt;
&#28151;&#21512;&#38544;&#39532;&#23572;&#21487;&#22827;LSTM&#29992;&#20110;&#30701;&#26399;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hybrid hidden Markov LSTM for short-term traffic flow prediction. (arXiv:2307.04954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#38544;&#39532;&#23572;&#21487;&#22827;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#30701;&#26399;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20132;&#36890;&#21464;&#37327;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#21442;&#25968;&#27169;&#22411;&#12290;&#36825;&#31181;&#27169;&#22411;&#32467;&#21512;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#25429;&#25417;&#20132;&#36890;&#31995;&#32479;&#30340;&#22797;&#26434;&#21160;&#24577;&#27169;&#24335;&#21644;&#38750;&#24179;&#31283;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20132;&#36890;&#21464;&#37327;&#30340;&#30701;&#26399;&#21644;&#36817;&#30701;&#26399;&#26410;&#26469;&#26041;&#38754;&#24050;&#32463;&#20248;&#20110;&#21442;&#25968;&#27169;&#22411;&#65292;&#22914;&#21382;&#21490;&#24179;&#22343;&#12289;ARIMA&#21644;&#20854;&#21464;&#20307;&#65292;&#36825;&#23545;&#20110;&#20132;&#36890;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21450;&#20854;&#21464;&#20307;&#65288;&#20363;&#22914;&#38271;&#30701;&#26399;&#35760;&#24518;&#65289;&#34987;&#35774;&#35745;&#29992;&#20110;&#20445;&#30041;&#38271;&#26399;&#26102;&#24207;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#29992;&#20110;&#24314;&#27169;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#22810;&#21046;&#24230;&#27169;&#22411;&#20551;&#35774;&#20132;&#36890;&#31995;&#32479;&#20197;&#19981;&#21516;&#29305;&#24449;&#30340;&#22810;&#20010;&#29366;&#24577;&#65288;&#20363;&#22914;&#30021;&#36890;&#12289;&#25317;&#22581;&#65289;&#28436;&#21464;&#65292;&#22240;&#27492;&#38656;&#35201;&#35757;&#32451;&#19981;&#21516;&#27169;&#22411;&#20197;&#34920;&#24449;&#27599;&#20010;&#21046;&#24230;&#20869;&#30340;&#20132;&#36890;&#21160;&#24577;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#36827;&#34892;&#21046;&#24230;&#35782;&#21035;&#30340;&#39532;&#23572;&#21487;&#22827;&#20999;&#25442;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#21160;&#24577;&#27169;&#24335;&#21644;&#38750;&#24179;&#31283;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;LSTM&#37117;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20174;&#19968;&#32452;&#28508;&#22312;&#30340;&#25110;&#38544;&#34255;&#29366;&#24577;&#21464;&#37327;&#20013;&#30340;&#35266;&#23519;&#24207;&#21015;&#12290;&#22312;LSTM&#20013;&#65292;&#28508;&#22312;&#21464;&#37327;&#21487;&#20197;&#20174;&#19978;&#19968;&#20010;&#26102;&#38388;&#27493;&#30340;&#38544;&#34255;&#29366;&#24577;&#21464;&#37327;&#20256;&#36882;&#36807;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) methods have outperformed parametric models such as historical average, ARIMA and variants in predicting traffic variables into short and near-short future, that are critical for traffic management. Specifically, recurrent neural network (RNN) and its variants (e.g. long short-term memory) are designed to retain long-term temporal correlations and therefore are suitable for modeling sequences. However, multi-regime models assume the traffic system to evolve through multiple states (say, free-flow, congestion in traffic) with distinct characteristics, and hence, separate models are trained to characterize the traffic dynamics within each regime. For instance, Markov-switching models with a hidden Markov model (HMM) for regime identification is capable of capturing complex dynamic patterns and non-stationarity. Interestingly, both HMM and LSTM can be used for modeling an observation sequence from a set of latent or, hidden state variables. In LSTM, the latent variable 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#22411;&#21452;&#37325;&#34701;&#21512;&#32593;&#32476;&#65288;CTFN&#65289;&#65292;&#29992;&#20110;&#36793;&#32536;&#26816;&#27979;&#65292;&#22312;&#20445;&#25345;&#27169;&#22411;&#32039;&#20945;&#24615;&#30340;&#21516;&#26102;&#23436;&#20840;&#25972;&#21512;&#22810;&#23610;&#24230;&#29305;&#24449;&#12290;&#20854;&#20013;&#21253;&#25324;&#35821;&#20041;&#22686;&#24378;&#27169;&#22359;&#21644;&#20266;&#20687;&#32032;&#32423;&#21152;&#26435;&#27169;&#22359;&#65292;&#36824;&#20351;&#29992;&#21160;&#24577;&#28966;&#28857;&#25439;&#22833;&#20989;&#25968;&#22788;&#29702;&#32441;&#29702;&#22122;&#22768;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.04952</link><description>&lt;p&gt;
&#32039;&#20945;&#22411;&#21452;&#37325;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#36793;&#32536;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Compact Twice Fusion Network for Edge Detection. (arXiv:2307.04952v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#22411;&#21452;&#37325;&#34701;&#21512;&#32593;&#32476;&#65288;CTFN&#65289;&#65292;&#29992;&#20110;&#36793;&#32536;&#26816;&#27979;&#65292;&#22312;&#20445;&#25345;&#27169;&#22411;&#32039;&#20945;&#24615;&#30340;&#21516;&#26102;&#23436;&#20840;&#25972;&#21512;&#22810;&#23610;&#24230;&#29305;&#24449;&#12290;&#20854;&#20013;&#21253;&#25324;&#35821;&#20041;&#22686;&#24378;&#27169;&#22359;&#21644;&#20266;&#20687;&#32032;&#32423;&#21152;&#26435;&#27169;&#22359;&#65292;&#36824;&#20351;&#29992;&#21160;&#24577;&#28966;&#28857;&#25439;&#22833;&#20989;&#25968;&#22788;&#29702;&#32441;&#29702;&#22122;&#22768;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#26816;&#27979;&#31038;&#21306;&#36880;&#28176;&#35748;&#35782;&#21040;&#22810;&#23610;&#24230;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#22810;&#23610;&#24230;&#29305;&#24449;&#30340;&#34701;&#21512;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#23545;&#23454;&#38469;&#24212;&#29992;&#19981;&#21451;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#22411;&#21452;&#37325;&#34701;&#21512;&#32593;&#32476;&#65288;CTFN&#65289;&#65292;&#20197;&#23436;&#20840;&#25972;&#21512;&#22810;&#23610;&#24230;&#29305;&#24449;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#32039;&#20945;&#24615;&#12290;CTFN&#21253;&#25324;&#20004;&#20010;&#36731;&#37327;&#32423;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65306;&#35821;&#20041;&#22686;&#24378;&#27169;&#22359;&#65288;SEM&#65289;&#65292;&#21487;&#20197;&#21033;&#29992;&#31895;&#23610;&#24230;&#29305;&#24449;&#20013;&#21253;&#21547;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#24341;&#23548;&#32454;&#23610;&#24230;&#29305;&#24449;&#30340;&#23398;&#20064;&#65307;&#20266;&#20687;&#32032;&#32423;&#21152;&#26435;&#65288;PPW&#65289;&#27169;&#22359;&#65292;&#36890;&#36807;&#32473;&#25152;&#26377;&#29305;&#24449;&#20998;&#37197;&#26435;&#37325;&#26469;&#32858;&#21512;&#22810;&#23610;&#24230;&#29305;&#24449;&#30340;&#20114;&#34917;&#20248;&#28857;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#32441;&#29702;&#22122;&#22768;&#30340;&#24178;&#25200;&#20173;&#28982;&#20351;&#24471;&#19968;&#20123;&#20687;&#32032;&#30340;&#27491;&#30830;&#20998;&#31867;&#25104;&#20026;&#25361;&#25112;&#12290;&#23545;&#20110;&#36825;&#20123;&#38590;&#26679;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#21160;&#24577;&#28966;&#28857;&#25439;&#22833;&#65292;&#23427;&#37325;&#26032;&#22609;&#36896;&#20102;&#26631;&#20934;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The significance of multi-scale features has been gradually recognized by the edge detection community. However, the fusion of multi-scale features increases the complexity of the model, which is not friendly to practical application. In this work, we propose a Compact Twice Fusion Network (CTFN) to fully integrate multi-scale features while maintaining the compactness of the model. CTFN includes two lightweight multi-scale feature fusion modules: a Semantic Enhancement Module (SEM) that can utilize the semantic information contained in coarse-scale features to guide the learning of fine-scale features, and a Pseudo Pixel-level Weighting (PPW) module that aggregate the complementary merits of multi-scale features by assigning weights to all features. Notwithstanding all this, the interference of texture noise makes the correct classification of some pixels still a challenge. For these hard samples, we propose a novel loss function, coined Dynamic Focal Loss, which reshapes the standard
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#21435;&#22122;&#26799;&#24230;&#21152;&#26435;&#26368;&#23567;&#21270;&#27714;&#35299;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#20256;&#32479;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#19982;&#21435;&#22122;&#30456;&#32467;&#21512;&#65292;&#27599;&#19968;&#27493;&#37117;&#28155;&#21152;&#22122;&#22768;&#65292;&#20351;&#24471;&#36845;&#20195;&#21160;&#21147;&#23398;&#31867;&#20284;&#20110;Langevin&#25110;&#25193;&#25955;&#36807;&#31243;&#12290;&#36890;&#36807;&#27169;&#25311;&#20542;&#26012;&#35270;&#22270;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;50&#20010;&#20542;&#26012;&#35270;&#22270;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#39640;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.04946</link><description>&lt;p&gt;
DDGM: &#36890;&#36807;&#25193;&#25955;&#21435;&#22122;&#26799;&#24230;&#21152;&#26435;&#26368;&#23567;&#21270;&#27714;&#35299;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
DDGM: Solving inverse problems by Diffusive Denoising of Gradient-based Minimization. (arXiv:2307.04946v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#21435;&#22122;&#26799;&#24230;&#21152;&#26435;&#26368;&#23567;&#21270;&#27714;&#35299;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#20256;&#32479;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#19982;&#21435;&#22122;&#30456;&#32467;&#21512;&#65292;&#27599;&#19968;&#27493;&#37117;&#28155;&#21152;&#22122;&#22768;&#65292;&#20351;&#24471;&#36845;&#20195;&#21160;&#21147;&#23398;&#31867;&#20284;&#20110;Langevin&#25110;&#25193;&#25955;&#36807;&#31243;&#12290;&#36890;&#36807;&#27169;&#25311;&#20542;&#26012;&#35270;&#22270;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;50&#20010;&#20542;&#26012;&#35270;&#22270;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#38382;&#39064;&#19968;&#33324;&#38656;&#35201;&#19968;&#20010;&#27491;&#21017;&#21270;&#22120;&#25110;&#20808;&#39564;&#26469;&#24471;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#36817;&#19968;&#31181;&#36235;&#21183;&#26159;&#36890;&#36807;&#35757;&#32451;&#21367;&#31215;&#32593;&#32476;&#21435;&#38477;&#22122;&#22270;&#20687;&#65292;&#24182;&#22312;&#27714;&#35299;&#36870;&#38382;&#39064;&#26102;&#23558;&#36825;&#20010;&#32593;&#32476;&#20316;&#20026;&#20808;&#39564;&#12290;&#19968;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#27491;&#21521;&#31639;&#23376;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#22312;&#36816;&#34892;&#26102;&#36890;&#36807;&#21435;&#22122;&#32593;&#32476;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23558;&#20256;&#32479;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#19982;&#21435;&#22122;&#30456;&#32467;&#21512;&#12290;&#27599;&#19968;&#27493;&#37117;&#28155;&#21152;&#22122;&#22768;&#65292;&#20351;&#24471;&#36845;&#20195;&#21160;&#21147;&#23398;&#31867;&#20284;&#20110;Langevin&#25110;&#25193;&#25955;&#36807;&#31243;&#12290;&#28155;&#21152;&#30340;&#22122;&#22768;&#27700;&#24179;&#21644;&#21435;&#22122;&#27493;&#39588;&#30340;&#22823;&#23567;&#37117;&#20250;&#38543;&#26102;&#38388;&#25351;&#25968;&#34928;&#20943;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20174;&#22810;&#20010;&#20542;&#35282;&#33719;&#21462;&#30340;&#30005;&#23376;&#26174;&#24494;&#22270;&#30340;&#23618;&#26512;&#37325;&#24314;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#20542;&#26012;&#35270;&#22270;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#36866;&#29992;&#20110;&#25105;&#20204;&#26041;&#27861;&#30340;&#21442;&#25968;&#35774;&#32622;&#65292;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#38656;50&#20010;&#20542;&#26012;&#35270;&#22270;&#23601;&#21487;&#20197;&#36798;&#21040;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems generally require a regularizer or prior for a good solution. A recent trend is to train a convolutional net to denoise images, and use this net as a prior when solving the inverse problem. Several proposals depend on a singular value decomposition of the forward operator, and several others backpropagate through the denoising net at runtime. Here we propose a simpler approach that combines the traditional gradient-based minimization of reconstruction error with denoising. Noise is also added at each step, so the iterative dynamics resembles a Langevin or diffusion process. Both the level of added noise and the size of the denoising step decay exponentially with time. We apply our method to the problem of tomographic reconstruction from electron micrographs acquired at multiple tilt angles. With empirical studies using simulated tilt views, we find parameter settings for our method that produce good results. We show that high accuracy can be achieved with as few as 50 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;13&#31181;&#32852;&#37030;DG&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#37327;&#23458;&#25143;&#31471;&#21644;&#39640;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;DG&#20173;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2307.04942</link><description>&lt;p&gt;
&#22522;&#20934;&#27979;&#35797;&#38024;&#23545;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Algorithms for Federated Domain Generalization. (arXiv:2307.04942v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;13&#31181;&#32852;&#37030;DG&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#37327;&#23458;&#25143;&#31471;&#21644;&#39640;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;DG&#20173;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20808;&#21069;&#30340;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#22522;&#20934;&#32771;&#34385;&#20102;&#35757;&#32451;-&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24341;&#20837;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#29305;&#23450;&#25361;&#25112;&#30340;&#32852;&#37030;DG&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#23458;&#25143;&#31471;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#25506;&#32034;&#22522;&#20110;&#39046;&#22495;&#30340;&#24322;&#36136;&#24615;-&#19968;&#20010;&#29616;&#23454;&#30340;&#32852;&#37030;DG&#22330;&#26223;&#12290;&#20808;&#21069;&#30340;&#32852;&#37030;DG&#35780;&#20272;&#22312;&#23458;&#25143;&#31471;&#25968;&#37327;&#25110;&#24322;&#36136;&#24615;&#20197;&#21450;&#25968;&#25454;&#38598;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;DG&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#23458;&#25143;&#31471;&#30340;&#25968;&#37327;&#21644;&#24322;&#36136;&#24615;&#65292;&#24182;&#25552;&#20379;&#25968;&#25454;&#38598;&#38590;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;13&#31181;&#32852;&#37030;DG&#26041;&#27861;&#65292;&#21253;&#25324;&#36866;&#24212;FL&#29615;&#22659;&#30340;&#38598;&#20013;DG&#26041;&#27861;&#12289;&#22788;&#29702;&#23458;&#25143;&#31471;&#24322;&#36136;&#24615;&#30340;FL&#26041;&#27861;&#20197;&#21450;&#19987;&#20026;&#32852;&#37030;DG&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#22823;&#37327;&#23458;&#25143;&#31471;&#21644;&#39640;&#23458;&#25143;&#31471;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;DG&#20173;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
While prior domain generalization (DG) benchmarks consider train-test dataset heterogeneity, we evaluate Federated DG which introduces federated learning (FL) specific challenges. Additionally, we explore domain-based heterogeneity in clients' local datasets - a realistic Federated DG scenario. Prior Federated DG evaluations are limited in terms of the number or heterogeneity of clients and dataset diversity. To address this gap, we propose an Federated DG benchmark methodology that enables control of the number and heterogeneity of clients and provides metrics for dataset difficulty. We then apply our methodology to evaluate 13 Federated DG methods, which include centralized DG methods adapted to the FL context, FL methods that handle client heterogeneity, and methods designed specifically for Federated DG. Our results suggest that despite some progress, there remain significant performance gaps in Federated DG particularly when evaluating with a large number of clients, high client h
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20197;&#22240;&#26524;&#35270;&#35282;&#30475;&#24453;&#20844;&#24179;&#22270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26694;&#26550;CAF&#65292;&#36890;&#36807;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#21453;&#20107;&#23454;&#26469;&#36991;&#20813;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2307.04937</link><description>&lt;p&gt;
&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#20844;&#24179;&#24615;&#65306;&#19968;&#31181;&#22270;&#21453;&#20107;&#23454;&#35282;&#24230;
&lt;/p&gt;
&lt;p&gt;
Improving Fairness of Graph Neural Networks: A Graph Counterfactual Perspective. (arXiv:2307.04937v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04937
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20197;&#22240;&#26524;&#35270;&#35282;&#30475;&#24453;&#20844;&#24179;&#22270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26694;&#26550;CAF&#65292;&#36890;&#36807;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#21453;&#20107;&#23454;&#26469;&#36991;&#20813;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#21508;&#31181;&#20219;&#21153;&#30340;&#36827;&#34892;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#24314;&#27169;&#22270;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;GNN&#20542;&#21521;&#20110;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#32487;&#25215;&#21644;&#25918;&#22823;&#20559;&#35265;&#65292;&#24341;&#36215;&#20102;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#20013;&#20351;&#29992;GNN&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#35768;&#22810;&#21162;&#21147;&#26469;&#23454;&#29616;&#20844;&#24179;&#24863;&#30693;&#30340;GNN&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20844;&#24179;GNN&#36890;&#36807;&#37319;&#29992;&#32479;&#35745;&#20844;&#24179;&#27010;&#24565;&#26469;&#23398;&#20064;&#20844;&#24179;&#33410;&#28857;&#34920;&#31034;&#65292;&#20294;&#22312;&#32479;&#35745;&#24322;&#24120;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20943;&#36731;&#20559;&#35265;&#12290;&#21463;&#22240;&#26524;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#26377;&#20960;&#31181;&#26041;&#27861;&#21033;&#29992;&#22270;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#26469;&#20943;&#36731;&#19981;&#20844;&#24179;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20250;&#21463;&#21040;&#36890;&#36807;&#25200;&#21160;&#25110;&#29983;&#25104;&#33719;&#24471;&#30340;&#38750;&#29616;&#23454;&#21453;&#20107;&#23454;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#22240;&#26524;&#35270;&#35282;&#30475;&#24453;&#20844;&#24179;&#22270;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#22240;&#26524;&#20998;&#26512;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;CAF&#65292;&#23427;&#21487;&#20197;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#36873;&#25321;&#21453;&#20107;&#23454;&#20197;&#36991;&#20813;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks have shown great ability in representation (GNNs) learning on graphs, facilitating various tasks. Despite their great performance in modeling graphs, recent works show that GNNs tend to inherit and amplify the bias from training data, causing concerns of the adoption of GNNs in high-stake scenarios. Hence, many efforts have been taken for fairness-aware GNNs. However, most existing fair GNNs learn fair node representations by adopting statistical fairness notions, which may fail to alleviate bias in the presence of statistical anomalies. Motivated by causal theory, there are several attempts utilizing graph counterfactual fairness to mitigate root causes of unfairness. However, these methods suffer from non-realistic counterfactuals obtained by perturbation or generation. In this paper, we take a causal view on fair graph learning problem. Guided by the casual analysis, we propose a novel framework CAF, which can select counterfactuals from training data to avoid 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#35757;&#32451;&#20013;&#30340;&#21453;&#20363;&#26469;&#35299;&#20915;&#23433;&#20840;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#23558;&#36830;&#32493;&#21644;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#31995;&#32479;&#25277;&#35937;&#20026;&#32039;&#20945;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#27010;&#29575;&#24615;&#21453;&#20363;&#29983;&#25104;&#26500;&#24314;&#26368;&#23567;&#21270;&#20223;&#30495;&#23376;&#27169;&#22411;&#65292;&#20197;&#25581;&#31034;&#23433;&#20840;&#38656;&#27714;&#30340;&#36829;&#21453;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.04927</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#29575;&#24615;&#21453;&#20363;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Counterexample Guidance for Safer Reinforcement Learning. (arXiv:2307.04927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#35757;&#32451;&#20013;&#30340;&#21453;&#20363;&#26469;&#35299;&#20915;&#23433;&#20840;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#23558;&#36830;&#32493;&#21644;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#31995;&#32479;&#25277;&#35937;&#20026;&#32039;&#20945;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#27010;&#29575;&#24615;&#21453;&#20363;&#29983;&#25104;&#26500;&#24314;&#26368;&#23567;&#21270;&#20223;&#30495;&#23376;&#27169;&#22411;&#65292;&#20197;&#25581;&#31034;&#23433;&#20840;&#38656;&#27714;&#30340;&#36829;&#21453;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#25506;&#32034;&#26088;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#20854;&#20013;&#22312;&#35797;&#38169;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#22833;&#36133;&#21487;&#33021;&#20250;&#23548;&#33268;&#39640;&#25104;&#26412;&#12290;&#23384;&#22312;&#22810;&#31181;&#26041;&#27861;&#26469;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#25110;&#20351;&#29992;&#36817;&#36317;&#31163;&#20256;&#24863;&#22120;&#25968;&#25454;&#26469;&#38480;&#21046;&#23545;&#19981;&#23433;&#20840;&#29366;&#24577;&#30340;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#20943;&#23569;&#25506;&#32034;&#39118;&#38505;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20195;&#29702;&#24517;&#39035;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#21457;&#29616;&#23433;&#20840;&#23041;&#32961;&#12290;&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#23433;&#20840;&#38656;&#27714;&#30340;&#21453;&#20363;&#24341;&#23548;&#35757;&#32451;&#26469;&#35299;&#20915;&#23433;&#20840;&#25506;&#32034;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#36830;&#32493;&#21644;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#31995;&#32479;&#25277;&#35937;&#20026;&#32039;&#20945;&#30340;&#25277;&#35937;&#27169;&#22411;&#65292;&#20195;&#34920;&#20195;&#29702;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#27010;&#29575;&#24615;&#21453;&#20363;&#29983;&#25104;&#26500;&#24314;&#26368;&#23567;&#21270;&#20223;&#30495;&#23376;&#27169;&#22411;&#65292;&#20197;&#25581;&#31034;&#23433;&#20840;&#38656;&#27714;&#30340;&#36829;&#21453;&#24773;&#20917;&#65292;&#20195;&#29702;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#20197;&#20248;&#21270;&#20854;&#31574;&#30053;&#65292;&#20943;&#23567;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe exploration aims at addressing the limitations of Reinforcement Learning (RL) in safety-critical scenarios, where failures during trial-and-error learning may incur high costs. Several methods exist to incorporate external knowledge or to use proximal sensor data to limit the exploration of unsafe states. However, reducing exploration risks in unknown environments, where an agent must discover safety threats during exploration, remains challenging. In this paper, we target the problem of safe exploration by guiding the training with counterexamples of the safety requirement. Our method abstracts both continuous and discrete state-space systems into compact abstract models representing the safety-relevant knowledge acquired by the agent during exploration. We then exploit probabilistic counterexample generation to construct minimal simulation submodels eliciting safety requirement violations, where the agent can efficiently train offline to refine its policy towards minimising the 
&lt;/p&gt;</description></item><item><title>SimpleMTOD&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22810;&#27169;&#24577;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#23376;&#20219;&#21153;&#36716;&#21270;&#20026;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30340;&#23545;&#35937;&#26631;&#35760;&#26469;&#25429;&#25417;&#35270;&#35273;&#22330;&#26223;&#30340;&#35821;&#20041;&#12290;&#23427;&#22312;SIMMC 2.0&#27979;&#35797;&#38598;&#30340;&#22238;&#24212;&#29983;&#25104;&#23376;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;BLEU&#20998;&#25968;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#22810;&#27169;&#24577;&#23376;&#20219;&#21153;&#20013;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.04907</link><description>&lt;p&gt;
SimpleMTOD: &#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#21270;&#22330;&#26223;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#31616;&#26131;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SimpleMTOD: A Simple Language Model for Multimodal Task-Oriented Dialogue with Symbolic Scene Representation. (arXiv:2307.04907v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04907
&lt;/p&gt;
&lt;p&gt;
SimpleMTOD&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22810;&#27169;&#24577;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#23376;&#20219;&#21153;&#36716;&#21270;&#20026;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30340;&#23545;&#35937;&#26631;&#35760;&#26469;&#25429;&#25417;&#35270;&#35273;&#22330;&#26223;&#30340;&#35821;&#20041;&#12290;&#23427;&#22312;SIMMC 2.0&#27979;&#35797;&#38598;&#30340;&#22238;&#24212;&#29983;&#25104;&#23376;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;BLEU&#20998;&#25968;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#22810;&#27169;&#24577;&#23376;&#20219;&#21153;&#20013;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SimpleMTOD&#26159;&#19968;&#20010;&#31616;&#26131;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22810;&#27169;&#24577;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#20960;&#20010;&#23376;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;SimpleMTOD&#22522;&#20110;&#22823;&#35268;&#27169;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#33258;&#22238;&#24402;&#26550;&#26500;&#26500;&#24314;&#32780;&#25104;&#65292;&#35813;&#26550;&#26500;&#24050;&#32463;&#22312;&#21333;&#27169;&#24577;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#19988;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;GPT-2&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#20026;&#20102;&#25429;&#25417;&#35270;&#35273;&#22330;&#26223;&#30340;&#35821;&#20041;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30340;&#23545;&#35937;&#26631;&#35760;&#12290;&#38750;&#23616;&#37096;&#30340;&#23545;&#35937;&#26631;&#35760;&#34920;&#31034;&#23545;&#35937;&#30340;&#31867;&#22411;&#32780;&#19981;&#26159;&#20855;&#20307;&#30340;&#23545;&#35937;&#26412;&#36523;&#65292;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#19968;&#33268;&#30340;&#21547;&#20041;&#12290;SimpleMTOD&#22312;SIMMC 2.0&#27979;&#35797;&#38598;&#20013;&#30340;&#22238;&#24212;&#29983;&#25104;&#23376;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;BLEU&#20998;&#25968;&#65288;0.327&#65289;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#22810;&#27169;&#24577;&#23376;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65306;&#28040;&#27495;&#12289;&#25351;&#20195;&#28040;&#35299;&#21644;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#12290;&#23613;&#31649;&#37319;&#21462;&#20102;&#26497;&#31616;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#35270;&#35273;&#65288;&#21644;&#38750;&#35270;&#35273;&#65289;&#20449;&#24687;&#65292;&#20294;SimpleMTOD&#20173;&#28982;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
SimpleMTOD is a simple language model which recasts several sub-tasks in multimodal task-oriented dialogues as sequence prediction tasks. SimpleMTOD is built on a large-scale transformer-based auto-regressive architecture, which has already proven to be successful in uni-modal task-oriented dialogues, and effectively leverages transfer learning from pre-trained GPT-2. In-order to capture the semantics of visual scenes, we introduce both local and de-localized tokens for objects within a scene. De-localized tokens represent the type of an object rather than the specific object itself and so possess a consistent meaning across the dataset. SimpleMTOD achieves a state-of-the-art BLEU score (0.327) in the Response Generation sub-task of the SIMMC 2.0 test-std dataset while performing on par in other multimodal sub-tasks: Disambiguation, Coreference Resolution, and Dialog State Tracking. This is despite taking a minimalist approach for extracting visual (and non-visual) information. In addi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#26469;&#23454;&#29616;&#22312;&#35774;&#22791;&#19978;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#27169;&#22359;&#21270;&#30340;&#20316;&#29992;&#65292;&#24182;&#35777;&#26126;&#20102;&#35268;&#27169;&#36739;&#22823;&#21487;&#20197;&#25552;&#39640;&#24322;&#26500;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04905</link><description>&lt;p&gt;
FedYolo: &#20351;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedYolo: Augmenting Federated Learning with Pretrained Transformers. (arXiv:2307.04905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#26469;&#23454;&#29616;&#22312;&#35774;&#22791;&#19978;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#27169;&#22359;&#21270;&#30340;&#20316;&#29992;&#65292;&#24182;&#35777;&#26126;&#20102;&#35268;&#27169;&#36739;&#22823;&#21487;&#20197;&#25552;&#39640;&#24322;&#26500;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#22686;&#38271;&#21644;&#22810;&#26679;&#24615;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20351;&#29992;&#31227;&#21160;&#21644;&#36793;&#32536;&#35774;&#22791;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#22914;&#20309;&#35299;&#20915;&#19981;&#21516;&#23458;&#25143;&#30446;&#26631;&#21644;&#31232;&#32570;&#24322;&#26500;&#25968;&#25454;&#30340;&#23398;&#20064;&#38382;&#39064;&#65311;&#34429;&#28982;&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#23427;&#38754;&#20020;&#30528;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#30340;&#25361;&#25112;&#12290;&#22823;&#22411;Transformer&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#24037;&#20316;&#65292;&#24182;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#23569;&#26679;&#26412;&#36866;&#24212;&#33021;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#23458;&#25143;&#33021;&#21542;&#20351;&#29992;&#21333;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#21019;&#24314;&#23450;&#21046;&#27169;&#22411;&#65292;&#24182;&#36981;&#23432;&#35774;&#22791;&#21644;&#32593;&#32476;&#30340;&#38480;&#21046;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#20123;&#35774;&#22791;&#19978;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#27169;&#22411;&#22823;&#23567;&#21644;&#27169;&#22359;&#21270;&#30340;&#20316;&#29992;&#65292;&#20854;&#20013;&#27169;&#22359;&#21270;&#25351;&#30340;&#26159;&#36890;&#36807;&#25552;&#31034;&#25110;&#36866;&#37197;&#22120;&#31561;&#27169;&#22359;&#36827;&#34892;&#36866;&#24212;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65306;&#65288;1&#65289;&#35268;&#27169;&#36739;&#22823;&#21487;&#20197;&#32553;&#23567;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#24046;&#36317;&#65292;&#24182;&#25552;&#39640;&#24322;&#26500;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growth and diversity of machine learning applications motivate a rethinking of learning with mobile and edge devices. How can we address diverse client goals and learn with scarce heterogeneous data? While federated learning aims to address these issues, it has challenges hindering a unified solution. Large transformer models have been shown to work across a variety of tasks achieving remarkable few-shot adaptation. This raises the question: Can clients use a single general-purpose model, rather than custom models for each task, while obeying device and network constraints? In this work, we investigate pretrained transformers (PTF) to achieve these on-device learning goals and thoroughly explore the roles of model size and modularity, where the latter refers to adaptation through modules such as prompts or adapters. Focusing on federated learning, we demonstrate that: (1) Larger scale shrinks the accuracy gaps between alternative approaches and improves heterogeneity robustness. Sc
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#35777;&#20840;&#23616;&#26368;&#20248;&#24615;&#19981;&#26159;&#24517;&#38656;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#21644;k-medoids&#32858;&#31867;&#26469;&#22686;&#21152;&#36895;&#24230;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#20219;&#21153;&#32423;&#24182;&#34892;&#21270;&#65292;&#36890;&#36807;&#27979;&#35797;&#21457;&#29616;&#27604;&#20854;&#20182;&#36873;&#39033;&#24555;33%&#65292;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36895;&#24230;&#25552;&#39640;&#21040;64%&#12290;</title><link>http://arxiv.org/abs/2307.04904</link><description>&lt;p&gt;
&#24555;&#36895;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#21644;&#32858;&#31867;&#22312;C++&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fast dynamic time warping and clustering in C++. (arXiv:2307.04904v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04904
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#35777;&#20840;&#23616;&#26368;&#20248;&#24615;&#19981;&#26159;&#24517;&#38656;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#21644;k-medoids&#32858;&#31867;&#26469;&#22686;&#21152;&#36895;&#24230;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#20219;&#21153;&#32423;&#24182;&#34892;&#21270;&#65292;&#36890;&#36807;&#27979;&#35797;&#21457;&#29616;&#27604;&#20854;&#20182;&#36873;&#39033;&#24555;33%&#65292;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36895;&#24230;&#25552;&#39640;&#21040;64%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32858;&#31867;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21160;&#24577;&#35268;&#25972;&#38382;&#39064;&#35270;&#20026;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#27714;&#35299;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#28982;&#21518;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#27714;&#35299;&#31532;&#20108;&#20010;&#20248;&#21270;&#38382;&#39064;&#26469;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32858;&#31867;&#12290;&#24403;&#20840;&#23616;&#26368;&#20248;&#24615;&#30340;&#35777;&#26126;&#19981;&#26159;&#24517;&#38656;&#26102;&#65292;&#36824;&#21487;&#20197;&#20351;&#29992;k-medoids&#32858;&#31867;&#26469;&#22686;&#21152;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#29575;&#28304;&#20110;&#32858;&#31867;&#21644;DTW&#30340;&#20219;&#21153;&#32423;&#24182;&#34892;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;UCR&#26102;&#38388;&#24207;&#21015;&#23384;&#26723;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21457;&#29616;&#24403;&#20351;&#29992;&#30456;&#21516;&#30340;&#32858;&#31867;&#26041;&#27861;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#36895;&#24230;&#24179;&#22343;&#27604;&#19979;&#19968;&#20010;&#26368;&#24555;&#30340;&#36873;&#39033;&#24555;33%&#12290;&#22312;&#32771;&#34385;&#21482;&#26377;&#36739;&#22823;&#25968;&#25454;&#38598;&#65288;&#36229;&#36807;1000&#20010;&#26102;&#38388;&#24207;&#21015;&#65289;&#26102;&#65292;&#36895;&#24230;&#25552;&#39640;&#21040;64%&#12290;&#30001;&#20110;DTW&#35745;&#31639;&#36895;&#24230;&#36739;&#24555;&#65292;MIP&#32858;&#31867;&#22312;&#23569;&#37327;&#36739;&#38271;&#26102;&#38388;&#24207;&#21015;&#19978;&#25928;&#26524;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an approach for computationally efficient dynamic time warping (DTW) and clustering of time-series data. The method frames the dynamic warping of time series datasets as an optimisation problem solved using dynamic programming, and then clusters time series data by solving a second optimisation problem using mixed-integer programming (MIP). There is also an option to use k-medoids clustering for increased speed, when a certificate for global optimality is not essential. The improved efficiency of our approach is due to task-level parallelisation of the clustering alongside DTW. Our approach was tested using the UCR Time Series Archive, and was found to be, on average, 33% faster than the next fastest option when using the same clustering method. This increases to 64% faster when considering only larger datasets (with more than 1000 time series). The MIP clustering is most effective on small numbers of longer time series, because the DTW computation is faster than other appro
&lt;/p&gt;</description></item><item><title>&#24490;&#29615;Transformer&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#35299;&#20915;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#12290;&#30456;&#27604;&#20110;&#31867;&#20284;&#30340;&#26041;&#27861;&#65292;&#24490;&#29615;Transformer&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#65292;&#25104;&#21151;&#35299;&#20915;&#31526;&#21495;&#22522;&#30784;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.04895</link><description>&lt;p&gt;
&#20351;&#29992;&#24490;&#29615;Transformer&#23398;&#20064;&#35299;&#20915;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning to Solve Constraint Satisfaction Problems with Recurrent Transformer. (arXiv:2307.04895v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04895
&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;Transformer&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#35299;&#20915;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#12290;&#30456;&#27604;&#20110;&#31867;&#20284;&#30340;&#26041;&#27861;&#65292;&#24490;&#29615;Transformer&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#65292;&#25104;&#21151;&#35299;&#20915;&#31526;&#21495;&#22522;&#30784;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65288;CSPs&#65289;&#26159;&#20851;&#20110;&#25214;&#21040;&#28385;&#36275;&#32473;&#23450;&#32422;&#26463;&#26465;&#20214;&#30340;&#21464;&#37327;&#20540;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#22686;&#21152;&#24490;&#29615;&#24615;&#36136;&#30340;Transformer&#26469;&#23398;&#20064;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#35299;&#20915;CSPs&#26159;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;Graph&#31070;&#32463;&#32593;&#32476;&#12289;SATNet&#21644;&#19968;&#20123;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;&#30001;&#20110;Transformer&#21487;&#20197;&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#30340;&#24490;&#29615;Transformer&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35270;&#35273;&#32422;&#26463;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#31526;&#21495;&#22522;&#30784;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#31163;&#25955;&#32422;&#26463;&#30340;&#28436;&#32462;&#30693;&#35782;&#26469;&#23454;&#29616;Transformer&#30340;&#24402;&#32435;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;CSPs&#30340;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constraint satisfaction problems (CSPs) are about finding values of variables that satisfy the given constraints. We show that Transformer extended with recurrence is a viable approach to learning to solve CSPs in an end-to-end manner, having clear advantages over state-of-the-art methods such as Graph Neural Networks, SATNet, and some neuro-symbolic models. With the ability of Transformer to handle visual input, the proposed Recurrent Transformer can straightforwardly be applied to visual constraint reasoning problems while successfully addressing the symbol grounding problem. We also show how to leverage deductive knowledge of discrete constraints in the Transformer's inductive learning to achieve sample-efficient learning and semi-supervised learning for CSPs.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;2L&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#24341;&#23548;&#21512;&#25104;&#31243;&#24207;&#21270;&#31574;&#30053;&#30340;&#21442;&#32771;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#21644;&#22312;MicroRTS&#38182;&#26631;&#36187;&#20013;&#30340;&#32988;&#21033;&#65292;&#35777;&#26126;&#20102;2L&#31639;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.04893</link><description>&lt;p&gt;
&#36873;&#25321;&#22909;&#23545;&#25163;&#65306;&#22914;&#20309;&#25351;&#23548;&#31243;&#24207;&#21270;&#31574;&#30053;&#30340;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Choosing Well Your Opponents: How to Guide the Synthesis of Programmatic Strategies. (arXiv:2307.04893v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;2L&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#24341;&#23548;&#21512;&#25104;&#31243;&#24207;&#21270;&#31574;&#30053;&#30340;&#21442;&#32771;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#21644;&#22312;MicroRTS&#38182;&#26631;&#36187;&#20013;&#30340;&#32988;&#21033;&#65292;&#35777;&#26126;&#20102;2L&#31639;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Local Learner (2L)&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#20379;&#19968;&#32452;&#21442;&#32771;&#31574;&#30053;&#65292;&#20197;&#25351;&#23548;&#22312;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#25628;&#32034;&#31243;&#24207;&#21270;&#31574;&#30053;&#12290;&#20043;&#21069;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;&#36845;&#20195;&#26368;&#20339;&#21709;&#24212;&#31639;&#27861;(IBR)&#65292;&#34394;&#26500;&#28216;&#25103;&#31639;&#27861;(FP)&#21644;&#21452;&#27491;&#20132;&#31639;&#27861;(DO)&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#25110;&#20250;&#28431;&#25481;&#25351;&#23548;&#25628;&#32034;&#31639;&#27861;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;2L&#20027;&#21160;&#36873;&#25321;&#19968;&#32452;&#21442;&#32771;&#31574;&#30053;&#20197;&#25552;&#39640;&#25628;&#32034;&#20449;&#21495;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19977;&#20010;&#28216;&#25103;&#20013;&#24341;&#23548;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#26469;&#23454;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#21183;&#65292;&#20854;&#20013;&#21253;&#25324;MicroRTS&#65292;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#26102;&#25112;&#30053;&#28216;&#25103;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;2L&#23398;&#20064;&#21040;&#30340;&#21442;&#32771;&#31574;&#30053;&#25552;&#20379;&#20102;&#27604;IBR&#65292;FP&#21644;DO&#26356;&#24378;&#30340;&#25628;&#32034;&#20449;&#21495;&#12290;&#25105;&#20204;&#36824;&#27169;&#25311;&#20102;&#19968;&#22330;MicroRTS&#38182;&#26631;&#36187;&#65292;&#20854;&#20013;&#20351;&#29992;2L&#21512;&#25104;&#22120;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#20004;&#20010;&#26368;&#26032;MicroRTS&#27604;&#36187;&#30340;&#32988;&#32773;&#65292;&#36825;&#20123;&#32988;&#32773;&#22343;&#20026;&#20154;&#31867;&#32534;&#31243;&#21592;&#32534;&#20889;&#30340;&#31243;&#24207;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Local Learner (2L), an algorithm for providing a set of reference strategies to guide the search for programmatic strategies in two-player zero-sum games. Previous learning algorithms, such as Iterated Best Response (IBR), Fictitious Play (FP), and Double-Oracle (DO), can be computationally expensive or miss important information for guiding search algorithms. 2L actively selects a set of reference strategies to improve the search signal. We empirically demonstrate the advantages of our approach while guiding a local search algorithm for synthesizing strategies in three games, including MicroRTS, a challenging real-time strategy game. Results show that 2L learns reference strategies that provide a stronger search signal than IBR, FP, and DO. We also simulate a tournament of MicroRTS, where a synthesizer using 2L outperformed the winners of the two latest MicroRTS competitions, which were programmatic strategies written by human programmers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#25913;&#36827;&#31639;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#21152;&#24555;&#23545;&#31216;&#21464;&#25442;&#30340;&#21457;&#29616;&#36895;&#24230;&#65292;&#24182;&#25104;&#21151;&#22320;&#25512;&#23548;&#20986;&#20102;&#29305;&#27530;&#26446;&#32676;G2&#65292;F4&#21644;E6&#30340;&#23436;&#25972;&#19968;&#32452;&#29983;&#25104;&#20803;&#12290;&#36825;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#21457;&#29616;&#21508;&#31181;&#31867;&#22411;&#30340;&#23545;&#31216;&#24615;&#26159;&#36890;&#29992;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.04891</link><description>&lt;p&gt;
&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#23545;&#31216;&#24615;&#30340;&#21457;&#29616;&#65306;&#25512;&#23548;&#20986;&#29305;&#27530;&#26446;&#32676;G2&#65292;F4&#21644;E6
&lt;/p&gt;
&lt;p&gt;
Accelerated Discovery of Machine-Learned Symmetries: Deriving the Exceptional Lie Groups G2, F4 and E6. (arXiv:2307.04891v1 [hep-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#25913;&#36827;&#31639;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#21152;&#24555;&#23545;&#31216;&#21464;&#25442;&#30340;&#21457;&#29616;&#36895;&#24230;&#65292;&#24182;&#25104;&#21151;&#22320;&#25512;&#23548;&#20986;&#20102;&#29305;&#27530;&#26446;&#32676;G2&#65292;F4&#21644;E6&#30340;&#23436;&#25972;&#19968;&#32452;&#29983;&#25104;&#20803;&#12290;&#36825;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#21457;&#29616;&#21508;&#31181;&#31867;&#22411;&#30340;&#23545;&#31216;&#24615;&#26159;&#36890;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#24050;&#32463;&#24212;&#29992;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#25512;&#23548;&#36830;&#32493;&#23545;&#31216;&#21464;&#25442;&#20197;&#20445;&#25345;&#25968;&#25454;&#26631;&#31614;&#65292;&#24182;&#33719;&#24471;&#30456;&#24212;&#30340;&#23545;&#31216;&#29983;&#25104;&#20803;&#20195;&#25968;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;&#31639;&#27861;&#65292;&#26174;&#33879;&#21152;&#24555;&#20102;&#23545;&#31216;&#21464;&#25442;&#30340;&#21457;&#29616;&#36895;&#24230;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#37193;&#32676;U(n)&#21644;&#29305;&#27530;&#26446;&#32676;G2&#65292;F4&#21644;E6&#30340;&#23436;&#25972;&#19968;&#32452;&#29983;&#25104;&#20803;&#26469;&#28436;&#31034;&#36825;&#20123;&#26032;&#26041;&#27861;&#12290;&#31532;&#19977;&#20010;&#21518;&#22788;&#29702;&#31639;&#27861;&#23558;&#25214;&#21040;&#30340;&#29983;&#25104;&#20803;&#21576;&#29616;&#20026;&#31232;&#30095;&#24418;&#24335;&#12290;&#25105;&#20204;&#23545;&#27604;&#20102;&#26032;&#31639;&#27861;&#19982;&#26631;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;&#37492;&#20110;&#29305;&#27530;&#26446;&#32676;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#21457;&#29616;&#23545;&#31216;&#24615;&#26159;&#23436;&#20840;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has applied supervised deep learning to derive continuous symmetry transformations that preserve the data labels and to obtain the corresponding algebras of symmetry generators. This letter introduces two improved algorithms that significantly speed up the discovery of these symmetry transformations. The new methods are demonstrated by deriving the complete set of generators for the unitary groups U(n) and the exceptional Lie groups $G_2$, $F_4$, and $E_6$. A third post-processing algorithm renders the found generators in sparse form. We benchmark the performance improvement of the new algorithms relative to the standard approach. Given the significant complexity of the exceptional Lie groups, our results demonstrate that this machine-learning method for discovering symmetries is completely general and can be applied to a wide variety of labeled datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#34913;&#37327;&#24378;&#21270;&#23398;&#20064;&#20013;&#24178;&#25200;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31867;&#22312;&#32447;&#24863;&#30693;&#31639;&#27861;&#26469;&#20943;&#36731;&#24178;&#25200;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#32463;&#20856;&#25511;&#21046;&#29615;&#22659;&#20013;&#25552;&#39640;&#20102;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.04887</link><description>&lt;p&gt;
&#27979;&#37327;&#21644;&#32531;&#35299;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Measuring and Mitigating Interference in Reinforcement Learning. (arXiv:2307.04887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#34913;&#37327;&#24378;&#21270;&#23398;&#20064;&#20013;&#24178;&#25200;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31867;&#22312;&#32447;&#24863;&#30693;&#31639;&#27861;&#26469;&#20943;&#36731;&#24178;&#25200;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#32463;&#20856;&#25511;&#21046;&#29615;&#22659;&#20013;&#25552;&#39640;&#20102;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28798;&#38590;&#24615;&#24178;&#25200;&#22312;&#35768;&#22810;&#22522;&#20110;&#32593;&#32476;&#30340;&#23398;&#20064;&#31995;&#32479;&#20013;&#24456;&#24120;&#35265;&#65292;&#24182;&#19988;&#23384;&#22312;&#35768;&#22810;&#20943;&#36731;&#24178;&#25200;&#30340;&#24314;&#35758;&#12290;&#22312;&#20811;&#26381;&#24178;&#25200;&#20043;&#21069;&#65292;&#25105;&#20204;&#24517;&#39035;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;Fitted Q-Iteration&#21644;DQN&#31561;&#22522;&#20110;&#20540;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#24178;&#25200;&#30340;&#23450;&#20041;&#21644;&#26032;&#22411;&#24230;&#37327;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#24178;&#25200;&#24230;&#37327;&#65292;&#22312;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#19978;&#26174;&#31034;&#20986;&#23427;&#19982;&#25511;&#21046;&#24615;&#33021;&#30340;&#19981;&#31283;&#23450;&#24615;&#30456;&#20851;&#12290;&#25105;&#20204;&#30340;&#26032;&#24178;&#25200;&#24230;&#37327;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#20851;&#20110;&#24120;&#29992;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#26032;&#31185;&#23398;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20943;&#36731;&#24178;&#25200;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#31867;&#25105;&#20204;&#31216;&#20026;&#22312;&#32447;&#24863;&#30693;&#31639;&#27861;&#30340;&#31639;&#27861;&#65292;&#26088;&#22312;&#20943;&#36731;&#24178;&#25200;&#65292;&#24182;&#19988;&#26681;&#25454;&#25105;&#20204;&#30340;&#24230;&#37327;&#26174;&#31034;&#23427;&#20204;&#20943;&#23569;&#20102;&#24178;&#25200;&#65292;&#24182;&#22312;&#20960;&#20010;&#32463;&#20856;&#30340;&#25511;&#21046;&#29615;&#22659;&#20013;&#25552;&#39640;&#20102;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic interference is common in many network-based learning systems, and many proposals exist for mitigating it. Before overcoming interference we must understand it better. In this work, we provide a definition and novel measure of interference for value-based reinforcement learning methods such as Fitted Q-Iteration and DQN. We systematically evaluate our measure of interference, showing that it correlates with instability in control performance, across a variety of network architectures. Our new interference measure allows us to ask novel scientific questions about commonly used deep learning architectures and study learning algorithms which mitigate interference. Lastly, we outline a class of algorithms which we call online-aware that are designed to mitigate interference, and show they do reduce interference according to our measure and that they improve stability and performance in several classic control environments.
&lt;/p&gt;</description></item><item><title>&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#27169;&#22411;&#12290;&#23427;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#36866;&#29992;&#20110;&#26080;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.04870</link><description>&lt;p&gt;
&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;&#65306;&#22312;&#24369;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Onion Universe Algorithm: Applications in Weakly Supervised Learning. (arXiv:2307.04870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04870
&lt;/p&gt;
&lt;p&gt;
&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#27169;&#22411;&#12290;&#23427;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#36866;&#29992;&#20110;&#26080;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;(OUA)&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#26631;&#31614;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;OUA&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#25110;&#24369;&#20449;&#21495;&#30340;&#20219;&#20309;&#20551;&#35774;&#12290;&#35813;&#27169;&#22411;&#38750;&#24120;&#36866;&#29992;&#20110;&#27809;&#26377;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#23545;&#30001;&#24369;&#20449;&#21495;&#25152;&#26500;&#25104;&#30340;&#31354;&#38388;&#30340;&#20960;&#20309;&#35299;&#37322;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;OUA&#22312;&#19968;&#33324;&#30340;&#24369;&#20449;&#21495;&#38598;&#21512;&#19979;&#20855;&#26377;&#28508;&#22312;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#65292;OUA&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26631;&#31614;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Onion Universe Algorithm (OUA), a novel classification method in ensemble learning. In particular, we show its applicability as a label model for weakly supervised learning. OUA offers simplicity in implementation, computational efficiency, and does not rely on any assumptions regarding the data or weak signals. The model is well suited for scenarios where fully labeled data is not available. Our method is built upon geometrical interpretation of the space spanned by weak signals. Empirical results support our analysis of the hidden geometric structure underlying general set of weak signals and also illustrates that OUA works well in practice. We show empirical evidence that OUA performs favorably on common benchmark datasets compared to existing label models for weakly supervised learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-CPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#37325;&#22797;&#23398;&#20064;&#30340;&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24322;&#27493;&#25552;&#31034;&#23398;&#20064;&#21644;&#23545;&#27604;&#25345;&#32493;&#25439;&#22833;&#22788;&#29702;&#24322;&#27493;&#20219;&#21153;&#21040;&#36798;&#21644;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.04869</link><description>&lt;p&gt;
Fed-CPrompt: &#26080;&#37325;&#22797;&#23398;&#20064;&#30340;&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#30340;&#23545;&#27604;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning. (arXiv:2307.04869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-CPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#37325;&#22797;&#23398;&#20064;&#30340;&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24322;&#27493;&#25552;&#31034;&#23398;&#20064;&#21644;&#23545;&#27604;&#25345;&#32493;&#25439;&#22833;&#22788;&#29702;&#24322;&#27493;&#20219;&#21153;&#21040;&#36798;&#21644;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#65288;FCL&#65289;&#20174;&#20998;&#24067;&#22312;&#23458;&#25143;&#31471;&#19978;&#30340;&#26426;&#23494;&#25968;&#25454;&#38598;&#20013;&#36880;&#27493;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#26080;&#37325;&#22797;&#23398;&#20064;&#30340;FCL&#65292;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#23384;&#22312;&#22240;&#26080;&#27861;&#35775;&#38382;&#21382;&#21490;&#20219;&#21153;&#25968;&#25454;&#32780;&#23548;&#33268;&#20005;&#37325;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#25216;&#26415;&#30340;Fed-CPrompt&#65292;&#20197;&#19968;&#31181;&#39640;&#25928;&#30340;&#36890;&#20449;&#26041;&#24335;&#33719;&#24471;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#12290;Fed-CPrompt&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#24322;&#27493;&#25552;&#31034;&#23398;&#20064;&#21644;&#23545;&#27604;&#25345;&#32493;&#25439;&#22833;&#65292;&#20197;&#20998;&#21035;&#22788;&#29702;FCL&#20013;&#30340;&#24322;&#27493;&#20219;&#21153;&#21040;&#36798;&#21644;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;Fed-CPrompt&#22312;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#26080;&#37325;&#22797;&#23398;&#20064;FCL&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated continual learning (FCL) learns incremental tasks over time from confidential datasets distributed across clients. This paper focuses on rehearsal-free FCL, which has severe forgetting issues when learning new tasks due to the lack of access to historical task data. To address this issue, we propose Fed-CPrompt based on prompt learning techniques to obtain task-specific prompts in a communication-efficient way. Fed-CPrompt introduces two key components, asynchronous prompt learning, and contrastive continual loss, to handle asynchronous task arrival and heterogeneous data distributions in FCL, respectively. Extensive experiments demonstrate the effectiveness of Fed-CPrompt in achieving SOTA rehearsal-free FCL performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#23454;&#20363;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#23545;&#40784;&#38598;&#21512;&#26469;&#23398;&#20064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.04868</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#40784;&#38598;&#21512;&#26469;&#35299;&#20915;&#23454;&#20363;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Leveraging an Alignment Set in Tackling Instance-Dependent Label Noise. (arXiv:2307.04868v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#23454;&#20363;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#23545;&#40784;&#38598;&#21512;&#26469;&#23398;&#20064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#35757;&#32451;&#26631;&#31614;&#20250;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#22823;&#22810;&#25968;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#30340;&#26041;&#27861;&#37117;&#20551;&#35774;&#26631;&#31614;&#22122;&#22768;&#19982;&#36755;&#20837;&#29305;&#24449;&#26080;&#20851;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#26631;&#31614;&#22122;&#22768;&#24448;&#24448;&#19982;&#29305;&#24449;&#25110;&#23454;&#20363;&#30456;&#20851;&#65292;&#22240;&#27492;&#26159;&#26377;&#20559;&#35265;&#30340;&#65288;&#21363;&#26576;&#20123;&#23454;&#20363;&#26356;&#21487;&#33021;&#34987;&#38169;&#35823;&#26631;&#35760;&#65289;&#12290;&#20363;&#22914;&#65292;&#22312;&#20020;&#24202;&#25252;&#29702;&#20013;&#65292;&#22899;&#24615;&#24739;&#32773;&#30456;&#27604;&#30007;&#24615;&#24739;&#32773;&#26356;&#23481;&#26131;&#34987;&#35823;&#35786;&#20026;&#24515;&#34880;&#31649;&#30142;&#30149;&#12290;&#24573;&#30053;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#20135;&#29983;&#21306;&#20998;&#24615;&#33021;&#24046;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#21487;&#33021;&#21152;&#21095;&#20581;&#24247;&#19981;&#24179;&#31561;&#38382;&#39064;&#12290;&#37492;&#20110;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23454;&#20363;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;\anchor&#38170;&#28857;&#65292;&#36825;&#26159;&#19968;&#20010;&#23567;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#25105;&#20204;&#30693;&#36947;&#20854;&#20013;&#30340;&#35266;&#27979;&#21644;&#30495;&#23454;&#26631;&#31614;&#12290;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noisy training labels can hurt model performance. Most approaches that aim to address label noise assume label noise is independent from the input features. In practice, however, label noise is often feature or \textit{instance-dependent}, and therefore biased (i.e., some instances are more likely to be mislabeled than others). E.g., in clinical care, female patients are more likely to be under-diagnosed for cardiovascular disease compared to male patients. Approaches that ignore this dependence can produce models with poor discriminative performance, and in many healthcare settings, can exacerbate issues around health disparities. In light of these limitations, we propose a two-stage approach to learn in the presence instance-dependent label noise. Our approach utilizes \textit{\anchor points}, a small subset of data for which we know the observed and ground truth labels. On several tasks, our approach leads to consistent improvements over the state-of-the-art in discriminative perfor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33136;&#37096;&#20329;&#25140;&#30340;&#21152;&#36895;&#35745;&#33258;&#21160;&#26816;&#27979;&#27493;&#24577;&#20107;&#20214;&#21644;&#34892;&#36208;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#24066;&#21806;&#26234;&#33021;&#25163;&#26426;&#21152;&#36895;&#35745;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#20174;&#24191;&#27867;&#30340;&#27493;&#24577;&#36895;&#24230;&#33539;&#22260;&#20013;&#25552;&#21462;&#27493;&#24577;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#23545;Duchenne&#32908;&#32905;&#33806;&#32553;&#24739;&#20799;&#21644;&#20856;&#22411;&#21457;&#32946;&#27491;&#24120;&#24739;&#32773;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.04866</link><description>&lt;p&gt;
&#20351;&#29992;&#33136;&#37096;&#20329;&#25140;&#30340;&#21152;&#36895;&#35745;&#22312;&#20856;&#22411;&#30340;&#34892;&#36208;&#21644;&#36305;&#27493;&#36895;&#24230;&#33539;&#22260;&#20869;&#33258;&#21160;&#26816;&#27979;&#27493;&#24577;&#20107;&#20214;&#21644;&#34892;&#36208;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Automated Detection of Gait Events and Travel Distance Using Waist-worn Accelerometers Across a Typical Range of Walking and Running Speeds. (arXiv:2307.04866v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04866
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33136;&#37096;&#20329;&#25140;&#30340;&#21152;&#36895;&#35745;&#33258;&#21160;&#26816;&#27979;&#27493;&#24577;&#20107;&#20214;&#21644;&#34892;&#36208;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#24066;&#21806;&#26234;&#33021;&#25163;&#26426;&#21152;&#36895;&#35745;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#20174;&#24191;&#27867;&#30340;&#27493;&#24577;&#36895;&#24230;&#33539;&#22260;&#20013;&#25552;&#21462;&#27493;&#24577;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#23545;Duchenne&#32908;&#32905;&#33806;&#32553;&#24739;&#20799;&#21644;&#20856;&#22411;&#21457;&#32946;&#27491;&#24120;&#24739;&#32773;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20272;&#35745;&#27493;&#24577;&#65288;CFs&#65289;&#30340;&#26102;&#38388;&#31354;&#38388;&#20020;&#24202;&#29305;&#24449;&#65292;&#22914;&#27493;&#25968;&#21644;&#38271;&#24230;&#12289;&#27493;&#38271;&#12289;&#27493;&#39057;&#12289;&#27493;&#36895;&#21644;&#34892;&#36208;&#36317;&#31163;&#31561;&#65292;&#22312;&#20351;&#29992;&#21487;&#31359;&#25140;&#24335;&#21152;&#36895;&#35745;&#36827;&#34892;&#22522;&#20110;&#31038;&#21306;&#30340;&#31227;&#21160;&#24615;&#35780;&#20272;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#22791;&#22797;&#26434;&#24615;&#21644;&#21487;&#29992;&#24615;&#12289;&#25104;&#26412;&#21644;&#20998;&#26512;&#26041;&#27861;&#23398;&#24341;&#36215;&#30340;&#25361;&#25112;&#38480;&#21046;&#20102;&#27492;&#31867;&#24037;&#20855;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#30740;&#31350;&#38382;&#39064;&#65306;&#33021;&#21542;&#20351;&#29992;&#24066;&#21806;&#26234;&#33021;&#25163;&#26426;&#30340;&#21152;&#36895;&#35745;&#25968;&#25454;&#26469;&#25552;&#21462;Duchenne&#32908;&#32905;&#33806;&#32553;&#65288;DMD&#65289;&#24739;&#20799;&#21644;&#20856;&#22411;&#21457;&#32946;&#27491;&#24120;&#65288;TDs&#65289;&#24739;&#32773;&#22312;&#24191;&#27867;&#27493;&#24577;&#36895;&#24230;&#33539;&#22260;&#20869;&#30340;&#27493;&#24577;CFs&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;15&#21517;DMD&#24739;&#20799;&#21644;15&#21517;TDs&#34987;&#35201;&#27714;&#22312;10MRW&#12289;25MRW&#12289;100MRW&#12289;6MWT&#21644;FW&#35780;&#20272;&#20013;&#20197;&#19968;&#31995;&#21015;&#27493;&#24577;&#36895;&#24230;&#36827;&#34892;&#30417;&#30563;&#24615;&#20020;&#24202;&#27979;&#35797;&#65292;&#21516;&#26102;&#20329;&#25140;&#25163;&#26426;&#22522;&#30784;&#21152;&#36895;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Estimation of temporospatial clinical features of gait (CFs), such as step count and length, step duration, step frequency, gait speed and distance traveled is an important component of community-based mobility evaluation using wearable accelerometers. However, challenges arising from device complexity and availability, cost and analytical methodology have limited widespread application of such tools. Research Question: Can accelerometer data from commercially-available smartphones be used to extract gait CFs across a broad range of attainable gait velocities in children with Duchenne muscular dystrophy (DMD) and typically developing controls (TDs) using machine learning (ML)-based methods Methods: Fifteen children with DMD and 15 TDs underwent supervised clinical testing across a range of gait speeds using 10 or 25m run/walk (10MRW, 25MRW), 100m run/walk (100MRW), 6-minute walk (6MWT) and free-walk (FW) evaluations while wearing a mobile phone-based accelerometer at the wa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#24341;&#23548;&#30340;3D&#22836;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#22312;&#21487;&#20851;&#33410;&#21270;&#30340;3D&#27169;&#22411;&#19978;&#25805;&#20316;&#20960;&#20309;&#21644;&#32441;&#29702;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#22836;&#20687;&#30340;&#31934;&#30830;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.04859</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21487;&#20851;&#33410;&#21270;&#30340;3D&#22836;&#20687;
&lt;/p&gt;
&lt;p&gt;
Articulated 3D Head Avatar Generation using Text-to-Image Diffusion Models. (arXiv:2307.04859v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#24341;&#23548;&#30340;3D&#22836;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#22312;&#21487;&#20851;&#33410;&#21270;&#30340;3D&#27169;&#22411;&#19978;&#25805;&#20316;&#20960;&#20309;&#21644;&#32441;&#29702;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#22836;&#20687;&#30340;&#31934;&#30830;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;3D&#21487;&#20851;&#33410;&#21270;&#22836;&#20687;&#23545;&#20110;&#22686;&#24378;&#29616;&#23454;&#12289;&#30005;&#24433;&#21046;&#20316;&#21644;&#25945;&#32946;&#31561;&#20247;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20851;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;3D&#29289;&#20307;&#29983;&#25104;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#26469;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#12290;&#36825;&#20123;&#26041;&#27861;&#30452;&#25509;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;2D&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;3D&#22810;&#35270;&#35282;&#19968;&#33268;&#30340;&#36890;&#29992;&#29289;&#20307;&#36752;&#23556;&#22330;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#20960;&#20309;&#21644;&#32441;&#29702;&#20808;&#39564;&#30693;&#35782;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#29983;&#25104;&#30340;3D&#29289;&#20307;&#30340;&#25511;&#21046;&#33021;&#21147;&#26377;&#38480;&#65292;&#23548;&#33268;&#38590;&#20197;&#25805;&#20316;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#65292;&#27604;&#22914;&#20154;&#33080;&#22836;&#20687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#23454;&#29616;&#25991;&#26412;&#24341;&#23548;&#30340;3D&#22836;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30452;&#25509;&#22312;&#19968;&#20010;&#21487;&#20851;&#33410;&#21270;&#30340;3D&#21487;&#21464;&#24418;&#27169;&#22411;&#65288;3DMM&#65289;&#30340;&#20960;&#20309;&#21644;&#32441;&#29702;&#19978;&#25805;&#20316;&#65292;&#24182;&#24341;&#20837;&#20102;&#26356;&#26032;&#20960;&#20309;&#21644;&#32441;&#29702;&#30340;&#26032;&#22411;&#20248;&#21270;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#25345;2D&#21644;3D&#38754;&#37096;&#29305;&#24449;&#30340;&#23545;&#40784;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;3D&#22836;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generate diverse 3D articulated head avatars is vital to a plethora of applications, including augmented reality, cinematography, and education. Recent work on text-guided 3D object generation has shown great promise in addressing these needs. These methods directly leverage pre-trained 2D text-to-image diffusion models to generate 3D-multi-view-consistent radiance fields of generic objects. However, due to the lack of geometry and texture priors, these methods have limited control over the generated 3D objects, making it difficult to operate inside a specific domain, e.g., human heads. In this work, we develop a new approach to text-guided 3D head avatar generation to address this limitation. Our framework directly operates on the geometry and texture of an articulable 3D morphable model (3DMM) of a head, and introduces novel optimization procedures to update the geometry and texture while keeping the 2D and 3D facial features aligned. The result is a 3D head avatar tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SHAP@k&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26469;&#35299;&#20915;Top-k&#29305;&#24449;&#35782;&#21035;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;Explore-m&#38382;&#39064;&#24182;&#21033;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#25216;&#26415;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.04850</link><description>&lt;p&gt;
SHAP@k&#65306;&#39640;&#25928;&#19988;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;&#22320;&#35782;&#21035;Top-k&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
SHAP@k:Efficient and Probably Approximately Correct (PAC) Identification of Top-k Features. (arXiv:2307.04850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SHAP@k&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26469;&#35299;&#20915;Top-k&#29305;&#24449;&#35782;&#21035;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;Explore-m&#38382;&#39064;&#24182;&#21033;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#25216;&#26415;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SHAP&#26694;&#26550;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#37325;&#35201;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#21463;&#37329;&#34701;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Top-k&#35782;&#21035;&#38382;&#39064;&#65288;TkIP&#65289;&#65292;&#20854;&#30446;&#26631;&#26159;&#35782;&#21035;&#20855;&#26377;&#26368;&#39640;SHAP&#20540;&#30340;k&#20010;&#29305;&#24449;&#12290;&#34429;&#28982;&#20219;&#20309;&#35745;&#31639;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;SHAP&#20540;&#30340;&#26041;&#27861;&#65288;&#22914;KernelSHAP&#21644;SamplingSHAP&#65289;&#37117;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;TkIP&#30340;&#35299;&#20915;&#65292;&#20294;&#36825;&#26679;&#20570;&#20250;&#23548;&#33268;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30446;&#26631;&#26159;&#22312;&#35299;&#20915;TkIP&#30340;&#32972;&#26223;&#19979;&#25552;&#39640;&#29616;&#26377;&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;TkIP&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;Explore-m&#38382;&#39064;&#36827;&#34892;&#24314;&#27169;&#65292;&#35813;&#38382;&#39064;&#19982;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30456;&#20851;&#30340;&#38382;&#39064;&#24050;&#32463;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#36825;&#31181;&#32852;&#31995;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;MAB&#25991;&#29486;&#20013;&#30340;&#20004;&#31181;&#25216;&#26415;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65306;&#65288;1&#65289;&#26356;&#22909;&#30340;&#20572;&#27490;&#26465;&#20214;&#65288;&#20572;&#27490;&#37319;&#26679;&#65289;&#65292;&#35782;&#21035;PAC&#65288;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65289;&#20445;&#35777;&#24050;&#32463;&#28385;&#36275;&#65307;&#65288;2&#65289;&#19968;&#31181;&#36138;&#23146;&#37319;&#26679;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The SHAP framework provides a principled method to explain the predictions of a model by computing feature importance. Motivated by applications in finance, we introduce the Top-k Identification Problem (TkIP), where the objective is to identify the k features with the highest SHAP values. While any method to compute SHAP values with uncertainty estimates (such as KernelSHAP and SamplingSHAP) can be trivially adapted to solve TkIP, doing so is highly sample inefficient. The goal of our work is to improve the sample efficiency of existing methods in the context of solving TkIP. Our key insight is that TkIP can be framed as an Explore-m problem--a well-studied problem related to multi-armed bandits (MAB). This connection enables us to improve sample efficiency by leveraging two techniques from the MAB literature: (1) a better stopping-condition (to stop sampling) that identifies when PAC (Probably Approximately Correct) guarantees have been met and (2) a greedy sampling scheme that judic
&lt;/p&gt;</description></item><item><title>SigOpt Mulch&#26159;&#19968;&#31181;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#35843;&#25972;&#26799;&#24230;&#25552;&#21319;&#26641;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#12290;&#19982;&#20854;&#20182;&#29616;&#26377;&#31995;&#32479;&#19981;&#21516;&#65292;SigOpt Mulch&#26159;&#8220;&#27169;&#22411;&#24863;&#30693;&#22411;&#8221;&#30340;&#65292;&#33021;&#22815;&#38024;&#23545;GBTs&#36827;&#34892;&#26356;&#20248;&#21270;&#30340;&#24615;&#33021;&#35843;&#25972;&#65292;&#24182;&#19988;&#26080;&#38656;&#39046;&#22495;&#30693;&#35782;&#65292;&#24110;&#21161;&#23454;&#29616;&#33258;&#21160;&#21270;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.04849</link><description>&lt;p&gt;
SigOpt Mulch: &#19968;&#31181;&#33258;&#21160;&#23398;&#20064;&#26799;&#24230;&#25552;&#21319;&#26641;&#30340;&#26234;&#33021;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SigOpt Mulch: An Intelligent System for AutoML of Gradient Boosted Trees. (arXiv:2307.04849v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04849
&lt;/p&gt;
&lt;p&gt;
SigOpt Mulch&#26159;&#19968;&#31181;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#35843;&#25972;&#26799;&#24230;&#25552;&#21319;&#26641;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#12290;&#19982;&#20854;&#20182;&#29616;&#26377;&#31995;&#32479;&#19981;&#21516;&#65292;SigOpt Mulch&#26159;&#8220;&#27169;&#22411;&#24863;&#30693;&#22411;&#8221;&#30340;&#65292;&#33021;&#22815;&#38024;&#23545;GBTs&#36827;&#34892;&#26356;&#20248;&#21270;&#30340;&#24615;&#33021;&#35843;&#25972;&#65292;&#24182;&#19988;&#26080;&#38656;&#39046;&#22495;&#30693;&#35782;&#65292;&#24110;&#21161;&#23454;&#29616;&#33258;&#21160;&#21270;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#25552;&#21319;&#26641;(GBTs)&#26159;&#30740;&#31350;&#20154;&#21592;&#12289;&#26426;&#22120;&#23398;&#20064;(ML)&#23454;&#36341;&#32773;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#26222;&#36941;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#31283;&#20581;&#30340;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#30340;&#34892;&#20026;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#29305;&#28857;&#12290;&#35757;&#32451;GBTs&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#35843;&#25972;&#36229;&#21442;&#25968;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36873;&#25321;&#36825;&#20123;&#36229;&#21442;&#25968;&#36890;&#24120;&#26159;&#25163;&#21160;&#23436;&#25104;&#30340;&#12290;&#26368;&#36817;&#65292;ML&#31038;&#21306;&#25552;&#20513;&#36890;&#36807;&#40657;&#30418;&#20248;&#21270;&#26469;&#35843;&#25972;&#36229;&#21442;&#25968;&#65292;&#24182;&#24320;&#21457;&#20102;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#31995;&#32479;&#24212;&#29992;&#20110;&#35843;&#25972;GBTs&#23384;&#22312;&#20004;&#20010;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#31995;&#32479;&#19981;&#20855;&#22791;&#8220;&#27169;&#22411;&#24863;&#30693;&#24615;&#8221;&#65292;&#32780;&#26159;&#35774;&#35745;&#29992;&#20110;&#8220;&#36890;&#29992;&#8221;&#27169;&#22411;&#65292;&#36825;&#23548;&#33268;&#20102;&#20248;&#21270;&#24615;&#33021;&#30340;&#26174;&#33879;&#38477;&#20302;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#36825;&#20123;&#31995;&#32479;&#38656;&#35201;&#8220;&#39046;&#22495;&#30693;&#35782;&#8221;&#65292;&#27604;&#22914;&#36229;&#21442;&#25968;&#25628;&#32034;&#31354;&#38388;&#30340;&#36873;&#25321;&#65292;&#36825;&#19982;&#40657;&#30418;&#20248;&#21270;&#26088;&#22312;&#25552;&#20379;&#30340;&#33258;&#21160;&#23454;&#39564;&#30456;&#24726;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SigOpt Mulch
&lt;/p&gt;
&lt;p&gt;
Gradient boosted trees (GBTs) are ubiquitous models used by researchers, machine learning (ML) practitioners, and data scientists because of their robust performance, interpretable behavior, and ease-of-use. One critical challenge in training GBTs is the tuning of their hyperparameters. In practice, selecting these hyperparameters is often done manually. Recently, the ML community has advocated for tuning hyperparameters through black-box optimization and developed state-of-the-art systems to do so. However, applying such systems to tune GBTs suffers from two drawbacks. First, these systems are not \textit{model-aware}, rather they are designed to apply to a \textit{generic} model; this leaves significant optimization performance on the table. Second, using these systems requires \textit{domain knowledge} such as the choice of hyperparameter search space, which is an antithesis to the automatic experimentation that black-box optimization aims to provide. In this paper, we present SigOp
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#19979;&#30340;&#20856;&#22411;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#30001;&#20110;&#23376;&#37319;&#26679;&#21487;&#33021;&#30340;&#36712;&#36857;&#31354;&#38388;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#21322;&#26799;&#24230;&#22122;&#22768;&#20250;&#23548;&#33268;&#20540;&#35823;&#24046;&#20986;&#29616;&#26174;&#33879;&#30340;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2307.04841</link><description>&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Temporal Difference Reinforcement Learning. (arXiv:2307.04841v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04841
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#19979;&#30340;&#20856;&#22411;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#30001;&#20110;&#23376;&#37319;&#26679;&#21487;&#33021;&#30340;&#36712;&#36857;&#31354;&#38388;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#21322;&#26799;&#24230;&#22122;&#22768;&#20250;&#23548;&#33268;&#20540;&#35823;&#24046;&#20986;&#29616;&#26174;&#33879;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#38656;&#35201;&#23398;&#20064;&#22312;&#21453;&#39304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#34892;&#21160;&#30340;&#22810;&#20010;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#36825;&#31181;&#32463;&#39564;&#19978;&#30340;&#25104;&#21151;&#65292;&#20173;&#28982;&#27809;&#26377;&#23545;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#21644;&#29992;&#20110;&#34920;&#31034;&#29366;&#24577;&#30340;&#29305;&#24449;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#25511;&#21046;&#23398;&#20064;&#21160;&#24577;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#19979;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#20215;&#20540;&#20989;&#25968;&#30340;&#20856;&#22411;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#26159;&#22312;&#19968;&#20010;&#39640;&#26031;&#31561;&#25928;&#20551;&#35774;&#19979;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#20854;&#20013;&#23545;&#38543;&#26426;&#36712;&#36857;&#30340;&#24179;&#22343;&#20540;&#34987;&#26367;&#25442;&#20026;&#26102;&#24577;&#30456;&#20851;&#30340;&#39640;&#26031;&#29305;&#24449;&#24179;&#22343;&#20540;&#65292;&#24182;&#19988;&#25105;&#20204;&#22312;&#23567;&#35268;&#27169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#23545;&#21487;&#33021;&#30340;&#36712;&#36857;&#31354;&#38388;&#36827;&#34892;&#23376;&#37319;&#26679;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#21322;&#26799;&#24230;&#22122;&#22768;&#23548;&#33268;&#20540;&#35823;&#24046;&#20986;&#29616;&#26174;&#33879;&#30340;&#24179;&#21488;&#65292;&#36825;&#19982;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been successful across several applications in which agents have to learn to act in environments with sparse feedback. However, despite this empirical success there is still a lack of theoretical understanding of how the parameters of reinforcement learning models and the features used to represent states interact to control the dynamics of learning. In this work, we use concepts from statistical physics, to study the typical case learning curves for temporal difference learning of a value function with linear function approximators. Our theory is derived under a Gaussian equivalence hypothesis where averages over the random trajectories are replaced with temporally correlated Gaussian feature averages and we validate our assumptions on small scale Markov Decision Processes. We find that the stochastic semi-gradient noise due to subsampling the space of possible episodes leads to significant plateaus in the value error, unlike in traditional gradient descent 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;CLIP&#27169;&#22411;&#25552;&#39640;&#35270;&#35273;&#20851;&#31995;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#22312;UVTransE&#26694;&#26550;&#20013;&#37319;&#29992;&#22522;&#20110;CLIP&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#21450;&#24341;&#20837;&#23545;&#27604;&#35757;&#32451;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CREPE&#27169;&#22411;&#65292;&#31616;&#21270;&#20102;&#29616;&#26377;&#22797;&#26434;&#30340;&#22270;&#24418;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.04838</link><description>&lt;p&gt;
CREPE&#65306;&#20351;&#29992;CLIP&#30340;&#21487;&#23398;&#20064;&#25552;&#31034;&#25552;&#39640;&#35270;&#35273;&#20851;&#31995;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction. (arXiv:2307.04838v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;CLIP&#27169;&#22411;&#25552;&#39640;&#35270;&#35273;&#20851;&#31995;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#22312;UVTransE&#26694;&#26550;&#20013;&#37319;&#29992;&#22522;&#20110;CLIP&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#21450;&#24341;&#20837;&#23545;&#27604;&#35757;&#32451;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CREPE&#27169;&#22411;&#65292;&#31616;&#21270;&#20102;&#29616;&#26377;&#22797;&#26434;&#30340;&#22270;&#24418;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#29305;&#21035;&#26159;CLIP&#65292;&#22312;&#39044;&#27979;&#35270;&#35273;&#30446;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20854;&#20013;&#28041;&#21450;&#23558;&#22270;&#20687;&#30340;&#35270;&#35273;&#29305;&#24449;&#35299;&#37322;&#20026;&#22522;&#20110;&#35821;&#35328;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20351;&#29992;&#22797;&#26434;&#30340;&#22270;&#24418;&#27169;&#22411;&#65292;&#21033;&#29992;&#35821;&#35328;&#32447;&#32034;&#21644;&#35270;&#35273;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#20551;&#35774;CLIP&#23884;&#20837;&#20013;&#30340;&#24378;&#35821;&#35328;&#20808;&#39564;&#21487;&#20197;&#31616;&#21270;&#36825;&#20123;&#22270;&#24418;&#27169;&#22411;&#65292;&#20026;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#38138;&#24179;&#36947;&#36335;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;UVTransE&#20851;&#31995;&#39044;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22330;&#26223;&#20013;&#30340;&#20027;&#20307;&#12289;&#23458;&#20307;&#21644;&#24182;&#38598;&#26694;&#23884;&#20837;&#26469;&#23398;&#20064;&#20851;&#31995;&#20316;&#20026;&#19968;&#20010;&#24179;&#31227;&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;UVTransE&#26694;&#26550;&#20869;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#22522;&#20110;CLIP&#30340;&#20027;&#20307;&#12289;&#23458;&#20307;&#21644;&#24182;&#38598;&#26694;&#34920;&#31034;&#30340;&#35774;&#35745;&#65292;&#24182;&#25552;&#20986;&#20102;CREPE&#65288;CLIP&#22686;&#24378;&#35859;&#35789;&#20272;&#35745;&#65289;&#12290;CREPE&#21033;&#29992;&#25152;&#26377;&#19977;&#20010;&#36793;&#30028;&#26694;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#33258;&#21160;&#23398;&#20064;&#35270;&#35273;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the potential of Vision-Language Models (VLMs), specifically CLIP, in predicting visual object relationships, which involves interpreting visual features from images into language-based relations. Current state-of-the-art methods use complex graphical models that utilize language cues and visual features to address this challenge. We hypothesize that the strong language priors in CLIP embeddings can simplify these graphical models paving for a simpler approach. We adopt the UVTransE relation prediction framework, which learns the relation as a translational embedding with subject, object, and union box embeddings from a scene. We systematically explore the design of CLIP-based subject, object, and union-box representations within the UVTransE framework and propose CREPE (CLIP Representation Enhanced Predicate Estimation). CREPE utilizes text-based representations for all three bounding boxes and introduces a novel contrastive training strategy to automatically
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;&#35299;&#20915;&#20102;&#32676;&#20307;&#27979;&#35797;&#20013;&#35782;&#21035;&#29305;&#23450;&#25968;&#37327;&#26377;&#32570;&#38519;&#39033;&#30446;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#33258;&#36866;&#24212;&#21644;&#38750;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#19978;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2307.04822</link><description>&lt;p&gt;
&#20851;&#20110;&#26816;&#27979;&#32676;&#20307;&#27979;&#35797;&#20013;&#30340;&#26576;&#20123;&#26377;&#32570;&#38519;&#30340;&#39033;&#30446;
&lt;/p&gt;
&lt;p&gt;
On Detecting Some Defective Items in Group Testing. (arXiv:2307.04822v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#35299;&#20915;&#20102;&#32676;&#20307;&#27979;&#35797;&#20013;&#35782;&#21035;&#29305;&#23450;&#25968;&#37327;&#26377;&#32570;&#38519;&#39033;&#30446;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#33258;&#36866;&#24212;&#21644;&#38750;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#19978;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#27979;&#35797;&#26159;&#19968;&#31181;&#26088;&#22312;&#22312;&#24635;&#20849;n&#20010;&#20803;&#32032;&#20013;&#35782;&#21035;&#26368;&#22810;d&#20010;&#26377;&#32570;&#38519;&#30340;&#39033;&#30446;&#30340;&#26041;&#27861;&#12290;&#36825;&#26159;&#36890;&#36807;&#26816;&#26597;&#23376;&#38598;&#26469;&#30830;&#23450;&#26159;&#21542;&#33267;&#23569;&#23384;&#22312;&#19968;&#20010;&#26377;&#32570;&#38519;&#30340;&#39033;&#30446;&#26469;&#23454;&#29616;&#30340;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20102;&#35782;&#21035;&#26368;&#22810;$\ell \leq d$&#20010;&#26377;&#32570;&#38519;&#39033;&#30446;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#33258;&#36866;&#24212;&#21644;&#38750;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#65292;&#22312;&#27809;&#26377;&#20851;&#20110;d&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20197;&#21450;&#22312;&#26377;&#20851;&#20110;d&#30340;&#20272;&#35745;&#25110;&#33267;&#23569;&#19968;&#20123;&#38750;&#24179;&#20961;&#30340;&#19978;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#24320;&#21457;&#20102;&#26816;&#27979;$\ell$&#20010;&#26377;&#32570;&#38519;&#39033;&#30446;&#25152;&#38656;&#30340;&#27979;&#35797;&#27425;&#25968;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;&#24403;&#27809;&#26377;&#20851;&#20110;d&#30340;&#20808;&#39564;&#30693;&#35782;&#26102;&#65292;&#25105;&#20204;&#22312;&#38543;&#26426;&#38750;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#35777;&#26126;&#20102;&#19968;&#20010;&#19979;&#30028;&#65306;$\Omega(\frac{\ell \log^2n}{\log \ell +\log\log n})$&#30340;&#27979;&#35797;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#21516;&#35774;&#32622;&#30340;&#19978;&#30028;&#65306;$O(\ell \log^2 n)$&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20219;&#20309;&#38750;&#33258;&#36866;&#24212;&#30830;&#23450;&#24615;&#31639;&#27861;&#24517;&#39035;&#35810;&#38382;$\Theta(n)$&#27425;&#27979;&#35797;&#65292;&#36825;&#24847;&#21619;&#30528;&#23384;&#22312;&#19968;&#31181;&#26681;&#26412;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group testing is an approach aimed at identifying up to $d$ defective items among a total of $n$ elements. This is accomplished by examining subsets to determine if at least one defective item is present. In our study, we focus on the problem of identifying a subset of $\ell\leq d$ defective items. We develop upper and lower bounds on the number of tests required to detect $\ell$ defective items in both the adaptive and non-adaptive settings while considering scenarios where no prior knowledge of $d$ is available, and situations where an estimate of $d$ or at least some non-trivial upper bound on $d$ is available.  When no prior knowledge on $d$ is available, we prove a lower bound of $ \Omega(\frac{\ell \log^2n}{\log \ell +\log\log n})$ tests in the randomized non-adaptive settings and an upper bound of $O(\ell \log^2 n)$ for the same settings. Furthermore, we demonstrate that any non-adaptive deterministic algorithm must ask $\Theta(n)$ tests, signifying a fundamental limitation in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#26426;&#21046;&#19982;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#29289;&#29702;&#24847;&#20041;&#21644;&#39640;&#31934;&#24230;&#30340;&#26080;&#38388;&#26029;&#22320;&#34920;&#28201;&#24230;&#20272;&#35745;&#12290;&#27169;&#22411;&#21033;&#29992;&#20102;&#36731;&#37327;&#26799;&#24230;&#25552;&#21319;&#26426;&#20316;&#20026;&#32431;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#29289;&#29702;&#32422;&#26463;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#22806;&#25512;&#33021;&#21147;&#12290;&#27169;&#22411;&#30340;&#36755;&#20837;&#21464;&#37327;&#21253;&#25324;&#36965;&#24863;&#25968;&#25454;&#12289;&#20851;&#38190;Community Land Model&#65288;CLM&#65289;&#24378;&#21046;&#25968;&#25454;&#20197;&#21450;CLM&#27169;&#25311;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.04817</link><description>&lt;p&gt;
&#29992;&#20110;&#26144;&#23556;&#26080;&#38388;&#26029;&#22320;&#34920;&#28201;&#24230;&#30340;&#29289;&#29702;&#32422;&#26463;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A physics-constrained machine learning method for mapping gapless land surface temperature. (arXiv:2307.04817v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#26426;&#21046;&#19982;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#29289;&#29702;&#24847;&#20041;&#21644;&#39640;&#31934;&#24230;&#30340;&#26080;&#38388;&#26029;&#22320;&#34920;&#28201;&#24230;&#20272;&#35745;&#12290;&#27169;&#22411;&#21033;&#29992;&#20102;&#36731;&#37327;&#26799;&#24230;&#25552;&#21319;&#26426;&#20316;&#20026;&#32431;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#29289;&#29702;&#32422;&#26463;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#22806;&#25512;&#33021;&#21147;&#12290;&#27169;&#22411;&#30340;&#36755;&#20837;&#21464;&#37327;&#21253;&#25324;&#36965;&#24863;&#25968;&#25454;&#12289;&#20851;&#38190;Community Land Model&#65288;CLM&#65289;&#24378;&#21046;&#25968;&#25454;&#20197;&#21450;CLM&#27169;&#25311;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#29699;&#31995;&#32479;&#30740;&#31350;&#20013;&#65292;&#26356;&#20934;&#30830;&#12289;&#26102;&#31354;&#19968;&#33268;&#19988;&#20855;&#26377;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#22320;&#34920;&#28201;&#24230;&#20272;&#35745;&#19968;&#30452;&#26159;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#21457;&#23637;&#29289;&#29702;&#39537;&#21160;&#26426;&#21046;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#26159;&#26080;&#38388;&#26029;&#22320;&#34920;&#28201;&#24230;&#20272;&#35745;&#30340;&#20004;&#31181;&#20027;&#35201;&#33539;&#24335;&#65292;&#23427;&#20204;&#21508;&#33258;&#20855;&#26377;&#20248;&#32570;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29289;&#29702;&#32422;&#26463;&#30340;ML&#27169;&#22411;&#65292;&#23558;&#26426;&#21046;&#27169;&#22411;&#21644;ML&#27169;&#22411;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#29289;&#29702;&#24847;&#20041;&#21644;&#39640;&#31934;&#24230;&#30340;&#26080;&#38388;&#26029;&#22320;&#34920;&#28201;&#24230;&#12290;&#28151;&#21512;&#27169;&#22411;&#23558;ML&#20316;&#20026;&#20027;&#35201;&#26694;&#26550;&#65292;&#20854;&#20013;&#23558;&#36755;&#20837;&#21464;&#37327;&#30340;&#29289;&#29702;&#32422;&#26463;&#34701;&#20837;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#22806;&#25512;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36731;&#37327;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;LGBM&#65289;&#27169;&#22411;&#65292;&#21482;&#20351;&#29992;&#36965;&#24863;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#20316;&#20026;&#32431;ML&#27169;&#22411;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25972;&#21512;&#20851;&#38190;Community Land Model&#65288;CLM&#65289;&#24378;&#21046;&#25968;&#25454;&#65288;&#21407;&#22240;&#65289;&#21644;CLM&#27169;&#25311;&#25968;&#25454;&#65288;&#25928;&#26524;&#65289;&#65292;&#29289;&#29702;&#32422;&#26463;&#65288;PCs&#65289;&#34987;&#32806;&#21512;&#36827;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
More accurate, spatio-temporally, and physically consistent LST estimation has been a main interest in Earth system research. Developing physics-driven mechanism models and data-driven machine learning (ML) models are two major paradigms for gapless LST estimation, which have their respective advantages and disadvantages. In this paper, a physics-constrained ML model, which combines the strengths in the mechanism model and ML model, is proposed to generate gapless LST with physical meanings and high accuracy. The hybrid model employs ML as the primary architecture, under which the input variable physical constraints are incorporated to enhance the interpretability and extrapolation ability of the model. Specifically, the light gradient-boosting machine (LGBM) model, which uses only remote sensing data as input, serves as the pure ML model. Physical constraints (PCs) are coupled by further incorporating key Community Land Model (CLM) forcing data (cause) and CLM simulation data (effect)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21327;&#21516;&#20998;&#25968;&#33976;&#39311;&#65288;Collaborative Score Distillation&#65292;CSD&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;SVGD&#65289;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#26679;&#26412;&#20316;&#20026;&#8220;&#31890;&#23376;&#8221;&#24182;&#32467;&#21512;&#23427;&#20204;&#30340;&#20998;&#25968;&#20989;&#25968;&#26469;&#23454;&#29616;&#23545;&#19968;&#32452;&#22270;&#20687;&#30340;&#29983;&#25104;&#20808;&#39564;&#36827;&#34892;&#21516;&#27493;&#33976;&#39311;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#26679;&#26412;&#20043;&#38388;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#35270;&#35273;&#21512;&#25104;&#12290;CSD&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20840;&#26223;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#22330;&#26223;&#30340;&#35270;&#35273;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2307.04787</link><description>&lt;p&gt;
&#21327;&#21516;&#20998;&#25968;&#33976;&#39311;&#29992;&#20110;&#19968;&#33268;&#30340;&#35270;&#35273;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Collaborative Score Distillation for Consistent Visual Synthesis. (arXiv:2307.04787v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21327;&#21516;&#20998;&#25968;&#33976;&#39311;&#65288;Collaborative Score Distillation&#65292;CSD&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;SVGD&#65289;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#26679;&#26412;&#20316;&#20026;&#8220;&#31890;&#23376;&#8221;&#24182;&#32467;&#21512;&#23427;&#20204;&#30340;&#20998;&#25968;&#20989;&#25968;&#26469;&#23454;&#29616;&#23545;&#19968;&#32452;&#22270;&#20687;&#30340;&#29983;&#25104;&#20808;&#39564;&#36827;&#34892;&#21516;&#27493;&#33976;&#39311;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#26679;&#26412;&#20043;&#38388;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#35270;&#35273;&#21512;&#25104;&#12290;CSD&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20840;&#26223;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#22330;&#26223;&#30340;&#35270;&#35273;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#20808;&#39564;&#20351;&#24471;&#22312;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#27169;&#24577;&#19978;&#26377;&#20102;&#24191;&#27867;&#30340;&#26032;&#29983;&#25104;&#21644;&#32534;&#36753;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#20123;&#20808;&#39564;&#36866;&#24212;&#20110;&#22797;&#26434;&#30340;&#35270;&#35273;&#27169;&#24577;&#65292;&#36890;&#24120;&#34920;&#31034;&#20026;&#22810;&#20010;&#22270;&#20687;&#65288;&#20363;&#22914;&#35270;&#39057;&#65289;&#26102;&#65292;&#23454;&#29616;&#19968;&#32452;&#22270;&#20687;&#30340;&#19968;&#33268;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21327;&#21516;&#20998;&#25968;&#33976;&#39311;&#65288;CSD&#65289;&#12290;CSD&#22522;&#20110;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;SVGD&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#22810;&#20010;&#26679;&#26412;&#35270;&#20026;SVGD&#26356;&#26032;&#20013;&#30340;&#8220;&#31890;&#23376;&#8221;&#65292;&#24182;&#32467;&#21512;&#23427;&#20204;&#30340;&#20998;&#25968;&#20989;&#25968;&#20197;&#21516;&#26102;&#33976;&#39311;&#19968;&#32452;&#22270;&#20687;&#19978;&#30340;&#29983;&#25104;&#20808;&#39564;&#12290;&#22240;&#27492;&#65292;CSD&#20419;&#36827;&#20102;2D&#22270;&#20687;&#20043;&#38388;&#20449;&#24687;&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22810;&#20010;&#26679;&#26412;&#30340;&#19968;&#33268;&#35270;&#35273;&#21512;&#25104;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CSD&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20840;&#26223;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#22330;&#26223;&#30340;&#35270;&#35273;&#32534;&#36753;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;CSD&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative priors of large-scale text-to-image diffusion models enable a wide range of new generation and editing applications on diverse visual modalities. However, when adapting these priors to complex visual modalities, often represented as multiple images (e.g., video), achieving consistency across a set of images is challenging. In this paper, we address this challenge with a novel method, Collaborative Score Distillation (CSD). CSD is based on the Stein Variational Gradient Descent (SVGD). Specifically, we propose to consider multiple samples as "particles" in the SVGD update and combine their score functions to distill generative priors over a set of images synchronously. Thus, CSD facilitates seamless integration of information across 2D images, leading to a consistent visual synthesis across multiple samples. We show the effectiveness of CSD in a variety of tasks, encompassing the visual editing of panorama images, videos, and 3D scenes. Our results underline the competency of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#28857;&#20113;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#26469;&#36827;&#34892;&#37327;&#28909;&#22120;&#24555;&#36895;&#27169;&#25311;&#65292;&#21457;&#29616;&#28857;&#20113;&#26356;&#33258;&#28982;&#22320;&#34920;&#31034;&#20102;&#37327;&#28909;&#22120;&#28107;&#28020;&#65292;&#22788;&#29702;&#31232;&#30095;&#25968;&#25454;&#38598;&#26356;&#20248;&#31168;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#26356;&#32039;&#20945;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#25991;&#20214;&#36827;&#34892;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.04780</link><description>&lt;p&gt;
&#28857;&#20113;&#19982;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#22312;&#37327;&#28909;&#22120;&#24555;&#36895;&#27169;&#25311;&#20013;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of Point Cloud and Image-based Models for Calorimeter Fast Simulation. (arXiv:2307.04780v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#28857;&#20113;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#26469;&#36827;&#34892;&#37327;&#28909;&#22120;&#24555;&#36895;&#27169;&#25311;&#65292;&#21457;&#29616;&#28857;&#20113;&#26356;&#33258;&#28982;&#22320;&#34920;&#31034;&#20102;&#37327;&#28909;&#22120;&#28107;&#28020;&#65292;&#22788;&#29702;&#31232;&#30095;&#25968;&#25454;&#38598;&#26356;&#20248;&#31168;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#26356;&#32039;&#20945;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#25991;&#20214;&#36827;&#34892;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#65292;&#24050;&#34987;&#35777;&#26126;&#33021;&#20934;&#30830;&#29983;&#25104;&#39640;&#32500;&#24230;&#30340;&#37327;&#28909;&#22120;&#25968;&#25454;&#38598;&#12290;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#29992;&#22270;&#20687;&#19982;3D&#20307;&#32032;&#34920;&#31034;&#21644;&#24314;&#27169;&#22797;&#26434;&#30340;&#37327;&#28909;&#22120;&#28107;&#28020;&#12290;&#28982;&#32780;&#65292;&#28857;&#20113;&#21487;&#33021;&#26159;&#37327;&#28909;&#22120;&#28107;&#28020;&#26356;&#33258;&#28982;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#39640;&#31890;&#24230;&#30340;&#37327;&#28909;&#22120;&#20013;&#12290;&#28857;&#20113;&#20445;&#30041;&#20102;&#21407;&#22987;&#27169;&#25311;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#26356;&#33258;&#28982;&#22320;&#22788;&#29702;&#31232;&#30095;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#26356;&#32039;&#20945;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#25991;&#20214;&#36827;&#34892;&#23454;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#29992;&#21516;&#19968;&#32452;&#37327;&#28909;&#22120;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#30452;&#25509;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score based generative models are a new class of generative models that have been shown to accurately generate high dimensional calorimeter datasets. Recent advances in generative models have used images with 3D voxels to represent and model complex calorimeter showers. Point clouds, however, are likely a more natural representation of calorimeter showers, particularly in calorimeters with high granularity. Point clouds preserve all of the information of the original simulation, more naturally deal with sparse datasets, and can be implemented with more compact models and data files. In this work, two state-of-the-art score based models are trained on the same set of calorimeter simulation and directly compared.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20998;&#26512;&#30340;&#25112;&#30053;&#35745;&#21010;&#65292;&#36890;&#36807;&#30740;&#31350;&#37329;&#34701;&#20844;&#21496;LendingClub&#30340;&#23454;&#38469;&#26696;&#20363;&#65292;&#25506;&#32034;&#24341;&#20837;&#22823;&#25968;&#25454;&#24179;&#21488;&#21644;&#20808;&#36827;&#29305;&#24449;&#36873;&#25321;&#33021;&#21147;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#22686;&#21152;&#25910;&#20837;&#24182;&#38477;&#20302;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.04778</link><description>&lt;p&gt;
&#22522;&#20110;&#32479;&#35745;&#20998;&#26512;&#21644;&#24212;&#29992;&#31243;&#24207;&#22312;&#37329;&#34701;&#20844;&#21496;&#20013;&#21046;&#23450;&#25112;&#30053;&#35745;&#21010;&#30340;&#19968;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Formulating A Strategic Plan Based On Statistical Analyses And Applications For Financial Companies Through A Real-World Use Case. (arXiv:2307.04778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20998;&#26512;&#30340;&#25112;&#30053;&#35745;&#21010;&#65292;&#36890;&#36807;&#30740;&#31350;&#37329;&#34701;&#20844;&#21496;LendingClub&#30340;&#23454;&#38469;&#26696;&#20363;&#65292;&#25506;&#32034;&#24341;&#20837;&#22823;&#25968;&#25454;&#24179;&#21488;&#21644;&#20808;&#36827;&#29305;&#24449;&#36873;&#25321;&#33021;&#21147;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#22686;&#21152;&#25910;&#20837;&#24182;&#38477;&#20302;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;&#32479;&#35745;&#22312;&#20225;&#19994;&#32423;&#21035;&#23454;&#26045;&#25968;&#25454;&#39537;&#21160;&#30340;&#25112;&#30053;&#35745;&#21010;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20197;&#36816;&#29992;&#21508;&#31181;&#20998;&#26512;&#25163;&#27573;&#65292;&#36825;&#26679;&#30340;&#35745;&#21010;&#30340;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#20225;&#19994;&#25913;&#36827;&#20915;&#31574;&#36807;&#31243;&#25110;&#38477;&#20302;&#32452;&#32455;&#39118;&#38505;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20998;&#26512;&#30340;&#25112;&#30053;&#35745;&#21010;&#65292;&#38024;&#23545;&#19968;&#23478;&#37329;&#34701;&#20844;&#21496;LendingClub&#65292;&#35745;&#21010;&#21253;&#25324;&#25506;&#32034;&#24341;&#20837;&#22823;&#25968;&#25454;&#24179;&#21488;&#21644;&#20808;&#36827;&#30340;&#29305;&#24449;&#36873;&#25321;&#33021;&#21147;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#20010;&#35745;&#21010;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22686;&#21152;&#20844;&#21496;&#30340;&#25910;&#20837;&#65292;&#21516;&#26102;&#20943;&#23569;&#21521;&#26080;&#27861;&#24402;&#36824;&#36151;&#27454;&#30340;&#20511;&#27454;&#20154;&#25480;&#20104;&#36151;&#27454;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#23545;&#20844;&#21496;&#25285;&#24551;&#36827;&#34892;&#20102;&#19981;&#21516;&#20551;&#35774;&#30340;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;&#36151;&#27454;&#37329;&#39069;&#23545;&#26080;&#27861;&#24402;&#36824;&#36151;&#27454;&#30340;&#20511;&#27454;&#20154;&#20154;&#25968;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;&#25552;&#20986;&#30340;&#25112;&#30053;&#35745;&#21010;&#36824;&#21253;&#25324;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#31561;&#20808;&#36827;&#20998;&#26512;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Business statistics play a crucial role in implementing a data-driven strategic plan at the enterprise level to employ various analytics where the outcomes of such a plan enable an enterprise to enhance the decision-making process or to mitigate risks to the organization. In this work, a strategic plan informed by the statistical analysis is introduced for a financial company called LendingClub, where the plan is comprised of exploring the possibility of onboarding a big data platform along with advanced feature selection capacities. The main objectives of such a plan are to increase the company's revenue while reducing the risks of granting loans to borrowers who cannot return their loans. In this study, different hypotheses formulated to address the company's concerns are studied, where the results reveal that the amount of loans profoundly impacts the number of borrowers charging off their loans. Also, the proposed strategic plan includes onboarding advanced analytics such as machin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#24515;&#29702;&#20581;&#24247;&#36319;&#36394;&#19982;&#24773;&#32490;&#39044;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#20010;&#20154;&#20581;&#24247;&#35774;&#22791;&#25910;&#38598;&#30340;&#29983;&#29702;&#25968;&#25454;&#65292;&#36890;&#36807;&#20998;&#25955;&#23398;&#20064;&#26426;&#21046;&#23454;&#29616;&#31934;&#31070;&#30149;&#23398;&#27835;&#30103;&#21644;&#31649;&#29702;&#30340;&#26377;&#25928;&#36861;&#36394;&#65292;&#24182;&#20197;&#38544;&#31169;&#24847;&#35782;&#21644;&#26377;&#36131;&#20219;&#24863;&#30340;&#26041;&#24335;&#25552;&#20379;&#31934;&#31070;&#31185;&#21307;&#29983;&#23545;&#24739;&#32773;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#30340;&#36827;&#19968;&#27493;&#20102;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.04777</link><description>&lt;p&gt;
MentalHealthAI: &#21033;&#29992;&#20010;&#20154;&#20581;&#24247;&#35774;&#22791;&#25968;&#25454;&#20248;&#21270;&#31934;&#31070;&#30149;&#23398;&#27835;&#30103;
&lt;/p&gt;
&lt;p&gt;
MentalHealthAI: Utilizing Personal Health Device Data to Optimize Psychiatry Treatment. (arXiv:2307.04777v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#24515;&#29702;&#20581;&#24247;&#36319;&#36394;&#19982;&#24773;&#32490;&#39044;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#20010;&#20154;&#20581;&#24247;&#35774;&#22791;&#25910;&#38598;&#30340;&#29983;&#29702;&#25968;&#25454;&#65292;&#36890;&#36807;&#20998;&#25955;&#23398;&#20064;&#26426;&#21046;&#23454;&#29616;&#31934;&#31070;&#30149;&#23398;&#27835;&#30103;&#21644;&#31649;&#29702;&#30340;&#26377;&#25928;&#36861;&#36394;&#65292;&#24182;&#20197;&#38544;&#31169;&#24847;&#35782;&#21644;&#26377;&#36131;&#20219;&#24863;&#30340;&#26041;&#24335;&#25552;&#20379;&#31934;&#31070;&#31185;&#21307;&#29983;&#23545;&#24739;&#32773;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#30340;&#36827;&#19968;&#27493;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#21307;&#30103;&#20013;&#65292;&#24515;&#29702;&#20581;&#24247;&#38556;&#30861;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#35786;&#26029;&#21644;&#27835;&#30103;&#24448;&#24448;&#20381;&#36182;&#20110;&#24739;&#32773;&#30340;&#20027;&#35266;&#25551;&#36848;&#21644;&#36807;&#21435;&#30340;&#30149;&#21490;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#24515;&#29702;&#20581;&#24247;&#36319;&#36394;&#19982;&#24773;&#32490;&#39044;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#36890;&#36807;&#20010;&#20154;&#20581;&#24247;&#35774;&#22791;&#25910;&#38598;&#30340;&#24739;&#32773;&#29983;&#29702;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21033;&#29992;&#20102;&#20998;&#25955;&#23398;&#20064;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#36716;&#31227;&#23398;&#20064;&#21644;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#26234;&#33021;&#21512;&#32422;&#65292;&#20351;&#25968;&#25454;&#20445;&#30041;&#22312;&#29992;&#25143;&#35774;&#22791;&#19978;&#65292;&#24182;&#20197;&#38544;&#31169;&#24847;&#35782;&#21644;&#26377;&#36131;&#20219;&#24863;&#30340;&#26041;&#24335;&#26377;&#25928;&#36319;&#36394;&#31934;&#31070;&#30149;&#23398;&#27835;&#30103;&#21644;&#31649;&#29702;&#20013;&#30340;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#27969;&#34892;&#30340;&#24515;&#29702;&#20581;&#24247;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#36830;&#25509;&#30340;&#20581;&#24247;&#31995;&#32479;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#25552;&#20379;&#31934;&#31070;&#31185;&#21307;&#29983;&#23545;&#24739;&#32773;&#30340;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#22806;&#37096;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mental health disorders remain a significant challenge in modern healthcare, with diagnosis and treatment often relying on subjective patient descriptions and past medical history. To address this issue, we propose a personalized mental health tracking and mood prediction system that utilizes patient physiological data collected through personal health devices. Our system leverages a decentralized learning mechanism that combines transfer and federated machine learning concepts using smart contracts, allowing data to remain on users' devices and enabling effective tracking of mental health conditions for psychiatric treatment and management in a privacy-aware and accountable manner. We evaluate our model using a popular mental health dataset that demonstrates promising results. By utilizing connected health systems and machine learning models, our approach offers a novel solution to the challenge of providing psychiatrists with further insight into their patients' mental health outside
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24739;&#32773;&#20581;&#24247;&#25968;&#25454;&#26500;&#24314;&#20026;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#20351;&#29992;&#38381;&#24335;&#36830;&#32493;&#26102;&#38388;&#28082;&#20307;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23454;&#26102;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#30142;&#30149;&#24314;&#27169;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#19968;&#26694;&#26550;&#26377;&#26395;&#25512;&#21160;&#25968;&#23383;&#21452;&#29983;&#25216;&#26415;&#22312;&#24739;&#32773;&#25252;&#29702;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.04772</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#21644;&#38381;&#24335;&#36830;&#32493;&#26102;&#38388;&#28082;&#20307;&#31070;&#32463;&#32593;&#32476;&#30340;&#24739;&#32773;&#25252;&#29702;&#25968;&#23383;&#21452;&#29983;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Digital Twins for Patient Care via Knowledge Graphs and Closed-Form Continuous-Time Liquid Neural Networks. (arXiv:2307.04772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24739;&#32773;&#20581;&#24247;&#25968;&#25454;&#26500;&#24314;&#20026;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#20351;&#29992;&#38381;&#24335;&#36830;&#32493;&#26102;&#38388;&#28082;&#20307;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23454;&#26102;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#30142;&#30149;&#24314;&#27169;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#19968;&#26694;&#26550;&#26377;&#26395;&#25512;&#21160;&#25968;&#23383;&#21452;&#29983;&#25216;&#26415;&#22312;&#24739;&#32773;&#25252;&#29702;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21452;&#29983;&#25216;&#26415;&#34987;&#39044;&#35745;&#23558;&#25913;&#21464;&#21307;&#30103;&#20445;&#20581;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#33647;&#29289;&#21644;&#25903;&#25345;&#12289;&#26089;&#26399;&#35786;&#26029;&#12289;&#27169;&#25311;&#27835;&#30103;&#32467;&#26524;&#21644;&#20248;&#21270;&#25163;&#26415;&#35745;&#21010;&#12290;&#25968;&#23383;&#21452;&#29983;&#25216;&#26415;&#22312;&#21046;&#36896;&#19994;&#12289;&#20379;&#24212;&#38142;&#29289;&#27969;&#21644;&#27665;&#29992;&#22522;&#30784;&#35774;&#26045;&#31561;&#34892;&#19994;&#24050;&#32463;&#33719;&#24471;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#22312;&#24739;&#32773;&#25252;&#29702;&#39046;&#22495;&#23578;&#26410;&#24471;&#21040;&#24212;&#29992;&#12290;&#24314;&#27169;&#22797;&#26434;&#30142;&#30149;&#19982;&#22810;&#27169;&#24577;&#24739;&#32773;&#25968;&#25454;&#20197;&#21450;&#35745;&#31639;&#20998;&#26512;&#30340;&#22797;&#26434;&#24615;&#26159;&#21046;&#32422;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25968;&#23383;&#21452;&#29983;&#25216;&#26415;&#24212;&#29992;&#30340;&#37325;&#35201;&#38556;&#30861;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#19981;&#21516;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#33021;&#22815;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#30001;&#35745;&#31639;&#25104;&#26412;&#21644;&#24314;&#27169;&#22797;&#26434;&#24615;&#36896;&#25104;&#30340;&#20020;&#24202;&#21452;&#29983;&#27169;&#22411;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#24739;&#32773;&#20581;&#24247;&#25968;&#25454;&#26500;&#24314;&#20026;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#20351;&#29992;&#38381;&#24335;&#36830;&#32493;&#26102;&#38388;&#28082;&#20307;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23454;&#26102;&#20998;&#26512;&#12290;&#36890;&#36807;&#32508;&#21512;&#22810;&#27169;&#24577;&#24739;&#32773;&#25968;&#25454;&#24182;&#21033;&#29992;&#36825;&#19968;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#24739;&#32773;&#25252;&#29702;&#30340;&#25968;&#23383;&#21452;&#29983;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital twin technology has is anticipated to transform healthcare, enabling personalized medicines and support, earlier diagnoses, simulated treatment outcomes, and optimized surgical plans. Digital twins are readily gaining traction in industries like manufacturing, supply chain logistics, and civil infrastructure. Not in patient care, however. The challenge of modeling complex diseases with multimodal patient data and the computational complexities of analyzing it have stifled digital twin adoption in the biomedical vertical. Yet, these major obstacles can potentially be handled by approaching these models in a different way. This paper proposes a novel framework for addressing the barriers to clinical twin modeling created by computational costs and modeling complexities. We propose structuring patient health data as a knowledge graph and using closed-form continuous-time liquid neural networks, for real-time analytics. By synthesizing multimodal patient data and leveraging the fle
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#31354;&#20851;&#27880;&#26426;&#21046;&#26469;&#39044;&#27979;&#38271;&#26399;COVID&#24739;&#32773;&#30340;&#32467;&#26524;&#65292;&#35299;&#20915;&#20102;&#20854;&#32437;&#21521;&#25968;&#25454;&#39044;&#27979;&#30340;&#22256;&#38590;&#24615;&#65292;&#24182;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04770</link><description>&lt;p&gt;
&#38024;&#23545;&#38271;&#26399;COVID&#30149;&#24739;&#32773;&#30340;&#26102;&#31354;&#20851;&#27880;&#26426;&#21046;&#39044;&#27979;&#32467;&#26524;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Predicting Outcomes in Long COVID Patients with Spatiotemporal Attention. (arXiv:2307.04770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#31354;&#20851;&#27880;&#26426;&#21046;&#26469;&#39044;&#27979;&#38271;&#26399;COVID&#24739;&#32773;&#30340;&#32467;&#26524;&#65292;&#35299;&#20915;&#20102;&#20854;&#32437;&#21521;&#25968;&#25454;&#39044;&#27979;&#30340;&#22256;&#38590;&#24615;&#65292;&#24182;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;COVID&#26159;&#25351;COVID-19&#21518;&#20986;&#29616;&#30340;&#38271;&#26399;&#21518;&#36951;&#30151;&#30340;&#19968;&#33324;&#26415;&#35821;&#12290;&#38271;&#26399;COVID&#24739;&#32773;&#21487;&#33021;&#32463;&#21382;&#25345;&#20037;&#30340;&#30151;&#29366;&#65292;&#21253;&#25324;&#30130;&#21171;&#12289;&#22836;&#30171;&#12289;&#21628;&#21560;&#22256;&#38590;&#21644;&#21957;&#35273;&#20007;&#22833;&#31561;&#12290;&#20174;&#38271;&#26399;COVID&#24739;&#32773;&#30340;&#32437;&#21521;&#25968;&#25454;&#20013;&#39044;&#27979;&#20854;&#32467;&#26524;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#24739;&#32773;&#21576;&#29616;&#20986;&#24322;&#36136;&#30340;&#34920;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#31354;&#20851;&#27880;&#26426;&#21046;&#65292;&#20174;&#26102;&#38388;&#32500;&#24230;&#21644;&#29305;&#24449;&#31354;&#38388;&#20849;&#21516;&#26435;&#34913;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#32771;&#34385;&#21040;&#21307;&#23398;&#26816;&#26597;&#22312;&#30456;&#37051;&#26102;&#38388;&#28857;&#21487;&#33021;&#26377;&#21487;&#20114;&#25442;&#30340;&#39034;&#24207;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26412;&#22320;LSTM&#38480;&#21046;&#30701;&#26399;&#20381;&#36182;&#24615;&#30340;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#32852;&#21512;&#26102;&#31354;&#20851;&#27880;&#26426;&#21046;&#26469;&#23398;&#20064;&#38271;&#26399;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#20020;&#24202;&#23454;&#36341;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30740;&#31350;&#20013;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long COVID is a general term of post-acute sequelae of COVID-19. Patients with long COVID can endure long-lasting symptoms including fatigue, headache, dyspnea and anosmia, etc. Identifying the cohorts with severe long-term complications in COVID-19 could benefit the treatment planning and resource arrangement. However, due to the heterogeneous phenotype presented in long COVID patients, it is difficult to predict their outcomes from their longitudinal data. In this study, we proposed a spatiotemporal attention mechanism to weigh feature importance jointly from the temporal dimension and feature space. Considering that medical examinations can have interchangeable orders in adjacent time points, we restricted the learning of short-term dependency with a Local-LSTM and the learning of long-term dependency with the joint spatiotemporal attention. We also compared the proposed method with several state-of-the-art methods and a method in clinical practice. The methods are evaluated on a ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#32479;&#35745;&#23398;&#20064;&#26102;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#22810;&#20010;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#23548;&#20986;&#32039;&#23494;&#21305;&#37197;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#20809;&#28369;&#12289;&#24378;&#20984;&#21644;&#28385;&#36275;Polyak-Lojasiewicz&#20551;&#35774;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04679</link><description>&lt;p&gt;
&#19968;&#38454;&#26041;&#27861;&#22312;&#20855;&#26377;&#36890;&#29992;&#39044;&#27979;&#22120;&#30340;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generalization Error of First-Order Methods for Statistical Learning with Generic Oracles. (arXiv:2307.04679v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#32479;&#35745;&#23398;&#20064;&#26102;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#22810;&#20010;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#23548;&#20986;&#32039;&#23494;&#21305;&#37197;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#20809;&#28369;&#12289;&#24378;&#20984;&#21644;&#28385;&#36275;Polyak-Lojasiewicz&#20551;&#35774;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#22312;&#32479;&#35745;&#23398;&#20064;&#20013;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#26102;&#65292;&#24403;&#26799;&#24230;&#21482;&#33021;&#36890;&#36807;&#39044;&#27979;&#22120;&#32473;&#20986;&#30340;&#37096;&#20998;&#35266;&#27979;&#26469;&#35775;&#38382;&#26102;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20381;&#36182;&#20110;&#26799;&#24230;&#23545;&#25968;&#25454;&#26679;&#26412;&#30340;&#35268;&#21017;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#23548;&#20986;&#22810;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#32039;&#23494;&#21305;&#37197;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#65292;&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#40065;&#26834;&#23398;&#20064;&#12289;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#20351;&#29992;&#26799;&#24230;&#37327;&#21270;&#30340;&#36890;&#20449;&#25928;&#29575;&#23398;&#20064;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#20809;&#28369;&#19988;&#24378;&#20984;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#21450;&#28385;&#36275;Polyak-Lojasiewicz&#20551;&#35774;&#30340;&#20809;&#28369;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#20381;&#36182;&#20110;&#19968;&#20010;&#25193;&#23637;&#20102;&#26465;&#20214;&#26631;&#20934;&#24046;&#27010;&#24565;&#30340;&#26032;&#37327;&#65292;&#23427;&#34913;&#37327;&#20102;&#36890;&#36807;&#35775;&#38382;&#39044;&#27979;&#22120;&#21487;&#20197;&#36817;&#20284;&#26799;&#24230;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a novel framework for the analysis of generalization error of first-order optimization algorithms for statistical learning when the gradient can only be accessed through partial observations given by an oracle. Our analysis relies on the regularity of the gradient w.r.t. the data samples, and allows to derive near matching upper and lower bounds for the generalization error of multiple learning problems, including supervised learning, transfer learning, robust learning, distributed learning and communication efficient learning using gradient quantization. These results hold for smooth and strongly-convex optimization problems, as well as smooth non-convex optimization problems verifying a Polyak-Lojasiewicz assumption. In particular, our upper and lower bounds depend on a novel quantity that extends the notion of conditional standard deviation, and is a measure of the extent to which the gradient can be approximated by having access to the oracle. As a consequ
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#26799;&#24230;&#26469;&#33258;&#21160;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#23485;&#24230;&#21644;&#28145;&#24230;&#65292;&#20197;&#22312;&#35757;&#32451;&#25439;&#22833;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.04526</link><description>&lt;p&gt;
&#33258;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Self Expanding Neural Networks. (arXiv:2307.04526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04526
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#26799;&#24230;&#26469;&#33258;&#21160;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#23485;&#24230;&#21644;&#28145;&#24230;&#65292;&#20197;&#22312;&#35757;&#32451;&#25439;&#22833;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#32467;&#26524;&#20005;&#37325;&#20381;&#36182;&#20110;&#25152;&#36873;&#25321;&#30340;&#26550;&#26500;&#65307;&#21363;&#20351;&#21482;&#26159;&#23545;&#32593;&#32476;&#22823;&#23567;&#20570;&#24494;&#23567;&#20462;&#25913;&#65292;&#36890;&#24120;&#20063;&#38656;&#35201;&#37325;&#26032;&#24320;&#22987;&#35757;&#32451;&#36807;&#31243;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#20197;&#19968;&#20010;&#23567;&#30340;&#26550;&#26500;&#24320;&#22987;&#35757;&#32451;&#65292;&#21482;&#22312;&#38382;&#39064;&#38656;&#35201;&#26102;&#25193;&#23637;&#20854;&#23481;&#37327;&#65292;&#24182;&#36991;&#20813;&#24178;&#25200;&#20808;&#21069;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#24403;&#36825;&#26679;&#20570;&#21487;&#33021;&#22823;&#24133;&#38477;&#20302;&#20551;&#35774;&#25910;&#25947;&#35757;&#32451;&#25439;&#22833;&#26102;&#65292;&#30452;&#35266;&#22320;&#25193;&#23637;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#23485;&#24230;&#21644;&#28145;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31070;&#32463;&#20803;&#28155;&#21152;&#30340;&#8220;&#36895;&#29575;&#8221;&#19978;&#30028;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#35745;&#31639;&#24265;&#20215;&#30340;&#25193;&#23637;&#35780;&#20998;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#33258;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#37027;&#20123;&#21512;&#36866;&#30340;&#26550;&#26500;&#22823;&#23567;&#22312;&#20808;&#39564;&#19978;&#30456;&#24403;&#19981;&#30830;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The results of training a neural network are heavily dependent on the architecture chosen; and even a modification of only the size of the network, however small, typically involves restarting the training process. In contrast to this, we begin training with a small architecture, only increase its capacity as necessary for the problem, and avoid interfering with previous optimization while doing so. We thereby introduce a natural gradient based approach which intuitively expands both the width and depth of a neural network when this is likely to substantially reduce the hypothetical converged training loss. We prove an upper bound on the "rate" at which neurons are added, and a computationally cheap lower bound on the expansion score. We illustrate the benefits of such Self-Expanding Neural Networks in both classification and regression problems, including those where the appropriate architecture size is substantially uncertain a priori.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;SRRD&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26131;&#20110;&#33719;&#24471;&#30340;CT&#22270;&#20687;&#36827;&#34892;&#33181;&#20851;&#33410;&#39592;&#24615;&#39592;&#36136;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#37197;&#23545;&#30340;MR&#22270;&#20687;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#20998;&#26512;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#26377;&#26395;&#22312;&#33181;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#30340;&#20020;&#24202;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.04390</link><description>&lt;p&gt;
&#22522;&#20110;CT&#30340;&#33181;&#20851;&#33410;&#39592;&#24615;&#39592;&#36136;&#20998;&#26512;&#22312;&#33181;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#20013;&#30340;&#24212;&#29992;&#65288;&#36890;&#36807;MR&#24341;&#23548;&#30340;&#33976;&#39311;&#23398;&#20064;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
CT-based Subchondral Bone Microstructural Analysis in Knee Osteoarthritis via MR-Guided Distillation Learning. (arXiv:2307.04390v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;SRRD&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26131;&#20110;&#33719;&#24471;&#30340;CT&#22270;&#20687;&#36827;&#34892;&#33181;&#20851;&#33410;&#39592;&#24615;&#39592;&#36136;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#37197;&#23545;&#30340;MR&#22270;&#20687;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#20998;&#26512;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#26377;&#26395;&#22312;&#33181;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#30340;&#20020;&#24202;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22522;&#20110;MR&#30340;&#39592;&#24615;&#39592;&#36136;&#26377;&#25928;&#39044;&#27979;&#33181;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#12290;&#28982;&#32780;&#65292;&#20854;&#20020;&#24202;&#24212;&#29992;&#21463;&#21040;MR&#25104;&#26412;&#21644;&#26102;&#38388;&#30340;&#38480;&#21046;&#12290;&#30446;&#30340;&#65306;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#21517;&#20026;SRRD&#30340;&#22522;&#20110;&#33976;&#39311;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#26131;&#20110;&#33719;&#24471;&#30340;CT&#22270;&#20687;&#36827;&#34892;&#33181;&#20851;&#33410;&#39592;&#24615;&#39592;&#36136;&#20998;&#26512;&#65292;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#37197;&#23545;&#30340;MR&#22270;&#20687;&#22686;&#24378;CT&#22270;&#20687;&#20998;&#26512;&#27169;&#22411;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#20174;2020&#24180;10&#26376;&#21040;2021&#24180;5&#26376;&#25910;&#38598;&#20102;CT&#21644;MR&#21452;&#27169;&#24577;&#30340;&#33181;&#20851;&#33410;&#22270;&#20687;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;GAN&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;MR&#22270;&#20687;&#36716;&#25442;&#20026;CT&#22270;&#20687;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#24314;&#31435;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#35299;&#21078;&#23545;&#24212;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22238;&#24402;&#20174;&#30456;&#24212;&#30340;CT&#22270;&#20687;&#22359;&#20013;&#33719;&#21462;&#20102;&#22823;&#37327;&#39592;&#24615;&#39592;&#36136;&#21306;&#22495;&#30340;MR&#22270;&#20687;&#22359;&#20197;&#21450;&#20854;&#26753;&#32467;&#26500;&#21442;&#25968;&#65288;BV / TV&#65292;Tb. Th&#65292;Tb. Sp&#65292;Tb. N&#65289;&#12290;&#33976;&#39311;&#23398;&#20064;&#25216;&#26415;&#29992;&#20110;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#24182;&#20256;&#36755;MR&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: MR-based subchondral bone effectively predicts knee osteoarthritis. However, its clinical application is limited by the cost and time of MR. Purpose: We aim to develop a novel distillation-learning-based method named SRRD for subchondral bone microstructural analysis using easily-acquired CT images, which leverages paired MR images to enhance the CT-based analysis model during training. Materials and Methods: Knee joint images of both CT and MR modalities were collected from October 2020 to May 2021. Firstly, we developed a GAN-based generative model to transform MR images into CT images, which was used to establish the anatomical correspondence between the two modalities. Next, we obtained numerous patches of subchondral bone regions of MR images, together with their trabecular parameters (BV / TV, Tb. Th, Tb. Sp, Tb. N) from the corresponding CT image patches via regression. The distillation-learning technique was used to train the regression model and transfer MR structu
&lt;/p&gt;</description></item><item><title>DEFT&#26159;&#19968;&#31181;&#21033;&#29992;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#26799;&#24230;&#33539;&#25968;&#24046;&#24322;&#26469;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#27969;&#37327;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#26799;&#24230;&#32047;&#31215;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.03500</link><description>&lt;p&gt;
DEFT:&#21033;&#29992;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#26799;&#24230;&#33539;&#25968;&#24046;&#24322;&#26469;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
DEFT: Exploiting Gradient Norm Difference between Model Layers for Scalable Gradient Sparsification. (arXiv:2307.03500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03500
&lt;/p&gt;
&lt;p&gt;
DEFT&#26159;&#19968;&#31181;&#21033;&#29992;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#26799;&#24230;&#33539;&#25968;&#24046;&#24322;&#26469;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#27969;&#37327;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#26799;&#24230;&#32047;&#31215;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#31232;&#30095;&#21270;&#26159;&#20943;&#23569;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#36807;&#22810;&#36890;&#20449;&#27969;&#37327;&#30340;&#24191;&#27867;&#24212;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;&#26041;&#27861;&#30001;&#20110;&#26799;&#24230;&#36873;&#25321;&#30340;&#35745;&#31639;&#25104;&#26412;&#30456;&#24403;&#22823;&#21644;&#26799;&#24230;&#32047;&#31215;&#22686;&#21152;&#30340;&#36890;&#20449;&#27969;&#37327;&#65292;&#20854;&#21487;&#25193;&#23637;&#24615;&#30456;&#23545;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;&#26041;&#26696;DEFT&#65292;&#23558;&#26799;&#24230;&#36873;&#25321;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#20998;&#37197;&#32473;&#24037;&#20316;&#33410;&#28857;&#12290; DEFT&#19982;&#29616;&#26377;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#19981;&#21516;&#65292;&#27599;&#20010;&#24037;&#20316;&#33410;&#28857;&#20165;&#20174;&#25152;&#26377;&#26799;&#24230;&#20013;&#36873;&#25321;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#38543;&#30528;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#35745;&#31639;&#25104;&#26412;&#21487;&#20197;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;DEFT&#20801;&#35768;&#24037;&#20316;&#33410;&#28857;&#22312;&#38750;&#20132;&#21449;&#30340;&#20998;&#21306;&#20013;&#36873;&#25321;&#26799;&#24230;&#65292;&#22240;&#27492;&#21363;&#20351;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#65292;&#36890;&#20449;&#27969;&#37327;&#20063;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35201;&#27714;&#36827;&#34892;&#32500;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient sparsification is a widely adopted solution for reducing the excessive communication traffic in distributed deep learning. However, most existing gradient sparsifiers have relatively poor scalability because of considerable computational cost of gradient selection and/or increased communication traffic owing to gradient build-up. To address these challenges, we propose a novel gradient sparsification scheme, DEFT, that partitions the gradient selection task into sub tasks and distributes them to workers. DEFT differs from existing sparsifiers, wherein every worker selects gradients among all gradients. Consequently, the computational cost can be reduced as the number of workers increases. Moreover, gradient build-up can be eliminated because DEFT allows workers to select gradients in partitions that are non-intersecting (between workers). Therefore, even if the number of workers increases, the communication traffic can be maintained as per user requirement.  To avoid the loss 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#30340;&#20132;&#20114;&#24037;&#20855;QI2&#65292;&#35813;&#24037;&#20855;&#25903;&#25345;&#23545;&#22810;&#20010;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#39564;&#35777;&#21644;&#23450;&#37327;&#25968;&#25454;&#36136;&#37327;&#35201;&#27714;&#30340;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#28436;&#31034;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.03419</link><description>&lt;p&gt;
QI2 -- &#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#30340;&#20132;&#20114;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
QI2 -- an Interactive Tool for Data Quality Assurance. (arXiv:2307.03419v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#30340;&#20132;&#20114;&#24037;&#20855;QI2&#65292;&#35813;&#24037;&#20855;&#25903;&#25345;&#23545;&#22810;&#20010;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#39564;&#35777;&#21644;&#23450;&#37327;&#25968;&#25454;&#36136;&#37327;&#35201;&#27714;&#30340;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#28436;&#31034;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25968;&#25454;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21644;&#22823;&#25968;&#25454;&#30340;&#22686;&#38271;&#24433;&#21709;&#21644;&#20998;&#24067;&#32780;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#27431;&#27954;&#22996;&#21592;&#20250;&#35745;&#21010;&#30340;AI&#27861;&#26696;&#20026;&#25968;&#25454;&#36136;&#37327;&#23450;&#20041;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27861;&#24459;&#35201;&#27714;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24066;&#22330;&#25512;&#20986;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25903;&#25345;&#22810;&#20010;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#36807;&#31243;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#39564;&#35777;&#23450;&#37327;&#25968;&#25454;&#36136;&#37327;&#35201;&#27714;&#12290;&#36890;&#36807;&#23567;&#20363;&#23376;&#25968;&#25454;&#38598;&#20171;&#32461;&#21644;&#35299;&#37322;&#20102;&#35813;&#27010;&#24565;&#21644;&#20248;&#21183;&#12290;&#22914;&#20309;&#24212;&#29992;&#35813;&#26041;&#27861;&#22312;&#22522;&#20110;&#25163;&#20889;&#25968;&#23383;&#30340;&#30693;&#21517;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The importance of high data quality is increasing with the growing impact and distribution of ML systems and big data. Also the planned AI Act from the European commission defines challenging legal requirements for data quality especially for the market introduction of safety relevant ML systems. In this paper we introduce a novel approach that supports the data quality assurance process of multiple data quality aspects. This approach enables the verification of quantitative data quality requirements. The concept and benefits are introduced and explained on small example data sets. How the method is applied is demonstrated on the well known MNIST data set based an handwritten digits.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26159;&#23545;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#36827;&#34892;&#30340;&#19968;&#27425;&#39318;&#27425;&#30340;&#22836;&#23545;&#22836;&#27604;&#36739;&#65292;&#30740;&#31350;&#20102;&#20381;&#36182;&#23646;&#24615;&#12289;&#23481;&#24525;&#22122;&#22768;&#21644;&#30450;&#30446;&#23646;&#24615;&#30340;&#31639;&#27861;&#22312;&#39044;&#27979;&#24615;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#30450;&#30446;&#23646;&#24615;&#21644;&#23481;&#24525;&#22122;&#22768;&#30340;&#20844;&#24179;&#20998;&#31867;&#22120;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.03306</link><description>&lt;p&gt;
&#24403;&#20844;&#24179;&#20998;&#31867;&#36935;&#21040;&#22024;&#26434;&#30340;&#20445;&#25252;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
When Fair Classification Meets Noisy Protected Attributes. (arXiv:2307.03306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03306
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26159;&#23545;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#36827;&#34892;&#30340;&#19968;&#27425;&#39318;&#27425;&#30340;&#22836;&#23545;&#22836;&#27604;&#36739;&#65292;&#30740;&#31350;&#20102;&#20381;&#36182;&#23646;&#24615;&#12289;&#23481;&#24525;&#22122;&#22768;&#21644;&#30450;&#30446;&#23646;&#24615;&#30340;&#31639;&#27861;&#22312;&#39044;&#27979;&#24615;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#30450;&#30446;&#23646;&#24615;&#21644;&#23481;&#24525;&#22122;&#22768;&#30340;&#20844;&#24179;&#20998;&#31867;&#22120;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#23454;&#26045;&#38754;&#20020;&#30528;&#35768;&#22810;&#23454;&#38469;&#25361;&#25112;&#65292;&#20854;&#20013;&#20043;&#19968;&#23601;&#26159;&#25968;&#25454;&#38598;&#20013;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#21487;&#29992;&#24615;&#25110;&#21487;&#38752;&#24615;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#20013;&#65292;&#23454;&#38469;&#21644;&#27861;&#24459;&#19978;&#30340;&#38556;&#30861;&#21487;&#33021;&#20250;&#38459;&#27490;&#25910;&#38598;&#21644;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#65292;&#20351;&#24471;&#30830;&#20445;&#31639;&#27861;&#20844;&#24179;&#24615;&#21464;&#24471;&#22256;&#38590;&#12290;&#23613;&#31649;&#26368;&#21021;&#30340;&#20844;&#24179;&#31639;&#27861;&#27809;&#26377;&#32771;&#34385;&#36825;&#20123;&#38480;&#21046;&#65292;&#20294;&#26368;&#36817;&#30340;&#25552;&#35758;&#26088;&#22312;&#36890;&#36807;&#23558;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#22024;&#26434;&#24615;&#32435;&#20837;&#32771;&#34385;&#25110;&#26681;&#26412;&#19981;&#20351;&#29992;&#21463;&#20445;&#25252;&#23646;&#24615;&#26469;&#23454;&#29616;&#20998;&#31867;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23545;&#27604;&#22522;&#20110;&#23646;&#24615;&#12289;&#23481;&#24525;&#22122;&#22768;&#21644;&#30450;&#30446;&#23646;&#24615;&#30340;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#22312;&#39044;&#27979;&#24615;&#21644;&#20844;&#24179;&#24615;&#36825;&#20004;&#20010;&#26041;&#38754;&#36827;&#34892;&#22836;&#23545;&#22836;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22235;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25200;&#21160;&#30340;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30450;&#30446;&#23646;&#24615;&#21644;&#23481;&#24525;&#22122;&#22768;&#30340;&#20844;&#24179;&#20998;&#31867;&#22120;&#21487;&#33021;&#20250;&#22312;&#39044;&#27979;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#21452;&#37325;&#36724;&#19978;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The operationalization of algorithmic fairness comes with several practical challenges, not the least of which is the availability or reliability of protected attributes in datasets. In real-world contexts, practical and legal impediments may prevent the collection and use of demographic data, making it difficult to ensure algorithmic fairness. While initial fairness algorithms did not consider these limitations, recent proposals aim to achieve algorithmic fairness in classification by incorporating noisiness in protected attributes or not using protected attributes at all.  To the best of our knowledge, this is the first head-to-head study of fair classification algorithms to compare attribute-reliant, noise-tolerant and attribute-blind algorithms along the dual axes of predictivity and fairness. We evaluated these algorithms via case studies on four real-world datasets and synthetic perturbations. Our study reveals that attribute-blind and noise-tolerant fair classifiers can potentia
&lt;/p&gt;</description></item><item><title>FLuID&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#21464;&#24615;&#20002;&#22833;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#24615;&#33021;&#36739;&#20302;&#35774;&#22791;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#35757;&#32451;&#26694;&#26550;&#12290;&#36890;&#36807;&#21160;&#24577;&#24179;&#34913;&#35757;&#32451;&#36127;&#36733;&#65292;FLuID&#33021;&#26377;&#25928;&#22320;&#20943;&#36731;&#38459;&#22622;&#35774;&#22791;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.02623</link><description>&lt;p&gt;
FLuID: &#20351;&#29992;&#19981;&#21464;&#24615;&#20002;&#22833;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38459;&#22622;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout. (arXiv:2307.02623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02623
&lt;/p&gt;
&lt;p&gt;
FLuID&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#21464;&#24615;&#20002;&#22833;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#24615;&#33021;&#36739;&#20302;&#35774;&#22791;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#35757;&#32451;&#26694;&#26550;&#12290;&#36890;&#36807;&#21160;&#24577;&#24179;&#34913;&#35757;&#32451;&#36127;&#36733;&#65292;FLuID&#33021;&#26377;&#25928;&#22320;&#20943;&#36731;&#38459;&#22622;&#35774;&#22791;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20010;&#20307;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#20849;&#20139;&#26381;&#21153;&#22120;&#21516;&#27493;&#27169;&#22411;&#26356;&#26032;&#12290;&#36825;&#31181;&#26041;&#27861;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#65292;&#20294;&#20063;&#30001;&#20110;&#19981;&#21516;&#35774;&#22791;&#30340;&#24615;&#33021;&#24046;&#24322;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#24322;&#26500;&#30340;&#35757;&#32451;&#29615;&#22659;&#12290;&#22240;&#27492;&#65292;&#22312;FL&#20013;&#65292;&#24615;&#33021;&#36739;&#20302;&#30340;&#38459;&#22622;&#35774;&#22791;&#32463;&#24120;&#20915;&#23450;&#25972;&#20307;&#35757;&#32451;&#26102;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#21160;&#24577;&#24179;&#34913;&#35757;&#32451;&#36127;&#36733;&#26469;&#20943;&#36731;&#30001;&#20110;&#38459;&#22622;&#22120;&#20135;&#29983;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21464;&#24615;&#20002;&#22833;&#65292;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#26356;&#26032;&#38408;&#20540;&#25552;&#21462;&#23376;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#23545;&#20934;&#30830;&#24615;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#22312;&#27492;&#20002;&#22833;&#25216;&#26415;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35757;&#32451;&#26694;&#26550;FLuID&#12290;FLuID&#25552;&#20379;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#23376;&#27169;&#22411;&#25552;&#21462;&#26041;&#27861;&#26469;&#35843;&#33410;&#35745;&#31639;&#24378;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#38459;&#22622;&#35774;&#22791;&#30340;&#36127;&#36733;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) allows machine learning models to train locally on individual mobile devices, synchronizing model updates via a shared server. This approach safeguards user privacy; however, it also generates a heterogeneous training environment due to the varying performance capabilities across devices. As a result, straggler devices with lower performance often dictate the overall training time in FL. In this work, we aim to alleviate this performance bottleneck due to stragglers by dynamically balancing the training load across the system. We introduce Invariant Dropout, a method that extracts a sub-model based on the weight update threshold, thereby minimizing potential impacts on accuracy. Building on this dropout technique, we develop an adaptive training framework, Federated Learning using Invariant Dropout (FLuID). FLuID offers a lightweight sub-model extraction to regulate computational intensity, thereby reducing the load on straggler devices without affecting model q
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24212;&#29992;&#25968;&#25454;&#24037;&#31243;&#26041;&#27861;&#35299;&#20915;&#24494;&#29983;&#29289;&#32452;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#39640;&#32500;&#24230;&#38382;&#39064;&#65292;&#38024;&#23545;&#22218;&#24615;&#32420;&#32500;&#21270;&#23156;&#20799;&#30340;&#25968;&#25454;&#38598;&#23454;&#26045;&#20102;&#22235;&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#39640;&#20102;&#21307;&#23398;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00033</link><description>&lt;p&gt;
&#24212;&#29992;&#25968;&#25454;&#24037;&#31243;&#26041;&#27861;&#35299;&#20915;&#24494;&#29983;&#29289;&#32452;&#25968;&#25454;&#22312;&#21307;&#23398;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Application of data engineering approaches to address challenges in microbiome data for optimal medical decision-making. (arXiv:2307.00033v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24212;&#29992;&#25968;&#25454;&#24037;&#31243;&#26041;&#27861;&#35299;&#20915;&#24494;&#29983;&#29289;&#32452;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#39640;&#32500;&#24230;&#38382;&#39064;&#65292;&#38024;&#23545;&#22218;&#24615;&#32420;&#32500;&#21270;&#23156;&#20799;&#30340;&#25968;&#25454;&#38598;&#23454;&#26045;&#20102;&#22235;&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#39640;&#20102;&#21307;&#23398;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23427;&#20204;&#19982;&#22810;&#20010;&#22120;&#23448;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20154;&#20307;&#32928;&#36947;&#24494;&#29983;&#29289;&#32676;&#33853;&#24050;&#34987;&#35777;&#26126;&#23545;&#26426;&#20307;&#30340;&#20247;&#22810;&#29983;&#29702;&#21151;&#33021;&#36215;&#30528;&#36129;&#29486;&#65292;&#24182;&#19988;&#36824;&#19982;&#19968;&#31995;&#21015;&#30149;&#29702;&#26465;&#20214;&#26377;&#20851;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#30340;&#22823;&#37327;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#25552;&#20379;&#20102;&#26377;&#20851;&#32928;&#36947;&#24494;&#29983;&#29289;&#32676;&#33853;&#30340;&#30456;&#23545;&#20998;&#31867;&#20998;&#24067;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#23454;&#29616;&#20010;&#20307;&#21270;&#21307;&#23398;&#12290;&#28982;&#32780;&#65292;&#24494;&#29983;&#29289;&#32452;&#25968;&#25454;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#39640;&#32500;&#24230;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#24517;&#39035;&#21152;&#20197;&#35299;&#20915;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#25968;&#25454;&#24037;&#31243;&#31639;&#27861;&#26469;&#35299;&#20915;&#19982;&#24494;&#29983;&#29289;&#32452;&#25968;&#25454;&#30456;&#20851;&#30340;&#19978;&#36848;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#20197;&#22218;&#24615;&#32420;&#32500;&#21270;&#23156;&#20799;&#30340;&#20808;&#21069;&#21457;&#34920;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#22235;&#31181;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65288;&#36923;&#36753;&#22238;&#24402; (LR)&#12289;&#25903;&#25345;&#21521;&#37327;&#26426; (SVM)&#12289;&#38543;&#26426;&#26862;&#26519; (RF) &#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319; (XGB) &#20915;&#31574;&#26641;&#65289;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25968;&#25454;&#24037;&#31243;&#26041;&#27861;&#33021;&#26377;&#25928;&#35299;&#20915;&#24494;&#29983;&#29289;&#32452;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#39640;&#32500;&#24230;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#21307;&#23398;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human gut microbiota is known to contribute to numerous physiological functions of the body through their interplay with multiple organs and also implicated in a myriad of pathological conditions. Prolific research work in the past few decades have yielded valuable information regarding the relative taxonomic distribution of the gut microbiota that could enable personalized medicine. Unfortunately, the microbiome data suffers from class imbalance and high dimensionality issues that must be addressed. In this study, we have implemented data engineering algorithms to address the above-mentioned issues inherent to microbiome data. Four standard machine learning classifiers (logistic regression (LR), support vector machines (SVM), random forests (RF), and extreme gradient boosting (XGB) decision trees) were implemented on a previously published dataset of infants with cystic fibrosis exhibiting normal vs abnormal growth patterns. The issue of class imbalance and high dimensionality of 
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#31185;&#23398;&#26159;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#28508;&#21147;&#21644;&#24212;&#29992;&#24191;&#27867;&#24615;&#65292;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#23450;&#20041;&#30340;&#20887;&#20313;&#21644;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16177</link><description>&lt;p&gt;
&#23450;&#20041;&#25968;&#25454;&#31185;&#23398;&#65306;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defining data science: a new field of inquiry. (arXiv:2306.16177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16177
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#26159;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#28508;&#21147;&#21644;&#24212;&#29992;&#24191;&#27867;&#24615;&#65292;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#23450;&#20041;&#30340;&#20887;&#20313;&#21644;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#12290;&#23427;&#30340;&#21147;&#37327;&#12289;&#33539;&#22260;&#21644;&#35268;&#27169;&#23558;&#36229;&#36234;&#31185;&#23398;&#65292;&#25104;&#20026;&#20419;&#20351;&#30693;&#35782;&#21457;&#29616;&#24182;&#25913;&#21464;&#19990;&#30028;&#30340;&#37325;&#35201;&#25163;&#27573;&#12290;&#25105;&#20204;&#23578;&#26410;&#29702;&#35299;&#21644;&#23450;&#20041;&#23427;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#20854;&#28508;&#21147;&#21644;&#31649;&#29702;&#20854;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#33258;1962&#24180;&#20197;&#26469;&#32531;&#24930;&#21457;&#23637;&#65292;&#24182;&#19988;&#33258;2000&#24180;&#20197;&#26469;&#21457;&#23637;&#36805;&#36895;&#65292;&#23427;&#26159;&#19968;&#31181;&#26681;&#26412;&#24615;&#30340;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26159;21&#19990;&#32426;&#26368;&#27963;&#36291;&#12289;&#26368;&#24378;&#22823;&#21644;&#21457;&#23637;&#26368;&#24555;&#30340;&#21019;&#26032;&#20043;&#19968;&#12290;&#30001;&#20110;&#20854;&#20215;&#20540;&#12289;&#21147;&#37327;&#21644;&#36866;&#29992;&#24615;&#65292;&#23427;&#27491;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#25968;&#20197;&#30334;&#19975;&#35745;&#30340;&#25968;&#25454;&#31185;&#23398;&#20986;&#29256;&#29289;&#20013;&#21253;&#21547;&#20102;&#26080;&#25968;&#20851;&#20110;&#25968;&#25454;&#31185;&#23398;&#21644;&#25968;&#25454;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;&#23450;&#20041;&#12290;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#35768;&#22810;&#23450;&#20041;&#26159;&#29420;&#31435;&#30340;&#12289;&#24212;&#29992;&#29305;&#23450;&#30340;&#12289;&#30456;&#20114;&#19981;&#23436;&#25972;&#30340;&#12289;&#20887;&#20313;&#30340;&#25110;&#19981;&#19968;&#33268;&#30340;&#65292;&#22240;&#27492;&#25968;&#25454;&#31185;&#23398;&#20063;&#26159;&#22914;&#27492;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#22810;&#37325;&#23450;&#20041;&#25361;&#25112;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data science is not a science. It is a research paradigm. Its power, scope, and scale will surpass science, our most powerful research paradigm, to enable knowledge discovery and change our world. We have yet to understand and define it, vital to realizing its potential and managing its risks. Modern data science is in its infancy. Emerging slowly since 1962 and rapidly since 2000, it is a fundamentally new field of inquiry, one of the most active, powerful, and rapidly evolving 21st century innovations. Due to its value, power, and applicability, it is emerging in 40+ disciplines, hundreds of research areas, and thousands of applications. Millions of data science publications contain myriad definitions of data science and data science problem solving. Due to its infancy, many definitions are independent, application-specific, mutually incomplete, redundant, or inconsistent, hence so is data science. This research addresses this data science multiple definitions challenge by proposing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#25945;&#24072;&#23545;&#25239;&#40065;&#26834;&#24615;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#30340;&#24178;&#20928;&#26679;&#26412;&#25945;&#24072;&#21644;&#40065;&#26834;&#24615;&#25945;&#24072;&#26469;&#25913;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.16170</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#25945;&#24072;&#23545;&#25239;&#33976;&#39311;&#20943;&#36731;&#20934;&#30830;&#24615;&#19982;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Mitigating the Accuracy-Robustness Trade-off via Multi-Teacher Adversarial Distillation. (arXiv:2306.16170v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#25945;&#24072;&#23545;&#25239;&#40065;&#26834;&#24615;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#30340;&#24178;&#20928;&#26679;&#26412;&#25945;&#24072;&#21644;&#40065;&#26834;&#24615;&#25945;&#24072;&#26469;&#25913;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#19968;&#31181;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#34429;&#28982;&#26377;&#25928;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#65292;&#20294;&#23545;&#24178;&#20928;&#26679;&#26412;&#30340;&#24615;&#33021;&#21364;&#26377;&#25152;&#19979;&#38477;&#65292;&#36825;&#24847;&#21619;&#30528;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#26435;&#34913;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#25552;&#39640;&#40065;&#26834;&#24615;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#65292;&#20294;&#24182;&#27809;&#26377;&#26174;&#33879;&#25913;&#21892;&#23545;&#24178;&#20928;&#26679;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#22810;&#25945;&#24072;&#23545;&#25239;&#40065;&#26834;&#24615;&#33976;&#39311;&#65288;MTARD&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#24378;&#22823;&#30340;&#24178;&#20928;&#26679;&#26412;&#25945;&#24072;&#21644;&#24378;&#22823;&#30340;&#40065;&#26834;&#26679;&#26412;&#25945;&#24072;&#26469;&#25351;&#23548;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#20026;&#20102;&#30830;&#20445;&#19981;&#21516;&#25945;&#24072;&#26174;&#31034;&#30456;&#20284;&#30340;&#30693;&#35782;&#27700;&#24179;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#29109;&#30340;&#24179;&#34913;&#31639;&#27861;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is a practical approach for improving the robustness of deep neural networks against adversarial attacks. Although bringing reliable robustness, the performance toward clean examples is negatively affected after adversarial training, which means a trade-off exists between accuracy and robustness. Recently, some studies have tried to use knowledge distillation methods in adversarial training, achieving competitive performance in improving the robustness but the accuracy for clean samples is still limited. In this paper, to mitigate the accuracy-robustness trade-off, we introduce the Multi-Teacher Adversarial Robustness Distillation (MTARD) to guide the model's adversarial training process by applying a strong clean teacher and a strong robust teacher to handle the clean examples and adversarial examples, respectively. During the optimization process, to ensure that different teachers show similar knowledge scales, we design the Entropy-Based Balance algorithm to adj
&lt;/p&gt;</description></item><item><title>BayesFlow&#26159;&#19968;&#20010;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#21151;&#33021;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#27169;&#22411;&#20223;&#30495;&#19978;&#35757;&#32451;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20219;&#20309;&#21518;&#32493;&#24212;&#29992;&#12290;&#36825;&#31181;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2306.16015</link><description>&lt;p&gt;
BayesFlow: &#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#25674;&#36824;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;
BayesFlow: Amortized Bayesian Workflows With Neural Networks. (arXiv:2306.16015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16015
&lt;/p&gt;
&lt;p&gt;
BayesFlow&#26159;&#19968;&#20010;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#21151;&#33021;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#27169;&#22411;&#20223;&#30495;&#19978;&#35757;&#32451;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20219;&#20309;&#21518;&#32493;&#24212;&#29992;&#12290;&#36825;&#31181;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36125;&#21494;&#26031;&#25512;&#26029;&#28041;&#21450;&#19968;&#31995;&#21015;&#35745;&#31639;&#25216;&#26415;&#65292;&#29992;&#20110;&#20272;&#35745;&#12289;&#39564;&#35777;&#21644;&#20174;&#27010;&#29575;&#27169;&#22411;&#20013;&#24471;&#20986;&#32467;&#35770;&#65292;&#20316;&#20026;&#25968;&#25454;&#20998;&#26512;&#20013;&#26377;&#21407;&#21017;&#30340;&#24037;&#20316;&#27969;&#30340;&#19968;&#37096;&#20998;&#12290;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#20013;&#30340;&#20856;&#22411;&#38382;&#39064;&#21253;&#25324;&#36817;&#20284;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#27169;&#22411;&#31867;&#22411;&#65292;&#20197;&#21450;&#36890;&#36807;&#22797;&#26434;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#27604;&#36739;&#21516;&#19968;&#36807;&#31243;&#30340;&#31454;&#20105;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Python&#24211;BayesFlow&#65292;&#29992;&#20110;&#22522;&#20110;&#20223;&#30495;&#35757;&#32451;&#24050;&#24314;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#25674;&#36824;&#25968;&#25454;&#21387;&#32553;&#21644;&#25512;&#26029;&#12290;&#22312;BayesFlow&#20013;&#23454;&#29616;&#30340;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#27169;&#22411;&#20223;&#30495;&#19978;&#35757;&#32451;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#36825;&#20123;&#32593;&#32476;&#37325;&#29992;&#20110;&#27169;&#22411;&#30340;&#20219;&#20309;&#21518;&#32493;&#24212;&#29992;&#12290;&#30001;&#20110;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#21487;&#20197;&#20960;&#20046;&#21363;&#26102;&#22320;&#25191;&#34892;&#25512;&#26029;&#65292;&#22240;&#27492;&#21069;&#26399;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#24456;&#24555;&#23601;&#33021;&#22815;&#25674;&#36824;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern Bayesian inference involves a mixture of computational techniques for estimating, validating, and drawing conclusions from probabilistic models as part of principled workflows for data analysis. Typical problems in Bayesian workflows are the approximation of intractable posterior distributions for diverse model types and the comparison of competing models of the same process in terms of their complexity and predictive performance. This manuscript introduces the Python library BayesFlow for simulation-based training of established neural network architectures for amortized data compression and inference. Amortized Bayesian inference, as implemented in BayesFlow, enables users to train custom neural networks on model simulations and re-use these networks for any subsequent application of the models. Since the trained networks can perform inference almost instantaneously, the upfront neural network training is quickly amortized.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#32771;&#34385;&#28237;&#27969;&#27169;&#22411;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#20934;&#30830;&#25968;&#25454;&#31232;&#32570;&#26102;&#33021;&#22815;&#23454;&#29616;&#21069;&#32622;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.13370</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#30340;&#38543;&#26426;&#26862;&#26519;&#29992;&#20110;&#28237;&#27969;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Physics-constrained Random Forests for Turbulence Model Uncertainty Estimation. (arXiv:2306.13370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#32771;&#34385;&#28237;&#27969;&#27169;&#22411;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#20934;&#30830;&#25968;&#25454;&#31232;&#32570;&#26102;&#33021;&#22815;&#23454;&#29616;&#21069;&#32622;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#29616;&#24037;&#19994;&#35774;&#35745;&#30340;&#34394;&#25311;&#35748;&#35777;&#36807;&#31243;&#20013;&#65292;&#23545;&#20110;&#27169;&#25311;&#39537;&#21160;&#36807;&#31243;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#26041;&#27861;&#26469;&#32771;&#34385;&#28237;&#27969;&#27169;&#22411;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#28040;&#38500;&#29992;&#25143;&#36755;&#20837;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#22312;&#20934;&#30830;&#25968;&#25454;&#31232;&#32570;&#26102;&#24320;&#21457;&#20808;&#39564;&#20272;&#35745;&#39044;&#27979;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
To achieve virtual certification for industrial design, quantifying the uncertainties in simulation-driven processes is crucial. We discuss a physics-constrained approach to account for epistemic uncertainty of turbulence models. In order to eliminate user input, we incorporate a data-driven machine learning strategy. In addition to it, our study focuses on developing an a priori estimation of prediction confidence when accurate data is scarce.
&lt;/p&gt;</description></item><item><title>TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13339</link><description>&lt;p&gt;
TrustGuard: &#22522;&#20110;GNN&#30340;&#21160;&#24577;&#25903;&#25345;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#20449;&#20219;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13339
&lt;/p&gt;
&lt;p&gt;
TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#35780;&#20272;&#35780;&#20272;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#24182;&#20419;&#36827;&#20915;&#31574;&#12290;&#26426;&#22120;&#23398;&#20064;&#30001;&#20110;&#20854;&#23398;&#20064;&#33021;&#21147;&#32780;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#23545;&#20449;&#20219;&#35780;&#20272;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22788;&#29702;&#22270;&#24418;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;&#36825;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#23558;&#20854;&#29992;&#20110;&#20449;&#20219;&#35780;&#20272;&#65292;&#22240;&#20026;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#21487;&#20197;&#24314;&#27169;&#20026;&#22270;&#24418;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;GNN&#30340;&#24403;&#21069;&#20449;&#20219;&#35780;&#20272;&#26041;&#27861;&#26410;&#33021;&#23436;&#20840;&#28385;&#36275;&#20449;&#20219;&#30340;&#21160;&#24577;&#24615;&#65292;&#24573;&#30053;&#20102;&#25915;&#20987;&#23545;&#20449;&#20219;&#35780;&#20272;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#27861;&#25552;&#20379;&#20196;&#20154;&#20449;&#26381;&#30340;&#35780;&#20272;&#32467;&#26524;&#35299;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrustGuard &#65306;&#19968;&#31181;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#12289;&#25239;&#20987;&#40065;&#26834;&#19988;&#36890;&#36807;&#21487;&#35270;&#21270;&#25552;&#20379;&#35299;&#37322;&#30340;&#31934;&#30830;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TrustGuard &#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#21160;&#24577;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#23618;&#12289;&#22270;&#21367;&#31215;&#23618;&#12289;&#27880;&#24847;&#26426;&#21046;&#23618;&#21644;&#20449;&#20219;&#39044;&#27979;&#23618;&#32452;&#25104;&#30340;&#20998;&#23618;&#26550;&#26500;&#12290;&#20026;&#20102;&#35780;&#20272;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;TrustGuard&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TrustGuard &#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trust evaluation assesses trust relationships between entities and facilitates decision-making. Machine Learning (ML) shows great potential for trust evaluation owing to its learning capabilities. In recent years, Graph Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in dealing with graph data. This has motivated researchers to explore their use in trust evaluation, as trust relationships among entities can be modeled as a graph. However, current trust evaluation methods that employ GNNs fail to fully satisfy the dynamicity nature of trust, overlook the adverse effects of attacks on trust evaluation, and cannot provide convincing explanations on evaluation results. To address these problems, in this paper, we propose TrustGuard, a GNN-based accurate trust evaluation model that supports trust dynamicity, is robust against typical attacks, and provides explanations through visualization. Specifically, TrustGuard is designed with a layered architecture that con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986; TeCoS-LVM &#27169;&#22411;&#65292;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20197;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36991;&#20813;&#20002;&#22833;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.12045</link><description>&lt;p&gt;
&#22788;&#29702;&#33258;&#28982;&#35270;&#35273;&#22330;&#26223;&#31070;&#32463;&#21709;&#24212;&#30340;&#26102;&#38388;&#26465;&#20214;&#33033;&#20914;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes. (arXiv:2306.12045v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986; TeCoS-LVM &#27169;&#22411;&#65292;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20197;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36991;&#20813;&#20002;&#22833;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#31070;&#32463;&#21709;&#24212;&#30340;&#35745;&#31639;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#24863;&#30693;&#22788;&#29702;&#21644;&#31070;&#32463;&#35745;&#31639;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#20351;&#29992;&#26102;&#38388;&#36807;&#28388;&#22120;&#26469;&#22788;&#29702;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#23548;&#33268;&#22788;&#29702;&#27969;&#31243;&#19981;&#29616;&#23454;&#19988;&#19981;&#28789;&#27963;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#38024;&#23545;&#35797;&#39564;&#24179;&#22343;&#21457;&#25918;&#29575;&#65292;&#26410;&#33021;&#25429;&#25417;&#21040;&#33033;&#20914;&#21015;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#26102;&#38388;&#26465;&#20214;&#33033;&#20914;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;TeCoS-LVM&#65289;&#26469;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20135;&#29983;&#30452;&#25509;&#21305;&#37197;&#35760;&#24405;&#33033;&#20914;&#21015;&#30340;&#33033;&#20914;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#36991;&#20813;&#20002;&#22833;&#23884;&#20837;&#22312;&#21407;&#22987;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20174;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#20013;&#25490;&#38500;&#26102;&#38388;&#32500;&#24230;&#65292;&#24182;&#24341;&#20837;&#26102;&#38388;&#26465;&#20214;&#25805;&#20316;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#33258;&#28982;&#33539;&#24335;&#20013;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; TeCoS-LVM &#27169;&#22411;&#33021;&#22815;&#20135;&#29983;...
&lt;/p&gt;
&lt;p&gt;
Developing computational models of neural response is crucial for understanding sensory processing and neural computations. Current state-of-the-art neural network methods use temporal filters to handle temporal dependencies, resulting in an unrealistic and inflexible processing flow. Meanwhile, these methods target trial-averaged firing rates and fail to capture important features in spike trains. This work presents the temporal conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural response to natural visual stimuli. We use spiking neurons to produce spike outputs that directly match the recorded trains. This approach helps to avoid losing information embedded in the original spike trains. We exclude the temporal dimension from the model parameter space and introduce a temporal conditioning operation to allow the model to adaptively explore and exploit temporal dependencies in stimuli sequences in a natural paradigm. We show that TeCoS-LVM models can produce m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25209;&#35268;&#33539;&#21270;&#35757;&#32451;&#32447;&#24615;&#27169;&#22411;&#21644;&#20004;&#23618;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#38544;&#24335;&#20559;&#24046;&#65292;&#24182;&#35777;&#26126;&#25209;&#35268;&#33539;&#21270;&#23545;&#20110;&#22343;&#21248;&#38388;&#38548;&#20855;&#26377;&#38544;&#21547;&#20559;&#24046;&#12290;&#36890;&#36807;&#20004;&#20010;&#20363;&#23376;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#29305;&#23450;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#22343;&#21248;&#38388;&#38548;&#20998;&#31867;&#22120;&#30340;&#34920;&#29616;&#29978;&#33267;&#20248;&#20110;&#26368;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.11680</link><description>&lt;p&gt;
&#25209;&#35268;&#33539;&#21270;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;&#20004;&#23618;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#24335;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
The Implicit Bias of Batch Normalization in Linear Models and Two-layer Linear Convolutional Neural Networks. (arXiv:2306.11680v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25209;&#35268;&#33539;&#21270;&#35757;&#32451;&#32447;&#24615;&#27169;&#22411;&#21644;&#20004;&#23618;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#38544;&#24335;&#20559;&#24046;&#65292;&#24182;&#35777;&#26126;&#25209;&#35268;&#33539;&#21270;&#23545;&#20110;&#22343;&#21248;&#38388;&#38548;&#20855;&#26377;&#38544;&#21547;&#20559;&#24046;&#12290;&#36890;&#36807;&#20004;&#20010;&#20363;&#23376;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#29305;&#23450;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#22343;&#21248;&#38388;&#38548;&#20998;&#31867;&#22120;&#30340;&#34920;&#29616;&#29978;&#33267;&#20248;&#20110;&#26368;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30001;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#25209;&#35268;&#33539;&#21270;&#30340;&#38544;&#21547;&#20559;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;&#25209;&#35268;&#33539;&#21270;&#35757;&#32451;&#20108;&#20998;&#31867;&#32447;&#24615;&#27169;&#22411;&#26102;&#65292;&#26799;&#24230;&#19979;&#38477;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#20855;&#26377;&#22343;&#21248;&#38388;&#38548;&#30340;&#20998;&#31867;&#22120;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;$\exp&#65288;- \Omega&#65288;\log ^ 2 t&#65289;&#65289;$&#12290;&#36825;&#23558;&#25209;&#35268;&#33539;&#21270;&#30340;&#32447;&#24615;&#27169;&#22411;&#19982;&#19981;&#20351;&#29992;&#25209;&#35268;&#33539;&#21270;&#30340;&#27169;&#22411;&#21306;&#20998;&#24320;&#26469;&#65292;&#20854;&#38544;&#21547;&#20559;&#24046;&#21644;&#25910;&#25947;&#36895;&#24230;&#22343;&#19981;&#21516;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#19968;&#31867;&#20004;&#23618;&#21333;&#28388;&#27874;&#22120;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#34920;&#26126;&#25209;&#35268;&#33539;&#21270;&#23545;&#20110;&#22343;&#21248;&#38388;&#38548;&#20855;&#26377;&#38544;&#21547;&#20559;&#24046;&#12290;&#36890;&#36807;&#20004;&#20010;&#20363;&#23376;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29305;&#23450;&#23398;&#20064;&#38382;&#39064;&#20013;&#22343;&#21248;&#38388;&#38548;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#21487;&#20197;&#20248;&#20110;&#26368;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;&#25209;&#35268;&#33539;&#21270;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the implicit bias of batch normalization trained by gradient descent. We show that when learning a linear model with batch normalization for binary classification, gradient descent converges to a uniform margin classifier on the training data with an $\exp(-\Omega(\log^2 t))$ convergence rate. This distinguishes linear models with batch normalization from those without batch normalization in terms of both the type of implicit bias and the convergence rate. We further extend our result to a class of two-layer, single-filter linear convolutional neural networks, and show that batch normalization has an implicit bias towards a patch-wise uniform margin. Based on two examples, we demonstrate that patch-wise uniform margin classifiers can outperform the maximum margin classifiers in certain learning problems. Our results contribute to a better theoretical understanding of batch normalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#12289;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.06210</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#32456;&#23618;&#21453;&#28436;&#36827;&#34892;&#21333;&#27169;&#22411;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Single-Model Attribution via Final-Layer Inversion. (arXiv:2306.06210v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#12289;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#30340;&#24320;&#21019;&#24615;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#23454;&#29992;&#21333;&#27169;&#22411;&#24402;&#22240;&#30340;&#20852;&#36259;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#19968;&#20010;&#26679;&#26412;&#26159;&#30001;&#29305;&#23450;&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#36824;&#26159;&#19981;&#26159;&#65292;&#20363;&#22914;&#65292;&#20026;&#20102;&#35777;&#26126;&#30693;&#35782;&#20135;&#26435;&#30423;&#31363;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#65292;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FLIPAD&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#21033;&#29992;&#30340;&#26368;&#32456;&#23618;&#21453;&#28436;&#21487;&#20197;&#31616;&#21270;&#20026;&#19968;&#20010;&#20984;&#30340; Lasso &#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#21487;&#38752;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;&#29702;&#35770;&#32467;&#26524;&#36824;&#24471;&#21040;&#20102;&#23454;&#39564;&#30740;&#31350;&#30340;&#25903;&#25345;&#65292;&#35777;&#26126;&#26412;&#25991;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent groundbreaking developments on generative modeling have sparked interest in practical single-model attribution. Such methods predict whether a sample was generated by a specific generator or not, for instance, to prove intellectual property theft. However, previous works are either limited to the closed-world setting or require undesirable changes of the generative model. We address these shortcomings by proposing FLIPAD, a new approach for single-model attribution in the open-world setting based on final-layer inversion and anomaly detection. We show that the utilized final-layer inversion can be reduced to a convex lasso optimization problem, making our approach theoretically sound and computationally efficient. The theoretical findings are accompanied by an experimental study demonstrating the effectiveness of our approach, outperforming the existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#35299;&#20915;&#24191;&#20041;&#33258;&#30001;&#33021;&#65288;FE&#65289;&#30446;&#26631;&#30340;&#21512;&#25104;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#30340;&#21464;&#20998;&#20449;&#24687;&#26356;&#26032;&#21644;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;T&#24418;&#36855;&#23467;&#23548;&#33322;&#20219;&#21153;&#30340;&#27169;&#25311;&#27604;&#36739;&#65292;&#34920;&#26126;AIF&#21487;&#24341;&#36215;&#35748;&#30693;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.02733</link><description>&lt;p&gt;
&#23454;&#29616;&#21512;&#25104;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#65292;&#31532;&#20108;&#37096;&#20998;&#65306;&#21464;&#20998;&#20449;&#24687;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Realising Synthetic Active Inference Agents, Part II: Variational Message Updates. (arXiv:2306.02733v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#35299;&#20915;&#24191;&#20041;&#33258;&#30001;&#33021;&#65288;FE&#65289;&#30446;&#26631;&#30340;&#21512;&#25104;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#30340;&#21464;&#20998;&#20449;&#24687;&#26356;&#26032;&#21644;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;T&#24418;&#36855;&#23467;&#23548;&#33322;&#20219;&#21153;&#30340;&#27169;&#25311;&#27604;&#36739;&#65292;&#34920;&#26126;AIF&#21487;&#24341;&#36215;&#35748;&#30693;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30001;&#33021;&#21407;&#29702;&#65288;FEP&#65289;&#25551;&#36848;&#29983;&#29289;&#20195;&#29702;&#36890;&#36807;&#30456;&#24212;&#29615;&#22659;&#30340;&#29983;&#25104;&#27169;&#22411;&#26368;&#23567;&#21270;&#21464;&#20998;&#33258;&#30001;&#33021;&#65288;FE&#65289;&#12290;&#20027;&#21160;&#25512;&#29702;&#65288;AIF&#65289;&#26159;FEP&#30340;&#25512;&#35770;&#65292;&#25551;&#36848;&#20102;&#20195;&#29702;&#20154;&#36890;&#36807;&#26368;&#23567;&#21270;&#26399;&#26395;&#30340;FE&#30446;&#26631;&#26469;&#25506;&#32034;&#21644;&#21033;&#29992;&#20854;&#29615;&#22659;&#12290;&#22312;&#20004;&#31687;&#30456;&#20851;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#30001;&#24418;&#24335;Forney-style&#22240;&#23376;&#22270;&#65288;FFG&#65289;&#19978;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#25551;&#36848;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21512;&#25104;AIF&#20195;&#29702;&#30340;&#35748;&#30693;&#26041;&#27861;&#12290;&#26412;&#25991;&#65288;&#31532;&#20108;&#37096;&#20998;&#65289;&#26681;&#25454;&#21464;&#20998;&#28436;&#31639;&#27861;&#65292;&#23548;&#20986;&#20102;&#26368;&#23567;&#21270;CFFG&#19978;&#65288;&#24191;&#20041;&#65289;FE&#30446;&#26631;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#12290;&#27604;&#36739;&#20102;&#27169;&#25311;Bethe&#21644;&#24191;&#20041;FE&#20195;&#29702;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#35828;&#26126;&#20102;&#21512;&#25104;AIF&#22914;&#20309;&#22312;T&#24418;&#36855;&#23467;&#23548;&#33322;&#20219;&#21153;&#19978;&#24341;&#36215;&#35748;&#30693;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;AIF&#20195;&#29702;&#30340;&#23436;&#25972;&#28040;&#24687;&#20256;&#36882;&#25551;&#36848;&#65292;&#21487;&#20197;&#25512;&#23548;&#21644;&#37325;&#29992;&#35813;&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Free Energy Principle (FEP) describes (biological) agents as minimising a variational Free Energy (FE) with respect to a generative model of their environment. Active Inference (AIF) is a corollary of the FEP that describes how agents explore and exploit their environment by minimising an expected FE objective. In two related papers, we describe a scalable, epistemic approach to synthetic AIF agents, by message passing on free-form Forney-style Factor Graphs (FFGs). A companion paper (part I) introduces a Constrained FFG (CFFG) notation that visually represents (generalised) FE objectives for AIF. The current paper (part II) derives message passing algorithms that minimise (generalised) FE objectives on a CFFG by variational calculus. A comparison between simulated Bethe and generalised FE agents illustrates how synthetic AIF induces epistemic behaviour on a T-maze navigation task. With a full message passing account of synthetic AIF agents, it becomes possible to derive and reuse 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;--&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#31616;&#21333;&#26131;&#29992;&#65292;&#22312;&#21333;&#35843;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.01147</link><description>&lt;p&gt;
&#24179;&#28369;&#21333;&#35843;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Smooth Monotonic Networks. (arXiv:2306.01147v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;--&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#31616;&#21333;&#26131;&#29992;&#65292;&#22312;&#21333;&#35843;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35843;&#24615;&#32422;&#26463;&#26159;&#32479;&#35745;&#24314;&#27169;&#20013;&#30340;&#24378;&#21147;&#27491;&#21017;&#21270;&#24037;&#20855;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#35745;&#31639;&#26426;&#25903;&#25345;&#30340;&#20915;&#31574;&#21046;&#23450;&#20013;&#25903;&#25345;&#20844;&#24179;&#24615;&#65292;&#24182;&#22686;&#21152;&#25968;&#25454;&#39537;&#21160;&#31185;&#23398;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;&#32463;&#20856;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30830;&#20445;&#20102;&#21333;&#35843;&#24615;&#65292;&#20294;&#30001;&#20110;&#26799;&#24230;&#28040;&#22833;&#32780;&#24448;&#24448;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38519;&#20837;&#19981;&#33391;&#23616;&#37096;&#26368;&#20248;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;MM&#32593;&#32476;&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#20351;&#29992;&#20005;&#26684;&#36882;&#22686;&#30340;&#24179;&#28369;&#38750;&#32447;&#24615;&#20989;&#25968;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#24471;&#21040;&#30340;&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#27169;&#22359;&#32487;&#25215;&#20102;MM&#26550;&#26500;&#30340;&#28176;&#36817;&#36924;&#36817;&#24615;&#36136;&#12290;&#23427;&#21487;&#20197;&#23884;&#20837;&#21040;&#26356;&#22823;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#21333;&#35843;&#24314;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#65292;SMM&#27169;&#22359;&#35201;&#31616;&#21333;&#24471;&#22810;&#65292;&#35745;&#31639;&#38656;&#27714;&#20063;&#35201;&#23569;&#24471;&#22810;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#23427;&#22312;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#19982;&#26367;&#20195;&#31070;&#32463;&#21644;&#38750;&#31070;&#32463;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#24471;&#26356;&#20026;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monotonicity constraints are powerful regularizers in statistical modelling. They can support fairness in computer supported decision making and increase plausibility in data-driven scientific models. The seminal min-max (MM) neural network architecture ensures monotonicity, but often gets stuck in undesired local optima during training because of vanishing gradients. We propose a simple modification of the MM network using strictly-increasing smooth non-linearities that alleviates this problem. The resulting smooth min-max (SMM) network module inherits the asymptotic approximation properties from the MM architecture. It can be used within larger deep learning systems trained end-to-end. The SMM module is considerably simpler and less computationally demanding than state-of-the-art neural networks for monotonic modelling. Still, in our experiments, it compared favorably to alternative neural and non-neural approaches in terms of generalization performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#40657;&#30418;&#27169;&#22411;&#36716;&#21270;&#25104;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#24182;&#22312;&#30446;&#26631;&#39046;&#22495;&#25104;&#21151;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.17303</link><description>&lt;p&gt;
&#20174;&#40657;&#30418;&#27169;&#22411;&#21040;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#36716;&#21270;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distilling BlackBox to Interpretable models for Efficient Transfer Learning. (arXiv:2305.17303v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#40657;&#30418;&#27169;&#22411;&#36716;&#21270;&#25104;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#24182;&#22312;&#30446;&#26631;&#39046;&#22495;&#25104;&#21151;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;AI&#27169;&#22411;&#26159;&#21307;&#30103;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#21363;&#20351;&#36755;&#20837;&#20998;&#24067;&#36731;&#24494;&#31227;&#20301;&#65288;&#20363;&#22914;&#25195;&#25551;&#20202;&#31867;&#22411;&#65289;&#65292;&#20063;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#32780;&#25918;&#23556;&#31185;&#21307;&#29983;&#21017;&#20381;&#36182;&#20110;&#24322;&#24120;&#24615;&#30340;&#36890;&#29992;&#25551;&#36848;&#24615;&#35268;&#21017;&#12290;&#24494;&#35843;&#27169;&#22411;&#20197;&#23558;&#30693;&#35782;&#20174;&#19968;&#20010;&#39046;&#22495;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#35745;&#31639;&#25104;&#26412;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#22320;&#38024;&#23545;&#26410;&#30693;&#30340;&#30446;&#26631;&#22495;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35748;&#20026;NN&#30340;&#21487;&#35299;&#37322;&#32452;&#20214;&#22823;&#33268;&#26159;&#22495;&#19981;&#21464;&#30340;&#12290;&#28982;&#32780;&#65292;&#21487;&#35299;&#37322;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#19981;&#21450;&#23427;&#20204;&#30340;BB&#21464;&#20307;&#12290;&#22312;&#28304;&#22495;&#20013;&#25105;&#20204;&#20808;&#20351;&#29992;&#20154;&#31867;&#29702;&#35299;&#30340;&#27010;&#24565;&#20174;BB&#24320;&#22987;&#65292;&#23558;&#20854;&#25552;&#28860;&#25104;&#19968;&#32452;&#27973;&#26174;&#26131;&#25026;&#30340;interpretable&#27169;&#22411;&#12290;&#30001;&#20110;&#27599;&#20010;interpretable&#27169;&#22411;&#37117;&#35206;&#30422;&#20102;&#25968;&#25454;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20855;&#26377;&#19968;&#32452;interpretable&#27169;&#22411;&#30340;&#28151;&#21512;&#21487;&#20197;&#23454;&#29616;&#19982;BB&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (\eg scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this paper, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain-invariant. However, interpretable models typically underperform compared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a \emph{mixture} of shallow interpretable models using human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Fu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; TOAST &#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#32858;&#28966;&#27880;&#24847;&#21147;&#65292;&#36873;&#25321;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20803;&#32032;&#24182;&#21453;&#39304;&#22238;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#23567;&#37096;&#20998;&#21487;&#35843;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.15542</link><description>&lt;p&gt;
&#32858;&#28966;&#26159;&#36801;&#31227;&#23398;&#20064;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Refocusing Is Key to Transfer Learning. (arXiv:2305.15542v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15542
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; TOAST &#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#32858;&#28966;&#27880;&#24847;&#21147;&#65292;&#36873;&#25321;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20803;&#32032;&#24182;&#21453;&#39304;&#22238;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#23567;&#37096;&#20998;&#21487;&#35843;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#28041;&#21450;&#23558;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36866;&#24212;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#21069;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#24120;&#24120;&#26080;&#27861;&#32858;&#28966;&#20110;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#37325;&#26032;&#32858;&#28966;&#27880;&#24847;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;-Top-Down Attention Steering&#65288;TOAST&#65289;&#65292;&#23427;&#20445;&#25345;&#39044;&#20808;&#35757;&#32451;&#30340;&#39592;&#24178;&#32467;&#26500;&#19981;&#21464;&#65292;&#21516;&#26102;&#36873;&#25321;&#36755;&#20986;&#20013;&#19982;&#20219;&#21153;&#26377;&#20851;&#30340;&#20803;&#32032;&#65292;&#24182;&#23558;&#23427;&#20204;&#21453;&#39304;&#22238;&#27169;&#22411;&#65292;&#20197;&#24341;&#23548;&#20854;&#27880;&#24847;&#20219;&#21153;&#29305;&#23450;&#30340;&#29305;&#24449;&#12290;&#20165;&#36890;&#36807;&#37325;&#26032;&#32858;&#28966;&#27880;&#24847;&#21147;&#65292;TOAST&#22312;&#35768;&#22810;&#36801;&#31227;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20855;&#26377;&#23567;&#37096;&#20998;&#21487;&#35843;&#21442;&#25968;&#12290;&#19982;&#23436;&#20840;&#24494;&#35843;&#12289;LoRA&#21644;&#25552;&#31034;&#24494;&#35843;&#30456;&#27604;&#65292;TOAST&#22312;&#19968;&#31995;&#21015;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65288;&#20363;&#22914;&#65292;&#22312; FGVC &#19978;&#20174; 81.1% &#25552;&#39640;&#21040; 86.2%&#65289;&#26174;&#30528;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;TOAST&#22312;&#25351;&#20196;&#36319;&#38543;&#26041;&#38754;&#20063;&#20248;&#20110;&#23436;&#20840;&#24494;&#35843;&#30340; Alpaca &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning involves adapting a pre-trained model to novel downstream tasks. However, we observe that current transfer learning methods often fail to focus on task-relevant features. In this work, we emphasize the importance of refocusing the attention in transfer learning. We introduce Top-Down Attention Steering (TOAST), a novel transfer learning algorithm that keeps the pre-trained backbone frozen, while selecting the task-relevant elements in the output and feeding them back to the model to steer its attention to the task-specific features. By refocusing the attention only, TOAST achieves state-of-the-art results on a number of transfer learning benchmarks, while having a small portion of tunable parameters. Compared to fully fine-tuning, LoRA, and prompt tuning, TOAST substantially improves performance across a range of fine-grained visual classification datasets (e.g., 81.1% -&gt; 86.2% on FGVC). TOAST also outperforms the fully fine-tuned Alpaca model on instruction-following
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Parameter Isolation GNN (PI-GNN)&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#38548;&#31163;&#21644;&#25193;&#23637;&#26469;&#36991;&#20813;&#23398;&#20064;&#26032;&#27169;&#24335;&#21644;&#20445;&#30041;&#26087;&#27169;&#24335;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.13825</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#25968;&#38548;&#31163;&#30340;&#21160;&#24577;&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning on Dynamic Graphs via Parameter Isolation. (arXiv:2305.13825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13825
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Parameter Isolation GNN (PI-GNN)&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#38548;&#31163;&#21644;&#25193;&#23637;&#26469;&#36991;&#20813;&#23398;&#20064;&#26032;&#27169;&#24335;&#21644;&#20445;&#30041;&#26087;&#27169;&#24335;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#38656;&#35201;&#22788;&#29702;&#26032;&#33410;&#28857;&#21644;&#36793;&#20986;&#29616;&#30340;&#21160;&#24577;&#22270;&#12290;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#36973;&#36935;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#21363;&#20026;&#20197;&#21069;&#30340;&#22270;&#25152;&#23398;&#30340;&#30693;&#35782;&#20250;&#34987;&#26032;&#22270;&#30340;&#26356;&#26032;&#35206;&#30422;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25345;&#32493;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25345;&#32493;&#22270;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#26032;&#30340;&#27169;&#24335;&#24182;&#32500;&#25252;&#26087;&#30340;&#27169;&#24335;&#65292;&#20294;&#20351;&#29992;&#30456;&#21516;&#22266;&#23450;&#22823;&#23567;&#30340;&#21442;&#25968;&#38598;&#65292;&#22240;&#27492;&#38754;&#20020;&#20004;&#31181;&#30446;&#26631;&#20043;&#38388;&#30340;&#26681;&#26412;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Parameter Isolation GNN (PI-GNN)&#65292;&#29992;&#20110;&#21160;&#24577;&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#21442;&#25968;&#38548;&#31163;&#21644;&#25193;&#23637;&#26469;&#36991;&#20813;&#36825;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#22312;&#20110;&#19981;&#21516;&#30340;&#21442;&#25968;&#23545;&#20110;&#23398;&#20064;&#19981;&#21516;&#30340;&#22270;&#27169;&#24335;&#26377;&#36129;&#29486;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#25193;&#23637;&#27169;&#22411;&#21442;&#25968;&#20197;&#25345;&#32493;&#23398;&#20064;&#20986;&#29616;&#30340;&#22270;&#27169;&#24335;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20026;&#20102;&#26377;&#25928;&#22320;&#20445;&#23384;&#26410;&#21463;&#24433;&#21709;&#27169;&#24335;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#25214;&#21040;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world graph learning tasks require handling dynamic graphs where new nodes and edges emerge. Dynamic graph learning methods commonly suffer from the catastrophic forgetting problem, where knowledge learned for previous graphs is overwritten by updates for new graphs. To alleviate the problem, continual graph learning methods are proposed. However, existing continual graph learning methods aim to learn new patterns and maintain old ones with the same set of parameters of fixed size, and thus face a fundamental tradeoff between both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN) for continual learning on dynamic graphs that circumvents the tradeoff via parameter isolation and expansion. Our motivation lies in that different parameters contribute to learning different graph patterns. Based on the idea, we expand model parameters to continually learn emerging graph patterns. Meanwhile, to effectively preserve knowledge for unaffected patterns, we find parameter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#20223;&#23556;&#23398;&#20064;&#65288;SAL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#12290; &#35813;&#27169;&#22411;&#36890;&#36807;&#35299;&#20915;&#28041;&#21450;&#28608;&#27963;&#20989;&#25968;&#30340;&#20108;&#27425;/&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#23398;&#20064;&#20223;&#23556;&#26144;&#23556;&#65292;&#20294;&#26159;&#22312;&#26435;&#37325;&#30697;&#38453;&#21644;&#20559;&#24046;&#20043;&#21518;&#12290;</title><link>http://arxiv.org/abs/2305.07996</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#32493;&#20223;&#23556;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Successive Affine Learning for Deep Neural Networks. (arXiv:2305.07996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#20223;&#23556;&#23398;&#20064;&#65288;SAL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#12290; &#35813;&#27169;&#22411;&#36890;&#36807;&#35299;&#20915;&#28041;&#21450;&#28608;&#27963;&#20989;&#25968;&#30340;&#20108;&#27425;/&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#23398;&#20064;&#20223;&#23556;&#26144;&#23556;&#65292;&#20294;&#26159;&#22312;&#26435;&#37325;&#30697;&#38453;&#21644;&#20559;&#24046;&#20043;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#36830;&#32493;&#20223;&#23556;&#23398;&#20064;(SAL)&#27169;&#22411;&#12290;&#20256;&#32479;&#19978;&#65292;DNN&#26159;&#36890;&#36807;&#35299;&#20915;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#26500;&#24314;&#30340;&#12290;&#30001;&#20110;&#20854;&#38750;&#20984;&#24615;&#21644;&#23618;&#25968;&#20247;&#22810;&#65292;&#36890;&#24120;&#38590;&#20197;&#22312;&#25968;&#20540;&#19978;&#35299;&#20915;&#36825;&#31181;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#20316;&#32773;&#21551;&#21457;&#20110;&#20154;&#31867;&#25945;&#32946;&#31995;&#32479;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#22810;&#32423;&#28145;&#24230;&#23398;&#20064;(MGDL)&#27169;&#22411;&#12290;MGDL&#27169;&#22411;&#20197;&#22810;&#20010;&#24180;&#32423;&#30340;&#24418;&#24335;&#23398;&#20064;DNN&#65292;&#22312;&#27599;&#20010;&#24180;&#32423;&#20013;&#26500;&#24314;&#30001;&#23569;&#37327;&#23618;&#25968;&#32452;&#25104;&#30340;&#27973;&#23618;DNN&#12290;MGDL&#27169;&#22411;&#20173;&#38656;&#35201;&#35299;&#20915;&#20960;&#20010;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#30340;SAL&#27169;&#22411;&#26159;&#22312;MGDL&#27169;&#22411;&#22522;&#30784;&#19978;&#28436;&#21464;&#32780;&#26469;&#12290;&#21457;&#29616;DNN&#30340;&#27599;&#23618;&#37117;&#30001;&#20223;&#23556;&#26144;&#23556;&#21644;&#28608;&#27963;&#20989;&#25968;&#32452;&#25104;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#35299;&#20915;&#28041;&#21450;&#28608;&#27963;&#20989;&#25968;&#30340;&#20108;&#27425;/&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#23398;&#20064;&#20223;&#23556;&#26144;&#23556;&#65292;&#20294;&#26159;&#22312;&#26435;&#37325;&#30697;&#38453;&#21644;&#20559;&#24046;&#20043;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a successive affine learning (SAL) model for constructing deep neural networks (DNNs). Traditionally, a DNN is built by solving a non-convex optimization problem. It is often challenging to solve such a problem numerically due to its non-convexity and having a large number of layers. To address this challenge, inspired by the human education system, the multi-grade deep learning (MGDL) model was recently initiated by the author of this paper. The MGDL model learns a DNN in several grades, in each of which one constructs a shallow DNN consisting of a small number of layers. The MGDL model still requires solving several non-convex optimization problems. The proposed SAL model mutates from the MGDL model. Noting that each layer of a DNN consists of an affine map followed by an activation function, we propose to learn the affine map by solving a quadratic/convex optimization problem which involves the activation function only {\it after} the weight matrix and the bias
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#24212;&#29992;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#32467;&#26500;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#35757;&#32451;&#21518;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#65292;&#26696;&#20363;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#29983;&#25104;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.04934</link><description>&lt;p&gt;
&#24212;&#29992;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#33258;&#22238;&#24402;Transformer&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#20998;&#26512;&#21644;&#21457;&#29616;&#26032;&#22411;&#34507;&#30333;&#36136;
&lt;/p&gt;
&lt;p&gt;
Generative Pretrained Autoregressive Transformer Graph Neural Network applied to the Analysis and Discovery of Novel Proteins. (arXiv:2305.04934v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#24212;&#29992;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#32467;&#26500;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#35757;&#32451;&#21518;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#65292;&#26696;&#20363;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#29983;&#25104;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#20010;&#25972;&#21512;&#20102;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#22312;&#22240;&#26524;&#22810;&#22836;&#22270;&#26426;&#21046;&#20013;&#23454;&#29616;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#34987;&#29992;&#20110;&#39044;&#27979;&#20108;&#32423;&#32467;&#26500;&#20869;&#23481;&#65288;&#27599;&#20010;&#27531;&#22522;&#30340;&#27700;&#24179;&#21644;&#24635;&#20307;&#20869;&#23481;&#65289;&#12289;&#34507;&#30333;&#36136;&#21487;&#28342;&#24615;&#21644;&#27979;&#24207;&#20219;&#21153;&#12290;&#36827;&#19968;&#27493;&#22312;&#21453;&#21521;&#20219;&#21153;&#19978;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#36825;&#20123;&#24615;&#36136;&#20316;&#20026;&#30446;&#26631;&#29305;&#24449;&#30340;&#34507;&#30333;&#36136;&#12290;&#35813;&#27169;&#22411;&#34987;&#21046;&#23450;&#20026;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23436;&#20840;&#22522;&#20110;&#25552;&#31034;&#65292;&#21487;&#20197;&#20026;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#36866;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#28155;&#21152;&#39069;&#22806;&#20219;&#21153;&#20250;&#20135;&#29983;&#30456;&#20114;&#21327;&#21516;&#20316;&#29992;&#65292;&#20351;&#27169;&#22411;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#24471;&#21040;&#25552;&#39640;&#65292;&#36229;&#36807;&#20165;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#26696;&#20363;&#30740;&#31350;&#29992;&#20110;&#39564;&#35777;&#35813;&#26041;&#27861;&#65292;&#29983;&#25104;&#20855;&#26377;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#65292;&#21253;&#25324;&#31283;&#23450;&#24615;&#21644;&#21487;&#28342;&#24615;&#30340;&#34507;&#30333;&#36136;&#65292;&#24182;&#36827;&#34892;&#23454;&#39564;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report a flexible language-model based deep learning strategy, applied here to solve complex forward and inverse problems in protein modeling, based on an attention neural network that integrates transformer and graph convolutional architectures in a causal multi-headed graph mechanism, to realize a generative pretrained model. The model is applied to predict secondary structure content (per-residue level and overall content), protein solubility, and sequencing tasks. Further trained on inverse tasks, the model is rendered capable of designing proteins with these properties as target features. The model is formulated as a general framework, completely prompt-based, and can be adapted for a variety of downstream tasks. We find that adding additional tasks yields emergent synergies that the model exploits in improving overall performance, beyond what would be possible by training a model on each dataset alone. Case studies are presented to validate the method, yielding protein designs
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#22810;&#31181;&#27835;&#30103;&#30340;&#22240;&#26524;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#21307;&#30103;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#22122;&#22768;&#22810;&#30340;&#21307;&#30103;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03829</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22240;&#26524;&#27169;&#22411;&#25552;&#39640;&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#21307;&#30103;
&lt;/p&gt;
&lt;p&gt;
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models. (arXiv:2305.03829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#22810;&#31181;&#27835;&#30103;&#30340;&#22240;&#26524;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#21307;&#30103;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#22122;&#22768;&#22810;&#30340;&#21307;&#30103;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#21307;&#30103;&#26088;&#22312;&#26681;&#25454;&#20010;&#20307;&#30340;&#29420;&#29305;&#25104;&#20687;&#29305;&#24449;&#20010;&#24615;&#21270;&#35786;&#30103;&#20915;&#31574;&#65292;&#20197;&#25913;&#21892;&#20854;&#20020;&#24202;&#32467;&#26524;&#12290;&#38598;&#25104;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20316;&#20026;&#27835;&#30103;&#24314;&#35758;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#23558;&#26356;&#21152;&#23433;&#20840;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#22312;&#31934;&#20934;&#21307;&#30103;&#20013;&#65292;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25216;&#26415;&#21644;&#39564;&#35777;&#25351;&#26631;&#12290;&#26412;&#25991;&#37319;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#22810;&#31181;&#27835;&#30103;&#30340;&#22240;&#26524;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#20272;&#35745;&#27599;&#31181;&#27835;&#30103;&#36873;&#39033;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#20219;&#24847;&#20004;&#31181;&#27835;&#30103;&#20043;&#38388;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;&#25105;&#20204;&#23545;&#24739;&#26377;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#30340;&#24739;&#32773;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20197;&#39044;&#27979;&#26032;&#30340;&#21644;&#25193;&#22823;&#30340;T2&#30149;&#21464;&#25968;&#37327;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-based precision medicine aims to personalize treatment decisions based on an individual's unique imaging features so as to improve their clinical outcome. Machine learning frameworks that integrate uncertainty estimation as part of their treatment recommendations would be safer and more reliable. However, little work has been done in adapting uncertainty estimation techniques and validation metrics for precision medicine. In this paper, we use Bayesian deep learning for estimating the posterior distribution over factual and counterfactual outcomes on several treatments. This allows for estimating the uncertainty for each treatment option and for the individual treatment effects (ITE) between any two treatments. We train and evaluate this model to predict future new and enlarging T2 lesion counts on a large, multi-center dataset of MR brain images of patients with multiple sclerosis, exposed to several treatments during randomized controlled trials. We evaluate the correlation of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Stack Overflow&#19978;&#30340;Java&#32534;&#31243;&#35821;&#35328;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.03017</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study. (arXiv:2305.03017v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Stack Overflow&#19978;&#30340;Java&#32534;&#31243;&#35821;&#35328;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21644;&#26368;&#36817;&#19968;&#30452;&#22312;&#36827;&#34892;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#23436;&#25104;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#12290;&#30001;&#20110;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#22312;&#20114;&#32852;&#32593;&#19978;&#23547;&#25214;&#30456;&#20851;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#21033;&#29992;&#24320;&#28304;&#39033;&#30446;&#21644;&#38750;&#27491;&#24335;&#25991;&#26723;&#12290;&#20026;&#20102;&#25214;&#21040;&#26377;&#29992;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#38750;&#27491;&#24335;&#25991;&#26723;&#65288;&#22914;Stack Overflow&#35752;&#35770;&#21644;&#35770;&#22363;&#65289;&#21487;&#20197;&#38750;&#24120;&#23453;&#36149;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;Stack Overflow&#65292;&#23427;&#26159;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#35752;&#35770;&#19981;&#21516;&#20027;&#39064;&#30340;&#27969;&#34892;&#36164;&#28304;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#33616;&#20195;&#30721;&#31034;&#20363;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#25512;&#33616;&#20102;Java&#32534;&#31243;&#35821;&#35328;&#20013;&#26368;&#20339;&#30340;&#20195;&#30721;&#31034;&#20363;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;BERT&#26469;&#36827;&#34892;&#22788;&#29702;&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#26159;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of code example recommendation has been conducted extensively in the past and recently in order to assist developers in their software development tasks. This is because developers often spend significant time searching for relevant code examples on the internet, utilizing open-source projects and informal documentation. For finding useful code examples, informal documentation, such as Stack Overflow discussions and forums, can be invaluable. We have focused our research on Stack Overflow, which is a popular resource for discussing different topics among software developers. For increasing the quality of the recommended code examples, we have collected and recommended the best code examples in the Java programming language. We have utilized BERT in our approach, which is a Large Language Model (LLM) for text representation that can effectively extract semantic information from textual data. Our first step involved using BERT to convert code examples into numerical vectors. Su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#38382;&#39064;&#65292;&#26126;&#30830;&#23545;&#25239;&#29615;&#22659;&#19979;&#38656;&#35201;&#39069;&#22806;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#25552;&#20986;&#21033;&#29992;&#24265;&#20215;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#30340;&#26032;&#26041;&#26696;&#38477;&#20302;&#35843;&#33410;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.02497</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#25239;&#40065;&#26834;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
Hyper-parameter Tuning for Adversarially Robust Models. (arXiv:2304.02497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#38382;&#39064;&#65292;&#26126;&#30830;&#23545;&#25239;&#29615;&#22659;&#19979;&#38656;&#35201;&#39069;&#22806;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#25552;&#20986;&#21033;&#29992;&#24265;&#20215;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#30340;&#26032;&#26041;&#26696;&#38477;&#20302;&#35843;&#33410;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#38382;&#39064;&#65292;&#26088;&#22312;&#30830;&#23450;&#22312;&#23545;&#25239;&#29615;&#22659;&#19979;&#21738;&#20123;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#26159;&#38656;&#35201;&#35843;&#33410;&#30340;&#65292;&#21516;&#26102;&#38477;&#20302;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#30340;&#35843;&#33410;&#25104;&#26412;&#12290;&#36890;&#36807;&#23545;3&#20010;&#24191;&#27867;&#24212;&#29992;&#20110;&#20808;&#21069;&#26377;&#20851;&#23545;&#25239;&#40065;&#26834;&#24615;&#25991;&#29486;&#20013;&#30340;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#23545;&#36825;&#19968;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#31350;&#65292;&#24182;&#21457;&#29616;&#35813;&#38382;&#39064;&#22312;&#23545;&#25239;&#29615;&#22659;&#19979;&#30340;&#22797;&#26434;&#24615;&#20027;&#35201;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#38656;&#35201;&#35843;&#25972;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#26469;&#24179;&#34913;&#26631;&#20934;&#35757;&#32451;&#21644;&#23545;&#25239;&#35757;&#32451;&#65307;&#38656;&#35201;&#29420;&#31435;&#35843;&#25972;&#26631;&#20934;&#35757;&#32451;&#21644;&#23545;&#25239;&#35757;&#32451;&#38454;&#27573;&#30340;&#36229;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#21033;&#29992;&#24265;&#20215;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#26469;&#38477;&#20302;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#36229;&#21442;&#25968;&#35843;&#33410;&#25104;&#26412;&#30340;&#26032;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work focuses on the problem of hyper-parameter tuning (HPT) for robust (i.e., adversarially trained) models, with the twofold goal of i) establishing which additional HPs are relevant to tune in adversarial settings, and ii) reducing the cost of HPT for robust models. We pursue the first goal via an extensive experimental study based on 3 recent models widely adopted in the prior literature on adversarial robustness. Our findings show that the complexity of the HPT problem, already notoriously expensive, is exacerbated in adversarial settings due to two main reasons: i) the need of tuning additional HPs which balance standard and adversarial training; ii) the need of tuning the HPs of the standard and adversarial training phases independently. Fortunately, we also identify new opportunities to reduce the cost of HPT for robust models. Specifically, we propose to leverage cheap adversarial training methods to obtain inexpensive, yet highly correlated, estimations of the quality ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ImprovisetoInitialize(I2I)&#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#26469;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#36825;&#20351;&#24471;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.02168</link><description>&lt;p&gt;
I2I: &#29992;&#25913;&#36827;&#30340;&#30693;&#35782;&#21021;&#22987;&#21270;&#36716;&#25509;&#22120;
&lt;/p&gt;
&lt;p&gt;
I2I: Initializing Adapters with Improvised Knowledge. (arXiv:2304.02168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ImprovisetoInitialize(I2I)&#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#26469;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#36825;&#20351;&#24471;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#25509;&#22120;&#26159;&#24310;&#32493;&#23398;&#20064;&#20013;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#26032;&#20219;&#21153;&#35757;&#32451;&#29420;&#31435;&#30340;&#36866;&#37197;&#22120;&#27169;&#22359;&#38169;&#22833;&#20102;&#36328;&#20219;&#21153;&#30693;&#35782;&#36716;&#31227;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Improvise to Initialize (I2I) &#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#65292;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#24207;&#21015;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102; I2I &#22312; CLiMB&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#20351;&#29992; I2I &#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#22987;&#32456;&#27604;&#29420;&#31435;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#20219;&#21153;&#31934;&#24230;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20419;&#36827;&#20102;&#20219;&#21153;&#36866;&#37197;&#22120;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20808;&#36827;&#30340; AdapterFusion&#65292;I2I &#20063;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#36328;&#20219;&#21153;&#30693;&#35782;&#36716;&#31227;&#32780;&#19981;&#20135;&#29983;&#30456;&#20851;&#30340;&#21442;&#25968;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapters present a promising solution to the catastrophic forgetting problem in continual learning. However, training independent Adapter modules for every new task misses an opportunity for cross-task knowledge transfer. We propose Improvise to Initialize (I2I), a continual learning algorithm that initializes Adapters for incoming tasks by distilling knowledge from previously-learned tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning benchmark, by conducting experiments on sequences of visual question answering tasks. Adapters trained with I2I consistently achieve better task accuracy than independently-trained Adapters, demonstrating that our algorithm facilitates knowledge transfer between task Adapters. I2I also results in better cross-task knowledge transfer than the state-of-the-art AdapterFusion without incurring the associated parametric cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; DISDE &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27169;&#22411;&#22312;&#19981;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;&#35813;&#26041;&#27861;&#23558;&#24615;&#33021;&#19979;&#38477;&#20998;&#35299;&#20026;&#19977;&#20010;&#26041;&#38754;&#65306;&#38590;&#24230;&#26356;&#22823;&#20294;&#26356;&#39057;&#32321;&#20986;&#29616;&#30340;&#31034;&#20363;&#22686;&#21152;&#12289;&#29305;&#24449;&#21644;&#32467;&#26524;&#20043;&#38388;&#20851;&#31995;&#30340;&#21464;&#21270;&#21644;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#39057;&#32321;&#25110;&#26410;&#35265;&#36807;&#30340;&#31034;&#20363;&#24615;&#33021;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.02011</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#35786;&#26029;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Diagnosing Model Performance Under Distribution Shift. (arXiv:2303.02011v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; DISDE &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27169;&#22411;&#22312;&#19981;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;&#35813;&#26041;&#27861;&#23558;&#24615;&#33021;&#19979;&#38477;&#20998;&#35299;&#20026;&#19977;&#20010;&#26041;&#38754;&#65306;&#38590;&#24230;&#26356;&#22823;&#20294;&#26356;&#39057;&#32321;&#20986;&#29616;&#30340;&#31034;&#20363;&#22686;&#21152;&#12289;&#29305;&#24449;&#21644;&#32467;&#26524;&#20043;&#38388;&#20851;&#31995;&#30340;&#21464;&#21270;&#21644;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#39057;&#32321;&#25110;&#26410;&#35265;&#36807;&#30340;&#31034;&#20363;&#24615;&#33021;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#27169;&#22411;&#22312;&#19981;&#21516;&#20110;&#35757;&#32451;&#20998;&#24067;&#30340;&#30446;&#26631;&#20998;&#24067;&#19979;&#36816;&#34892;&#26102;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#20123;&#25805;&#20316;&#22833;&#36133;&#27169;&#24335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#31216;&#20026; DIstribution Shift DEcomposition&#65288;DISDE&#65289;&#65292;&#23558;&#24615;&#33021;&#19979;&#38477;&#24402;&#22240;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#24615;&#33021;&#19979;&#38477;&#20998;&#35299;&#20026;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#65306;1&#65289;&#26469;&#33258;&#35757;&#32451;&#30340;&#26356;&#38590;&#20294;&#26356;&#39057;&#32321;&#30340;&#31034;&#20363;&#22686;&#21152;&#65307;2&#65289;&#29305;&#24449;&#21644;&#32467;&#26524;&#20043;&#38388;&#20851;&#31995;&#30340;&#21464;&#21270;&#65307;3&#65289;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#39057;&#32321;&#25110;&#26410;&#35265;&#36807;&#30340;&#31034;&#20363;&#24615;&#33021;&#24046;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#22266;&#23450; $X$ &#30340;&#20998;&#24067;&#30340;&#21516;&#26102;&#25913;&#21464; $Y \mid X$ &#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#25110;&#22312;&#22266;&#23450; $Y \mid X$ &#30340;&#26465;&#20214;&#20998;&#24067;&#30340;&#21516;&#26102;&#25913;&#21464; $X$ &#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#19968;&#20010;&#20851;&#20110; $X$ &#30340;&#20551;&#35774;&#20998;&#24067;&#65292;&#20854;&#20013;&#21253;&#21547;&#35757;&#32451;&#21644;&#30446;&#26631;&#20013;&#20849;&#21516;&#30340;&#20540;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#27604;&#36739; $Y \mid X$ &#24182;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction models can perform poorly when deployed to target distributions different from the training distribution. To understand these operational failure modes, we develop a method, called DIstribution Shift DEcomposition (DISDE), to attribute a drop in performance to different types of distribution shifts. Our approach decomposes the performance drop into terms for 1) an increase in harder but frequently seen examples from training, 2) changes in the relationship between features and outcomes, and 3) poor performance on examples infrequent or unseen during training. These terms are defined by fixing a distribution on $X$ while varying the conditional distribution of $Y \mid X$ between training and target, or by fixing the conditional distribution of $Y \mid X$ while varying the distribution on $X$. In order to do this, we define a hypothetical distribution on $X$ consisting of values common in both training and target, over which it is easy to compare $Y \mid X$ and thus predictive
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;BBOB&#19978;&#20116;&#31181;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#20197;&#21450;CMA-ES&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;... (&#26681;&#25454;&#35770;&#25991;&#30340;&#20855;&#20307;&#20869;&#23481;&#36827;&#34892;&#24635;&#32467;)</title><link>http://arxiv.org/abs/2303.00890</link><description>&lt;p&gt;
BBOB&#19978;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of High-Dimensional Bayesian Optimization Algorithms on BBOB. (arXiv:2303.00890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;BBOB&#19978;&#20116;&#31181;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#20197;&#21450;CMA-ES&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;... (&#26681;&#25454;&#35770;&#25991;&#30340;&#20855;&#20307;&#20869;&#23481;&#36827;&#34892;&#24635;&#32467;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31867;&#22522;&#20110;&#40657;&#30418;&#12289;&#22522;&#20110;&#20195;&#29702;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20248;&#21270;&#35780;&#20272;&#25104;&#26412;&#39640;&#12289;&#21482;&#33021;&#25317;&#26377;&#26377;&#38480;&#30340;&#35780;&#20272;&#39044;&#31639;&#30340;&#38382;&#39064;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#35299;&#20915;&#24037;&#19994;&#30028;&#30340;&#25968;&#20540;&#20248;&#21270;&#38382;&#39064;&#20013;&#23588;&#20026;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#30446;&#26631;&#20989;&#25968;&#30340;&#35780;&#20272;&#36890;&#24120;&#20381;&#36182;&#32791;&#26102;&#30340;&#27169;&#25311;&#25110;&#29289;&#29702;&#23454;&#39564;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24037;&#19994;&#38382;&#39064;&#28041;&#21450;&#22823;&#37327;&#21442;&#25968;&#65292;&#36825;&#32473;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#20854;&#24615;&#33021;&#22312;&#32500;&#24230;&#36229;&#36807;15&#20010;&#21464;&#37327;&#26102;&#24120;&#24120;&#19979;&#38477;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#21738;&#31181;&#31639;&#27861;&#22312;&#21738;&#31181;&#20248;&#21270;&#22330;&#26223;&#20013;&#34920;&#29616;&#26368;&#22909;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;5&#31181;&#26368;&#26032;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#19982;&#20256;&#32479;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;CMA-ES&#31639;&#27861;&#22312;COCA&#29615;&#22659;&#19979;24&#20010;BBOB&#20989;&#25968;&#19978;&#30340;&#24615;&#33021;&#65292;&#22312;&#32500;&#24230;&#20174;10&#21040;60&#20010;&#21464;&#37327;&#19981;&#26029;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#23454;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is a class of black-box, surrogate-based heuristics that can efficiently optimize problems that are expensive to evaluate, and hence admit only small evaluation budgets. BO is particularly popular for solving numerical optimization problems in industry, where the evaluation of objective functions often relies on time-consuming simulations or physical experiments. However, many industrial problems depend on a large number of parameters. This poses a challenge for BO algorithms, whose performance is often reported to suffer when the dimension grows beyond 15 variables. Although many new algorithms have been proposed to address this problem, it is not well understood which one is the best for which optimization scenario.  In this work, we compare five state-of-the-art high-dimensional BO algorithms, with vanilla BO and CMA-ES on the 24 BBOB functions of the COCO environment at increasing dimensionality, ranging from 10 to 60 variables. Our results confirm the su
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#23548;&#21521;&#24615;&#30340;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#27880;&#24341;&#23548;&#22312;&#22270;&#20687;&#20013;&#30452;&#25509;&#25511;&#21046;&#23545;&#35937;&#30340;&#25918;&#32622;&#12290;&#36890;&#36807;&#35266;&#23519;&#25552;&#31034;&#35789;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26144;&#23556;&#65292;&#24341;&#20837;&#20248;&#21270;&#30446;&#26631;&#65292;&#22312;&#29305;&#23450;&#20301;&#32622;&#20135;&#29983;&#8220;&#28608;&#27963;&#8221;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22330;&#26223;&#32452;&#21512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.13153</link><description>&lt;p&gt;
&#26377;&#23548;&#21521;&#24615;&#30340;&#25193;&#25955;: &#36890;&#36807;&#20851;&#27880;&#24341;&#23548;&#23454;&#29616;&#23545;&#35937;&#25918;&#32622;&#30340;&#30452;&#25509;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Directed Diffusion: Direct Control of Object Placement through Attention Guidance. (arXiv:2302.13153v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#23548;&#21521;&#24615;&#30340;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#27880;&#24341;&#23548;&#22312;&#22270;&#20687;&#20013;&#30452;&#25509;&#25511;&#21046;&#23545;&#35937;&#30340;&#25918;&#32622;&#12290;&#36890;&#36807;&#35266;&#23519;&#25552;&#31034;&#35789;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26144;&#23556;&#65292;&#24341;&#20837;&#20248;&#21270;&#30446;&#26631;&#65292;&#22312;&#29305;&#23450;&#20301;&#32622;&#20135;&#29983;&#8220;&#28608;&#27963;&#8221;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22330;&#26223;&#32452;&#21512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;DALLE-2&#12289;Imagen&#21644;&#31283;&#23450;&#25193;&#25955;&#65289;&#33021;&#22815;&#20165;&#36890;&#36807;&#25551;&#36848;&#25152;&#38656;&#22270;&#20687;&#20869;&#23481;&#30340;&#31616;&#30701;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#21508;&#31181;&#24418;&#24335;&#30340;&#22270;&#20687;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#22270;&#20687;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#32452;&#21512;&#21253;&#21547;&#22810;&#20010;&#20851;&#38190;&#23545;&#35937;&#65288;&#22914;&#25351;&#23450;&#20301;&#32622;&#20851;&#31995;&#20013;&#30340;&#23383;&#31526;&#65289;&#30340;&#22330;&#26223;&#12290;&#22312;&#35762;&#36848;&#25925;&#20107;&#20013;&#65292;&#33021;&#22815;&#8220;&#30452;&#25509;&#8221;&#25351;&#23548;&#20154;&#29289;&#21644;&#23545;&#35937;&#30340;&#25918;&#32622;&#65292;&#26080;&#35770;&#26159;&#22312;&#22270;&#20687;&#20869;&#36824;&#26159;&#36328;&#22270;&#20687;&#20869;&#65292;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#19968;&#28857;&#22312;&#30005;&#24433;&#21644;&#21160;&#30011;&#29702;&#35770;&#30340;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#35748;&#21487;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#29305;&#21035;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25552;&#20379;&#25152;&#38656;&#30340;&#25351;&#23548;&#12290;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#25552;&#31034;&#35789;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26144;&#23556;&#21453;&#26144;&#20986;&#30001;&#36825;&#20123;&#35789;&#25152;&#34920;&#31034;&#30340;&#23545;&#35937;&#30340;&#31354;&#38388;&#24067;&#23616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20248;&#21270;&#30446;&#26631;&#65292;&#22312;&#36825;&#20123;&#20132;&#21449;&#27880;&#24847;&#21147;&#26144;&#23556;&#20013;&#30340;&#26399;&#26395;&#20301;&#32622;&#20135;&#29983;&#8220;&#28608;&#27963;&#8221;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26041;&#27861;&#26159;&#36890;&#21521;&#27867;&#21270;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-guided diffusion models such as DALLE-2, Imagen, and Stable Diffusion are able to generate an effectively endless variety of images given only a short text prompt describing the desired image content. In many cases the images are of very high quality. However, these models often struggle to compose scenes containing several key objects such as characters in specified positional relationships. The missing capability to "direct" the placement of characters and objects both within and across images is crucial in storytelling, as recognized in the literature on film and animation theory. In this work, we take a particularly straightforward approach to providing the needed direction. Drawing on the observation that the cross-attention maps for prompt words reflect the spatial layout of objects denoted by those words, we introduce an optimization objective that produces ``activation'' at desired positions in these cross-attention maps. The resulting approach is a step toward generalizin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33033;&#20914;&#24418;&#29366;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24674;&#22797;PMT&#39281;&#21644;&#21709;&#24212;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20272;&#35745;&#32447;&#24615;&#21306;&#22495;&#24182;&#25552;&#39640;&#20809;&#23376;&#35745;&#25968;&#21644;&#33021;&#37327;&#37325;&#24314;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.06170</link><description>&lt;p&gt;
&#20351;&#29992;&#33033;&#20914;&#24418;&#29366;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24674;&#22797;PMT&#30340;&#39281;&#21644;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Restoring the saturation response of a PMT using pulse-shape and artificial-neural-networks. (arXiv:2302.06170v2 [physics.ins-det] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06170
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33033;&#20914;&#24418;&#29366;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24674;&#22797;PMT&#39281;&#21644;&#21709;&#24212;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20272;&#35745;&#32447;&#24615;&#21306;&#22495;&#24182;&#25552;&#39640;&#20809;&#23376;&#35745;&#25968;&#21644;&#33021;&#37327;&#37325;&#24314;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#20493;&#22686;&#31649;&#65288;PMT&#65289;&#30340;&#32447;&#24615;&#21709;&#24212;&#26159;&#20809;&#23376;&#35745;&#25968;&#21644;&#20013;&#24494;&#23376;&#33021;&#37327;&#37325;&#24314;&#30340;&#24517;&#35201;&#23646;&#24615;&#12290;&#20351;&#29992;&#22522;&#20110;&#32447;&#24615;&#28919;&#22522;&#33519;&#65288;LAB&#65289;&#30340;&#28082;&#20307;&#38378;&#28865;&#20307;&#30740;&#31350;&#20102;PMT&#30340;&#32447;&#24615;&#26377;&#25928;&#21306;&#22495;&#21644;&#39281;&#21644;&#21709;&#24212;&#12290;&#35266;&#23519;&#21040;&#20102;&#20004;&#31181;&#19981;&#21516;&#39281;&#21644;&#21709;&#24212;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21363;&#33033;&#20914;&#24418;&#29366;&#22833;&#30495;&#21644;&#33033;&#20914;&#38754;&#31215;&#20943;&#23567;&#12290;&#35266;&#23519;&#21040;&#30340;&#33033;&#20914;&#24418;&#29366;&#20026;&#20272;&#35745;&#33033;&#20914;&#38754;&#31215;&#30456;&#23545;&#32447;&#24615;&#21306;&#22495;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#35786;&#26029;&#20801;&#35768;&#21407;&#22320;&#20272;&#35745;&#32447;&#24615;&#33539;&#22260;&#65292;&#36825;&#22312;&#20197;&#21069;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#21033;&#29992;&#27979;&#24471;&#30340;&#20004;&#20010;&#39281;&#21644;&#21709;&#24212;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#26469;&#39044;&#27979;&#20174;&#35266;&#23519;&#21040;&#30340;&#33033;&#20914;&#24418;&#29366;&#20013;&#20943;&#23567;&#30340;&#33033;&#20914;&#38754;&#31215;&#12290;ANN&#39044;&#27979;&#30340;&#33033;&#20914;&#38754;&#31215;&#20943;&#23567;&#20351;&#24471;&#21487;&#20197;&#39044;&#27979;&#29420;&#31435;&#20110;&#39281;&#21644;&#34892;&#20026;&#30340;&#29702;&#24819;&#20809;&#30005;&#23376;&#25968;&#12290;&#36825;&#31181;&#22522;&#20110;&#33033;&#20914;&#24418;&#29366;&#30340;&#26041;&#27861;&#20272;&#35745;PMT&#30340;&#32447;&#24615;&#33539;&#22260;&#24182;&#20351;&#29992;ANN&#24674;&#22797;&#39281;&#21644;&#21709;&#24212;&#26159;&#22686;&#24378;&#20013;&#24494;&#23376;&#25506;&#27979;&#22120;&#20013;&#20809;&#23376;&#35745;&#25968;&#21644;&#33021;&#37327;&#37325;&#24314;&#25928;&#29575;&#30340;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The linear response of a photomultiplier tube (PMT) is a required property for photon counting and reconstruction of the neutrino energy. The linearity valid region and the saturation response of PMT were investigated using a linear-alkyl-benzene (LAB)-based liquid scintillator. A correlation was observed between the two different saturation responses, with pulse-shape distortion and pulse-area decrease. The observed pulse-shape provides useful information for the estimation of the linearity region relative to the pulse-area. This correlation-based diagnosis allows an ${in}$-${situ}$ estimation of the linearity range, which was previously challenging. The measured correlation between the two saturation responses was employed to train an artificial-neural-network (ANN) to predict the decrease in pulse-area from the observed pulse-shape. The ANN-predicted pulse-area decrease enables the prediction of the ideal number of photoelectrons irrelevant to the saturation behavior. This pulse-sha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23618;&#27425;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#23398;&#26415;&#20986;&#29256;&#29289;&#36890;&#36807;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#30740;&#31350;&#27963;&#21160;&#22312;&#19981;&#21516;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20801;&#35768;&#36328;&#23398;&#31185;&#21644;&#36328;&#39046;&#22495;&#30340;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2302.00390</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;"Web of Science"&#20013;&#23545;&#30740;&#31350;&#39046;&#22495;&#36827;&#34892;&#23618;&#27425;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Classification of Research Fields in the "Web of Science" Using Deep Learning. (arXiv:2302.00390v2 [cs.DL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23618;&#27425;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#23398;&#26415;&#20986;&#29256;&#29289;&#36890;&#36807;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#30740;&#31350;&#27963;&#21160;&#22312;&#19981;&#21516;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20801;&#35768;&#36328;&#23398;&#31185;&#21644;&#36328;&#39046;&#22495;&#30340;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#20998;&#31867;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#25277;&#35937;&#23558;&#23398;&#26415;&#20986;&#29256;&#29289;&#33258;&#21160;&#20998;&#31867;&#21040;&#19977;&#32423;&#23618;&#27425;&#26631;&#31614;&#38598;&#65288;&#23398;&#31185;&#12289;&#39046;&#22495;&#12289;&#23376;&#39046;&#22495;&#65289;&#20013;&#65292;&#20197;&#22810;&#31867;&#21035;&#35774;&#32622;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25991;&#31456;&#30340;&#30693;&#35782;&#29983;&#20135;&#21644;&#24341;&#29992;&#30340;&#24433;&#21709;&#65292;&#23545;&#30740;&#31350;&#27963;&#21160;&#22312;&#25152;&#36848;&#23618;&#27425;&#32467;&#26500;&#20013;&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#31867;&#65292;&#24182;&#20801;&#35768;&#36825;&#20123;&#27963;&#21160;&#34987;&#24402;&#20026;&#22810;&#20010;&#31867;&#21035;&#12290;&#35813;&#20998;&#31867;&#31995;&#32479;&#22312;Microsoft Academic Graph (&#29256;&#26412;2018-05-17)&#30340;160 million&#20221;&#25688;&#35201;&#29255;&#27573;&#20013;&#21306;&#20998;&#20102;44&#20010;&#23398;&#31185;&#12289;718&#20010;&#39046;&#22495;&#21644;1,485&#20010;&#23376;&#39046;&#22495;&#12290;&#25105;&#20204;&#20197;&#27169;&#22359;&#21270;&#21644;&#20998;&#24067;&#24335;&#26041;&#24335;&#36827;&#34892;&#25209;&#37327;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#21644;&#20801;&#35768;&#36328;&#23398;&#31185;&#21644;&#36328;&#39046;&#22495;&#30340;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#24635;&#20849;&#65292;&#25105;&#20204;&#22312;&#25152;&#26377;&#32771;&#34385;&#30340;&#27169;&#22411;&#65288;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#21464;&#24418;&#22120;&#65289;&#20013;&#36827;&#34892;&#20102;3,140&#27425;&#23454;&#39564;&#12290;&#20998;&#31867;&#20934;&#30830;&#29575;&gt; 90&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a hierarchical classification system that automatically categorizes a scholarly publication using its abstract into a three-tier hierarchical label set (discipline, field, subfield) in a multi-class setting. This system enables a holistic categorization of research activities in the mentioned hierarchy in terms of knowledge production through articles and impact through citations, permitting those activities to fall into multiple categories. The classification system distinguishes 44 disciplines, 718 fields and 1,485 subfields among 160 million abstract snippets in Microsoft Academic Graph (version 2018-05-17). We used batch training in a modularized and distributed fashion to address and allow for interdisciplinary and interfield classifications in single-label and multi-label settings. In total, we have conducted 3,140 experiments in all considered models (Convolutional Neural Networks, Recurrent Neural Networks, Transformers). The classification accuracy is &gt; 90%
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#22686;&#24378;&#26041;&#27861;&#23545;&#23398;&#20064;&#21040;&#30340;&#33016;&#37096; X &#20809;&#29255;&#24322;&#24120;&#26816;&#27979;&#30340;&#23402;&#29983;&#34920;&#31034;&#30340;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19968;&#32452;&#33021;&#22815;&#20135;&#29983;&#33391;&#22909;&#27867;&#21270;&#25928;&#26524;&#30340;&#40065;&#26834;&#34920;&#31034;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12636</link><description>&lt;p&gt;
&#25506;&#31350;&#29992;&#20110;&#33016;&#37096; X &#20809;&#29255;&#30340;&#23402;&#29983;&#34920;&#31034;&#23398;&#20064;&#30340;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays. (arXiv:2301.12636v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#22686;&#24378;&#26041;&#27861;&#23545;&#23398;&#20064;&#21040;&#30340;&#33016;&#37096; X &#20809;&#29255;&#24322;&#24120;&#26816;&#27979;&#30340;&#23402;&#29983;&#34920;&#31034;&#30340;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19968;&#32452;&#33021;&#22815;&#20135;&#29983;&#33391;&#22909;&#27867;&#21270;&#25928;&#26524;&#30340;&#40065;&#26834;&#34920;&#31034;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#22686;&#24378;&#23545;&#20110;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#33258;&#28982;&#22270;&#20687;&#30340;&#22686;&#24378;&#31574;&#30053;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#21307;&#23398;&#22270;&#20687;&#19982;&#33258;&#28982;&#22270;&#20687;&#26377;&#24456;&#22823;&#30340;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#28165;&#26970;&#22312;&#23402;&#29983;&#34920;&#31034;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#22686;&#24378;&#31574;&#30053;&#26159;&#21542;&#36866;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#65292;&#20197;&#21450;&#36866;&#29992;&#30340;&#31243;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;&#22686;&#24378;&#26041;&#27861;&#23545;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#30340;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;MIMIC-CXR&#12289;CheXpert &#21644; VinDR-CXR&#65289;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;&#29992;&#20110;&#33016;&#37096; X &#20809;&#29255;&#24322;&#24120;&#26816;&#27979;&#30340;&#23402;&#29983;&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#32447;&#24615;&#25506;&#27979;&#12289;&#24494;&#35843;&#12289;&#38646;&#26679;&#26412;&#36801;&#31227;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#23454;&#39564;&#26469;&#30740;&#31350;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#32452;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image augmentations are quintessential for effective visual representation learning across self-supervised learning techniques. While augmentation strategies for natural imaging have been studied extensively, medical images are vastly different from their natural counterparts. Thus, it is unknown whether common augmentation strategies employed in Siamese representation learning generalize to medical images and to what extent. To address this challenge, in this study, we systematically assess the effect of various augmentations on the quality and robustness of the learned representations. We train and evaluate Siamese Networks for abnormality detection on chest X-Rays across three large datasets (MIMIC-CXR, CheXpert and VinDR-CXR). We investigate the efficacy of the learned representations through experiments involving linear probing, fine-tuning, zero-shot transfer, and data efficiency. Finally, we identify a set of augmentations that yield robust representations that generalize well t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#20998;&#25968;&#36866;&#24212;&#27169;&#22411;&#26469;&#37325;&#26032;&#26657;&#20934;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#20998;&#25968;&#20197;&#35299;&#20915;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#22312;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.12313</link><description>&lt;p&gt;
&#29992;&#20110;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#30340;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#30340;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Adapting Neural Link Predictors for Complex Query Answering. (arXiv:2301.12313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#20998;&#25968;&#36866;&#24212;&#27169;&#22411;&#26469;&#37325;&#26032;&#26657;&#20934;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#20998;&#25968;&#20197;&#35299;&#20915;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#22312;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#23436;&#25972;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#27169;&#22411;&#38656;&#35201;&#22312;&#32570;&#22833;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#22238;&#31572;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#12290;&#26368;&#36817;&#65292;Arakelyan&#31561;&#20154;&#65288;2021&#65289;&#65307;Minervini&#31561;&#20154;&#65288;2022&#65289;&#34920;&#26126;&#65292;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#20063;&#21487;&#20197;&#29992;&#20110;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#65306;&#20182;&#20204;&#30340;&#36830;&#32493;&#26597;&#35810;&#20998;&#35299;&#65288;CQD&#65289;&#26041;&#27861;&#36890;&#36807;&#23558;&#22797;&#26434;&#26597;&#35810;&#20998;&#35299;&#20026;&#21407;&#23376;&#23376;&#26597;&#35810;&#65292;&#20351;&#29992;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#22238;&#31572;&#24182;&#36890;&#36807;t-&#33539;&#25968;&#26469;&#32858;&#21512;&#20854;&#20998;&#25968;&#65292;&#20197;&#23545;&#27599;&#20010;&#22797;&#26434;&#26597;&#35810;&#30340;&#31572;&#26696;&#36827;&#34892;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;CQD&#19981;&#22788;&#29702;&#21542;&#23450;&#24182;&#19988;&#20165;&#20351;&#29992;&#21407;&#23376;&#35757;&#32451;&#26597;&#35810;&#30340;&#35757;&#32451;&#20449;&#21495;&#65306;&#22312;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#26399;&#38388;&#65292;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#20998;&#25968;&#27809;&#26377;&#36890;&#36807;&#27169;&#31946;&#36923;&#36753;t-&#33539;&#25968;&#36827;&#34892;&#26657;&#20934;&#20197;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#20998;&#25968;&#36866;&#24212;&#27169;&#22411;&#26469;&#37325;&#26032;&#26657;&#20934;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#20998;&#25968;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#36825;&#20010;&#26032;&#32452;&#20214;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#27861;&#22312;&#22797;&#26434;&#26597;&#35810;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering complex queries on incomplete knowledge graphs is a challenging task where a model needs to answer complex logical queries in the presence of missing knowledge. Recently, Arakelyan et al. (2021); Minervini et al. (2022) showed that neural link predictors could also be used for answering complex queries: their Continuous Query Decomposition (CQD) method works by decomposing complex queries into atomic sub-queries, answers them using neural link predictors and aggregates their scores via t-norms for ranking the answers to each complex query. However, CQD does not handle negations and only uses the training signal from atomic training queries: neural link prediction scores are not calibrated to interact together via fuzzy logic t-norms during complex query answering. In this work, we propose to address this problem by training a parameter-efficient score adaptation model to re-calibrate neural link prediction scores: this new component is trained on complex queries by back-propa
&lt;/p&gt;</description></item><item><title>ClimaX&#26159;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2301.10343</link><description>&lt;p&gt;
ClimaX:&#19968;&#31181;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ClimaX: A foundation model for weather and climate. (arXiv:2301.10343v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10343
&lt;/p&gt;
&lt;p&gt;
ClimaX&#26159;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#20808;&#36827;&#30340;&#22825;&#27668;&#21644;&#27668;&#20505;&#27169;&#22411;&#37117;&#26159;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#25968;&#20540;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#27169;&#25311;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#22810;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#24456;&#38590;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#36825;&#26679;&#30340;&#25968;&#20540;&#27169;&#22411;&#22312;&#27169;&#25311;&#32454;&#31890;&#24230;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#22823;&#27668;&#29616;&#35937;&#26102;&#35745;&#31639;&#37327;&#24456;&#22823;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#21151;&#33021;&#26144;&#23556;&#26469;&#30452;&#25509;&#35299;&#20915;&#19979;&#28216;&#39044;&#27979;&#25110;&#25237;&#23556;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#26159;&#20351;&#29992;&#20026;&#29305;&#23450;&#26102;&#31354;&#20219;&#21153;&#31574;&#21010;&#21644;&#21516;&#36136;&#21270;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#32570;&#20047;&#25968;&#20540;&#27169;&#22411;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#28436;&#31034;&#20102;ClimaX&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#36328;&#36234;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most state-of-the-art approaches for weather and climate modeling are based on physics-informed numerical models of the atmosphere. These approaches aim to model the non-linear dynamics and complex interactions between multiple variables, which are challenging to approximate. Additionally, many such numerical models are computationally intensive, especially when modeling the atmospheric phenomenon at a fine-grained spatial and temporal resolution. Recent data-driven approaches based on machine learning instead aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of numerical models. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning dif
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#24490;&#29615;&#39044;&#27979;&#23398;&#20064;&#31574;&#30053;&#30340;&#32806;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;CPINN&#65289;&#65292;&#29992;&#20110;&#36719;&#20256;&#24863;&#24314;&#27169;&#20013;&#30340;&#21547;&#38750;&#27979;&#37327;&#28304;&#39033;&#30340;PDEs&#27714;&#35299;&#65292;&#35777;&#26126;&#20102;CPINN&#20855;&#26377;&#28385;&#36275;PDEs&#35299;&#30340;&#36817;&#20284;&#23481;&#37327;&#65292;&#25552;&#39640;&#20102;&#36719;&#20256;&#24863;&#24314;&#27169;&#22312;&#26102;&#31354;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2301.08618</link><description>&lt;p&gt;
&#20351;&#29992;&#24490;&#29615;&#39044;&#27979;&#32806;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#36719;&#20256;&#24863;&#24314;&#27169;&#20013;&#27714;&#35299;&#21547;&#38750;&#27979;&#37327;&#28304;&#39033;&#30340;PDEs
&lt;/p&gt;
&lt;p&gt;
Solving PDEs with Unmeasurable Source Terms Using Coupled Physics-Informed Neural Network with Recurrent Prediction in Soft Sensor Modeling. (arXiv:2301.08618v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08618
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#24490;&#29615;&#39044;&#27979;&#23398;&#20064;&#31574;&#30053;&#30340;&#32806;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;CPINN&#65289;&#65292;&#29992;&#20110;&#36719;&#20256;&#24863;&#24314;&#27169;&#20013;&#30340;&#21547;&#38750;&#27979;&#37327;&#28304;&#39033;&#30340;PDEs&#27714;&#35299;&#65292;&#35777;&#26126;&#20102;CPINN&#20855;&#26377;&#28385;&#36275;PDEs&#35299;&#30340;&#36817;&#20284;&#23481;&#37327;&#65292;&#25552;&#39640;&#20102;&#36719;&#20256;&#24863;&#24314;&#27169;&#22312;&#26102;&#31354;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21547;&#38750;&#27979;&#37327;&#28304;&#39033;&#30340;&#38750;&#40784;&#27425;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#22312;&#36719;&#20256;&#24863;&#24314;&#27169;&#20013;&#38590;&#20197;&#24456;&#22909;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#24490;&#29615;&#39044;&#27979;&#65288;RP&#65289;&#23398;&#20064;&#31574;&#30053;&#30340;&#32806;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;CPINN&#65289;&#26469;&#36827;&#34892;&#36719;&#20256;&#24863;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#21253;&#21547;NetU&#21644;NetG&#30340;CPINN&#65292;&#20854;&#20013;NetU&#29992;&#20110;&#36817;&#20284;&#30740;&#31350;PDEs&#30340;&#35299;&#65292;NetG&#29992;&#20110;&#27491;&#21017;&#21270;NetU&#30340;&#35757;&#32451;&#65292;&#20004;&#20010;&#32593;&#32476;&#34987;&#38598;&#25104;&#21040;&#19968;&#20010;&#25968;&#25454;-&#29289;&#29702;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#20013;&#12290;&#38543;&#21518;&#65292;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;CPINN&#20855;&#26377;&#28385;&#36275;PDEs&#35299;&#30340;&#36817;&#20284;&#23481;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#35757;&#32451;&#31574;&#30053;&#26469;&#20248;&#21270;&#21644;&#32806;&#21512;&#20004;&#20010;&#32593;&#32476;&#20197;&#23454;&#29616;CPINN&#30340;&#21442;&#25968;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#22312;NetU&#20013;&#24341;&#20837;&#24490;&#29615;&#26426;&#21046;&#26469;&#36827;&#34892;&#26410;&#26469;&#29366;&#24577;&#30340;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;NetU-RP&#65292;&#20197;&#25552;&#39640;&#36719;&#20256;&#24863;&#24314;&#27169;&#22312;&#26102;&#31354;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#27169;&#25311;&#25391;&#21160;&#20301;&#31227;&#31995;&#32479;&#30340;&#20363;&#23376;&#65292;&#28436;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;CPINN-RP&#26041;&#27861;&#22312;&#27714;&#35299;&#21547;&#38750;&#27979;&#37327;&#28304;&#39033;&#30340;PDEs&#21644;&#36719;&#20256;&#24863;&#24314;&#27169;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonhomogeneous partial differential equations (PDEs) are an applicable model in soft sensor modeling for describing spatiotemporal industrial systems with unmeasurable source terms, which cannot be well solved by existing physics-informed neural networks (PINNs). To this end, a coupled PINN (CPINN) with a recurrent prediction (RP) learning strategy (CPINN-RP) is proposed for soft sensor modeling in spatiotemporal industrial processes, such as vibration displacement. First, CPINN containing NetU and NetG is proposed. NetU is used to approximate the solutions to PDEs under study and NetG is used to regularize the training of NetU. The two networks are integrated into a data-physics-hybrid loss function. Then, we theoretically prove that the proposed CPINN has a satisfying approximation capacity to the PDEs solutions. Besides the theoretical aspects, we propose a hierarchical training strategy to optimize and couple the two networks to achieve the parameters of CPINN. Secondly, NetU-RP is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LATTICE&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#32858;&#31867;&#32467;&#26500;&#30340;&#28508;&#22312;bandit&#38382;&#39064;&#65292;&#24182;&#22312;&#26368;&#23567;&#21270;&#36951;&#25022;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2301.07040</link><description>&lt;p&gt;
&#20855;&#26377;&#32858;&#31867;&#32467;&#26500;&#30340;&#28508;&#22312;bandit&#38382;&#39064;&#30340;&#26368;&#20248;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Algorithms for Latent Bandits with Cluster Structure. (arXiv:2301.07040v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LATTICE&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#32858;&#31867;&#32467;&#26500;&#30340;&#28508;&#22312;bandit&#38382;&#39064;&#65292;&#24182;&#22312;&#26368;&#23567;&#21270;&#36951;&#25022;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#32858;&#31867;&#32467;&#26500;&#30340;&#28508;&#22312;bandit&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#26377;&#22810;&#20010;&#29992;&#25143;&#65292;&#27599;&#20010;&#29992;&#25143;&#37117;&#26377;&#19968;&#20010;&#30456;&#20851;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#36825;&#20123;&#29992;&#25143;&#34987;&#20998;&#25104;&#8220;&#28508;&#22312;&#8221;&#31751;&#65292;&#20351;&#24471;&#21516;&#19968;&#31751;&#20869;&#30340;&#29992;&#25143;&#30340;&#24179;&#22343;&#22870;&#21169;&#21521;&#37327;&#30456;&#21516;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#29992;&#25143;&#65292;&#25289;&#21160;&#19968;&#20010;&#25163;&#33218;&#24182;&#35266;&#23519;&#30456;&#24212;&#30340;&#22122;&#22768;&#22870;&#21169;&#12290;&#29992;&#25143;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#12290;&#36825;&#20010;&#38382;&#39064;&#23545;&#20110;&#23454;&#38469;&#30340;&#25512;&#33616;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#65292;&#24182;&#19988;&#26368;&#36817;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27599;&#20010;&#29992;&#25143;&#37117;&#29420;&#31435;&#34892;&#21160;&#65292;&#20182;&#20204;&#23558;&#19981;&#24471;&#19981;&#29420;&#31435;&#25506;&#32034;&#27599;&#20010;&#25163;&#33218;&#65292;&#24182;&#19988;&#19981;&#21487;&#36991;&#20813;&#22320;&#20135;&#29983;$\Omega(\sqrt{\mathsf{MNT}})$&#30340;&#36951;&#25022;&#65292;&#20854;&#20013;$\mathsf{M}$&#21644;$\mathsf{N}$&#20998;&#21035;&#26159;&#25163;&#33218;&#21644;&#29992;&#25143;&#30340;&#25968;&#37327;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LATTICE (&#36890;&#36807;&#30697;&#38453;&#34917;&#20840;&#23454;&#29616;&#30340;&#28508;&#22312;bandit&#38382;&#39064;)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#28508;&#22312;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#25552;&#20379;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of latent bandits with cluster structure where there are multiple users, each with an associated multi-armed bandit problem. These users are grouped into \emph{latent} clusters such that the mean reward vectors of users within the same cluster are identical. At each round, a user, selected uniformly at random, pulls an arm and observes a corresponding noisy reward. The goal of the users is to maximize their cumulative rewards. This problem is central to practical recommendation systems and has received wide attention of late \cite{gentile2014online, maillard2014latent}. Now, if each user acts independently, then they would have to explore each arm independently and a regret of $\Omega(\sqrt{\mathsf{MNT}})$ is unavoidable, where $\mathsf{M}, \mathsf{N}$ are the number of arms and users, respectively. Instead, we propose LATTICE (Latent bAndiTs via maTrIx ComplEtion) which allows exploitation of the latent cluster structure to provide the minimax optimal regret of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FedTiny&#65292;&#19968;&#20010;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#21098;&#26525;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#20869;&#23384;&#21644;&#35745;&#31639;&#21463;&#38480;&#30340;&#35774;&#22791;&#29983;&#25104;&#19987;&#38376;&#30340;&#23567;&#22411;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#20316;&#32773;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#30340;&#25209;&#24402;&#19968;&#21270;&#36873;&#25321;&#27169;&#22359;&#26469;&#35299;&#20915;&#21098;&#26525;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.01977</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Distributed Pruning Towards Tiny Neural Networks in Federated Learning. (arXiv:2212.01977v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01977
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FedTiny&#65292;&#19968;&#20010;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#21098;&#26525;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#20869;&#23384;&#21644;&#35745;&#31639;&#21463;&#38480;&#30340;&#35774;&#22791;&#29983;&#25104;&#19987;&#38376;&#30340;&#23567;&#22411;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#20316;&#32773;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#30340;&#25209;&#24402;&#19968;&#21270;&#36873;&#25321;&#27169;&#22359;&#26469;&#35299;&#20915;&#21098;&#26525;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26159;&#20943;&#23567;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#20351;&#24471;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#21487;&#20197;&#36827;&#34892;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21098;&#26525;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#35757;&#32451;&#25968;&#25454;&#26469;&#25351;&#23548;&#21098;&#26525;&#31574;&#30053;&#65292;&#20351;&#24471;&#23427;&#20204;&#23545;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;&#21644;&#20445;&#23494;&#25968;&#25454;&#38598;&#25928;&#26524;&#19981;&#22909;&#12290;&#27492;&#22806;&#65292;&#20869;&#23384;&#21644;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#21098;&#26525;&#36807;&#31243;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedTiny&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#21098;&#26525;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#20869;&#23384;&#21644;&#35745;&#31639;&#21463;&#38480;&#30340;&#35774;&#22791;&#29983;&#25104;&#19987;&#38376;&#30340;&#23567;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;FedTiny&#20013;&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65292;&#20197;&#36866;&#24212;&#31232;&#30095;&#21644;&#24265;&#20215;&#30340;&#23616;&#37096;&#35745;&#31639;&#37096;&#32626;&#22330;&#26223;&#65292;&#33258;&#36866;&#24212;&#22320;&#25628;&#32034;&#31895;&#21098;&#26525;&#21644;&#32454;&#21098;&#26525;&#30340;&#19987;&#29992;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#25209;&#24402;&#19968;&#21270;&#36873;&#25321;&#27169;&#22359;&#65292;&#26469;&#20943;&#36731;&#21098;&#26525;&#20013;&#30001;&#24322;&#36136;&#24615;&#24341;&#36215;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning is an essential technique for reducing the size and complexity of deep neural networks, enabling large-scale models on devices with limited resources. However, existing pruning approaches heavily rely on training data for guiding the pruning strategies, making them ineffective for federated learning over distributed and confidential datasets. Additionally, the memory- and computation-intensive pruning process becomes infeasible for recourse-constrained devices in federated learning. To address these challenges, we propose FedTiny, a distributed pruning framework for federated learning that generates specialized tiny models for memory- and computing-constrained devices. We introduce two key modules in FedTiny to adaptively search coarse- and finer-pruned specialized models to fit deployment scenarios with sparse and cheap local computation. First, an adaptive batch normalization selection module is designed to mitigate biases in pruning caused by the heterogeneity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#22312;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#65292;&#24182;&#25552;&#20986;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#31639;&#27861;&#36827;&#34892;&#23545;&#25163;&#35757;&#32451;&#12290;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#23545;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.11030</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Adversarial Cheap Talk. (arXiv:2211.11030v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#22312;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#65292;&#24182;&#25552;&#20986;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#31639;&#27861;&#36827;&#34892;&#23545;&#25163;&#35757;&#32451;&#12290;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#23545;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36890;&#24120;&#20551;&#23450;&#25915;&#20987;&#32773;&#21487;&#20197;&#39640;&#24230;&#29305;&#26435;&#22320;&#35775;&#38382;&#21463;&#23475;&#32773;&#30340;&#21442;&#25968;&#12289;&#29615;&#22659;&#25110;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24265;&#20215;&#20132;&#27969;MDP&#30340;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#30830;&#23450;&#24615;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#12290;&#23545;&#25163;&#19981;&#33021;&#25513;&#30422;&#22320;&#38754;&#20107;&#23454;&#65292;&#24433;&#21709;&#22522;&#26412;&#29615;&#22659;&#21160;&#24577;&#25110;&#22870;&#21169;&#20449;&#21495;&#65292;&#24341;&#20837;&#19981;&#31283;&#23450;&#24615;&#65292;&#22686;&#21152;&#38543;&#26426;&#24615;&#65292;&#30475;&#21040;&#21463;&#23475;&#32773;&#30340;&#21160;&#20316;&#25110;&#35775;&#38382;&#20182;&#20204;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#23545;&#23545;&#25163;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#26174;&#30528;&#24433;&#21709;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#12290;&#24433;&#21709;&#35757;&#32451;&#26102;&#38388;&#34920;&#29616;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#65292;&#24182;&#20026;&#29616;&#26377;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25104;&#21151;&#21644;&#22833;&#36133;&#27169;&#24335;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks in reinforcement learning (RL) often assume highly-privileged access to the victim's parameters, environment, or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk MDP in which an Adversary can merely append deterministic messages to the Victim's observation, resulting in a minimal range of influence. The Adversary cannot occlude ground truth, influence underlying environment dynamics or reward signals, introduce non-stationarity, add stochasticity, see the Victim's actions, or access their parameters. Additionally, we present a simple meta-learning algorithm called Adversarial Cheap Talk (ACT) to train Adversaries in this setting. We demonstrate that an Adversary trained with ACT still significantly influences the Victim's training and testing performance, despite the highly constrained setting. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing RL algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#26469;&#35299;&#20915;&#20256;&#32479;&#20013;&#24515;&#21270;FL&#65288;CFL&#65289;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#30740;&#31350;DFL&#19982;CFL&#30340;&#24046;&#24322;&#12289;DFL&#30340;&#22522;&#30784;&#29702;&#35770;&#12289;DFL&#26694;&#26550;&#30340;&#35774;&#35745;&#19982;&#35780;&#20272;&#20197;&#21450;DFL&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2211.08413</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;: &#22522;&#30784;&#12289;&#29616;&#29366;&#12289;&#26694;&#26550;&#12289;&#36235;&#21183;&#21644;&#25361;&#25112; (arXiv:2211.08413v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Learning: Fundamentals, State-of-the-art, Frameworks, Trends, and Challenges. (arXiv:2211.08413v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#26469;&#35299;&#20915;&#20256;&#32479;&#20013;&#24515;&#21270;FL&#65288;CFL&#65289;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#30740;&#31350;DFL&#19982;CFL&#30340;&#24046;&#24322;&#12289;DFL&#30340;&#22522;&#30784;&#29702;&#35770;&#12289;DFL&#26694;&#26550;&#30340;&#35774;&#35745;&#19982;&#35780;&#20272;&#20197;&#21450;DFL&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#22312;&#19981;&#20849;&#20139;&#25935;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#21327;&#20316;&#27169;&#22411;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#12290;&#33258;&#38382;&#19990;&#20197;&#26469;&#65292;&#20013;&#24515;&#21270;FL&#65288;CFL&#65289;&#19968;&#30452;&#26159;&#25991;&#29486;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#20013;&#24515;&#21270;&#23454;&#20307;&#21019;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20013;&#24515;&#21270;&#26041;&#27861;&#20250;&#23548;&#33268;&#29942;&#39048;&#22686;&#21152;&#12289;&#31995;&#32479;&#25925;&#38556;&#39118;&#38505;&#22686;&#39640;&#65292;&#24433;&#21709;&#36127;&#36131;&#21019;&#24314;&#20840;&#23616;&#27169;&#22411;&#30340;&#23454;&#20307;&#30340;&#21487;&#20449;&#24230;&#12290;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#24212;&#36816;&#32780;&#29983;&#65292;&#36890;&#36807;&#25512;&#24191;&#21435;&#20013;&#24515;&#21270;&#27169;&#22411;&#32858;&#21512;&#24182;&#26368;&#23567;&#21270;&#23545;&#20013;&#24515;&#21270;&#26550;&#26500;&#30340;&#20381;&#36182;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;&#22312;DFL&#26041;&#38754;&#26377;&#25152;&#21162;&#21147;&#65292;&#25991;&#29486;&#36824;&#27809;&#26377;&#30740;&#31350;(i)DFL&#21644;CFL&#20043;&#38388;&#30340;&#20027;&#35201;&#24046;&#24322;;(ii)&#20998;&#26512;DFL&#26694;&#26550;&#20197;&#21019;&#24314;&#21644;&#35780;&#20272;&#26032;&#35299;&#20915;&#26041;&#26696;;(iii)&#22238;&#39038;&#20351;&#29992;DFL&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#22312;&#32852;&#37030;&#26550;&#26500;&#12289;&#23433;&#20840;&#24615;&#12289;&#36890;&#20449;&#31561;&#26041;&#38754;&#35782;&#21035;&#24182;&#20998;&#26512;&#20102;DFL&#30340;&#20027;&#35201;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, Federated Learning (FL) has gained relevance in training collaborative models without sharing sensitive data. Since its birth, Centralized FL (CFL) has been the most common approach in the literature, where a central entity creates a global model. However, a centralized approach leads to increased latency due to bottlenecks, heightened vulnerability to system failures, and trustworthiness concerns affecting the entity responsible for the global model creation. Decentralized Federated Learning (DFL) emerged to address these concerns by promoting decentralized model aggregation and minimizing reliance on centralized architectures. However, despite the work done in DFL, the literature has not (i) studied the main aspects differentiating DFL and CFL; (ii) analyzed DFL frameworks to create and evaluate new solutions; and (iii) reviewed application scenarios using DFL. Thus, this article identifies and analyzes the main fundamentals of DFL in terms of federation architect
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#30340;&#20840;&#38754;&#21644;&#32467;&#26500;&#21270;&#30340;&#20449;&#24687;&#65292;&#24110;&#21161;&#23454;&#36341;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25214;&#21040;&#26368;&#36866;&#21512;&#20182;&#20204;&#38656;&#27714;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.06959</link><description>&lt;p&gt;
&#12298;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#35843;&#26597;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Survey on Explainable Anomaly Detection. (arXiv:2210.06959v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06959
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#30340;&#20840;&#38754;&#21644;&#32467;&#26500;&#21270;&#30340;&#20449;&#24687;&#65292;&#24110;&#21161;&#23454;&#36341;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25214;&#21040;&#26368;&#36866;&#21512;&#20182;&#20204;&#38656;&#27714;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#22823;&#37096;&#20998;&#20851;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#25552;&#39640;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#19978;&#65292;&#32780;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#30456;&#24212;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#27492;&#23558;&#32467;&#26524;&#30340;&#35299;&#37322;&#30041;&#32473;&#20102;&#23454;&#36341;&#32773;&#12290;&#30001;&#20110;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20570;&#20986;&#30340;&#39640;&#39118;&#38505;&#20915;&#31574;&#38656;&#35201;&#25552;&#20379;&#35299;&#37322;&#24050;&#25104;&#20026;&#19968;&#39033;&#36947;&#24503;&#21644;&#30417;&#31649;&#35201;&#27714;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#23545;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#21644;&#32467;&#26500;&#21270;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27599;&#31181;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#30340;&#20027;&#35201;&#29305;&#24449;&#30340;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#23454;&#36341;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25214;&#21040;&#26368;&#36866;&#21512;&#20182;&#20204;&#38656;&#27714;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past two decades, most research on anomaly detection has focused on improving the accuracy of the detection, while largely ignoring the explainability of the corresponding methods and thus leaving the explanation of outcomes to practitioners. As anomaly detection algorithms are increasingly used in safety-critical domains, providing explanations for the high-stakes decisions made in those domains has become an ethical and regulatory requirement. Therefore, this work provides a comprehensive and structured survey on state-of-the-art explainable anomaly detection techniques. We propose a taxonomy based on the main aspects that characterize each explainable anomaly detection technique, aiming to help practitioners and researchers find the explainable anomaly detection method that best suits their needs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#26435;&#19981;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#21487;&#38752;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24773;&#22659;&#65292;&#21487;&#25193;&#23637;&#20026;&#21442;&#25968;&#21270;&#20989;&#25968;&#30340;PI&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2210.04318</link><description>&lt;p&gt;
&#20351;&#29992;&#21152;&#26435;&#19981;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Prediction intervals for neural network models using weighted asymmetric loss functions. (arXiv:2210.04318v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#26435;&#19981;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#21487;&#38752;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24773;&#22659;&#65292;&#21487;&#25193;&#23637;&#20026;&#21442;&#25968;&#21270;&#20989;&#25968;&#30340;PI&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#36817;&#20284;&#21644;&#39044;&#27979;&#36235;&#21183;&#30340;&#39044;&#27979;&#21306;&#38388;&#65288;PIs&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#21152;&#26435;&#19981;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#26469;&#20272;&#35745;PI&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#65292;&#26435;&#37325;&#30001;&#21306;&#38388;&#23485;&#24230;&#30830;&#23450;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#26041;&#27861;&#30340;&#31616;&#27905;&#25968;&#23398;&#35777;&#26126;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#25193;&#23637;&#21040;&#20026;&#21442;&#25968;&#21270;&#20989;&#25968;&#25512;&#23548;PI&#65292;&#24182;&#35770;&#35777;&#20102;&#35813;&#26041;&#27861;&#20026;&#39044;&#27979;&#30456;&#20851;&#21464;&#37327;&#30340;PI&#32780;&#26377;&#25928;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#30340;&#30495;&#23454;&#19990;&#30028;&#39044;&#27979;&#20219;&#21153;&#19978;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24773;&#22659;&#19979;&#21487;&#20197;&#20135;&#29983;&#21487;&#38752;&#30340;PI&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple and efficient approach to generate prediction intervals (PIs) for approximated and forecasted trends. Our method leverages a weighted asymmetric loss function to estimate the lower and upper bounds of the PIs, with the weights determined by the interval width. We provide a concise mathematical proof of the method, show how it can be extended to derive PIs for parametrised functions and argue why the method works for predicting PIs of dependent variables. The presented tests of the method on a real-world forecasting task using a neural network-based model show that it can produce reliable PIs in complex machine learning scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24212;&#29992;&#27169;&#22411;&#37327;&#21270;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#21464;&#20301;&#23485;&#20248;&#21270;&#26469;&#25552;&#39640;&#26080;&#32447;&#36890;&#20449;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#22312;&#26080;&#32447;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#22810;&#23610;&#24230;&#37327;&#21270;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#25913;&#21892;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#20855;&#22791;&#26356;&#39640;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23569;&#30340;&#35757;&#32451;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2209.10200</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#20013;&#22522;&#20110;&#21487;&#21464;&#20301;&#23485;&#30340;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Performance Optimization for Variable Bitwidth Federated Learning in Wireless Networks. (arXiv:2209.10200v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24212;&#29992;&#27169;&#22411;&#37327;&#21270;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#21464;&#20301;&#23485;&#20248;&#21270;&#26469;&#25552;&#39640;&#26080;&#32447;&#36890;&#20449;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#22312;&#26080;&#32447;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#22810;&#23610;&#24230;&#37327;&#21270;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#25913;&#21892;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#20855;&#22791;&#26356;&#39640;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23569;&#30340;&#35757;&#32451;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#27169;&#22411;&#37327;&#21270;&#26469;&#25913;&#21892;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#30340;&#26080;&#32447;&#36890;&#20449;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#20301;&#23485;FL&#26041;&#26696;&#20013;&#65292;&#36793;&#32536;&#35774;&#22791;&#35757;&#32451;&#21644;&#20256;&#36755;&#20854;&#26412;&#22320;FL&#27169;&#22411;&#21442;&#25968;&#30340;&#37327;&#21270;&#29256;&#26412;&#21040;&#19968;&#20010;&#21327;&#35843;&#26381;&#21153;&#22120;&#65292;&#23558;&#23427;&#20204;&#32858;&#21512;&#25104;&#19968;&#20010;&#37327;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#24182;&#21516;&#27493;&#35774;&#22791;&#12290;&#30446;&#26631;&#26159;&#20849;&#21516;&#30830;&#23450;&#29992;&#20110;&#26412;&#22320;FL&#27169;&#22411;&#37327;&#21270;&#30340;&#20301;&#23485;&#21644;&#27599;&#27425;&#36845;&#20195;&#21442;&#19982;FL&#35757;&#32451;&#30340;&#35774;&#22791;&#38598;&#21512;&#12290;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#22312;&#27599;&#27425;&#36845;&#20195;&#30340;&#35774;&#22791;&#25277;&#26679;&#39044;&#31639;&#21644;&#24310;&#36831;&#35201;&#27714;&#19979;&#26368;&#23567;&#21270;&#37327;&#21270;FL&#30340;&#35757;&#32451;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#25152;&#21046;&#23450;&#30340;&#38382;&#39064;&#38590;&#20197;&#35299;&#20915;&#65292;&#27809;&#26377;(i)&#23545;&#37327;&#21270;&#22914;&#20309;&#24433;&#21709;&#20840;&#23616;ML&#24615;&#33021;&#30340;&#20855;&#20307;&#29702;&#35299;&#20197;&#21450;(ii)&#26381;&#21153;&#22120;&#26500;&#24314;&#36825;&#20010;&#36807;&#31243;&#20272;&#35745;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20998;&#26512;&#22320;&#34920;&#24449;&#20102;&#26377;&#38480;&#30340;&#26080;&#32447;&#36164;&#28304;&#22914;&#20309;&#24433;&#21709;&#37327;&#21270;FL&#24615;&#33021;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#26368;&#20248;&#30340;&#20301;&#23485;&#20998;&#37197;&#31574;&#30053;&#12290;&#20026;&#20102;&#24212;&#23545;&#31532;&#20108;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#37327;&#21270;FL&#32858;&#21512;&#31639;&#27861;&#65292;&#20351;&#26381;&#21153;&#22120;&#33021;&#22815;&#36731;&#26494;&#22320;&#20174;&#20998;&#24067;&#24335;&#37327;&#21270;FL&#27169;&#22411;&#20013;&#37325;&#26500;&#20840;&#23616;&#37327;&#21270;FL&#27169;&#22411;&#12290;&#24191;&#27867;&#30340;&#20223;&#30495;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#22521;&#35757;&#25439;&#22833;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#27604;&#29616;&#26377;&#25216;&#26415;&#26041;&#26696;&#25552;&#39640;&#20102;&#39640;&#36798;35&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers improving wireless communication and computation efficiency in federated learning (FL) via model quantization. In the proposed bitwidth FL scheme, edge devices train and transmit quantized versions of their local FL model parameters to a coordinating server, which aggregates them into a quantized global model and synchronizes the devices. The goal is to jointly determine the bitwidths employed for local FL model quantization and the set of devices participating in FL training at each iteration. We pose this as an optimization problem that aims to minimize the training loss of quantized FL under a per-iteration device sampling budget and delay requirement. However, the formulated problem is difficult to solve without (i) a concrete understanding of how quantization impacts global ML performance and (ii) the ability of the server to construct estimates of this process efficiently. To address the first challenge, we analytically characterize how limited wireless resou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#39640;&#32500;&#22122;&#22768;&#19979;&#30340;&#27969;&#24418;&#23494;&#24230;&#21644;&#20960;&#20309;&#36827;&#34892;&#31283;&#20581;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#37325;&#38543;&#26426;&#32553;&#25918;&#39640;&#26031;&#26680;&#36827;&#34892;&#26631;&#20934;&#21270;&#65292;&#20197;&#35299;&#20915;&#39640;&#32500;&#22122;&#22768;&#23545;&#20256;&#32479;&#26631;&#20934;&#21270;&#26041;&#27861;&#30340;&#19981;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.08004</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#37325;&#38543;&#26426;&#32553;&#25918;&#26041;&#27861;&#23545;&#27969;&#24418;&#23494;&#24230;&#21644;&#20960;&#20309;&#30340;&#31283;&#20581;&#25512;&#26029; (arXiv:2209.08004v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Robust Inference of Manifold Density and Geometry by Doubly Stochastic Scaling. (arXiv:2209.08004v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#39640;&#32500;&#22122;&#22768;&#19979;&#30340;&#27969;&#24418;&#23494;&#24230;&#21644;&#20960;&#20309;&#36827;&#34892;&#31283;&#20581;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#37325;&#38543;&#26426;&#32553;&#25918;&#39640;&#26031;&#26680;&#36827;&#34892;&#26631;&#20934;&#21270;&#65292;&#20197;&#35299;&#20915;&#39640;&#32500;&#22122;&#22768;&#23545;&#20256;&#32479;&#26631;&#20934;&#21270;&#26041;&#27861;&#30340;&#19981;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#26680;&#21450;&#20854;&#20256;&#32479;&#30340;&#26631;&#20934;&#21270;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;&#34892;&#38543;&#26426;&#21270;&#65289;&#26159;&#35780;&#20272;&#25968;&#25454;&#28857;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#22122;&#22768;&#19979;&#65292;&#23427;&#20204;&#21487;&#33021;&#19981;&#20934;&#30830;&#65292;&#29305;&#21035;&#26159;&#24403;&#22122;&#22768;&#30340;&#24133;&#24230;&#22312;&#25968;&#25454;&#20013;&#21464;&#21270;&#36739;&#22823;&#26102;&#65292;&#20363;&#22914;&#22312;&#24322;&#26041;&#24046;&#24615;&#25110;&#24322;&#24120;&#20540;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26356;&#31283;&#20581;&#30340;&#26367;&#20195;&#26041;&#26696;--&#39640;&#26031;&#26680;&#30340;&#21452;&#37325;&#38543;&#26426;&#26631;&#20934;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#24773;&#20917;&#65292;&#21363;&#20174;&#39640;&#32500;&#31354;&#38388;&#20013;&#23884;&#20837;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#26410;&#30693;&#23494;&#24230;&#20013;&#37319;&#26679;&#30340;&#28857;&#65292;&#24182;&#19988;&#21487;&#33021;&#21463;&#21040;&#21487;&#33021;&#24378;&#28872;&#30340;&#12289;&#38750;&#21516;&#20998;&#24067;&#30340;&#12289;&#20122;&#39640;&#26031;&#22122;&#22768;&#30340;&#27745;&#26579;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21452;&#37325;&#38543;&#26426;&#20146;&#21644;&#30697;&#38453;&#21450;&#20854;&#32553;&#25918;&#22240;&#23376;&#22312;&#26576;&#20123;&#31181;&#32676;&#24418;&#24335;&#38468;&#36817;&#38598;&#20013;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#26377;&#38480;&#26679;&#26412;&#27010;&#29575;&#35823;&#24046;&#30028;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#24320;&#21457;&#20102;&#20960;&#31181;&#22312;&#19968;&#33324;&#39640;&#32500;&#22122;&#22768;&#19979;&#30340;&#31283;&#20581;&#25512;&#26029;&#24037;&#20855;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#31283;&#20581;&#23494;&#24230;...
&lt;/p&gt;
&lt;p&gt;
The Gaussian kernel and its traditional normalizations (e.g., row-stochastic) are popular approaches for assessing similarities between data points. Yet, they can be inaccurate under high-dimensional noise, especially if the noise magnitude varies considerably across the data, e.g., under heteroskedasticity or outliers. In this work, we investigate a more robust alternative -- the doubly stochastic normalization of the Gaussian kernel. We consider a setting where points are sampled from an unknown density on a low-dimensional manifold embedded in high-dimensional space and corrupted by possibly strong, non-identically distributed, sub-Gaussian noise. We establish that the doubly stochastic affinity matrix and its scaling factors concentrate around certain population forms, and provide corresponding finite-sample probabilistic error bounds. We then utilize these results to develop several tools for robust inference under general high-dimensional noise. First, we derive a robust density 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#29305;&#24449;&#23398;&#20064;&#30340;&#22522;&#20110;&#34892;&#20026;&#30340;&#26089;&#26399;&#33258;&#38381;&#30151;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#19988;&#23569;&#37327;&#30340;&#34987;&#35797;&#21160;&#20316;&#35270;&#39057;&#21098;&#36753;&#33258;&#21160;&#21270;&#35786;&#26029;&#33258;&#38381;&#30151;&#65292;&#20811;&#26381;&#20102;&#21487;&#29992;&#25968;&#25454;&#37327;&#23567;&#21644;&#26679;&#26412;&#21464;&#21270;&#22823;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2209.05379</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#29305;&#24449;&#23398;&#20064;&#30340;&#22522;&#20110;&#34892;&#20026;&#30340;&#26089;&#26399;&#33258;&#38381;&#30151;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Action-based Early Autism Diagnosis Using Contrastive Feature Learning. (arXiv:2209.05379v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#29305;&#24449;&#23398;&#20064;&#30340;&#22522;&#20110;&#34892;&#20026;&#30340;&#26089;&#26399;&#33258;&#38381;&#30151;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#19988;&#23569;&#37327;&#30340;&#34987;&#35797;&#21160;&#20316;&#35270;&#39057;&#21098;&#36753;&#33258;&#21160;&#21270;&#35786;&#26029;&#33258;&#38381;&#30151;&#65292;&#20811;&#26381;&#20102;&#21487;&#29992;&#25968;&#25454;&#37327;&#23567;&#21644;&#26679;&#26412;&#21464;&#21270;&#22823;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#65292;&#20063;&#34987;&#31216;&#20026;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#65292;&#26159;&#19968;&#31181;&#31070;&#32463;&#24615;&#30142;&#30149;&#12290;&#20854;&#20027;&#35201;&#30151;&#29366;&#21253;&#25324;(&#21475;&#35821;&#21644;/&#25110;&#38750;&#21475;&#35821;)&#27807;&#36890;&#22256;&#38590;&#21644;&#20725;&#21270;/&#37325;&#22797;&#34892;&#20026;&#12290;&#36825;&#20123;&#30151;&#29366;&#24120;&#24120;&#38590;&#20197;&#19982;&#27491;&#24120;&#65288;&#23545;&#29031;&#65289;&#20010;&#20307;&#21306;&#20998;&#65292;&#22240;&#27492;&#36825;&#31181;&#38556;&#30861;&#22312;&#20799;&#31461;&#26089;&#26399;&#20173;&#28982;&#26410;&#34987;&#35786;&#26029;&#20986;&#26469;&#65292;&#20174;&#32780;&#23548;&#33268;&#24310;&#36831;&#27835;&#30103;&#12290;&#30001;&#20110;&#22312;&#26089;&#26399;&#24180;&#40836;&#27573;&#23398;&#20064;&#26354;&#32447;&#38497;&#23789;&#65292;&#26089;&#26399;&#35786;&#26029;&#33258;&#38381;&#30151;&#21487;&#20197;&#22312;&#27491;&#30830;&#30340;&#26102;&#38388;&#37319;&#21462;&#36866;&#24403;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#21487;&#33021;&#23545;&#33258;&#38381;&#30151;&#23401;&#23376;&#30340;&#25104;&#38271;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#33258;&#38381;&#30151;&#35786;&#26029;&#26041;&#27861;&#38656;&#35201;&#22810;&#27425;&#23601;&#35786;&#20110;&#19987;&#31185;&#31934;&#31070;&#31185;&#21307;&#29983;&#65292;&#28982;&#32780;&#36825;&#19968;&#36807;&#31243;&#21487;&#33021;&#32791;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31616;&#21333;&#19988;&#23567;&#30340;&#34987;&#35797;&#21160;&#20316;&#35270;&#39057;&#21098;&#36753;&#26469;&#33258;&#21160;&#21270;&#33258;&#38381;&#30151;&#35786;&#26029;&#12290;&#36825;&#39033;&#20219;&#21153;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21487;&#29992;&#30340;&#27880;&#37322;&#25968;&#25454;&#37327;&#24456;&#23567;&#65292;&#24182;&#19988;&#26679;&#26412;&#20043;&#38388;&#23384;&#22312;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autism, also known as Autism Spectrum Disorder (or ASD), is a neurological disorder. Its main symptoms include difficulty in (verbal and/or non-verbal) communication, and rigid/repetitive behavior. These symptoms are often indistinguishable from a normal (control) individual, due to which this disorder remains undiagnosed in early childhood leading to delayed treatment. Since the learning curve is steep during the initial age, an early diagnosis of autism could allow to take adequate interventions at the right time, which might positively affect the growth of an autistic child. Further, the traditional methods of autism diagnosis require multiple visits to a specialized psychiatrist, however this process can be time-consuming. In this paper, we present a learning based approach to automate autism diagnosis using simple and small action video clips of subjects. This task is particularly challenging because the amount of annotated data available is small, and the variations among samples
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Treeformer&#27169;&#22359;&#65292;&#23427;&#20511;&#37492;&#20102;CKY&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32452;&#21512;&#36816;&#31639;&#31526;&#21644;&#27719;&#32858;&#20989;&#25968;&#26469;&#26500;&#24314;&#30701;&#35821;&#21644;&#21477;&#23376;&#30340;&#23618;&#27425;&#32534;&#30721;&#65292;&#20174;&#32780;&#23558;&#23618;&#27425;&#32467;&#26500;&#32435;&#20837;Transformer&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#27169;&#22359;&#22312;&#32452;&#21512;&#27867;&#21270;&#21644;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2207.06960</link><description>&lt;p&gt;
&#29992;Treeformers&#29983;&#25104;&#26641;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Forming Trees with Treeformers. (arXiv:2207.06960v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Treeformer&#27169;&#22359;&#65292;&#23427;&#20511;&#37492;&#20102;CKY&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32452;&#21512;&#36816;&#31639;&#31526;&#21644;&#27719;&#32858;&#20989;&#25968;&#26469;&#26500;&#24314;&#30701;&#35821;&#21644;&#21477;&#23376;&#30340;&#23618;&#27425;&#32534;&#30721;&#65292;&#20174;&#32780;&#23558;&#23618;&#27425;&#32467;&#26500;&#32435;&#20837;Transformer&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#27169;&#22359;&#22312;&#32452;&#21512;&#27867;&#21270;&#21644;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35821;&#35328;&#20855;&#26377;&#23884;&#22871;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#36739;&#23567;&#30340;&#29255;&#27573;&#20013;&#26500;&#24314;&#22797;&#26434;&#30340;&#21477;&#23376;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;&#22914;Transformers&#65289;&#22312;&#20854;&#26550;&#26500;&#20013;&#27809;&#26377;&#26126;&#30830;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#21363;&#23427;&#20204;&#23545;&#23618;&#27425;&#32467;&#26500;&#27809;&#26377;&#24402;&#32435;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#24050;&#30693;Transformers&#22312;&#38656;&#35201;&#36825;&#31181;&#32467;&#26500;&#30340;&#32452;&#21512;&#27867;&#21270;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Treeformer&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#21463;&#21040;CKY&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#20010;&#32452;&#21512;&#36816;&#31639;&#31526;&#21644;&#27719;&#32858;&#20989;&#25968;&#65292;&#29992;&#20110;&#26500;&#24314;&#30701;&#35821;&#21644;&#21477;&#23376;&#30340;&#23618;&#27425;&#32534;&#30721;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#23618;&#27425;&#32467;&#26500;&#32435;&#20837;Transformer&#27169;&#22411;&#20013;&#30340;&#22909;&#22788;&#65292;&#24182;&#19988;&#22312;&#32452;&#21512;&#27867;&#21270;&#20197;&#21450;&#26426;&#22120;&#32763;&#35793;&#12289;&#25277;&#35937;&#25688;&#35201;&#21644;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human language is known to exhibit a nested, hierarchical structure, allowing us to form complex sentences out of smaller pieces. However, many state-of-the-art neural networks models such as Transformers have no explicit hierarchical structure in its architecture -- that is, they don't have an inductive bias toward hierarchical structure. Additionally, Transformers are known to perform poorly on compositional generalization tasks which require such structures. In this paper, we introduce Treeformer, a general-purpose encoder module inspired by the CKY algorithm which learns a composition operator and pooling function to construct hierarchical encodings for phrases and sentences. Our extensive experiments demonstrate the benefits of incorporating hierarchical structure into the Transformer and show significant improvements in compositional generalization as well as in downstream tasks such as machine translation, abstractive summarization, and various natural language understanding tas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#23567;&#22411;&#37327;&#23376;&#35745;&#31639;&#26426;&#36827;&#34892;&#39640;&#32500;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#22320;&#22312;&#25968;&#23383;&#35782;&#21035;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#21033;&#29992;&#26356;&#23569;&#30340;&#30005;&#36335;&#35780;&#20272;&#26469;&#36817;&#20284;&#36739;&#22823;&#30005;&#36335;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2203.13739</link><description>&lt;p&gt;
&#20351;&#29992;&#23567;&#22411;&#37327;&#23376;&#35745;&#31639;&#26426;&#36827;&#34892;&#39640;&#32500;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
High Dimensional Quantum Machine Learning With Small Quantum Computers. (arXiv:2203.13739v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#23567;&#22411;&#37327;&#23376;&#35745;&#31639;&#26426;&#36827;&#34892;&#39640;&#32500;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#22320;&#22312;&#25968;&#23383;&#35782;&#21035;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#21033;&#29992;&#26356;&#23569;&#30340;&#30005;&#36335;&#35780;&#20272;&#26469;&#36817;&#20284;&#36739;&#22823;&#30005;&#36335;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#26426;&#23545;&#20110;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#38480;&#21046;&#20102;&#36825;&#19968;&#28508;&#21147;&#30340;&#23454;&#29616;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38480;&#21046;&#65292;&#21487;&#20197;&#37319;&#29992;&#19968;&#31181;&#25216;&#26415;&#65292;&#22312;&#27604;&#30005;&#36335;&#25152;&#38656;&#30340;&#27604;&#29305;&#23569;&#30340;&#26426;&#22120;&#19978;&#35780;&#20272;&#37327;&#23376;&#30005;&#36335;&#12290;&#36825;&#20123;&#25216;&#26415;&#36890;&#36807;&#22312;&#36739;&#23567;&#30340;&#26426;&#22120;&#19978;&#35780;&#20272;&#35768;&#22810;&#36739;&#23567;&#30340;&#30005;&#36335;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#19968;&#20010;&#22810;&#39033;&#24335;&#26469;&#22797;&#21046;&#36739;&#22823;&#26426;&#22120;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#26696;&#23545;&#20110;&#19968;&#33324;&#30005;&#36335;&#32780;&#35328;&#38656;&#35201;&#26356;&#22810;&#30340;&#30005;&#36335;&#35780;&#20272;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#21487;&#33021;&#24615;&#65292;&#21363;&#23545;&#20110;&#26576;&#20123;&#24212;&#29992;&#26469;&#35828;&#65292;&#35768;&#22810;&#36825;&#20123;&#23376;&#30005;&#36335;&#26159;&#22810;&#20313;&#30340;&#65292;&#24182;&#19988;&#19968;&#20010;&#26356;&#23567;&#30340;&#27714;&#21644;&#36275;&#20197;&#20272;&#35745;&#23436;&#25972;&#30340;&#30005;&#36335;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#21033;&#29992;&#26356;&#23569;&#30340;&#30005;&#36335;&#35780;&#20272;&#26469;&#36924;&#36817;&#36739;&#22823;&#30005;&#36335;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#25968;&#23383;&#35782;&#21035;&#20219;&#21153;&#65292;&#20351;&#29992;&#27169;&#25311;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computers hold great promise to enhance machine learning, but their current qubit counts restrict the realisation of this promise. In an attempt to placate this limitation techniques can be applied for evaluating a quantum circuit using a machine with fewer qubits than the circuit naively requires. These techniques work by evaluating many smaller circuits on the smaller machine, that are then combined in a polynomial to replicate the output of the larger machine. This scheme requires more circuit evaluations than are practical for general circuits. However, we investigate the possibility that for certain applications many of these subcircuits are superfluous, and that a much smaller sum is sufficient to estimate the full circuit. We construct a machine learning model that may be capable of approximating the outputs of the larger circuit with much fewer circuit evaluations. We successfully apply our model to the task of digit recognition, using simulated quantum computers much s
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#26631;&#24230;&#22312;&#32447;&#23398;&#20064;&#30340;&#31561;&#35843;&#33410;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20855;&#26377;&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#12289;&#38543;&#26102;&#38543;&#22320;&#21644;&#26080;&#26631;&#24230;&#30340;&#29305;&#28857;&#65292;&#24182;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;&#36951;&#25022;&#30340;&#36895;&#29575;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22312;&#32447;&#26657;&#27491;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.14586</link><description>&lt;p&gt;
&#20351;&#29992;&#20110;&#26080;&#26631;&#24230;&#22312;&#32447;&#23398;&#20064;&#30340;&#31561;&#35843;&#33410;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Isotuning With Applications To Scale-Free Online Learning. (arXiv:2112.14586v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14586
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#26631;&#24230;&#22312;&#32447;&#23398;&#20064;&#30340;&#31561;&#35843;&#33410;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20855;&#26377;&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#12289;&#38543;&#26102;&#38543;&#22320;&#21644;&#26080;&#26631;&#24230;&#30340;&#29305;&#28857;&#65292;&#24182;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;&#36951;&#25022;&#30340;&#36895;&#29575;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22312;&#32447;&#26657;&#27491;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25193;&#23637;&#21644;&#32467;&#21512;&#20102;&#25991;&#29486;&#20013;&#30340;&#20960;&#31181;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#12289;&#38543;&#26102;&#38543;&#22320;&#21644;&#26080;&#26631;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#26080;&#26631;&#24230;&#30340;&#36951;&#25022;&#30028;&#38480;&#24517;&#39035;&#19982;&#26368;&#22823;&#25439;&#22833;&#25104;&#32447;&#24615;&#20851;&#31995;&#65292;&#19981;&#35770;&#26159;&#23545;&#20110;&#22823;&#25439;&#22833;&#36824;&#26159;&#23545;&#20110;&#38750;&#24120;&#23567;&#30340;&#25439;&#22833;&#12290;&#33258;&#36866;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#34920;&#26126;&#31639;&#27861;&#21487;&#20197;&#21033;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#24182;&#21487;&#33021;&#20855;&#26377;&#24120;&#25968;&#36951;&#25022;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#23613;&#21487;&#33021;&#23569;&#20381;&#36182;&#21442;&#25968;&#30340;&#24555;&#36895;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#24212;&#35813;&#26159;&#38543;&#26102;&#21487;&#29992;&#30340;&#65292;&#22240;&#27492;&#19981;&#20381;&#36182;&#20110;&#26102;&#38388;&#33539;&#22260;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#21644;&#20027;&#35201;&#24037;&#20855;&#26159;&#31561;&#35843;&#33410;&#25216;&#26415;&#65292;&#23427;&#26159;&#24179;&#34913;&#36951;&#25022;&#26435;&#34913;&#30340;&#24605;&#24819;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#24037;&#20855;&#26469;&#36731;&#26494;&#35774;&#35745;&#21644;&#20998;&#26512;&#36825;&#26679;&#30340;&#23398;&#20064;&#36895;&#24230;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#36951;&#25022;&#30340;&#36895;&#29575;&#65288;&#26080;&#35770;&#26159;&#24120;&#25968;&#12289;$O(\log T)$&#12289;$O(\sqrt{T})$&#31561;&#65289;&#65292;&#24182;&#19988;&#22312;&#21516;&#26679;&#30340;&#35266;&#23519;&#37327;&#19978;&#27604;&#22312;&#20107;&#21518;&#36873;&#25321;&#30340;&#26368;&#20248;&#23398;&#20064;&#36895;&#24230;&#39640;&#20986;2&#20493;&#12290;&#31532;&#20108;&#20010;&#24037;&#20855;&#26159;&#22312;&#32447;&#26657;&#27491;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;...
&lt;/p&gt;
&lt;p&gt;
We extend and combine several tools of the literature to design fast, adaptive, anytime and scale-free online learning algorithms. Scale-free regret bounds must scale linearly with the maximum loss, both toward large losses and toward very small losses. Adaptive regret bounds demonstrate that an algorithm can take advantage of easy data and potentially have constant regret. We seek to develop fast algorithms that depend on as few parameters as possible, in particular they should be anytime and thus not depend on the time horizon. Our first and main tool, isotuning, is a generalization of the idea of balancing the trade-off of the regret. We develop a set of tools to design and analyze such learning rates easily and show that they adapts automatically to the rate of the regret (whether constant, $O(\log T)$, $O(\sqrt{T})$, etc.) within a factor 2 of the optimal learning rate in hindsight for the same observed quantities. The second tool is an online correction, which allows us to obtain
&lt;/p&gt;</description></item><item><title>RELDEC&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#31561;&#38271;&#24230;LDPC&#30721;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26234;&#33021;&#20307;&#39034;&#24207;&#35843;&#24230;CN&#38598;&#32676;&#65292;&#20197;&#20248;&#21270;&#35299;&#30721;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#25913;&#36827;MDP&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#36866;&#29992;&#20110;&#26356;&#22823;&#22359;&#38271;&#30340;LDPC&#30721;&#12290;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#20449;&#36947;&#26465;&#20214;&#19979;&#30340;&#35299;&#30721;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;AM-RELDEC&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.13934</link><description>&lt;p&gt;
RELDEC: &#24378;&#21270;&#23398;&#20064;&#22522;&#20110;&#30340;&#20013;&#31561;&#38271;&#24230;LDPC&#30721;&#30340;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RELDEC: Reinforcement Learning-Based Decoding of Moderate Length LDPC Codes. (arXiv:2112.13934v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.13934
&lt;/p&gt;
&lt;p&gt;
RELDEC&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#31561;&#38271;&#24230;LDPC&#30721;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26234;&#33021;&#20307;&#39034;&#24207;&#35843;&#24230;CN&#38598;&#32676;&#65292;&#20197;&#20248;&#21270;&#35299;&#30721;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#25913;&#36827;MDP&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#36866;&#29992;&#20110;&#26356;&#22823;&#22359;&#38271;&#30340;LDPC&#30721;&#12290;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#20449;&#36947;&#26465;&#20214;&#19979;&#30340;&#35299;&#30721;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;AM-RELDEC&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RELDEC&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20013;&#31561;&#38271;&#24230;&#20302;&#23494;&#24230;&#22855;&#20598;&#26657;&#39564;&#65288;LDPC&#65289;&#30721;&#30340;&#39034;&#24207;&#35299;&#30721;&#12290; RELDEC&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#20248;&#21270;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290;&#19982;&#25105;&#20204;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#26234;&#33021;&#20307;&#20165;&#23398;&#20064;&#19968;&#27425;&#36845;&#20195;&#20013;&#38598;&#32676;&#20869;&#30340;&#21333;&#20010;&#26816;&#26597;&#33410;&#28857;&#65288;CN&#65289;&#30340;&#35843;&#24230;&#65292;&#32780;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#26234;&#33021;&#20307;&#35843;&#24230;&#27599;&#20010;&#38598;&#32676;&#20013;&#30340;&#25152;&#26377;CN&#20197;&#21450;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;&#25152;&#26377;&#38598;&#32676;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;RELDEC&#30340;&#27599;&#20010;&#23398;&#20064;&#27493;&#39588;&#20013;&#65292;&#26234;&#33021;&#20307;&#26681;&#25454;&#19982;&#35843;&#24230;&#29305;&#23450;&#38598;&#32676;&#30456;&#20851;&#32852;&#30340;&#22870;&#21169;&#26469;&#39034;&#24207;&#23398;&#20064;&#35843;&#24230;CN&#38598;&#32676;&#12290;&#25105;&#20204;&#36824;&#20462;&#25913;&#20102;MDP&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#20351;RELDEC&#36866;&#29992;&#20110;&#27604;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#30340;&#36739;&#22823;&#22359;&#38271;&#30340;LDPC&#30721;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#22312;&#19981;&#21516;&#20449;&#36947;&#26465;&#20214;&#19979;&#30340;&#35299;&#30721;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28789;&#27963;&#30340;&#20803;-RELDEC&#65288;AM-RELDEC&#65289;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#30340;&#26159;&#25935;&#25463;&#30340;&#20803;-RELDEC&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we propose RELDEC, a novel approach for sequential decoding of moderate length low-density parity-check (LDPC) codes. The main idea behind RELDEC is that an optimized decoding policy is subsequently obtained via reinforcement learning based on a Markov decision process (MDP). In contrast to our previous work, where an agent learns to schedule only a single check node (CN) within a group (cluster) of CNs per iteration, in this work we train the agent to schedule all CNs in a cluster, and all clusters in every iteration. That is, in each learning step of RELDEC an agent learns to schedule CN clusters sequentially depending on a reward associated with the outcome of scheduling a particular cluster. We also modify the state space representation of the MDP, enabling RELDEC to be suitable for larger block length LDPC codes than those studied in our previous work. Furthermore, to address decoding under varying channel conditions, we propose agile meta-RELDEC (AM-RELDEC) that empl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#20132;&#20114;&#24335;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#20026;&#26679;&#26412;&#39640;&#25928;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2112.13487</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#20915;&#31574;&#21046;&#23450;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Statistical Complexity of Interactive Decision Making. (arXiv:2112.13487v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.13487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#20132;&#20114;&#24335;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#20026;&#26679;&#26412;&#39640;&#25928;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#23398;&#20064;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#25552;&#20379;&#26679;&#26412;&#39640;&#25928;&#12289;&#33258;&#36866;&#24212;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#30340;&#36951;&#25022;&#12290;&#36825;&#20010;&#38382;&#39064;&#31867;&#20284;&#20110;&#32463;&#20856;&#30340;&#26368;&#20248;&#65288;&#30417;&#30563;&#65289;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#37027;&#37324;&#26377;&#30528;&#34987;&#24191;&#27867;&#35748;&#21487;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65288;&#20363;&#22914;VC&#32500;&#21644;Rademacher&#22797;&#26434;&#24230;&#65289;&#26469;&#25511;&#21046;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38382;&#39064;&#30340;&#33258;&#36866;&#24212;&#24615;&#36136;&#65292;&#34920;&#24449;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#20250;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#32467;&#26524;&#25552;&#20379;&#20102;&#19968;&#31181;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#21363;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;&#65292;&#35813;&#24230;&#37327;&#34987;&#35777;&#26126;&#26159;&#26679;&#26412;&#39640;&#25928;&#30340;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#65306;1. &#20219;&#20309;&#20132;&#20114;&#24335;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#30340;&#26368;&#20248;&#36951;&#25022;&#30340;&#19979;&#30028;&#65292;&#30830;&#31435;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;&#20316;&#20026;&#19968;&#20010;&#22522;&#26412;&#38480;&#21046;&#65307;2. &#19968;&#31181;&#32479;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#33258;&#36866;&#24212;&#24615;&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental challenge in interactive learning and decision making, ranging from bandit problems to reinforcement learning, is to provide sample-efficient, adaptive learning algorithms that achieve near-optimal regret. This question is analogous to the classical problem of optimal (supervised) statistical learning, where there are well-known complexity measures (e.g., VC dimension and Rademacher complexity) that govern the statistical complexity of learning. However, characterizing the statistical complexity of interactive learning is substantially more challenging due to the adaptive nature of the problem. The main result of this work provides a complexity measure, the Decision-Estimation Coefficient, that is proven to be both necessary and sufficient for sample-efficient interactive learning. In particular, we provide:  1. a lower bound on the optimal regret for any interactive decision making problem, establishing the Decision-Estimation Coefficient as a fundamental limit.  2. a un
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#21644;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#21709;&#24212;&#30340;&#24182;&#34892;&#21270;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#23454;&#26102;&#29983;&#20135;&#29615;&#22659;&#20013;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#37319;&#29992;&#23618;&#27425;&#21270;&#32454;&#21270;&#30340;&#26631;&#31614;&#27880;&#24847;&#21147;&#32593;&#32476;&#39044;&#27979;CV&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24182;&#34892;&#39044;&#27979;&#12290;&#36890;&#36807;&#36873;&#25321;&#36731;&#37327;&#32423;&#24494;&#22411;Web&#26694;&#26550;&#21644;&#20351;&#29992;&#24494;&#26381;&#21153;&#26469;&#37096;&#32626;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31649;&#36947;&#65292;&#36798;&#21040;&#22312;&#23569;&#20110;700&#27627;&#31186;&#30340;&#26102;&#38388;&#20869;&#35299;&#26512;&#26222;&#36890;CV&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2112.08933</link><description>&lt;p&gt;
&#21487;&#21709;&#24212;&#30340;&#24182;&#34892;&#21270;&#26550;&#26500;&#29992;&#20110;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Responsive parallelized architecture for deploying deep learning models in production environments. (arXiv:2112.08933v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#21644;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#21709;&#24212;&#30340;&#24182;&#34892;&#21270;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#23454;&#26102;&#29983;&#20135;&#29615;&#22659;&#20013;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#37319;&#29992;&#23618;&#27425;&#21270;&#32454;&#21270;&#30340;&#26631;&#31614;&#27880;&#24847;&#21147;&#32593;&#32476;&#39044;&#27979;CV&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24182;&#34892;&#39044;&#27979;&#12290;&#36890;&#36807;&#36873;&#25321;&#36731;&#37327;&#32423;&#24494;&#22411;Web&#26694;&#26550;&#21644;&#20351;&#29992;&#24494;&#26381;&#21153;&#26469;&#37096;&#32626;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31649;&#36947;&#65292;&#36798;&#21040;&#22312;&#23569;&#20110;700&#27627;&#31186;&#30340;&#26102;&#38388;&#20869;&#35299;&#26512;&#26222;&#36890;CV&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25307;&#32856;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#26597;&#30475;&#27714;&#32844;&#32773;&#30340;&#31616;&#21382;(CV)&#25991;&#26723;&#36731;&#26494;&#31579;&#36873;&#20505;&#36873;&#20154;&#12290;&#26080;&#32467;&#26500;&#30340;&#25991;&#26723;CV&#21253;&#21547;&#20505;&#36873;&#20154;&#30340;&#20316;&#21697;&#38598;&#21644;&#21629;&#21517;&#23454;&#20307;&#21015;&#34920;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35774;&#35745;&#21644;&#25552;&#20986;&#19968;&#20010;&#38754;&#21521;Web&#30340;&#12289;&#39640;&#24230;&#21709;&#24212;&#30340;&#35745;&#31639;&#31649;&#36947;&#65292;&#20351;&#29992;&#23618;&#27425;&#21270;&#32454;&#21270;&#30340;&#26631;&#31614;&#27880;&#24847;&#21147;&#32593;&#32476;&#31995;&#32479;&#22320;&#39044;&#27979;CV&#23454;&#20307;&#12290;&#20351;&#29992;&#19987;&#38376;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#30456;&#20851;&#23383;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24182;&#34892;&#20013;&#20351;&#29992;&#19968;&#23450;&#25968;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24182;&#23454;&#26102;&#39044;&#27979;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#26512;&#23618;&#27425;&#22788;&#29702;&#31639;&#27861;&#36873;&#25321;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#24494;&#22411;Web&#26694;&#26550;&#65292;&#24182;&#19987;&#27880;&#20110;&#19968;&#31181;&#26377;&#21161;&#20110;&#22312;&#29983;&#20135;&#23601;&#32490;&#29615;&#22659;&#20013;&#20351;&#29992;&#24494;&#26381;&#21153;&#37096;&#32626;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31649;&#36947;&#30340;&#26041;&#27861;&#12290;&#37096;&#32626;&#30340;&#27169;&#22411;&#21644;&#25552;&#20986;&#30340;&#26550;&#26500;&#26377;&#21161;&#20110;&#22312;&#23569;&#20110;700&#27627;&#31186;&#30340;&#26102;&#38388;&#20869;&#35299;&#26512;&#26222;&#36890;CV&#12290;
&lt;/p&gt;
&lt;p&gt;
Recruiters can easily shortlist candidates for jobs via viewing their curriculum vitae (CV) document. Unstructured document CV beholds candidate's portfolio and named entities listing details. The main aim of this study is to design and propose a web oriented, highly responsive, computational pipeline that systematically predicts CV entities using hierarchically-refined label attention networks. Deep learning models specialized for named entity recognition were trained on large dataset to predict relevant fields. The article suggests an optimal strategy to use a number of deep learning models in parallel and predict in real time. We demonstrate selection of light weight micro web framework using Analytical Hierarchy Processing algorithm and focus on an approach useful to deploy large deep learning model-based pipelines in production ready environments using microservices. Deployed models and architecture proposed helped in parsing normal CV in less than 700 milliseconds for sequential 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#28151;&#21512;&#30340;&#37327;&#23376;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#21270;&#23398;&#27169;&#22411;&#21152;&#36895;&#33647;&#29289;&#21457;&#29616;&#12290;&#36890;&#36807;&#22312;D-Wave&#37327;&#23376;&#36864;&#28779;&#22120;&#19978;&#35757;&#32451;&#65292;&#25104;&#21151;&#29983;&#25104;&#20102;&#20855;&#26377;&#33647;&#29289;&#21270;&#23398;&#21644;&#21512;&#25104;&#21487;&#21450;&#24615;&#29305;&#24615;&#30340;2331&#20010;&#26032;&#21270;&#23398;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2108.11644</link><description>&lt;p&gt;
&#28151;&#21512;&#37327;&#23376;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#29983;&#25104;&#21270;&#23398;&#21644;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hybrid quantum-classical machine learning for generative chemistry and drug design. (arXiv:2108.11644v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#28151;&#21512;&#30340;&#37327;&#23376;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#21270;&#23398;&#27169;&#22411;&#21152;&#36895;&#33647;&#29289;&#21457;&#29616;&#12290;&#36890;&#36807;&#22312;D-Wave&#37327;&#23376;&#36864;&#28779;&#22120;&#19978;&#35757;&#32451;&#65292;&#25104;&#21151;&#29983;&#25104;&#20102;&#20855;&#26377;&#33647;&#29289;&#21270;&#23398;&#21644;&#21512;&#25104;&#21487;&#21450;&#24615;&#29305;&#24615;&#30340;2331&#20010;&#26032;&#21270;&#23398;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#21270;&#23398;&#27169;&#22411;&#25104;&#20026;&#21152;&#36895;&#33647;&#29289;&#21457;&#29616;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#21487;&#33021;&#30340;&#33647;&#29289;&#20998;&#23376;&#30340;&#32467;&#26500;&#31354;&#38388;&#30340;&#24040;&#22823;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#30340;&#38556;&#30861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#37327;&#23376;&#35745;&#31639;&#26426;&#19982;&#28145;&#24230;&#32463;&#20856;&#32593;&#32476;&#30456;&#32467;&#21512;&#26469;&#20811;&#26381;&#12290;&#20316;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#31163;&#25955;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;DVAE&#65289;&#65292;&#20854;&#20013;&#28508;&#22312;&#23618;&#20013;&#30340;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#65288;RBM&#65289;&#30340;&#22823;&#23567;&#34987;&#20943;&#23567;&#12290;&#25311;&#35758;&#27169;&#22411;&#30340;&#22823;&#23567;&#36275;&#22815;&#23567;&#65292;&#21487;&#20197;&#36866;&#24212;&#26368;&#20808;&#36827;&#30340;D-Wave&#37327;&#23376;&#36864;&#28779;&#22120;&#65292;&#24182;&#20801;&#35768;&#22312;ChEMBL&#29983;&#29289;&#27963;&#24615;&#21270;&#21512;&#29289;&#25968;&#25454;&#38598;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;2331&#20010;&#20855;&#26377;&#33647;&#29289;&#21270;&#23398;&#21644;&#21512;&#25104;&#21487;&#21450;&#24615;&#29305;&#24615;&#30340;&#26032;&#21270;&#23398;&#32467;&#26500;&#65292;&#20854;&#33539;&#22260;&#19982;&#26469;&#33258;ChEMBL&#30340;&#20998;&#23376;&#31867;&#20284;&#12290;&#25152;&#21576;&#29616;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#20351;&#29992;&#24050;&#32463;&#23384;&#22312;&#25110;&#21363;&#23558;&#25512;&#20986;&#30340;&#37327;&#23376;&#35745;&#31639;&#35774;&#22791;&#20316;&#20026;&#26410;&#26469;&#27979;&#35797;&#24179;&#21488;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative chemistry models emerge as powerful tools to expedite drug discovery. However, the immense size and complexity of the structural space of all possible drug-like molecules pose significant obstacles, which could be overcome with hybrid architectures combining quantum computers with deep classical networks. As the first step toward this goal, we built a compact discrete variational autoencoder (DVAE) with a Restricted Boltzmann Machine (RBM) of reduced size in its latent layer. The size of the proposed model was small enough to fit on a state-of-the-art D-Wave quantum annealer and allowed training on a subset of the ChEMBL dataset of biologically active compounds. Finally, we generated 2331 novel chemical structures with medicinal chemistry and synthetic accessibility properties in the ranges typical for molecules from ChEMBL. The presented results demonstrate the feasibility of using already existing or soon-to-be-available quantum computing devices as testbeds for futur
&lt;/p&gt;</description></item><item><title>&#33258;&#20027;&#23398;&#20064;&#29575;&#25511;&#21046;&#22120;&#26159;&#19968;&#20010;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#23398;&#20064;&#29575;&#35774;&#32622;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#36991;&#20813;&#32791;&#26102;&#19988;&#29712;&#30862;&#30340;&#36873;&#25321;&#21644;&#20462;&#25913;&#36807;&#31243;&#65292;&#20197;&#21450;&#23545;&#32593;&#32476;&#26550;&#26500;&#12289;&#20248;&#21270;&#22120;&#12289;&#25968;&#25454;&#38598;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#24494;&#23567;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.08767</link><description>&lt;p&gt;
&#25552;&#21319;&#36824;&#26159;&#19981;&#25552;&#21319;&#65306;&#33258;&#20027;&#23398;&#20064;&#29575;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
To Raise or Not To Raise: The Autonomous Learning Rate Question. (arXiv:2106.08767v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.08767
&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#23398;&#20064;&#29575;&#25511;&#21046;&#22120;&#26159;&#19968;&#20010;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#23398;&#20064;&#29575;&#35774;&#32622;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#36991;&#20813;&#32791;&#26102;&#19988;&#29712;&#30862;&#30340;&#36873;&#25321;&#21644;&#20462;&#25913;&#36807;&#31243;&#65292;&#20197;&#21450;&#23545;&#32593;&#32476;&#26550;&#26500;&#12289;&#20248;&#21270;&#22120;&#12289;&#25968;&#25454;&#38598;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#24494;&#23567;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29575;&#26159;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#26080;&#22788;&#19981;&#22312;&#30340;&#21442;&#25968;&#12290;&#21516;&#26102;&#20063;&#23384;&#22312;&#19968;&#20010;&#26222;&#36941;&#30340;&#38382;&#39064;&#65306;&#23398;&#20064;&#29575;&#24212;&#35813;&#35774;&#32622;&#20026;&#22810;&#23569;&#65311;&#28982;&#32780;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;&#30495;&#27491;&#31572;&#26696;&#36890;&#24120;&#26159;&#29712;&#30862;&#19988;&#32791;&#26102;&#30340;&#65292;&#24182;&#19988;&#36817;&#24180;&#26469;&#31215;&#32047;&#20102;&#22823;&#37327;&#20851;&#20110;&#22914;&#20309;&#36873;&#25321;&#21644;&#20462;&#25913;&#23398;&#20064;&#29575;&#20197;&#23454;&#29616;&#26368;&#20339;&#35757;&#32451;&#24615;&#33021;&#30340;&#28145;&#22885;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#33457;&#36153;&#25968;&#23567;&#26102;&#31934;&#24515;&#35843;&#25972;&#23398;&#20064;&#29575;&#30340;&#21162;&#21147;&#21487;&#33021;&#20250;&#22240;&#20026;&#32593;&#32476;&#26550;&#26500;&#12289;&#20248;&#21270;&#22120;&#12289;&#25968;&#25454;&#38598;&#25110;&#21021;&#22987;&#26465;&#20214;&#24494;&#23567;&#30340;&#25913;&#21464;&#32780;&#21270;&#20026;&#20044;&#26377;&#12290;&#20294;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#21487;&#20197;&#25913;&#21464;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31572;&#26696;&#26469;&#35299;&#20915;&#36825;&#20010;&#37325;&#35201;&#30340;&#23398;&#20064;&#29575;&#38382;&#39064;&#65306;&#33258;&#20027;&#23398;&#20064;&#29575;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a parameter ubiquitous throughout the deep learning world: learning rate. There is likewise a ubiquitous question: what should that learning rate be? The true answer to this question is often tedious and time consuming to obtain, and a great deal of arcane knowledge has accumulated in recent years over how to pick and modify learning rates to achieve optimal training performance. Moreover, the long hours spent carefully crafting the perfect learning rate can come to nothing the moment your network architecture, optimizer, dataset, or initial conditions change ever so slightly. But it need not be this way. We propose a new answer to the great learning rate question: the Autonomous Learning Rate Controller. Find it at https://github.com/fastestimator/ARC/tree/v2.0
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38543;&#26426;&#21333;&#32500;&#25628;&#32034;&#30340;&#25968;&#25454;&#22686;&#24378;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;6&#27425;&#35757;&#32451;&#21363;&#21487;&#33719;&#24471;&#19982;&#20351;&#29992;100&#27425;&#35757;&#32451;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.08756</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#21333;&#32500;&#25628;&#32034;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automating Augmentation Through Random Unidimensional Search. (arXiv:2106.08756v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.08756
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#21333;&#32500;&#25628;&#32034;&#30340;&#25968;&#25454;&#22686;&#24378;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;6&#27425;&#35757;&#32451;&#21363;&#21487;&#33719;&#24471;&#19982;&#20351;&#29992;100&#27425;&#35757;&#32451;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#32773;&#26469;&#35828;&#65292;&#25214;&#21040;&#35757;&#32451;&#26399;&#38388;&#26368;&#20339;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#23545;&#20110;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#32780;&#35328;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#31038;&#21306;&#24050;&#32463;&#35265;&#35777;&#20102;&#35768;&#22810;&#21162;&#21147;&#26469;&#33258;&#21160;&#21270;&#25214;&#21040;&#36866;&#21512;&#20219;&#20309;&#20219;&#21153;&#30340;&#23436;&#32654;&#22686;&#24378;&#31243;&#24207;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#26032;&#30340;&#23574;&#31471;&#26041;&#27861;&#20063;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#21487;&#33021;&#38656;&#35201;&#22810;&#36798;100&#20010;&#23436;&#25972;&#27169;&#22411;&#30340;&#35757;&#32451;&#26469;&#30830;&#23450;&#29702;&#24819;&#30340;&#37197;&#32622;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#20165;6&#27425;&#35757;&#32451;&#23601;&#33021;&#36798;&#21040;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#38543;&#26426;&#21333;&#32500;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/fastestimator/RUA/tree/v1.0&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is no secret amongst deep learning researchers that finding the optimal data augmentation strategy during training can mean the difference between state-of-the-art performance and a run-of-the-mill result. To that end, the community has seen many efforts to automate the process of finding the perfect augmentation procedure for any task at hand. Unfortunately, even recent cutting-edge methods bring massive computational overhead, requiring as many as 100 full model trainings to settle on an ideal configuration. We show how to achieve equivalent performance using just 6 trainings with Random Unidimensional Augmentation. Source code is available at https://github.com/fastestimator/RUA/tree/v1.0
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21333;&#20010;&#21152;&#36895;&#24230;&#35745;&#23545;DMD&#24739;&#32773;&#21644;&#27491;&#24120;&#20799;&#31461;&#30340;&#27493;&#24577;&#29305;&#24449;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#37319;&#21462;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#20004;&#32452;&#20043;&#38388;&#30340;&#21306;&#20998;&#12290;</title><link>http://arxiv.org/abs/2105.06295</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#20256;&#24863;&#22120;&#21152;&#36895;&#24230;&#35745;&#23545;&#26460;&#20852;&#32908;&#32905;&#33829;&#20859;&#19981;&#33391;&#24739;&#32773;&#65288;DMD&#65289;&#30340;&#27493;&#24577;&#29305;&#24449;&#36827;&#34892;&#34920;&#24449;&#65306;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gait Characterization in Duchenne Muscular Dystrophy (DMD) Using a Single-Sensor Accelerometer: Classical Machine Learning and Deep Learning Approaches. (arXiv:2105.06295v3 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.06295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21333;&#20010;&#21152;&#36895;&#24230;&#35745;&#23545;DMD&#24739;&#32773;&#21644;&#27491;&#24120;&#20799;&#31461;&#30340;&#27493;&#24577;&#29305;&#24449;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#37319;&#21462;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#20004;&#32452;&#20043;&#38388;&#30340;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#26460;&#20852;&#32908;&#32905;&#33829;&#20859;&#19981;&#33391;&#24739;&#32773;&#65288;DMD&#65289;&#19982;&#27491;&#24120;&#21457;&#32946;&#30340;&#21516;&#40836;&#20154;&#30340;&#27493;&#24577;&#27169;&#24335;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#20294;&#22312;&#27493;&#24577;&#23454;&#39564;&#23460;&#20043;&#22806;&#23545;&#36825;&#20123;&#24046;&#24322;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#33136;&#37096;&#20329;&#25140;&#30340;iPhone&#21152;&#36895;&#24230;&#35745;&#27979;&#37327;&#20102;&#22402;&#30452;&#12289;&#24038;&#21491;&#21644;&#21069;&#21518;&#21152;&#36895;&#24230;&#65292;&#35760;&#24405;&#20102;DMD&#21644;&#27491;&#24120;&#20799;&#31461;&#22312;&#20856;&#22411;&#36895;&#24230;&#33539;&#22260;&#20869;&#30340;&#34892;&#36208;&#24773;&#20917;&#12290;&#21442;&#19982;&#30740;&#31350;&#30340;&#19977;&#21040;&#21313;&#20845;&#23681;&#30340;15&#21517;&#27491;&#24120;&#20799;&#31461;&#21644;15&#21517;DMD&#24739;&#20799;&#36827;&#34892;&#20102;&#20843;&#39033;&#27493;&#34892;/&#22868;&#36305;&#27963;&#21160;&#65292;&#21253;&#25324;&#20116;&#20010;25&#31859;&#36208;/&#36305;&#36895;&#24230;&#26657;&#20934;&#27979;&#35797;&#65288;SC-L1&#21040;SC-L5&#65289;&#12289;6&#20998;&#38047;&#27493;&#34892;&#35797;&#39564;&#65288;6MWT&#65289;&#12289;&#19968;&#30334;&#31859;&#24555;&#36208;/&#24930;&#36305;/&#36305;&#27493;&#65288;100MRW&#65289;&#21644;&#33258;&#30001;&#34892;&#36208;&#65288;FW&#65289;&#12290;&#20026;&#20102;&#36827;&#34892;&#20020;&#24202;&#35780;&#20272;&#65292;&#21442;&#19982;&#32773;&#36824;&#23436;&#25104;&#20102;&#21271;&#26041;&#35780;&#20272;&#65288;NSAA&#65289;&#12290;&#25105;&#20204;&#25552;&#21462;&#20102;&#26102;&#31354;&#27493;&#24577;&#20020;&#24202;&#29305;&#24449;&#65288;CFs&#65289;&#65292;&#24182;&#24212;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#21306;&#20998;DMD&#21644;&#27491;&#24120;&#20799;&#31461;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differences in gait patterns of children with Duchenne muscular dystrophy (DMD) and typically-developing (TD) peers are visible to the eye, but quantifications of those differences outside of the gait laboratory have been elusive. In this work, we measured vertical, mediolateral, and anteroposterior acceleration using a waist-worn iPhone accelerometer during ambulation across a typical range of velocities. Fifteen TD and fifteen DMD children from 3-16 years of age underwent eight walking/running activities, including five 25 meters walk/run speed-calibration tests at a slow walk to running speeds (SC-L1 to SC-L5), a 6-minute walk test (6MWT), a 100 meters fast-walk/jog/run (100MRW), and a free walk (FW). For clinical anchoring purposes, participants completed a Northstar Ambulatory Assessment (NSAA). We extracted temporospatial gait clinical features (CFs) and applied multiple machine learning (ML) approaches to differentiate between DMD and TD children using extracted temporospatial g
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOTHIC&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21452;&#26680;&#26143;&#31995;&#65292;&#21457;&#29616;&#20102;&#35768;&#22810;&#21452;&#37325;&#27963;&#21160;&#26143;&#31995;&#26680;&#12290;&#25105;&#20204;&#22312;&#22823;&#26679;&#26412;&#20013;&#26816;&#27979;&#21040;&#20102;159&#20010;&#21452;&#37325;AGN&#65292;&#20854;&#20013;2&#20010;&#26159;&#19977;&#37325;AGN&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2011.12177</link><description>&lt;p&gt;
&#20351;&#29992;GOTHIC&#33258;&#21160;&#26816;&#27979;&#21452;&#26680;&#26143;&#31995;&#24182;&#21457;&#29616;&#19968;&#22823;&#25209;&#21452;&#37325;AGN
&lt;/p&gt;
&lt;p&gt;
Automated Detection of Double Nuclei Galaxies using GOTHIC and the Discovery of a Large Sample of Dual AGN. (arXiv:2011.12177v3 [astro-ph.GA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.12177
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOTHIC&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21452;&#26680;&#26143;&#31995;&#65292;&#21457;&#29616;&#20102;&#35768;&#22810;&#21452;&#37325;&#27963;&#21160;&#26143;&#31995;&#26680;&#12290;&#25105;&#20204;&#22312;&#22823;&#26679;&#26412;&#20013;&#26816;&#27979;&#21040;&#20102;159&#20010;&#21452;&#37325;AGN&#65292;&#20854;&#20013;2&#20010;&#26159;&#19977;&#37325;AGN&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;GOTHIC&#65288;&#22270;&#24418;&#22686;&#24378;&#36845;&#20195;&#29228;&#23665;&#31639;&#27861;&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#20855;&#26377;&#20004;&#20010;&#25110;&#22810;&#20010;&#23494;&#38598;&#20998;&#31163;&#26680;&#30340;&#26143;&#31995;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#26143;&#31995;&#20013;&#26816;&#27979;&#21452;&#37325;&#25110;&#22810;&#37325;&#27963;&#21160;&#26143;&#31995;&#26680;&#65288;AGN&#65289;&#30340;&#26679;&#26412;&#12290;&#23613;&#31649;&#26143;&#31995;&#21512;&#24182;&#24456;&#24120;&#35265;&#65292;&#20294;&#21452;&#37325;AGN&#30340;&#26816;&#27979;&#24456;&#32597;&#35265;&#12290;&#23427;&#20204;&#30340;&#26816;&#27979;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#36229;&#22823;&#36136;&#37327;&#40657;&#27934;&#65288;SMBH&#65289;&#21452;&#26143;&#30340;&#24418;&#25104;&#12289;SMBH&#30340;&#22686;&#38271;&#21644;&#22810;&#26680;&#31995;&#32479;&#20013;&#30340;AGN&#21453;&#39304;&#25928;&#24212;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#31639;&#27861;&#23545;&#29616;&#26377;&#30340;&#25104;&#20687;&#25968;&#25454;&#36827;&#34892;&#31995;&#32479;&#35843;&#26597;&#65292;&#20197;&#21457;&#29616;&#21452;&#26680;&#26143;&#31995;&#21644;&#21452;&#37325;AGN&#12290;&#25105;&#20204;&#22312;&#24050;&#30693;&#30340;DNG&#26679;&#26412;&#19978;&#27979;&#35797;&#20102;GOTHIC&#65292;&#24182;&#38543;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;SDSS DR16&#20013;&#22823;&#32422;100&#19975;&#20010;&#22788;&#20110;&#32418;&#31227;&#33539;&#22260;0&#33267;0.75&#30340;&#26143;&#31995;&#26679;&#26412;&#65292;&#24182;&#20855;&#26377;&#21487;&#29992;&#30340;&#20809;&#35889;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#26679;&#26412;&#20013;&#26816;&#27979;&#21040;159&#20010;&#21452;&#37325;AGN&#65292;&#20854;&#20013;2&#20010;&#26159;&#19977;&#37325;AGN&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel algorithm to detect double nuclei galaxies (DNG) called GOTHIC (Graph BOosted iterated HIll Climbing) - that detects whether a given image of a galaxy has two or more closely separated nuclei. Our aim is to detect samples of dual or multiple active galactic nuclei (AGN) in galaxies. Although galaxy mergers are common, the detection of dual AGN is rare. Their detection is very important as they help us understand the formation of supermassive black hole (SMBH) binaries, SMBH growth and AGN feedback effects in multiple nuclei systems. There is thus a need for an algorithm to do a systematic survey of existing imaging data for the discovery of DNGs and dual AGN. We have tested GOTHIC on a known sample of DNGs and subsequently applied it to a sample of a million SDSS DR16 galaxies lying in the redshift range of 0 to 0.75 approximately, and have available spectroscopic data. We have detected 159 dual AGN in this sample, of which 2 are triple AGN systems. Our results show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#20013;&#30340;&#20004;&#31181;&#38543;&#26426;&#31639;&#27861;&#65292;GLM-TSL&#21644;GLM-FPL&#12290;GLM-TSL&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65292;GLM-FPL&#21017;&#23558;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#25311;&#21512;&#21040;&#36807;&#21435;&#22870;&#21169;&#30340;&#38543;&#26426;&#25200;&#21160;&#21382;&#21490;&#20013;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20004;&#31181;&#31639;&#27861;&#24182;&#24471;&#20986;&#20102;&#23427;&#20204;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#27492;&#21069;&#30340;&#24037;&#20316;&#20013;&#30340;&#36951;&#25022;&#19978;&#30028;&#24471;&#21040;&#20102;&#25913;&#36827;&#65292;&#24182;&#19988;&#23545;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#38382;&#39064;&#65292;GLM-FPL&#26159;&#39318;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#22312;&#36923;&#36753;&#36172;&#33218;&#38382;&#39064;&#21644;&#31070;&#32463;&#32593;&#32476;&#36172;&#33218;&#38382;&#39064;&#19978;&#23545;&#36825;&#20004;&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#38543;&#26426;&#21270;&#22312;&#25506;&#32034;&#20013;&#30340;&#20316;&#29992;&#65292;&#36229;&#36234;&#20102;&#20165;&#20165;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/1906.08947</link><description>&lt;p&gt;
&#22312;&#24191;&#20041;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#20013;&#30340;&#38543;&#26426;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Randomized Exploration in Generalized Linear Bandits. (arXiv:1906.08947v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1906.08947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#20013;&#30340;&#20004;&#31181;&#38543;&#26426;&#31639;&#27861;&#65292;GLM-TSL&#21644;GLM-FPL&#12290;GLM-TSL&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65292;GLM-FPL&#21017;&#23558;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#25311;&#21512;&#21040;&#36807;&#21435;&#22870;&#21169;&#30340;&#38543;&#26426;&#25200;&#21160;&#21382;&#21490;&#20013;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20004;&#31181;&#31639;&#27861;&#24182;&#24471;&#20986;&#20102;&#23427;&#20204;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#27492;&#21069;&#30340;&#24037;&#20316;&#20013;&#30340;&#36951;&#25022;&#19978;&#30028;&#24471;&#21040;&#20102;&#25913;&#36827;&#65292;&#24182;&#19988;&#23545;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#38382;&#39064;&#65292;GLM-FPL&#26159;&#39318;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#22312;&#36923;&#36753;&#36172;&#33218;&#38382;&#39064;&#21644;&#31070;&#32463;&#32593;&#32476;&#36172;&#33218;&#38382;&#39064;&#19978;&#23545;&#36825;&#20004;&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#38543;&#26426;&#21270;&#22312;&#25506;&#32034;&#20013;&#30340;&#20316;&#29992;&#65292;&#36229;&#36234;&#20102;&#20165;&#20165;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#24191;&#20041;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#30340;&#38543;&#26426;&#31639;&#27861;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;GLM-TSL&#20174;&#21518;&#39564;&#20998;&#24067;&#30340;&#25289;&#26222;&#25289;&#26031;&#25311;&#21512;&#20013;&#37319;&#26679;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;(GLM)&#12290;&#31532;&#20108;&#31181;&#31639;&#27861;GLM-FPL&#23558;&#19968;&#20010;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#25311;&#21512;&#21040;&#36807;&#21435;&#22870;&#21169;&#30340;&#38543;&#26426;&#25200;&#21160;&#21382;&#21490;&#20013;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20004;&#31181;&#31639;&#27861;&#65292;&#24182;&#24471;&#20986;&#20102;&#23427;&#20204;&#22312;n&#36718;&#20013;&#36951;&#25022;&#19978;&#30028;$\tilde{O}(d \sqrt{n \log K})$&#65292;&#20854;&#20013;$d$&#26159;&#29305;&#24449;&#30340;&#25968;&#37327;&#65292;$K$&#26159;&#33218;&#30340;&#25968;&#37327;&#12290;&#21069;&#32773;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#32780;&#21518;&#32773;&#26159;&#38750;&#32447;&#24615;&#27169;&#22411;&#20013;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#22312;&#36923;&#36753;&#36172;&#33218;&#38382;&#39064;&#20013;&#23545;GLM-TSL&#21644;GLM-FPL&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#23558;GLM-FPL&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#36172;&#33218;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#25506;&#32034;&#20013;&#38543;&#26426;&#21270;&#30340;&#20316;&#29992;&#65292;&#19981;&#20165;&#20165;&#26159;&#21518;&#39564;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study two randomized algorithms for generalized linear bandits. The first, GLM-TSL, samples a generalized linear model (GLM) from the Laplace approximation to the posterior distribution. The second, GLM-FPL, fits a GLM to a randomly perturbed history of past rewards. We analyze both algorithms and derive $\tilde{O}(d \sqrt{n \log K})$ upper bounds on their $n$-round regret, where $d$ is the number of features and $K$ is the number of arms. The former improves on prior work while the latter is the first for Gaussian noise perturbations in non-linear models. We empirically evaluate both GLM-TSL and GLM-FPL in logistic bandits, and apply GLM-FPL to neural network bandits. Our work showcases the role of randomization, beyond posterior sampling, in exploration.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20110;&#20854;&#24178;&#25200;&#21382;&#21490;&#30340;&#32447;&#24615;&#27169;&#22411;&#19978;&#36873;&#25321;&#20272;&#35745;&#22870;&#21169;&#26368;&#39640;&#30340;&#33218;&#65292;&#29992;&#20110;&#22312;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#26368;&#23567;&#21270;&#32047;&#31215;&#36951;&#25022;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#31639;&#27861;&#36951;&#25022;&#30340;&#36739;&#22909;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/1903.09132</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#30340;&#24178;&#25200;&#21382;&#21490;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Perturbed-History Exploration in Stochastic Linear Bandits. (arXiv:1903.09132v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1903.09132
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20110;&#20854;&#24178;&#25200;&#21382;&#21490;&#30340;&#32447;&#24615;&#27169;&#22411;&#19978;&#36873;&#25321;&#20272;&#35745;&#22870;&#21169;&#26368;&#39640;&#30340;&#33218;&#65292;&#29992;&#20110;&#22312;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#26368;&#23567;&#21270;&#32047;&#31215;&#36951;&#25022;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#31639;&#27861;&#36951;&#25022;&#30340;&#36739;&#22909;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#26368;&#23567;&#21270;&#32047;&#31215;&#36951;&#25022;&#12290;&#35813;&#31639;&#27861;&#22312;&#35757;&#32451;&#20110;&#20854;&#24178;&#25200;&#21382;&#21490;&#30340;&#32447;&#24615;&#27169;&#22411;&#19978;&#36873;&#25321;&#20272;&#35745;&#22870;&#21169;&#26368;&#39640;&#30340;&#33218;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#30340;&#24178;&#25200;&#21382;&#21490;&#25506;&#32034;&#65288;LinPHE&#65289;&#12290;&#25152;&#35859;&#24178;&#25200;&#21382;&#21490;&#26159;&#25351;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#21644;&#38543;&#26426;&#29983;&#25104;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20266;&#22870;&#21169;&#30340;&#28151;&#21512;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#23545;&#20110;LinPHE&#30340;$n$&#36718;&#36951;&#25022;&#65292;&#20854;&#20013;$d$&#26159;&#29305;&#24449;&#25968;&#37327;&#65292;&#26377;&#19968;&#20010;$\tilde{O}(d \sqrt{n})$&#30340;&#38388;&#38553;&#33258;&#30001;&#30028;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#27493;&#39588;&#26159;&#20851;&#20110;&#20271;&#21162;&#21033;&#38543;&#26426;&#21464;&#37327;&#30340;&#21152;&#26435;&#21644;&#30340;&#26032;&#30340;&#38598;&#20013;&#21644;&#21453;&#38598;&#20013;&#36793;&#30028;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#35774;&#35745;&#30340;&#26222;&#36941;&#24615;&#65292;&#25105;&#20204;&#23558;LinPHE&#25512;&#24191;&#21040;&#19968;&#20010;&#36923;&#36753;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new online algorithm for cumulative regret minimization in a stochastic linear bandit. The algorithm pulls the arm with the highest estimated reward in a linear model trained on its perturbed history. Therefore, we call it perturbed-history exploration in a linear bandit (LinPHE). The perturbed history is a mixture of observed rewards and randomly generated i.i.d. pseudo-rewards. We derive a $\tilde{O}(d \sqrt{n})$ gap-free bound on the $n$-round regret of LinPHE, where $d$ is the number of features. The key steps in our analysis are new concentration and anti-concentration bounds on the weighted sum of Bernoulli random variables. To show the generality of our design, we generalize LinPHE to a logistic model. We evaluate our algorithms empirically and show that they are practical.
&lt;/p&gt;</description></item></channel></rss>