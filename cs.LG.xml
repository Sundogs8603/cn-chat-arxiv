<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>EAGLE&#26159;&#19968;&#20010;&#26080;&#25439;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#27425;&#39030;&#23618;&#29305;&#24449;&#23618;&#38754;&#19978;&#33258;&#22238;&#24402;&#25512;&#29702;&#65292;&#24182;&#35299;&#20915;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;3&#20493;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.15077</link><description>&lt;p&gt;
EAGLE: &#25512;&#27979;&#37319;&#26679;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#29305;&#24449;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. (arXiv:2401.15077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15077
&lt;/p&gt;
&lt;p&gt;
EAGLE&#26159;&#19968;&#20010;&#26080;&#25439;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#27425;&#39030;&#23618;&#29305;&#24449;&#23618;&#38754;&#19978;&#33258;&#22238;&#24402;&#25512;&#29702;&#65292;&#24182;&#35299;&#20915;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;3&#20493;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#35299;&#30721;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#21464;&#24471;&#32791;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;EAGLE&#65288;&#29992;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#25928;&#29575;&#30340;&#22806;&#25512;&#31639;&#27861;&#65289;&#65292;&#23454;&#29616;&#20102;&#26080;&#25439;&#21152;&#36895;&#12290;&#19982;&#20256;&#32479;&#30340;&#25512;&#27979;&#37319;&#26679;&#26041;&#27861;&#19981;&#21516;&#65292;EAGLE&#22312;&#26356;&#35268;&#24459;&#30340;&#65288;&#27425;&#39030;&#23618;&#65289;&#29305;&#24449;&#23618;&#38754;&#19978;&#33258;&#22238;&#24402;&#36827;&#34892;&#32534;&#20889;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#25552;&#21069;&#19968;&#20010;&#26102;&#38388;&#27493;&#30340;&#26631;&#35760;&#26469;&#35299;&#20915;&#19979;&#19968;&#20010;&#29305;&#24449;&#39044;&#27979;&#38382;&#39064;&#20013;&#30340;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#12290;EAGLE&#25152;&#25552;&#20379;&#30340;&#21152;&#36895;&#26159;&#26080;&#25439;&#30340;&#65306;&#23427;&#19981;&#38656;&#35201;&#24494;&#35843;&#30446;&#26631;LLM&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#21407;&#22987;&#30340;&#33258;&#22238;&#24402;&#35299;&#30721;&#30340;&#20998;&#24067;&#30456;&#21516;&#12290;&#25130;&#33267;&#26412;&#25991;&#25552;&#20132;&#26102;&#65292;EAGLE&#26159;&#24050;&#30693;&#25512;&#27979;&#37319;&#26679;&#23478;&#26063;&#20013;&#36895;&#24230;&#26368;&#24555;&#30340;&#26694;&#26550;&#12290;&#22312;MT-bench&#19978;&#65292;EAGLE&#27604;&#21407;&#22987;&#35299;&#30721;&#24555;3&#20493;&#65292;&#27604;Lookahead&#24555;2&#20493;&#65292;&#27604;Medusa&#24555;1.6&#20493;&#12290;&#20351;&#29992;gpt-fast&#65292;EAGLE&#24179;&#22343;&#27599;&#31186;&#36798;&#21040;160&#20010;&#26631;&#35760;&#19982;LLaMA2-Chat&#25645;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auto-regressive decoding makes the inference of Large Language Models (LLMs) time-consuming. We propose a simple framework, EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), for lossless acceleration. Unlike traditional speculative sampling methods, EAGLE operates the drafting process auto-regressively at the more regular (second-top-layer) feature level and addresses the sampling uncertainty issues in the next-feature prediction problems by integrating tokens from one time step ahead. The acceleration provided by EAGLE is lossless: it involves no fine-tuning of the target LLM, and the generated text maintains the same distribution as that of vanilla auto-regressive decoding. As of the submission of this paper, EAGLE is the fastest known framework within the speculative sampling family. On MT-bench, EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x faster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s with LLaMA2-Chat 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Expert with Clustering (EWC)&#30340;&#20998;&#23618;&#22312;&#32447;&#20559;&#22909;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21644;&#19987;&#23478;&#24314;&#35758;&#30340;&#39044;&#27979;&#26469;&#21152;&#36895;&#29992;&#25143;&#20559;&#22909;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#24341;&#23548;&#36317;&#31163;&#24230;&#37327;&#29983;&#25104;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#32858;&#31867;&#20013;&#24515;&#12290;</title><link>http://arxiv.org/abs/2401.15062</link><description>&lt;p&gt;
&#19987;&#23478;&#19982;&#32858;&#31867;&#65306;&#20998;&#23618;&#22312;&#32447;&#20559;&#22909;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Expert with Clustering: Hierarchical Online Preference Learning Framework. (arXiv:2401.15062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Expert with Clustering (EWC)&#30340;&#20998;&#23618;&#22312;&#32447;&#20559;&#22909;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21644;&#19987;&#23478;&#24314;&#35758;&#30340;&#39044;&#27979;&#26469;&#21152;&#36895;&#29992;&#25143;&#20559;&#22909;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#24341;&#23548;&#36317;&#31163;&#24230;&#37327;&#29983;&#25104;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#32858;&#31867;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#31227;&#21160;&#31995;&#32479;&#36234;&#26469;&#36234;&#33021;&#22815;&#21521;&#31227;&#21160;&#29992;&#25143;&#25512;&#33616;&#36873;&#39033;&#65292;&#20197;&#24341;&#23548;&#20182;&#20204;&#26397;&#21521;&#20010;&#24615;&#21270;&#20294;&#21487;&#25345;&#32493;&#30340;&#31995;&#32479;&#32467;&#26524;&#12290;&#19982;&#20856;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#30456;&#27604;&#65292;&#26368;&#23567;&#21270;&#21518;&#24724;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;1&#65289;&#31227;&#21160;&#36873;&#39033;&#30452;&#25509;&#24433;&#21709;&#29992;&#25143;&#30340;&#29983;&#27963;&#65292;2&#65289;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#24615;&#20381;&#36182;&#20110;&#36275;&#22815;&#30340;&#29992;&#25143;&#21442;&#19982;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#21033;&#29992;&#25429;&#25417;&#29992;&#25143;&#31227;&#21160;&#20559;&#22909;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#26469;&#21152;&#36895;&#29992;&#25143;&#20559;&#22909;&#23398;&#20064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Expert with Clustering (EWC)&#30340;&#20998;&#23618;&#19978;&#19979;&#25991;Bandit&#26694;&#26550;&#65292;&#23427;&#38598;&#25104;&#20102;&#32858;&#31867;&#25216;&#26415;&#21644;&#19987;&#23478;&#24314;&#35758;&#30340;&#39044;&#27979;&#12290;EWC&#26377;&#25928;&#22320;&#21033;&#29992;&#20998;&#23618;&#29992;&#25143;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#24341;&#23548;&#36317;&#31163;&#24230;&#37327;&#12290;&#35813;&#24230;&#37327;&#22312;&#29983;&#25104;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#32858;&#31867;&#20013;&#24515;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#27599;&#20010;&#29992;&#25143;$T$&#36718;&#65292;$N$&#29992;&#25143;&#21644;$K$&#36873;&#39033;&#30340;&#25512;&#33616;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26681;&#25454;&#29992;&#25143;&#30340;&#23454;&#26102;&#21453;&#39304;&#26469;&#22312;&#32447;&#23398;&#20064;&#21644;&#25913;&#36827;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging mobility systems are increasingly capable of recommending options to mobility users, to guide them towards personalized yet sustainable system outcomes. Even more so than the typical recommendation system, it is crucial to minimize regret, because 1) the mobility options directly affect the lives of the users, and 2) the system sustainability relies on sufficient user participation. In this study, we consider accelerating user preference learning by exploiting a low-dimensional latent space that captures the mobility preferences of users. We introduce a hierarchical contextual bandit framework named Expert with Clustering (EWC), which integrates clustering techniques and prediction with expert advice. EWC efficiently utilizes hierarchical user information and incorporates a novel Loss-guided Distance metric. This metric is instrumental in generating more representative cluster centroids. In a recommendation scenario with $N$ users, $T$ rounds per user, and $K$ options, our alg
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23436;&#20840;&#29420;&#31435;&#36890;&#20449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#35777;&#26126;&#29420;&#31435;&#26234;&#33021;&#20307;&#20173;&#21487;&#20197;&#23398;&#20064;&#36890;&#20449;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#36890;&#20449;&#22312;&#19981;&#21516;&#32593;&#32476;&#23481;&#37327;&#19979;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.15059</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23436;&#20840;&#29420;&#31435;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Fully Independent Communication in Multi-Agent Reinforcement Learning. (arXiv:2401.15059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15059
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23436;&#20840;&#29420;&#31435;&#36890;&#20449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#35777;&#26126;&#29420;&#31435;&#26234;&#33021;&#20307;&#20173;&#21487;&#20197;&#23398;&#20064;&#36890;&#20449;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#36890;&#20449;&#22312;&#19981;&#21516;&#32593;&#32476;&#23481;&#37327;&#19979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26159;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#39046;&#22495;&#30340;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#19987;&#27880;&#20110;&#30740;&#31350;MARL&#20013;&#30340;&#36890;&#20449;&#26041;&#27861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#36890;&#20449;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20173;&#28982;&#36807;&#20110;&#22797;&#26434;&#65292;&#19981;&#23481;&#26131;&#36801;&#31227;&#21040;&#26356;&#23454;&#38469;&#30340;&#24773;&#22659;&#20013;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#20351;&#29992;&#20102;&#33879;&#21517;&#30340;&#21442;&#25968;&#20849;&#20139;&#25216;&#24039;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;MARL&#20013;&#19981;&#20849;&#20139;&#21442;&#25968;&#30340;&#29420;&#31435;&#23398;&#20064;&#32773;&#22914;&#20309;&#36827;&#34892;&#36890;&#20449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#35774;&#32622;&#21487;&#33021;&#20250;&#24102;&#26469;&#19968;&#20123;&#38382;&#39064;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#26696;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#38754;&#20020;&#25361;&#25112;&#65292;&#29420;&#31435;&#30340;&#26234;&#33021;&#20307;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#36890;&#20449;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#26469;&#30740;&#31350;MARL&#20013;&#30340;&#36890;&#20449;&#22914;&#20309;&#21463;&#21040;&#19981;&#21516;&#32593;&#32476;&#23481;&#37327;&#30340;&#24433;&#21709;&#65292;&#26080;&#35770;&#26159;&#20849;&#20139;&#21442;&#25968;&#36824;&#26159;&#19981;&#20849;&#20139;&#21442;&#25968;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36890;&#20449;&#30340;&#33021;&#21147;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#37117;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research within the field of multi-agent systems. Several recent works have focused specifically on the study of communication approaches in MARL. While multiple communication methods have been proposed, these might still be too complex and not easily transferable to more practical contexts. One of the reasons for that is due to the use of the famous parameter sharing trick. In this paper, we investigate how independent learners in MARL that do not share parameters can communicate. We demonstrate that this setting might incur into some problems, to which we propose a new learning scheme as a solution. Our results show that, despite the challenges, independent agents can still learn communication strategies following our method. Additionally, we use this method to investigate how communication in MARL is affected by different network capacities, both for sharing and not sharing parameters. We observe that communication 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15043</link><description>&lt;p&gt;
&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#65306;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#21644;&#22686;&#24378;&#23398;&#20064;&#30340;&#26032;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20581;&#24247;&#25945;&#32946;&#26448;&#26009;&#30340;&#38405;&#35835;&#27700;&#24179;&#26174;&#33879;&#24433;&#21709;&#20449;&#24687;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#25509;&#35302;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#26063;&#35028;&#20154;&#32676;&#12290;&#35768;&#22810;&#24739;&#32773;&#25945;&#32946;&#36164;&#28304;&#36229;&#36807;&#20102;&#24191;&#27867;&#25509;&#21463;&#30340;&#26631;&#20934;&#30340;&#38405;&#35835;&#27700;&#24179;&#21644;&#22797;&#26434;&#24615;&#12290;&#22312;&#20581;&#24247;&#20449;&#24687;&#20013;&#65292;&#24613;&#38656;&#39640;&#24615;&#33021;&#30340;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#20197;&#22686;&#24378;&#20256;&#25773;&#21644;&#35782;&#23383;&#33021;&#21147;&#12290;&#36825;&#31181;&#38656;&#35201;&#22312;&#30284;&#30151;&#25945;&#32946;&#20013;&#23588;&#20026;&#36843;&#20999;&#65292;&#26377;&#25928;&#30340;&#39044;&#38450;&#21644;&#31579;&#26597;&#25945;&#32946;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24341;&#20837;&#20102;&#31616;&#21270;&#30340;&#28040;&#21270;&#30284;&#30151;&#65288;SimpleDC&#65289;&#24182;&#34892;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#12290;&#21033;&#29992;SimpleDC&#21644;&#29616;&#26377;&#30340;Med-EASi&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based app
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#22810;&#27169;&#24577;&#27867;&#21270;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20855;&#26377;&#22810;&#20010;&#27880;&#24847;&#21147;&#23618;&#25110;&#21033;&#29992;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.15030</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#22312;&#36890;&#29992;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the generalization capacity of neural networks during generic multimodal reasoning. (arXiv:2401.15030v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#22810;&#27169;&#24577;&#27867;&#21270;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20855;&#26377;&#22810;&#20010;&#27880;&#24847;&#21147;&#23618;&#25110;&#21033;&#29992;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;, &#36825;&#20123;&#27169;&#22411;&#20284;&#20046;&#23637;&#31034;&#20102;&#31867;&#20284;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31867;&#27169;&#22411;&#21644;&#20854;&#20182;&#22522;&#26412;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#30340;&#19968;&#33324;&#24615;&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#27867;&#21270;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#38382;&#31572;&#22522;&#20934;&#26469;&#35780;&#20272;&#19977;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#24615;&#33021;&#65306;&#20998;&#24515;&#27867;&#21270;&#65288;&#22312;&#20998;&#24515;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#27867;&#21270;&#65289;&#65292;&#31995;&#32479;&#30340;&#32452;&#21512;&#27867;&#21270;&#65288;&#23545;&#26032;&#30340;&#20219;&#21153;&#25490;&#21015;&#30340;&#27867;&#21270;&#65289;&#21644;&#26377;&#30410;&#30340;&#32452;&#21512;&#27867;&#21270;&#65288;&#23545;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#32467;&#26500;&#36827;&#34892;&#27867;&#21270;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#19978;&#65288;&#22914;RNN&#65292;Transformer&#65292;Perceivers&#31561;&#65289;&#65292;&#20855;&#26377;&#22810;&#20010;&#27880;&#24847;&#21147;&#23618;&#25110;&#32773;&#21033;&#29992;&#36755;&#20837;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#30340;&#27169;&#22411;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#31215;&#26497;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#22810;&#27169;&#24577;&#27867;&#21270;&#65292;&#27169;&#22411;&#26550;&#26500;&#26159;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks structures). We found that across model architectures (e.g., RNNs, Transformers, Perceivers, etc.), models with multiple attention layers, or models that leveraged cross-attention mechanisms between input domains, fared better. Our positive results demonstrate that for multi
&lt;/p&gt;</description></item><item><title>SliceGPT&#26159;&#19968;&#31181;&#26032;&#30340;&#20107;&#21518;&#35757;&#32451;&#31232;&#30095;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#26367;&#25442;&#20026;&#36739;&#23567;&#30340;&#30697;&#38453;&#20197;&#20943;&#23567;&#32593;&#32476;&#30340;&#32500;&#24230;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.15024</link><description>&lt;p&gt;
SliceGPT: &#36890;&#36807;&#21024;&#38500;&#34892;&#21644;&#21015;&#26469;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SliceGPT: Compress Large Language Models by Deleting Rows and Columns. (arXiv:2401.15024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15024
&lt;/p&gt;
&lt;p&gt;
SliceGPT&#26159;&#19968;&#31181;&#26032;&#30340;&#20107;&#21518;&#35757;&#32451;&#31232;&#30095;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#26367;&#25442;&#20026;&#36739;&#23567;&#30340;&#30697;&#38453;&#20197;&#20943;&#23567;&#32593;&#32476;&#30340;&#32500;&#24230;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30707;&#65292;&#20294;&#20351;&#29992;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#12290;&#31232;&#30095;&#21270;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#36164;&#28304;&#38480;&#21046;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#20107;&#21518;&#30340;&#31232;&#30095;&#21270;&#22788;&#29702;&#12290;&#29616;&#26377;&#30340;&#31232;&#30095;&#21270;&#25216;&#26415;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#24403;&#21069;&#30828;&#20214;&#19978;&#36895;&#24230;&#21463;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#21518;&#35757;&#32451;&#31232;&#30095;&#21270;&#26041;&#26696;SliceGPT&#65292;&#35813;&#26041;&#26696;&#29992;&#36739;&#23567;&#30340;&#65288;&#31264;&#23494;&#30340;&#65289;&#30697;&#38453;&#26367;&#25442;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#65292;&#20174;&#32780;&#20943;&#23567;&#32593;&#32476;&#30340;&#23884;&#20837;&#32500;&#24230;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SliceGPT&#22312;&#20445;&#25345;&#30456;&#24212;&#31264;&#23494;&#27169;&#22411;&#30340;99%&#12289;99%&#21644;90%&#30340;&#38646;-shot&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#31227;&#38500;LLAMA2-70B&#12289;OPT 66B&#21644;Phi-2&#27169;&#22411;&#20013;&#22810;&#36798;25%&#30340;&#27169;&#22411;&#21442;&#25968;&#65288;&#21253;&#25324;&#23884;&#20837;&#65289;&#12290;&#25105;&#20204;&#30340;&#20999;&#29255;&#27169;&#22411;&#22312;&#36739;&#23569;&#30340;GPU&#19978;&#36816;&#34892;&#24182;&#19988;&#26356;&#24555;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20195;&#30721;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimi
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#20013;&#20855;&#26377;&#35786;&#26029;&#21644;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#24403;&#21069;&#30740;&#31350;&#32858;&#28966;&#20110;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#65292;&#20197;&#21450;&#23545;&#35813;&#30142;&#30149;&#30340;&#20998;&#31867;&#12289;&#20998;&#32423;&#12289;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#21644;&#29983;&#23384;&#39044;&#27979;&#31561;&#20020;&#24202;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.15022</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based analysis of glioma tissue sections: a review. (arXiv:2401.15022v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15022
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#20013;&#20855;&#26377;&#35786;&#26029;&#21644;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#24403;&#21069;&#30740;&#31350;&#32858;&#28966;&#20110;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#65292;&#20197;&#21450;&#23545;&#35813;&#30142;&#30149;&#30340;&#20998;&#31867;&#12289;&#20998;&#32423;&#12289;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#21644;&#29983;&#23384;&#39044;&#27979;&#31561;&#20020;&#24202;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33014;&#36136;&#30244;&#30340;&#35786;&#26029;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#20351;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#33014;&#36136;&#30244;&#32452;&#32455;&#36827;&#34892;&#32452;&#32455;&#23398;&#35780;&#20272;&#65292;&#20026;&#35786;&#26029;&#21644;&#39044;&#27979;&#32467;&#26524;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#23545;&#24403;&#21069;&#30740;&#31350;&#30340;&#29616;&#29366;&#36827;&#34892;&#27010;&#36848;&#65292;&#26412;&#32508;&#36848;&#23545;70&#20010;&#20844;&#24320;&#21487;&#24471;&#30340;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#20123;&#35770;&#25991;&#20851;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#26579;&#33394;&#30340;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#65292;&#28085;&#30422;&#20102;&#20998;&#31867;&#65288;16/70&#65289;&#65292;&#20998;&#32423;&#65288;23/70&#65289;&#65292;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#65288;13/70&#65289;&#21644;&#29983;&#23384;&#39044;&#27979;&#65288;27/70&#65289;&#31561;&#35786;&#26029;&#20219;&#21153;&#12290;&#25152;&#26377;&#30340;&#30740;&#31350;&#37117;&#22312;&#26041;&#27861;&#23398;&#26041;&#38754;&#21450;&#20854;&#20020;&#24202;&#36866;&#29992;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#23545;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#36827;&#34892;&#35780;&#20272;&#12290;&#22810;&#25968;&#30740;&#31350;&#65288;49/70&#65289;&#22522;&#20110;&#20844;&#24320;&#30340;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#21644;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#25968;&#25454;&#38598;&#65292;&#20165;&#26377;&#23569;&#25968;&#30740;&#31350;&#20351;&#29992;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the diagnosis of gliomas has become increasingly complex. Histological assessment of glioma tissue using modern machine learning techniques offers new opportunities to support diagnosis and outcome prediction. To give an overview of the current state of research, this review examines 70 publicly available research studies on machine learning-based analysis of stained human glioma tissue sections, covering the diagnostic tasks of subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and survival prediction (27/70). All studies were reviewed with regard to methodological aspects as well as clinical applicability. It was found that the focus of current research is the assessment of hematoxylin and eosin-stained tissue sections of adult-type diffuse gliomas. The majority of studies (49/70) are based on the publicly available glioblastoma and low-grade glioma datasets from The Cancer Genome Atlas (TCGA) and only a few studies employed other datasets in is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#32452;&#21512;&#21644;&#24182;&#34892;&#32467;&#26500;&#20998;&#31867;&#22120;&#26469;&#22686;&#24378;&#26080;&#25991;&#26412;&#32422;&#26463;&#30340;&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#38454;&#27573;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#22768;&#23398;&#29305;&#24449;&#30340;&#32452;&#21512;&#27604;&#36739;&#65292;&#24182;&#38024;&#23545;&#20256;&#32479;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#30340;&#24369;&#28857;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.15018</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#32452;&#21512;&#21644;&#24182;&#34892;&#32467;&#26500;&#20998;&#31867;&#22120;&#22686;&#24378;&#26080;&#25991;&#26412;&#32422;&#26463;&#30340;&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Enhancement of a Text-Independent Speaker Verification System by using Feature Combination and Parallel-Structure Classifiers. (arXiv:2401.15018v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#32452;&#21512;&#21644;&#24182;&#34892;&#32467;&#26500;&#20998;&#31867;&#22120;&#26469;&#22686;&#24378;&#26080;&#25991;&#26412;&#32422;&#26463;&#30340;&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#38454;&#27573;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#22768;&#23398;&#29305;&#24449;&#30340;&#32452;&#21512;&#27604;&#36739;&#65292;&#24182;&#38024;&#23545;&#20256;&#32479;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#30340;&#24369;&#28857;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;&#20027;&#35201;&#21253;&#21547;&#20004;&#20010;&#29420;&#31435;&#30340;&#38454;&#27573;&#65306;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#36825;&#20004;&#20010;&#27169;&#22359;&#65292;&#25552;&#39640;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#22768;&#23398;&#29305;&#24449;&#26159;&#36827;&#34892;&#40065;&#26834;&#24615;&#35828;&#35805;&#20154;&#39564;&#35777;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;&#22768;&#23398;&#21442;&#25968;&#21253;&#25324;&#65306;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#65288;MFCC&#65289;&#65292;&#23427;&#20204;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#23548;&#25968;&#65288;Deltas&#21644;Delta-Deltas&#65289;&#65292;&#24052;&#20811;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#65288;BFCC&#65289;&#65292;&#24863;&#30693;&#32447;&#24615;&#39044;&#27979;&#65288;PLP&#65289;&#21644;&#30456;&#23545;&#35889;&#21464;&#25442;&#24863;&#30693;&#32447;&#24615;&#39044;&#27979;&#65288;RASTA-PLP&#65289;&#12290;&#26412;&#25991;&#23545;&#19981;&#21516;&#29305;&#24449;&#32452;&#21512;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#12290;&#20854;&#27425;&#65292;&#20256;&#32479;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#20998;&#31867;&#22120;&#30340;&#20027;&#35201;&#24369;&#28857;&#26159;&#20351;&#29992;&#36890;&#29992;&#20256;&#32479;&#30340;&#26680;&#20989;&#25968;&#35745;&#31639;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speaker Verification (SV) systems involve mainly two individual stages: feature extraction and classification. In this paper, we explore these two modules with the aim of improving the performance of a speaker verification system under noisy conditions. On the one hand, the choice of the most appropriate acoustic features is a crucial factor for performing robust speaker verification. The acoustic parameters used in the proposed system are: Mel Frequency Cepstral Coefficients (MFCC), their first and second derivatives (Deltas and Delta- Deltas), Bark Frequency Cepstral Coefficients (BFCC), Perceptual Linear Predictive (PLP), and Relative Spectral Transform Perceptual Linear Predictive (RASTA-PLP). In this paper, a complete comparison of different combinations of the previous features is discussed. On the other hand, the major weakness of a conventional Support Vector Machine (SVM) classifier is the use of generic traditional kernel functions to compute the distances among data points
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20462;&#22797;&#23454;&#20307;&#32858;&#31867;&#20013;&#30340;&#38169;&#35823;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#30784;&#30456;&#20284;&#24615;&#22270;&#25512;&#23548;&#20986;&#30340;&#22270;&#24230;&#37327;&#65292;&#24182;&#32467;&#21512;&#20102;&#20027;&#21160;&#23398;&#20064;&#26469;&#35299;&#20915;&#25968;&#25454;&#35757;&#32451;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.14992</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#23454;&#20307;&#32858;&#31867;&#20462;&#22797;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph-based Active Learning for Entity Cluster Repair. (arXiv:2401.14992v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20462;&#22797;&#23454;&#20307;&#32858;&#31867;&#20013;&#30340;&#38169;&#35823;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#30784;&#30456;&#20284;&#24615;&#22270;&#25512;&#23548;&#20986;&#30340;&#22270;&#24230;&#37327;&#65292;&#24182;&#32467;&#21512;&#20102;&#20027;&#21160;&#23398;&#20064;&#26469;&#35299;&#20915;&#25968;&#25454;&#35757;&#32451;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#20462;&#22797;&#26041;&#27861;&#26088;&#22312;&#30830;&#23450;&#32858;&#31867;&#20013;&#30340;&#38169;&#35823;&#24182;&#20462;&#25913;&#23427;&#20204;&#65292;&#20197;&#20351;&#27599;&#20010;&#32858;&#31867;&#21253;&#21547;&#20195;&#34920;&#21516;&#19968;&#23454;&#20307;&#30340;&#35760;&#24405;&#12290;&#24403;&#21069;&#30340;&#32858;&#31867;&#20462;&#22797;&#26041;&#27861;&#20027;&#35201;&#20551;&#35774;&#25968;&#25454;&#28304;&#20013;&#27809;&#26377;&#37325;&#22797;&#35760;&#24405;&#65292;&#21363;&#27599;&#20010;&#26469;&#33258;&#19968;&#31181;&#25968;&#25454;&#28304;&#30340;&#35760;&#24405;&#23545;&#24212;&#20110;&#21478;&#19968;&#31181;&#25968;&#25454;&#28304;&#30340;&#21807;&#19968;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24448;&#24448;&#19981;&#31526;&#21512;&#36825;&#19968;&#20551;&#35774;&#65292;&#30001;&#20110;&#36136;&#37327;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23558;&#32858;&#31867;&#26041;&#27861;&#19982;&#38142;&#25509;&#20998;&#31867;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20415;&#21487;&#20197;&#24212;&#29992;&#20110;&#20855;&#26377;&#37325;&#22797;&#35760;&#24405;&#30340;&#25968;&#25454;&#28304;&#12290;&#28982;&#32780;&#65292;&#32467;&#26524;&#24182;&#27809;&#26377;&#23637;&#29616;&#20986;&#28165;&#26224;&#30340;&#22270;&#29255;&#65292;&#22240;&#20026;&#36136;&#37327;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#37197;&#32622;&#21644;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#30784;&#30456;&#20284;&#24615;&#22270;&#25512;&#23548;&#30340;&#22270;&#24230;&#37327;&#30340;&#26032;&#26041;&#27861;&#36827;&#34892;&#32858;&#31867;&#20462;&#22797;&#12290;&#36825;&#20123;&#24230;&#37327;&#22312;&#26500;&#24314;&#20998;&#31867;&#27169;&#22411;&#26102;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29992;&#20110;&#21306;&#20998;&#27491;&#30830;&#21644;&#38169;&#35823;&#30340;&#36793;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#35757;&#32451;&#19981;&#36275;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36824;&#25972;&#21512;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cluster repair methods aim to determine errors in clusters and modify them so that each cluster consists of records representing the same entity. Current cluster repair methodologies primarily assume duplicate-free data sources, where each record from one source corresponds to a unique record from another. However, real-world data often deviates from this assumption due to quality issues. Recent approaches apply clustering methods in combination with link categorization methods so they can be applied to data sources with duplicates. Nevertheless, the results do not show a clear picture since the quality highly varies depending on the configuration and dataset. In this study, we introduce a novel approach for cluster repair that utilizes graph metrics derived from the underlying similarity graphs. These metrics are pivotal in constructing a classification model to distinguish between correct and incorrect edges. To address the challenge of limited training data, we integrate an active l
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20989;&#25968;&#25968;&#25454;&#26144;&#23556;&#21040;&#26377;&#38480;&#32500;&#21442;&#25968;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#33258;&#30001;&#33410;&#28857;&#25918;&#32622;&#31639;&#27861;&#26469;&#21516;&#26102;&#36817;&#20284;&#22810;&#20010;&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#26681;&#25454;&#36755;&#20837;&#25110;&#36755;&#20986;&#20989;&#25968;&#30340;&#23616;&#37096;&#22797;&#26434;&#24615;&#26469;&#20915;&#23450;&#33410;&#28857;&#20301;&#32622;&#65292;&#24615;&#33021;&#31283;&#20581;&#12290;</title><link>http://arxiv.org/abs/2401.14989</link><description>&lt;p&gt;
&#20351;&#29992;&#26032;&#22411;B&#26679;&#26465;&#33258;&#30001;&#33410;&#28857;&#25918;&#32622;&#31639;&#27861;&#30340;&#26144;&#23556;&#21040;&#21442;&#25968;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Mapping-to-Parameter Nonlinear Functional Regression with Novel B-spline Free Knot Placement Algorithm. (arXiv:2401.14989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14989
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20989;&#25968;&#25968;&#25454;&#26144;&#23556;&#21040;&#26377;&#38480;&#32500;&#21442;&#25968;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#33258;&#30001;&#33410;&#28857;&#25918;&#32622;&#31639;&#27861;&#26469;&#21516;&#26102;&#36817;&#20284;&#22810;&#20010;&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#26681;&#25454;&#36755;&#20837;&#25110;&#36755;&#20986;&#20989;&#25968;&#30340;&#23616;&#37096;&#22797;&#26434;&#24615;&#26469;&#20915;&#23450;&#33410;&#28857;&#20301;&#32622;&#65292;&#24615;&#33021;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#31216;&#20026;&#26144;&#23556;&#21040;&#21442;&#25968;&#20989;&#25968;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#20219;&#20309;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#22788;&#29702;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#30340;&#26680;&#24515;&#26159;&#23558;&#20989;&#25968;&#25968;&#25454;&#20174;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#26144;&#23556;&#21040;&#26377;&#38480;&#32500;&#21442;&#25968;&#31354;&#38388;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#20844;&#29992;&#30340;B&#26679;&#26465;&#22522;&#20989;&#25968;&#21516;&#26102;&#36817;&#20284;&#22810;&#20010;&#20989;&#25968;&#26469;&#23454;&#29616;&#30340;&#65292;&#20854;&#33410;&#28857;&#20998;&#24067;&#30001;&#36845;&#20195;&#23616;&#37096;&#25918;&#32622;&#31639;&#27861;&#30830;&#23450;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#33258;&#30001;&#33410;&#28857;&#25918;&#32622;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#31561;&#36317;&#33410;&#28857;&#25918;&#32622;&#31574;&#30053;&#30456;&#27604;&#65292;&#21518;&#32773;&#26681;&#25454;&#36755;&#20837;&#25110;&#36755;&#20986;&#20989;&#25968;&#30340;&#23616;&#37096;&#22797;&#26434;&#24615;&#26469;&#30830;&#23450;&#33410;&#28857;&#20301;&#32622;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#39044;&#23450;&#20041;&#30340;&#33410;&#28857;&#25968;&#22343;&#21248;&#20998;&#24067;&#33410;&#28857;&#20301;&#32622;&#12290;&#25105;&#20204;&#30340;&#33410;&#28857;&#25918;&#32622;&#31639;&#27861;&#30340;&#24615;&#33021;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#20102;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach to nonlinear functional regression, called the Mapping-to-Parameter function model, which addresses complex and nonlinear functional regression problems in parameter space by employing any supervised learning technique. Central to this model is the mapping of function data from an infinite-dimensional function space to a finite-dimensional parameter space. This is accomplished by concurrently approximating multiple functions with a common set of B-spline basis functions by any chosen order, with their knot distribution determined by the Iterative Local Placement Algorithm, a newly proposed free knot placement algorithm. In contrast to the conventional equidistant knot placement strategy that uniformly distributes knot locations based on a predefined number of knots, our proposed algorithms determine knot location according to the local complexity of the input or output functions. The performance of our knot placement algorithms is shown to be robust in both 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;JetBrains IDE&#20013;&#23454;&#26045;&#30340;&#22522;&#20110;&#23884;&#20837;&#24335;&#30340;&#25628;&#32034;&#21151;&#33021;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#25628;&#32034;&#39033;&#30340;&#21487;&#21457;&#29616;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14975</link><description>&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#24335;&#30340;JetBrains IDE&#30340;&#25628;&#32034;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
Embedding-based search in JetBrains IDEs. (arXiv:2401.14975v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14975
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;JetBrains IDE&#20013;&#23454;&#26045;&#30340;&#22522;&#20110;&#23884;&#20837;&#24335;&#30340;&#25628;&#32034;&#21151;&#33021;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#25628;&#32034;&#39033;&#30340;&#21487;&#21457;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#20195;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;&#65288;IDE&#65289;&#21644;&#20195;&#30721;&#32534;&#36753;&#22120;&#37117;&#20855;&#26377;&#36890;&#36807;&#21487;&#29992;&#21151;&#33021;&#21644;&#39033;&#30446;&#20013;&#30340;&#39033;&#36827;&#34892;&#25628;&#32034;&#30340;&#21151;&#33021;&#12290;&#22312;JetBrains IDE&#20013;&#65292;&#36825;&#20010;&#21151;&#33021;&#31216;&#20026;"&#25628;&#32034;&#20840;&#37096;"&#65306;&#23427;&#20801;&#35768;&#29992;&#25143;&#20174;&#19968;&#20010;&#20837;&#21475;&#28857;&#25628;&#32034;&#25991;&#20214;&#12289;&#25805;&#20316;&#12289;&#31867;&#12289;&#31526;&#21495;&#12289;&#35774;&#32622;&#20197;&#21450;&#29256;&#26412;&#25511;&#21046;&#31995;&#32479;(VCS)&#21382;&#21490;&#35760;&#24405;&#20013;&#30340;&#20219;&#20309;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#23427;&#20165;&#36890;&#36807;&#19981;&#32771;&#34385;&#35821;&#20041;&#30340;&#31639;&#27861;&#33719;&#21462;&#20505;&#36873;&#39033;&#65292;&#27604;&#22914;&#21516;&#20041;&#35789;&#12289;&#22797;&#26434;&#30340;&#21333;&#35789;&#25490;&#21015;&#12289;&#35789;&#24615;&#20462;&#25913;&#21644;&#25340;&#20889;&#38169;&#35823;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#23454;&#26045;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25628;&#32034;&#39033;&#30340;&#21487;&#21457;&#29616;&#24615;&#12290;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#38556;&#30861;&#20197;&#21450;&#22914;&#20309;&#20811;&#26381;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most modern Integrated Development Environments (IDEs) and code editors have a feature to search across available functionality and items in an open project. In JetBrains IDEs, this feature is called Search Everywhere: it allows users to search for files, actions, classes, symbols, settings, and anything from VCS history from a single entry point. However, it works with the candidates obtained by algorithms that don't account for semantics, e.g., synonyms, complex word permutations, part of the speech modifications, and typos. In this work, we describe the machine learning approach we implemented to improve the discoverability of search items. We also share the obstacles encountered during this process and how we overcame them.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#23618;&#30340;&#24490;&#29615;&#20999;&#25442;&#29366;&#24577;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#26080;&#30417;&#30563;&#22320;&#21516;&#26102;&#35299;&#37322;&#31995;&#32479;&#32423;&#21644;&#20010;&#20307;&#32423;&#30340;&#21160;&#24577;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#21516;&#27493;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32676;&#20307;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2401.14973</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#24490;&#29615;&#20999;&#25442;&#29366;&#24577;&#27169;&#22411;&#21457;&#29616;&#21516;&#27493;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32676;&#20307;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Discovering group dynamics in synchronous time series via hierarchical recurrent switching-state models. (arXiv:2401.14973v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14973
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#30340;&#24490;&#29615;&#20999;&#25442;&#29366;&#24577;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#26080;&#30417;&#30563;&#22320;&#21516;&#26102;&#35299;&#37322;&#31995;&#32479;&#32423;&#21644;&#20010;&#20307;&#32423;&#30340;&#21160;&#24577;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#21516;&#27493;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32676;&#20307;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33268;&#21147;&#20110;&#23545;&#21516;&#19968;&#26102;&#38388;&#27573;&#20869;&#22810;&#20010;&#23454;&#20307;&#30456;&#20114;&#20316;&#29992;&#32780;&#20135;&#29983;&#30340;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#36827;&#34892;&#24314;&#27169;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24314;&#27169;&#20010;&#20307;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#23545;&#25105;&#20204;&#30340;&#39044;&#26399;&#24212;&#29992;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#20854;&#20013;&#38598;&#20307;&#31995;&#32479;&#32423;&#34892;&#20026;&#24433;&#21709;&#30528;&#20010;&#20307;&#23454;&#20307;&#30340;&#36712;&#36857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#20999;&#25442;&#29366;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;&#65292;&#21516;&#26102;&#35299;&#37322;&#31995;&#32479;&#32423;&#21644;&#20010;&#20307;&#32423;&#30340;&#21160;&#24577;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#38544;&#21547;&#30340;&#31995;&#32479;&#32423;&#31163;&#25955;&#29366;&#24577;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#39537;&#21160;&#30528;&#38544;&#21547;&#30340;&#23454;&#20307;&#32423;&#38142;&#65292;&#36827;&#32780;&#25511;&#21046;&#27599;&#20010;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#12290;&#35266;&#27979;&#32467;&#26524;&#22312;&#23454;&#20307;&#21644;&#31995;&#32479;&#32423;&#30340;&#38142;&#20043;&#38388;&#36827;&#34892;&#21453;&#39304;&#65292;&#36890;&#36807;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#29366;&#24577;&#36716;&#25442;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#23618;&#20999;&#25442;&#24490;&#29615;&#21160;&#21147;&#23398;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#30340;&#21464;&#20998;&#22352;&#26631;&#19978;&#21319;&#26356;&#26032;&#26469;&#23398;&#20064;&#65292;&#20854;&#22312;&#20010;&#20307;&#25968;&#37327;&#19978;&#21576;&#32447;&#24615;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to model a collection of time series arising from multiple entities interacting over the same time period. Recent work focused on modeling individual time series is inadequate for our intended applications, where collective system-level behavior influences the trajectories of individual entities. To address such problems, we present a new hierarchical switching-state model that can be trained in an unsupervised fashion to simultaneously explain both system-level and individual-level dynamics. We employ a latent system-level discrete state Markov chain that drives latent entity-level chains which in turn govern the dynamics of each observed time series. Feedback from the observations to the chains at both the entity and system levels improves flexibility via context-dependent state transitions. Our hierarchical switching recurrent dynamical models can be learned via closed-form variational coordinate ascent updates to all latent chains that scale linearly in the number of indivi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#31616;&#21270;&#39564;&#35777;&#36807;&#31243;&#24182;&#26377;&#25928;&#35757;&#32451;&#20986;&#26131;&#20110;&#39564;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.14961</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-To-End Set-Based Training for Neural Network Verification. (arXiv:2401.14961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#31616;&#21270;&#39564;&#35777;&#36807;&#31243;&#24182;&#26377;&#25928;&#35757;&#32451;&#20986;&#26131;&#20110;&#39564;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21363;&#24494;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#21487;&#33021;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#20135;&#29983;&#37325;&#22823;&#21464;&#21270;&#12290;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#38656;&#35201;&#23545;&#36755;&#20837;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21644;&#24418;&#24335;&#21270;&#39564;&#35777;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#39318;&#27425;&#37319;&#29992;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#35813;&#35757;&#32451;&#26041;&#27861;&#33021;&#22815;&#35757;&#32451;&#20986;&#21487;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#30340;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#33021;&#22815;&#22823;&#22823;&#31616;&#21270;&#24050;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#32493;&#24418;&#24335;&#21270;&#40065;&#26834;&#24615;&#39564;&#35777;&#36807;&#31243;&#12290;&#30456;&#27604;&#20110;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#38598;&#21512;&#30340;&#35745;&#31639;&#26469;&#35757;&#32451;&#25972;&#20010;&#25200;&#21160;&#36755;&#20837;&#38598;&#21512;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#20986;&#26131;&#20110;&#39564;&#35777;&#30340;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are vulnerable to adversarial attacks, i.e., small input perturbations can result in substantially different outputs of a neural network. Safety-critical environments require neural networks that are robust against input perturbations. However, training and formally verifying robust neural networks is challenging. We address this challenge by employing, for the first time, a end-to-end set-based training procedure that trains robust neural networks for formal verification. Our training procedure drastically simplifies the subsequent formal robustness verification of the trained neural network. While previous research has predominantly focused on augmenting neural network training with adversarial attacks, our approach leverages set-based computing to train neural networks with entire sets of perturbed inputs. Moreover, we demonstrate that our set-based training procedure effectively trains robust neural networks, which are easier to verify. In many cases, set-based trai
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;Solomonoff Induction&#65288;SI&#65289;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20351;&#29992;&#19975;&#33021;&#22270;&#28789;&#26426;&#65288;UTMs&#65289;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;UTM&#25968;&#25454;&#26159;&#20803;&#23398;&#20064;&#30340;&#26377;&#20215;&#20540;&#36164;&#28304;&#65292;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.14953</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#29992;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Universal Predictors. (arXiv:2401.14953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;Solomonoff Induction&#65288;SI&#65289;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20351;&#29992;&#19975;&#33021;&#22270;&#28789;&#26426;&#65288;UTMs&#65289;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;UTM&#25968;&#25454;&#26159;&#20803;&#23398;&#20064;&#30340;&#26377;&#20215;&#20540;&#36164;&#28304;&#65292;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24555;&#36895;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#23545;&#19981;&#21516;&#20219;&#21153;&#30340;&#24191;&#27867;&#26292;&#38706;&#23548;&#33268;&#20102;&#22810;&#21151;&#33021;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36890;&#29992;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#20803;&#23398;&#20064;&#30340;&#38480;&#21046;&#26159;&#20160;&#20040;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#26368;&#24378;&#22823;&#30340;&#36890;&#29992;&#39044;&#27979;&#22120;Solomonoff Induction&#65288;SI&#65289;&#36890;&#36807;&#20803;&#23398;&#20064;&#30340;&#26497;&#38480;&#36827;&#34892;&#20998;&#25285;&#65292;&#25506;&#32034;&#20854;&#28508;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19975;&#33021;&#22270;&#28789;&#26426;&#65288;UTMs&#65289;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#29992;&#20110;&#23558;&#32593;&#32476;&#26292;&#38706;&#20110;&#24191;&#27867;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;UTM&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#20803;&#35757;&#32451;&#21327;&#35758;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#22797;&#26434;&#24615;&#21644;&#26222;&#36866;&#24615;&#30340;&#31639;&#27861;&#25968;&#25454;&#29983;&#25104;&#22120;&#23545;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;&#22914;LSTMs&#12289;Transformers&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;UTM&#25968;&#25454;&#26159;&#20803;&#23398;&#20064;&#30340;&#23453;&#36149;&#36164;&#28304;&#65292;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning has emerged as a powerful approach to train neural networks to learn new tasks quickly from limited data. Broad exposure to different tasks leads to versatile representations enabling general problem solving. But, what are the limits of meta-learning? In this work, we explore the potential of amortizing the most powerful universal predictor, namely Solomonoff Induction (SI), into neural networks via leveraging meta-learning to its limits. We use Universal Turing Machines (UTMs) to generate training data used to expose networks to a broad range of patterns. We provide theoretical analysis of the UTM data generation processes and meta-training protocols. We conduct comprehensive experiments with neural architectures (e.g. LSTMs, Transformers) and algorithmic data generators of varying complexity and universality. Our results suggest that UTM data is a valuable resource for meta-learning, and that it can be used to train neural networks capable of learning universal predicti
&lt;/p&gt;</description></item><item><title>&#23545;&#25239;&#35757;&#32451;&#22312;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26159;&#29306;&#29298;&#20102;&#26631;&#20934;&#21644;&#40065;&#26834;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36873;&#25321;&#24615;&#22320;&#26356;&#26032;&#29305;&#23450;&#23618;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32593;&#32476;&#23398;&#20064;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;CURE&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#20445;&#30041;&#12289;&#26356;&#26032;&#21644;&#20462;&#35746;&#26435;&#37325;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#35760;&#24518;&#21270;&#21644;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.14948</link><description>&lt;p&gt;
&#20445;&#30041;-&#26356;&#26032;-&#20462;&#35746;&#20197;&#35299;&#20915;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training. (arXiv:2401.14948v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14948
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#22312;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26159;&#29306;&#29298;&#20102;&#26631;&#20934;&#21644;&#40065;&#26834;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36873;&#25321;&#24615;&#22320;&#26356;&#26032;&#29305;&#23450;&#23618;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32593;&#32476;&#23398;&#20064;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;CURE&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#20445;&#30041;&#12289;&#26356;&#26032;&#21644;&#20462;&#35746;&#26435;&#37325;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#35760;&#24518;&#21270;&#21644;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#20250;&#23548;&#33268;&#26631;&#20934;&#27867;&#21270;&#21644;&#40065;&#26834;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#25581;&#31034;&#39537;&#21160;&#36825;&#19968;&#29616;&#35937;&#30340;&#28508;&#22312;&#22240;&#32032;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#20174;&#26631;&#20934;&#35774;&#32622;&#21040;&#23545;&#25239;&#35774;&#32622;&#36807;&#28193;&#26102;&#30340;&#36880;&#23618;&#23398;&#20064;&#33021;&#21147;&#12290;&#32463;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#24615;&#22320;&#26356;&#26032;&#29305;&#23450;&#23618;&#32780;&#20445;&#30041;&#20854;&#20182;&#23618;&#21487;&#20197;&#22823;&#24133;&#22686;&#24378;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;CURE&#65292;&#21033;&#29992;&#26799;&#24230;&#26174;&#33879;&#24615;&#20934;&#21017;&#23545;&#26435;&#37325;&#36827;&#34892;&#36873;&#25321;&#24615;&#20445;&#30041;&#12289;&#26356;&#26032;&#21644;&#20462;&#35746;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;CURE&#30340;&#35774;&#35745;&#26159;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#19981;&#21487;&#30693;&#30340;&#65292;&#30830;&#20445;&#20854;&#36866;&#29992;&#20110;&#21508;&#31181;&#24773;&#20917;&#12290;&#23427;&#26377;&#25928;&#35299;&#20915;&#20102;&#35760;&#24518;&#21270;&#21644;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#19988;&#36825;&#31181;&#35757;&#32451;&#26041;&#27861;&#36824;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization. To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. Our empirical findings demonstrate that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity. We therefore propose CURE, a novel training framework that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights. Importantly, CURE is designed to be dataset- and architecture-agnostic, ensuring its applicability across various scenarios. It effectively tackles both memorization and overfitting issues, thus enhancing the trade-off between robustness and generalization and additionally, this training approach also 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#34892;&#20026;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;BMRL&#65289;&#26694;&#26550;&#65292;&#22312;&#25705;&#25830;&#24615;&#20219;&#21153;&#20013;&#65292;AI&#20195;&#29702;&#23545;&#26377;&#38480;&#29702;&#24615;&#20154;&#31867;&#20195;&#29702;&#30340;&#21442;&#25968;&#36827;&#34892;&#24178;&#39044;&#65292;&#36890;&#36807;&#23545;&#20154;&#31867;&#31574;&#30053;&#36827;&#34892;&#35299;&#37322;&#65292;&#24110;&#21161;&#29702;&#35299;&#34892;&#20026;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2401.14923</link><description>&lt;p&gt;
&#22312;&#26377;&#38480;&#29702;&#24615;&#20154;&#31867;&#20195;&#29702;&#22312;&#25705;&#25830;&#20219;&#21153;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Interventions on Boundedly Rational Human Agents in Frictionful Tasks. (arXiv:2401.14923v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#34892;&#20026;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;BMRL&#65289;&#26694;&#26550;&#65292;&#22312;&#25705;&#25830;&#24615;&#20219;&#21153;&#20013;&#65292;AI&#20195;&#29702;&#23545;&#26377;&#38480;&#29702;&#24615;&#20154;&#31867;&#20195;&#29702;&#30340;&#21442;&#25968;&#36827;&#34892;&#24178;&#39044;&#65292;&#36890;&#36807;&#23545;&#20154;&#31867;&#31574;&#30053;&#36827;&#34892;&#35299;&#37322;&#65292;&#24110;&#21161;&#29702;&#35299;&#34892;&#20026;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#37325;&#35201;&#30340;&#34892;&#20026;&#21464;&#21270;&#26159;&#20855;&#26377;&#25705;&#25830;&#30340;&#65307;&#23427;&#20204;&#35201;&#27714;&#20010;&#20307;&#38271;&#26399;&#20184;&#20986;&#21162;&#21147;&#65292;&#20294;&#27809;&#26377;&#21363;&#26102;&#30340;&#28385;&#36275;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20195;&#29702;&#21487;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#24178;&#39044;&#65292;&#24110;&#21161;&#20010;&#20307;&#22362;&#25345;&#33258;&#24049;&#30340;&#30446;&#26631;&#12290;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;AI&#20195;&#29702;&#24517;&#39035;&#24555;&#36895;&#20010;&#24615;&#21270;&#65288;&#22312;&#20010;&#20307;&#22833;&#21435;&#20852;&#36259;&#20043;&#21069;&#65289;&#24182;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#34892;&#20026;&#24178;&#39044;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#34892;&#20026;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;BMRL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;AI&#20195;&#29702;&#23545;&#23646;&#20110;&#26377;&#38480;&#29702;&#24615;&#20154;&#31867;&#20195;&#29702;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#21442;&#25968;&#36827;&#34892;&#24178;&#39044;&#12290;&#25105;&#20204;&#23558;&#20154;&#31867;&#20915;&#31574;&#32773;&#30340;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#35268;&#21010;&#20195;&#29702;&#65292;&#36825;&#26679;&#25105;&#20204;&#23601;&#33021;&#22815;&#23558;&#19981;&#29702;&#24819;&#30340;&#20154;&#31867;&#31574;&#30053;&#65288;&#19981;&#20250;&#36798;&#21040;&#30446;&#26631;&#30340;&#31574;&#30053;&#65289;&#24402;&#22240;&#20110;&#20854;&#19981;&#36866;&#24212;&#30340;MDP&#21442;&#25968;&#65292;&#27604;&#22914;&#26497;&#20302;&#30340;&#25240;&#25187;&#22240;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#27169;&#22411;&#65292;&#25429;&#25417;&#20102;&#25705;&#25830;&#20219;&#21153;&#20013;&#30340;&#22522;&#26412;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many important behavior changes are frictionful; they require individuals to expend effort over a long period with little immediate gratification. Here, an artificial intelligence (AI) agent can provide personalized interventions to help individuals stick to their goals. In these settings, the AI agent must personalize rapidly (before the individual disengages) and interpretably, to help us understand the behavioral interventions. In this paper, we introduce Behavior Model Reinforcement Learning (BMRL), a framework in which an AI agent intervenes on the parameters of a Markov Decision Process (MDP) belonging to a boundedly rational human agent. Our formulation of the human decision-maker as a planning agent allows us to attribute undesirable human policies (ones that do not lead to the goal) to their maladapted MDP parameters, such as an extremely low discount factor. Furthermore, we propose a class of tractable human models that captures fundamental behaviors in frictionful tasks. Int
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21551;&#29992;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26500;&#24314;&#26412;&#22320;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65292;&#20197;&#20445;&#35777;&#24191;&#27867;&#31867;&#21035;&#30340;&#38750;&#32447;&#24615;&#28151;&#21512;&#21160;&#21147;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#35813;&#26041;&#27861;&#26159;&#39640;&#25928;&#30340;&#65292;&#23545;&#21442;&#32771;&#25511;&#21046;&#22120;&#30340;&#24178;&#39044;&#26368;&#23567;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#21644;&#27604;&#36739;&#26696;&#20363;&#23637;&#31034;&#20102;&#20854;&#21151;&#25928;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14907</link><description>&lt;p&gt;
&#23398;&#20064;&#26412;&#22320;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#20197;&#23454;&#29616;&#28151;&#21512;&#31995;&#32479;&#30340;&#23433;&#20840;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning Local Control Barrier Functions for Safety Control of Hybrid Systems. (arXiv:2401.14907v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14907
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21551;&#29992;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26500;&#24314;&#26412;&#22320;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65292;&#20197;&#20445;&#35777;&#24191;&#27867;&#31867;&#21035;&#30340;&#38750;&#32447;&#24615;&#28151;&#21512;&#21160;&#21147;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#35813;&#26041;&#27861;&#26159;&#39640;&#25928;&#30340;&#65292;&#23545;&#21442;&#32771;&#25511;&#21046;&#22120;&#30340;&#24178;&#39044;&#26368;&#23567;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#21644;&#27604;&#36739;&#26696;&#20363;&#23637;&#31034;&#20102;&#20854;&#21151;&#25928;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#21160;&#21147;&#31995;&#32479;&#22312;&#23454;&#38469;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24120;&#28041;&#21450;&#36830;&#32493;&#29366;&#24577;&#21644;&#31163;&#25955;&#29366;&#24577;&#20999;&#25442;&#12290;&#23433;&#20840;&#24615;&#26159;&#28151;&#21512;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#39318;&#35201;&#20851;&#27880;&#28857;&#12290;&#29616;&#26377;&#30340;&#28151;&#21512;&#31995;&#32479;&#30340;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#26041;&#27861;&#35201;&#20040;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#65292;&#23545;&#31995;&#32479;&#24615;&#33021;&#26377;&#25439;&#65292;&#35201;&#20040;&#20165;&#36866;&#29992;&#20110;&#23567;&#35268;&#27169;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21551;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#26412;&#22320;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBFs&#65289;&#65292;&#20197;&#20445;&#35777;&#24191;&#27867;&#31867;&#21035;&#30340;&#38750;&#32447;&#24615;&#28151;&#21512;&#21160;&#21147;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#23433;&#20840;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;CBF&#20999;&#25442;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#39640;&#25928;&#65292;&#23545;&#20219;&#20309;&#21442;&#32771;&#25511;&#21046;&#22120;&#30340;&#24178;&#39044;&#26368;&#23567;&#65292;&#24182;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#31995;&#32479;&#12290;&#36890;&#36807;&#20004;&#20010;&#26426;&#22120;&#20154;&#31034;&#20363;&#65288;&#21253;&#25324;&#39640;&#32500;&#33258;&#20027;&#36187;&#36710;&#26696;&#20363;&#65289;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#19982;&#20854;&#20182;&#22522;&#20110;CBF&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#20854;&#21151;&#25928;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hybrid dynamical systems are ubiquitous as practical robotic applications often involve both continuous states and discrete switchings. Safety is a primary concern for hybrid robotic systems. Existing safety-critical control approaches for hybrid systems are either computationally inefficient, detrimental to system performance, or limited to small-scale systems. To amend these drawbacks, in this paper, we propose a learningenabled approach to construct local Control Barrier Functions (CBFs) to guarantee the safety of a wide class of nonlinear hybrid dynamical systems. The end result is a safe neural CBFbased switching controller. Our approach is computationally efficient, minimally invasive to any reference controller, and applicable to large-scale systems. We empirically evaluate our framework and demonstrate its efficacy and flexibility through two robotic examples including a high-dimensional autonomous racing case, against other CBF-based approaches and model predictive control.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#26500;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#19981;&#21516;&#20132;&#21449;&#23376;&#32676;&#20307;&#38388;&#30340;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#25552;&#20379;&#21487;&#38752;&#30340;&#31995;&#32479;&#24615;&#33021;&#20272;&#35745;&#65292;&#21363;&#20351;&#23545;&#20110;&#24456;&#23567;&#30340;&#23376;&#32676;&#20307;&#12290;</title><link>http://arxiv.org/abs/2401.14893</link><description>&lt;p&gt;
&#35780;&#20272;&#27169;&#22411;&#22312;&#20132;&#21449;&#23376;&#32676;&#20307;&#38388;&#24615;&#33021;&#30340;&#32467;&#26500;&#22238;&#24402;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A structured regression approach for evaluating model performance across intersectional subgroups. (arXiv:2401.14893v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#26500;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#19981;&#21516;&#20132;&#21449;&#23376;&#32676;&#20307;&#38388;&#30340;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#25552;&#20379;&#21487;&#38752;&#30340;&#31995;&#32479;&#24615;&#33021;&#20272;&#35745;&#65292;&#21363;&#20351;&#23545;&#20110;&#24456;&#23567;&#30340;&#23376;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#20844;&#24179;&#24615;&#35780;&#20272;&#20013;&#65292;&#20998;&#35299;&#24335;&#35780;&#20272;&#26159;&#19968;&#39033;&#26680;&#24515;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#30001;&#20154;&#21475;&#32479;&#35745;&#23398;&#25110;&#20854;&#20182;&#25935;&#24863;&#23646;&#24615;&#32452;&#21512;&#23450;&#20041;&#30340;&#19981;&#21516;&#23376;&#32676;&#20307;&#20013;&#30340;&#24615;&#33021;&#12290;&#26631;&#20934;&#26041;&#27861;&#26159;&#23558;&#35780;&#20272;&#25968;&#25454;&#20998;&#23618;&#21040;&#23376;&#32676;&#20307;&#20013;&#65292;&#24182;&#20998;&#21035;&#35745;&#31639;&#27599;&#20010;&#32452;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#22312;&#32771;&#34385;&#21040;&#20132;&#21449;&#23376;&#32676;&#20307;&#26102;&#26679;&#26412;&#25968;&#37327;&#20063;&#20250;&#36805;&#36895;&#21464;&#23567;&#65292;&#36825;&#22823;&#22823;&#38480;&#21046;&#20102;&#35768;&#22810;&#20998;&#35299;&#35780;&#20272;&#20013;&#23545;&#20132;&#21449;&#32676;&#20307;&#30340;&#32771;&#34385;&#31243;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32467;&#26500;&#22238;&#24402;&#26041;&#27861;&#26469;&#36827;&#34892;&#20998;&#35299;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#23567;&#30340;&#23376;&#32676;&#20307;&#65292;&#35813;&#26041;&#27861;&#20063;&#33021;&#20135;&#29983;&#21487;&#38752;&#30340;&#31995;&#32479;&#24615;&#33021;&#20272;&#35745;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#25512;&#26029;&#31574;&#30053;&#26469;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#25506;&#32034;&#20102;&#25311;&#21512;&#20248;&#24230;&#27979;&#35797;&#22914;&#20309;&#25581;&#31034;&#20132;&#21449;&#23376;&#32676;&#20307;&#25152;&#32463;&#21382;&#30340;&#19982;&#20844;&#24179;&#30456;&#20851;&#30340;&#20260;&#23475;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disaggregated evaluation is a central task in AI fairness assessment, with the goal to measure an AI system's performance across different subgroups defined by combinations of demographic or other sensitive attributes. The standard approach is to stratify the evaluation data across subgroups and compute performance metrics separately for each group. However, even for moderately-sized evaluation datasets, sample sizes quickly get small once considering intersectional subgroups, which greatly limits the extent to which intersectional groups are considered in many disaggregated evaluations. In this work, we introduce a structured regression approach to disaggregated evaluation that we demonstrate can yield reliable system performance estimates even for very small subgroups. We also provide corresponding inference strategies for constructing confidence intervals and explore how goodness-of-fit testing can yield insight into the structure of fairness-related harms experienced by intersectio
&lt;/p&gt;</description></item><item><title>P3LS&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#20559;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#31227;&#21160;&#30340;&#38543;&#26426;&#25513;&#30721;&#20445;&#25252;&#27599;&#20010;&#25968;&#25454;&#25345;&#26377;&#32773;&#30340;&#38544;&#31169;&#65292;&#23454;&#29616;&#20102;&#36328;&#32452;&#32455;&#25968;&#25454;&#38598;&#25104;&#21644;&#36807;&#31243;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2401.14884</link><description>&lt;p&gt;
P3LS: &#38544;&#31169;&#20445;&#25252;&#19979;&#30340;&#20559;&#26368;&#23567;&#20108;&#20056;&#27861;
&lt;/p&gt;
&lt;p&gt;
P3LS: Partial Least Squares under Privacy Preservation. (arXiv:2401.14884v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14884
&lt;/p&gt;
&lt;p&gt;
P3LS&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#20559;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#31227;&#21160;&#30340;&#38543;&#26426;&#25513;&#30721;&#20445;&#25252;&#27599;&#20010;&#25968;&#25454;&#25345;&#26377;&#32773;&#30340;&#38544;&#31169;&#65292;&#23454;&#29616;&#20102;&#36328;&#32452;&#32455;&#25968;&#25454;&#38598;&#25104;&#21644;&#36807;&#31243;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#21046;&#36896;&#19994;&#20215;&#20540;&#38142;&#38656;&#35201;&#36328;&#20844;&#21496;&#36793;&#30028;&#26234;&#33021;&#21327;&#35843;&#27969;&#31243;&#65292;&#20197;&#26368;&#22823;&#21270;&#21033;&#28070;&#21516;&#26102;&#20419;&#36827;&#31038;&#20250;&#21644;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25968;&#25454;&#30340;&#20215;&#20540;&#38142;&#20915;&#31574;&#30340;&#38598;&#25104;&#24335;&#31995;&#32479;&#32423;&#26041;&#27861;&#30340;&#23454;&#26045;&#65292;&#30446;&#21069;&#21463;&#21040;&#19982;&#36328;&#32452;&#32455;&#25968;&#25454;&#20132;&#25442;&#21644;&#38598;&#25104;&#30456;&#20851;&#30340;&#38544;&#31169;&#20851;&#27880;&#30340;&#38459;&#30861;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#38544;&#31169;&#20445;&#25252;&#30340;&#20559;&#26368;&#23567;&#20108;&#20056;&#65288;P3LS&#65289;&#22238;&#24402;&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#26377;&#38544;&#31169;&#20445;&#35777;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36328;&#32452;&#32455;&#25968;&#25454;&#38598;&#25104;&#21644;&#36807;&#31243;&#24314;&#27169;&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#12290;P3LS&#28041;&#21450;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;PLS&#31639;&#27861;&#65292;&#24182;&#37319;&#29992;&#30001;&#21487;&#20449;&#26426;&#26500;&#29983;&#25104;&#30340;&#21487;&#31227;&#21160;&#30340;&#38543;&#26426;&#25513;&#30721;&#26469;&#20445;&#25252;&#27599;&#20010;&#25968;&#25454;&#25345;&#26377;&#32773;&#36129;&#29486;&#30340;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;P3LS&#22312;&#30001;&#19977;&#20010;&#21442;&#19982;&#26041;&#32452;&#25104;&#30340;&#20551;&#24819;&#20215;&#20540;&#38142;&#19978;&#22402;&#30452;&#25972;&#21512;&#36807;&#31243;&#25968;&#25454;&#24182;&#25552;&#39640;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern manufacturing value chains require intelligent orchestration of processes across company borders in order to maximize profits while fostering social and environmental sustainability. However, the implementation of integrated, systems-level approaches for data-informed decision-making along value chains is currently hampered by privacy concerns associated with cross-organizational data exchange and integration. We here propose Privacy-Preserving Partial Least Squares (P3LS) regression, a novel federated learning technique that enables cross-organizational data integration and process modeling with privacy guarantees. P3LS involves a singular value decomposition (SVD) based PLS algorithm and employs removable, random masks generated by a trusted authority in order to protect the privacy of the data contributed by each data holder. We demonstrate the capability of P3LS to vertically integrate process data along a hypothetical value chain consisting of three parties and to improve t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#65288;CSF&#65289;&#65292;&#21487;&#20197;&#20174;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#31354;&#38388;&#20013;&#25552;&#21462;&#33258;&#36866;&#24212;&#39057;&#29575;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14876</link><description>&lt;p&gt;
&#36328;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#65306;&#38598;&#25104;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#20197;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem. (arXiv:2401.14876v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#65288;CSF&#65289;&#65292;&#21487;&#20197;&#20174;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#31354;&#38388;&#20013;&#25552;&#21462;&#33258;&#36866;&#24212;&#39057;&#29575;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#20351;&#29992;&#20302;&#36890;&#28388;&#27874;&#22120;&#20174;&#22270;&#25299;&#25169;&#20013;&#25552;&#21462;&#20302;&#39057;&#20449;&#21495;&#65292;&#20294;&#24403;GCN&#28145;&#24230;&#22686;&#21152;&#26102;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#20174;&#22270;&#25299;&#25169;&#20013;&#25552;&#21462;&#30340;&#39069;&#22806;&#28388;&#27874;&#22120;&#65288;&#22914;&#39640;&#36890;&#28388;&#27874;&#22120;&#65289;&#26469;&#21019;&#24314;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#24573;&#35270;&#20102;&#33410;&#28857;&#23646;&#24615;&#31354;&#38388;&#65292;&#36825;&#20005;&#37325;&#29306;&#29298;&#20102;&#28145;&#23618;GCN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#38750;&#21516;&#37197;&#22270;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#65292;&#31216;&#20026;CSF&#65292;&#33021;&#22815;&#20174;&#25299;&#25169;&#21644;&#23646;&#24615;&#31354;&#38388;&#20013;&#25552;&#21462;&#33258;&#36866;&#24212;&#39057;&#29575;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#20110;&#23646;&#24615;&#30340;&#39640;&#36890;&#28388;&#27874;&#22120;&#65292;&#21487;&#20197;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20026;&#21322;&#30417;&#30563;&#26680;&#23725;&#22238;&#24402;&#30340;&#26368;&#23567;&#21270;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#25299;&#25169;&#30340;&#20302;&#36890;&#28388;&#27874;&#22120;&#35270;&#20026;Mercer's&#26680;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to extract low-frequency signals from graph topology, which may lead to the over-smoothing problem when GCN goes deep. To this end, various methods have been proposed to create an adaptive filter by incorporating an extra filter (e.g., a high-pass filter) extracted from the graph topology. However, these methods heavily rely on topological information and ignore the node attribute space, which severely sacrifices the expressive power of the deep GCNs, especially when dealing with disassortative graphs. In this paper, we propose a cross-space adaptive filter, called CSF, to produce the adaptive-frequency information extracted from both the topology and attribute spaces. Specifically, we first derive a tailored attribute-based high-pass filter that can be interpreted theoretically as a minimizer for semi-supervised kernel ridge regression. Then, we cast the topology-based low-pass filter as a Mercer's kernel within the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#36807;&#31243;&#25968;&#25454;&#20013;&#25552;&#21462;&#36807;&#31243;&#24863;&#30693;&#30340;&#20915;&#31574;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20915;&#31574;&#25366;&#25496;&#20013;&#30340;&#22810;&#20010;&#23545;&#35937;&#21644;&#39034;&#24207;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14847</link><description>&lt;p&gt;
&#20174;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#36807;&#31243;&#25968;&#25454;&#20013;&#25552;&#21462;&#36807;&#31243;&#24863;&#30693;&#30340;&#20915;&#31574;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Extracting Process-Aware Decision Models from Object-Centric Process Data. (arXiv:2401.14847v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#36807;&#31243;&#25968;&#25454;&#20013;&#25552;&#21462;&#36807;&#31243;&#24863;&#30693;&#30340;&#20915;&#31574;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20915;&#31574;&#25366;&#25496;&#20013;&#30340;&#22810;&#20010;&#23545;&#35937;&#21644;&#39034;&#24207;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#22312;&#26085;&#24120;&#19994;&#21153;&#36807;&#31243;&#20013;&#25191;&#34892;&#20915;&#31574;&#26102;&#24517;&#39035;&#32771;&#34385;&#22810;&#20010;&#21033;&#30410;&#30456;&#20851;&#26041;&#65292;&#36825;&#20123;&#21033;&#30410;&#30456;&#20851;&#26041;&#21487;&#33021;&#38656;&#35201;&#23545;&#21516;&#19968;&#27969;&#31243;&#26377;&#22810;&#20010;&#19981;&#21516;&#30340;&#35266;&#28857;&#12290;&#27492;&#22806;&#65292;&#36816;&#34892;&#36825;&#20123;&#19994;&#21153;&#27969;&#31243;&#30340;&#20449;&#24687;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#36890;&#24120;&#24456;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#23384;&#20648;&#25152;&#26377;&#30456;&#20851;&#25968;&#25454;&#21644;&#27969;&#31243;&#26041;&#38754;&#30340;&#25968;&#25454;&#24211;&#30456;&#36830;&#12290;&#37492;&#20110;&#20449;&#24687;&#31995;&#32479;&#20013;&#23384;&#22312;&#25903;&#25345;&#27969;&#31243;&#23454;&#26045;&#30340;&#22810;&#20010;&#23545;&#35937;&#65292;&#20915;&#31574;&#33258;&#28982;&#21463;&#21040;&#36825;&#20004;&#20010;&#35270;&#35282;&#30340;&#24433;&#21709;&#65292;&#35760;&#24405;&#22312;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#36807;&#31243;&#26085;&#24535;&#20013;&#12290;&#28982;&#32780;&#65292;&#20174;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#27969;&#31243;&#26085;&#24535;&#20013;&#21457;&#29616;&#36825;&#26679;&#30340;&#20915;&#31574;&#24182;&#19981;&#31616;&#21333;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#27491;&#30830;&#22320;&#38142;&#25509;&#25152;&#28041;&#21450;&#30340;&#23545;&#35937;&#65292;&#21516;&#26102;&#32771;&#34385;&#19994;&#21153;&#27969;&#31243;&#25152;&#26045;&#21152;&#30340;&#39034;&#24207;&#32422;&#26463;&#65292;&#24182;&#27491;&#30830;&#22320;&#21457;&#29616;&#20915;&#31574;&#30340;&#23454;&#38469;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#20915;&#31574;&#25366;&#25496;&#31639;&#27861;&#65292;&#31216;&#20026;&#32508;&#21512;&#23545;&#35937;&#20013;&#24515;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Organizations execute decisions within business processes on a daily basis whilst having to take into account multiple stakeholders who might require multiple point of views of the same process. Moreover, the complexity of the information systems running these business processes is generally high as they are linked to databases storing all the relevant data and aspects of the processes. Given the presence of multiple objects within an information system which support the processes in their enactment, decisions are naturally influenced by both these perspectives, logged in object-centric process logs. However, the discovery of such decisions from object-centric process logs is not straightforward as it requires to correctly link the involved objects whilst considering the sequential constraints that business processes impose as well as correctly discovering what a decision actually does. This paper proposes the first object-centric decision-mining algorithm called Integrated Object-cent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22495;&#27867;&#21270;&#31639;&#27861;&#21644;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22495;&#27867;&#21270;&#31639;&#27861;&#22312;&#26377;&#38480;&#26679;&#26412;&#35757;&#32451;&#20013;&#26174;&#31034;&#20986;&#38544;&#24615;&#30340;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#20551;&#20887;&#20313;&#30456;&#20851;&#21644;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#22312;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#30456;&#27604;ERM&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14846</link><description>&lt;p&gt;
&#29702;&#35299;&#22495;&#27867;&#21270;&#65306;&#20174;&#22122;&#22768;&#40065;&#26834;&#24615;&#30340;&#35282;&#24230;
&lt;/p&gt;
&lt;p&gt;
Understanding Domain Generalization: A Noise Robustness Perspective. (arXiv:2401.14846v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22495;&#27867;&#21270;&#31639;&#27861;&#21644;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22495;&#27867;&#21270;&#31639;&#27861;&#22312;&#26377;&#38480;&#26679;&#26412;&#35757;&#32451;&#20013;&#26174;&#31034;&#20986;&#38544;&#24615;&#30340;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#20551;&#20887;&#20313;&#30456;&#20851;&#21644;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#22312;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#30456;&#27604;ERM&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24555;&#36895;&#21457;&#23637;&#20102;&#29992;&#20110;&#22495;&#27867;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#27809;&#26377;&#26126;&#30830;&#30340;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#29616;&#26377;&#30340;&#22495;&#27867;&#21270;&#31639;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#32463;&#20856;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#31639;&#27861;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#36890;&#36807;&#26631;&#31614;&#22122;&#22768;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#22495;&#27867;&#21270;&#31639;&#27861;&#30456;&#23545;&#20110;ERM&#30340;&#20248;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;&#25581;&#31034;&#20102;&#26631;&#31614;&#22122;&#22768;&#21152;&#21095;&#20102;ERM&#20013;&#20551;&#20887;&#20313;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#65292;&#21066;&#24369;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#26679;&#26412;&#35757;&#32451;&#20013;&#65292;&#22495;&#27867;&#21270;&#31639;&#27861;&#22312;&#23384;&#22312;&#20551;&#20887;&#20313;&#30456;&#20851;&#26102;&#20855;&#26377;&#38544;&#24615;&#30340;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;&#36825;&#31181;&#26377;&#21033;&#24615;&#26377;&#21161;&#20110;&#20943;&#36731;&#20551;&#20887;&#20313;&#30456;&#20851;&#24615;&#24182;&#25913;&#21892;&#21512;&#25104;&#23454;&#39564;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#39069;&#22806;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#30456;&#27604;ERM&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rapid development of machine learning algorithms for domain generalization (DG), there is no clear empirical evidence that the existing DG algorithms outperform the classic empirical risk minimization (ERM) across standard benchmarks. To better understand this phenomenon, we investigate whether there are benefits of DG algorithms over ERM through the lens of label noise. Specifically, our finite-sample analysis reveals that label noise exacerbates the effect of spurious correlations for ERM, undermining generalization. Conversely, we illustrate that DG algorithms exhibit implicit label-noise robustness during finite-sample training even when spurious correlation is present. Such desirable property helps mitigate spurious correlations and improve generalization in synthetic experiments. However, additional comprehensive experiments on real-world benchmark datasets indicate that label-noise robustness does not necessarily translate to better performance compared to ERM. We co
&lt;/p&gt;</description></item><item><title>AdaPT&#26159;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#28857;&#20113;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#20943;&#23569;token&#30340;&#25968;&#37327;&#26469;&#39640;&#25928;&#22788;&#29702;&#22823;&#22411;&#28857;&#20113;&#65292;&#24182;&#19988;&#21487;&#20197;&#28789;&#27963;&#35843;&#25972;&#35745;&#31639;&#25104;&#26412;&#12290;Experimental evaluation demonstrates that AdaPT significantly reduces computational cost.</title><link>http://arxiv.org/abs/2401.14845</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#28857;&#20113; Transformer
&lt;/p&gt;
&lt;p&gt;
Adaptive Point Transformer. (arXiv:2401.14845v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14845
&lt;/p&gt;
&lt;p&gt;
AdaPT&#26159;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#28857;&#20113;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#20943;&#23569;token&#30340;&#25968;&#37327;&#26469;&#39640;&#25928;&#22788;&#29702;&#22823;&#22411;&#28857;&#20113;&#65292;&#24182;&#19988;&#21487;&#20197;&#28789;&#27963;&#35843;&#25972;&#35745;&#31639;&#25104;&#26412;&#12290;Experimental evaluation demonstrates that AdaPT significantly reduces computational cost.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;3D&#25968;&#25454;&#33719;&#21462;&#30340;&#28608;&#22686;&#25512;&#21160;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#28857;&#20113;&#22788;&#29702;&#20013;&#30340;&#21457;&#23637;&#65292;&#36825;&#19968;&#21457;&#23637;&#21463;&#21040;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013; Transformer &#30340;&#26174;&#33879;&#25104;&#21151;&#30340;&#25512;&#21160;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#28857;&#20113; Transformer &#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#23427;&#20204;&#19982;&#28857;&#20113;&#22823;&#23567;&#21576;&#20108;&#27425;&#27604;&#20363;&#20851;&#31995;&#65292;&#23545;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#23384;&#22312;&#21487;&#20280;&#32553;&#24615;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#28857;&#20113; Transformer (AdaPT)&#65292;&#36825;&#26159;&#19968;&#20010;&#26631;&#20934;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;token&#36873;&#25321;&#26426;&#21046;&#21152;&#20197;&#25193;&#23637;&#12290;AdaPT&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#21160;&#24577;&#20943;&#23569;&#20102;token&#30340;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#28857;&#20113;&#30340;&#39640;&#25928;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39044;&#31639;&#26426;&#21046;&#65292;&#22312;&#25512;&#26029;&#26102;&#28789;&#27963;&#35843;&#25972;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#20998;&#31163;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#28857;&#20113;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;AdaPT&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge in 3D data acquisition has spurred the development of geometric deep learning models for point cloud processing, boosted by the remarkable success of transformers in natural language processing. While point cloud transformers (PTs) have achieved impressive results recently, their quadratic scaling with respect to the point cloud size poses a significant scalability challenge for real-world applications. To address this issue, we propose the Adaptive Point Cloud Transformer (AdaPT), a standard PT model augmented by an adaptive token selection mechanism. AdaPT dynamically reduces the number of tokens during inference, enabling efficient processing of large point clouds. Furthermore, we introduce a budget mechanism to flexibly adjust the computational cost of the model at inference time without the need for retraining or fine-tuning separate models. Our extensive experimental evaluation on point cloud classification tasks demonstrates that AdaPT significantly reduces comp
&lt;/p&gt;</description></item><item><title>GuardML&#24341;&#20837;&#20102;&#28151;&#21512;&#21516;&#24577;&#21152;&#23494;&#65288;HHE&#65289;&#21040;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#38754;&#21521;&#32456;&#31471;&#35774;&#22791;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26041;&#26696;&#65292;&#21487;&#20197;&#23433;&#20840;&#22320;&#23545;&#21152;&#23494;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#23398;&#20064;&#65292;&#20855;&#26377;&#39640;&#25928;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14840</link><description>&lt;p&gt;
GuardML: &#36890;&#36807;&#28151;&#21512;&#21516;&#24577;&#21152;&#23494;&#23454;&#29616;&#39640;&#25928;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
GuardML: Efficient Privacy-Preserving Machine Learning Services Through Hybrid Homomorphic Encryption. (arXiv:2401.14840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14840
&lt;/p&gt;
&lt;p&gt;
GuardML&#24341;&#20837;&#20102;&#28151;&#21512;&#21516;&#24577;&#21152;&#23494;&#65288;HHE&#65289;&#21040;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#38754;&#21521;&#32456;&#31471;&#35774;&#22791;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26041;&#26696;&#65292;&#21487;&#20197;&#23433;&#20840;&#22320;&#23545;&#21152;&#23494;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#23398;&#20064;&#65292;&#20855;&#26377;&#39640;&#25928;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#25104;&#20026;&#25968;&#25454;&#31185;&#23398;&#20013;&#26368;&#20855;&#21464;&#38761;&#24615;&#21644;&#24433;&#21709;&#21147;&#30340;&#39046;&#22495;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;ML&#30340;&#24191;&#27867;&#24212;&#29992;&#24341;&#20837;&#20102;&#38544;&#31169;&#30456;&#20851;&#30340;&#25285;&#24551;&#65292;&#22240;&#20026;&#24694;&#24847;&#25915;&#20987;&#38024;&#23545;ML&#27169;&#22411;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#65288;PPML&#65289;&#26041;&#27861;&#26469;&#20445;&#25252;ML&#27169;&#22411;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;HE&#30340;&#26174;&#33879;&#32570;&#28857;&#21644;&#20302;&#25928;&#24615;&#20351;&#20854;&#22312;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#22330;&#26223;&#20013;&#19981;&#23454;&#29992;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#29616;&#20195;&#23494;&#30721;&#26041;&#26696;&#65292;&#28151;&#21512;&#21516;&#24577;&#21152;&#23494;&#65288;HHE&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#23545;&#31216;&#23494;&#30721;&#21644;HE&#30340;&#20248;&#28857;&#65292;&#20811;&#26381;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#38024;&#23545;&#32456;&#31471;&#35774;&#22791;&#30340;PPML&#26041;&#26696;&#26469;&#24341;&#20837;HHE&#21040;ML&#20013;&#12290;&#25105;&#20204;&#21033;&#29992;HHE&#20316;&#20026;&#22522;&#26412;&#26500;&#24314;&#22359;&#65292;&#23454;&#29616;&#23545;&#21152;&#23494;&#25968;&#25454;&#19978;&#30340;&#20998;&#31867;&#32467;&#26524;&#30340;&#23433;&#20840;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#35777;&#39640;&#25928;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has emerged as one of data science's most transformative and influential domains. However, the widespread adoption of ML introduces privacy-related concerns owing to the increasing number of malicious attacks targeting ML models. To address these concerns, Privacy-Preserving Machine Learning (PPML) methods have been introduced to safeguard the privacy and security of ML models. One such approach is the use of Homomorphic Encryption (HE). However, the significant drawbacks and inefficiencies of traditional HE render it impractical for highly scalable scenarios. Fortunately, a modern cryptographic scheme, Hybrid Homomorphic Encryption (HHE), has recently emerged, combining the strengths of symmetric cryptography and HE to surmount these challenges. Our work seeks to introduce HHE to ML by designing a PPML scheme tailored for end devices. We leverage HHE as the fundamental building block to enable secure learning of classification outcomes over encrypted data, all wh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#34507;&#30333;&#36136;&#32467;&#26500;&#25968;&#25454;&#26469;&#22686;&#24378;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#30001;&#20110;&#20381;&#36182;&#22823;&#37327;&#24207;&#21015;&#25968;&#25454;&#21644;&#21442;&#25968;&#38598;&#32780;&#38480;&#21046;&#20854;&#28789;&#27963;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14819</link><description>&lt;p&gt;
&#36171;&#20104;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Endowing Protein Language Models with Structural Knowledge. (arXiv:2401.14819v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#34507;&#30333;&#36136;&#32467;&#26500;&#25968;&#25454;&#26469;&#22686;&#24378;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#30001;&#20110;&#20381;&#36182;&#22823;&#37327;&#24207;&#21015;&#25968;&#25454;&#21644;&#21442;&#25968;&#38598;&#32780;&#38480;&#21046;&#20854;&#28789;&#27963;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#34507;&#30333;&#36136;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#21151;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#29983;&#29289;&#23398;&#25361;&#25112;&#65292;&#23545;&#20110;&#33647;&#29289;&#35774;&#35745;&#21644;&#25105;&#20204;&#23545;&#36827;&#21270;&#30340;&#29702;&#35299;&#20855;&#26377;&#22810;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#26368;&#36817;&#65292;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#36825;&#19968;&#25361;&#25112;&#30340;&#39318;&#36873;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#24207;&#21015;&#25968;&#25454;&#24211;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#24191;&#27867;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#30340;&#28789;&#27963;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#24207;&#21015;&#25968;&#25454;&#21644;&#21442;&#25968;&#38598;&#30340;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36817;&#24180;&#26469;&#35745;&#31639;&#39044;&#27979;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#22686;&#38271;&#20026;&#34507;&#30333;&#36136;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#34429;&#28982;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#36825;&#26679;&#22797;&#26434;&#25968;&#25454;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#36127;&#25285;&#20173;&#28982;&#38459;&#30861;&#20102;&#24191;&#27867;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#34507;&#30333;&#36136;&#32467;&#26500;&#25968;&#25454;&#26469;&#22686;&#24378;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#12290;&#20174;&#22270;&#21464;&#25442;&#30340;&#26368;&#26032;&#36827;&#23637;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the relationships between protein sequence, structure and function is a long-standing biological challenge with manifold implications from drug design to our understanding of evolution. Recently, protein language models have emerged as the preferred method for this challenge, thanks to their ability to harness large sequence databases. Yet, their reliance on expansive sequence data and parameter sets limits their flexibility and practicality in real-world scenarios. Concurrently, the recent surge in computationally predicted protein structures unlocks new opportunities in protein representation learning. While promising, the computational burden carried by such complex data still hinders widely-adopted practical applications. To address these limitations, we introduce a novel framework that enhances protein language models by integrating protein structural data. Drawing from recent advances in graph transformers, our approach refines the self-attention mechanisms of pretr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26631;&#37327;&#12289;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#22870;&#21169;&#20989;&#25968;&#26080;&#27861;&#34920;&#36798;&#22810;&#30446;&#26631;&#12289;&#39118;&#38505;&#25935;&#24863;&#21644;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#22823;&#37096;&#20998;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2401.14811</link><description>&lt;p&gt;
&#20851;&#20110;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#22312;&#22810;&#30446;&#26631;&#12289;&#39118;&#38505;&#25935;&#24863;&#21644;&#27169;&#24577;&#20219;&#21153;&#20013;&#34920;&#36798;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Limitations of Markovian Rewards to Express Multi-Objective, Risk-Sensitive, and Modal Tasks. (arXiv:2401.14811v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26631;&#37327;&#12289;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#22870;&#21169;&#20989;&#25968;&#26080;&#27861;&#34920;&#36798;&#22810;&#30446;&#26631;&#12289;&#39118;&#38505;&#25935;&#24863;&#21644;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#22823;&#37096;&#20998;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26631;&#37327;&#12289;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31867;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65306;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#12289;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#12290;&#23545;&#20110;&#27599;&#19968;&#31867;&#20219;&#21153;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#25551;&#36848;&#20160;&#20040;&#26679;&#30340;&#38382;&#39064;&#21487;&#20197;&#20351;&#29992;&#26631;&#37327;&#12289;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20989;&#25968;&#26469;&#34920;&#36798;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#19977;&#20010;&#31867;&#21035;&#20013;&#65292;&#26631;&#37327;&#12289;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20989;&#25968;&#26080;&#27861;&#34920;&#36798;&#22823;&#37096;&#20998;&#23454;&#20363;&#12290;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#20026;&#20102;&#35299;&#26631;&#20934;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#21644;&#19981;&#33021;&#34920;&#36798;&#30340;&#20869;&#23481;&#20570;&#20986;&#20102;&#26356;&#23436;&#25972;&#30340;&#36129;&#29486;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#27169;&#24577;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#31867;&#21035;&#24341;&#20837;&#65292;&#22240;&#20026;&#22312;&#24378;&#21270;&#23398;&#20064;&#25991;&#29486;&#20013;&#65292;&#30446;&#21069;&#23578;&#26410;&#23545;&#20854;&#36827;&#34892;&#31995;&#32479;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#31616;&#35201;&#27010;&#36848;&#20102;&#36890;&#36807;&#23450;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#25152;&#35752;&#35770;&#38382;&#39064;&#30340;&#19968;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the expressivity of scalar, Markovian reward functions in Reinforcement Learning (RL), and identify several limitations to what they can express. Specifically, we look at three classes of RL tasks; multi-objective RL, risk-sensitive RL, and modal RL. For each class, we derive necessary and sufficient conditions that describe when a problem in this class can be expressed using a scalar, Markovian reward. Moreover, we find that scalar, Markovian rewards are unable to express most of the instances in each of these three classes. We thereby contribute to a more complete understanding of what standard reward functions can and cannot express. In addition to this, we also call attention to modal problems as a new class of problems, since they have so far not been given any systematic treatment in the RL literature. We also briefly outline some approaches for solving some of the problems we discuss, by means of bespoke RL algorithms.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#24490;&#29615;&#32676;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21015;&#20030;&#21644;&#35780;&#20272;&#20934;&#24490;&#29615;&#30721;&#20013;&#30340;&#38519;&#38449;&#38598;&#65292;&#21033;&#29992;&#34920;&#26684;&#25216;&#26415;&#31616;&#21270;&#37325;&#35201;&#37319;&#26679;&#27493;&#39588;&#24182;&#21033;&#29992;&#25968;&#23398;&#26694;&#26550;&#38416;&#26126;&#25237;&#24433;&#21644;&#25260;&#21319;&#21464;&#25442;&#23545;&#25311;&#30721;&#23383;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2401.14810</link><description>&lt;p&gt;
&#24490;&#29615;&#32676;&#25237;&#24433;&#29992;&#20110;&#21015;&#20030;&#38519;&#38449;&#38598;&#20013;&#30340;&#20934;&#24490;&#29615;&#30721;
&lt;/p&gt;
&lt;p&gt;
Cyclic Group Projection for Enumerating Quasi-Cyclic Codes Trapping Sets. (arXiv:2401.14810v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14810
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#24490;&#29615;&#32676;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21015;&#20030;&#21644;&#35780;&#20272;&#20934;&#24490;&#29615;&#30721;&#20013;&#30340;&#38519;&#38449;&#38598;&#65292;&#21033;&#29992;&#34920;&#26684;&#25216;&#26415;&#31616;&#21270;&#37325;&#35201;&#37319;&#26679;&#27493;&#39588;&#24182;&#21033;&#29992;&#25968;&#23398;&#26694;&#26550;&#38416;&#26126;&#25237;&#24433;&#21644;&#25260;&#21319;&#21464;&#25442;&#23545;&#25311;&#30721;&#23383;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21015;&#20030;&#21644;&#35780;&#20272;&#20934;&#24490;&#29615;&#30721;&#20013;&#30340;&#38519;&#38449;&#38598;&#65292;&#36825;&#20123;&#30721;&#20855;&#26377;&#38750;&#32032;&#25968;&#30340;&#24490;&#29615;&#22823;&#23567;&#12290;&#21033;&#29992;&#20934;&#24490;&#29615;&#30721;&#30340;&#29305;&#24615;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#34920;&#26684;&#25216;&#26415;&#26469;&#31616;&#21270;&#37325;&#35201;&#37319;&#26679;&#27493;&#39588;&#65292;&#29992;&#20110;&#20272;&#35745;&#38519;&#38449;&#38598;&#30340;&#25311;&#30721;&#23383;&#26435;&#37325;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#25152;&#25552;&#20379;&#30340;&#23450;&#29702;&#20013;&#24314;&#31435;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#38416;&#26126;&#20102;&#25237;&#24433;&#21644;&#25260;&#21319;&#21464;&#25442;&#23545;&#25311;&#30721;&#23383;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to enumerate and assess Trapping sets in quasi-cyclic codes, those with circulant sizes that are non-prime numbers. Leveraging the quasi-cyclic properties, the method employs a tabular technique to streamline the importance sampling step for estimating the pseudo-codeword weight of Trapping sets. The presented methodology draws on the mathematical framework established in the provided theorem, which elucidates the behavior of projection and lifting transformations on pseudo-codewords
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#20449;&#24687;&#35770;&#38544;&#31169;&#28431;&#26007;&#27169;&#22411;&#24320;&#21457;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#65292;&#20855;&#26377;&#36866;&#24212;&#24615;&#21644;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.14792</link><description>&lt;p&gt;
&#28145;&#24230;&#21464;&#20998;&#38544;&#31169;&#28431;&#26007;&#65306;&#36890;&#29992;&#24314;&#27169;&#21450;&#22312;&#20154;&#33080;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Variational Privacy Funnel: General Modeling with Applications in Face Recognition. (arXiv:2401.14792v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#20449;&#24687;&#35770;&#38544;&#31169;&#28431;&#26007;&#27169;&#22411;&#24320;&#21457;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#65292;&#20855;&#26377;&#36866;&#24212;&#24615;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20449;&#24687;&#35770;&#38544;&#31169;&#28431;&#26007;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#20005;&#26684;&#32771;&#34385;&#20102;&#27169;&#31946;&#19982;&#25928;&#29992;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#36890;&#36807;&#23545;&#25968;&#25439;&#22833;&#37327;&#21270;&#20004;&#32773;&#65292;&#36825;&#20063;&#26159;&#19968;&#20010;&#34987;&#35270;&#20026;&#33258;&#20449;&#24687;&#25439;&#22833;&#30340;&#24230;&#37327;&#12290;&#36825;&#31181;&#25506;&#32034;&#28145;&#21270;&#20102;&#20449;&#24687;&#35770;&#38544;&#31169;&#19982;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20026;&#21028;&#21035;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#25454;&#20445;&#25252;&#26426;&#21046;&#25552;&#20379;&#20102;&#23454;&#36136;&#24615;&#30340;&#35265;&#35299;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#12290;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#36755;&#20837;&#19978;&#34920;&#29616;&#20986;&#20102;&#36866;&#24212;&#24615;&#65292;&#21253;&#25324;&#21407;&#22987;&#20154;&#33080;&#22270;&#20687;&#20197;&#21450;&#27966;&#29983;&#25110;&#32463;&#36807;&#20248;&#21270;&#30340;&#23884;&#20837;&#65292;&#33021;&#22815;&#32988;&#20219;&#20998;&#31867;&#12289;&#37325;&#26500;&#21644;&#29983;&#25104;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we harness the information-theoretic Privacy Funnel (PF) model to develop a method for privacy-preserving representation learning using an end-to-end training framework. We rigorously address the trade-off between obfuscation and utility. Both are quantified through the logarithmic loss, a measure also recognized as self-information loss. This exploration deepens the interplay between information-theoretic privacy and representation learning, offering substantive insights into data protection mechanisms for both discriminative and generative models. Importantly, we apply our model to state-of-the-art face recognition systems. The model demonstrates adaptability across diverse inputs, from raw facial images to both derived or refined embeddings, and is competent in tasks such as classification, reconstruction, and generation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31163;&#31574;&#30053;&#21407;&#21452;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20445;&#23432;&#31574;&#30053;&#20248;&#21270;&#21644;&#23616;&#37096;&#31574;&#30053;&#20984;&#21270;&#26469;&#35299;&#20915;&#32047;&#31215;&#25104;&#26412;&#20272;&#35745;&#35823;&#24046;&#23548;&#33268;&#30340;&#23433;&#20840;&#32422;&#26463;&#19981;&#28385;&#36275;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14758</link><description>&lt;p&gt;
Off-Policy Primal-Dual Safe Reinforcement Learning. (arXiv:2401.14758v1 [cs.LG]) &#35770;&#25991;&#30340;&#39064;&#30446;&#26159;&#65306;&#31163;&#31574;&#30053;&#21407;&#21452;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Primal-Dual Safe Reinforcement Learning. (arXiv:2401.14758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14758
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31163;&#31574;&#30053;&#21407;&#21452;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20445;&#23432;&#31574;&#30053;&#20248;&#21270;&#21644;&#23616;&#37096;&#31574;&#30053;&#20984;&#21270;&#26469;&#35299;&#20915;&#32047;&#31215;&#25104;&#26412;&#20272;&#35745;&#35823;&#24046;&#23548;&#33268;&#30340;&#23433;&#20840;&#32422;&#26463;&#19981;&#28385;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#21452;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#22312;&#31574;&#30053;&#30340;&#21407;&#22987;&#26356;&#26032;&#21644;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#30340;&#23545;&#20598;&#26356;&#26032;&#20043;&#38388;&#36827;&#34892;&#36845;&#20195;&#12290;&#30001;&#20110;&#32047;&#31215;&#25104;&#26412;&#20272;&#35745;&#20316;&#20026;&#36830;&#25509;&#21407;&#22987;&#21644;&#23545;&#20598;&#26356;&#26032;&#36807;&#31243;&#30340;&#20851;&#38190;&#32852;&#31995;&#65292;&#36825;&#31181;&#35757;&#32451;&#33539;&#24335;&#26497;&#26131;&#21463;&#21040;&#32047;&#31215;&#25104;&#26412;&#20272;&#35745;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20010;&#38382;&#39064;&#23548;&#33268;&#31163;&#31574;&#30053;&#26041;&#27861;&#20351;&#29992;&#26102;&#25104;&#26412;&#34987;&#20005;&#37325;&#20302;&#20272;&#65292;&#26080;&#27861;&#28385;&#36275;&#23433;&#20840;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20445;&#23432;&#31574;&#30053;&#20248;&#21270;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#25104;&#26412;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#32422;&#26463;&#28385;&#36275;&#30340;&#21306;&#22495;&#23398;&#20064;&#31574;&#30053;&#12290;&#36825;&#25552;&#39640;&#20102;&#32422;&#26463;&#30340;&#28385;&#36275;&#24615;&#65292;&#20294;&#20063;&#21487;&#33021;&#38459;&#30861;&#20102;&#22870;&#21169;&#26368;&#22823;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#23616;&#37096;&#31574;&#30053;&#20984;&#21270;&#8221;&#26469;&#21161;&#20110;&#28040;&#38500;&#36825;&#31181;&#27425;&#20248;&#24615;&#65292;&#36880;&#28176;&#20943;&#23567;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;&#36825;&#20004;&#20010;&#25104;&#20998;&#30340;&#32852;&#21512;&#20316;&#29992;&#36827;&#34892;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Primal-dual safe RL methods commonly perform iterations between the primal update of the policy and the dual update of the Lagrange Multiplier. Such a training paradigm is highly susceptible to the error in cumulative cost estimation since this estimation serves as the key bond connecting the primal and dual update processes. We show that this problem causes significant underestimation of cost when using off-policy methods, leading to the failure to satisfy the safety constraint. To address this issue, we propose \textit{conservative policy optimization}, which learns a policy in a constraint-satisfying area by considering the uncertainty in cost estimation. This improves constraint satisfaction but also potentially hinders reward maximization. We then introduce \textit{local policy convexification} to help eliminate such suboptimality by gradually reducing the estimation uncertainty. We provide theoretical interpretations of the joint coupling effect of these two ingredients and furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#19981;&#22343;&#21248;&#20998;&#24067;&#30340;&#30005;&#33655;&#21644;&#19981;&#35268;&#21017;&#32593;&#26684;&#19978;&#23454;&#29616;&#33021;&#37327;&#27169;&#22411;&#22343;&#34913;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;QC-LDPC&#30721;&#21644;&#29627;&#23572;&#20857;&#26364;&#26426;&#65292;&#23558;&#31995;&#32479;&#30340;&#32500;&#24230;&#25193;&#23637;&#65292;&#23558;&#30005;&#33655;&#26367;&#25442;&#20026;&#24490;&#29615;&#29289;&#36136;&#65292;&#24182;&#36890;&#36807;&#24490;&#29615;&#31227;&#20301;&#34920;&#31034;&#36317;&#31163;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#19981;&#35268;&#21017;&#32593;&#26684;&#36716;&#21270;&#20026;&#22343;&#21248;&#37197;&#32622;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#36824;&#35299;&#20915;&#20102;&#20195;&#30721;&#22312;&#22270;&#24418;&#27010;&#29575;&#27169;&#22411;&#20013;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#19981;&#21516;&#25299;&#25169;&#19979;&#29627;&#23572;&#20857;&#26364;&#26426;&#36798;&#21040;&#22343;&#34913;&#29366;&#24577;&#30340;&#20005;&#26684;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2401.14749</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#30340;&#33021;&#37327;&#27169;&#22411;&#22343;&#34913;&#25506;&#32034;&#65306;Toric QC-LDPC&#30721;&#21644;Hyperbolic MET QC-LDPC&#30721;
&lt;/p&gt;
&lt;p&gt;
Topology-Aware Exploration of Energy-Based Models Equilibrium: Toric QC-LDPC Codes and Hyperbolic MET QC-LDPC Codes. (arXiv:2401.14749v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#19981;&#22343;&#21248;&#20998;&#24067;&#30340;&#30005;&#33655;&#21644;&#19981;&#35268;&#21017;&#32593;&#26684;&#19978;&#23454;&#29616;&#33021;&#37327;&#27169;&#22411;&#22343;&#34913;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;QC-LDPC&#30721;&#21644;&#29627;&#23572;&#20857;&#26364;&#26426;&#65292;&#23558;&#31995;&#32479;&#30340;&#32500;&#24230;&#25193;&#23637;&#65292;&#23558;&#30005;&#33655;&#26367;&#25442;&#20026;&#24490;&#29615;&#29289;&#36136;&#65292;&#24182;&#36890;&#36807;&#24490;&#29615;&#31227;&#20301;&#34920;&#31034;&#36317;&#31163;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#19981;&#35268;&#21017;&#32593;&#26684;&#36716;&#21270;&#20026;&#22343;&#21248;&#37197;&#32622;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#36824;&#35299;&#20915;&#20102;&#20195;&#30721;&#22312;&#22270;&#24418;&#27010;&#29575;&#27169;&#22411;&#20013;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#19981;&#21516;&#25299;&#25169;&#19979;&#29627;&#23572;&#20857;&#26364;&#26426;&#36798;&#21040;&#22343;&#34913;&#29366;&#24577;&#30340;&#20005;&#26684;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#22343;&#21248;&#20998;&#24067;&#30340;&#30005;&#33655;&#21644;&#19981;&#35268;&#21017;&#32593;&#26684;&#19978;&#23454;&#29616;ISING&#21704;&#23494;&#39039;&#37327;&#22343;&#34913;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#65288;&#22810;&#36793;&#32536;&#65289;QC-LDPC&#30721;&#21644;&#29627;&#23572;&#20857;&#26364;&#26426;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#31995;&#32479;&#30340;&#32500;&#24230;&#25193;&#23637;&#65292;&#29992;&#24490;&#29615;&#29289;&#36136;&#26367;&#20195;&#30005;&#33655;&#65292;&#24182;&#36890;&#36807;&#24490;&#29615;&#31227;&#20301;&#34920;&#31034;&#36317;&#31163;&#12290;&#36825;&#23548;&#33268;&#30005;&#33655;&#31995;&#32479;&#22312;&#31354;&#38388;&#19978;&#30340;&#31995;&#32479;&#26144;&#23556;&#65292;&#23558;&#19981;&#35268;&#21017;&#32593;&#26684;&#36716;&#21270;&#20026;&#22343;&#21248;&#37197;&#32622;&#65292;&#36866;&#29992;&#20110;Torical&#21644;Circular Hyperboloid&#25299;&#25169;&#12290;&#26412;&#25991;&#28085;&#30422;&#20102;&#19982;QC-LDPC&#30721;&#12289;&#22810;&#36793;&#32536;QC-LDPC&#30721;&#21644;&#29627;&#23572;&#20857;&#26364;&#26426;&#30456;&#20851;&#30340;&#22522;&#26412;&#23450;&#20041;&#21644;&#31526;&#21495;&#12290;&#23427;&#25506;&#35752;&#20102;&#29992;&#20110;&#35780;&#20272;&#20998;&#21306;&#20989;&#25968;&#30340;&#20195;&#30721;&#22312;&#22270;&#24418;&#27010;&#29575;&#27169;&#22411;&#20013;&#30340;&#36793;&#38469;&#21270;&#38382;&#39064;&#65292;&#21253;&#25324;&#31934;&#30830;&#21644;&#36817;&#20284;&#20272;&#35745;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#22312;Torical&#21644;Circular Hyper&#25299;&#25169;&#19979;&#65292;&#29627;&#23572;&#20857;&#26364;&#26426;&#21487;&#20197;&#36798;&#21040;&#22343;&#34913;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method for achieving equilibrium in the ISING Hamiltonian when confronted with unevenly distributed charges on an irregular grid. Employing (Multi-Edge) QC-LDPC codes and the Boltzmann machine, our approach involves dimensionally expanding the system, substituting charges with circulants, and representing distances through circulant shifts. This results in a systematic mapping of the charge system onto a space, transforming the irregular grid into a uniform configuration, applicable to Torical and Circular Hyperboloid Topologies. The paper covers fundamental definitions and notations related to QC-LDPC Codes, Multi-Edge QC-LDPC codes, and the Boltzmann machine. It explores the marginalization problem in code on the graph probabilistic models for evaluating the partition function, encompassing exact and approximate estimation techniques. Rigorous proof is provided for the attainability of equilibrium states for the Boltzmann machine under Torical and Circular Hyper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;QINCo&#65292;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#27531;&#20313;&#37327;&#21270;&#21464;&#20307;&#65292;&#36890;&#36807;&#39044;&#27979;&#27599;&#20010;&#30690;&#37327;&#30340;&#19987;&#38376;&#30721;&#20070;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#30721;&#20070;&#22823;&#23567;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.14732</link><description>&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#30721;&#20070;&#30340;&#27531;&#20313;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Residual Quantization with Implicit Neural Codebooks. (arXiv:2401.14732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;QINCo&#65292;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#27531;&#20313;&#37327;&#21270;&#21464;&#20307;&#65292;&#36890;&#36807;&#39044;&#27979;&#27599;&#20010;&#30690;&#37327;&#30340;&#19987;&#38376;&#30721;&#20070;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#30721;&#20070;&#22823;&#23567;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30690;&#37327;&#37327;&#21270;&#26159;&#25968;&#25454;&#21387;&#32553;&#21644;&#30690;&#37327;&#25628;&#32034;&#30340;&#22522;&#26412;&#25805;&#20316;&#12290;&#20026;&#20102;&#33719;&#24471;&#39640;&#20934;&#30830;&#24615;&#65292;&#22810;&#30721;&#20070;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#30721;&#20070;&#20013;&#30340;&#30721;&#23383;&#26469;&#34920;&#31034;&#27599;&#20010;&#30690;&#37327;&#26469;&#22686;&#21152;&#36895;&#29575;&#12290;&#27531;&#20313;&#37327;&#21270;&#65288;RQ&#65289;&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#37327;&#21270;&#19978;&#19968;&#27493;&#30340;&#35823;&#24046;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#35823;&#24046;&#20998;&#24067;&#20381;&#36182;&#20110;&#20808;&#21069;&#36873;&#25321;&#30340;&#30721;&#23383;&#65292;&#22312;&#20256;&#32479;RQ&#20013;&#26410;&#23545;&#27492;&#36827;&#34892;&#32771;&#34385;&#65292;&#22240;&#20026;&#23427;&#22312;&#27599;&#20010;&#37327;&#21270;&#27493;&#39588;&#20013;&#20351;&#29992;&#36890;&#29992;&#30721;&#20070;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;QINCo&#65292;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#27531;&#20313;&#37327;&#21270;&#21464;&#20307;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#27599;&#20010;&#30690;&#37327;&#30340;&#19987;&#38376;&#30721;&#20070;&#65292;&#26465;&#20214;&#26159;&#20808;&#21069;&#27493;&#39588;&#30340;&#21521;&#37327;&#36817;&#20284;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#30721;&#20070;&#22823;&#23567;&#19978;&#65292;QINCo&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#24456;&#22810;&#12290;&#20363;&#22914;&#65292;QINCo&#20351;&#29992;12&#23383;&#33410;&#30340;&#30721;&#23383;&#22312;BigANN&#19978;&#27604;&#20351;&#29992;16&#23383;&#33410;&#30340;&#20854;&#20182;&#26041;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#26368;&#36817;&#37051;&#25628;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vector quantization is a fundamental operation for data compression and vector search. To obtain high accuracy, multi-codebook methods increase the rate by representing each vector using codewords across multiple codebooks. Residual quantization (RQ) is one such method, which increases accuracy by iteratively quantizing the error of the previous step. The error distribution is dependent on previously selected codewords. This dependency is, however, not accounted for in conventional RQ as it uses a generic codebook per quantization step. In this paper, we propose QINCo, a neural RQ variant which predicts specialized codebooks per vector using a neural network that is conditioned on the approximation of the vector from previous steps. Experiments show that QINCo outperforms state-of-the-art methods by a large margin on several datasets and code sizes. For example, QINCo achieves better nearest-neighbor search accuracy using 12 bytes codes than other methods using 16 bytes on the BigANN a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#22312;&#32447;&#27963;&#21160;&#30340;&#29992;&#25143;&#25968;&#37327;&#21644;&#36798;&#21040;&#25152;&#38656;&#29992;&#25143;&#21442;&#19982;&#38376;&#27099;&#25152;&#38656;&#30340;&#26102;&#38388;&#36712;&#36857;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25429;&#25417;&#29992;&#25143;&#21442;&#19982;&#30340;&#28508;&#22312;&#24322;&#36136;&#24615;&#65292;&#25552;&#20379;&#20102;&#23454;&#39564;&#32773;&#22312;&#22312;&#32447;&#23454;&#39564;&#20013;&#37325;&#35201;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2401.14722</link><description>&lt;p&gt;
&#19968;&#31181;&#38750;&#21442;&#25968;&#36125;&#21494;&#26031;&#26041;&#27861;&#29992;&#20110;&#22312;&#32447;&#27963;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Nonparametric Bayes Approach to Online Activity Prediction. (arXiv:2401.14722v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#22312;&#32447;&#27963;&#21160;&#30340;&#29992;&#25143;&#25968;&#37327;&#21644;&#36798;&#21040;&#25152;&#38656;&#29992;&#25143;&#21442;&#19982;&#38376;&#27099;&#25152;&#38656;&#30340;&#26102;&#38388;&#36712;&#36857;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25429;&#25417;&#29992;&#25143;&#21442;&#19982;&#30340;&#28508;&#22312;&#24322;&#36136;&#24615;&#65292;&#25552;&#20379;&#20102;&#23454;&#39564;&#32773;&#22312;&#22312;&#32447;&#23454;&#39564;&#20013;&#37325;&#35201;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20934;&#30830;&#39044;&#27979;&#29305;&#23450;&#27963;&#21160;&#30340;&#21457;&#29983;&#26102;&#38388;&#20869;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#32972;&#26223;&#12290;&#23545;&#20110;&#36816;&#34892;&#22312;&#32447;&#23454;&#39564;&#65288;A/B&#27979;&#35797;&#65289;&#30340;&#23454;&#39564;&#32773;&#26469;&#35828;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#23558;&#25509;&#21463;&#24178;&#39044;&#30340;&#29992;&#25143;&#25968;&#37327;&#26159;&#19968;&#39033;&#37325;&#35201;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#32473;&#23450;&#26102;&#38388;&#27573;&#20869;&#27963;&#21160;&#29992;&#25143;&#25968;&#37327;&#20197;&#21450;&#36798;&#21040;&#25152;&#38656;&#29992;&#25143;&#21442;&#19982;&#38376;&#27099;&#25152;&#38656;&#30340;&#26102;&#38388;&#36712;&#36857;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#26469;&#24314;&#27169;&#29992;&#25143;&#27963;&#21160;&#65292;&#20197;&#25429;&#25417;&#29992;&#25143;&#21442;&#19982;&#30340;&#28508;&#22312;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#22312;&#32473;&#23450;&#26102;&#38388;&#27573;&#20869;&#26399;&#26395;&#30340;&#26032;&#29992;&#25143;&#25968;&#37327;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#26469;&#20272;&#35745;&#36798;&#21040;&#25152;&#38656;&#29992;&#25143;&#25968;&#37327;&#25152;&#38656;&#30340;&#22825;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65307;&#21518;&#32773;&#23545;&#20110;&#23454;&#39564;&#35268;&#21010;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#29992;&#25143;&#27963;&#21160;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting the onset of specific activities within defined timeframes holds significant importance in several applied contexts. In particular, accurate prediction of the number of future users that will be exposed to an intervention is an important piece of information for experimenters running online experiments (A/B tests). In this work, we propose a novel approach to predict the number of users that will be active in a given time period, as well as the temporal trajectory needed to attain a desired user participation threshold. We model user activity using a Bayesian nonparametric approach which allows us to capture the underlying heterogeneity in user engagement. We derive closed-form expressions for the number of new users expected in a given period, and a simple Monte Carlo algorithm targeting the posterior distribution of the number of days needed to attain a desired number of users; the latter is important for experimental planning. We illustrate the performance of o
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22768;&#23398;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#26367;&#21644;&#22238;&#24212;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#36825;&#20004;&#31181;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21475;&#35821;&#23545;&#35805;&#20013;&#23454;&#29616;&#26356;&#21152;&#33258;&#28982;&#21644;&#23545;&#35805;&#24335;&#30340;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.14717</link><description>&lt;p&gt;
&#29992;&#22768;&#23398;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34701;&#21512;&#36827;&#34892;&#20132;&#26367;&#21644;&#22238;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion. (arXiv:2401.14717v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14717
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22768;&#23398;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#26367;&#21644;&#22238;&#24212;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#36825;&#20004;&#31181;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21475;&#35821;&#23545;&#35805;&#20013;&#23454;&#29616;&#26356;&#21152;&#33258;&#28982;&#21644;&#23545;&#35805;&#24335;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#39044;&#27979;&#21475;&#35821;&#23545;&#35805;&#20013;&#20132;&#26367;&#21644;&#22238;&#24212;&#20301;&#32622;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#31070;&#32463;&#22768;&#23398;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#22312;Switchboard&#20154;&#38469;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#21333;&#27169;&#24577;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#31574;&#30053;&#65292;&#20197;&#36827;&#19968;&#27493;&#20174;LLM&#32534;&#30721;&#30340;&#30693;&#35782;&#20013;&#21463;&#30410;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20351;&#29992;&#32452;&#21512;&#30340;LLM&#21644;&#22768;&#23398;&#27169;&#22411;&#22312;&#20154;&#31867;&#21644;&#35821;&#38899;AI&#20195;&#29702;&#20043;&#38388;&#23454;&#29616;&#26356;&#33258;&#28982;&#21644;&#23545;&#35805;&#24335;&#20114;&#21160;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach for continuous prediction of turn-taking and backchanneling locations in spoken dialogue by fusing a neural acoustic model with a large language model (LLM). Experiments on the Switchboard human-human conversation dataset demonstrate that our approach consistently outperforms the baseline models with single modality. We also develop a novel multi-task instruction fine-tuning strategy to further benefit from LLM-encoded knowledge for understanding the tasks and conversational contexts, leading to additional improvements. Our approach demonstrates the potential of combined LLMs and acoustic models for a more natural and conversational interaction between humans and speech-enabled AI agents.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#35299;&#32544;&#26469;&#32531;&#35299;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#29305;&#24449;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#24314;&#27169;&#21644;&#28040;&#38500;&#23548;&#33268;&#29305;&#24449;&#24046;&#36317;&#30340;&#28508;&#22312;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14707</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#35299;&#32544;&#26469;&#32531;&#35299;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#30340;&#29305;&#24449;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Mitigating Feature Gap for Adversarial Robustness by Feature Disentanglement. (arXiv:2401.14707v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14707
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#35299;&#32544;&#26469;&#32531;&#35299;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#29305;&#24449;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#24314;&#27169;&#21644;&#28040;&#38500;&#23548;&#33268;&#29305;&#24449;&#24046;&#36317;&#30340;&#28508;&#22312;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#26679;&#26412;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23545;&#25239;&#24494;&#35843;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#23545;&#24050;&#32463;&#22312;&#33258;&#28982;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24335;&#24494;&#35843;&#26469;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#25239;&#26679;&#26412;&#20013;&#30340;&#19968;&#20123;&#28508;&#22312;&#29305;&#24449;&#34987;&#23545;&#25239;&#25200;&#21160;&#25152;&#28151;&#28102;&#65292;&#24182;&#23548;&#33268;&#33258;&#28982;&#26679;&#26412;&#21644;&#23545;&#25239;&#26679;&#26412;&#22312;&#26368;&#21518;&#19968;&#23618;&#38544;&#34255;&#23618;&#30340;&#29305;&#24449;&#20043;&#38388;&#20986;&#29616;&#24847;&#22806;&#22686;&#21152;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#32544;&#30340;&#26041;&#27861;&#26469;&#26126;&#30830;&#24314;&#27169;&#21644;&#36827;&#19968;&#27493;&#28040;&#38500;&#23548;&#33268;&#29305;&#24449;&#24046;&#36317;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#24449;&#35299;&#32544;&#22120;&#65292;&#23558;&#23545;&#25239;&#26679;&#26412;&#30340;&#28508;&#22312;&#29305;&#24449;&#19982;&#23545;&#25239;&#26679;&#26412;&#30340;&#29305;&#24449;&#20998;&#31163;&#24320;&#26469;&#65292;&#20174;&#32780;&#36890;&#36807;&#28040;&#38500;&#28508;&#22312;&#29305;&#24449;&#26469;&#25552;&#21319;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#19982;&#23545;&#25239;&#26679;&#26412;&#22312;&#24494;&#35843;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#23545;&#40784;&#65292;&#36827;&#19968;&#27493;&#20174;&#33258;&#28982;&#26679;&#26412;&#30340;&#29305;&#24449;&#20013;&#33719;&#30410;&#65292;&#36991;&#20813;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are vulnerable to adversarial samples. Adversarial fine-tuning methods aim to enhance adversarial robustness through fine-tuning the naturally pre-trained model in an adversarial training manner. However, we identify that some latent features of adversarial samples are confused by adversarial perturbation and lead to an unexpectedly increasing gap between features in the last hidden layer of natural and adversarial samples. To address this issue, we propose a disentanglement-based approach to explicitly model and further remove the latent features that cause the feature gap. Specifically, we introduce a feature disentangler to separate out the latent features from the features of the adversarial samples, thereby boosting robustness by eliminating the latent features. Besides, we align features in the pre-trained model with features of adversarial samples in the fine-tuned model, to further benefit from the features from natural samples without confusion. Empirical 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FairSample&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#35757;&#32451;&#20844;&#24179;&#20934;&#30830;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23545;&#22270;&#32467;&#26500;&#36827;&#34892;&#32416;&#27491;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#37051;&#23621;&#37319;&#26679;&#31574;&#30053;&#26469;&#21516;&#26102;&#20943;&#36731;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22270;&#32467;&#26500;&#20559;&#35265;&#12289;&#33410;&#28857;&#23646;&#24615;&#20559;&#35265;&#21644;&#27169;&#22411;&#21442;&#25968;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.14702</link><description>&lt;p&gt;
FairSample: &#39640;&#25928;&#35757;&#32451;&#20844;&#24179;&#20934;&#30830;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FairSample: Training Fair and Accurate Graph Convolutional Neural Networks Efficiently. (arXiv:2401.14702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FairSample&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#35757;&#32451;&#20844;&#24179;&#20934;&#30830;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23545;&#22270;&#32467;&#26500;&#36827;&#34892;&#32416;&#27491;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#37051;&#23621;&#37319;&#26679;&#31574;&#30053;&#26469;&#21516;&#26102;&#20943;&#36731;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22270;&#32467;&#26500;&#20559;&#35265;&#12289;&#33410;&#28857;&#23646;&#24615;&#20559;&#35265;&#21644;&#27169;&#22411;&#21442;&#25968;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#65292;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20844;&#24179;&#24615;&#36234;&#26469;&#36234;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#23384;&#22312;&#23545;&#25935;&#24863;&#32676;&#20307;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20844;&#24179;&#24615;&#30340;&#32463;&#20856;&#27010;&#24565;&#8220;&#20154;&#21475;&#32479;&#35745;&#24179;&#22343;&#20540;&#8221;&#65292;&#24182;&#35299;&#20915;&#20102;&#39640;&#25928;&#35757;&#32451;&#20844;&#24179;&#20934;&#30830;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#22270;&#32467;&#26500;&#20559;&#35265;&#12289;&#33410;&#28857;&#23646;&#24615;&#20559;&#35265;&#21644;&#27169;&#22411;&#21442;&#25968;&#23545;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#21475;&#32479;&#35745;&#24179;&#22343;&#24615;&#33021;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#27934;&#23519;&#21147;&#23548;&#33268;&#20102;FairSample&#65292;&#19968;&#20010;&#21487;&#20197;&#21516;&#26102;&#20943;&#36731;&#36825;&#19977;&#31181;&#20559;&#35265;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#30452;&#35266;&#30340;&#31574;&#30053;&#26469;&#32416;&#27491;&#22270;&#32467;&#26500;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#25935;&#24863;&#32676;&#20307;&#20294;&#20855;&#26377;&#30456;&#20284;&#33410;&#28857;&#29305;&#24449;&#30340;&#33410;&#28857;&#20043;&#38388;&#25554;&#20837;&#36793;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#37051;&#23621;&#37319;&#26679;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness in Graph Convolutional Neural Networks (GCNs) becomes a more and more important concern as GCNs are adopted in many crucial applications. Societal biases against sensitive groups may exist in many real world graphs. GCNs trained on those graphs may be vulnerable to being affected by such biases. In this paper, we adopt the well-known fairness notion of demographic parity and tackle the challenge of training fair and accurate GCNs efficiently. We present an in-depth analysis on how graph structure bias, node attribute bias, and model parameters may affect the demographic parity of GCNs. Our insights lead to FairSample, a framework that jointly mitigates the three types of biases. We employ two intuitive strategies to rectify graph structures. First, we inject edges across nodes that are in different sensitive groups but similar in node features. Second, to enhance model fairness and retain model quality, we develop a learnable neighbor sampling policy using reinforcement learni
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#28176;&#36827;&#20013;&#28857;&#28151;&#21512;&#26041;&#27861;&#26469;&#35299;&#20915;&#31895;&#32454;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#31867;&#20869;&#23849;&#22604;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36880;&#28176;&#23558;&#22686;&#24378;&#29305;&#24449;&#31227;&#21160;&#33267;&#31867;&#38388;&#29305;&#24449;&#23545;&#30340;&#20013;&#28857;&#65292;&#23454;&#29616;&#20102;&#36793;&#36317;&#24179;&#34913;&#20197;&#21450;&#36866;&#24230;&#25193;&#23637;&#36793;&#36317;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.14696</link><description>&lt;p&gt;
&#28176;&#36827;&#20013;&#28857;&#28151;&#21512;&#29992;&#20110;&#36793;&#36317;&#24179;&#34913;&#21644;&#36866;&#24230;&#25193;&#23637;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Asymptotic Midpoint Mixup for Margin Balancing and Moderate Broadening. (arXiv:2401.14696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14696
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#28176;&#36827;&#20013;&#28857;&#28151;&#21512;&#26041;&#27861;&#26469;&#35299;&#20915;&#31895;&#32454;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#31867;&#20869;&#23849;&#22604;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36880;&#28176;&#23558;&#22686;&#24378;&#29305;&#24449;&#31227;&#21160;&#33267;&#31867;&#38388;&#29305;&#24449;&#23545;&#30340;&#20013;&#28857;&#65292;&#23454;&#29616;&#20102;&#36793;&#36317;&#24179;&#34913;&#20197;&#21450;&#36866;&#24230;&#25193;&#23637;&#36793;&#36317;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#29305;&#24449;&#20043;&#38388;&#30340;&#23849;&#22604;&#23548;&#33268;&#20102;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#65292;&#20351;&#24471;&#29305;&#24449;&#26080;&#27861;&#21306;&#20998;&#12290;&#22522;&#20110;&#25554;&#20540;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#22914;mixup&#65292;&#22312;&#20943;&#36731;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#23849;&#22604;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#36825;&#34987;&#31216;&#20026;&#31867;&#38388;&#23849;&#22604;&#12290;&#28982;&#32780;&#65292;&#31895;&#32454;&#36716;&#31227;&#23398;&#20064;&#20013;&#24341;&#36215;&#30340;&#31867;&#20869;&#23849;&#22604;&#24182;&#26410;&#22312;&#22686;&#24378;&#26041;&#27861;&#20013;&#35752;&#35770;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;&#28176;&#36827;&#20013;&#28857;&#28151;&#21512;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25554;&#20540;&#29983;&#25104;&#22686;&#24378;&#29305;&#24449;&#65292;&#20294;&#23558;&#23427;&#20204;&#36880;&#28176;&#31227;&#21160;&#21040;&#31867;&#38388;&#29305;&#24449;&#23545;&#30340;&#20013;&#28857;&#12290;&#32467;&#26524;&#26159;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#20004;&#20010;&#25928;&#26524;&#65306;1&#65289;&#24179;&#34913;&#25152;&#26377;&#31867;&#21035;&#30340;&#36793;&#36317;&#65292;2&#65289;&#20165;&#36866;&#24230;&#25193;&#23637;&#36793;&#36317;&#65292;&#30452;&#21040;&#36798;&#21040;&#26368;&#22823;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#21487;&#35270;&#21270;&#34920;&#31034;&#30340;&#23545;&#40784;&#24615;&#21644;&#22343;&#21248;&#24615;&#26469;&#32463;&#39564;&#20998;&#26512;&#20102;&#23849;&#22604;&#25928;&#24212;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#22686;&#24378;&#26041;&#27861;&#25152;&#24102;&#26469;&#30340;&#31867;&#20869;&#23849;&#22604;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the feature space, the collapse between features invokes critical problems in representation learning by remaining the features undistinguished. Interpolation-based augmentation methods such as mixup have shown their effectiveness in relieving the collapse problem between different classes, called inter-class collapse. However, intra-class collapse raised in coarse-to-fine transfer learning has not been discussed in the augmentation approach. To address them, we propose a better feature augmentation method, asymptotic midpoint mixup. The method generates augmented features by interpolation but gradually moves them toward the midpoint of inter-class feature pairs. As a result, the method induces two effects: 1) balancing the margin for all classes and 2) only moderately broadening the margin until it holds maximal confidence. We empirically analyze the collapse effects by measuring alignment and uniformity with visualizing representations. Then, we validate the intra-class collapse e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#25345;&#32493;&#28436;&#21270;&#22270;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;CEGNCDE&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#25429;&#25417;&#36830;&#32493;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#21644;&#38543;&#26102;&#38388;&#28436;&#21270;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20132;&#36890;&#39044;&#27979;&#20219;&#21153;&#19978;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14695</link><description>&lt;p&gt;
&#25345;&#32493;&#28436;&#21270;&#30340;&#22270;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Continuously Evolving Graph Neural Controlled Differential Equations for Traffic Forecasting. (arXiv:2401.14695v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#25345;&#32493;&#28436;&#21270;&#22270;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;CEGNCDE&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#25429;&#25417;&#36830;&#32493;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#21644;&#38543;&#26102;&#38388;&#28436;&#21270;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20132;&#36890;&#39044;&#27979;&#20219;&#21153;&#19978;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#21457;&#23637;&#26234;&#33021;&#22478;&#24066;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20132;&#36890;&#39044;&#27979;&#24050;&#25104;&#20026;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#30740;&#31350;&#28966;&#28857;&#12290;&#30001;&#20110;&#20132;&#36890;&#32593;&#32476;&#20013;&#23384;&#22312;&#22797;&#26434;&#19988;&#21160;&#24577;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#24456;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30740;&#31350;&#24573;&#35270;&#20102;&#38543;&#26102;&#38388;&#28436;&#21270;&#30340;&#36830;&#32493;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25345;&#32493;&#28436;&#21270;&#22270;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;CEGNCDE&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#36830;&#32493;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#21644;&#38543;&#26102;&#38388;&#28436;&#21270;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;NCDE&#30340;&#25345;&#32493;&#28436;&#21270;&#22270;&#29983;&#25104;&#22120;&#65288;CEGG&#65289;&#65292;&#29992;&#20110;&#20174;&#31163;&#25955;&#30340;&#21382;&#21490;&#35266;&#27979;&#20013;&#29983;&#25104;&#38543;&#26102;&#38388;&#28436;&#21270;&#30340;&#31354;&#38388;&#20381;&#36182;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#22270;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;GNCDE&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#36830;&#32493;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#21644;&#38543;&#26102;&#38388;&#28436;&#21270;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CEGNCDE&#26041;&#27861;&#22312;&#20132;&#36890;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a crucial technique for developing a smart city, traffic forecasting has become a popular research focus in academic and industrial communities for decades. This task is highly challenging due to complex and dynamic spatial-temporal dependencies in traffic networks. Existing works ignore continuous temporal dependencies and spatial dependencies evolving over time. In this paper, we propose Continuously Evolving Graph Neural Controlled Differential Equations (CEGNCDE) to capture continuous temporal dependencies and spatial dependencies over time simultaneously. Specifically, a continuously evolving graph generator (CEGG) based on NCDE is introduced to generate the spatial dependencies graph that continuously evolves over time from discrete historical observations. Then, a graph neural controlled differential equations (GNCDE) framework is introduced to capture continuous temporal dependencies and spatial dependencies over time simultaneously. Extensive experiments demonstrate that CE
&lt;/p&gt;</description></item><item><title>TA-RNN&#21644;TA-RNN-AE&#26159;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20998;&#26512;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24182;&#39044;&#27979;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#36825;&#20123;&#26550;&#26500;&#32771;&#34385;&#20102;EHR&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;&#26102;&#38388;&#38388;&#38548;&#65292;&#24182;&#37319;&#29992;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14694</link><description>&lt;p&gt;
TA-RNN&#65306;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38754;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26102;&#38388;&#24863;&#30693;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records. (arXiv:2401.14694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14694
&lt;/p&gt;
&lt;p&gt;
TA-RNN&#21644;TA-RNN-AE&#26159;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20998;&#26512;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24182;&#39044;&#27979;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#36825;&#20123;&#26550;&#26500;&#32771;&#34385;&#20102;EHR&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;&#26102;&#38388;&#38388;&#38548;&#65292;&#24182;&#37319;&#29992;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#65306;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#24739;&#32773;&#21307;&#30103;&#21382;&#21490;&#30340;&#20840;&#38754;&#36164;&#28304;&#12290;EHR&#23545;&#20110;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31561;&#20808;&#36827;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#21307;&#30103;&#25552;&#20379;&#32773;&#33021;&#22815;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#65292;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#24182;&#20570;&#20986;&#31934;&#30830;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#20020;&#24202;&#20915;&#31574;&#12290;DL&#26041;&#27861;&#22914;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#24050;&#34987;&#29992;&#20110;&#20998;&#26512;EHR&#20197;&#24314;&#27169;&#30142;&#30149;&#36827;&#23637;&#24182;&#39044;&#27979;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#35299;&#20915;EHR&#25968;&#25454;&#20013;&#19968;&#20123;&#22266;&#26377;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#22914;&#20020;&#24202;&#35775;&#38382;&#20043;&#38388;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;DL&#27169;&#22411;&#37117;&#19981;&#21487;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;DL&#26550;&#26500;&#65292;&#20998;&#21035;&#26159;&#26102;&#38388;&#24863;&#30693;RNN&#65288;TA-RNN&#65289;&#21644;TA-RNN-Autoencoder&#65288;TA-RNN-AE&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#27425;&#35775;&#38382;&#21644;&#22810;&#27425;&#26410;&#26469;&#35775;&#38382;&#20013;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#23558;&#26102;&#38388;&#20449;&#24687;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: Electronic Health Records (EHR) represent a comprehensive resource of a patient's medical history. EHR are essential for utilizing advanced technologies such as deep learning (DL), enabling healthcare providers to analyze extensive data, extract valuable insights, and make precise and data-driven clinical decisions. DL methods such as Recurrent Neural Networks (RNN) have been utilized to analyze EHR to model disease progression and predict diagnosis. However, these methods do not address some inherent irregularities in EHR data such as irregular time intervals between clinical visits. Furthermore, most DL models are not interpretable. In this study, we propose two interpretable DL architectures based on RNN, namely Time-Aware RNN (TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patient's clinical outcome in EHR at next visit and multiple visits ahead, respectively. To mitigate the impact of irregular time intervals, we propose incorporating time embedding of the elaps
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36229;&#20998;&#36776;&#29575;&#21644;&#32463;&#36807;&#35843;&#25972;&#30340;&#36731;&#37327;&#32423;YOLOv5&#26550;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#33322;&#31354;&#24433;&#20687;&#20013;&#23567;&#32780;&#23494;&#38598;&#29289;&#20307;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#36229;&#20998;&#36776;&#29575;YOLOv5&#27169;&#22411;&#37319;&#29992;Transformer&#32534;&#30721;&#22120;&#22359;&#65292;&#33021;&#22815;&#25429;&#25417;&#20840;&#23616;&#32972;&#26223;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#39640;&#23494;&#24230;&#12289;&#36974;&#25377;&#26465;&#20214;&#19979;&#25552;&#39640;&#26816;&#27979;&#32467;&#26524;&#12290;&#36825;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;&#19981;&#20165;&#20934;&#30830;&#24615;&#26356;&#39640;&#65292;&#32780;&#19988;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#39640;&#65292;&#38750;&#24120;&#36866;&#21512;&#23454;&#26102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.14661</link><description>&lt;p&gt;
&#20174;&#27169;&#31946;&#21040;&#26126;&#20142;&#30340;&#26816;&#27979;&#65306;&#22522;&#20110;YOLOv5&#30340;&#36229;&#20998;&#36776;&#29575;&#33322;&#31354;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
From Blurry to Brilliant Detection: YOLOv5-Based Aerial Object Detection with Super Resolution. (arXiv:2401.14661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14661
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36229;&#20998;&#36776;&#29575;&#21644;&#32463;&#36807;&#35843;&#25972;&#30340;&#36731;&#37327;&#32423;YOLOv5&#26550;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#33322;&#31354;&#24433;&#20687;&#20013;&#23567;&#32780;&#23494;&#38598;&#29289;&#20307;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#36229;&#20998;&#36776;&#29575;YOLOv5&#27169;&#22411;&#37319;&#29992;Transformer&#32534;&#30721;&#22120;&#22359;&#65292;&#33021;&#22815;&#25429;&#25417;&#20840;&#23616;&#32972;&#26223;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#39640;&#23494;&#24230;&#12289;&#36974;&#25377;&#26465;&#20214;&#19979;&#25552;&#39640;&#26816;&#27979;&#32467;&#26524;&#12290;&#36825;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;&#19981;&#20165;&#20934;&#30830;&#24615;&#26356;&#39640;&#65292;&#32780;&#19988;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#39640;&#65292;&#38750;&#24120;&#36866;&#21512;&#23454;&#26102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26080;&#20154;&#26426;&#21644;&#21355;&#26143;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#23545;&#33322;&#31354;&#24433;&#20687;&#20013;&#20934;&#30830;&#29289;&#20307;&#26816;&#27979;&#30340;&#38656;&#27714;&#22823;&#22823;&#22686;&#21152;&#12290;&#20256;&#32479;&#30340;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#22312;&#20559;&#21521;&#22823;&#29289;&#20307;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#23545;&#20110;&#33322;&#31354;&#22330;&#26223;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#23567;&#32780;&#23494;&#38598;&#30340;&#29289;&#20307;&#38590;&#20197;&#21457;&#25381;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36229;&#20998;&#36776;&#29575;&#21644;&#32463;&#36807;&#35843;&#25972;&#30340;&#36731;&#37327;&#32423;YOLOv5&#26550;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;VisDrone-2023&#12289;SeaDroneSee&#12289;VEDAI&#21644;NWPU VHR-10&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#36229;&#20998;&#36776;&#29575;YOLOv5&#26550;&#26500;&#37319;&#29992;Transformer&#32534;&#30721;&#22120;&#22359;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21040;&#20840;&#23616;&#32972;&#26223;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#27979;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#23494;&#24230;&#12289;&#36974;&#25377;&#26465;&#20214;&#19979;&#12290;&#36825;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;&#19981;&#20165;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36824;&#30830;&#20445;&#20102;&#36164;&#28304;&#30340;&#26377;&#25928;&#21033;&#29992;&#65292;&#38750;&#24120;&#36866;&#21512;&#23454;&#26102;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#33322;&#31354;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demand for accurate object detection in aerial imagery has surged with the widespread use of drones and satellite technology. Traditional object detection models, trained on datasets biased towards large objects, struggle to perform optimally in aerial scenarios where small, densely clustered objects are prevalent. To address this challenge, we present an innovative approach that combines super-resolution and an adapted lightweight YOLOv5 architecture. We employ a range of datasets, including VisDrone-2023, SeaDroneSee, VEDAI, and NWPU VHR-10, to evaluate our model's performance. Our Super Resolved YOLOv5 architecture features Transformer encoder blocks, allowing the model to capture global context and context information, leading to improved detection results, especially in high-density, occluded conditions. This lightweight model not only delivers improved accuracy but also ensures efficient resource utilization, making it well-suited for real-time applications. Our experimental 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#29699;&#38754;&#21367;&#31215;Wasserstein&#36317;&#31163;&#26469;&#39564;&#35777;&#27668;&#20505;&#27169;&#22411;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20840;&#38754;&#22320;&#34913;&#37327;&#27668;&#20505;&#27169;&#22411;&#21644;&#20877;&#20998;&#26512;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#24212;&#29992;&#20110;&#35780;&#20272;CMIP&#25104;&#21592;&#30340;&#27169;&#22411;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#21457;&#29616;CMIP&#31532;6&#38454;&#27573;&#27169;&#22411;&#30456;&#36739;&#20110;&#31532;5&#38454;&#27573;&#26377;&#36866;&#24230;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.14657</link><description>&lt;p&gt;
&#29992;&#29699;&#38754;&#21367;&#31215;Wasserstein&#36317;&#31163;&#39564;&#35777;&#27668;&#20505;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Validating Climate Models with Spherical Convolutional Wasserstein Distance. (arXiv:2401.14657v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14657
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#29699;&#38754;&#21367;&#31215;Wasserstein&#36317;&#31163;&#26469;&#39564;&#35777;&#27668;&#20505;&#27169;&#22411;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20840;&#38754;&#22320;&#34913;&#37327;&#27668;&#20505;&#27169;&#22411;&#21644;&#20877;&#20998;&#26512;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#24212;&#29992;&#20110;&#35780;&#20272;CMIP&#25104;&#21592;&#30340;&#27169;&#22411;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#21457;&#29616;CMIP&#31532;6&#38454;&#27573;&#27169;&#22411;&#30456;&#36739;&#20110;&#31532;5&#38454;&#27573;&#26377;&#36866;&#24230;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39564;&#35777;&#20840;&#29699;&#27668;&#20505;&#27169;&#22411;&#23545;&#20110;&#30830;&#20445;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#29699;&#38754;&#21367;&#31215;Wasserstein&#36317;&#31163;&#26469;&#26356;&#20840;&#38754;&#22320;&#34913;&#37327;&#27668;&#20505;&#27169;&#22411;&#21644;&#20877;&#20998;&#26512;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36825;&#20010;&#26032;&#30340;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#21033;&#29992;&#21367;&#31215;&#25237;&#24433;&#32771;&#34385;&#20102;&#31354;&#38388;&#21464;&#24322;&#24615;&#65292;&#24182;&#37327;&#21270;&#20102;&#27668;&#20505;&#21464;&#37327;&#20998;&#24067;&#30340;&#23616;&#37096;&#24046;&#24322;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#35780;&#20272;&#32806;&#21512;&#27169;&#24335;&#27604;&#36739;&#39033;&#30446;&#65288;CMIP&#65289;&#25104;&#21592;&#30340;&#21382;&#21490;&#27169;&#22411;&#36755;&#20986;&#65292;&#23558;&#20854;&#19982;&#35266;&#27979;&#25968;&#25454;&#21644;&#20877;&#20998;&#26512;&#25968;&#25454;&#20135;&#21697;&#36827;&#34892;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;CMIP&#31532;5&#38454;&#27573;&#21040;&#31532;6&#38454;&#27573;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#31532;6&#38454;&#27573;&#27169;&#22411;&#22312;&#29983;&#25104;&#30495;&#23454;&#27668;&#20505;&#23398;&#33021;&#21147;&#26041;&#38754;&#26377;&#36866;&#24230;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The validation of global climate models is crucial to ensure the accuracy and efficacy of model output. We introduce the spherical convolutional Wasserstein distance to more comprehensively measure differences between climate models and reanalysis data. This new similarity measure accounts for spatial variability using convolutional projections and quantifies local differences in the distribution of climate variables. We apply this method to evaluate the historical model outputs of the Coupled Model Intercomparison Project (CMIP) members by comparing them to observational and reanalysis data products. Additionally, we investigate the progression from CMIP phase 5 to phase 6 and find modest improvements in the phase 6 models regarding their ability to produce realistic climatologies.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#20445;&#38505;&#20105;&#35758;&#30340;&#38889;&#22269;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21477;&#23376;&#36716;&#25442;&#22120;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#19982;&#22823;&#22411;&#25968;&#25454;&#38598;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14654</link><description>&lt;p&gt;
&#38754;&#21521;&#20445;&#38505;&#20105;&#35758;&#30340;&#38889;&#22269;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Korean Legal Judgment Prediction Dataset for Insurance Disputes. (arXiv:2401.14654v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14654
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#20445;&#38505;&#20105;&#35758;&#30340;&#38889;&#22269;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21477;&#23376;&#36716;&#25442;&#22120;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#19982;&#22823;&#22411;&#25968;&#25454;&#38598;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#20445;&#38505;&#20105;&#35758;&#30340;&#38889;&#22269;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#25968;&#25454;&#38598;&#12290;&#25104;&#21151;&#39044;&#27979;&#20445;&#38505;&#20105;&#35758;&#30340;LJP&#27169;&#22411;&#21487;&#20197;&#20351;&#20445;&#38505;&#20844;&#21496;&#21450;&#20854;&#23458;&#25143;&#21463;&#30410;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#22914;&#26524;&#36827;&#34892;&#20105;&#35758;&#35843;&#35299;&#36807;&#31243;&#65292;&#32467;&#26524;&#23558;&#22914;&#20309;&#20986;&#29616;&#26469;&#33410;&#30465;&#21452;&#26041;&#30340;&#26102;&#38388;&#21644;&#37329;&#38065;&#12290;&#27491;&#22914;&#20302;&#36164;&#28304;&#35821;&#35328;&#32463;&#24120;&#38754;&#20020;&#30340;&#24773;&#20917;&#19968;&#26679;&#65292;&#35813;&#29305;&#23450;&#20219;&#21153;&#30340;&#21487;&#29992;&#25968;&#25454;&#37327;&#26377;&#38480;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21477;&#23376;&#36716;&#25442;&#22120;&#24494;&#35843;&#65288;SetFit&#65289;&#65288;Tunstall&#31561;&#65292;2022&#65289;&#26159;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#26631;&#20934;&#24494;&#35843;&#30340;&#33391;&#22909;&#26367;&#20195;&#26041;&#27861;&#12290;&#20351;&#29992;SetFit&#26041;&#27861;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#19982;&#38889;&#22269;LJP&#22522;&#20934;&#27169;&#22411;&#65288;Hwang&#31561;&#65292;2022&#65289;&#22312;&#24615;&#33021;&#19978;&#26174;&#31034;&#20986;&#30456;&#20284;&#30340;&#34920;&#29616;&#65292;&#23613;&#31649;&#25968;&#25454;&#35268;&#27169;&#35201;&#23567;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a Korean legal judgment prediction (LJP) dataset for insurance disputes. Successful LJP models on insurance disputes can benefit insurance companies and their customers. It can save both sides' time and money by allowing them to predict how the result would come out if they proceed to the dispute mediation process. As is often the case with low-resource languages, there is a limitation on the amount of data available for this specific task. To mitigate this issue, we investigate how one can achieve a good performance despite the limitation in data. In our experiment, we demonstrate that Sentence Transformer Fine-tuning (SetFit, Tunstall et al., 2022) is a good alternative to standard fine-tuning when training data are limited. The models fine-tuned with the SetFit approach on our data show similar performance to the Korean LJP benchmark models (Hwang et al., 2022) despite the much smaller data size.
&lt;/p&gt;</description></item><item><title>&#20840;&#33021;&#39044;&#27979;&#22120;&#26159;&#19968;&#31181;&#39044;&#27979;&#22120;&#65292;&#20854;&#39044;&#27979;&#30340;&#25439;&#22833;&#23567;&#20110;&#20219;&#24847;&#25439;&#22833;&#19979;&#30340;&#26368;&#20339;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#29992;&#26469;&#26368;&#23567;&#21270;&#20219;&#20309;&#25439;&#22833;&#30340;&#26399;&#26395;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2401.14645</link><description>&lt;p&gt;
&#22238;&#24402;&#38382;&#39064;&#30340;&#20840;&#33021;&#39044;&#27979;&#22120;&#19982;&#20984;&#20989;&#25968;&#30340;&#36817;&#20284;&#31209;
&lt;/p&gt;
&lt;p&gt;
Omnipredictors for Regression and the Approximate Rank of Convex Functions. (arXiv:2401.14645v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14645
&lt;/p&gt;
&lt;p&gt;
&#20840;&#33021;&#39044;&#27979;&#22120;&#26159;&#19968;&#31181;&#39044;&#27979;&#22120;&#65292;&#20854;&#39044;&#27979;&#30340;&#25439;&#22833;&#23567;&#20110;&#20219;&#24847;&#25439;&#22833;&#19979;&#30340;&#26368;&#20339;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#29992;&#26469;&#26368;&#23567;&#21270;&#20219;&#20309;&#25439;&#22833;&#30340;&#26399;&#26395;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#22312;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#36890;&#36807;&#32473;&#23450;&#20998;&#24067;&#20013;&#30340;&#28857;&#119857;&#26469;&#39044;&#27979;&#26631;&#31614;&#119858;&#12290;&#23545;&#20110;&#25439;&#22833;&#20989;&#25968;&#31867;&#119936;&#21644;&#20551;&#35774;&#31867;&#119966;&#65292;&#20840;&#33021;&#39044;&#27979;&#22120;&#26159;&#19968;&#31181;&#39044;&#27979;&#22120;&#65292;&#20854;&#39044;&#27979;&#30340;&#25439;&#22833;&#23567;&#20110;&#119936;&#20013;&#20219;&#24847;&#25439;&#22833;&#19979;&#30340;&#26368;&#20339;&#20551;&#35774;&#12290;&#33258;&#20174;&#24341;&#20837;&#36825;&#20010;&#27010;&#24565;&#30340;[GKR+21]&#30340;&#24037;&#20316;&#20197;&#26469;&#65292;&#22312;&#20108;&#20998;&#31867;&#26631;&#31614;&#30340;&#35774;&#32622;&#19979;&#65292;&#21363;&#119858;&#8712;{0,1}&#65292;&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#24037;&#20316;&#65292;&#20294;&#26159;&#23545;&#20110;&#36830;&#32493;&#26631;&#31614;&#119858;&#8712;[0,1]&#30340;&#22238;&#24402;&#35774;&#32622;&#21364;&#30693;&#20043;&#29978;&#23569;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#27010;&#24565;&#36129;&#29486;&#26159;&#20851;&#20110;&#19968;&#31867;&#25439;&#22833;&#20989;&#25968;&#30340;&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#30340;&#27010;&#24565;&#65306;&#36825;&#26159;&#19968;&#32452;&#20851;&#20110;&#20998;&#24067;&#30340;&#32479;&#35745;&#37327;&#65292;&#36890;&#36807;&#20102;&#35299;&#23427;&#20204;&#21487;&#20197;&#37319;&#21462;&#26368;&#23567;&#21270;&#20219;&#20309;&#25439;&#22833;&#30340;&#26399;&#26395;&#25439;&#22833;&#30340;&#34892;&#21160;&#12290;&#20805;&#20998;&#32479;&#35745;&#37327;&#30340;&#27010;&#24565;&#30452;&#25509;&#30456;&#20851;&#21040;&#20984;&#20989;&#25968;&#36817;&#20284;&#30340;&#27010;&#24565;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider the supervised learning setting where the goal is to learn to predict labels $\mathbf y$ given points $\mathbf x$ from a distribution. An \textit{omnipredictor} for a class $\mathcal L$ of loss functions and a class $\mathcal C$ of hypotheses is a predictor whose predictions incur less expected loss than the best hypothesis in $\mathcal C$ for every loss in $\mathcal L$. Since the work of [GKR+21] that introduced the notion, there has been a large body of work in the setting of binary labels where $\mathbf y \in \{0, 1\}$, but much less is known about the regression setting where $\mathbf y \in [0,1]$ can be continuous. Our main conceptual contribution is the notion of \textit{sufficient statistics} for loss minimization over a family of loss functions: these are a set of statistics about a distribution such that knowing them allows one to take actions that minimize the expected loss for any loss in the family. The notion of sufficient statistics relates directly to the approx
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#25512;&#26029;&#25968;&#25454;&#21069;&#25552;&#30340;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#30830;&#23450;&#20854;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#20174;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#20013;&#25512;&#23548;&#20986;&#30340;&#35268;&#21017;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25277;&#35937;&#34920;&#31034;&#65292;&#20197;&#36827;&#34892;&#26368;&#24369;&#21069;&#25552;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.14628</link><description>&lt;p&gt;
&#20174;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#25512;&#26029;&#25968;&#25454;&#21069;&#25552;&#65292;&#20197;&#23454;&#29616;&#21487;&#38752;&#30340;&#37096;&#32626;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Inferring Data Preconditions from Deep Learning Models for Trustworthy Prediction in Deployment. (arXiv:2401.14628v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#25512;&#26029;&#25968;&#25454;&#21069;&#25552;&#30340;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#30830;&#23450;&#20854;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#20174;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#20013;&#25512;&#23548;&#20986;&#30340;&#35268;&#21017;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25277;&#35937;&#34920;&#31034;&#65292;&#20197;&#36827;&#34892;&#26368;&#24369;&#21069;&#25552;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#38454;&#27573;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#26681;&#25454;&#23545;&#25968;&#25454;&#30340;&#26576;&#20123;&#20551;&#35774;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#28982;&#21518;&#22312;&#37096;&#32626;&#38454;&#27573;&#29992;&#20110;&#39044;&#27979;&#12290;&#22312;&#37096;&#32626;&#26399;&#38388;&#65292;&#25512;&#29702;&#27169;&#22411;&#23545;&#20110;&#26410;&#35265;&#25968;&#25454;&#30340;&#39044;&#27979;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#25351;&#23450;&#21644;&#39564;&#35777;&#20256;&#32479;&#36719;&#20214;&#30340;&#26041;&#27861;&#23545;&#20110;&#22788;&#29702;DNN&#27169;&#22411;&#26550;&#26500;&#21644;&#39044;&#26399;&#32467;&#26524;&#30340;&#22797;&#26434;&#24615;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#20174;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#20013;&#25512;&#23548;&#20986;&#30340;&#35268;&#21017;&#26469;&#25512;&#26029;&#19968;&#20010;DNN&#27169;&#22411;&#30340;&#25968;&#25454;&#21069;&#25552;&#65292;&#20197;&#30830;&#23450;&#20854;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;DeepInfer&#28041;&#21450;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;DNN&#27169;&#22411;&#25277;&#35937;&#34920;&#31034;&#65292;&#20197;&#20351;&#29992;Dijkstra&#30340;&#35859;&#35789;&#21464;&#25442;&#35821;&#20041;&#36827;&#34892;&#26368;&#24369;&#21069;&#25552;&#25512;&#29702;&#12290;&#36890;&#36807;&#23548;&#20986;&#22312;&#24402;&#32435;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#25277;&#35937;&#34920;&#31034;&#19978;&#30340;&#35268;&#21017;&#65292;&#25105;&#20204;&#21487;&#20197;&#20811;&#26381;&#30001;&#20110;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#24341;&#36215;&#30340;&#30697;&#38453;&#32500;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models are trained with certain assumptions about the data during the development stage and then used for prediction in the deployment stage. It is important to reason about the trustworthiness of the model's predictions with unseen data during deployment. Existing methods for specifying and verifying traditional software are insufficient for this task, as they cannot handle the complexity of DNN model architecture and expected outcomes. In this work, we propose a novel technique that uses rules derived from neural network computations to infer data preconditions for a DNN model to determine the trustworthiness of its predictions. Our approach, DeepInfer involves introducing a novel abstraction for a trained DNN model that enables weakest precondition reasoning using Dijkstra's Predicate Transformer Semantics. By deriving rules over the inductive type of neural network abstract representation, we can overcome the matrix dimensionality issues that arise from the backward n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#23454;&#29992;&#30340;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#25209;&#37327;&#24402;&#19968;&#21270;&#23545;&#40784;&#21644;&#29109;&#39537;&#21160;&#30340;&#20869;&#23384;&#24211;&#26469;&#20943;&#36731;&#30446;&#26631;&#39046;&#22495;&#36716;&#31227;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.14619</link><description>&lt;p&gt;
&#24377;&#24615;&#23454;&#29992;&#30340;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65306;&#36719;&#25209;&#37327;&#24402;&#19968;&#21270;&#23545;&#40784;&#21644;&#29109;&#39537;&#21160;&#30340;&#20869;&#23384;&#24211;
&lt;/p&gt;
&lt;p&gt;
Resilient Practical Test-Time Adaptation: Soft Batch Normalization Alignment and Entropy-driven Memory Bank. (arXiv:2401.14619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#23454;&#29992;&#30340;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#25209;&#37327;&#24402;&#19968;&#21270;&#23545;&#40784;&#21644;&#29109;&#39537;&#21160;&#30340;&#20869;&#23384;&#24211;&#26469;&#20943;&#36731;&#30446;&#26631;&#39046;&#22495;&#36716;&#31227;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#22495;&#36866;&#24212;&#26377;&#25928;&#22320;&#35843;&#25972;&#28304;&#22495;&#27169;&#22411;&#20197;&#36866;&#24212;&#30446;&#26631;&#22495;&#20013;&#30340;&#26410;&#35265;&#39046;&#22495;&#36716;&#31227;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#12290;&#28982;&#32780;&#65292;&#36830;&#32493;&#30340;&#30446;&#26631;&#39046;&#22495;&#20998;&#24067;&#21464;&#21270;&#20197;&#21450;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-i.i.d.&#65289;&#27979;&#35797;&#26679;&#26412;&#65292;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#20869;&#23384;&#24211;&#26041;&#27861;&#20351;&#29992;&#20869;&#23384;&#23384;&#20648;&#26679;&#26412;&#24182;&#20943;&#36731;non-i.i.d.&#25928;&#24212;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#33021;&#20174;&#26681;&#26412;&#19978;&#38450;&#27490;&#28508;&#22312;&#30340;&#27169;&#22411;&#36864;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#23454;&#29992;&#30340;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;ResiTTA&#65289;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#21442;&#25968;&#24377;&#24615;&#21644;&#25968;&#25454;&#36136;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24377;&#24615;&#25209;&#37327;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#24402;&#19968;&#21270;&#32479;&#35745;&#37327;&#21644;&#36719;&#23545;&#40784;&#26469;&#20943;&#36731;&#36807;&#25311;&#21512;&#21644;&#27169;&#22411;&#36864;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#29109;&#39537;&#21160;&#30340;&#20869;&#23384;&#24211;&#65292;&#32771;&#34385;&#20102;&#26102;&#25928;&#24615;&#12289;&#36807;&#20998;&#33258;&#20449;&#26679;&#26412;&#30340;&#25345;&#20037;&#24615;&#20197;&#21450;&#26679;&#26412;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-time domain adaptation effectively adjusts the source domain model to accommodate unseen domain shifts in a target domain during inference. However, the model performance can be significantly impaired by continuous distribution changes in the target domain and non-independent and identically distributed (non-i.i.d.) test samples often encountered in practical scenarios. While existing memory bank methodologies use memory to store samples and mitigate non-i.i.d. effects, they do not inherently prevent potential model degradation. To address this issue, we propose a resilient practical test-time adaptation (ResiTTA) method focused on parameter resilience and data quality. Specifically, we develop a resilient batch normalization with estimation on normalization statistics and soft alignments to mitigate overfitting and model degradation. We use an entropy-driven memory bank that accounts for timeliness, the persistence of over-confident samples, and sample uncertainty for high-qualit
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#20449;&#24687;&#21516;&#27493;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#27861;&#65288;PISAL&#65289;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#31995;&#32479;&#22312;&#24322;&#36136;&#20171;&#36136;&#20013;&#24314;&#27169;&#30340;PDEs&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;Net1&#12289;Net2&#21644;NetI&#26469;&#21516;&#27493;&#36924;&#36817;&#35299;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#19981;&#21487;&#29992;&#30340;&#26102;&#38388;&#21464;&#21270;&#30028;&#38754;&#12290;</title><link>http://arxiv.org/abs/2401.14609</link><description>&lt;p&gt;
&#24322;&#36136;&#20171;&#36136;&#20013;&#20855;&#26377;&#19981;&#21487;&#29992;&#26102;&#38388;&#21464;&#21270;&#30028;&#38754;&#30340;&#24037;&#19994;&#31995;&#32479;&#24314;&#27169;&#30340;&#29289;&#29702;&#20449;&#24687;&#21516;&#27493;&#33258;&#36866;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Physically Informed Synchronic-adaptive Learning for Industrial Systems Modeling in Heterogeneous Media with Unavailable Time-varying Interface. (arXiv:2401.14609v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14609
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#21516;&#27493;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#27861;&#65288;PISAL&#65289;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#31995;&#32479;&#22312;&#24322;&#36136;&#20171;&#36136;&#20013;&#24314;&#27169;&#30340;PDEs&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;Net1&#12289;Net2&#21644;NetI&#26469;&#21516;&#27493;&#36924;&#36817;&#35299;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#19981;&#21487;&#29992;&#30340;&#26102;&#38388;&#21464;&#21270;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#36890;&#24120;&#29992;&#20110;&#24314;&#27169;&#20855;&#26377;&#22810;&#21464;&#37327;&#20381;&#36182;&#24615;&#30340;&#22797;&#26434;&#24037;&#19994;&#31995;&#32479;&#12290;&#29616;&#26377;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#22343;&#36136;&#20171;&#36136;&#20013;&#35299;&#20915;PDEs&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#24403;PDE&#21442;&#25968;&#30001;&#20110;&#32570;&#20047;&#29289;&#29702;&#23646;&#24615;&#21644;&#26080;&#27861;&#33719;&#24471;&#26102;&#38388;&#21464;&#21270;&#30028;&#38754;&#32780;&#19981;&#30693;&#36947;&#26102;&#65292;&#23427;&#20204;&#30340;&#21487;&#34892;&#24615;&#20250;&#38477;&#20302;&#65292;&#36825;&#26159;&#30001;&#20110;&#24322;&#36136;&#20171;&#36136;&#24341;&#36215;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;-&#29289;&#29702;&#28151;&#21512;&#26041;&#27861;&#65292;&#29289;&#29702;&#20449;&#24687;&#21516;&#27493;&#33258;&#36866;&#24212;&#23398;&#20064;&#65288;PISAL&#65289;&#65292;&#20197;&#35299;&#20915;&#29992;&#20110;&#24322;&#36136;&#20171;&#36136;&#20013;&#30340;&#24037;&#19994;&#31995;&#32479;&#24314;&#27169;&#30340;PDEs&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#26500;&#24314;Net1&#12289;Net2&#21644;NetI&#26469;&#36924;&#36817;&#28385;&#36275;PDEs&#21644;&#30028;&#38754;&#30340;&#35299;&#12290;Net1&#21644;Net2&#34987;&#29992;&#20110;&#21516;&#27493;&#23398;&#20064;&#28385;&#36275;&#20855;&#26377;&#19981;&#21516;&#21442;&#25968;&#30340;&#27599;&#20010;PDEs&#30340;&#35299;&#65292;&#32780;NetI&#21017;&#29992;&#20110;&#33258;&#36866;&#24212;&#23398;&#20064;&#19981;&#21487;&#29992;&#30340;&#26102;&#38388;&#21464;&#21270;&#30028;&#38754;&#12290;&#28982;&#21518;&#65292;&#24341;&#20837;&#19968;&#20010;&#19982;NetI&#30456;&#32467;&#21512;&#30340;&#26631;&#20934;&#26469;&#33258;&#36866;&#24212;&#22320;&#21306;&#20998;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations (PDEs) are commonly employed to model complex industrial systems characterized by multivariable dependence. Existing physics-informed neural networks (PINNs) excel in solving PDEs in a homogeneous medium. However, their feasibility is diminished when PDE parameters are unknown due to a lack of physical attributions and time-varying interface is unavailable arising from heterogeneous media. To this end, we propose a data-physics-hybrid method, physically informed synchronic-adaptive learning (PISAL), to solve PDEs for industrial systems modeling in heterogeneous media. First, Net1, Net2, and NetI, are constructed to approximate the solutions satisfying PDEs and the interface. Net1 and Net2 are utilized to synchronously learn each solution satisfying PDEs with diverse parameters, while NetI is employed to adaptively learn the unavailable time-varying interface. Then, a criterion combined with NetI is introduced to adaptively distinguish the attributions of 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#23398;&#20064;&#27969;&#24418;&#65292;&#24182;&#20351;&#29992;Ricci&#27969;&#20351;&#27969;&#24418;&#28508;&#31354;&#38388;&#36880;&#27493;&#36866;&#24212;&#21160;&#21147;&#23398;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;PDE&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.14591</link><description>&lt;p&gt;
&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#26102;&#21464;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Ricci flow-guided autoencoders in learning time-dependent dynamics. (arXiv:2401.14591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14591
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#23398;&#20064;&#27969;&#24418;&#65292;&#24182;&#20351;&#29992;Ricci&#27969;&#20351;&#27969;&#24418;&#28508;&#31354;&#38388;&#36880;&#27493;&#36866;&#24212;&#21160;&#21147;&#23398;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;PDE&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#26102;&#38388;&#19978;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#65292;&#20854;&#20013;&#27969;&#24418;&#28508;&#31354;&#38388;&#26681;&#25454;Ricci&#27969;&#21457;&#23637;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#22312;&#29289;&#29702;&#20449;&#24687;&#35774;&#32622;&#20013;&#27169;&#25311;Ricci&#27969;&#26469;&#23454;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#21305;&#37197;&#27969;&#24418;&#37327;&#65292;&#20197;&#20415;&#23454;&#29616;Ricci&#27969;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#27969;&#24418;&#26159;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#23398;&#20064;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#35782;&#21035;&#20986;&#29702;&#24819;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#21516;&#26102;&#28436;&#21464;&#20063;&#33021;&#22312;&#38745;&#24577;&#26041;&#27861;&#19978;&#24341;&#36215;&#26356;&#23485;&#23481;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#31561;&#29702;&#24819;&#29305;&#24449;&#30340;PDE&#65292;&#24182;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#36827;&#34892;&#35823;&#24046;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a manifold-based autoencoder method for learning nonlinear dynamics in time, notably partial differential equations (PDEs), in which the manifold latent space evolves according to Ricci flow. This can be accomplished by simulating Ricci flow in a physics-informed setting, and manifold quantities can be matched so that Ricci flow is empirically achieved. With our methodology, the manifold is learned as part of the training procedure, so ideal geometries may be discerned, while the evolution simultaneously induces a more accommodating latent representation over static methods. We present our method on a range of numerical experiments consisting of PDEs that encompass desirable characteristics such as periodicity and randomness, remarking error on in-distribution and extrapolation scenarios.
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>http://arxiv.org/abs/2401.14585</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Diffusion Stochastic Optimization for Min-Max Problems. (arXiv:2401.14585v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14585
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
The optimistic gradient method is useful in addressing minimax optimization problems. Motivated by the observation that the conventional stochastic version suffers from the need for a large batch size on the order of $\mathcal{O}(\varepsilon^{-2})$ to achieve an $\varepsilon$-stationary solution, we introduce and analyze a new formulation termed Diffusion Stochastic Same-Sample Optimistic Gradient (DSS-OG). We prove its convergence and resolve the large batch issue by establishing a tighter upper bound, under the more general setting of nonconvex Polyak-Lojasiewicz (PL) risk functions. We also extend the applicability of the proposed method to the distributed scenario, where agents communicate with their neighbors via a left-stochastic protocol. To implement DSS-OG, we can query the stochastic gradient oracles in parallel with some extra memory overhead, resulting in a complexity comparable to its conventional counterpart. To demonstrate the efficacy of the proposed algorithm, we condu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#24341;&#23548;&#30340;&#26080;&#20559;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#38468;&#21152;&#33410;&#28857;&#21644;&#20351;&#29992;&#27491;&#36127;&#26435;&#37325;&#37325;&#36830;&#36830;&#25509;&#26469;&#20016;&#23500;&#22270;&#32467;&#26500;&#65292;&#20197;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14580</link><description>&lt;p&gt;
&#35774;&#35745;&#20320;&#33258;&#24049;&#30340;&#23431;&#23449;&#65306;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#24341;&#23548;&#30340;&#26080;&#20559;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Design Your Own Universe: A Physics-Informed Agnostic Method for Enhancing Graph Neural Networks. (arXiv:2401.14580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#24341;&#23548;&#30340;&#26080;&#20559;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#38468;&#21152;&#33410;&#28857;&#21644;&#20351;&#29992;&#27491;&#36127;&#26435;&#37325;&#37325;&#36830;&#36830;&#25509;&#26469;&#20016;&#23500;&#22270;&#32467;&#26500;&#65292;&#20197;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#24341;&#23548;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#32531;&#35299;&#24120;&#35265;&#30340;GNN&#25361;&#25112;&#65288;&#22914;&#36807;&#24230;&#24179;&#28369;&#21270;&#12289;&#36807;&#24230;&#21387;&#32553;&#21644;&#24322;&#36136;&#36866;&#24212;&#65289;&#22312;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20173;&#28982;&#22312;&#24320;&#21457;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33539;&#24335;&#26469;&#36866;&#24403;&#22320;&#25972;&#21512;&#22788;&#29702;&#25152;&#26377;&#36825;&#20123;&#25361;&#25112;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;GNN&#30340;&#20256;&#25773;&#19982;&#29289;&#29702;&#31890;&#23376;&#31995;&#32479;&#36827;&#34892;&#31867;&#27604;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22686;&#24378;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#38468;&#21152;&#33410;&#28857;&#21644;&#20351;&#29992;&#27491;&#36127;&#26435;&#37325;&#37325;&#36830;&#36830;&#25509;&#26469;&#20016;&#23500;&#22270;&#32467;&#26500;&#65292;&#21463;&#33410;&#28857;&#26631;&#35760;&#20449;&#24687;&#30340;&#25351;&#23548;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#39564;&#35777;&#20102;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#30340;GNN&#21487;&#20197;&#26377;&#25928;&#22320;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#24182;&#23545;&#36807;&#24230;&#21387;&#32553;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#37325;&#36830;&#22270;&#36827;&#34892;&#20102;&#35889;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#30456;&#24212;&#30340;GNN&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
Physics-informed Graph Neural Networks have achieved remarkable performance in learning through graph-structured data by mitigating common GNN challenges such as over-smoothing, over-squashing, and heterophily adaption. Despite these advancements, the development of a simple yet effective paradigm that appropriately integrates previous methods for handling all these challenges is still underway. In this paper, we draw an analogy between the propagation of GNNs and particle systems in physics, proposing a model-agnostic enhancement framework. This framework enriches the graph structure by introducing additional nodes and rewiring connections with both positive and negative weights, guided by node labeling information. We theoretically verify that GNNs enhanced through our approach can effectively circumvent the over-smoothing issue and exhibit robustness against over-squashing. Moreover, we conduct a spectral analysis on the rewired graph to demonstrate that the corresponding GNNs can f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Output Attribution&#65288;GOAt&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;GNN&#25193;&#23637;&#20026;&#28041;&#21450;&#33410;&#28857;&#29305;&#24449;&#12289;&#36793;&#29305;&#24449;&#21644;&#28608;&#27963;&#27169;&#24335;&#30340;&#26631;&#37327;&#31215;&#20043;&#21644;&#65292;&#35745;&#31639;&#27599;&#20010;&#33410;&#28857;&#25110;&#36793;&#29305;&#24449;&#23545;&#27599;&#20010;&#26631;&#37327;&#31215;&#30340;&#36129;&#29486;&#65292;&#24182;&#23558;&#36129;&#29486;&#32858;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#23558;&#22270;&#36755;&#20986;&#24402;&#22240;&#20110;&#36755;&#20837;&#22270;&#29305;&#24449;&#30340;GNN&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.14578</link><description>&lt;p&gt;
GOAt: &#36890;&#36807;&#22270;&#36755;&#20986;&#23646;&#24615;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GOAt: Explaining Graph Neural Networks via Graph Output Attribution. (arXiv:2401.14578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Output Attribution&#65288;GOAt&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;GNN&#25193;&#23637;&#20026;&#28041;&#21450;&#33410;&#28857;&#29305;&#24449;&#12289;&#36793;&#29305;&#24449;&#21644;&#28608;&#27963;&#27169;&#24335;&#30340;&#26631;&#37327;&#31215;&#20043;&#21644;&#65292;&#35745;&#31639;&#27599;&#20010;&#33410;&#28857;&#25110;&#36793;&#29305;&#24449;&#23545;&#27599;&#20010;&#26631;&#37327;&#31215;&#30340;&#36129;&#29486;&#65292;&#24182;&#23558;&#36129;&#29486;&#32858;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#23558;&#22270;&#36755;&#20986;&#24402;&#22240;&#20110;&#36755;&#20837;&#22270;&#29305;&#24449;&#30340;GNN&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#20915;&#31574;&#36807;&#31243;&#23545;&#20110;&#20854;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#35299;&#37322;GNNs&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#35757;&#32451;&#36741;&#21161;&#27169;&#22411;&#65292;&#23548;&#33268;&#35299;&#37322;&#32467;&#26524;&#20173;&#28982;&#26159;&#40657;&#30418;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Output Attribution&#65288;GOAt&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22270;&#36755;&#20986;&#24402;&#22240;&#20110;&#36755;&#20837;&#22270;&#29305;&#24449;&#65292;&#20174;&#32780;&#21019;&#24314;&#26082;&#24544;&#23454;&#12289;&#26377;&#21306;&#21035;&#65292;&#21448;&#22312;&#30456;&#20284;&#26679;&#26412;&#19978;&#31283;&#23450;&#30340;GNN&#35299;&#37322;&#12290;&#36890;&#36807;&#23558;GNN&#25193;&#23637;&#20026;&#28041;&#21450;&#33410;&#28857;&#29305;&#24449;&#12289;&#36793;&#29305;&#24449;&#21644;&#28608;&#27963;&#27169;&#24335;&#30340;&#26631;&#37327;&#31215;&#20043;&#21644;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#27599;&#20010;&#33410;&#28857;&#25110;&#36793;&#29305;&#24449;&#23545;&#27599;&#20010;&#26631;&#37327;&#31215;&#30340;&#36129;&#29486;&#65292;&#24182;&#23558;&#25193;&#23637;&#24418;&#24335;&#20013;&#25152;&#26377;&#26631;&#37327;&#31215;&#30340;&#36129;&#29486;&#32858;&#21512;&#36215;&#26469;&#65292;&#20197;&#25512;&#23548;&#20986;&#27599;&#20010;&#33410;&#28857;&#21644;&#36793;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#35780;&#20272;GNN&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#21508;&#31181;&#26368;&#26032;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Understanding the decision-making process of Graph Neural Networks (GNNs) is crucial to their interpretability. Most existing methods for explaining GNNs typically rely on training auxiliary models, resulting in the explanations remain black-boxed. This paper introduces Graph Output Attribution (GOAt), a novel method to attribute graph outputs to input graph features, creating GNN explanations that are faithful, discriminative, as well as stable across similar samples. By expanding the GNN as a sum of scalar products involving node features, edge features and activation patterns, we propose an efficient analytical method to compute contribution of each node or edge feature to each scalar product and aggregate the contributions from all scalar products in the expansion form to derive the importance of each node and edge. Through extensive experiments on synthetic and real-world data, we show that our method not only outperforms various state-ofthe-art GNN explainers in terms of the comm
&lt;/p&gt;</description></item><item><title>PrivStream&#26159;&#19968;&#31181;&#29992;&#20110;&#27969;&#24335;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#31163;&#32447;&#24212;&#29992;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#25928;&#29992;&#38382;&#39064;&#12290;&#31639;&#27861;&#21487;&#20197;&#38024;&#23545;&#31354;&#38388;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#25104;&#27969;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#20379;&#20102;&#36890;&#29992;&#30340;&#22312;&#32447;&#36873;&#25321;&#24615;&#35745;&#25968;&#26694;&#26550;&#65292;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14577</link><description>&lt;p&gt;
PrivStream&#65306;&#19968;&#31181;&#29992;&#20110;&#27969;&#24335;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
PrivStream: An Algorithm for Streaming Differentially Private Data. (arXiv:2401.14577v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14577
&lt;/p&gt;
&lt;p&gt;
PrivStream&#26159;&#19968;&#31181;&#29992;&#20110;&#27969;&#24335;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#31163;&#32447;&#24212;&#29992;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#25928;&#29992;&#38382;&#39064;&#12290;&#31639;&#27861;&#21487;&#20197;&#38024;&#23545;&#31354;&#38388;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#25104;&#27969;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#20379;&#20102;&#36890;&#29992;&#30340;&#22312;&#32447;&#36873;&#25321;&#24615;&#35745;&#25968;&#26694;&#26550;&#65292;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24046;&#20998;&#38544;&#31169;&#30740;&#31350;&#37117;&#30528;&#37325;&#20110;&#20551;&#35774;&#25152;&#26377;&#25968;&#25454;&#19968;&#27425;&#24615;&#21487;&#29992;&#30340;&#31163;&#32447;&#24212;&#29992;&#12290;&#20294;&#24403;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#20013;&#30340;&#25968;&#25454;&#27969;&#65292;&#35201;&#20040;&#36829;&#21453;&#20102;&#38544;&#31169;&#20445;&#35777;&#65292;&#35201;&#20040;&#23548;&#33268;&#20102;&#31967;&#31957;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31354;&#38388;&#25968;&#25454;&#38598;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#27969;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#32447;&#36873;&#25321;&#24615;&#35745;&#25968;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#26597;&#35810;&#24212;&#31572;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Much of the research in differential privacy has focused on offline applications with the assumption that all data is available at once. When these algorithms are applied in practice to streams where data is collected over time, this either violates the privacy guarantees or results in poor utility. We derive an algorithm for differentially private synthetic streaming data generation, especially curated towards spatial datasets. Furthermore, we provide a general framework for online selective counting among a collection of queries which forms a basis for many tasks such as query answering and synthetic data generation. The utility of our algorithm is verified on both real-world and simulated datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#29305;&#23450;RC&#20307;&#31995;&#32467;&#26500;&#19982;&#30456;&#24212;&#24490;&#29615;&#20869;&#26680;&#24418;&#24335;&#31561;&#20215;&#24615;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;Leaky RC&#12289;Sparse RC&#21644;Deep RC&#31561;&#24050;&#24314;&#31435;&#30340;RC&#33539;&#20363;&#23578;&#26410;&#36827;&#34892;&#20998;&#26512;&#30340;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#31232;&#30095;&#36830;&#25509;&#22312;RC&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20648;&#22791;&#22823;&#23567;&#30340;&#26368;&#20339;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#26368;&#21518;&#65292;&#35813;&#30740;&#31350;&#30340;&#31995;&#32479;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;Deep RC&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20943;&#23567;&#23610;&#23544;&#30340;&#36830;&#32493;&#20648;&#22791;&#21487;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2401.14557</link><description>&lt;p&gt;
&#19981;&#21516;&#30340;&#24490;&#29615;&#20869;&#26680;&#25299;&#23637;&#21040;&#19981;&#21516;&#30340;&#20648;&#22791;&#35745;&#31639;&#25299;&#25169;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Extension of Recurrent Kernels to different Reservoir Computing topologies. (arXiv:2401.14557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14557
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#29305;&#23450;RC&#20307;&#31995;&#32467;&#26500;&#19982;&#30456;&#24212;&#24490;&#29615;&#20869;&#26680;&#24418;&#24335;&#31561;&#20215;&#24615;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;Leaky RC&#12289;Sparse RC&#21644;Deep RC&#31561;&#24050;&#24314;&#31435;&#30340;RC&#33539;&#20363;&#23578;&#26410;&#36827;&#34892;&#20998;&#26512;&#30340;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#31232;&#30095;&#36830;&#25509;&#22312;RC&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20648;&#22791;&#22823;&#23567;&#30340;&#26368;&#20339;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#26368;&#21518;&#65292;&#35813;&#30740;&#31350;&#30340;&#31995;&#32479;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;Deep RC&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20943;&#23567;&#23610;&#23544;&#30340;&#36830;&#32493;&#20648;&#22791;&#21487;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#24555;&#36895;&#39640;&#25928;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#20648;&#22791;&#35745;&#31639;&#65288;RC&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#26631;&#20934;&#30340;RC&#22312;&#28176;&#36817;&#26497;&#38480;&#19979;&#24050;&#34987;&#35777;&#26126;&#19982;&#24490;&#29615;&#20869;&#26680;&#31561;&#25928;&#65292;&#36825;&#26377;&#21161;&#20110;&#20998;&#26512;&#20854;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24050;&#24314;&#31435;&#30340;RC&#33539;&#20363;&#65292;&#22914;Leaky RC&#12289;Sparse RC&#21644;Deep RC&#65292;&#23578;&#26410;&#20197;&#36825;&#31181;&#26041;&#24335;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#29305;&#23450;RC&#20307;&#31995;&#32467;&#26500;&#19982;&#30456;&#24212;&#24490;&#29615;&#20869;&#26680;&#24418;&#24335;&#31561;&#20215;&#24615;&#30340;&#32463;&#39564;&#20998;&#26512;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#27599;&#20010;&#20307;&#31995;&#32467;&#26500;&#20013;&#23454;&#26045;&#30340;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#25910;&#25947;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#31232;&#30095;&#36830;&#25509;&#22312;RC&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20648;&#22791;&#22823;&#23567;&#30340;&#26368;&#20339;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;Deep RC&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20943;&#23567;&#23610;&#23544;&#30340;&#36830;&#32493;&#20648;&#22791;&#21487;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir Computing (RC) has become popular in recent years due to its fast and efficient computational capabilities. Standard RC has been shown to be equivalent in the asymptotic limit to Recurrent Kernels, which helps in analyzing its expressive power. However, many well-established RC paradigms, such as Leaky RC, Sparse RC, and Deep RC, are yet to be analyzed in such a way. This study aims to fill this gap by providing an empirical analysis of the equivalence of specific RC architectures with their corresponding Recurrent Kernel formulation. We conduct a convergence study by varying the activation function implemented in each architecture. Our study also sheds light on the role of sparse connections in RC architectures and propose an optimal sparsity level that depends on the reservoir size. Furthermore, our systematic analysis shows that in Deep RC models, convergence is better achieved with successive reservoirs of decreasing sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#30784;&#35270;&#35273;&#27169;&#22411;&#23545;&#26377;&#25928;&#20027;&#21160;&#23398;&#20064;&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31616;&#21333;&#20248;&#38597;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#24179;&#34913;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14555</link><description>&lt;p&gt;
&#22312;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#23457;&#35270;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Revisiting Active Learning in the Era of Vision Foundation Models. (arXiv:2401.14555v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#30784;&#35270;&#35273;&#27169;&#22411;&#23545;&#26377;&#25928;&#20027;&#21160;&#23398;&#20064;&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31616;&#21333;&#20248;&#38597;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#24179;&#34913;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#35270;&#35273;&#25110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26159;&#22312;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25110;&#22122;&#22768;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#24182;&#23398;&#20064;&#21040;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26631;&#27880;&#25110;&#23569;&#26631;&#27880;&#24615;&#33021;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;&#37492;&#20110;&#36825;&#20123;&#29305;&#24615;&#65292;&#23427;&#20204;&#26159;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#30340;&#33258;&#28982;&#36873;&#25321;&#65292;&#26088;&#22312;&#23454;&#29616;&#26631;&#35760;&#25928;&#29575;&#30340;&#26368;&#22823;&#21270;&#65292;&#20294;&#22312;&#20302;&#39044;&#31639;&#26465;&#20214;&#19979;&#65292;&#22522;&#30784;&#27169;&#22411;&#30340;&#20840;&#37096;&#28508;&#21147;&#22312;AL&#29615;&#22659;&#20013;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#30784;&#27169;&#22411;&#23545;&#26377;&#25928;AL&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#65292;&#21363;1&#65289;&#21021;&#22987;&#26631;&#35760;&#26679;&#26412;&#27744;&#30340;&#36873;&#25321;&#65292;2&#65289;&#30830;&#20445;&#22810;&#26679;&#24615;&#25277;&#26679;&#65292;&#20197;&#21450;3&#65289;&#20195;&#34920;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#25277;&#26679;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22522;&#30784;&#27169;&#22411;&#65288;DINOv2&#12289;OpenCLIP&#65289;&#30340;&#40065;&#26834;&#34920;&#31034;&#22914;&#20309;&#25361;&#25112;&#24050;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#20026;&#19968;&#20010;&#26032;&#30340;&#31616;&#21333;&#20248;&#38597;&#30340;AL&#31574;&#30053;&#30340;&#26377;&#21407;&#21017;&#26500;&#24314;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20351;&#29992;dropout&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation vision or vision-language models are trained on large unlabeled or noisy data and learn robust representations that can achieve impressive zeroor few-shot performance on diverse tasks. Given these properties, they are a natural fit for active learning (AL), which aims to maximize labeling efficiency, but the full potential of foundation models has not been explored in the context of AL, specifically in the low-budget regime. In this work, we evaluate how foundation models influence three critical components of effective AL, namely, 1) initial labeled pool selection, 2) ensuring diverse sampling, and 3) the trade-off between representative and uncertainty sampling. We systematically study how the robust representations of foundation models (DINOv2, OpenCLIP) challenge existing findings in active learning. Our observations inform the principled construction of a new simple and elegant AL strategy that balances uncertainty estimated via dropout with sample diversity. We exten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#22823;&#21518;&#39564;&#25512;&#26029;&#39640;&#26031;Cox&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#36890;&#36807;&#20351;&#29992;&#35813;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#26031;Cox&#36807;&#31243;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2401.14544</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;Cox&#36807;&#31243;&#27169;&#22411;&#30340;&#26102;&#31354;&#25968;&#25454;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization through Gaussian Cox Process Models for Spatio-temporal Data. (arXiv:2401.14544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#22823;&#21518;&#39564;&#25512;&#26029;&#39640;&#26031;Cox&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#36890;&#36807;&#20351;&#29992;&#35813;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#26031;Cox&#36807;&#31243;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#20248;&#21270;&#26114;&#36149;&#35780;&#20272;&#20989;&#25968;&#30340;&#20027;&#35201;&#31574;&#30053;&#12290;&#29616;&#26377;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#39640;&#26031;&#36807;&#31243;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#19981;&#36866;&#29992;&#20110;&#65288;&#21452;&#38543;&#26426;&#65289;&#39640;&#26031;Cox&#36807;&#31243;&#65292;&#20854;&#20013;&#35266;&#27979;&#36807;&#31243;&#30001;&#20316;&#20026;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#28508;&#22312;&#24378;&#24230;&#20989;&#25968;&#35843;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#22823;&#21518;&#39564;&#25512;&#26029;&#39640;&#26031;Cox&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#21644;&#26680;&#20989;&#25968;&#21464;&#25442;&#25216;&#26415;&#65292;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#26032;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#35745;&#31639;&#26356;&#23481;&#26131;&#22788;&#29702;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#28508;&#22312;&#24378;&#24230;&#20989;&#25968;&#30340;&#20989;&#25968;&#21518;&#39564;&#21644;&#21518;&#39564;&#30340;&#21327;&#26041;&#24046;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#29616;&#26377;&#24037;&#20316;&#36890;&#24120;&#20851;&#27880;&#29305;&#23450;&#30340;&#36830;&#25509;&#20989;&#25968;&#25110;&#20272;&#35745;&#21518;&#39564;&#22343;&#20540;&#30340;&#38382;&#39064;&#12290;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#26031;Cox&#36807;&#31243;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;Nystrom&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) has established itself as a leading strategy for efficiently optimizing expensive-to-evaluate functions. Existing BO methods mostly rely on Gaussian process (GP) surrogate models and are not applicable to (doubly-stochastic) Gaussian Cox processes, where the observation process is modulated by a latent intensity function modeled as a GP. In this paper, we propose a novel maximum a posteriori inference of Gaussian Cox processes. It leverages the Laplace approximation and change of kernel technique to transform the problem into a new reproducing kernel Hilbert space, where it becomes more tractable computationally. It enables us to obtain both a functional posterior of the latent intensity function and the covariance of the posterior, thus extending existing works that often focus on specific link functions or estimating the posterior mean. Using the result, we propose a BO framework based on the Gaussian Cox process model and further develop a Nystr\"om approx
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#39564;&#35780;&#20272;&#20102;&#20107;&#21518;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#21327;&#21464;&#37327;&#20559;&#31227;&#12289;&#27010;&#24565;&#36716;&#21464;&#21644;&#30465;&#30053;&#21327;&#21464;&#37327;&#20250;&#22686;&#21152;&#35299;&#37322;&#24046;&#24322;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#24433;&#21709;&#26356;&#22823;&#12290;</title><link>http://arxiv.org/abs/2401.14539</link><description>&lt;p&gt;
&#29702;&#35299;&#20107;&#21518;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#20013;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Understanding Disparities in Post Hoc Machine Learning Explanation. (arXiv:2401.14539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14539
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#39564;&#35780;&#20272;&#20102;&#20107;&#21518;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#21327;&#21464;&#37327;&#20559;&#31227;&#12289;&#27010;&#24565;&#36716;&#21464;&#21644;&#30465;&#30053;&#21327;&#21464;&#37327;&#20250;&#22686;&#21152;&#35299;&#37322;&#24046;&#24322;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#24433;&#21709;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25351;&#20986;&#65292;&#29616;&#26377;&#30340;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#22312;&#35299;&#37322;&#20934;&#30830;&#24615;&#19978;&#23384;&#22312;&#24046;&#24322;&#65288;&#28041;&#21450;&#8220;&#31181;&#26063;&#8221;&#21644;&#8220;&#24615;&#21035;&#8221;&#31561;&#25935;&#24863;&#23646;&#24615;&#65289;&#65292;&#34429;&#28982;&#24050;&#26377;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#22312;&#35299;&#37322;&#24230;&#37327;&#27700;&#24179;&#19978;&#20943;&#23569;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#40657;&#30418;&#27169;&#22411;&#19982;&#35299;&#37322;&#24046;&#24322;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#26410;&#34987;&#24191;&#27867;&#25506;&#35752;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#27169;&#25311;&#21644;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#29305;&#21035;&#35780;&#20272;&#20102;&#35299;&#37322;&#24046;&#24322;&#38754;&#20020;&#30340;&#25361;&#25112;&#65306;&#25968;&#25454;&#24615;&#36136;&#24341;&#36215;&#30340;&#23616;&#38480;&#26679;&#26412;&#37327;&#12289;&#21327;&#21464;&#37327;&#20559;&#31227;&#12289;&#27010;&#24565;&#36716;&#21464;&#12289;&#34987;&#30465;&#30053;&#30340;&#21464;&#37327;&#20559;&#24046;&#65292;&#20197;&#21450;&#27169;&#22411;&#24615;&#36136;&#24341;&#36215;&#30340;&#25361;&#25112;&#65306;&#25935;&#24863;&#23646;&#24615;&#30340;&#21253;&#21547;&#21644;&#36866;&#24403;&#30340;&#20989;&#25968;&#24418;&#24335;&#12290;&#36890;&#36807;&#21463;&#25511;&#27169;&#25311;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#22686;&#21152;&#21327;&#21464;&#37327;&#20559;&#31227;&#12289;&#27010;&#24565;&#36716;&#21464;&#21644;&#30465;&#30053;&#21327;&#21464;&#37327;&#20250;&#22686;&#21152;&#35299;&#37322;&#24046;&#24322;&#65292;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#32780;&#35328;&#65292;&#36825;&#31181;&#25928;&#24212;&#26356;&#21152;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous work has highlighted that existing post-hoc explanation methods exhibit disparities in explanation fidelity (across 'race' and 'gender' as sensitive attributes), and while a large body of work focuses on mitigating these issues at the explanation metric level, the role of the data generating process and black box model in relation to explanation disparities remains largely unexplored. Accordingly, through both simulations as well as experiments on a real-world dataset, we specifically assess challenges to explanation disparities that originate from properties of the data: limited sample size, covariate shift, concept shift, omitted variable bias, and challenges based on model properties: inclusion of the sensitive attribute and appropriate functional form. Through controlled simulation analyses, our study demonstrates that increased covariate shift, concept shift, and omission of covariates increase explanation disparities, with the effect pronounced higher for neural network 
&lt;/p&gt;</description></item><item><title>CaRiNG&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#36776;&#35782;&#24615;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#38750;&#21487;&#36870;&#29983;&#25104;&#36807;&#31243;&#30340;&#26102;&#38388;&#22240;&#26524;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#32452;&#20998;&#65292;&#21363;&#20351;&#23427;&#20204;&#26469;&#33258;&#20110;&#38750;&#32447;&#24615;&#19988;&#38750;&#21487;&#36870;&#30340;&#28151;&#21512;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.14535</link><description>&lt;p&gt;
CaRiNG: &#22312;&#38750;&#21487;&#36870;&#29983;&#25104;&#36807;&#31243;&#19979;&#23398;&#20064;&#26102;&#38388;&#22240;&#26524;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
CaRiNG: Learning Temporal Causal Representation under Non-Invertible Generation Process. (arXiv:2401.14535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14535
&lt;/p&gt;
&lt;p&gt;
CaRiNG&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#36776;&#35782;&#24615;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#38750;&#21487;&#36870;&#29983;&#25104;&#36807;&#31243;&#30340;&#26102;&#38388;&#22240;&#26524;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#32452;&#20998;&#65292;&#21363;&#20351;&#23427;&#20204;&#26469;&#33258;&#20110;&#38750;&#32447;&#24615;&#19988;&#38750;&#21487;&#36870;&#30340;&#28151;&#21512;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#21035;&#39034;&#24207;&#25968;&#25454;&#20013;&#28508;&#22312;&#30340;&#24310;&#36831;&#26102;&#38388;&#22240;&#26524;&#36807;&#31243;&#23545;&#20110;&#25226;&#25569;&#26102;&#38388;&#21160;&#21147;&#23398;&#21644;&#36827;&#34892;&#19979;&#28216;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#19968;&#20123;&#26041;&#27861;&#21487;&#20197;&#31283;&#20581;&#22320;&#35782;&#21035;&#36825;&#20123;&#28508;&#22312;&#30340;&#22240;&#26524;&#21464;&#37327;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#20174;&#28508;&#22312;&#21464;&#37327;&#21040;&#35266;&#27979;&#25968;&#25454;&#30340;&#21487;&#36870;&#29983;&#25104;&#36807;&#31243;&#30340;&#20005;&#26684;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20551;&#35774;&#36890;&#24120;&#22312;&#21253;&#21547;&#20449;&#24687;&#25439;&#22833;&#30340;&#29616;&#23454;&#24212;&#29992;&#20013;&#38590;&#20197;&#28385;&#36275;&#12290;&#20363;&#22914;&#65292;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#23558;3D&#31354;&#38388;&#36716;&#21270;&#20026;2D&#22270;&#20687;&#65292;&#25110;&#32773;&#35270;&#35273;&#22362;&#25345;&#29616;&#35937;&#22312;&#24403;&#21069;&#24863;&#30693;&#20013;&#34701;&#20837;&#21382;&#21490;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#36776;&#35782;&#24615;&#29702;&#35770;&#65292;&#20801;&#35768;&#22312;&#38750;&#32447;&#24615;&#21644;&#38750;&#21487;&#36870;&#28151;&#21512;&#24773;&#20917;&#19979;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#32452;&#20998;&#12290;&#22312;&#27492;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;CaRiNG&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#21487;&#36776;&#35782;&#24615;&#30340;&#38750;&#21487;&#36870;&#29983;&#25104;&#26102;&#38388;&#25968;&#25454;&#30340;&#22240;&#26524;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying the underlying time-delayed latent causal processes in sequential data is vital for grasping temporal dynamics and making downstream reasoning. While some recent methods can robustly identify these latent causal variables, they rely on strict assumptions about the invertible generation process from latent variables to observed data. However, these assumptions are often hard to satisfy in real-world applications containing information loss. For instance, the visual perception process translates a 3D space into 2D images, or the phenomenon of persistence of vision incorporates historical data into current perceptions. To address this challenge, we establish an identifiability theory that allows for the recovery of independent latent components even when they come from a nonlinear and non-invertible mix. Using this theory as a foundation, we propose a principled approach, CaRiNG, to learn the CAusal RepresentatIon of Non-invertible Generative temporal data with identifiability
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#20219;&#21153;&#12289;&#24322;&#26500;&#21644;&#26080;&#27169;&#22411;&#29615;&#22659;&#19979;&#23398;&#20064;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#19982;&#27599;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#26368;&#20248;&#25511;&#21046;&#22120;&#25509;&#36817;&#30340;&#25511;&#21046;&#22120;&#65292;&#24182;&#22312;&#27169;&#22411;&#22522;&#30784;&#35774;&#32622;&#19979;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14534</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;: &#19968;&#31181;&#38024;&#23545;&#26080;&#27169;&#22411;LQR&#30340;&#31574;&#30053;&#26799;&#24230;MAML&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Linear Quadratic Regulators: A Policy Gradient MAML Approach for the Model-free LQR. (arXiv:2401.14534v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#20219;&#21153;&#12289;&#24322;&#26500;&#21644;&#26080;&#27169;&#22411;&#29615;&#22659;&#19979;&#23398;&#20064;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#19982;&#27599;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#26368;&#20248;&#25511;&#21046;&#22120;&#25509;&#36817;&#30340;&#25511;&#21046;&#22120;&#65292;&#24182;&#22312;&#27169;&#22411;&#22522;&#30784;&#35774;&#32622;&#19979;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#22810;&#20219;&#21153;&#12289;&#24322;&#26500;&#21644;&#26080;&#27169;&#22411;&#29615;&#22659;&#20013;&#23398;&#20064;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#26041;&#27861;&#65288;Finn&#31561;&#20154;&#65292;2017&#65289;&#22312;&#19981;&#21516;&#20219;&#21153;&#24322;&#36136;&#24615;&#35774;&#32622;&#19979;&#30340;LQR&#38382;&#39064;&#30340;&#31283;&#23450;&#24615;&#21644;&#20010;&#24615;&#21270;&#20445;&#35777;&#36827;&#34892;&#20102;&#21051;&#30011;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#27169;&#22411;&#22522;&#30784;&#21644;&#26080;&#27169;&#22411;&#35774;&#32622;&#19979;&#65292;MAML-LQR&#26041;&#27861;&#20135;&#29983;&#30340;&#25511;&#21046;&#22120;&#19982;&#27599;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#26368;&#20248;&#25511;&#21046;&#22120;&#25509;&#36817;&#65292;&#38500;&#20102;&#20219;&#21153;&#24322;&#36136;&#24615;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#27169;&#22411;&#22522;&#30784;&#35774;&#32622;&#19979;&#65292;&#36825;&#31181;&#25511;&#21046;&#22120;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#23454;&#29616;&#65292;&#36825;&#25913;&#36827;&#20102;&#29616;&#26377;MAML-LQR&#24037;&#20316;&#20013;&#30340;&#27425;&#32447;&#24615;&#36895;&#29575;&#12290;&#19982;&#29616;&#26377;&#30340;MAML-LQR&#32467;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20445;&#35777;&#34920;&#26126;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#22120;&#21487;&#20197;&#39640;&#25928;&#22320;&#36866;&#24212;&#26410;&#30693;&#30340;LQR&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of learning Linear Quadratic Regulators (LQR) in a multi-task, heterogeneous, and model-free setting. We characterize the stability and personalization guarantees of a Policy Gradient-based (PG) Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) approach for the LQR problem under different task-heterogeneity settings. We show that the MAML-LQR approach produces a stabilizing controller close to each task-specific optimal controller up to a task-heterogeneity bias for both model-based and model-free settings. Moreover, in the model-based setting, we show that this controller is achieved with a linear convergence rate, which improves upon sub-linear rates presented in existing MAML-LQR work. In contrast to existing MAML-LQR results, our theoretical guarantees demonstrate that the learned controller can efficiently adapt to unseen LQR tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20570;&#36873;&#25321;&#26102;&#34920;&#29616;&#20986;&#20102;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#30456;&#20284;&#30340;&#30456;&#23545;&#20215;&#20540;&#20559;&#24046;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#20154;&#31867;&#36873;&#25321;&#20013;&#30340;&#32972;&#26223;&#20381;&#36182;&#24615;&#26426;&#21046;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.14530</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30456;&#23545;&#20215;&#20540;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Relative Value Biases in Large Language Models. (arXiv:2401.14530v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14530
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20570;&#36873;&#25321;&#26102;&#34920;&#29616;&#20986;&#20102;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#30456;&#20284;&#30340;&#30456;&#23545;&#20215;&#20540;&#20559;&#24046;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#20154;&#31867;&#36873;&#25321;&#20013;&#30340;&#32972;&#26223;&#20381;&#36182;&#24615;&#26426;&#21046;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#21160;&#29289;&#22312;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#37027;&#20123;&#36873;&#39033;&#19982;&#36739;&#20302;&#30340;&#32477;&#23545;&#22870;&#21169;&#30456;&#20851;&#65292;&#20182;&#20204;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#36807;&#21435;&#30456;&#23545;&#26356;&#22909;&#32467;&#26524;&#30340;&#36873;&#39033;&#12290;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20250;&#34920;&#29616;&#20986;&#31867;&#20284;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#35753;gpt-4-1106-preview(GPT-4 Turbo)&#21644;Llama-2-70B&#22312;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#30446;&#26631;&#19979;&#21453;&#22797;&#22312;&#36873;&#39033;&#23545;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#12290;&#27599;&#20010;&#25552;&#31034;&#20013;&#37117;&#21253;&#21547;&#20102;&#20808;&#21069;&#32467;&#26524;&#30340;&#23436;&#25972;&#35760;&#24405;&#12290;&#20004;&#20010;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#35266;&#23519;&#21040;&#30340;&#30456;&#23545;&#20215;&#20540;&#20915;&#31574;&#20559;&#24046;&#31867;&#20284;&#30340;&#34892;&#20026;&#12290;&#26356;&#26126;&#30830;&#22320;&#36827;&#34892;&#32467;&#26524;&#20043;&#38388;&#30340;&#30456;&#23545;&#27604;&#36739;&#20250;&#25918;&#22823;&#36825;&#31181;&#20559;&#24046;&#65292;&#32780;&#20419;&#20351;&#27169;&#22411;&#20272;&#35745;&#39044;&#26399;&#32467;&#26524;&#20250;&#20351;&#20559;&#24046;&#28040;&#22833;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#20102;&#35299;&#20154;&#31867;&#36873;&#25321;&#20013;&#36129;&#29486;&#21040;&#32972;&#26223;&#20381;&#36182;&#24615;&#30340;&#28508;&#22312;&#26426;&#21046;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studies of reinforcement learning in humans and animals have demonstrated a preference for options that yielded relatively better outcomes in the past, even when those options are associated with lower absolute reward. The present study tested whether large language models would exhibit a similar bias. We had gpt-4-1106-preview (GPT-4 Turbo) and Llama-2-70B make repeated choices between pairs of options with the goal of maximizing payoffs. A complete record of previous outcomes was included in each prompt. Both models exhibited relative value decision biases similar to those observed in humans and animals. Making relative comparisons among outcomes more explicit magnified the bias, whereas prompting the models to estimate expected outcomes caused the bias to disappear. These results have implications for the potential mechanisms that contribute to context-dependent choice in human agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#27700;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22312;&#20445;&#25345;&#31616;&#27905;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#27969;&#37327;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14521</link><description>&lt;p&gt;
&#20197;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;-&#27010;&#24565;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron. (arXiv:2401.14521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#27700;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22312;&#20445;&#25345;&#31616;&#27905;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#27969;&#37327;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#31616;&#27905;&#21487;&#35299;&#37322;&#30340;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#37319;&#29992;&#22522;&#20110;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#30340;&#26377;&#21521;&#22270;&#32467;&#26500;&#20316;&#20026;&#22522;&#26412;&#35745;&#31639;&#21333;&#20803;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#21333;&#20010;&#20301;&#32622;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#65288;&#28145;&#24230;&#65289;&#65292;&#32780;&#19981;&#26159;&#23545;&#22823;&#26679;&#26412;&#38598;&#27700;&#21306;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#24191;&#24230;&#12290;&#30446;&#26631;&#26159;&#21457;&#29616;&#19968;&#20010;&#26368;&#23567;&#30340;&#34920;&#31034;&#65288;&#21333;&#20803;&#29366;&#24577;&#25968;&#21644;&#27969;&#37327;&#36335;&#24452;&#25968;&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#33021;&#22815;&#35299;&#37322;&#32473;&#23450;&#38598;&#27700;&#21306;&#36755;&#20837;&#29366;&#24577;&#21644;&#36755;&#20986;&#34892;&#20026;&#30340;&#20027;&#35201;&#36807;&#31243;&#65292;&#29305;&#21035;&#24378;&#35843;&#27169;&#25311;&#20840;&#33539;&#22260;&#65288;&#39640;&#12289;&#20013;&#12289;&#20302;&#65289;&#30340;&#27969;&#37327;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#21306;&#22495;&#65292;&#37319;&#29992;&#31867;&#20284;HyMod&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;3&#20010;&#21333;&#20803;&#29366;&#24577;&#21644;2&#20010;&#20027;&#35201;&#27969;&#21160;&#36335;&#24452;&#65292;&#33021;&#22815;&#23454;&#29616;&#36825;&#26679;&#30340;&#34920;&#31034;&#65292;&#20294;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#27700;&#25991;&#22270;&#30340;&#26102;&#38388;&#21644;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the applicability of machine learning technologies to the development of parsimonious, interpretable, catchment-scale hydrologic models using directed-graph architectures based on the mass-conserving perceptron (MCP) as the fundamental computational unit. Here, we focus on architectural complexity (depth) at a single location, rather than universal applicability (breadth) across large samples of catchments. The goal is to discover a minimal representation (numbers of cell-states and flow paths) that represents the dominant processes that can explain the input-state-output behaviors of a given catchment, with particular emphasis given to simulating the full range (high, medium, and low) of flow dynamics. We find that a HyMod-like architecture with three cell-states and two major flow pathways achieves such a representation at our study location, but that the additional incorporation of an input-bypass mechanism significantly improves the timing and shape of the hydrograph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;Rashomon Set of Optimal Trees (ROOT)&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25551;&#36848;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#30340;&#23569;&#25968;&#20154;&#32676;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#26041;&#24046;&#26469;&#20248;&#21270;&#30446;&#26631;&#23376;&#32676;&#20307;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#31934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.14512</link><description>&lt;p&gt;
&#25105;&#20204;&#38169;&#36807;&#20102;&#35841;&#65311;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#25581;&#31034;&#23569;&#25968;&#20154;&#32676;&#29305;&#24449;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population. (arXiv:2401.14512v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;Rashomon Set of Optimal Trees (ROOT)&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25551;&#36848;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#30340;&#23569;&#25968;&#20154;&#32676;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#26041;&#24046;&#26469;&#20248;&#21270;&#30446;&#26631;&#23376;&#32676;&#20307;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#31934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#22312;&#29702;&#35299;&#22240;&#26524;&#25928;&#24212;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#32780;&#23558;&#25512;&#35770;&#25193;&#23637;&#21040;&#30446;&#26631;&#20154;&#32676;&#26102;&#38754;&#20020;&#25928;&#24212;&#24322;&#36136;&#24615;&#21644;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#35782;&#21035;&#21644;&#25551;&#36848;&#23569;&#25968;&#20154;&#32676;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30446;&#26631;&#20154;&#32676;&#20197;&#25552;&#21319;&#26222;&#36866;&#24615;&#30340;&#21019;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#8212;&#8212;Rashomon Set of Optimal Trees (ROOT)&#65292;&#26469;&#25551;&#36848;&#23569;&#25968;&#20154;&#32676;&#12290;ROOT&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#26041;&#24046;&#26469;&#20248;&#21270;&#30446;&#26631;&#23376;&#32676;&#20307;&#20998;&#24067;&#65292;&#20174;&#32780;&#30830;&#20445;&#26356;&#31934;&#30830;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ROOT&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#23569;&#25968;&#20154;&#32676;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#26377;&#25928;&#27807;&#36890;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#23637;&#29616;&#20102;&#25913;&#36827;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized controlled trials (RCTs) serve as the cornerstone for understanding causal effects, yet extending inferences to target populations presents challenges due to effect heterogeneity and underrepresentation. Our paper addresses the critical issue of identifying and characterizing underrepresented subgroups in RCTs, proposing a novel framework for refining target populations to improve generalizability. We introduce an optimization-based approach, Rashomon Set of Optimal Trees (ROOT), to characterize underrepresented groups. ROOT optimizes the target subpopulation distribution by minimizing the variance of the target average treatment effect estimate, ensuring more precise treatment effect estimations. Notably, ROOT generates interpretable characteristics of the underrepresented population, aiding researchers in effective communication. Our approach demonstrates improved precision and interpretability compared to alternatives, as illustrated with synthetic data experiments. We ap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#22522;&#20110;&#26694;&#26550;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#20915;&#23450;&#35266;&#27979;&#26102;&#38388;&#24182;&#20174;&#31232;&#30095;&#37319;&#26679;&#30340;&#35266;&#27979;&#20013;&#37325;&#24314;&#25968;&#25454;&#27969;&#65292;&#20197;&#23454;&#29616;&#22312;&#30005;&#21147;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;&#38271;&#26399;&#20132;&#36890;&#25968;&#25454;&#25910;&#38598;&#30340;&#26368;&#23567;&#24615;&#33021;&#25439;&#22833;&#21644;&#26174;&#33879;&#24310;&#38271;&#31995;&#32479;&#23551;&#21629;&#12290;</title><link>http://arxiv.org/abs/2401.14504</link><description>&lt;p&gt;
&#23398;&#20064;&#20309;&#26102;&#22312;&#30005;&#21147;&#21463;&#38480;&#35774;&#22791;&#19978;&#36827;&#34892;&#38271;&#26399;&#20132;&#36890;&#25968;&#25454;&#25910;&#38598;
&lt;/p&gt;
&lt;p&gt;
Learning When to See for Long-term Traffic Data Collection on Power-constrained Devices. (arXiv:2401.14504v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#22522;&#20110;&#26694;&#26550;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#20915;&#23450;&#35266;&#27979;&#26102;&#38388;&#24182;&#20174;&#31232;&#30095;&#37319;&#26679;&#30340;&#35266;&#27979;&#20013;&#37325;&#24314;&#25968;&#25454;&#27969;&#65292;&#20197;&#23454;&#29616;&#22312;&#30005;&#21147;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;&#38271;&#26399;&#20132;&#36890;&#25968;&#25454;&#25910;&#38598;&#30340;&#26368;&#23567;&#24615;&#33021;&#25439;&#22833;&#21644;&#26174;&#33879;&#24310;&#38271;&#31995;&#32479;&#23551;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#20132;&#36890;&#25968;&#25454;&#23545;&#20110;&#20132;&#36890;&#31995;&#32479;&#21644;&#22478;&#24066;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#36890;&#24120;&#26356;&#24076;&#26395;&#36890;&#36807;&#26131;&#20110;&#37096;&#32626;&#20294;&#30005;&#21147;&#21463;&#38480;&#30340;&#35774;&#22791;&#36827;&#34892;&#65292;&#36825;&#26159;&#30001;&#20110;&#30005;&#21147;&#21644;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#30340;&#19981;&#21487;&#29992;&#24615;&#25110;&#39640;&#25104;&#26412;&#25152;&#33268;&#12290;&#26377;&#38480;&#30340;&#30005;&#21147;&#24847;&#21619;&#30528;&#25968;&#25454;&#25910;&#38598;&#25345;&#32493;&#26102;&#38388;&#21644;&#20934;&#30830;&#24615;/&#20998;&#36776;&#29575;&#20043;&#38388;&#19981;&#21487;&#36991;&#20813;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#20915;&#23450;&#30005;&#27744;&#20379;&#30005;&#35774;&#22791;&#30340;&#35266;&#27979;&#26102;&#38388;&#65292;&#24182;&#20174;&#31232;&#30095;&#37319;&#26679;&#30340;&#35266;&#27979;&#20013;&#37325;&#24314;&#23436;&#25972;&#30340;&#25968;&#25454;&#27969;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#23567;&#30340;&#24615;&#33021;&#25439;&#22833;&#21644;&#26174;&#33879;&#24310;&#38271;&#31995;&#32479;&#23551;&#21629;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#39044;&#27979;&#22120;&#12289;&#25511;&#21046;&#22120;&#21644;&#20272;&#35745;&#22120;&#32452;&#25104;&#12290;&#39044;&#27979;&#22120;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#22266;&#23450;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#26410;&#26469;&#36235;&#21183;&#12290;&#25511;&#21046;&#22120;&#20351;&#29992;&#36825;&#20123;&#39044;&#27979;&#26469;&#30830;&#23450;&#19979;&#19968;&#20010;&#26368;&#20248;&#30340;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#12290;&#26368;&#21518;&#65292;&#20272;&#35745;&#22120;&#20174;&#37319;&#26679;&#35266;&#27979;&#20013;&#37325;&#24314;&#23436;&#25972;&#30340;&#25968;&#25454;&#37197;&#32622;&#25991;&#20214;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting traffic data is crucial for transportation systems and urban planning, and is often more desirable through easy-to-deploy but power-constrained devices, due to the unavailability or high cost of power and network infrastructure. The limited power means an inevitable trade-off between data collection duration and accuracy/resolution. We introduce a novel learning-based framework that strategically decides observation timings for battery-powered devices and reconstructs the full data stream from sparsely sampled observations, resulting in minimal performance loss and a significantly prolonged system lifetime. Our framework comprises a predictor, a controller, and an estimator. The predictor utilizes historical data to forecast future trends within a fixed time horizon. The controller uses the forecasts to determine the next optimal timing for data collection. Finally, the estimator reconstructs the complete data profile from the sampled observations. We evaluate the performanc
&lt;/p&gt;</description></item><item><title>MResT&#26159;&#19968;&#20010;&#22522;&#20110;&#22810;&#20998;&#36776;&#29575;&#24863;&#30693;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#26102;&#25511;&#21046;&#26694;&#26550;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#31895;&#31890;&#24230;&#21644;&#31934;&#30830;&#21160;&#20316;&#65292;&#24182;&#36890;&#36807;&#21512;&#29702;&#22320;&#21033;&#29992;&#19981;&#21516;&#30340;&#24863;&#30693;&#27169;&#24335;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14502</link><description>&lt;p&gt;
MResT: &#22810;&#20998;&#36776;&#29575;&#24863;&#30693;&#19982;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#26102;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
MResT: Multi-Resolution Sensing for Real-Time Control with Vision-Language Models. (arXiv:2401.14502v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14502
&lt;/p&gt;
&lt;p&gt;
MResT&#26159;&#19968;&#20010;&#22522;&#20110;&#22810;&#20998;&#36776;&#29575;&#24863;&#30693;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#26102;&#25511;&#21046;&#26694;&#26550;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#31895;&#31890;&#24230;&#21644;&#31934;&#30830;&#21160;&#20316;&#65292;&#24182;&#36890;&#36807;&#21512;&#29702;&#22320;&#21033;&#29992;&#19981;&#21516;&#30340;&#24863;&#30693;&#27169;&#24335;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#19981;&#21516;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#24863;&#30693;&#27169;&#24335;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#22810;&#31354;&#38388;&#20998;&#36776;&#29575;&#24863;&#30693;&#25552;&#20379;&#20102;&#22312;&#19981;&#21516;&#31354;&#38388;&#23610;&#24230;&#19978;&#25429;&#33719;&#30340;&#23618;&#27425;&#20449;&#24687;&#65292;&#24182;&#23454;&#29616;&#31895;&#31890;&#24230;&#21644;&#31934;&#30830;&#21160;&#20316;&#12290;&#21516;&#26102;&#65292;&#22810;&#26102;&#38388;&#20998;&#36776;&#29575;&#24863;&#30693;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#34920;&#29616;&#20986;&#39640;&#21453;&#24212;&#24615;&#21644;&#23454;&#26102;&#25511;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MResT&#65288;&#22810;&#20998;&#36776;&#29575;Transformer&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36890;&#29992;&#30340;&#35821;&#35328;&#26465;&#20214;&#30340;&#22810;&#20219;&#21153;&#31574;&#30053;&#65292;&#21033;&#29992;&#19981;&#21516;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#24863;&#30693;&#65292;&#20351;&#29992;&#19981;&#21516;&#23481;&#37327;&#30340;&#32593;&#32476;&#26469;&#26377;&#25928;&#22320;&#25191;&#34892;&#31934;&#30830;&#21644;&#21453;&#24212;&#24615;&#20219;&#21153;&#30340;&#23454;&#26102;&#25511;&#21046;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#22788;&#29702;&#20302;&#39057;&#20840;&#23616;&#29305;&#24449;&#65292;&#21516;&#26102;&#20351;&#29992;&#23567;&#22411;&#30340;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#36866;&#24212;&#39640;&#39057;&#26412;&#22320;&#21453;&#39304;&#12290;&#36890;&#36807;&#22312;&#31895;&#31890;&#24230;&#12289;&#31934;&#30830;&#21644;&#21160;&#24577;&#25805;&#20316;&#20219;&#21153;&#30340;&#19977;&#20010;&#39046;&#22495;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MResT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging sensing modalities across diverse spatial and temporal resolutions can improve performance of robotic manipulation tasks. Multi-spatial resolution sensing provides hierarchical information captured at different spatial scales and enables both coarse and precise motions. Simultaneously multi-temporal resolution sensing enables the agent to exhibit high reactivity and real-time control. In this work, we propose a framework, MResT (Multi-Resolution Transformer), for learning generalizable language-conditioned multi-task policies that utilize sensing at different spatial and temporal resolutions using networks of varying capacities to effectively perform real time control of precise and reactive tasks. We leverage off-the-shelf pretrained vision-language models to operate on low-frequency global features along with small non-pretrained models to adapt to high frequency local feedback. Through extensive experiments in 3 domains (coarse, precise and dynamic manipulation tasks), we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#31454;&#20105;&#39044;&#27979;&#21644;&#20998;&#31867;&#33021;&#21147;&#30340;&#28207;&#21475;&#36816;&#33829;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#33337;&#33334;&#22312;&#28207;&#21475;&#30340;&#24635;&#26102;&#38388;&#21644;&#24310;&#36831;&#26102;&#38388;&#65292;&#22635;&#34917;&#20102;&#28207;&#21475;&#20998;&#26512;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#30340;&#31354;&#30333;&#65292;&#24182;&#20026;&#28023;&#20107;&#29289;&#27969;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2401.14498</link><description>&lt;p&gt;
&#20248;&#21270;&#28207;&#21475;&#36816;&#33829;&#30340;&#39044;&#27979;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Predictive Analysis for Optimizing Port Operations. (arXiv:2401.14498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#31454;&#20105;&#39044;&#27979;&#21644;&#20998;&#31867;&#33021;&#21147;&#30340;&#28207;&#21475;&#36816;&#33829;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#33337;&#33334;&#22312;&#28207;&#21475;&#30340;&#24635;&#26102;&#38388;&#21644;&#24310;&#36831;&#26102;&#38388;&#65292;&#22635;&#34917;&#20102;&#28207;&#21475;&#20998;&#26512;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#30340;&#31354;&#30333;&#65292;&#24182;&#20026;&#28023;&#20107;&#29289;&#27969;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#36816;&#26159;&#36828;&#36317;&#31163;&#21644;&#22823;&#23447;&#36135;&#29289;&#36816;&#36755;&#30340;&#37325;&#35201;&#29289;&#27969;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36816;&#36755;&#27169;&#24335;&#20013;&#22797;&#26434;&#30340;&#35268;&#21010;&#32463;&#24120;&#21463;&#21040;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#22825;&#27668;&#26465;&#20214;&#12289;&#36135;&#29289;&#22810;&#26679;&#24615;&#21644;&#28207;&#21475;&#21160;&#24577;&#65292;&#23548;&#33268;&#25104;&#26412;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#20272;&#35745;&#33337;&#33334;&#22312;&#28207;&#21475;&#20572;&#30041;&#30340;&#24635;&#26102;&#38388;&#21644;&#28508;&#22312;&#24310;&#36831;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#20415;&#22312;&#28207;&#21475;&#36816;&#33829;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#35268;&#21010;&#21644;&#23433;&#25490;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#20855;&#26377;&#31454;&#20105;&#39044;&#27979;&#21644;&#20998;&#31867;&#33021;&#21147;&#30340;&#28207;&#21475;&#36816;&#33829;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20272;&#35745;&#33337;&#33334;&#30340;&#24635;&#26102;&#38388;&#21644;&#24310;&#36831;&#26102;&#38388;&#12290;&#35813;&#30740;&#31350;&#22635;&#34917;&#20102;&#28207;&#21475;&#20998;&#26512;&#27169;&#22411;&#22312;&#33337;&#33334;&#20572;&#30041;&#21644;&#24310;&#36831;&#26102;&#38388;&#26041;&#38754;&#30340;&#37325;&#35201;&#31354;&#30333;&#65292;&#20026;&#28023;&#20107;&#29289;&#27969;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36129;&#29486;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26088;&#22312;&#21327;&#21161;&#28207;&#21475;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#24182;&#39044;&#27979;&#26381;&#21153;&#24310;&#36831;&#12290;&#36890;&#36807;&#23545;&#24052;&#35199;&#28207;&#21475;&#30340;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#39564;&#35777;&#65292;&#21516;&#26102;&#20351;&#29992;&#29305;&#24449;&#20998;&#26512;&#26469;&#29702;&#35299;...
&lt;/p&gt;
&lt;p&gt;
Maritime transport is a pivotal logistics mode for the long-distance and bulk transportation of goods. However, the intricate planning involved in this mode is often hindered by uncertainties, including weather conditions, cargo diversity, and port dynamics, leading to increased costs. Consequently, accurately estimating vessel total (stay) time at port and potential delays becomes imperative for effective planning and scheduling in port operations. This study aims to develop a port operation solution with competitive prediction and classification capabilities for estimating vessel Total and Delay times. This research addresses a significant gap in port analysis models for vessel Stay and Delay times, offering a valuable contribution to the field of maritime logistics. The proposed solution is designed to assist decision-making in port environments and predict service delays. This is demonstrated through a case study on Brazil ports. Additionally, feature analysis is used to understand
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;DermaMNIST&#21644;Fitzpatrick17k&#30382;&#32932;&#31185;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#38382;&#39064;&#65292;&#23545;&#25968;&#25454;&#37325;&#22797;&#12289;&#25968;&#25454;&#27844;&#28431;&#12289;&#38169;&#35823;&#26631;&#35760;&#21644;&#32570;&#20047;&#27979;&#35797;&#20998;&#21306;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#32416;&#27491;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2401.14497</link><description>&lt;p&gt;
&#30740;&#31350;DermaMNIST&#21644;Fitzpatrick17k&#30382;&#32932;&#31185;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Investigating the Quality of DermaMNIST and Fitzpatrick17k Dermatological Image Datasets. (arXiv:2401.14497v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;DermaMNIST&#21644;Fitzpatrick17k&#30382;&#32932;&#31185;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#38382;&#39064;&#65292;&#23545;&#25968;&#25454;&#37325;&#22797;&#12289;&#25968;&#25454;&#27844;&#28431;&#12289;&#38169;&#35823;&#26631;&#35760;&#21644;&#32570;&#20047;&#27979;&#35797;&#20998;&#21306;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#32416;&#27491;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#30382;&#32932;&#31185;&#20219;&#21153;&#20013;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#20351;&#25105;&#20204;&#26356;&#25509;&#36817;&#20110;&#36798;&#21040;&#19982;&#20154;&#31867;&#19987;&#23478;&#30456;&#24403;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22823;&#22411;&#25968;&#25454;&#38598;&#22312;&#21487;&#38752;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24320;&#21457;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#25454;&#36136;&#37327;&#21644;&#20854;&#27491;&#30830;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22810;&#31181;&#22240;&#32032;&#21487;&#20197;&#24433;&#21709;&#25968;&#25454;&#36136;&#37327;&#65292;&#22914;&#37325;&#22797;&#25968;&#25454;&#30340;&#23384;&#22312;&#65292;&#35757;&#32451;-&#27979;&#35797;&#20998;&#21306;&#30340;&#25968;&#25454;&#27844;&#28431;&#65292;&#38169;&#35823;&#26631;&#35760;&#30340;&#22270;&#20687;&#20197;&#21450;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#30340;&#27979;&#35797;&#20998;&#21306;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#27969;&#34892;&#30340;&#30382;&#32932;&#31185;&#22270;&#20687;&#25968;&#25454;&#38598;DermaMNIST&#21644;Fitzpatrick17k&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65292;&#27979;&#37327;&#20102;&#36825;&#20123;&#38382;&#39064;&#23545;&#22522;&#20934;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#23545;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#32416;&#27491;&#25514;&#26045;&#12290;&#36890;&#36807;&#20844;&#24320;&#25105;&#20204;&#30340;&#20998;&#26512;&#27969;&#31243;&#21644;&#37197;&#22871;&#20195;&#30721;&#65292;&#30830;&#20445;&#25105;&#20204;&#20998;&#26512;&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#26088;&#22312;&#40723;&#21169;&#31867;&#20284;&#30340;&#25506;&#32034;&#24182;&#20419;&#36827;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable progress of deep learning in dermatological tasks has brought us closer to achieving diagnostic accuracies comparable to those of human experts. However, while large datasets play a crucial role in the development of reliable deep neural network models, the quality of data therein and their correct usage are of paramount importance. Several factors can impact data quality, such as the presence of duplicates, data leakage across train-test partitions, mislabeled images, and the absence of a well-defined test partition. In this paper, we conduct meticulous analyses of two popular dermatological image datasets: DermaMNIST and Fitzpatrick17k, uncovering these data quality issues, measure the effects of these problems on the benchmark results, and propose corrections to the datasets. Besides ensuring the reproducibility of our analysis, by making our analysis pipeline and the accompanying code publicly available, we aim to encourage similar explorations and to facilitate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;K-QA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1212&#20010;&#30495;&#23454;&#19990;&#30028;&#21307;&#30103;&#23545;&#35805;&#20013;&#30340;&#24739;&#32773;&#38382;&#39064;&#65292;&#24182;&#32856;&#35831;&#20869;&#37096;&#21307;&#29983;&#22238;&#31572;&#21644;&#20998;&#35299;&#12290;&#30740;&#31350;&#36824;&#21046;&#23450;&#20102;&#20004;&#20010;&#22522;&#20110;NLI&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#21484;&#22238;&#29575;&#21644;&#31934;&#30830;&#24230;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#29615;&#22659;&#19979;&#30340;&#20934;&#30830;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.14493</link><description>&lt;p&gt;
K-QA&#65306;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21307;&#30103;&#38382;&#31572;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
K-QA: A Real-World Medical Q&amp;A Benchmark. (arXiv:2401.14493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;K-QA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1212&#20010;&#30495;&#23454;&#19990;&#30028;&#21307;&#30103;&#23545;&#35805;&#20013;&#30340;&#24739;&#32773;&#38382;&#39064;&#65292;&#24182;&#32856;&#35831;&#20869;&#37096;&#21307;&#29983;&#22238;&#31572;&#21644;&#20998;&#35299;&#12290;&#30740;&#31350;&#36824;&#21046;&#23450;&#20102;&#20004;&#20010;&#22522;&#20110;NLI&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#21484;&#22238;&#29575;&#21644;&#31934;&#30830;&#24230;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#29615;&#22659;&#19979;&#30340;&#20934;&#30830;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#30340;&#22238;&#31572;&#20934;&#30830;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#65292;&#38169;&#35823;&#30340;&#20449;&#24687;&#21487;&#33021;&#30452;&#25509;&#24433;&#21709;&#24739;&#32773;&#20581;&#24247;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;K-QA&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;1212&#20010;&#30001;K Health&#65288;&#19968;&#23478;AI&#39537;&#21160;&#30340;&#20020;&#24202;&#24179;&#21488;&#65289;&#19978;&#30340;&#30495;&#23454;&#23545;&#35805;&#20013;&#30340;&#24739;&#32773;&#38382;&#39064;&#12290;&#25105;&#20204;&#32856;&#35831;&#19968;&#32452;&#20869;&#37096;&#21307;&#29983;&#26469;&#22238;&#31572;&#24182;&#25163;&#21160;&#20998;&#35299;K-QA&#30340;&#23376;&#38598;&#20026;&#33258;&#21253;&#21547;&#30340;&#38472;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20004;&#20010;&#22522;&#20110;NLI&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36817;&#20284;&#20110;&#21484;&#22238;&#29575;&#21644;&#31934;&#30830;&#24230;&#65306;&#65288;1&#65289;&#20840;&#38754;&#24615;&#65292;&#34913;&#37327;&#29983;&#25104;&#22238;&#31572;&#20013;&#25152;&#21547;&#30340;&#22522;&#26412;&#20020;&#24202;&#20449;&#24687;&#30340;&#30334;&#20998;&#27604;&#65292;&#65288;2&#65289;&#34394;&#26500;&#29575;&#65292;&#34913;&#37327;LLM&#22238;&#31572;&#25152;&#30683;&#30462;&#30340;&#21307;&#29983;&#31574;&#21010;&#22238;&#22797;&#20013;&#30340;&#38472;&#36848;&#25968;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;K-QA&#21644;&#36825;&#20123;&#25351;&#26631;&#26469;&#35780;&#20272;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#21307;&#23398;&#23548;&#21521;&#22686;&#24378;&#26816;&#32034;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring the accuracy of responses provided by large language models (LLMs) is crucial, particularly in clinical settings where incorrect information may directly impact patient health. To address this challenge, we construct K-QA, a dataset containing 1,212 patient questions originating from real-world conversations held on K Health (an AI-driven clinical platform). We employ a panel of in-house physicians to answer and manually decompose a subset of K-QA into self-contained statements. Additionally, we formulate two NLI-based evaluation metrics approximating recall and precision: (1) comprehensiveness, measuring the percentage of essential clinical information in the generated answer and (2) hallucination rate, measuring the number of statements from the physician-curated response contradicted by the LLM answer. Finally, we use K-QA along with these metrics to evaluate several state-of-the-art models, as well as the effect of in-context learning and medically-oriented augmented retri
&lt;/p&gt;</description></item><item><title>Scilab-RL&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#35748;&#30693;&#24314;&#27169;&#21644;&#22686;&#24378;&#23398;&#20064;&#30740;&#31350;&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31283;&#23450;&#30340;&#22522;&#32447;3&#21644;OpenAI gym&#25509;&#21475;&#65292;&#20197;&#21450;&#23454;&#39564;&#21487;&#35270;&#21270;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#21151;&#33021;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#20102;&#30740;&#31350;&#20135;&#20986;&#12290;</title><link>http://arxiv.org/abs/2401.14488</link><description>&lt;p&gt;
Scilab-RL&#65306;&#29992;&#20110;&#39640;&#25928;&#22686;&#24378;&#23398;&#20064;&#21644;&#35748;&#30693;&#24314;&#27169;&#30740;&#31350;&#30340;&#36719;&#20214;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Scilab-RL: A software framework for efficient reinforcement learning and cognitive modeling research. (arXiv:2401.14488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14488
&lt;/p&gt;
&lt;p&gt;
Scilab-RL&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#35748;&#30693;&#24314;&#27169;&#21644;&#22686;&#24378;&#23398;&#20064;&#30740;&#31350;&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31283;&#23450;&#30340;&#22522;&#32447;3&#21644;OpenAI gym&#25509;&#21475;&#65292;&#20197;&#21450;&#23454;&#39564;&#21487;&#35270;&#21270;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#21151;&#33021;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#20102;&#30740;&#31350;&#20135;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35748;&#30693;&#24314;&#27169;&#21644;&#22686;&#24378;&#23398;&#20064;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#30740;&#31350;&#20154;&#21592;&#33457;&#36153;&#22826;&#22810;&#26102;&#38388;&#26469;&#35774;&#32622;&#36866;&#24403;&#30340;&#35745;&#31639;&#26694;&#26550;&#36827;&#34892;&#23454;&#39564;&#12290;&#23384;&#22312;&#35768;&#22810;&#24403;&#21069;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#30340;&#24320;&#28304;&#23454;&#29616;&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#24037;&#20855;&#22871;&#20214;&#65292;&#32467;&#21512;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#21644;&#24179;&#21488;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#12289;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#22522;&#20934;&#23454;&#39564;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Scilab-RL&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#35748;&#30693;&#24314;&#27169;&#21644;&#22686;&#24378;&#23398;&#20064;&#30740;&#31350;&#30340;&#36719;&#20214;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#20351;&#29992;&#31283;&#23450;&#30340;&#22522;&#32447;3&#21644;OpenAI gym&#25509;&#21475;&#36827;&#34892;&#30446;&#26631;&#26465;&#20214;&#22686;&#24378;&#23398;&#20064;&#12290;&#23427;&#25552;&#20379;&#20102;&#21407;&#29983;&#30340;&#23454;&#39564;&#21487;&#35270;&#21270;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#36825;&#20123;&#21151;&#33021;&#22914;&#20309;&#20351;&#30740;&#31350;&#20154;&#21592;&#21482;&#38656;&#26368;&#23569;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#23601;&#33021;&#36827;&#34892;&#23454;&#39564;&#65292;&#20174;&#32780;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#30740;&#31350;&#20135;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
One problem with researching cognitive modeling and reinforcement learning (RL) is that researchers spend too much time on setting up an appropriate computational framework for their experiments. Many open source implementations of current RL algorithms exist, but there is a lack of a modular suite of tools combining different robotic simulators and platforms, data visualization, hyperparameter optimization, and baseline experiments. To address this problem, we present Scilab-RL, a software framework for efficient research in cognitive modeling and reinforcement learning for robotic agents. The framework focuses on goal-conditioned reinforcement learning using Stable Baselines 3 and the OpenAI gym interface. It enables native possibilities for experiment visualizations and hyperparameter optimization. We describe how these features enable researchers to conduct experiments with minimal time effort, thus maximizing research output.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CloudTracks&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;3560&#24352;&#24102;&#26377;&#36229;&#36807;12000&#20010;&#33337;&#33334;&#36335;&#24452;&#23454;&#20363;&#27880;&#37322;&#30340;&#21355;&#26143;&#22270;&#20687;&#12290;&#20316;&#32773;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#20182;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.14486</link><description>&lt;p&gt;
CloudTracks: &#29992;&#20110;&#23450;&#20301;&#20113;&#23618;&#20013;&#33337;&#33334;&#36335;&#24452;&#30340;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CloudTracks: A Dataset for Localizing Ship Tracks in Satellite Images of Clouds. (arXiv:2401.14486v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14486
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CloudTracks&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;3560&#24352;&#24102;&#26377;&#36229;&#36807;12000&#20010;&#33337;&#33334;&#36335;&#24452;&#23454;&#20363;&#27880;&#37322;&#30340;&#21355;&#26143;&#22270;&#20687;&#12290;&#20316;&#32773;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#20182;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#34892;&#26143;&#21453;&#29031;&#29575;&#30340;&#24433;&#21709;&#65292;&#20113;&#23618;&#22312;&#20840;&#29699;&#28201;&#24230;&#35843;&#33410;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#20154;&#20026;&#27668;&#28342;&#33014;&#25490;&#25918;&#21487;&#20197;&#25913;&#21464;&#20113;&#23618;&#30340;&#21453;&#29031;&#29575;&#65292;&#20294;&#36825;&#31181;&#24433;&#21709;&#30340;&#31243;&#24230;&#20197;&#21450;&#23545;&#28201;&#24230;&#21464;&#21270;&#30340;&#24433;&#21709;&#20173;&#19981;&#30830;&#23450;&#12290;&#30001;&#33337;&#33334;&#27668;&#28342;&#33014;&#25490;&#25918;&#24341;&#36215;&#30340;&#20154;&#20026;&#20113;&#23618;&#65292;&#36890;&#24120;&#31216;&#20026;&#33337;&#33334;&#36335;&#24452;&#65292;&#25552;&#20379;&#20102;&#19982;&#30456;&#37051;&#20113;&#23618;&#21306;&#22495;&#26377;&#25152;&#19981;&#21516;&#30340;&#21487;&#35265;&#25928;&#26524;&#65292;&#22240;&#27492;&#21487;&#29992;&#20316;&#30740;&#31350;&#20154;&#20026;&#20113;&#23618;&#30340;&#26377;&#29992;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#22823;&#35268;&#27169;&#33337;&#33334;&#36335;&#24452;&#25968;&#25454;&#20351;&#24471;&#24456;&#38590;&#25512;&#27979;&#20854;&#23545;&#20113;&#23618;&#24418;&#25104;&#30340;&#24635;&#20307;&#24433;&#21709;&#12290;&#20026;&#20102;&#24320;&#21457;&#33258;&#21160;&#23450;&#20301;&#33337;&#33334;&#36335;&#24452;&#30340;&#22823;&#35268;&#27169;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CloudTracks&#65292;&#19968;&#20010;&#21253;&#21547;3560&#20010;&#21355;&#26143;&#22270;&#20687;&#24182;&#26631;&#35760;&#20102;12000&#22810;&#20010;&#33337;&#33334;&#36335;&#24452;&#23454;&#20363;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#22522;&#32447;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#20808;&#21069;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clouds play a significant role in global temperature regulation through their effect on planetary albedo. Anthropogenic emissions of aerosols can alter the albedo of clouds, but the extent of this effect, and its consequent impact on temperature change, remains uncertain. Human-induced clouds caused by ship aerosol emissions, commonly referred to as ship tracks, provide visible manifestations of this effect distinct from adjacent cloud regions and therefore serve as a useful sandbox to study human-induced clouds. However, the lack of large-scale ship track data makes it difficult to deduce their general effects on cloud formation. Towards developing automated approaches to localize ship tracks at scale, we present CloudTracks, a dataset containing 3,560 satellite images labeled with more than 12,000 ship track instance annotations. We train semantic segmentation and instance segmentation model baselines on our dataset and find that our best model substantially outperforms previous stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#26657;&#20934;&#21644;&#36951;&#25022;&#22312;&#35780;&#20272;&#39044;&#27979;&#20013;&#30340;&#27010;&#24565;&#31561;&#20215;&#24615;&#65292;&#23558;&#35780;&#20272;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#39044;&#27979;&#32773;&#12289;&#19968;&#20010;&#36172;&#24466;&#21644;&#33258;&#28982;&#20043;&#38388;&#30340;&#21338;&#24328;&#65292;&#24182;&#23558;&#39044;&#27979;&#30340;&#35780;&#20272;&#19982;&#32467;&#26524;&#30340;&#38543;&#26426;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2401.14483</link><description>&lt;p&gt;
&#39044;&#27979;&#30340;&#22235;&#20010;&#26041;&#38754;&#65306;&#26657;&#20934;&#12289;&#39044;&#27979;&#24615;&#12289;&#38543;&#26426;&#24615;&#21644;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Four Facets of Forecast Felicity: Calibration, Predictiveness, Randomness and Regret. (arXiv:2401.14483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#26657;&#20934;&#21644;&#36951;&#25022;&#22312;&#35780;&#20272;&#39044;&#27979;&#20013;&#30340;&#27010;&#24565;&#31561;&#20215;&#24615;&#65292;&#23558;&#35780;&#20272;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#39044;&#27979;&#32773;&#12289;&#19968;&#20010;&#36172;&#24466;&#21644;&#33258;&#28982;&#20043;&#38388;&#30340;&#21338;&#24328;&#65292;&#24182;&#23558;&#39044;&#27979;&#30340;&#35780;&#20272;&#19982;&#32467;&#26524;&#30340;&#38543;&#26426;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26159;&#20851;&#20110;&#39044;&#27979;&#30340;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#21482;&#26377;&#32463;&#36807;&#35780;&#20272;&#21518;&#25165;&#20855;&#26377;&#20854;&#26377;&#29992;&#24615;&#12290;&#26426;&#22120;&#23398;&#20064;&#20256;&#32479;&#19978;&#20851;&#27880;&#25439;&#22833;&#31867;&#22411;&#21450;&#20854;&#30456;&#24212;&#30340;&#36951;&#25022;&#12290;&#30446;&#21069;&#65292;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#37325;&#26032;&#23545;&#26657;&#20934;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26657;&#20934;&#21644;&#36951;&#25022;&#22312;&#35780;&#20272;&#39044;&#27979;&#20013;&#30340;&#27010;&#24565;&#31561;&#20215;&#24615;&#12290;&#25105;&#20204;&#23558;&#35780;&#20272;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#39044;&#27979;&#32773;&#12289;&#19968;&#20010;&#36172;&#24466;&#21644;&#33258;&#28982;&#20043;&#38388;&#30340;&#21338;&#24328;&#12290;&#36890;&#36807;&#23545;&#36172;&#24466;&#21644;&#39044;&#27979;&#32773;&#26045;&#21152;&#30452;&#35266;&#30340;&#38480;&#21046;&#65292;&#26657;&#20934;&#21644;&#36951;&#25022;&#33258;&#28982;&#22320;&#25104;&#20026;&#20102;&#36825;&#20010;&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#21338;&#24328;&#23558;&#39044;&#27979;&#30340;&#35780;&#20272;&#19982;&#32467;&#26524;&#30340;&#38543;&#26426;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#30456;&#23545;&#20110;&#39044;&#27979;&#32780;&#35328;&#65292;&#32467;&#26524;&#30340;&#38543;&#26426;&#24615;&#31561;&#21516;&#20110;&#20851;&#20110;&#32467;&#26524;&#30340;&#22909;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#31216;&#36825;&#20004;&#20010;&#26041;&#38754;&#20026;&#26657;&#20934;&#21644;&#36951;&#25022;&#12289;&#39044;&#27979;&#24615;&#21644;&#38543;&#26426;&#24615;&#65292;&#21363;&#39044;&#27979;&#30340;&#22235;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is about forecasting. Forecasts, however, obtain their usefulness only through their evaluation. Machine learning has traditionally focused on types of losses and their corresponding regret. Currently, the machine learning community regained interest in calibration. In this work, we show the conceptual equivalence of calibration and regret in evaluating forecasts. We frame the evaluation problem as a game between a forecaster, a gambler and nature. Putting intuitive restrictions on gambler and forecaster, calibration and regret naturally fall out of the framework. In addition, this game links evaluation of forecasts to randomness of outcomes. Random outcomes with respect to forecasts are equivalent to good forecasts with respect to outcomes. We call those dual aspects, calibration and regret, predictiveness and randomness, the four facets of forecast felicity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#35757;&#32451;&#30340;&#21367;&#31215;&#26680;&#20013;&#20986;&#29616;&#30340;&#21487;&#36776;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#31867;&#20284;&#20110;&#39640;&#26031;&#24046;&#20998;&#20989;&#25968;&#21644;&#23427;&#20204;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#23548;&#25968;&#12290;&#30740;&#31350;&#36890;&#36807;&#23545;&#25968;&#30334;&#19975;&#20010;&#35757;&#32451;&#28388;&#27874;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#25104;&#21151;&#23558;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20013;&#30340;&#22823;&#37096;&#20998;&#28388;&#27874;&#22120;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.14469</link><description>&lt;p&gt;
&#25581;&#31034;&#30475;&#19981;&#35265;&#30340;&#65306;&#35757;&#32451;&#30340;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#26680;&#20013;&#30340;&#21487;&#35782;&#21035;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels. (arXiv:2401.14469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#35757;&#32451;&#30340;&#21367;&#31215;&#26680;&#20013;&#20986;&#29616;&#30340;&#21487;&#36776;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#31867;&#20284;&#20110;&#39640;&#26031;&#24046;&#20998;&#20989;&#25968;&#21644;&#23427;&#20204;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#23548;&#25968;&#12290;&#30740;&#31350;&#36890;&#36807;&#23545;&#25968;&#30334;&#19975;&#20010;&#35757;&#32451;&#28388;&#27874;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#25104;&#21151;&#23558;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20013;&#30340;&#22823;&#37096;&#20998;&#28388;&#27874;&#22120;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(DS-CNNs)&#30340;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#20102;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#26174;&#30528;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#24046;&#36317;&#36229;&#36234;&#20102;&#32463;&#20856;CNNs&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;DS-CNN&#26550;&#26500;&#30340;&#21478;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24615;&#65306;&#22312;&#20854;&#25152;&#26377;&#23618;&#30340;&#35757;&#32451;&#28145;&#24230;&#21367;&#31215;&#26680;&#20013;&#20986;&#29616;&#20102;&#21487;&#36776;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#24335;&#12290;&#36890;&#36807;&#23545;&#25968;&#30334;&#19975;&#20010;&#19981;&#21516;&#22823;&#23567;&#21644;&#26469;&#33258;&#21508;&#31181;&#27169;&#22411;&#30340;&#35757;&#32451;&#28388;&#27874;&#22120;&#30340;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#23558;&#36825;&#20123;&#28388;&#27874;&#22120;&#20998;&#31867;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#24335;&#25910;&#25947;&#25104;&#20960;&#20010;&#20027;&#35201;&#30340;&#32858;&#31867;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advances in depthwise-separable convolutional neural networks (DS-CNNs) have led to novel architectures, that surpass the performance of classical CNNs, by a considerable scalability and accuracy margin. This paper reveals another striking property of DS-CNN architectures: discernible and explainable patterns emerge in their trained depthwise convolutional kernels in all layers. Through an extensive analysis of millions of trained filters, with different sizes and from various models, we employed unsupervised clustering with autoencoders, to categorize these filters. Astonishingly, the patterns converged into a few main clusters, each resembling the difference of Gaussian (DoG) functions, and their first and second-order derivatives. Notably, we were able to classify over 95\% and 90\% of the filters from state-of-the-art ConvNextV2 and ConvNeXt models, respectively. This finding is not merely a technological curiosity; it echoes the foundational models neuroscientists have long
&lt;/p&gt;</description></item><item><title>Marabou 2.0&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#20998;&#26512;&#22120;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#24341;&#20837;&#30340;&#20027;&#35201;&#21151;&#33021;&#21644;&#32452;&#20214;&#12290;</title><link>http://arxiv.org/abs/2401.14461</link><description>&lt;p&gt;
Marabou 2.0: &#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#20998;&#26512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Marabou 2.0: A Versatile Formal Analyzer of Neural Networks. (arXiv:2401.14461v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14461
&lt;/p&gt;
&lt;p&gt;
Marabou 2.0&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#20998;&#26512;&#22120;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#24341;&#20837;&#30340;&#20027;&#35201;&#21151;&#33021;&#21644;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#20851;&#20110;Marabou&#26694;&#26550;2.0&#29256;&#26412;&#30340;&#32508;&#21512;&#31995;&#32479;&#25551;&#36848;&#65292;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#20998;&#26512;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#24037;&#20855;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#24182;&#20171;&#32461;&#20102;&#33258;&#21021;&#22987;&#21457;&#24067;&#20197;&#26469;&#24341;&#20837;&#30340;&#20027;&#35201;&#21151;&#33021;&#21644;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper serves as a comprehensive system description of version 2.0 of the Marabou framework for formal analysis of neural networks. We discuss the tool's architectural design and highlight the major features and components introduced since its initial release.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Wordflow&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#24335;&#35753;&#38750;&#19987;&#23478;&#29992;&#25143;&#26356;&#22909;&#22320;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#21019;&#24314;&#12289;&#36816;&#34892;&#12289;&#20849;&#20139;&#21644;&#21457;&#29616;LLM&#25552;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#32593;&#32476;&#25216;&#26415;&#65292;Wordflow&#20801;&#35768;&#29992;&#25143;&#22312;&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#21644;&#31169;&#19979;&#36816;&#34892;LLM&#12290;</title><link>http://arxiv.org/abs/2401.14447</link><description>&lt;p&gt;
Wordflow: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Wordflow: Social Prompt Engineering for Large Language Models. (arXiv:2401.14447v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Wordflow&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#24335;&#35753;&#38750;&#19987;&#23478;&#29992;&#25143;&#26356;&#22909;&#22320;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#21019;&#24314;&#12289;&#36816;&#34892;&#12289;&#20849;&#20139;&#21644;&#21457;&#29616;LLM&#25552;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#32593;&#32476;&#25216;&#26415;&#65292;Wordflow&#20801;&#35768;&#29992;&#25143;&#22312;&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#21644;&#31169;&#19979;&#36816;&#34892;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#25165;&#33021;&#26377;&#25928;&#20351;&#29992;&#12290;&#23545;&#20110;&#38750;&#19987;&#23478;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36807;&#31243;&#65292;&#22240;&#20026;&#20182;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19981;&#37027;&#20040;&#29087;&#24713;&#12290;&#34429;&#28982;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#21644;&#24037;&#20855;&#26469;&#24110;&#21161;LLM&#29992;&#25143;&#35774;&#35745;&#25552;&#31034;&#65292;&#20294;&#36825;&#20123;&#20316;&#21697;&#20027;&#35201;&#38024;&#23545;&#30340;&#26159;AI&#24212;&#29992;&#24320;&#21457;&#32773;&#32780;&#19981;&#26159;&#38750;&#19987;&#23478;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#31038;&#20132;&#35745;&#31639;&#25216;&#26415;&#20419;&#36827;&#21327;&#20316;&#25552;&#31034;&#35774;&#35745;&#30340;&#26032;&#33539;&#24335;&#12290;&#20026;&#20102;&#30740;&#31350;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Wordflow&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#31038;&#20132;&#25991;&#26412;&#32534;&#36753;&#22120;&#65292;&#20351;&#26222;&#36890;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#21019;&#24314;&#12289;&#36816;&#34892;&#12289;&#20849;&#20139;&#21644;&#21457;&#29616;LLM&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#32593;&#32476;&#25216;&#26415;&#65292;Wordflow&#20801;&#35768;&#29992;&#25143;&#22312;&#20854;&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#21644;&#31169;&#19979;&#36816;&#34892;LLM&#12290;&#20004;&#20010;&#20351;&#29992;&#22330;&#26223;&#31361;&#20986;&#20102;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;&#21644;&#25105;&#20204;&#30340;&#24037;&#20855;&#22914;&#20309;&#22686;&#24378;&#26222;&#36890;&#20154;&#19982;LLM&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) require well-crafted prompts for effective use. Prompt engineering, the process of designing prompts, is challenging, particularly for non-experts who are less familiar with AI technologies. While researchers have proposed techniques and tools to assist LLM users in prompt design, these works primarily target AI application developers rather than non-experts. To address this research gap, we propose social prompt engineering, a novel paradigm that leverages social computing techniques to facilitate collaborative prompt design. To investigate social prompt engineering, we introduce Wordflow, an open-source and social text editor that enables everyday users to easily create, run, share, and discover LLM prompts. Additionally, by leveraging modern web technologies, Wordflow allows users to run LLMs locally and privately in their browsers. Two usage scenarios highlight how social prompt engineering and our tool can enhance laypeople's interaction with LLMs. Wor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#19987;&#21033;&#25968;&#25454;&#25552;&#39640;&#20102;&#25239;&#20307;&#20154;&#24615;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#22810;&#38454;&#27573;&#12289;&#22810;&#25439;&#22833;&#30340;&#35757;&#32451;&#36807;&#31243;&#20197;&#21450;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#39044;&#27979;&#20102;&#25239;&#20307;&#24207;&#21015;&#30340;&#20154;&#24615;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2401.14442</link><description>&lt;p&gt;
&#21033;&#29992;&#19987;&#21033;&#25968;&#25454;&#25552;&#39640;&#25239;&#20307;&#20154;&#24615;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Antibody Humanness Prediction using Patent Data. (arXiv:2401.14442v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#19987;&#21033;&#25968;&#25454;&#25552;&#39640;&#20102;&#25239;&#20307;&#20154;&#24615;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#22810;&#38454;&#27573;&#12289;&#22810;&#25439;&#22833;&#30340;&#35757;&#32451;&#36807;&#31243;&#20197;&#21450;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#39044;&#27979;&#20102;&#25239;&#20307;&#24207;&#21015;&#30340;&#20154;&#24615;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#19987;&#21033;&#25968;&#25454;&#26469;&#25552;&#39640;&#25239;&#20307;&#20154;&#24615;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;&#20102;&#22810;&#38454;&#27573;&#12289;&#22810;&#25439;&#22833;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#25239;&#20307;&#20154;&#24615;&#20316;&#20026;&#23545;&#25239;&#20307;&#27835;&#30103;&#30340;&#20813;&#30123;&#21453;&#24212;&#30340;&#20195;&#29702;&#65292;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;&#25239;&#20307;&#27835;&#30103;&#38754;&#20020;&#30528;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#23558;&#21021;&#22987;&#23398;&#20064;&#38454;&#27573;&#35270;&#20026;&#19968;&#20010;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#38382;&#39064;&#65292;&#27599;&#20010;&#25239;&#20307;&#24207;&#21015;&#19982;&#21487;&#33021;&#26377;&#22810;&#20010;&#21151;&#33021;&#26631;&#35782;&#31526;&#30456;&#20851;&#32852;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#26681;&#25454;&#20854;&#19987;&#21033;&#23646;&#24615;&#23558;&#23427;&#20204;&#20998;&#32452;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20923;&#32467;&#23545;&#27604;&#32534;&#30721;&#22120;&#30340;&#19968;&#37096;&#20998;&#65292;&#24182;&#32487;&#32493;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#22312;&#19987;&#21033;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#32473;&#23450;&#25239;&#20307;&#24207;&#21015;&#30340;&#20154;&#24615;&#35780;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#19981;&#21516;&#30340;&#20813;&#30123;&#21407;&#24615;&#25968;&#25454;&#38598;&#36827;&#34892;&#25512;&#29702;&#65292;&#23637;&#31034;&#20102;&#19987;&#21033;&#25968;&#25454;&#21644;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;l
&lt;/p&gt;
&lt;p&gt;
We investigate the potential of patent data for improving the antibody humanness prediction using a multi-stage, multi-loss training process. Humanness serves as a proxy for the immunogenic response to antibody therapeutics, one of the major causes of attrition in drug discovery and a challenging obstacle for their use in clinical settings. We pose the initial learning stage as a weakly-supervised contrastive-learning problem, where each antibody sequence is associated with possibly multiple identifiers of function and the objective is to learn an encoder that groups them according to their patented properties. We then freeze a part of the contrastive encoder and continue training it on the patent data using the cross-entropy loss to predict the humanness score of a given antibody sequence. We illustrate the utility of the patent data and our approach by performing inference on three different immunogenicity datasets, unseen during training. Our empirical results demonstrate that the l
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;NLI&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#35821;&#20041;&#20445;&#25345;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#23548;&#33268;&#25512;&#26029;&#32467;&#26524;&#19981;&#19968;&#33268;&#12290;&#20854;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#29702;&#35299;&#19981;&#21516;&#65292;&#36825;&#23545;&#24403;&#21069;NLI&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.14440</link><description>&lt;p&gt;
&#35821;&#20041;&#25935;&#24863;&#24615;&#21644;&#19981;&#19968;&#33268;&#30340;&#39044;&#27979;&#65306;&#34913;&#37327;NLI&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models. (arXiv:2401.14440v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14440
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;NLI&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#35821;&#20041;&#20445;&#25345;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#23548;&#33268;&#25512;&#26029;&#32467;&#26524;&#19981;&#19968;&#33268;&#12290;&#20854;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#29702;&#35299;&#19981;&#21516;&#65292;&#36825;&#23545;&#24403;&#21069;NLI&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22522;&#20110;transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22411;&#30340;&#26032;&#33021;&#21147;&#36827;&#34892;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#20855;&#22791;&#23545;&#35789;&#27719;&#21644;&#32452;&#21512;&#35821;&#20041;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#36825;&#20123;&#35828;&#27861;&#24212;&#35813;&#25345;&#20445;&#30041;&#24577;&#24230;&#65306;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#20445;&#30041;&#35821;&#20041;&#30340;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#25935;&#24863;&#65292;&#36825;&#23548;&#33268;&#25512;&#26029;&#36807;&#31243;&#20013;&#20986;&#29616;&#22823;&#37327;&#19981;&#19968;&#33268;&#30340;&#27169;&#22411;&#20915;&#31574;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#21644;&#28145;&#20837;&#29702;&#35299;&#19981;&#21516;&#65292;&#32780;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#27169;&#22411;&#20934;&#30830;&#24230;&#25110;&#25506;&#31350;&#21477;&#27861;&#12289;&#21333;&#35843;&#24615;&#21644;&#36923;&#36753;&#40065;&#26834;&#24615;&#25512;&#29702;&#26102;&#22343;&#19981;&#20250;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;&#35821;&#20041;&#25935;&#24863;&#24615;&#30340;&#31243;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#21547;&#26377;&#24494;&#23567;&#20445;&#30041;&#35821;&#20041;&#30340;&#34920;&#38754;&#24418;&#24335;&#36755;&#20837;&#22122;&#22768;&#30340;&#23545;&#25239;&#29983;&#25104;&#26679;&#20363;&#26469;&#35780;&#20272;NLI&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies of the emergent capabilities of transformer-based Natural Language Understanding (NLU) models have indicated that they have an understanding of lexical and compositional semantics. We provide evidence that suggests these claims should be taken with a grain of salt: we find that state-of-the-art Natural Language Inference (NLI) models are sensitive towards minor semantics preserving surface-form variations, which lead to sizable inconsistent model decisions during inference. Notably, this behaviour differs from valid and in-depth comprehension of compositional semantics, however does neither emerge when evaluating model accuracy on standard benchmarks nor when probing for syntactic, monotonic, and logically robust reasoning. We propose a novel framework to measure the extent of semantic sensitivity. To this end, we evaluate NLI models on adversarially generated examples containing minor semantics-preserving surface-form input noise. This is achieved using conditional text
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31751;&#21512;&#24182;&#21644;&#20998;&#23618;&#30340;&#22686;&#37327;&#20146;&#21644;&#20256;&#25773;&#31639;&#27861;(APP)&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#25968;&#25454;&#38598;&#30340;&#22686;&#37327;&#32858;&#31867;&#65292;&#21516;&#26102;&#20445;&#25345;&#32858;&#31867;&#32467;&#26524;&#30340;&#24544;&#23454;&#24615;&#21644;&#36951;&#24536;&#36807;&#26102;&#31751;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14439</link><description>&lt;p&gt;
&#22522;&#20110;&#31751;&#21512;&#24182;&#21644;&#20998;&#23618;&#30340;&#22686;&#37327;&#20146;&#21644;&#20256;&#25773;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Incremental Affinity Propagation based on Cluster Consolidation and Stratification. (arXiv:2401.14439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31751;&#21512;&#24182;&#21644;&#20998;&#23618;&#30340;&#22686;&#37327;&#20146;&#21644;&#20256;&#25773;&#31639;&#27861;(APP)&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#25968;&#25454;&#38598;&#30340;&#22686;&#37327;&#32858;&#31867;&#65292;&#21516;&#26102;&#20445;&#25345;&#32858;&#31867;&#32467;&#26524;&#30340;&#24544;&#23454;&#24615;&#21644;&#36951;&#24536;&#36807;&#26102;&#31751;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25968;&#25454;&#25366;&#25496;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#36890;&#36807;&#36319;&#36394;&#32467;&#26524;&#31751;&#30340;&#26102;&#38388;&#21464;&#21270;&#26469;&#23545;&#21160;&#24577;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#37327;&#32858;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#20146;&#21644;&#20256;&#25773;&#31639;&#27861;&#65288;APP&#65289;&#65292;&#23427;&#26159;&#20146;&#21644;&#20256;&#25773;&#65288;AP&#65289;&#30340;&#22686;&#37327;&#25193;&#23637;&#65292;&#22522;&#20110;&#31751;&#21512;&#24182;&#21644;&#31751;&#20998;&#23618;&#26469;&#23454;&#29616;&#24544;&#23454;&#24615;&#21644;&#36951;&#24536;&#12290;APP&#23454;&#29616;&#20102;&#22686;&#37327;&#32858;&#31867;&#65292;&#20854;&#20013;&#26032;&#21040;&#36798;&#30340;&#23545;&#35937;&#20250;&#21160;&#24577;&#21512;&#24182;&#21040;&#20808;&#21069;&#30340;&#31751;&#20013;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#23545;&#25972;&#20010;&#23545;&#35937;&#25968;&#25454;&#38598;&#25191;&#34892;&#32858;&#31867;&#65292;&#24182;&#19988;&#20250;&#22312;&#26102;&#38388;&#19978;&#29983;&#25104;&#21644;&#32500;&#25252;&#19968;&#31995;&#21015;&#24544;&#23454;&#30340;&#32858;&#31867;&#32467;&#26524;&#65292;&#21516;&#26102;&#20801;&#35768;&#36951;&#24536;&#36807;&#26102;&#30340;&#31751;&#65292;&#24102;&#26377;&#36882;&#20943;&#23398;&#20064;&#21151;&#33021;&#12290;&#20351;&#29992;&#22235;&#20010;&#24120;&#29992;&#30340;&#24102;&#26631;&#31614;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;APP&#19982;&#20256;&#32479;AP&#21644;&#22522;&#20110;&#26368;&#36817;&#37051;&#20998;&#37197;&#30340;&#22686;&#37327;&#20146;&#21644;&#20256;&#25773;&#31639;&#27861;&#65288;IAPNA&#65289;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern data mining applications require to perform incremental clustering over dynamic datasets by tracing temporal changes over the resulting clusters. In this paper, we propose A-Posteriori affinity Propagation (APP), an incremental extension of Affinity Propagation (AP) based on cluster consolidation and cluster stratification to achieve faithfulness and forgetfulness. APP enforces incremental clustering where i) new arriving objects are dynamically consolidated into previous clusters without the need to re-execute clustering over the entire dataset of objects, and ii) a faithful sequence of clustering results is produced and maintained over time, while allowing to forget obsolete clusters with decremental learning functionalities. Four popular labeled datasets are used to test the performance of APP with respect to benchmark clustering performances obtained by conventional AP and Incremental Affinity Propagation based on Nearest neighbor Assignment (IAPNA) algorithms. Experimental 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#25903;&#25345;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#31435;&#21306;&#21035;&#26469;&#24378;&#35843;&#37325;&#35201;&#21306;&#22495;, &#24182;&#20943;&#23569;&#22270;&#20687;&#22122;&#38899;&#12290;&#23454;&#35777;&#35843;&#26597;&#34920;&#26126;&#36825;&#20123;&#21306;&#22495;&#22312;&#20419;&#36827;&#31867;&#21035;&#21306;&#20998;&#26041;&#38754;&#36215;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.14434</link><description>&lt;p&gt;
&#23558;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#36716;&#21270;&#20026;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transforming gradient-based techniques into interpretable methods. (arXiv:2401.14434v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#25903;&#25345;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#31435;&#21306;&#21035;&#26469;&#24378;&#35843;&#37325;&#35201;&#21306;&#22495;, &#24182;&#20943;&#23569;&#22270;&#20687;&#22122;&#38899;&#12290;&#23454;&#35777;&#35843;&#26597;&#34920;&#26126;&#36825;&#20123;&#21306;&#22495;&#22312;&#20419;&#36827;&#31867;&#21035;&#21306;&#20998;&#26041;&#38754;&#36215;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;xAI&#25216;&#26415;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36890;&#24120;&#22312;&#35299;&#37322;&#19978;&#38754;&#20020;&#25361;&#25112;&#12290;&#22270;&#20687;&#25552;&#21462;&#30340;&#20687;&#32032;&#31561;&#36755;&#20837;&#29305;&#24449;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#24341;&#21457;&#20102;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#12290;&#38598;&#25104;&#26799;&#24230;&#65288;IG&#65289;&#31561;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23637;&#31034;&#20102;&#36825;&#20123;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#35299;&#37322;&#36716;&#21270;&#20026;&#22270;&#20687;&#26102;&#24120;&#24120;&#20135;&#29983;&#22823;&#37327;&#22122;&#38899;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26799;&#24230;&#20154;&#24037;&#20998;&#31163;&#65288;GAD&#65289;&#20316;&#20026;&#26799;&#24230;&#22522;&#20110;&#25216;&#26415;&#30340;&#25903;&#25345;&#26694;&#26550;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#24314;&#31435;&#31867;&#21035;&#20043;&#38388;&#30340;&#21306;&#21035;&#26469;&#24378;&#35843;&#26377;&#24433;&#21709;&#21147;&#30340;&#21306;&#22495;&#12290;GAD&#30340;&#26680;&#24515;&#26159;&#22312;&#21487;&#35270;&#21270;&#36807;&#31243;&#20013;&#38480;&#21046;&#20998;&#26512;&#33539;&#22260;&#65292;&#20174;&#32780;&#20943;&#23569;&#22270;&#20687;&#22122;&#38899;&#12290;&#36890;&#36807;&#23545;&#34987;&#36974;&#25377;&#22270;&#20687;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#30830;&#23450;&#30340;&#21306;&#22495;&#30830;&#23454;&#22312;&#20419;&#36827;&#31867;&#21035;&#21306;&#20998;&#26041;&#38754;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The explication of Convolutional Neural Networks (CNN) through xAI techniques often poses challenges in interpretation. The inherent complexity of input features, notably pixels extracted from images, engenders complex correlations. Gradient-based methodologies, exemplified by Integrated Gradients (IG), effectively demonstrate the significance of these features. Nevertheless, the conversion of these explanations into images frequently yields considerable noise. Presently, we introduce GAD (Gradient Artificial Distancing) as a supportive framework for gradient-based techniques. Its primary objective is to accentuate influential regions by establishing distinctions between classes. The essence of GAD is to limit the scope of analysis during visualization and, consequently reduce image noise. Empirical investigations involving occluded images have demonstrated that the identified regions through this methodology indeed play a pivotal role in facilitating class differentiation.
&lt;/p&gt;</description></item><item><title>A2C&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#27169;&#22359;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#21512;&#20316;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21183;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21160;&#24577;&#21644;&#19981;&#26029;&#21464;&#21270;&#29615;&#22659;&#20013;&#22797;&#26434;&#20915;&#31574;&#21046;&#23450;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.14432</link><description>&lt;p&gt;
A2C&#65306;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#22810;&#38454;&#27573;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#21512;&#20316;&#20915;&#31574;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A2C: A Modular Multi-stage Collaborative Decision Framework for Human-AI Teams. (arXiv:2401.14432v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14432
&lt;/p&gt;
&lt;p&gt;
A2C&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#27169;&#22359;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#21512;&#20316;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21183;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21160;&#24577;&#21644;&#19981;&#26029;&#21464;&#21270;&#29615;&#22659;&#20013;&#22797;&#26434;&#20915;&#31574;&#21046;&#23450;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;A2C&#65292;&#19968;&#20010;&#22810;&#38454;&#27573;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#21512;&#20316;&#20915;&#31574;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#20869;&#24378;&#22823;&#30340;&#20915;&#31574;&#21046;&#23450;&#33021;&#21147;&#12290;A2C&#20174;&#25298;&#32477;&#23398;&#20064;&#21644;&#23398;&#20064;&#25512;&#36831;&#31561;&#27010;&#24565;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#23558;&#35757;&#32451;&#26377;&#32032;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#32435;&#20837;&#20854;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#33258;&#24049;&#20915;&#31574;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22312;&#38656;&#35201;&#26102;&#25512;&#36831;&#21040;&#20154;&#31867;&#19987;&#23478;&#12290;&#27492;&#22806;&#65292;A2C&#36866;&#29992;&#20110;&#21363;&#20351;&#26159;&#20154;&#31867;&#19987;&#23478;&#20063;&#20250;&#36935;&#21040;&#38480;&#21046;&#30340;&#22330;&#26223;&#65292;&#20363;&#22914;&#22312;&#32593;&#32476;&#23433;&#20840;&#36816;&#33829;&#20013;&#24515;&#65288;SOC&#65289;&#20013;&#30340;&#20107;&#20214;&#26816;&#27979;&#21644;&#21709;&#24212;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;A2C&#20419;&#36827;&#20102;&#21327;&#20316;&#25506;&#32034;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#25361;&#25112;&#30340;&#38598;&#20307;&#35299;&#20915;&#12290;A2C&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#20013;&#19977;&#31181;&#19981;&#21516;&#30340;&#20915;&#31574;&#27169;&#24335;&#65306;&#33258;&#21160;&#21270;&#12289;&#22686;&#24378;&#22411;&#21644;&#21327;&#20316;&#22411;&#65292;&#20026;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#26377;&#25928;&#21327;&#20316;&#31574;&#30053;&#24320;&#21457;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#24179;&#21488;&#12290;&#36890;&#36807;&#20805;&#20998;&#21457;&#25381;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21183;&#65292;A2C&#26174;&#33879;&#25552;&#39640;&#20102;&#21160;&#24577;&#21644;&#19981;&#26029;&#21464;&#21270;&#29615;&#22659;&#20013;&#22797;&#26434;&#20915;&#31574;&#21046;&#23450;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces A2C, a multi-stage collaborative decision framework designed to enable robust decision-making within human-AI teams. Drawing inspiration from concepts such as rejection learning and learning to defer, A2C incorporates AI systems trained to recognise uncertainty in their decisions and defer to human experts when needed. Moreover, A2C caters to scenarios where even human experts encounter limitations, such as in incident detection and response in cyber Security Operations Centres (SOC). In such scenarios, A2C facilitates collaborative explorations, enabling collective resolution of complex challenges. With support for three distinct decision-making modes in human-AI teams: Automated, Augmented, and Collaborative, A2C offers a flexible platform for developing effective strategies for human-AI collaboration. By harnessing the strengths of both humans and AI, it significantly improves the efficiency and effectiveness of complex decision-making in dynamic and evolving e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#32447;&#24615;&#21644;&#38750;&#39640;&#26031;&#35266;&#27979;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#28388;&#27874;&#30340;&#21028;&#21035;&#24335;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65292;&#24182;&#22312;&#31070;&#32463;&#31185;&#23398;&#32972;&#26223;&#19979;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14429</link><description>&lt;p&gt;
[&#20877;&#35770;] &#38750;&#32447;&#24615;&#21644;&#38750;&#39640;&#26031;&#35266;&#27979;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#28388;&#27874;&#30340;&#21028;&#21035;&#24335;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
[Re] The Discriminative Kalman Filter for Bayesian Filtering with Nonlinear and Non-Gaussian Observation Models. (arXiv:2401.14429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14429
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#32447;&#24615;&#21644;&#38750;&#39640;&#26031;&#35266;&#27979;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#28388;&#27874;&#30340;&#21028;&#21035;&#24335;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65292;&#24182;&#22312;&#31070;&#32463;&#31185;&#23398;&#32972;&#26223;&#19979;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#20026;&#20272;&#35745;&#38544;&#34255;&#25110;&#28508;&#22312;&#21464;&#37327;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#25511;&#21046;&#12289;&#26426;&#22120;&#20154;&#12289;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#39046;&#22495;&#24212;&#29992;&#24191;&#27867;&#12290;&#20854;&#20013;&#19968;&#31181;&#24212;&#29992;&#26159;&#31070;&#32463;&#33041;&#26426;&#25509;&#21475;&#30340;&#31070;&#32463;&#35299;&#30721;&#12290;2020&#24180;&#65292;Burkhart&#31561;&#20154;&#23545;&#20182;&#20204;&#30340;&#26032;&#29256;&#26412;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#23450;&#29702;&#25913;&#21892;&#20102;&#23545;&#39640;&#24230;&#38750;&#32447;&#24615;&#25110;&#38750;&#39640;&#26031;&#35266;&#27979;&#27169;&#22411;&#30340;&#28388;&#27874;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20316;&#32773;MATLAB&#31639;&#27861;&#30340;Python&#24320;&#28304;&#26367;&#20195;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#22797;&#29616;&#20102;&#20182;&#20204;&#22312;&#31070;&#32463;&#31185;&#23398;&#32972;&#26223;&#19979;&#26368;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#38543;&#26426;&#31181;&#23376;&#21644;&#20316;&#32773;&#25968;&#25454;&#38598;&#20013;&#26410;&#20351;&#29992;&#30340;&#35797;&#39564;&#36827;&#19968;&#27493;&#26816;&#39564;&#20102;&#28388;&#27874;&#22120;&#30340;&#25928;&#26524;&#12290;&#25152;&#26377;&#23454;&#39564;&#22312;&#19968;&#21488;&#35745;&#31639;&#26426;&#19978;&#31163;&#32447;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kalman filters provide a straightforward and interpretable means to estimate hidden or latent variables, and have found numerous applications in control, robotics, signal processing, and machine learning. One such application is neural decoding for neuroprostheses. In 2020, Burkhart et al. thoroughly evaluated their new version of the Kalman filter that leverages Bayes' theorem to improve filter performance for highly non-linear or non-Gaussian observation models. This work provides an open-source Python alternative to the authors' MATLAB algorithm. Specifically, we reproduce their most salient results for neuroscientific contexts and further examine the efficacy of their filter using multiple random seeds and previously unused trials from the authors' dataset. All experiments were performed offline on a single computer.
&lt;/p&gt;</description></item><item><title>Beimingwu is a learnware paradigm that enables users to reuse well-trained models, submitted by developers worldwide to a dock system, for solving new user tasks. The dock system assigns a specification to accommodate the model, allowing future users to identify and assemble the model for reuse, even without prior knowledge. This paradigm offers excellent capabilities for both planned and specialized tasks.</title><link>http://arxiv.org/abs/2401.14427</link><description>&lt;p&gt;
Beimingwu: &#19968;&#20010;&#23398;&#20064;&#36719;&#20214;&#38598;&#32447;&#22120;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Beimingwu: A Learnware Dock System. (arXiv:2401.14427v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14427
&lt;/p&gt;
&lt;p&gt;
Beimingwu is a learnware paradigm that enables users to reuse well-trained models, submitted by developers worldwide to a dock system, for solving new user tasks. The dock system assigns a specification to accommodate the model, allowing future users to identify and assemble the model for reuse, even without prior knowledge. This paradigm offers excellent capabilities for both planned and specialized tasks.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Beimingwu&#26159;&#19968;&#20010;&#23398;&#20064;&#36719;&#20214;&#33539;&#24335;&#65292;&#26088;&#22312;&#20351;&#29992;&#25143;&#33021;&#22815;&#37325;&#22797;&#20351;&#29992;&#35768;&#22810;&#29616;&#26377;&#30340;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#20174;&#22836;&#24320;&#22987;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26399;&#26395;&#33021;&#35299;&#20915;&#36229;&#20986;&#27169;&#22411;&#21407;&#22987;&#30446;&#30340;&#30340;&#26032;&#29992;&#25143;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#65292;&#20840;&#29699;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#38543;&#26102;&#23558;&#20182;&#20204;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#21311;&#21517;&#25552;&#20132;&#21040;&#23398;&#20064;&#36719;&#20214;&#38598;&#32447;&#22120;&#31995;&#32479;&#65288;&#20043;&#21069;&#31216;&#20026;&#23398;&#20064;&#36719;&#20214;&#24066;&#22330;&#65289;&#65292;&#32780;&#19981;&#29992;&#36879;&#38706;&#20182;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#19968;&#26086;&#38598;&#32447;&#22120;&#31995;&#32479;&#25509;&#21463;&#20102;&#35813;&#27169;&#22411;&#65292;&#23427;&#20250;&#20998;&#37197;&#19968;&#20010;&#35268;&#33539;&#65292;&#24182;&#23481;&#32435;&#35813;&#27169;&#22411;&#12290;&#36825;&#20010;&#35268;&#33539;&#20801;&#35768;&#26681;&#25454;&#26410;&#26469;&#29992;&#25143;&#30340;&#38656;&#27714;&#20805;&#20998;&#35782;&#21035;&#21644;&#32452;&#35013;&#27169;&#22411;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#21363;&#20351;&#20182;&#20204;&#27809;&#26377;&#23545;&#35813;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#36825;&#20010;&#33539;&#24335;&#19982;&#24403;&#21069;&#30340;&#22823;&#27169;&#22411;&#26041;&#21521;&#26377;&#24456;&#22823;&#21306;&#21035;&#65292;&#24182;&#19988;&#26377;&#26395;&#23398;&#20064;&#36719;&#20214;&#38598;&#32447;&#22120;&#31995;&#32479;&#21487;&#20197;&#23481;&#32435;&#25968;&#30334;&#19975;&#29978;&#33267;&#26356;&#22810;&#30340;&#39640;&#24615;&#33021;&#27169;&#22411;&#65292;&#20026;&#35745;&#21010;&#20219;&#21153;&#21644;&#38750;&#35745;&#21010;&#12289;&#19987;&#19994;&#20219;&#21153;&#25552;&#20379;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The learnware paradigm proposed by Zhou [2016] aims to enable users to reuse numerous existing well-trained models instead of building machine learning models from scratch, with the hope of solving new user tasks even beyond models' original purposes. In this paradigm, developers worldwide can submit their high-performing models spontaneously to the learnware dock system (formerly known as learnware market) without revealing their training data. Once the dock system accepts the model, it assigns a specification and accommodates the model. This specification allows the model to be adequately identified and assembled to reuse according to future users' needs, even if they have no prior knowledge of the model. This paradigm greatly differs from the current big model direction and it is expected that a learnware dock system housing millions or more high-performing models could offer excellent capabilities for both planned tasks where big models are applicable; and unplanned, specialized, d
&lt;/p&gt;</description></item><item><title>M$^3$TN&#26159;&#19968;&#31181;&#29992;&#20110;&#25552;&#21319;&#24314;&#27169;&#30340;&#26032;&#39062;&#32593;&#32476;&#65292;&#36890;&#36807;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#21644;&#26126;&#30830;&#24314;&#27169;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#19968;&#33268;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14426</link><description>&lt;p&gt;
M$^3$TN&#65306;&#22522;&#20110;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#30340;&#22810;&#20540;&#22788;&#29702;&#32593;&#32476;&#30340;&#25552;&#21319;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
M$^3$TN: Multi-gate Mixture-of-Experts based Multi-valued Treatment Network for Uplift Modeling. (arXiv:2401.14426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14426
&lt;/p&gt;
&lt;p&gt;
M$^3$TN&#26159;&#19968;&#31181;&#29992;&#20110;&#25552;&#21319;&#24314;&#27169;&#30340;&#26032;&#39062;&#32593;&#32476;&#65292;&#36890;&#36807;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#21644;&#26126;&#30830;&#24314;&#27169;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#19968;&#33268;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#24314;&#27169;&#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#22788;&#29702;&#65288;&#22914;&#25240;&#25187;&#65289;&#23545;&#20010;&#20307;&#21453;&#24212;&#30340;&#25216;&#26415;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#22810;&#20540;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#37117;&#26159;&#20174;&#20108;&#20540;&#22788;&#29702;&#26041;&#27861;&#25193;&#23637;&#32780;&#26469;&#30340;&#65292;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#26041;&#27861;&#22522;&#20110;&#39044;&#27979;&#30340;&#21709;&#24212;&#35745;&#31639;&#25552;&#21319;&#65292;&#36825;&#21487;&#33021;&#19981;&#33021;&#20445;&#35777;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#20043;&#38388;&#30340;&#19968;&#33268;&#25552;&#21319;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#22810;&#20540;&#22788;&#29702;&#20135;&#29983;&#32047;&#31215;&#35823;&#24046;&#12290;&#20854;&#27425;&#65292;&#38543;&#30528;&#35768;&#22810;&#39044;&#27979;&#22836;&#65292;&#27169;&#22411;&#21442;&#25968;&#21464;&#24471;&#38750;&#24120;&#22797;&#26434;&#65292;&#23548;&#33268;&#25928;&#29575;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#30340;&#22810;&#20540;&#22788;&#29702;&#32593;&#32476;&#65288;M$^3$TN&#65289;&#12290;M$^3$TN&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;1) &#22522;&#20110;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#30340;&#29305;&#24449;&#34920;&#31034;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#65307;2) &#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#25552;&#21319;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uplift modeling is a technique used to predict the effect of a treatment (e.g., discounts) on an individual's response. Although several methods have been proposed for multi-valued treatment, they are extended from binary treatment methods. There are still some limitations. Firstly, existing methods calculate uplift based on predicted responses, which may not guarantee a consistent uplift distribution between treatment and control groups. Moreover, this may cause cumulative errors for multi-valued treatment. Secondly, the model parameters become numerous with many prediction heads, leading to reduced efficiency. To address these issues, we propose a novel \underline{M}ulti-gate \underline{M}ixture-of-Experts based \underline{M}ulti-valued \underline{T}reatment \underline{N}etwork (M$^3$TN). M$^3$TN consists of two components: 1) a feature representation module with Multi-gate Mixture-of-Experts to improve the efficiency; 2) a reparameterization module by modeling uplift explicitly to i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;MCTS&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;SR-GPT&#65292;&#22312;&#21457;&#29616;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#20844;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.14424</link><description>&lt;p&gt;
&#36890;&#36807;GPT&#24341;&#23548;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#25968;&#23398;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search. (arXiv:2401.14424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14424
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;MCTS&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;SR-GPT&#65292;&#22312;&#21457;&#29616;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#20844;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#30740;&#31350;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#25214;&#21040;&#19968;&#20010;&#31616;&#27905;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#20844;&#24335;&#26469;&#20934;&#30830;&#25551;&#36848;&#25968;&#25454;&#20013;&#27599;&#20010;&#21464;&#37327;&#19982;&#39044;&#27979;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20063;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#65292;&#26159;&#19968;&#20010;NP&#22256;&#38590;&#38382;&#39064;&#12290;&#21435;&#24180;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;sota&#12290;&#34429;&#28982;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#22312;&#24674;&#22797;&#30446;&#26631;&#34920;&#36798;&#24335;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#25913;&#36827;&#65292;&#20294;&#26159;&#22312;MCTS&#36807;&#31243;&#20013;&#32570;&#20047;&#24341;&#23548;&#20005;&#37325;&#38459;&#30861;&#20102;&#20854;&#25628;&#32034;&#25928;&#29575;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#31639;&#27861;&#22312;MCTS&#30340;&#25628;&#32034;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#32593;&#32476;&#65292;&#20294;&#26159;&#36825;&#20010;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#24456;&#24046;&#12290;&#20026;&#20102;&#24179;&#34913;&#25928;&#29575;&#21644;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SR-GPT&#65292;&#32467;&#21512;&#20102;AlphaZero&#30340;&#24605;&#24819;&#12290;SR-GPT&#26159;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;&#65292;&#23558;MCTS&#19982;&#19968;&#20010;&#36890;&#29992;&#24615;&#36739;&#22909;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding a concise and interpretable mathematical formula that accurately describes the relationship between each variable and the predicted value in the data is a crucial task in scientific research, as well as a significant challenge in artificial intelligence. This problem is referred to as symbolic regression, which is an NP-hard problem. Last year, a symbolic regression method based on Monte Carlo Tree Search (MCTS) was proposed and sota was obtained on multiple datasets. While this algorithm has shown considerable improvement in recovering target expressions compared to previous methods, the lack of guidance during the MCTS process severely hampers its search efficiency. Recently, some algorithms have added a pre-trained policy network to guide the search of MCTS, but the pre-trained policy network generalizes poorly. To balance efficiency and generality, we propose SR-GPT combining ideas from AlphaZero. SR-GPT is a new symbolic regression algorithm that combines MCTS with a Gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#30340;&#20027;&#35201;&#27010;&#24565;&#65292;&#24182;&#22238;&#39038;&#20102;&#22522;&#26412;&#21644;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.14423</link><description>&lt;p&gt;
&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#65306;&#20171;&#32461;&#19982;&#39640;&#32423;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompt Design and Engineering: Introduction and Advanced Methods. (arXiv:2401.14423v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#30340;&#20027;&#35201;&#27010;&#24565;&#65292;&#24182;&#22238;&#39038;&#20102;&#22522;&#26412;&#21644;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#22312;&#36807;&#21435;&#20960;&#20010;&#26376;&#20013;&#25104;&#20026;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#23398;&#31185;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20027;&#35201;&#27010;&#24565;&#65292;&#24182;&#22238;&#39038;&#20102;&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#30340;&#22522;&#26412;&#21644;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt design and engineering has become an important discipline in just the past few months. In this paper, we provide an introduction to the main concepts as well as review basic and more advanced approaches to prompt design and engineering.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#27668;&#20505;&#29305;&#24449;&#39044;&#27979;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#65292;&#22312;&#19981;&#21516;&#22320;&#21306;&#38388;&#20855;&#26377;&#19968;&#23450;&#30340;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14422</link><description>&lt;p&gt;
&#23450;&#20301;&#26080;&#20851;&#28304;&#20813;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#39044;&#27979;&#22826;&#38451;&#33021;&#21457;&#30005;
&lt;/p&gt;
&lt;p&gt;
Location Agnostic Source-Free Domain Adaptive Learning to Predict Solar Power Generation. (arXiv:2401.14422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14422
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#27668;&#20505;&#29305;&#24449;&#39044;&#27979;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#65292;&#22312;&#19981;&#21516;&#22320;&#21306;&#38388;&#20855;&#26377;&#19968;&#23450;&#30340;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#38451;&#33021;&#21457;&#30005;&#30340;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#21576;&#29616;&#31354;&#38388;&#21644;&#26102;&#38388;&#21464;&#24322;&#24615;&#30340;&#27668;&#20505;&#29305;&#24449;&#12290;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#22240;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#32780;&#22312;&#19981;&#21516;&#22320;&#21306;&#20135;&#29983;&#21464;&#21270;&#65292;&#23548;&#33268;&#19968;&#20010;&#22312;&#26576;&#20010;&#22320;&#21306;&#24037;&#20316;&#33391;&#22909;&#20294;&#22312;&#20854;&#20182;&#22320;&#21306;&#19981;&#36215;&#20316;&#29992;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20840;&#29699;&#21464;&#26262;&#65292;&#27599;&#24180;&#22825;&#27668;&#27169;&#24335;&#30340;&#21464;&#21270;&#22312;&#21152;&#36895;&#12290;&#36825;&#31181;&#29616;&#35937;&#21487;&#33021;&#23548;&#33268;&#29616;&#26377;&#27169;&#22411;&#22312;&#21516;&#19968;&#22320;&#29702;&#21306;&#22495;&#20869;&#38543;&#26102;&#38388;&#25512;&#31227;&#32780;&#25928;&#26524;&#20943;&#24369;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#20197;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#30340;&#27668;&#20505;&#29305;&#24449;&#26469;&#20272;&#35745;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#12290;&#37319;&#29992;&#21069;&#39304;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;&#26469;&#22312;&#24050;&#30693;&#20301;&#32622;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#26377;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#29992;&#20110;&#21518;&#32493;&#39044;&#27979;&#26410;&#30693;&#20301;&#32622;&#30340;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of solar power generation is a challenging task due to its dependence on climatic characteristics that exhibit spatial and temporal variability. The performance of a prediction model may vary across different places due to changes in data distribution, resulting in a model that works well in one region but not in others. Furthermore, as a consequence of global warming, there is a notable acceleration in the alteration of weather patterns on an annual basis. This phenomenon introduces the potential for diminished efficacy of existing models, even within the same geographical region, as time progresses. In this paper, a domain adaptive deep learning-based framework is proposed to estimate solar power generation using weather features that can solve the aforementioned challenges. A feed-forward deep convolutional network model is trained for a known location dataset in a supervised manner and utilized to predict the solar power of an unknown location later. This adaptive da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;MA-BERT&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#24494;&#35843;&#26694;&#26550;&#26469;&#35299;&#20915;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#20013;&#30340;&#38271;&#35757;&#32451;&#26102;&#38388;&#21644;&#22823;&#25968;&#25454;&#38598;&#38656;&#27714;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20855;&#26377;&#23569;&#37327;&#25968;&#25454;&#25110;&#26080;&#21382;&#21490;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14421</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#31354;&#20013;&#20132;&#36890;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Based Transfer Learning for Data-Driven Air Traffic Applications. (arXiv:2401.14421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;MA-BERT&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#24494;&#35843;&#26694;&#26550;&#26469;&#35299;&#20915;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#20013;&#30340;&#38271;&#35757;&#32451;&#26102;&#38388;&#21644;&#22823;&#25968;&#25454;&#38598;&#38656;&#27714;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20855;&#26377;&#23569;&#37327;&#25968;&#25454;&#25110;&#26080;&#21382;&#21490;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24320;&#21457;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;(ATM)&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24040;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20855;&#26377;&#36739;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#38598;&#25165;&#33021;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#32771;&#34385;ATM&#31995;&#32479;&#22810;&#26234;&#33021;&#20307;&#29305;&#24615;&#30340;Multi-Agent Bidirectional Encoder Representations from Transformers (MA-BERT)&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;MA-BERT&#22312;&#19968;&#20010;&#20027;&#35201;&#26426;&#22330;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#20854;&#20182;&#26426;&#22330;&#21644;&#29305;&#23450;&#31354;&#20013;&#20132;&#36890;&#24212;&#29992;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#33410;&#30465;&#22823;&#37327;&#30340;&#24635;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26032;&#37319;&#29992;&#30340;&#31243;&#24207;&#21644;&#24314;&#31435;&#30340;&#26426;&#22330;&#65292;&#27809;&#26377;&#21382;&#21490;&#25968;&#25454;&#21487;&#29992;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#30340;MA-BERT&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#25968;&#25454;&#30340;&#23450;&#26399;&#26356;&#26032;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research in developing data-driven models for Air Traffic Management (ATM) has gained a tremendous interest in recent years. However, data-driven models are known to have long training time and require large datasets to achieve good performance. To address the two issues, this paper proposes a Multi-Agent Bidirectional Encoder Representations from Transformers (MA-BERT) model that fully considers the multi-agent characteristic of the ATM system and learns air traffic controllers' decisions, and a pre-training and fine-tuning transfer learning framework. By pre-training the MA-BERT on a large dataset from a major airport and then fine-tuning it to other airports and specific air traffic applications, a large amount of the total training time can be saved. In addition, for newly adopted procedures and constructed airports where no historical data is available, this paper shows that the pre-trained MA-BERT can achieve high performance by updating regularly with little data. The proposed t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#27169;&#31946;&#36923;&#36753;&#20989;&#25968;&#20316;&#20026;&#20998;&#31867;&#22120;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;&#65292;&#36890;&#36807;&#19982;&#40657;&#30418;&#20998;&#31867;&#22120;&#36827;&#34892;&#24182;&#34892;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#19982;&#40657;&#30418;&#20998;&#31867;&#22120;&#30456;&#21516;&#30340;&#20915;&#31574;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.14417</link><description>&lt;p&gt;
&#27169;&#31946;&#36923;&#36753;&#20989;&#25968;&#20316;&#20026;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Logic Function as a Post-hoc Explanator of the Nonlinear Classifier. (arXiv:2401.14417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#27169;&#31946;&#36923;&#36753;&#20989;&#25968;&#20316;&#20026;&#20998;&#31867;&#22120;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;&#65292;&#36890;&#36807;&#19982;&#40657;&#30418;&#20998;&#31867;&#22120;&#36827;&#34892;&#24182;&#34892;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#19982;&#40657;&#30418;&#20998;&#31867;&#22120;&#30456;&#21516;&#30340;&#20915;&#31574;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#27169;&#24335;&#35782;&#21035;&#31995;&#32479;&#27604;&#32447;&#24615;&#27169;&#22411;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#32570;&#28857;&#26159;&#40657;&#30418;&#23646;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#27809;&#26377;&#32463;&#39564;&#20351;&#29992;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#20154;&#21487;&#33021;&#38656;&#35201;&#24110;&#21161;&#29702;&#35299;&#20915;&#31574;&#32467;&#26524;&#12290;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#36127;&#36131;&#26368;&#32456;&#20915;&#31574;&#30340;&#29992;&#25143;&#26469;&#35828;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#20182;&#19981;&#20165;&#24517;&#39035;&#30456;&#20449;&#20915;&#31574;&#65292;&#36824;&#24517;&#39035;&#29702;&#35299;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#22120;&#24517;&#39035;&#20855;&#26377;&#20801;&#35768;&#35299;&#37322;&#32773;&#35299;&#37322;&#32467;&#26524;&#30340;&#26550;&#26500;&#12290;&#20107;&#21518;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#30340;&#24819;&#27861;&#26159;&#35774;&#35745;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#65292;&#19982;&#40657;&#30418;&#20998;&#31867;&#22120;&#24182;&#34892;&#65292;&#32473;&#20986;&#19982;&#40657;&#30418;&#20998;&#31867;&#22120;&#30456;&#21516;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22914;&#26524;Zadeh&#30340;&#27169;&#31946;&#36923;&#36753;&#20989;&#25968;&#24418;&#25104;&#20998;&#31867;&#22120;&#65292;DeconvNet&#30340;&#37325;&#35201;&#24615;&#32473;&#20986;&#20102;&#30495;&#20540;&#65292;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#22312;MNIST&#21644;FashionMNIST&#25968;&#25454;&#24211;&#19978;&#19982;&#40657;&#30418;&#20998;&#31867;&#22120;&#20135;&#29983;&#30456;&#21516;&#30340;&#20998;&#31867;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pattern recognition systems implemented using deep neural networks achieve better results than linear models. However, their drawback is the black box property. This property means that one with no experience utilising nonlinear systems may need help understanding the outcome of the decision. Such a solution is unacceptable to the user responsible for the final decision. He must not only believe in the decision but also understand it. Therefore, recognisers must have an architecture that allows interpreters to interpret the findings. The idea of post-hoc explainable classifiers is to design an interpretable classifier parallel to the black box classifier, giving the same decisions as the black box classifier. This paper shows that the explainable classifier completes matching classification decisions with the black box classifier on the MNIST and FashionMNIST databases if Zadeh`s fuzzy logic function forms the classifier and DeconvNet importance gives the truth values. Since the other 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#65292;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#22768;&#23398;&#22522;&#30784;&#19978;&#36827;&#19968;&#27493;&#30740;&#31350;&#35821;&#38899;&#33410;&#22863;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.14416</link><description>&lt;p&gt;
&#35821;&#38899;&#33410;&#22863;&#30340;&#22768;&#23398;&#29305;&#24449;&#21270;&#65306;&#36229;&#36234;&#25351;&#26631;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Acoustic characterization of speech rhythm: going beyond metrics with recurrent neural networks. (arXiv:2401.14416v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#65292;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#22768;&#23398;&#22522;&#30784;&#19978;&#36827;&#19968;&#27493;&#30740;&#31350;&#35821;&#38899;&#33410;&#22863;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#35821;&#35328;&#19968;&#30452;&#26681;&#25454;&#20854;&#24863;&#30693;&#30340;&#33410;&#22863;&#23646;&#24615;&#36827;&#34892;&#25551;&#36848;&#12290;&#30456;&#20851;&#30340;&#20998;&#31867;&#23545;&#20110;&#24515;&#29702;&#35821;&#35328;&#23398;&#24456;&#26377;&#24847;&#20041;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#39044;&#27979;&#20102;&#26032;&#29983;&#20799;&#36776;&#21035;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#25581;&#31034;&#20102;&#25104;&#24180;&#21548;&#32773;&#22914;&#20309;&#22788;&#29702;&#38750;&#27597;&#35821;&#12290;&#23613;&#31649;&#33410;&#22863;&#24230;&#37327;&#22312;&#25903;&#25345;&#35821;&#35328;&#33410;&#22863;&#31867;&#21035;&#23384;&#22312;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#23545;&#25104;&#21151;&#65292;&#20294;&#23450;&#37327;&#30740;&#31350;&#20173;&#28982;&#27809;&#26377;&#25429;&#25417;&#21040;&#19982;&#35821;&#38899;&#33410;&#22863;&#30456;&#20851;&#30340;&#26102;&#38388;&#35268;&#24459;&#30340;&#20840;&#37096;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#27169;&#24335;&#35782;&#21035;&#26041;&#27861;&#65292;&#21487;&#20197;&#25512;&#21160;&#23545;&#35821;&#38899;&#33410;&#22863;&#22768;&#23398;&#22522;&#30784;&#30340;&#26356;&#28145;&#20837;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#22312;21&#31181;&#35821;&#35328;&#30340;&#22823;&#22411;&#35821;&#38899;&#25968;&#25454;&#24211;&#19978;&#23545;&#20013;&#31561;&#22823;&#23567;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#30340;&#35757;&#32451;&#12290;&#35813;&#32593;&#32476;&#33021;&#22815;&#35775;&#38382;&#24133;&#24230;&#21253;&#32476;&#21644;&#19968;&#20010;&#26631;&#35782;&#26377;&#22768;&#27573;&#30340;&#21464;&#37327;&#65292;&#20551;&#35774;&#36825;&#20010;&#20449;&#21495;&#26080;&#27861;&#24456;&#22909;&#22320;&#20256;&#36798;&#35821;&#38899;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Languages have long been described according to their perceived rhythmic attributes. The associated typologies are of interest in psycholinguistics as they partly predict newborns' abilities to discriminate between languages and provide insights into how adult listeners process non-native languages. Despite the relative success of rhythm metrics in supporting the existence of linguistic rhythmic classes, quantitative studies have yet to capture the full complexity of temporal regularities associated with speech rhythm. We argue that deep learning offers a powerful pattern-recognition approach to advance the characterization of the acoustic bases of speech rhythm. To explore this hypothesis, we trained a medium-sized recurrent neural network on a language identification task over a large database of speech recordings in 21 languages. The network had access to the amplitude envelopes and a variable identifying the voiced segments, assuming that this signal would poorly convey phonetic in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#30005;&#21270;&#23398;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#20171;&#32461;&#20102;&#20854;&#22312;&#21307;&#23398;&#35786;&#26029;&#12289;&#21270;&#23398;&#21697;&#20998;&#31867;&#21644;&#29615;&#22659;&#30417;&#27979;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.14413</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#30005;&#21270;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Aprendizado de m\'aquina aplicado na eletroqu\'imica. (arXiv:2401.14413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#30005;&#21270;&#23398;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#20171;&#32461;&#20102;&#20854;&#22312;&#21307;&#23398;&#35786;&#26029;&#12289;&#21270;&#23398;&#21697;&#20998;&#31867;&#21644;&#29615;&#22659;&#30417;&#27979;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31995;&#32479;&#24615;&#32508;&#36848;&#26088;&#22312;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#35782;&#21035;&#21644;&#37327;&#21270;&#21508;&#31181;&#30005;&#21270;&#23398;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#20171;&#32461;&#20102;&#25991;&#29486;&#20013;&#21487;&#29992;&#30340;&#24212;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#20998;&#26512;&#24182;&#22686;&#24378;&#28041;&#21450;&#21508;&#31181;&#20998;&#26512;&#29289;&#30340;&#36807;&#31243;&#29702;&#35299;&#30340;&#24037;&#20855;&#12290;&#22312;&#30005;&#21270;&#23398;&#29983;&#29289;&#20256;&#24863;&#22120;&#20013;&#65292;&#23427;&#25552;&#39640;&#20102;&#21307;&#23398;&#35786;&#26029;&#30340;&#31934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#35782;&#21035;&#20855;&#26377;&#39640;&#21487;&#38752;&#24615;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#30149;&#21407;&#20307;&#30340;&#33021;&#21147;&#12290;&#23427;&#36824;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#22797;&#26434;&#21270;&#23398;&#21697;&#30340;&#20998;&#31867;&#65307;&#22312;&#29615;&#22659;&#30417;&#27979;&#20013;&#65292;&#20351;&#29992;&#20302;&#25104;&#26412;&#20256;&#24863;&#22120;&#65307;&#22312;&#20415;&#25658;&#35774;&#22791;&#21644;&#21487;&#31359;&#25140;&#31995;&#32479;&#20013;&#31561;&#31561;&#12290;&#30446;&#21069;&#65292;&#26576;&#20123;&#20998;&#26512;&#29289;&#30340;&#20998;&#26512;&#20173;&#28982;&#38656;&#35201;&#19987;&#23478;&#30340;&#19987;&#19994;&#30693;&#35782;&#24182;&#25163;&#21160;&#25191;&#34892;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#32467;&#26524;&#30340;&#26222;&#36941;&#21270;&#12290;&#22312;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#19979;&#65292;&#26412;&#30740;&#31350;&#25311;&#36827;&#34892;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
This systematic review focuses on analyzing the use of machine learning techniques for identifying and quantifying analytes in various electrochemical applications, presenting the available applications in the literature. Machine learning is a tool that can facilitate the analysis and enhance the understanding of processes involving various analytes. In electrochemical biosensors, it increases the precision of medical diagnostics, improving the identification of biomarkers and pathogens with high reliability. It can be effectively used for the classification of complex chemical products; in environmental monitoring, using low-cost sensors; in portable devices and wearable systems; among others. Currently, the analysis of some analytes is still performed manually, requiring the expertise of a specialist in the field and thus hindering the generalization of results. In light of the advancements in artificial intelligence today, this work proposes to carry out a systematic review of the l
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;VeriStable&#26041;&#27861;&#65292;&#22312;DNN&#39564;&#35777;&#20013;&#21033;&#29992;&#31283;&#23450;&#30340;&#31070;&#32463;&#20803;&#20943;&#23569;&#32452;&#21512;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25277;&#35937;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#24037;&#19994;&#21270;SAT&#22522;&#20934;&#20849;&#20139;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#22312;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.14412</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#31283;&#23450;&#24615;&#25913;&#36827;DNN&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Harnessing Neuron Stability to Improve DNN Verification. (arXiv:2401.14412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;VeriStable&#26041;&#27861;&#65292;&#22312;DNN&#39564;&#35777;&#20013;&#21033;&#29992;&#31283;&#23450;&#30340;&#31070;&#32463;&#20803;&#20943;&#23569;&#32452;&#21512;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25277;&#35937;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#24037;&#19994;&#21270;SAT&#22522;&#20934;&#20849;&#20139;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#22312;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20687;&#20154;&#31867;&#32534;&#20889;&#30340;&#36719;&#20214;&#19968;&#26679;&#65292;DNN&#20063;&#23481;&#26131;&#21463;&#21040;&#38169;&#35823;&#21644;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;DNN&#39564;&#35777;&#25216;&#26415;&#21644;&#24037;&#20855;&#30340;&#37325;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;VeriStable&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;DPLL&#32422;&#26463;DNN&#39564;&#35777;&#26041;&#27861;&#30340;&#25193;&#23637;&#12290;VeriStable&#21033;&#29992;&#20102;&#36825;&#26679;&#19968;&#20010;&#27934;&#35265;&#65306;&#23613;&#31649;&#31070;&#32463;&#20803;&#22312;&#25972;&#20010;DNN&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#34892;&#20026;&#21487;&#33021;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#22312;&#39564;&#35777;&#36807;&#31243;&#20013;&#35745;&#31639;&#24471;&#21040;&#30340;&#20013;&#38388;&#29366;&#24577;&#20013;&#65292;&#35768;&#22810;&#31070;&#32463;&#20803;&#21487;&#33021;&#34987;&#32422;&#26463;&#20026;&#20855;&#26377;&#32447;&#24615;&#34892;&#20026;-&#36825;&#20123;&#31070;&#32463;&#20803;&#26159;&#31283;&#23450;&#30340;&#12290;&#39640;&#25928;&#22320;&#26816;&#27979;&#31283;&#23450;&#30340;&#31070;&#32463;&#20803;&#21487;&#20197;&#20943;&#23569;&#32452;&#21512;&#22797;&#26434;&#24615;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#25277;&#35937;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;DNN&#39564;&#35777;&#38382;&#39064;&#20013;&#20135;&#29983;&#30340;&#23376;&#21477;&#32467;&#26500;&#19982;&#24037;&#19994;&#21270;SAT&#22522;&#20934;&#20855;&#26377;&#37325;&#35201;&#29305;&#24449;&#12290;&#25105;&#20204;&#35843;&#25972;&#24182;&#34701;&#21512;&#20102;&#22810;&#32447;&#31243;&#21644;&#37325;&#21551;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNN) have emerged as an effective approach to tackling real-world problems. However, like human-written software, DNNs are susceptible to bugs and attacks. This has generated significant interests in developing effective and scalable DNN verification techniques and tools. In this paper, we present VeriStable, a novel extension of recently proposed DPLL-based constraint DNN verification approach. VeriStable leverages the insight that while neuron behavior may be non-linear across the entire DNN input space, at intermediate states computed during verification many neurons may be constrained to have linear behavior - these neurons are stable. Efficiently detecting stable neurons reduces combinatorial complexity without compromising the precision of abstractions. Moreover, the structure of clauses arising in DNN verification problems shares important characteristics with industrial SAT benchmarks. We adapt and incorporate multi-threading and restart optimizations targ
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#28779;&#26143;&#36827;&#20837;&#23548;&#33322;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22823;&#27668;&#23494;&#24230;&#65292;&#24182;&#26681;&#25454;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#23454;&#26102;&#21442;&#25968;&#35843;&#25972;&#65292;&#20197;&#25552;&#39640;&#23548;&#33322;&#28388;&#27874;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14411</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22823;&#27668;&#23494;&#24230;&#33258;&#36866;&#24212;&#30340;&#28779;&#26143;&#31934;&#20934;&#36827;&#20837;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Precision Mars Entry Navigation with Atmospheric Density Adaptation via Neural Networks. (arXiv:2401.14411v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14411
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#28779;&#26143;&#36827;&#20837;&#23548;&#33322;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22823;&#27668;&#23494;&#24230;&#65292;&#24182;&#26681;&#25454;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#23454;&#26102;&#21442;&#25968;&#35843;&#25972;&#65292;&#20197;&#25552;&#39640;&#23548;&#33322;&#28388;&#27874;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#30340;&#28779;&#26143;&#22823;&#27668;&#23494;&#24230;&#19982;&#26426;&#36733;&#23494;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#20250;&#20005;&#37325;&#24433;&#21709;&#33322;&#22825;&#22120;&#36827;&#20837;&#23548;&#33322;&#28388;&#27874;&#22120;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28779;&#26143;&#36827;&#20837;&#22312;&#32447;&#28388;&#27874;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22823;&#27668;&#23494;&#24230;&#65292;&#24182;&#21033;&#29992;&#19968;&#31181;&#32771;&#34385;&#20998;&#26512;&#26469;&#32771;&#34385;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#32593;&#32476;&#20197;&#25351;&#25968;&#22823;&#27668;&#23494;&#24230;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#23427;&#30340;&#21442;&#25968;&#65292;&#20197;&#36866;&#24212;&#30495;&#23454;&#23494;&#24230;&#19982;&#20272;&#35745;&#23494;&#24230;&#20043;&#38388;&#30340;&#20219;&#20309;&#19981;&#21305;&#37197;&#12290;&#32593;&#32476;&#30340;&#35843;&#25972;&#34987;&#24418;&#24335;&#21270;&#20026;&#26368;&#22823;&#20284;&#28982;&#38382;&#39064;&#65292;&#21033;&#29992;&#28388;&#27874;&#22120;&#30340;&#27979;&#37327;&#21019;&#26032;&#26469;&#35782;&#21035;&#26368;&#20339;&#32593;&#32476;&#21442;&#25968;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#20351;&#24471;&#21487;&#20197;&#22312;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#30340;&#32972;&#26223;&#19979;&#20351;&#29992;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#39640;&#25928;&#30340;&#38543;&#26426;&#20248;&#21270;&#22120;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrepancies between the true Martian atmospheric density and the onboard density model can significantly impair the performance of spacecraft entry navigation filters. This work introduces a new approach to online filtering for Martian entry by using a neural network to estimate atmospheric density and employing a consider analysis to account for the uncertainty in the estimate. The network is trained on an exponential atmospheric density model, and its parameters are dynamically adapted in real time to account for any mismatches between the true and estimated densities. The adaptation of the network is formulated as a maximum likelihood problem, leveraging the measurement innovations of the filter to identify optimal network parameters. The incorporation of a neural network enables the use of stochastic optimizers known for their efficiency in the machine learning domain within the context of the maximum likelihood approach. Performance comparisons against previous approaches are co
&lt;/p&gt;</description></item><item><title>DeepSeek-Coder&#26159;&#19968;&#31995;&#21015;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39640;&#36136;&#37327;&#39033;&#30446;&#32423;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#37319;&#29992;&#22635;&#31354;&#20219;&#21153;&#21644;16K&#31383;&#21475;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#21644;&#22635;&#20805;&#65292;&#19981;&#20165;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19982;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#21516;&#26679;&#30340;&#26368;&#26032;&#34920;&#29616;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#38381;&#28304;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.14196</link><description>&lt;p&gt;
DeepSeek-Coder: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#32534;&#31243;&#30456;&#36935;&#30340;&#26102;&#20505;--&#20195;&#30721;&#26234;&#33021;&#30340;&#23835;&#36215;
&lt;/p&gt;
&lt;p&gt;
DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence. (arXiv:2401.14196v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14196
&lt;/p&gt;
&lt;p&gt;
DeepSeek-Coder&#26159;&#19968;&#31995;&#21015;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39640;&#36136;&#37327;&#39033;&#30446;&#32423;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#37319;&#29992;&#22635;&#31354;&#20219;&#21153;&#21644;16K&#31383;&#21475;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#21644;&#22635;&#20805;&#65292;&#19981;&#20165;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19982;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#21516;&#26679;&#30340;&#26368;&#26032;&#34920;&#29616;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#38381;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#20195;&#30721;&#26234;&#33021;&#24102;&#26469;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#38381;&#28304;&#27169;&#22411;&#30340;&#20027;&#23548;&#22320;&#20301;&#38480;&#21046;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DeepSeek-Coder&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#65292;&#22823;&#23567;&#20174;1.3B&#21040;33B&#65292;&#20174;&#22836;&#24320;&#22987;&#22312;2&#19975;&#20159;&#20010;&#26631;&#35760;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#39640;&#36136;&#37327;&#39033;&#30446;&#32423;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#22635;&#31354;&#20219;&#21153;&#21644;16K&#31383;&#21475;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#21644;&#22635;&#20805;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;DeepSeek-Coder&#19981;&#20165;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19982;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#21516;&#26679;&#30340;&#26368;&#26032;&#34920;&#29616;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;Codex&#21644;GPT-3.5&#31561;&#38381;&#28304;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;DeepSeek-Coder&#27169;&#22411;&#37319;&#29992;&#20102;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#65292;&#26082;&#20801;&#35768;&#30740;&#31350;&#65292;&#20063;&#20801;&#35768;&#26080;&#38480;&#21046;&#30340;&#21830;&#19994;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#30830;&#23450;&#24615;&#20316;&#20026;&#32622;&#20449;&#24230;&#20272;&#35745;&#21644;&#23884;&#20837;&#31354;&#38388;&#30340;&#27491;&#21017;&#39033;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2401.13721</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in Regression. (arXiv:2401.13721v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13721
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#30830;&#23450;&#24615;&#20316;&#20026;&#32622;&#20449;&#24230;&#20272;&#35745;&#21644;&#23884;&#20837;&#31354;&#38388;&#30340;&#27491;&#21017;&#39033;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#65288;UDAR&#65289;&#26088;&#22312;&#23558;&#26469;&#33258;&#26377;&#26631;&#31614;&#28304;&#39046;&#22495;&#30340;&#27169;&#22411;&#35843;&#25972;&#21040;&#26080;&#26631;&#31614;&#30446;&#26631;&#39046;&#22495;&#65292;&#20197;&#23436;&#25104;&#22238;&#24402;&#20219;&#21153;&#12290;&#26368;&#36817;&#22312;UDAR&#39046;&#22495;&#21462;&#24471;&#30340;&#25104;&#21151;&#20027;&#35201;&#38598;&#20013;&#22312;&#23376;&#31354;&#38388;&#23545;&#40784;&#19978;&#65292;&#28041;&#21450;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#25152;&#36873;&#25321;&#23376;&#31354;&#38388;&#30340;&#23545;&#40784;&#12290;&#36825;&#19982;&#29992;&#20110;&#20998;&#31867;&#30340;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#26088;&#22312;&#23545;&#40784;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#25928;&#26524;&#36739;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20998;&#31867;&#20219;&#21153;&#26088;&#22312;&#22312;&#25972;&#20010;&#23884;&#20837;&#31354;&#38388;&#30340;&#32500;&#24230;&#19978;&#35782;&#21035;&#29420;&#31435;&#30340;&#31751;&#65292;&#32780;&#22238;&#24402;&#20219;&#21153;&#23545;&#25968;&#25454;&#34920;&#31034;&#30340;&#32467;&#26500;&#24615;&#35201;&#27714;&#36739;&#20302;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#25351;&#23548;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26377;&#25928;UDAR&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#21452;&#37325;&#20316;&#29992;&#65306;&#25552;&#20379;&#20102;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#32622;&#20449;&#24230;&#34913;&#37327;&#65292;&#24182;&#20316;&#20026;&#23884;&#20837;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#35777;&#25454;&#27169;&#22411;&#26469;&#25552;&#20379;&#23545;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#23884;&#20837;&#31354;&#38388;&#30340;&#27491;&#21017;&#39033;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt a model from a labeled source domain to an unlabeled target domain for regression tasks. Recent successful works in UDAR mostly focus on subspace alignment, involving the alignment of a selected subspace within the entire feature space. This contrasts with the feature alignment methods used for classification, which aim at aligning the entire feature space and have proven effective but are less so in regression settings. Specifically, while classification aims to identify separate clusters across the entire embedding dimension, regression induces less structure in the data representation, necessitating additional guidance for efficient alignment. In this paper, we propose an effective method for UDAR by incorporating guidance from uncertainty. Our approach serves a dual purpose: providing a measure of confidence in predictions and acting as a regularization of the embedding space. Specifically, we leverage the Deep Evid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#20307;&#21644;&#22810;&#31890;&#24230;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#20840;&#23616;&#35270;&#39057;&#29305;&#24449;&#25552;&#21462;&#21644;&#23616;&#37096;&#22120;&#26800;&#20998;&#21106;&#65292;&#21487;&#29992;&#20110;&#22810;&#23618;&#27425;&#29702;&#35299;&#22806;&#31185;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.11174</link><description>&lt;p&gt;
&#20687;&#32032;&#32423;&#21035;&#35782;&#21035;&#29992;&#20110;&#25972;&#20307;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Pixel-Wise Recognition for Holistic Surgical Scene Understanding. (arXiv:2401.11174v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#20307;&#21644;&#22810;&#31890;&#24230;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#20840;&#23616;&#35270;&#39057;&#29305;&#24449;&#25552;&#21462;&#21644;&#23616;&#37096;&#22120;&#26800;&#20998;&#21106;&#65292;&#21487;&#29992;&#20110;&#22810;&#23618;&#27425;&#29702;&#35299;&#22806;&#31185;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Prostatectomies&#30340;&#25972;&#20307;&#21644;&#22810;&#31890;&#24230;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#65288;GraSP&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#36827;&#34892;&#20102;&#23618;&#27425;&#21270;&#24314;&#27169;&#65292;&#21253;&#25324;&#19981;&#21516;&#31890;&#24230;&#30340;&#20114;&#34917;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#22806;&#31185;&#27963;&#21160;&#30340;&#22810;&#23618;&#27425;&#29702;&#35299;&#65292;&#21253;&#25324;&#22806;&#31185;&#38454;&#27573;&#21644;&#27493;&#39588;&#30340;&#35782;&#21035;&#20197;&#21450;&#21253;&#25324;&#22806;&#31185;&#22120;&#26800;&#20998;&#21106;&#21644;&#21407;&#23376;&#21487;&#35270;&#21160;&#20316;&#26816;&#27979;&#22312;&#20869;&#30340;&#30701;&#26399;&#20219;&#21153;&#12290;&#20026;&#20102;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#21464;&#24418;&#22120;&#65288;Transformers&#65289;&#30340;&#34892;&#21160;&#12289;&#38454;&#27573;&#12289;&#27493;&#39588;&#21644;&#22120;&#26800;&#20998;&#21106;&#65288;TAPIS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#20840;&#23616;&#35270;&#39057;&#29305;&#24449;&#25552;&#21462;&#22120;&#19982;&#26469;&#33258;&#22120;&#26800;&#20998;&#21106;&#27169;&#22411;&#30340;&#23616;&#37096;&#21306;&#22495;&#24314;&#35758;&#30456;&#32467;&#21512;&#65292;&#20197;&#24212;&#23545;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#22810;&#31890;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30701;&#26399;&#35782;&#21035;&#20219;&#21153;&#20013;&#21253;&#25324;&#20998;&#21106;&#27880;&#37322;&#30340;&#24433;&#21709;&#65292;&#24182;&#31361;&#26174;&#20102;&#19981;&#21516;&#30340;&#31890;&#24230;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the Holistic and Multi-Granular Surgical Scene Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that models surgical scene understanding as a hierarchy of complementary tasks with varying levels of granularity. Our approach enables a multi-level comprehension of surgical activities, encompassing long-term tasks such as surgical phases and steps recognition and short-term tasks including surgical instrument segmentation and atomic visual actions detection. To exploit our proposed benchmark, we introduce the Transformers for Actions, Phases, Steps, and Instrument Segmentation (TAPIS) model, a general architecture that combines a global video feature extractor with localized region proposals from an instrument segmentation model to tackle the multi-granularity of our benchmark. Through extensive experimentation, we demonstrate the impact of including segmentation annotations in short-term recognition tasks, highlight the varying granularity require
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11143</link><description>&lt;p&gt;
&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26159;&#21807;&#19968;&#25152;&#38656;&#30340;&#65306;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20581;&#22766;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GAAM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#26031;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#65288;GAT&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#65288;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#65289;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;GAAM&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#34701;&#20837;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#37319;&#29992;&#22810;&#22836;&#26694;&#26550;&#23454;&#29616;&#65292;&#20351;&#20854;&#33021;&#22815;&#38598;&#20307;&#24314;&#27169;&#20219;&#20309;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#24230;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#36890;&#36807;&#35782;&#21035;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#29366;&#24577;&#65288;&#31934;&#24230;&#22686;&#21152;&#32422;20%&#65289;&#12290;GAAM&#19982;&#22522;&#20110;&#28857;&#31215;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#20855;&#26377;&#30456;&#23545;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#21644;&#25552;&#21319;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;GAAM&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#21644;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10189</link><description>&lt;p&gt;
Chem-FINESE: &#36890;&#36807;&#25991;&#26412;&#37325;&#26500;&#39564;&#35777;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#65292;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#38754;&#20020;&#20004;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#19982;&#19968;&#33324;&#39046;&#22495;&#30340;&#23454;&#20307;&#25552;&#21462;&#20219;&#21153;&#30456;&#27604;&#65292;&#21270;&#23398;&#35770;&#25991;&#20013;&#30340;&#21477;&#23376;&#36890;&#24120;&#21253;&#21547;&#26356;&#22810;&#30340;&#23454;&#20307;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#25552;&#21462;&#38271;&#23614;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;Chem-FINESE&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;Chem-FINESE&#21253;&#21547;&#20004;&#20010;&#32452;&#20214;&#65306;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#29992;&#20110;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#65292;&#20197;&#21450;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#23454;&#20307;&#20013;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#21463;&#21040;&#19968;&#20010;&#22909;&#30340;&#23454;&#20307;&#25552;&#21462;&#31995;&#32479;&#38656;&#35201;&#24544;&#23454;&#25552;&#21462;&#23454;&#20307;&#30340;&#20107;&#23454;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26032;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#21033;&#29992;&#23454;&#20307;&#25552;&#21462;&#32467;&#26524;&#26469;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#26469;&#20943;&#23569;&#22312;&#25552;&#21462;&#36807;&#31243;&#20013;&#30340;&#36807;&#24230;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
&lt;/p&gt;</description></item><item><title>DiConStruct&#26159;&#19968;&#31181;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#25552;&#20379;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.08534</link><description>&lt;p&gt;
DiConStruct: &#22522;&#20110;&#40657;&#30418;&#31934;&#21326;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
DiConStruct: Causal Concept-based Explanations through Black-Box Distillation. (arXiv:2401.08534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08534
&lt;/p&gt;
&lt;p&gt;
DiConStruct&#26159;&#19968;&#31181;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#25552;&#20379;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#22312;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#31995;&#32479;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#35299;&#37322;&#24212;&#35813;&#20351;&#29992;&#20154;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#27010;&#24565;&#26469;&#34920;&#36798;&#12290;&#27492;&#22806;&#65292;&#35299;&#37322;&#22120;&#24212;&#35813;&#25429;&#25417;&#36825;&#20123;&#27010;&#24565;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#20415;&#23545;&#35299;&#37322;&#36827;&#34892;&#25512;&#29702;&#12290;&#26368;&#21518;&#65292;&#35299;&#37322;&#26041;&#27861;&#24212;&#35813;&#39640;&#25928;&#65292;&#24182;&#19981;&#25439;&#23475;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;AI&#35299;&#37322;&#24615;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#33267;&#20170;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#28385;&#36275;&#36825;&#19977;&#20010;&#26465;&#20214;&#12290;&#20107;&#23454;&#19978;&#65292;&#20027;&#27969;&#30340;&#23616;&#37096;&#27010;&#24565;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#19981;&#20135;&#29983;&#22240;&#26524;&#35299;&#37322;&#65292;&#24182;&#22312;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DiConStruct&#65292;&#19968;&#31181;&#26082;&#22522;&#20110;&#27010;&#24565;&#21448;&#20855;&#26377;&#22240;&#26524;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#21019;&#24314;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#22120;&#20316;&#20026;&#19968;&#20010;&#31934;&#21326;&#27169;&#22411;&#36866;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the performance of the predictive task. Despite the rapid advances in AI explainability in recent years, as far as we know to date, no method fulfills these three properties. Indeed, mainstream methods for local concept explainability do not produce causal explanations and incur a trade-off between explainability and prediction performance. We present DiConStruct, an explanation method that is both concept-based and causal, with the goal of creating more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine l
&lt;/p&gt;</description></item><item><title>CCNETS&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33041;&#21551;&#21457;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#35782;&#21035;&#65292;&#29305;&#21035;&#20851;&#27880;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.04139</link><description>&lt;p&gt;
CCNETS:&#19968;&#31181;&#26032;&#39062;&#30340;&#33041;&#21551;&#21457;&#26041;&#27861;&#29992;&#20110;&#22686;&#24378;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
CCNETS: A Novel Brain-Inspired Approach for Enhanced Pattern Recognition in Imbalanced Datasets. (arXiv:2401.04139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04139
&lt;/p&gt;
&lt;p&gt;
CCNETS&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33041;&#21551;&#21457;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#35782;&#21035;&#65292;&#29305;&#21035;&#20851;&#27880;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CCNETS&#65288;&#20855;&#26377;&#22240;&#26524;&#21512;&#20316;&#32593;&#32476;&#30340;&#22240;&#26524;&#23398;&#20064;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#31867;&#22120;&#65292;&#26088;&#22312;&#35299;&#20915;&#27169;&#24335;&#35782;&#21035;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;CCNETS&#29420;&#29305;&#22320;&#35774;&#35745;&#25104;&#27169;&#25311;&#31867;&#20284;&#20110;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#24182;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#35299;&#37322;&#22120;&#12289;&#29983;&#25104;&#22120;&#21644;&#25512;&#29702;&#22120;&#12290;&#27599;&#20010;&#32452;&#20214;&#37117;&#34987;&#35774;&#35745;&#25104;&#27169;&#20223;&#29305;&#23450;&#30340;&#22823;&#33041;&#21151;&#33021;&#65292;&#26377;&#21161;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#24182;&#22686;&#24378;&#20998;&#31867;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#29305;&#21035;&#20851;&#27880;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#24120;&#35265;&#21644;&#37325;&#35201;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;CCNETS&#24212;&#29992;&#20110;&#19968;&#20010;&#8220;&#27450;&#35784;&#25968;&#25454;&#38598;&#8221;&#65292;&#20854;&#20013;&#27491;&#24120;&#20132;&#26131;&#26126;&#26174;&#22810;&#20110;&#27450;&#35784;&#20132;&#26131;&#65288;99.83&#65285; vs. 0.17&#65285;&#65289;&#65292;&#35777;&#26126;&#20102;CCNETS&#30340;&#26377;&#25928;&#24615;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#22312;&#22788;&#29702;&#36825;&#31181;&#19981;&#24179;&#34913;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#23548;&#33268;&#24615;&#33021;&#25351;&#26631;&#19981;&#22343;&#34913;&#12290;&#28982;&#32780;&#65292;CCNETS&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#20998;&#31867;&#33021;&#21147;&#65292;&#36890;&#36807;&#20854;&#24615;&#33021;&#25351;&#26631;&#30340;&#25913;&#21892;&#26469;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces CCNETS (Causal Learning with Causal Cooperative Nets), a novel generative model-based classifier designed to tackle the challenge of generating data for imbalanced datasets in pattern recognition. CCNETS is uniquely crafted to emulate brain-like information processing and comprises three main components: Explainer, Producer, and Reasoner. Each component is designed to mimic specific brain functions, which aids in generating high-quality datasets and enhancing classification performance.  The model is particularly focused on addressing the common and significant challenge of handling imbalanced datasets in machine learning. CCNETS's effectiveness is demonstrated through its application to a "fraud dataset," where normal transactions significantly outnumber fraudulent ones (99.83% vs. 0.17%). Traditional methods often struggle with such imbalances, leading to skewed performance metrics. However, CCNETS exhibits superior classification ability, as evidenced by its pe
&lt;/p&gt;</description></item><item><title>Agent AI&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#20132;&#20114;&#31995;&#32479;&#65292;&#21487;&#20197;&#24863;&#30693;&#35270;&#35273;&#21050;&#28608;&#12289;&#35821;&#35328;&#36755;&#20837;&#21644;&#20854;&#20182;&#29615;&#22659;&#30456;&#20851;&#25968;&#25454;&#65292;&#36890;&#36807;&#23558;&#20195;&#29702;&#20307;&#23884;&#20837;&#29289;&#29702;&#25110;&#34394;&#25311;&#29615;&#22659;&#20013;&#26469;&#23454;&#29616;&#26356;&#22797;&#26434;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.03568</link><description>&lt;p&gt;
Agent AI: &#23545;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#27178;&#21521;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Agent AI: Surveying the Horizons of Multimodal Interaction. (arXiv:2401.03568v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03568
&lt;/p&gt;
&lt;p&gt;
Agent AI&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#20132;&#20114;&#31995;&#32479;&#65292;&#21487;&#20197;&#24863;&#30693;&#35270;&#35273;&#21050;&#28608;&#12289;&#35821;&#35328;&#36755;&#20837;&#21644;&#20854;&#20182;&#29615;&#22659;&#30456;&#20851;&#25968;&#25454;&#65292;&#36890;&#36807;&#23558;&#20195;&#29702;&#20307;&#23884;&#20837;&#29289;&#29702;&#25110;&#34394;&#25311;&#29615;&#22659;&#20013;&#26469;&#23454;&#29616;&#26356;&#22797;&#26434;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24456;&#21487;&#33021;&#20250;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#26080;&#22788;&#19981;&#22312;&#30340;&#23384;&#22312;&#12290;&#20351;&#20854;&#26356;&#20114;&#21160;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#23558;&#23427;&#20204;&#20316;&#20026;&#20195;&#29702;&#20307;&#29616;&#22312;&#29289;&#29702;&#21644;&#34394;&#25311;&#29615;&#22659;&#20013;&#12290;&#30446;&#21069;&#65292;&#31995;&#32479;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#21019;&#24314;&#20195;&#29702;&#20307;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#23558;&#20195;&#29702;&#20307;&#23884;&#20837;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#26377;&#21161;&#20110;&#27169;&#22411;&#22788;&#29702;&#21644;&#35299;&#37322;&#35270;&#35273;&#21644;&#29615;&#22659;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#21019;&#24314;&#26356;&#22797;&#26434;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#31995;&#32479;&#21487;&#20197;&#24863;&#30693;&#29992;&#25143;&#21160;&#20316;&#12289;&#20154;&#31867;&#34892;&#20026;&#12289;&#29615;&#22659;&#29289;&#20307;&#12289;&#38899;&#39057;&#34920;&#36798;&#21644;&#22330;&#26223;&#30340;&#38598;&#20307;&#24773;&#24863;&#65292;&#20174;&#32780;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#20026;&#20195;&#29702;&#20307;&#25552;&#20379;&#20449;&#24687;&#21644;&#25351;&#23548;&#12290;&#20026;&#20102;&#21152;&#36895;&#20195;&#29702;&#20307;&#22810;&#27169;&#24577;&#26234;&#33021;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#8220;Agent AI&#8221;&#23450;&#20041;&#20026;&#19968;&#31867;&#21487;&#20197;&#24863;&#30693;&#35270;&#35273;&#21050;&#28608;&#12289;&#35821;&#35328;&#36755;&#20837;&#21644;&#20854;&#20182;&#29615;&#22659;&#30456;&#20851;&#25968;&#25454;&#30340;&#20132;&#20114;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal AI systems will likely become a ubiquitous presence in our everyday lives. A promising approach to making these systems more interactive is to embody them as agents within physical and virtual environments. At present, systems leverage existing foundation models as the basic building blocks for the creation of embodied agents. Embedding agents within such environments facilitates the ability of models to process and interpret visual and contextual data, which is critical for the creation of more sophisticated and context-aware AI systems. For example, a system that can perceive user actions, human behavior, environmental objects, audio expressions, and the collective sentiment of a scene can be used to inform and direct agent responses within the given environment. To accelerate research on agent-based multimodal intelligence, we define "Agent AI" as a class of interactive systems that can perceive visual stimuli, language inputs, and other environmentally-grounded data, an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#32771;&#34385;&#22240;&#26524;&#20851;&#31995;&#30340;&#35270;&#35282;&#65292;&#30830;&#23450;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#33030;&#24369;&#24615;&#30340;&#28304;&#22836;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.13628</link><description>&lt;p&gt;
&#22914;&#20309;&#21457;&#36215;&#25915;&#20987;&#65311;&#19968;&#31181;&#28789;&#24863;&#26469;&#28304;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#29983;&#25104;&#21453;&#20107;&#23454;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Where and How to Attack? A Causality-Inspired Recipe for Generating Counterfactual Adversarial Examples. (arXiv:2312.13628v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13628
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#32771;&#34385;&#22240;&#26524;&#20851;&#31995;&#30340;&#35270;&#35282;&#65292;&#30830;&#23450;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#33030;&#24369;&#24615;&#30340;&#28304;&#22836;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#24050;&#32463;&#34987;&#35777;&#26126;&#23545;&#31934;&#24515;&#35774;&#35745;&#30340;"&#23545;&#25239;&#26679;&#26412;"&#26131;&#21463;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#26159;&#36890;&#36807;&#21463;&#38480;&#25110;&#38750;&#21463;&#38480;&#30340;$\mathcal{L}_p$&#33539;&#25968;&#29983;&#25104;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20551;&#35774;&#23545;&#25163;&#21487;&#20197;&#20219;&#24847;&#20462;&#25913;&#20219;&#20309;&#29305;&#24449;&#65292;&#24182;&#24573;&#35270;&#20102;&#25968;&#25454;&#30340;&#22240;&#26524;&#29983;&#25104;&#36807;&#31243;&#65292;&#36825;&#26159;&#19981;&#21512;&#29702;&#21644;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#36890;&#36807;&#32771;&#34385;&#34987;&#20302;&#20272;&#30340;&#22240;&#26524;&#29983;&#25104;&#36807;&#31243;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22240;&#26524;&#20851;&#31995;&#30340;&#35270;&#35282;&#30830;&#23450;&#20102;DNNs&#30340;&#33030;&#24369;&#24615;&#30340;&#28304;&#22836;&#65292;&#28982;&#21518;&#32473;&#20986;&#20102;&#22238;&#31572;"&#22914;&#20309;&#21457;&#36215;&#25915;&#20987;"&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#20854;&#27425;&#65292;&#32771;&#34385;&#21040;&#25915;&#20987;&#24178;&#39044;&#23545;&#24403;&#21069;&#26679;&#26412;&#29366;&#24577;&#30340;&#24433;&#21709;&#65292;&#20197;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CADE&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have been demonstrated to be vulnerable to well-crafted \emph{adversarial examples}, which are generated through either well-conceived $\mathcal{L}_p$-norm restricted or unrestricted attacks. Nevertheless, the majority of those approaches assume that adversaries can modify any features as they wish, and neglect the causal generating process of the data, which is unreasonable and unpractical. For instance, a modification in income would inevitably impact features like the debt-to-income ratio within a banking system. By considering the underappreciated causal generating process, first, we pinpoint the source of the vulnerability of DNNs via the lens of causality, then give theoretical results to answer \emph{where to attack}. Second, considering the consequences of the attack interventions on the current state of the examples to generate more realistic adversarial examples, we propose CADE, a framework that can generate \textbf{C}ounterfactual \textbf{AD}vers
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20013;&#38388;&#25110;&#36793;&#32536;&#32593;&#32476;&#33410;&#28857;&#19978;&#30340;&#29983;&#25104;&#32593;&#32476;&#23618;&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#34920;&#31034;&#21387;&#32553;&#21518;&#30340;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#32593;&#32476;&#20013;&#25152;&#38656;&#30340;&#25968;&#25454;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2312.05398</link><description>&lt;p&gt;
&#20855;&#26377;&#20154;&#24037;&#26234;&#33021;&#30340;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#29983;&#25104;&#32593;&#32476;&#23618;
&lt;/p&gt;
&lt;p&gt;
Generative Network Layer for Communication Systems with Artificial Intelligence. (arXiv:2312.05398v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05398
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20013;&#38388;&#25110;&#36793;&#32536;&#32593;&#32476;&#33410;&#28857;&#19978;&#30340;&#29983;&#25104;&#32593;&#32476;&#23618;&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#34920;&#31034;&#21387;&#32553;&#21518;&#30340;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#32593;&#32476;&#20013;&#25152;&#38656;&#30340;&#25968;&#25454;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32593;&#32476;&#23618;&#30340;&#20316;&#29992;&#26159;&#36890;&#36807;&#20013;&#38388;&#32593;&#32476;&#33410;&#28857;&#23558;&#25968;&#25454;&#21253;&#20174;&#28304;&#20256;&#36755;&#21040;&#30446;&#30340;&#22320;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20013;&#38388;&#25110;&#36793;&#32536;&#32593;&#32476;&#33410;&#28857;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#30340;&#29983;&#25104;&#32593;&#32476;&#23618;&#65292;&#24182;&#20998;&#26512;&#20854;&#23545;&#32593;&#32476;&#20013;&#25152;&#38656;&#25968;&#25454;&#36895;&#29575;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20854;&#20013;&#20351;&#29992;GenAI&#36741;&#21161;&#33410;&#28857;&#20174;&#21253;&#21547;&#22823;&#24133;&#21387;&#32553;&#30340;&#28508;&#22312;&#34920;&#31034;&#30340;&#25552;&#31034;&#20013;&#29983;&#25104;&#22270;&#20687;&#12290;&#22312;&#22270;&#20687;&#36136;&#37327;&#32422;&#26463;&#19979;&#36827;&#34892;&#30340;&#32593;&#32476;&#27969;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#29983;&#25104;&#32593;&#32476;&#23618;&#21487;&#20197;&#23454;&#29616;&#36229;&#36807;100%&#30340;&#25968;&#25454;&#36895;&#29575;&#35201;&#27714;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The traditional role of the network layer is the transfer of packet replicas from source to destination through intermediate network nodes. We present a generative network layer that uses Generative AI (GenAI) at intermediate or edge network nodes and analyze its impact on the required data rates in the network. We conduct a case study where the GenAI-aided nodes generate images from prompts that consist of substantially compressed latent representations. The results from network flow analyses under image quality constraints show that the generative network layer can achieve an improvement of more than 100% in terms of the required data rate.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#30340;&#21322;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#37327;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.04402</link><description>&lt;p&gt;
&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#20351;&#29992;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#30340;&#21322;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Active Learning for Semantic Segmentation in Unknown Environments Using Informative Path Planning. (arXiv:2312.04402v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04402
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#30340;&#21322;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#37327;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#23545;&#20854;&#29615;&#22659;&#36827;&#34892;&#24863;&#30693;&#21644;&#25512;&#29702;&#65292;&#36229;&#20986;&#20102;&#20960;&#20309;&#23398;&#30340;&#33539;&#30068;&#12290;&#22823;&#22810;&#25968;&#36825;&#31867;&#31995;&#32479;&#24314;&#31435;&#22312;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#12290;&#30001;&#20110;&#33258;&#20027;&#26426;&#22120;&#20154;&#36890;&#24120;&#37096;&#32626;&#22312;&#21021;&#22987;&#26410;&#30693;&#29615;&#22659;&#20013;&#65292;&#23545;&#38745;&#24577;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#19981;&#33021;&#24635;&#26159;&#25429;&#25417;&#21040;&#22810;&#26679;&#30340;&#39046;&#22495;&#65292;&#38480;&#21046;&#20102;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#30340;&#24863;&#30693;&#24615;&#33021;&#12290;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#20123;&#33258;&#30417;&#30563;&#21644;&#23436;&#20840;&#30417;&#30563;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#26426;&#22120;&#20154;&#30340;&#35270;&#35273;&#12290;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#25110;&#32773;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#26631;&#27880;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#33258;&#36866;&#24212;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#30340;&#35821;&#20041;&#20998;&#21106;&#20027;&#21160;&#23398;&#20064;&#65292;&#30456;&#27604;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#20154;&#24037;&#26631;&#27880;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#36866;&#24212;&#36335;&#24452;&#35268;&#21010;&#22120;&#26469;&#24341;&#23548;&#26426;&#22120;&#20154;&#25506;&#32034;&#26410;&#30693;&#31354;&#38388;&#30340;&#36793;&#30028;&#65292;&#24182;&#25910;&#38598;&#20855;&#26377;&#39640;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#32467;&#21512;&#20102;&#31232;&#30095;&#30340;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation enables robots to perceive and reason about their environments beyond geometry. Most of such systems build upon deep learning approaches. As autonomous robots are commonly deployed in initially unknown environments, pre-training on static datasets cannot always capture the variety of domains and limits the robot's perception performance during missions. Recently, self-supervised and fully supervised active learning methods emerged to improve a robot's vision. These approaches rely on large in-domain pre-training datasets or require substantial human labelling effort. We propose a planning method for semi-supervised active learning of semantic segmentation that substantially reduces human labelling requirements compared to fully supervised approaches. We leverage an adaptive map-based planner guided towards the frontiers of unexplored space with high model uncertainty collecting training data for human labelling. A key aspect of our approach is to combine the spars
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#21253;&#21547;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#28201;&#39034;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.13544</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Piecewise polynomial regression of tame functions via integer programming. (arXiv:2311.13544v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#21253;&#21547;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#28201;&#39034;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20272;&#35745;&#23646;&#20110;&#19968;&#31867;&#29305;&#23450;&#30340;&#38750;&#20809;&#28369;&#20989;&#25968;&#30340;&#20989;&#25968;&#30340;&#20219;&#21153;&#65292;&#21363;&#25152;&#35859;&#30340;&#28201;&#39034;&#20989;&#25968;&#12290;&#36825;&#20123;&#20989;&#25968;&#20986;&#29616;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65306;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;&#12289;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#20215;&#20540;&#20989;&#25968;&#25110;&#23567;&#20998;&#23376;&#30340;&#27874;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28201;&#39034;&#20989;&#25968;&#22312;&#20219;&#20309;&#23436;&#20840;&#32500;&#24230;&#30340;&#31435;&#26041;&#20307;&#19978;&#21487;&#29992;&#20998;&#27573;&#22810;&#39033;&#24335;&#26469;&#36924;&#36817;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#24418;&#24335;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#28201;&#39034;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of estimating functions belonging to a specific class of nonsmooth functions, namely so-called tame functions. These functions appear in a wide range of applications: training deep learning, value functions of mixed-integer programs, or wave functions of small molecules. We show that tame functions are approximable by piecewise polynomials on any full-dimensional cube. We then present the first ever mixed-integer programming formulation of piecewise polynomial regression. Together, these can be used to estimate tame functions. We demonstrate promising computational results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36890;&#20449;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#30693;&#35782;&#33976;&#39311;&#65288;CC-BAKD&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#28151;&#21512;&#26426;&#21046;&#23558;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#19982;&#21387;&#32553;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22312;&#23398;&#20064;&#32773;&#19982;&#25945;&#24072;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#26102;&#20851;&#20110;&#25209;&#27425;&#36873;&#25321;&#21644;&#25209;&#27425;&#32534;&#30721;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.08053</link><description>&lt;p&gt;
&#36890;&#20449;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Communication-Constrained Bayesian Active Knowledge Distillation. (arXiv:2311.08053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36890;&#20449;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#30693;&#35782;&#33976;&#39311;&#65288;CC-BAKD&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#28151;&#21512;&#26426;&#21046;&#23558;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#19982;&#21387;&#32553;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22312;&#23398;&#20064;&#32773;&#19982;&#25945;&#24072;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#26102;&#20851;&#20110;&#25209;&#27425;&#36873;&#25321;&#21644;&#25209;&#27425;&#32534;&#30721;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#37325;&#20256;&#65288;ARQ&#65289;&#21327;&#35758;&#26088;&#22312;&#30830;&#20445;&#25509;&#25910;&#26041;&#27491;&#30830;&#25509;&#25910;&#21040;&#21457;&#23556;&#26041;&#30340;&#25152;&#26377;&#20998;&#32452;&#12290;&#24403;&#21457;&#23556;&#26041;&#26159;&#19968;&#20010;&#23398;&#20064;&#32773;&#19982;&#19968;&#20010;&#25945;&#24072;&#36827;&#34892;&#36890;&#20449;&#26102;&#65292;&#36825;&#20010;&#30446;&#26631;&#19982;&#23398;&#20064;&#32773;&#30340;&#23454;&#38469;&#30446;&#26631;&#30456;&#20914;&#31361;&#65292;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#20174;&#25945;&#24072;&#37027;&#37324;&#33719;&#21462;&#26368;&#30456;&#20851;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#20174;&#20027;&#21160;&#23398;&#20064;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#26412;&#25991;&#35299;&#20915;&#20197;&#19979;&#20851;&#38190;&#21327;&#35758;&#35774;&#35745;&#38382;&#39064;&#65306;(i)&#20027;&#21160;&#25209;&#27425;&#36873;&#25321;&#65306;&#24212;&#35813;&#21457;&#36865;&#21738;&#20010;&#25209;&#27425;&#30340;&#36755;&#20837;&#32473;&#25945;&#24072;&#20197;&#33719;&#21462;&#26368;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#23569;&#36890;&#20449;&#36718;&#27425;&#30340;&#25968;&#37327;&#65311;(ii)&#25209;&#27425;&#32534;&#30721;&#65306;&#26159;&#21542;&#21487;&#20197;&#32452;&#21512;&#25968;&#25454;&#28857;&#30340;&#25209;&#27425;&#20197;&#20943;&#23569;&#27599;&#20010;&#36890;&#20449;&#36718;&#27425;&#25152;&#38656;&#30340;&#36890;&#20449;&#36164;&#28304;&#65311;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#36890;&#20449;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#30693;&#35782;&#33976;&#39311;&#65288;CC-BAKD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#32447;&#24615;&#28151;&#21512;&#26426;&#21046;&#23558;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#19982;&#21387;&#32553;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional retransmission (ARQ) protocols are designed with the goal of ensuring the correct reception of all the individual transmitter's packets at the receiver. When the transmitter is a learner communicating with a teacher, this goal is at odds with the actual aim of the learner, which is that of eliciting the most relevant label information from the teacher. Taking an active learning perspective, this paper addresses the following key protocol design questions: (i) Active batch selection: Which batch of inputs should be sent to the teacher to acquire the most useful information and thus reduce the number of required communication rounds? (ii) Batch encoding: Can batches of data points be combined to reduce the communication resources required at each communication round? Specifically, this work introduces Communication-Constrained Bayesian Active Knowledge Distillation (CC-BAKD), a novel protocol that integrates Bayesian active learning with compression via a linear mix-up mecha
&lt;/p&gt;</description></item><item><title>ViR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#37319;&#29992;&#21452;&#37325;&#24182;&#34892;&#21644;&#36882;&#24402;&#20844;&#24335;&#65292;&#20174;&#32780;&#22312;&#24555;&#36895;&#25512;&#29702;&#21644;&#24182;&#34892;&#35757;&#32451;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#24179;&#34913;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19731</link><description>&lt;p&gt;
ViR: &#36808;&#21521;&#39640;&#25928;&#35270;&#35273;&#20445;&#30041;&#39592;&#24178;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ViR: Towards Efficient Vision Retention Backbones. (arXiv:2310.19731v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19731
&lt;/p&gt;
&lt;p&gt;
ViR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#37319;&#29992;&#21452;&#37325;&#24182;&#34892;&#21644;&#36882;&#24402;&#20844;&#24335;&#65292;&#20174;&#32780;&#22312;&#24555;&#36895;&#25512;&#29702;&#21644;&#24182;&#34892;&#35757;&#32451;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#24179;&#34913;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#22240;&#20854;&#22312;&#24314;&#27169;&#38271;&#31243;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#22823;&#35268;&#27169;&#35757;&#32451;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35757;&#32451;&#24182;&#34892;&#24615;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#20854;&#20108;&#27425;&#22797;&#26434;&#24230;&#38459;&#30861;&#20102;ViTs&#22312;&#35768;&#22810;&#38656;&#35201;&#24555;&#36895;&#25512;&#29702;&#30340;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#24433;&#21709;&#22312;&#38656;&#35201;&#33258;&#22238;&#24402;&#24314;&#27169;&#36755;&#20837;&#29305;&#24449;&#30340;&#24212;&#29992;&#20013;&#23588;&#20026;&#26126;&#26174;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#19968;&#31181;&#26032;&#30340;&#21162;&#21147;&#26041;&#21521;&#25552;&#20986;&#20102;&#20855;&#26377;&#21487;&#24182;&#34892;&#21270;&#27169;&#22411;&#21644;&#36882;&#24402;&#20844;&#24335;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#24212;&#29992;&#20013;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;&#12290;&#21463;&#21040;&#36825;&#19968;&#36235;&#21183;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#21517;&#20026;Vision Retention Networks&#65288;ViR&#65289;&#65292;&#20855;&#26377;&#21452;&#37325;&#24182;&#34892;&#21644;&#36882;&#24402;&#20844;&#24335;&#65292;&#21487;&#20197;&#22312;&#24555;&#36895;&#25512;&#29702;&#21644;&#24182;&#34892;&#35757;&#32451;&#26041;&#38754;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have attracted a lot of popularity in recent years, due to their exceptional capabilities in modeling long-range spatial dependencies and scalability for large scale training. Although the training parallelism of self-attention mechanism plays an important role in retaining great performance, its quadratic complexity baffles the application of ViTs in many scenarios which demand fast inference. This effect is even more pronounced in applications in which autoregressive modeling of input features is required. In Natural Language Processing (NLP), a new stream of efforts has proposed parallelizable models with recurrent formulation that allows for efficient inference in generative applications. Inspired by this trend, we propose a new class of computer vision models, dubbed Vision Retention Networks (ViR), with dual parallel and recurrent formulations, which strike an optimal balance between fast inference and parallel training with competitive performance. In 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20010;&#24615;&#21270;&#33976;&#39311;&#65292;&#23558;&#38381;&#28304;LLMs&#30340;&#33021;&#21147;&#20256;&#36882;&#32473;&#24320;&#28304;LLMs&#65292;&#24182;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#26631;&#20934;&#33976;&#39311;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21482;&#20351;&#29992;&#19977;&#20998;&#20043;&#19968;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.18628</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#33976;&#39311;&#65306;&#20026;&#20195;&#30721;&#29983;&#25104;&#36171;&#33021;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#24320;&#28304;LLMs
&lt;/p&gt;
&lt;p&gt;
Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation. (arXiv:2310.18628v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18628
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20010;&#24615;&#21270;&#33976;&#39311;&#65292;&#23558;&#38381;&#28304;LLMs&#30340;&#33021;&#21147;&#20256;&#36882;&#32473;&#24320;&#28304;LLMs&#65292;&#24182;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#26631;&#20934;&#33976;&#39311;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21482;&#20351;&#29992;&#19977;&#20998;&#20043;&#19968;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#22823;&#30340;&#38381;&#28304;LLMs&#65288;ChatGPT&#65292;GPT-4&#65289;&#30340;&#23835;&#36215;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#23558;&#38381;&#28304;LLMs&#30340;&#21151;&#33021;&#33976;&#39311;&#21040;&#36739;&#23567;&#30340;&#24320;&#28304;LLMs&#20013;&#34920;&#31034;&#20852;&#36259;&#12290;&#20197;&#24448;&#30340;&#33976;&#39311;&#26041;&#27861;&#36890;&#24120;&#26159;&#24341;&#23548;ChatGPT&#29983;&#25104;&#19968;&#32452;&#25351;&#20196;&#21644;&#31572;&#26696;&#65292;&#20197;&#20379;&#23398;&#29983;&#27169;&#22411;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26631;&#20934;&#33976;&#39311;&#26041;&#27861;&#24573;&#35270;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#20248;&#28857;&#21644;&#26465;&#20214;&#12290;&#21463;&#29616;&#20195;&#25945;&#23398;&#21407;&#21017;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#33976;&#39311;&#36807;&#31243;&#65292;&#20854;&#20013;&#23398;&#29983;&#39318;&#20808;&#23581;&#35797;&#35299;&#20915;&#19968;&#20010;&#20219;&#21153;&#65292;&#28982;&#21518;&#32769;&#24072;&#25552;&#20379;&#33258;&#36866;&#24212;&#30340;&#25913;&#36827;&#26041;&#27861;&#26469;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#12290;&#20010;&#24615;&#21270;&#33976;&#39311;&#19981;&#21516;&#20110;&#25552;&#20379;&#32473;&#23398;&#29983;&#32769;&#24072;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#23427;&#20351;&#23398;&#29983;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#20010;&#24615;&#21270;&#23398;&#20064;&#65292;&#21482;&#22312;&#33258;&#24049;&#29359;&#38169;&#35823;&#30340;&#31034;&#20363;&#19978;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25913;&#36827;&#33258;&#24049;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#65292;&#20010;&#24615;&#21270;&#33976;&#39311;&#22987;&#32456;&#20248;&#20110;&#21482;&#20351;&#29992;&#19977;&#20998;&#20043;&#19968;&#25968;&#25454;&#30340;&#26631;&#20934;&#33976;&#39311;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model to learn. However, such standard distillation approach neglects the merits and conditions of the student model. Inspired by modern teaching principles, we design a personalised distillation process, in which the student attempts to solve a task first, then the teacher provides an adaptive refinement for the student to improve. Instead of feeding the student with teacher's prior, personalised distillation enables personalised learning for the student model, as it only learns on examples it makes mistakes upon and learns to improve its own solution. On code generation, personalised distillation consistently outperforms standard distillation with only one third of the data. With only 2.5-3K perso
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;G-&#19977;&#37325;&#30456;&#20851;&#23618;&#65292;&#22312;G-&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#24378;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23436;&#22791;&#30340;&#19977;&#37325;&#30456;&#20851;&#29702;&#35770;&#65292;&#36825;&#20351;&#24471;G-TC&#23618;&#33021;&#22815;&#22312;&#38754;&#23545;&#19981;&#21464;&#24615;&#25915;&#20987;&#26102;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#19978;&#30456;&#27604;&#26631;&#20934;&#30340;Max G-Pooling&#26377;&#26126;&#26174;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2310.18564</link><description>&lt;p&gt;
&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#23454;&#29616;G-&#31561;&#21464;&#32593;&#32476;&#20013;&#30340;&#24378;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Robust G-Invariance in G-Equivariant Networks. (arXiv:2310.18564v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18564
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;G-&#19977;&#37325;&#30456;&#20851;&#23618;&#65292;&#22312;G-&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#24378;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23436;&#22791;&#30340;&#19977;&#37325;&#30456;&#20851;&#29702;&#35770;&#65292;&#36825;&#20351;&#24471;G-TC&#23618;&#33021;&#22815;&#22312;&#38754;&#23545;&#19981;&#21464;&#24615;&#25915;&#20987;&#26102;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#19978;&#30456;&#27604;&#26631;&#20934;&#30340;Max G-Pooling&#26377;&#26126;&#26174;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;G-&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;G-CNNs&#65289;&#20013;&#30340;&#24378;&#32452;&#19981;&#21464;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;G-&#19977;&#37325;&#30456;&#20851;&#65288;G-TC&#65289;&#23618;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#32676;&#19978;&#30340;&#19977;&#37325;&#30456;&#20851;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#26159;&#21807;&#19968;&#30340;&#12289;&#26368;&#20302;&#27425;&#25968;&#30340;&#22810;&#39033;&#24335;&#19981;&#21464;&#26144;&#23556;&#65292;&#21516;&#26102;&#20063;&#26159;&#23436;&#22791;&#30340;&#12290;&#35768;&#22810;&#24120;&#29992;&#30340;&#19981;&#21464;&#26144;&#23556;&#65292;&#20363;&#22914;max&#65292;&#26159;&#19981;&#23436;&#22791;&#30340;&#65306;&#23427;&#20204;&#20250;&#21516;&#26102;&#21435;&#38500;&#32676;&#21644;&#20449;&#21495;&#32467;&#26500;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23436;&#22791;&#30340;&#19981;&#21464;&#26144;&#23556;&#21482;&#31227;&#38500;&#30001;&#20110;&#32676;&#20316;&#29992;&#24341;&#36215;&#30340;&#21464;&#21270;&#65292;&#21516;&#26102;&#20445;&#30041;&#26377;&#20851;&#20449;&#21495;&#32467;&#26500;&#30340;&#25152;&#26377;&#20449;&#24687;&#12290;&#19977;&#37325;&#30456;&#20851;&#30340;&#23436;&#22791;&#24615;&#36171;&#20104;&#20102;G-TC&#23618;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#21487;&#20197;&#22312;&#20854;&#23545;&#19981;&#21464;&#24615;&#25915;&#20987;&#30340;&#25269;&#25239;&#20013;&#35266;&#23519;&#21040;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#30456;&#27604;&#20110;G-CNN&#26550;&#26500;&#20013;&#30340;&#26631;&#20934;Max G-Pooling&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#19978;&#26377;&#26126;&#26174;&#30340;&#25913;&#21892;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#26041;&#27861;&#30340;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a general method for achieving robust group-invariance in group-equivariant convolutional neural networks ($G$-CNNs), which we call the $G$-triple-correlation ($G$-TC) layer. The approach leverages the theory of the triple-correlation on groups, which is the unique, lowest-degree polynomial invariant map that is also complete. Many commonly used invariant maps--such as the max--are incomplete: they remove both group and signal structure. A complete invariant, by contrast, removes only the variation due to the actions of the group, while preserving all information about the structure of the signal. The completeness of the triple correlation endows the $G$-TC layer with strong robustness, which can be observed in its resistance to invariance-based adversarial attacks. In addition, we observe that it yields measurable improvements in classification accuracy over standard Max $G$-Pooling in $G$-CNN architectures. We provide a general and efficient implementation of the method 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;U-Nets&#65292;&#36890;&#36807;3D&#38647;&#36798;&#21453;&#23556;&#29575;&#65292;&#25104;&#21151;&#22320;&#20272;&#35745;&#20102;&#26368;&#22823;&#22402;&#30452;&#36895;&#24230;&#21450;&#20854;&#38754;&#31215;&#33539;&#22260;&#65292;&#24182;&#37319;&#29992;Sinh-arcsinh-normal&#65288;SHASH&#65289;&#20998;&#24067;&#21442;&#25968;&#22238;&#24402;&#25216;&#26415;&#36827;&#34892;&#20102;&#30830;&#23450;&#24615;&#21644;&#27010;&#29575;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.09392</link><description>&lt;p&gt;
&#20174;&#38647;&#36798;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#26368;&#22823;&#22402;&#30452;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Estimation of Maximum Vertical Velocity from Radar. (arXiv:2310.09392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;U-Nets&#65292;&#36890;&#36807;3D&#38647;&#36798;&#21453;&#23556;&#29575;&#65292;&#25104;&#21151;&#22320;&#20272;&#35745;&#20102;&#26368;&#22823;&#22402;&#30452;&#36895;&#24230;&#21450;&#20854;&#38754;&#31215;&#33539;&#22260;&#65292;&#24182;&#37319;&#29992;Sinh-arcsinh-normal&#65288;SHASH&#65289;&#20998;&#24067;&#21442;&#25968;&#22238;&#24402;&#25216;&#26415;&#36827;&#34892;&#20102;&#30830;&#23450;&#24615;&#21644;&#27010;&#29575;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26159;&#20005;&#37325;&#22825;&#27668;&#28798;&#23475;&#30340;&#28304;&#22836;&#65292;&#20294;&#23545;&#24555;&#36895;&#19978;&#21319;&#27668;&#27969;&#65288;&#21363;&#19978;&#21319;&#27668;&#27969;&#65289;&#30340;&#37327;&#21270;&#20173;&#26080;&#27861;&#29992;&#20110;&#25805;&#20316;&#39044;&#27979;&#12290;&#20687;&#21355;&#26143;&#22270;&#20687;&#20013;&#30340;&#36879;&#39030;&#21306;&#22495;&#36825;&#26679;&#30340;&#19978;&#21319;&#27668;&#27969;&#20195;&#29702;&#29289;&#24050;&#34987;&#19982;&#20005;&#37325;&#22825;&#27668;&#28798;&#23475;&#32852;&#31995;&#36215;&#26469;&#65292;&#20294;&#21482;&#19982;&#24635;&#20307;&#39118;&#26292;&#19978;&#21319;&#27668;&#27969;&#30340;&#19968;&#23567;&#37096;&#20998;&#26377;&#20851;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;U-Nets&#65292;&#26159;&#21542;&#33021;&#22815;&#20165;&#21033;&#29992;&#19977;&#32500;&#65288;3D&#65289;&#26684;&#32593;&#38647;&#36798;&#21453;&#23556;&#29575;&#65292;&#31934;&#30830;&#22320;&#25552;&#21462;&#26368;&#22823;&#22402;&#30452;&#36895;&#24230;&#21450;&#20854;&#38754;&#31215;&#33539;&#22260;&#12290;&#35813;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20351;&#29992;&#27169;&#25311;&#38647;&#36798;&#21453;&#23556;&#29575;&#21644;&#22402;&#30452;&#36895;&#24230;&#35757;&#32451;&#20110;&#22269;&#23478;&#20005;&#37325;&#39118;&#26292;&#23454;&#39564;&#23460;&#30340;&#39044;&#27979;&#39044;&#35686;&#31995;&#32479;&#65288;WoFS&#65289;&#12290;&#37319;&#29992;Sinh-arcsinh-normal&#65288;SHASH&#65289;&#20998;&#24067;&#30340;&#21442;&#25968;&#22238;&#24402;&#25216;&#26415;&#26469;&#36866;&#24212;UNets&#65292;&#20801;&#35768;&#23545;&#26368;&#22823;&#22402;&#30452;&#36895;&#24230;&#36827;&#34892;&#30830;&#23450;&#24615;&#21644;&#27010;&#29575;&#39044;&#27979;&#12290;&#32463;&#36807;&#36229;&#21442;&#25968;&#25628;&#32034;&#21518;&#65292;&#36873;&#20986;&#20102;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite being the source region of severe weather hazards, the quantification of the fast current of upward moving air (i.e., updraft) remains unavailable for operational forecasting. Updraft proxies, like overshooting top area from satellite images, have been linked to severe weather hazards but only relate to a limited portion of the total storm updraft. This study investigates if a machine learning model, namely U-Nets, can skillfully retrieve maximum vertical velocity and its areal extent from 3-dimensional (3D) gridded radar reflectivity alone. The machine learning model is trained using simulated radar reflectivity and vertical velocity from the National Severe Storm Laboratory's convection permitting Warn on Forecast System (WoFS). A parametric regression technique using the Sinh-arcsinh-normal (SHASH) distribution is adapted to run with UNets, allowing for both deterministic and probabilistic predictions of maximum vertical velocity. The best models after hyperparameter search 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;60 GHz FMCW&#38647;&#36798;&#30340;&#36731;&#37327;&#32423;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#20116;&#20010;&#29305;&#24449;&#21644;&#31934;&#31616;&#30340;&#22788;&#29702;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#39640;&#25928;&#35782;&#21035;&#25163;&#21183;&#65292;&#21516;&#26102;&#20855;&#26377;&#36739;&#20302;&#30340;&#20869;&#23384;&#12289;&#35745;&#31639;&#21644;&#21151;&#32791;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.08876</link><description>&lt;p&gt;
&#36793;&#32536;FMCW&#38647;&#36798;&#30340;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Gesture Recognition for FMCW Radar on the Edge. (arXiv:2310.08876v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;60 GHz FMCW&#38647;&#36798;&#30340;&#36731;&#37327;&#32423;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#20116;&#20010;&#29305;&#24449;&#21644;&#31934;&#31616;&#30340;&#22788;&#29702;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#39640;&#25928;&#35782;&#21035;&#25163;&#21183;&#65292;&#21516;&#26102;&#20855;&#26377;&#36739;&#20302;&#30340;&#20869;&#23384;&#12289;&#35745;&#31639;&#21644;&#21151;&#32791;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;60 GHz&#35843;&#39057;&#36830;&#32493;&#27874;(FMCW)&#38647;&#36798;&#30340;&#36731;&#37327;&#32423;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25163;&#21183;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#20116;&#20010;&#29305;&#24449;&#26377;&#25928;&#22320;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#31616;&#30340;&#38647;&#36798;&#22788;&#29702;&#31639;&#27861;&#26469;&#25552;&#21462;&#36825;&#20123;&#29305;&#24449;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#32321;&#37325;&#30340;&#20108;&#32500;&#22788;&#29702;&#65292;&#21363;&#36317;&#31163;-&#22810;&#26222;&#21202;&#25104;&#20687;&#65292;&#24182;&#25913;&#20026;&#36827;&#34892;&#26089;&#26399;&#30446;&#26631;&#26816;&#27979;-&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#31995;&#32479;&#31227;&#26893;&#21040;&#20855;&#26377;&#20869;&#23384;&#12289;&#35745;&#31639;&#21644;&#21151;&#32791;&#20005;&#26684;&#38480;&#21046;&#30340;&#23436;&#20840;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#12290;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#30340;&#26550;&#26500;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#20849;&#21516;&#26816;&#27979;&#21644;&#20998;&#31867;&#20116;&#31181;&#19981;&#21516;&#30340;&#25163;&#21183;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#25105;&#20204;&#30340;&#20445;&#30041;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20197;98.4%&#30340;F1&#20998;&#25968;&#35782;&#21035;&#25163;&#21183;&#65292;&#23427;&#22312;&#19968;&#20010;Arm Cortex-M4&#24494;&#25511;&#21046;&#22120;&#19978;&#36816;&#34892;&#65292;&#38656;&#35201;&#19981;&#21040;280 kB&#30340;&#38378;&#23384;&#23384;&#20648;&#22120;&#65292;120 kB&#30340;RAM&#65292;&#24182;&#28040;&#32791;75 mW&#30340;&#21151;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a lightweight gesture recognition system based on 60 GHz frequency modulated continuous wave (FMCW) radar. We show that gestures can be characterized efficiently by a set of five features, and propose a slim radar processing algorithm to extract these features. In contrast to previous approaches, we avoid heavy 2D processing, i.e. range-Doppler imaging, and perform instead an early target detection - this allows us to port the system to fully embedded platforms with tight constraints on memory, compute and power consumption. A recurrent neural network (RNN) based architecture exploits these features to jointly detect and classify five different gestures. The proposed system recognizes gestures with an F1 score of 98.4% on our hold-out test dataset, it runs on an Arm Cortex-M4 microcontroller requiring less than 280 kB of flash memory, 120 kB of RAM, and consuming 75 mW of power.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SafeDPA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#35299;&#20915;&#31574;&#30053;&#36866;&#24212;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;SafeDPA&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#32852;&#21512;&#23398;&#20064;&#33258;&#36866;&#24212;&#31574;&#30053;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#30830;&#20445;&#20102;SafeDPA&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08602</link><description>&lt;p&gt;
&#23433;&#20840;&#28145;&#24230;&#31574;&#30053;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Safe Deep Policy Adaptation. (arXiv:2310.08602v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08602
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SafeDPA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#35299;&#20915;&#31574;&#30053;&#36866;&#24212;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;SafeDPA&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#32852;&#21512;&#23398;&#20064;&#33258;&#36866;&#24212;&#31574;&#30053;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#30830;&#20445;&#20102;SafeDPA&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#26159;&#20351;&#33258;&#20027;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#21160;&#24577;&#21644;&#19981;&#30830;&#23450;&#30340;&#29615;&#22659;&#20013;&#24555;&#36895;&#36866;&#24212;&#12290;&#32463;&#20856;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#21644;&#23433;&#20840;&#25511;&#21046;&#25552;&#20379;&#20102;&#31283;&#23450;&#24615;&#21644;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#20294;&#20165;&#38480;&#20110;&#29305;&#23450;&#30340;&#31995;&#32479;&#31867;&#21035;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#31574;&#30053;&#36866;&#24212;&#25552;&#20379;&#20102;&#36890;&#29992;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SafeDPA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;RL&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#31574;&#30053;&#36866;&#24212;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;SafeDPA&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#32852;&#21512;&#23398;&#20064;&#33258;&#36866;&#24212;&#31574;&#30053;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#39044;&#27979;&#29615;&#22659;&#37197;&#32622;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#23545;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;RL&#31574;&#30053;&#20043;&#19978;&#24341;&#20837;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBF&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#20197;&#30830;&#20445;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;SafeDPA&#30340;&#29702;&#35770;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;SafeDPA&#23545;&#23398;&#20064;&#35823;&#24046;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A critical goal of autonomy and artificial intelligence is enabling autonomous robots to rapidly adapt in dynamic and uncertain environments. Classic adaptive control and safe control provide stability and safety guarantees but are limited to specific system classes. In contrast, policy adaptation based on reinforcement learning (RL) offers versatility and generalizability but presents safety and robustness challenges. We propose SafeDPA, a novel RL and control framework that simultaneously tackles the problems of policy adaptation and safe reinforcement learning. SafeDPA jointly learns adaptive policy and dynamics models in simulation, predicts environment configurations, and fine-tunes dynamics models with few-shot real-world data. A safety filter based on the Control Barrier Function (CBF) on top of the RL policy is introduced to ensure safety during real-world deployment. We provide theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA against learning errors 
&lt;/p&gt;</description></item><item><title>ECoFLaP&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#21387;&#32553;&#21644;&#37096;&#32626;&#26102;&#30340;&#35745;&#31639;&#21644;&#33021;&#32791;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02998</link><description>&lt;p&gt;
ECoFLaP: &#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models. (arXiv:2310.02998v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02998
&lt;/p&gt;
&lt;p&gt;
ECoFLaP&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#21387;&#32553;&#21644;&#37096;&#32626;&#26102;&#30340;&#35745;&#31639;&#21644;&#33021;&#32791;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#20840;&#38754;&#29702;&#35299;&#19990;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#35745;&#31639;/&#33021;&#32791;&#21644;&#30899;&#25490;&#25918;&#65292;&#37096;&#32626;LVLMs&#24448;&#24448;&#23384;&#22312;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20351;&#24471;&#37319;&#29992;&#20256;&#32479;&#30340;&#36845;&#20195;&#20840;&#23616;&#21098;&#26525;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#20854;&#38656;&#35201;&#35745;&#31639;&#25972;&#20010;&#22823;&#22411;&#27169;&#22411;&#30340;Hessian&#30697;&#38453;&#20197;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#20840;&#23616;&#21098;&#26525;&#30340;&#26114;&#36149;&#35745;&#31639;&#65292;&#24182;&#26681;&#25454;&#23618;&#20869;&#26435;&#37325;&#30340;&#37325;&#35201;&#24615;&#26377;&#25928;&#21387;&#32553;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#30001;&#20110;&#32570;&#20047;&#20840;&#23616;&#35270;&#35282;&#32780;&#23548;&#33268;&#27169;&#22411;&#21387;&#32553;&#19981;&#22815;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#27169;&#22411;&#26368;&#36817;&#39640;&#25928;&#21098;&#26525;&#26041;&#27861;&#30340;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65288;ECoFLaP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (LVLMs) can understand the world comprehensively by integrating rich information from different modalities, achieving remarkable performance improvements on various multimodal downstream tasks. However, deploying LVLMs is often problematic due to their massive computational/energy costs and carbon consumption. Such issues make it infeasible to adopt conventional iterative global pruning, which is costly due to computing the Hessian matrix of the entire large model for sparsification. Alternatively, several studies have recently proposed layer-wise pruning approaches to avoid the expensive computation of global pruning and efficiently compress model weights according to their importance within a layer. However, these methods often suffer from suboptimal model compression due to their lack of a global perspective. To address this limitation in recent efficient pruning methods for large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP), 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20132;&#25442;&#24335;&#20849;&#24418;&#39118;&#38505;&#25511;&#21046;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#19981;&#21487;&#20132;&#25442;&#30340;&#24773;&#20917;&#19979;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.01262</link><description>&lt;p&gt;
&#38750;&#20132;&#25442;&#24335;&#20849;&#24418;&#39118;&#38505;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Non-Exchangeable Conformal Risk Control. (arXiv:2310.01262v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20132;&#25442;&#24335;&#20849;&#24418;&#39118;&#38505;&#25511;&#21046;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#19981;&#21487;&#20132;&#25442;&#30340;&#24773;&#20917;&#19979;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#20026;&#40657;&#21283;&#23376;&#31070;&#32463;&#27169;&#22411;&#30340;&#39044;&#27979;&#25552;&#20379;&#24418;&#24335;&#19978;&#20445;&#35777;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#25110;&#21306;&#38388;&#65292;&#30830;&#20445;&#21253;&#21547;&#23454;&#38469;&#30495;&#23454;&#20540;&#30340;&#39044;&#23450;&#20041;&#27010;&#29575;&#65292;&#25286;&#20998;&#20849;&#24418;&#39044;&#27979;&#24341;&#21457;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#34429;&#28982;&#26368;&#21021;&#30340;&#20844;&#24335;&#20551;&#35774;&#25968;&#25454;&#21487;&#20132;&#25442;&#65292;&#20294;&#19968;&#20123;&#25193;&#23637;&#22788;&#29702;&#19981;&#21487;&#20132;&#25442;&#30340;&#25968;&#25454;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#21516;&#26102;&#65292;&#19968;&#20123;&#36827;&#23637;&#24050;&#32463;&#22312;&#20849;&#24418;&#26041;&#27861;&#20013;&#21462;&#24471;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#26356;&#24191;&#27867;&#30340;&#30446;&#26631;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#65292;&#20363;&#22914;&#38480;&#21046;&#26368;&#20339;F1&#20998;&#25968;&#25110;&#20197;&#26399;&#26395;&#26368;&#23567;&#21270;&#35823;&#25253;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21644;&#25193;&#23637;&#36825;&#20004;&#20010;&#24037;&#20316;&#32447;&#36335;&#65292;&#25552;&#20986;&#20102;&#38750;&#20132;&#25442;&#24335;&#20849;&#24418;&#39118;&#38505;&#25511;&#21046;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#19981;&#21487;&#20132;&#25442;&#30340;&#24773;&#20917;&#19979;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#28789;&#27963;&#65292;&#20551;&#35774;&#24456;&#23569;&#65292;&#24182;&#20801;&#35768;&#26681;&#25454;&#25968;&#25454;&#30340;&#32479;&#35745;&#30456;&#20284;&#24615;&#36827;&#34892;&#21152;&#26435;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split conformal prediction has recently sparked great interest due to its ability to provide formally guaranteed uncertainty sets or intervals for predictions made by black-box neural models, ensuring a predefined probability of containing the actual ground truth. While the original formulation assumes data exchangeability, some extensions handle non-exchangeable data, which is often the case in many real-world scenarios. In parallel, some progress has been made in conformal methods that provide statistical guarantees for a broader range of objectives, such as bounding the best F1-score or minimizing the false negative rate in expectation. In this paper, we leverage and extend these two lines of work by proposing non-exchangeable conformal risk control, which allows controlling the expected value of any monotone loss function when the data is not exchangeable. Our framework is flexible, makes very few assumptions, and allows weighting the data based on its statistical similarity with t
&lt;/p&gt;</description></item><item><title>TraCE&#26159;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#36827;&#23637;&#12290;&#23427;&#33021;&#22815;&#23558;&#39640;&#24230;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#36827;&#23637;&#20957;&#32451;&#20026;&#19968;&#20010;&#21333;&#19968;&#20540;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#27668;&#20505;&#21464;&#21270;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15965</link><description>&lt;p&gt;
TraCE: &#36712;&#36857;&#21453;&#20107;&#23454;&#35299;&#37322;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
TraCE: Trajectory Counterfactual Explanation Scores. (arXiv:2309.15965v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15965
&lt;/p&gt;
&lt;p&gt;
TraCE&#26159;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#36827;&#23637;&#12290;&#23427;&#33021;&#22815;&#23558;&#39640;&#24230;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#36827;&#23637;&#20957;&#32451;&#20026;&#19968;&#20010;&#21333;&#19968;&#20540;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#27668;&#20505;&#21464;&#21270;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#21644;&#30456;&#20851;&#31639;&#27861;&#34917;&#25937;&#36890;&#24120;&#34987;&#29992;&#20110;&#29702;&#35299;&#12289;&#35299;&#37322;&#21644;&#21487;&#33021;&#25913;&#21464;&#26469;&#33258;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#21453;&#20107;&#23454;&#25193;&#23637;&#24212;&#29992;&#20110;&#35780;&#20272;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#36827;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;TraCE&#65288;&#36712;&#36857;&#21453;&#20107;&#23454;&#35299;&#37322;&#65289;&#20998;&#25968;&#65292;&#33021;&#22815;&#23558;&#39640;&#24230;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#36827;&#23637;&#20957;&#32451;&#20026;&#19968;&#20010;&#21333;&#19968;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#22312;&#28085;&#30422;&#21307;&#30103;&#20445;&#20581;&#21644;&#27668;&#20505;&#21464;&#21270;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;TraCE&#30340;&#23454;&#29992;&#24615;&#26469;&#35777;&#26126;&#20854;&#20027;&#35201;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations, and their associated algorithmic recourse, are typically leveraged to understand, explain, and potentially alter a prediction coming from a black-box classifier. In this paper, we propose to extend the use of counterfactuals to evaluate progress in sequential decision making tasks. To this end, we introduce a model-agnostic modular framework, TraCE (Trajectory Counterfactual Explanation) scores, which is able to distill and condense progress in highly complex scenarios into a single value. We demonstrate TraCE's utility across domains by showcasing its main properties in two case studies spanning healthcare and climate change.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#22312;&#32622;&#25442;&#32676;&#20316;&#29992;&#19979;&#30340;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;&#65292;&#24182;&#36890;&#36807;&#34892;&#21015;&#24335;&#21464;&#37327;&#30340;&#30452;&#31215;&#25551;&#36848;&#20102;&#31561;&#21464;&#25110;&#19981;&#21464;&#23376;&#21464;&#37327;&#30340;&#29305;&#24615;&#21644;&#22855;&#24322;&#24615;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#26435;&#20540;&#20849;&#20139;&#23646;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#31561;&#21464;&#21644;&#19981;&#21464;&#32447;&#24615;&#32593;&#32476;&#21442;&#25968;&#21270;&#21644;&#35774;&#35745;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2309.13736</link><description>&lt;p&gt;
&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#24615;&#36136;&#65306;&#32622;&#25442;&#32676;&#19979;&#30340;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Geometry of Linear Neural Networks: Equivariance and Invariance under Permutation Groups. (arXiv:2309.13736v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#22312;&#32622;&#25442;&#32676;&#20316;&#29992;&#19979;&#30340;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;&#65292;&#24182;&#36890;&#36807;&#34892;&#21015;&#24335;&#21464;&#37327;&#30340;&#30452;&#31215;&#25551;&#36848;&#20102;&#31561;&#21464;&#25110;&#19981;&#21464;&#23376;&#21464;&#37327;&#30340;&#29305;&#24615;&#21644;&#22855;&#24322;&#24615;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#26435;&#20540;&#20849;&#20139;&#23646;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#31561;&#21464;&#21644;&#19981;&#21464;&#32447;&#24615;&#32593;&#32476;&#21442;&#25968;&#21270;&#21644;&#35774;&#35745;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#32447;&#24615;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#20989;&#25968;&#38598;&#21512;&#26159;&#19968;&#20010;&#34892;&#21015;&#24335;&#21464;&#37327;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32622;&#25442;&#32676;&#20316;&#29992;&#19979;&#31561;&#21464;&#25110;&#19981;&#21464;&#30340;&#20989;&#25968;&#23376;&#21464;&#37327;&#12290;&#36825;&#26679;&#30340;&#32676;&#20316;&#29992;&#31034;&#20363;&#21253;&#25324;&#23545;&#22270;&#20687;&#30340;&#24179;&#31227;&#25110;90&#24230;&#26059;&#36716;&#12290;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#31561;&#21464;&#25110;&#19981;&#21464;&#23376;&#21464;&#37327;&#25551;&#36848;&#20026;&#34892;&#21015;&#24335;&#21464;&#37327;&#30340;&#30452;&#31215;&#65292;&#20174;&#20013;&#25512;&#23548;&#20986;&#20854;&#32500;&#24230;&#12289;&#27425;&#25968;&#12289;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#12289;&#20197;&#21450;&#22855;&#24322;&#24615;&#12290;&#25105;&#20204;&#23436;&#20840;&#21051;&#30011;&#20102;&#20219;&#24847;&#32622;&#25442;&#32676;&#30340;&#19981;&#21464;&#24615;&#65292;&#20197;&#21450;&#24490;&#29615;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#26435;&#20540;&#20849;&#20139;&#23646;&#24615;&#65292;&#23545;&#31561;&#21464;&#21644;&#19981;&#21464;&#32447;&#24615;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#21644;&#35774;&#35745;&#24471;&#20986;&#32467;&#35770;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#19981;&#21464;&#30340;&#32447;&#24615;&#20989;&#25968;&#37117;&#21487;&#20197;&#30001;&#19968;&#20010;&#20855;&#26377;&#26435;&#20540;&#20849;&#20139;&#23646;&#24615;&#30340;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#26469;&#21442;&#25968;&#21270;&#65292;&#35813;&#23646;&#24615;&#26159;&#30001;&#25152;&#32771;&#34385;&#32622;&#25442;&#30340;&#24490;&#29615;&#20998;&#35299;&#25152;&#24378;&#21152;&#30340;&#12290;&#31561;&#21464;&#20989;&#25968;&#30340;&#31209;&#21463;&#38480;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
The set of functions parameterized by a linear fully-connected neural network is a determinantal variety. We investigate the subvariety of functions that are equivariant or invariant under the action of a permutation group. Examples of such group actions are translations or $90^\circ$ rotations on images. We describe such equivariant or invariant subvarieties as direct products of determinantal varieties, from which we deduce their dimension, degree, Euclidean distance degree, and their singularities. We fully characterize invariance for arbitrary permutation groups, and equivariance for cyclic groups. We draw conclusions for the parameterization and the design of equivariant and invariant linear networks in terms of sparsity and weight-sharing properties. We prove that all invariant linear functions can be parameterized by a single linear autoencoder with a weight-sharing property imposed by the cycle decomposition of the considered permutation. The space of rank-bounded equivariant f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;AI&#26412;&#22320;&#21270;&#30340;&#26080;&#32447;&#32593;&#32476;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;&#8220;AI for wireless&#8221;&#33539;&#24335;&#30340;&#30701;&#26495;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;AI&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#12289;&#26354;&#32447;&#25311;&#21512;&#29305;&#24615;&#12289;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#20197;&#21450;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#25928;&#29575;&#20302;&#19979;&#31561;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#39537;&#21160;&#22411;&#12289;&#35757;&#32451;&#23494;&#38598;&#22411;AI&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13223</link><description>&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#65306;&#20026;&#19979;&#19968;&#20195;AI&#26412;&#22320;&#21270;&#26080;&#32447;&#32593;&#32476;&#24320;&#36767;&#38761;&#21629;&#24615;&#36947;&#36335;
&lt;/p&gt;
&lt;p&gt;
Causal Reasoning: Charting a Revolutionary Course for Next-Generation AI-Native Wireless Networks. (arXiv:2309.13223v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;AI&#26412;&#22320;&#21270;&#30340;&#26080;&#32447;&#32593;&#32476;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;&#8220;AI for wireless&#8221;&#33539;&#24335;&#30340;&#30701;&#26495;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;AI&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#12289;&#26354;&#32447;&#25311;&#21512;&#29305;&#24615;&#12289;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#20197;&#21450;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#25928;&#29575;&#20302;&#19979;&#31561;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#39537;&#21160;&#22411;&#12289;&#35757;&#32451;&#23494;&#38598;&#22411;AI&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#26412;&#21069;&#25552;&#26159;&#19979;&#19968;&#20195;&#26080;&#32447;&#32593;&#32476;&#65288;&#20363;&#22914;6G&#65289;&#23558;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26412;&#22320;&#21270;&#30340;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#20173;&#28982;&#35201;&#20040;&#26159;&#23450;&#24615;&#30340;&#65292;&#35201;&#20040;&#26159;&#23545;&#29616;&#26377;&#8220;AI&#29992;&#20110;&#26080;&#32447;&#8221;&#33539;&#24335;&#30340;&#22686;&#37327;&#25193;&#23637;&#12290;&#23454;&#38469;&#19978;&#65292;&#21019;&#24314;AI&#26412;&#22320;&#21270;&#30340;&#26080;&#32447;&#32593;&#32476;&#38754;&#20020;&#30528;&#37325;&#35201;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#22240;&#20026;&#25968;&#25454;&#39537;&#21160;&#22411;&#12289;&#35757;&#32451;&#23494;&#38598;&#22411;&#30340;AI&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#38480;&#21046;&#21253;&#25324;AI&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#12289;&#23427;&#20204;&#30340;&#26354;&#32447;&#25311;&#21512;&#29305;&#24615;&#65288;&#36825;&#21487;&#33021;&#38480;&#21046;&#23427;&#20204;&#30340;&#25512;&#29702;&#21644;&#36866;&#24212;&#33021;&#21147;&#65289;&#12289;&#23427;&#20204;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#20197;&#21450;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#25928;&#29575;&#20302;&#19979;&#31561;&#12290;&#20316;&#20026;&#23545;&#36825;&#20123;&#38480;&#21046;&#30340;&#22238;&#24212;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#20855;&#26377;&#21069;&#30651;&#24615;&#30340;&#24895;&#26223;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#30340;&#26032;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#22240;&#26524;&#21457;&#29616;&#12289;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the basic premise that next-generation wireless networks (e.g., 6G) will be artificial intelligence (AI)-native, to date, most existing efforts remain either qualitative or incremental extensions to existing ``AI for wireless'' paradigms. Indeed, creating AI-native wireless networks faces significant technical challenges due to the limitations of data-driven, training-intensive AI. These limitations include the black-box nature of the AI models, their curve-fitting nature, which can limit their ability to reason and adapt, their reliance on large amounts of training data, and the energy inefficiency of large neural networks. In response to these limitations, this article presents a comprehensive, forward-looking vision that addresses these shortcomings by introducing a novel framework for building AI-native wireless networks; grounded in the emerging field of causal reasoning. Causal reasoning, founded on causal discovery, causal representation learning, and causal inference, c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22240;&#26524;&#32467;&#26500;&#30340;&#20449;&#24687;&#35770;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#26576;&#20010;&#29305;&#23450;&#32467;&#26524;&#21464;&#37327;&#30340;&#22240;&#26524;&#37325;&#35201;&#24615;&#65292;&#35299;&#20915;&#20102;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07703</link><description>&lt;p&gt;
&#27979;&#37327;&#22240;&#26524;&#25511;&#21046;&#30340;&#22240;&#26524;&#29109;&#21644;&#20449;&#24687;&#22686;&#30410;
&lt;/p&gt;
&lt;p&gt;
Causal Entropy and Information Gain for Measuring Causal Control. (arXiv:2309.07703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22240;&#26524;&#32467;&#26500;&#30340;&#20449;&#24687;&#35770;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#26576;&#20010;&#29305;&#23450;&#32467;&#26524;&#21464;&#37327;&#30340;&#22240;&#26524;&#37325;&#35201;&#24615;&#65292;&#35299;&#20915;&#20102;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#26041;&#27861;&#36890;&#24120;&#32570;&#20047;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;&#23613;&#31649;&#35299;&#37322;&#24615;&#26426;&#22120;&#23398;&#20064;&#65288;IML&#65289;&#26041;&#27861;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#23558;&#37325;&#35201;&#24615;&#36171;&#20104;&#37027;&#20123;&#23545;&#32467;&#26524;&#21464;&#37327;&#27809;&#26377;&#22240;&#26524;&#24433;&#21709;&#30340;&#29305;&#24449;&#12290;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#25110;&#20043;&#21518;&#65292;&#36873;&#25321;&#22240;&#26524;&#30456;&#20851;&#30340;&#29305;&#24449;&#23558;&#25552;&#20379;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#21033;&#29992;&#20449;&#24687;&#35770;&#37327;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#32479;&#35745;&#30456;&#20851;&#29305;&#24449;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#25152;&#22522;&#20110;&#30340;&#20449;&#24687;&#35770;&#37327;&#19981;&#21253;&#21547;&#22240;&#26524;&#20851;&#31995;&#65292;&#22240;&#27492;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#33021;&#22815;&#32771;&#34385;&#31995;&#32479;&#22240;&#26524;&#32467;&#26500;&#30340;&#20449;&#24687;&#35770;&#37327;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#26576;&#20010;&#32473;&#23450;&#32467;&#26524;&#21464;&#37327;&#30340;&#22240;&#26524;&#37325;&#35201;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22240;&#26524;&#29109;&#21644;&#22240;&#26524;&#20114;&#20449;&#24687;&#30340;&#22240;&#26524;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence models and methods commonly lack causal interpretability. Despite the advancements in interpretable machine learning (IML) methods, they frequently assign importance to features which lack causal influence on the outcome variable. Selecting causally relevant features among those identified as relevant by these methods, or even before model training, would offer a solution. Feature selection methods utilizing information theoretical quantities have been successful in identifying statistically relevant features. However, the information theoretical quantities they are based on do not incorporate causality, rendering them unsuitable for such scenarios. To address this challenge, this article proposes information theoretical quantities that incorporate the causal structure of the system, which can be used to evaluate causal importance of features for some given outcome variable. Specifically, we introduce causal versions of entropy and mutual information, termed cau
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;&#28382;&#21518;&#20449;&#24687;&#29942;&#39048;&#30340;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#31995;&#32479;&#26144;&#23556;&#21040;&#31616;&#21270;&#34920;&#31034;&#31354;&#38388;&#24182;&#27169;&#25311;&#26102;&#38388;&#19978;&#30340;&#22823;&#36339;&#36291;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#21407;&#22987;&#36807;&#31243;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#21160;&#21147;&#23398;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#28382;&#21518;&#38477;&#32500;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.07200</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#28382;&#21518;&#20449;&#24687;&#29942;&#39048;&#30340;&#28508;&#22312;&#34920;&#31034;&#21644;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck. (arXiv:2309.07200v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;&#28382;&#21518;&#20449;&#24687;&#29942;&#39048;&#30340;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#31995;&#32479;&#26144;&#23556;&#21040;&#31616;&#21270;&#34920;&#31034;&#31354;&#38388;&#24182;&#27169;&#25311;&#26102;&#38388;&#19978;&#30340;&#22823;&#36339;&#36291;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#21407;&#22987;&#36807;&#31243;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#21160;&#21147;&#23398;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#28382;&#21518;&#38477;&#32500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#26159;&#25551;&#36848;&#21508;&#20010;&#39046;&#22495;&#20013;&#21160;&#24577;&#31995;&#32479;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#20934;&#30830;&#31215;&#20998;&#30340;&#30701;&#26102;&#38388;&#27493;&#38271;&#65292;&#31934;&#30830;&#27169;&#25311;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#22823;&#35268;&#27169;&#31995;&#32479;&#35745;&#31639;&#37327;&#24456;&#22823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#22797;&#26434;&#31995;&#32479;&#26144;&#23556;&#21040;&#31616;&#21270;&#34920;&#31034;&#31354;&#38388;&#24182;&#27169;&#25311;&#26102;&#38388;&#19978;&#30340;&#22823;&#36339;&#36291;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#21407;&#21017;&#30446;&#26631;-&#26102;&#38388;&#28382;&#21518;&#20449;&#24687;&#29942;&#39048;&#65288;T-IB&#65289;&#65292;&#23427;&#26088;&#22312;&#25429;&#25417;&#30456;&#20851;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#21516;&#26102;&#20002;&#24323;&#39640;&#39057;&#20449;&#24687;&#20197;&#31616;&#21270;&#27169;&#25311;&#20219;&#21153;&#24182;&#26368;&#23567;&#21270;&#25512;&#29702;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;T-IB&#23398;&#20064;&#20102;&#20449;&#24687;&#26368;&#20248;&#30340;&#34920;&#31034;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21407;&#22987;&#36807;&#31243;&#22312;&#36873;&#25321;&#30340;&#26102;&#38388;&#28382;&#21518;&#19979;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#21160;&#21147;&#23398;&#65292;&#24182;&#19988;&#20248;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#28382;&#21518;&#38477;&#32500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov processes are widely used mathematical models for describing dynamic systems in various fields. However, accurately simulating large-scale systems at long time scales is computationally expensive due to the short time steps required for accurate integration. In this paper, we introduce an inference process that maps complex systems into a simplified representational space and models large jumps in time. To achieve this, we propose Time-lagged Information Bottleneck (T-IB), a principled objective rooted in information theory, which aims to capture relevant temporal features while discarding high-frequency information to simplify the simulation task and minimize the inference error. Our experiments demonstrate that T-IB learns information-optimal representations for accurately modeling the statistical properties and dynamics of the original process at a selected time lag, outperforming existing time-lagged dimensionality reduction methods.
&lt;/p&gt;</description></item><item><title>&#21019;&#26032;&#28857;&#22312;&#20110;&#24341;&#20837;&#20102;&#20113;&#31471;&#28145;&#24230;&#23398;&#20064;&#21644;&#23884;&#20837;&#24335;&#26580;&#24615;&#20256;&#24863;&#22120;&#65292;&#23454;&#29616;&#20102;&#26234;&#33021;&#19978;&#32930;&#22806;&#39592;&#39612;&#31995;&#32479;&#26469;&#39044;&#27979;&#20154;&#31867;&#19978;&#32930;&#36816;&#21160;&#30340;&#24847;&#22270;&#24182;&#25552;&#20379;&#24863;&#35273;&#21453;&#39304;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;96.2%&#65292;&#33021;&#20197;&#20154;&#31867;&#24847;&#22270;&#20026;&#22522;&#30784;&#36827;&#34892;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.04655</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#20154;&#31867;&#24847;&#22270;&#30340;&#26234;&#33021;&#19978;&#32930;&#22806;&#39592;&#39612;&#31995;&#32479;&#20197;&#22686;&#24378;&#24863;&#35273;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Intelligent upper-limb exoskeleton using deep learning to predict human intention for sensory-feedback augmentation. (arXiv:2309.04655v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04655
&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#28857;&#22312;&#20110;&#24341;&#20837;&#20102;&#20113;&#31471;&#28145;&#24230;&#23398;&#20064;&#21644;&#23884;&#20837;&#24335;&#26580;&#24615;&#20256;&#24863;&#22120;&#65292;&#23454;&#29616;&#20102;&#26234;&#33021;&#19978;&#32930;&#22806;&#39592;&#39612;&#31995;&#32479;&#26469;&#39044;&#27979;&#20154;&#31867;&#19978;&#32930;&#36816;&#21160;&#30340;&#24847;&#22270;&#24182;&#25552;&#20379;&#24863;&#35273;&#21453;&#39304;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;96.2%&#65292;&#33021;&#20197;&#20154;&#31867;&#24847;&#22270;&#20026;&#22522;&#30784;&#36827;&#34892;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24180;&#40836;&#22686;&#38271;&#21644;&#20013;&#39118;&#31561;&#22240;&#32032;&#65292;&#20154;&#20307;&#32908;&#32905;&#39592;&#39612;&#21147;&#37327;&#19979;&#38477;&#65292;&#24433;&#21709;&#20102;&#20351;&#29992;&#19978;&#32930;&#36827;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#29616;&#26377;&#19968;&#20123;&#22806;&#39592;&#39612;&#35013;&#32622;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#20256;&#24863;&#22120;&#21453;&#39304;&#21644;&#23545;&#36816;&#21160;&#24847;&#22270;&#30340;&#39044;&#27979;&#65292;&#38656;&#35201;&#25163;&#21160;&#25805;&#20316;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26234;&#33021;&#19978;&#32930;&#22806;&#39592;&#39612;&#31995;&#32479;&#65292;&#21033;&#29992;&#20113;&#31471;&#28145;&#24230;&#23398;&#20064;&#26469;&#39044;&#27979;&#20154;&#31867;&#30340;&#24847;&#22270;&#20197;&#22686;&#24378;&#21147;&#37327;&#12290;&#23884;&#20837;&#24335;&#26580;&#24615;&#20256;&#24863;&#22120;&#36890;&#36807;&#25910;&#38598;&#23454;&#26102;&#32908;&#32905;&#20449;&#21495;&#25552;&#20379;&#24863;&#35273;&#21453;&#39304;&#65292;&#24182;&#21516;&#26102;&#35745;&#31639;&#20197;&#30830;&#23450;&#29992;&#25143;&#30340;&#24847;&#22270;&#36816;&#21160;&#12290;&#20113;&#31471;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#22235;&#31181;&#19978;&#32930;&#20851;&#33410;&#36816;&#21160;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;96.2%&#65292;&#21709;&#24212;&#36895;&#24230;&#20026;200-250&#27627;&#31186;&#65292;&#34920;&#26126;&#22806;&#39592;&#39612;&#31995;&#32479;&#23436;&#20840;&#20381;&#38752;&#20154;&#31867;&#24847;&#22270;&#36827;&#34892;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#19968;&#32452;&#26580;&#24615;&#27668;&#21160;&#35013;&#32622;&#36890;&#36807;&#25552;&#20379;897&#29275;&#39039;&#21147;&#21644;78.7&#27627;&#31859;&#30340;&#20301;&#31227;&#26469;&#36741;&#21161;&#24847;&#22270;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
The age and stroke-associated decline in musculoskeletal strength degrades the ability to perform daily human tasks using the upper extremities. Although there are a few examples of exoskeletons, they need manual operations due to the absence of sensor feedback and no intention prediction of movements. Here, we introduce an intelligent upper-limb exoskeleton system that uses cloud-based deep learning to predict human intention for strength augmentation. The embedded soft wearable sensors provide sensory feedback by collecting real-time muscle signals, which are simultaneously computed to determine the user's intended movement. The cloud-based deep-learning predicts four upper-limb joint motions with an average accuracy of 96.2% at a 200-250 millisecond response rate, suggesting that the exoskeleton operates just by human intention. In addition, an array of soft pneumatics assists the intended movements by providing 897 newton of force and 78.7 millimeter of displacement at maximum. Col
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2308.12044</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24615;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20013;&#38750;&#24120;&#29702;&#24819;&#30340;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#20102;&#25968;&#20540;&#25928;&#29575;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;(&#30001;&#20110;&#30456;&#20851;&#29305;&#24449;&#30340;&#25968;&#37327;&#36739;&#23569;)&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#22522;&#20110;&#32447;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#20247;&#25152;&#21608;&#30693;&#22312;$\ell^1$&#33539;&#25968;(&#21363;&#38646;&#26435;&#37325;)&#30340;&#26368;&#31232;&#30095;&#35299;&#21644;&#38750;&#27491;&#21017;&#21270;&#35299;&#20043;&#38388;&#23384;&#22312;&#19968;&#26465;&#36830;&#25509;&#36335;&#24452;&#65292;&#36825;&#26465;&#36335;&#24452;&#34987;&#31216;&#20026;&#27491;&#21017;&#21270;&#36335;&#24452;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;&#32463;&#39564;&#25439;&#22833;&#21644;&#31232;&#30095;&#24615;($\ell^1$&#33539;&#25968;)&#20316;&#20026;&#20004;&#20010;&#20914;&#31361;&#30340;&#26631;&#20934;&#65292;&#24182;&#35299;&#20915;&#30001;&#27492;&#20135;&#29983;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#23581;&#35797;&#23558;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;DNNs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;$\ell^1$&#33539;&#25968;&#30340;&#19981;&#20809;&#28369;&#24615;&#21644;&#21442;&#25968;&#25968;&#37327;&#30340;&#39640;&#24230;&#65292;&#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#26159;&#24456;&#26377;&#25928;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#25972;&#20010;&#24085;&#32047;&#25176;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#26102;&#38388;KG&#21644;&#36229;&#20851;&#31995;KG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.10219</link><description>&lt;p&gt;
&#22312;&#22686;&#24378;&#30340;&#19981;&#21464;&#20851;&#31995;&#30693;&#35782;&#19978;&#25506;&#32034;&#36229;&#20851;&#31995;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge. (arXiv:2307.10219v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10219
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#26102;&#38388;KG&#21644;&#36229;&#20851;&#31995;KG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;(HKGs)&#26159;&#20256;&#32479;&#30693;&#35782;&#22270;(KGs)&#30340;&#24310;&#20280;&#65292;&#20026;&#27599;&#20010;KG&#20107;&#23454;&#25552;&#20379;&#39069;&#22806;&#30340;&#38190;&#20540;&#23545;(&#21363;&#38480;&#23450;&#35789;)&#65292;&#20197;&#26356;&#22909;&#22320;&#38480;&#21046;&#20107;&#23454;&#30340;&#26377;&#25928;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#22312;HKGs&#19978;&#36827;&#34892;&#22270;&#25512;&#29702;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30001;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#19981;&#26029;&#28436;&#21464;&#65292;&#22823;&#37327;&#24179;&#34892;&#24037;&#20316;&#38598;&#20013;&#22312;&#23545;&#26102;&#38388;KGs(TKGs)&#36827;&#34892;&#25512;&#29702;&#65292;&#20854;&#20013;&#27599;&#20010;TKG&#20107;&#23454;&#21487;&#20197;&#34987;&#35270;&#20026;&#24102;&#26377;&#26102;&#38388;&#25139;(&#25110;&#26102;&#38388;&#27573;)&#30340;KG&#20107;&#23454;&#65292;&#25351;&#23450;&#20854;&#26102;&#38388;&#26377;&#25928;&#24615;&#12290;&#29616;&#26377;&#30340;HKG&#25512;&#29702;&#26041;&#27861;&#19981;&#32771;&#34385;&#26102;&#38388;&#20449;&#24687;&#65292;&#22240;&#20026;&#22312;&#20043;&#21069;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#26174;&#24335;&#22320;&#25351;&#23450;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#20197;&#21069;&#30340;TKG&#25512;&#29702;&#26041;&#27861;&#21482;&#37325;&#35270;&#26102;&#38388;&#25512;&#29702;&#65292;&#24182;&#27809;&#26377;&#21150;&#27861;&#20174;&#38480;&#23450;&#35789;&#20013;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22635;&#34917;TKG&#25512;&#29702;&#21644;HKG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG(HTKG)&#25968;&#25454;&#38598;&#65292;&#21363;Wiki-hy&#21644;...
&lt;/p&gt;
&lt;p&gt;
Stemming from traditional knowledge graphs (KGs), hyper-relational KGs (HKGs) provide additional key-value pairs (i.e., qualifiers) for each KG fact that help to better restrict the fact validity. In recent years, there has been an increasing interest in studying graph reasoning over HKGs. In the meantime, due to the ever-evolving nature of world knowledge, extensive parallel works have been focusing on reasoning over temporal KGs (TKGs), where each TKG fact can be viewed as a KG fact coupled with a timestamp (or time period) specifying its time validity. The existing HKG reasoning approaches do not consider temporal information because it is not explicitly specified in previous benchmark datasets. Besides, all the previous TKG reasoning methods only lay emphasis on temporal reasoning and have no way to learn from qualifiers. To this end, we aim to fill the gap between TKG reasoning and HKG reasoning. We develop two new benchmark hyper-relational TKG (HTKG) datasets, i.e., Wiki-hy and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#20171;&#32461;&#20102;&#20351;&#29992;IBM Analog Hardware Acceleration Kit (AIHWKit)&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35813;&#24037;&#20855;&#21253;&#27169;&#25311;&#20102;&#27169;&#25311;&#20869;&#23384;&#35745;&#31639;&#65288;AIMC&#65289;&#30340;&#25512;&#26029;&#21644;&#35757;&#32451;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20339;&#23454;&#36341;&#21644;&#20113;&#29615;&#22659;&#20013;&#20351;&#29992;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.09357</link><description>&lt;p&gt;
&#20351;&#29992;IBM&#27169;&#25311;&#20869;&#23384;&#30828;&#20214;&#21152;&#36895;&#22871;&#20214;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Using the IBM Analog In-Memory Hardware Acceleration Kit for Neural Network Training and Inference. (arXiv:2307.09357v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#20171;&#32461;&#20102;&#20351;&#29992;IBM Analog Hardware Acceleration Kit (AIHWKit)&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35813;&#24037;&#20855;&#21253;&#27169;&#25311;&#20102;&#27169;&#25311;&#20869;&#23384;&#35745;&#31639;&#65288;AIMC&#65289;&#30340;&#25512;&#26029;&#21644;&#35757;&#32451;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20339;&#23454;&#36341;&#21644;&#20113;&#29615;&#22659;&#20013;&#20351;&#29992;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#20869;&#23384;&#35745;&#31639;&#65288;AIMC&#65289;&#26159;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#26029;&#21644;&#35757;&#32451;&#30340;&#24310;&#36831;&#21644;&#33021;&#28304;&#28040;&#32791;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;AIMC&#33455;&#29255;&#20013;&#30340;&#22122;&#22768;&#21644;&#38750;&#32447;&#24615;&#22120;&#20214;&#29305;&#24615;&#20197;&#21450;&#38750;&#29702;&#24819;&#30340;&#22806;&#22260;&#30005;&#36335;&#35201;&#27714;&#35843;&#25972;DNN&#20197;&#22312;&#27492;&#31867;&#30828;&#20214;&#19978;&#23454;&#29616;&#19982;&#25968;&#23383;&#35745;&#31639;&#31561;&#25928;&#30340;&#31934;&#24230;&#12290;&#22312;&#36825;&#20010;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;IBM&#27169;&#25311;&#30828;&#20214;&#21152;&#36895;&#22871;&#20214;&#65288;AIHWKit&#65289;&#36827;&#34892;&#36825;&#26679;&#30340;&#35843;&#25972;&#21644;&#35780;&#20272;&#65292;&#35813;&#22871;&#20214;&#21487;&#22312;https://github.com/IBM/aihwkit&#19978;&#20813;&#36153;&#33719;&#24471;&#12290;AIHWKit&#26159;&#19968;&#20010;Python&#24211;&#65292;&#21487;&#20197;&#20351;&#29992;AIMC&#27169;&#25311;&#25512;&#26029;&#21644;&#35757;&#32451;DNN&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;AIHWKit&#35774;&#35745;&#12289;&#21151;&#33021;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20197;&#27491;&#30830;&#36827;&#34892;&#25512;&#26029;&#21644;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#27169;&#25311;AI&#20113;&#32452;&#21512;&#22120;&#30340;&#27010;&#36848;&#65292;&#35813;&#32452;&#21512;&#25552;&#20379;&#20102;&#22312;&#23436;&#20840;&#25176;&#31649;&#30340;&#20113;&#29615;&#22659;&#20013;&#20351;&#29992;AIHWKit&#27169;&#25311;&#24179;&#21488;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analog In-Memory Computing (AIMC) is a promising approach to reduce the latency and energy consumption of Deep Neural Network (DNN) inference and training. However, the noisy and non-linear device characteristics, and the non-ideal peripheral circuitry in AIMC chips, require adapting DNNs to be deployed on such hardware to achieve equivalent accuracy to digital computing. In this tutorial, we provide a deep dive into how such adaptations can be achieved and evaluated using the recently released IBM Analog Hardware Acceleration Kit (AIHWKit), freely available at https://github.com/IBM/aihwkit. The AIHWKit is a Python library that simulates inference and training of DNNs using AIMC. We present an in-depth description of the AIHWKit design, functionality, and best practices to properly perform inference and training. We also present an overview of the Analog AI Cloud Composer, that provides the benefits of using the AIHWKit simulation platform in a fully managed cloud setting. Finally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#39532;&#23572;&#21487;&#22827;&#38142;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#25311;&#22120;&#65292;&#36866;&#29992;&#20110;&#39044;&#27979;&#27861;&#22269;&#21644;&#26031;&#22570;&#30340;&#32435;&#32500;&#20122;&#30340;&#38271;&#26102;&#38388;&#28909;&#28010;&#12290;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#35813;&#27169;&#25311;&#22120;&#22312;&#27010;&#29575;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#32463;&#36807;&#36866;&#24403;&#35780;&#20272;&#21644;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.09060</link><description>&lt;p&gt;
&#29992;&#31867;&#27604;&#39532;&#23572;&#21487;&#22827;&#38142;&#21644;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#26497;&#31471;&#28909;&#28010;&#30340;&#37319;&#26679;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Extreme heatwave sampling and prediction with analog Markov chain and comparisons with deep learning. (arXiv:2307.09060v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#39532;&#23572;&#21487;&#22827;&#38142;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#25311;&#22120;&#65292;&#36866;&#29992;&#20110;&#39044;&#27979;&#27861;&#22269;&#21644;&#26031;&#22570;&#30340;&#32435;&#32500;&#20122;&#30340;&#38271;&#26102;&#38388;&#28909;&#28010;&#12290;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#35813;&#27169;&#25311;&#22120;&#22312;&#27010;&#29575;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#32463;&#36807;&#36866;&#24403;&#35780;&#20272;&#21644;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#25311;&#22120;&#65292;&#38543;&#26426;&#22825;&#27668;&#29983;&#25104;&#22120;&#65288;SWG&#65289;&#65292;&#36866;&#29992;&#20110;&#20272;&#35745;&#27861;&#22269;&#21644;&#26031;&#22570;&#30340;&#32435;&#32500;&#20122;&#38271;&#26102;&#38388;&#28909;&#28010;&#30340;&#27010;&#29575;&#12290;&#36825;&#20010;&#27169;&#25311;&#22120;&#22522;&#20110;&#29615;&#27969;&#30340;&#31867;&#27604;&#26041;&#27861;&#65292;&#25105;&#20204;&#21152;&#20837;&#28201;&#24230;&#21644;&#22303;&#22756;&#28287;&#24230;&#20316;&#20026;&#39044;&#27979;&#23383;&#27573;&#12290;&#25105;&#20204;&#23558;&#27169;&#25311;&#22120;&#35757;&#32451;&#22312;&#19968;&#20010;&#20013;&#31561;&#22797;&#26434;&#24230;&#27668;&#20505;&#27169;&#22411;&#30340;&#36816;&#34892;&#19978;&#65292;&#24182;&#23637;&#31034;&#23427;&#33021;&#22815;&#39044;&#27979;&#26679;&#26412;&#22806;&#28909;&#28010;&#30340;&#26465;&#20214;&#27010;&#29575;&#65288;&#39044;&#27979;&#65289;&#12290;&#25105;&#20204;&#29305;&#21035;&#27880;&#24847;&#65292;&#20351;&#29992;&#36866;&#29992;&#20110;&#32597;&#35265;&#20107;&#20214;&#30340;&#21512;&#36866;&#35780;&#20998;&#26469;&#35780;&#20272;&#36825;&#20010;&#39044;&#27979;&#12290;&#20026;&#20102;&#21152;&#36895;&#31867;&#27604;&#30340;&#35745;&#31639;&#65292;&#38477;&#32500;&#25216;&#26415;&#34987;&#24212;&#29992;&#65292;&#24182;&#19988;&#24615;&#33021;&#24471;&#21040;&#35780;&#20272;&#12290;&#36890;&#36807;SWG&#23454;&#29616;&#30340;&#27010;&#29575;&#39044;&#27979;&#19982;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23454;&#29616;&#30340;&#27010;&#29575;&#39044;&#27979;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#38543;&#30528;&#25968;&#30334;&#24180;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;CNN&#22312;&#27010;&#29575;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#32463;&#36807;8&#20010;&#35757;&#32451;&#23454;&#20363;&#35757;&#32451;&#30340;SWG&#27169;&#25311;&#22120;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a data-driven emulator, stochastic weather generator (SWG), suitable for estimating probabilities of prolonged heatwaves in France and Scandinavia. This emulator is based on the method of analogs of circulation to which we add temperature and soil moisture as predictor fields. We train the emulator on an intermediate complexity climate model run and show that it is capable of predicting conditional probabilities (forecasting) of heatwaves out of sample. Special attention is payed that this prediction is evaluated using proper score appropriate for rare events. To accelerate the computation of analogs dimensionality reduction techniques are applied and the performance is evaluated. The probabilistic prediction achieved with SWG is compared with the one achieved with  Convolutional Neural Network (CNN). With the availability of hundreds of years of training data CNNs perform better at the task of probabilistic prediction. In addition, we show that the SWG emulator trained on 8
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#20174;&#27979;&#37327;&#20013;&#23398;&#20064;IMM&#28388;&#27874;&#22120;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#27979;&#37327;&#25968;&#25454;&#21363;&#21487;&#20248;&#21270;&#28388;&#27874;&#22120;&#30340;&#21442;&#25968;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#20351;&#29992;&#30495;&#20540;&#21442;&#25968;&#21270;&#30340;&#28388;&#27874;&#22120;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.06618</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#20174;&#27979;&#37327;&#20013;&#23398;&#20064;IMM&#28388;&#27874;&#22120;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning IMM Filter Parameters from Measurements using Gradient Descent. (arXiv:2307.06618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#20174;&#27979;&#37327;&#20013;&#23398;&#20064;IMM&#28388;&#27874;&#22120;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#27979;&#37327;&#25968;&#25454;&#21363;&#21487;&#20248;&#21270;&#28388;&#27874;&#22120;&#30340;&#21442;&#25968;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#20351;&#29992;&#30495;&#20540;&#21442;&#25968;&#21270;&#30340;&#28388;&#27874;&#22120;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#34701;&#21512;&#21644;&#36319;&#36394;&#31639;&#27861;&#30340;&#24615;&#33021;&#36890;&#24120;&#20381;&#36182;&#20110;&#26082;&#25551;&#36848;&#20256;&#24863;&#22120;&#31995;&#32479;&#21448;&#21487;&#20197;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#21442;&#25968;&#12290;&#23613;&#31649;&#35843;&#25972;&#36825;&#20123;&#21464;&#37327;&#23545;&#20110;&#20256;&#24863;&#22120;&#31995;&#32479;&#26469;&#35828;&#26159;&#32791;&#26102;&#30340;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#19987;&#23478;&#30693;&#35782;&#65292;&#20294;&#22312;&#36319;&#36394;&#30446;&#26631;&#30340;&#20869;&#22312;&#21442;&#25968;&#22312;&#31995;&#32479;&#37096;&#32626;&#20043;&#21069;&#29978;&#33267;&#21487;&#33021;&#23436;&#20840;&#19981;&#21487;&#35266;&#27979;&#12290;&#38543;&#30528;&#26368;&#20808;&#36827;&#30340;&#20256;&#24863;&#22120;&#31995;&#32479;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#21442;&#25968;&#30340;&#25968;&#37327;&#33258;&#28982;&#22686;&#21152;&#65292;&#38656;&#35201;&#33258;&#21160;&#20248;&#21270;&#27169;&#22411;&#21464;&#37327;&#12290;&#26412;&#25991;&#36890;&#36807;&#20165;&#20351;&#29992;&#27979;&#37327;&#26469;&#20248;&#21270;&#20132;&#20114;&#22810;&#27169;&#22411;&#65288;IMM&#65289;&#28388;&#27874;&#22120;&#30340;&#21442;&#25968;&#65292;&#22240;&#27492;&#26080;&#38656;&#20219;&#20309;&#22522;&#30784;&#25968;&#25454;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#28040;&#34701;&#30740;&#31350;&#35780;&#20272;&#20102;&#32467;&#26524;&#26041;&#27861;&#65292;&#32467;&#26524;&#26041;&#27861;&#25104;&#21151;&#21305;&#37197;&#20102;&#20351;&#29992;&#22522;&#30784;&#30495;&#20540;&#21442;&#25968;&#21270;&#30340;&#28388;&#27874;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of data fusion and tracking algorithms often depends on parameters that not only describe the sensor system, but can also be task-specific. While for the sensor system tuning these variables is time-consuming and mostly requires expert knowledge, intrinsic parameters of targets under track can even be completely unobservable until the system is deployed. With state-of-the-art sensor systems growing more and more complex, the number of parameters naturally increases, necessitating the automatic optimization of the model variables. In this paper, the parameters of an interacting multiple model (IMM) filter are optimized solely using measurements, thus without necessity for any ground-truth data. The resulting method is evaluated through an ablation study on simulated data, where the trained model manages to match the performance of a filter parametrized with ground-truth values.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;ECG&#22270;&#20687;&#30340;&#24037;&#20855;&#31665;&#65292;&#26088;&#22312;&#20419;&#36827;&#25195;&#25551;ECG&#25968;&#23383;&#21270;&#12290;&#36890;&#36807;&#24341;&#20837;&#30495;&#23454;&#20266;&#24433;&#65292;&#22914;&#25163;&#20889;&#25991;&#26412;&#20266;&#24433;&#12289;&#30385;&#32441;&#12289;&#25240;&#30165;&#21644;&#35270;&#35282;&#21464;&#25442;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26631;&#20934;&#32440;&#36136;ECG&#32972;&#26223;&#19978;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#24615;&#30340;ECG&#22270;&#20687;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#21512;&#25104;ECG&#22270;&#20687;&#20013;&#32570;&#20047;&#21442;&#32771;&#26102;&#38388;&#24207;&#21015;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01946</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21512;&#25104;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#22270;&#20687;&#29983;&#25104;&#24037;&#20855;&#31665;&#65292;&#20197;&#20419;&#36827;&#25195;&#25551;ECG&#25968;&#23383;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization. (arXiv:2307.01946v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;ECG&#22270;&#20687;&#30340;&#24037;&#20855;&#31665;&#65292;&#26088;&#22312;&#20419;&#36827;&#25195;&#25551;ECG&#25968;&#23383;&#21270;&#12290;&#36890;&#36807;&#24341;&#20837;&#30495;&#23454;&#20266;&#24433;&#65292;&#22914;&#25163;&#20889;&#25991;&#26412;&#20266;&#24433;&#12289;&#30385;&#32441;&#12289;&#25240;&#30165;&#21644;&#35270;&#35282;&#21464;&#25442;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26631;&#20934;&#32440;&#36136;ECG&#32972;&#26223;&#19978;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#24615;&#30340;ECG&#22270;&#20687;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#21512;&#25104;ECG&#22270;&#20687;&#20013;&#32570;&#20047;&#21442;&#32771;&#26102;&#38388;&#24207;&#21015;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#19968;&#31181;&#20934;&#30830;&#19988;&#24191;&#27867;&#24212;&#29992;&#20110;&#35786;&#26029;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#24037;&#20855;&#12290;&#20960;&#21313;&#24180;&#26469;&#65292;ECG&#20197;&#21360;&#21047;&#26684;&#24335;&#35760;&#24405;&#65292;&#24182;&#19988;&#23558;&#23427;&#20204;&#30340;&#25968;&#23383;&#21270;&#22312;&#31639;&#27861;&#24615;&#24515;&#30005;&#22270;&#35786;&#26029;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#29289;&#29702;&#24615;ECG&#23384;&#26723;&#38754;&#20020;&#36864;&#21270;&#39118;&#38505;&#65292;&#20165;&#25195;&#25551;&#21360;&#21047;ECG&#26159;&#19981;&#22815;&#30340;&#65292;&#22240;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;ECG&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#23558;&#32440;&#36136;ECG&#23384;&#26723;&#25968;&#23383;&#21270;&#21644;&#36716;&#25442;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22270;&#20687;&#22788;&#29702;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#21442;&#32771;&#26102;&#38388;&#24207;&#21015;&#30340;ECG&#23384;&#26723;&#31232;&#32570;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#21033;&#29992;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#33021;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#20266;&#24433;&#30340;&#26631;&#20934;&#32440;&#36136;ECG&#32972;&#26223;&#19979;&#30340;&#21512;&#25104;ECG&#22270;&#20687;&#12290;&#21253;&#25324;&#25163;&#20889;&#25991;&#26412;&#20266;&#24433;&#12289;&#30385;&#32441;&#12289;&#25240;&#30165;&#21644;&#35270;&#35282;&#36716;&#25442;&#31561;&#30072;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electrocardiogram (ECG) is an accurate and widely available tool for diagnosing cardiovascular diseases. ECGs have been recorded in printed formats for decades and their digitization holds great potential for training machine learning (ML) models in algorithmic ECG diagnosis. Physical ECG archives are at risk of deterioration and scanning printed ECGs alone is insufficient, as ML models require ECG time-series data. Therefore, the digitization and conversion of paper ECG archives into time-series data is of utmost importance. Deep learning models for image processing show promise in this regard. However, the scarcity of ECG archives with reference time-series is a challenge. Data augmentation techniques utilizing \textit{digital twins} present a potential solution.  We introduce a novel method for generating synthetic ECG images on standard paper-like ECG backgrounds with realistic artifacts. Distortions including handwritten text artifacts, wrinkles, creases and perspective transf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#20613;&#37324;&#21494;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#20250;&#35805;&#20013;&#25214;&#21040;&#33258;&#36866;&#24212;&#19988;&#32039;&#20945;&#30340;&#20613;&#37324;&#21494;&#31354;&#38388;&#23376;&#27169;&#22359;&#26469;&#32534;&#30721;&#39034;&#24207;&#35270;&#39057;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#26041;&#27861;&#22312;&#22810;&#20010;&#22797;&#26434;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11305</link><description>&lt;p&gt;
&#28176;&#36827;&#20613;&#37324;&#21494;&#31070;&#32463;&#34920;&#31034;&#29992;&#20110;&#39034;&#24207;&#35270;&#39057;&#32534;&#35793;
&lt;/p&gt;
&lt;p&gt;
Progressive Fourier Neural Representation for Sequential Video Compilation. (arXiv:2306.11305v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#20613;&#37324;&#21494;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#20250;&#35805;&#20013;&#25214;&#21040;&#33258;&#36866;&#24212;&#19988;&#32039;&#20945;&#30340;&#20613;&#37324;&#21494;&#31354;&#38388;&#23376;&#27169;&#22359;&#26469;&#32534;&#30721;&#39034;&#24207;&#35270;&#39057;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#26041;&#27861;&#22312;&#22810;&#20010;&#22797;&#26434;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;(NIR)&#22240;&#20854;&#23558;&#22797;&#26434;&#21644;&#39640;&#32500;&#25968;&#25454;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#24182;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#26144;&#23556;&#20989;&#25968;&#36731;&#26494;&#37325;&#26500;&#25968;&#25454;&#30340;&#38750;&#20961;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;NIR&#26041;&#27861;&#20551;&#23450;&#30446;&#26631;&#25968;&#25454;&#21644;&#34920;&#31034;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19968;&#23545;&#19968;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#32771;&#34385;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#25110;&#30456;&#20284;&#24615;&#12290;&#36825;&#23548;&#33268;&#22312;&#22810;&#32452;&#22797;&#26434;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#24182;&#38480;&#21046;&#20102;&#20854;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#21463;&#25345;&#32493;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#39034;&#24207;&#32534;&#30721;&#20250;&#35805;&#20013;&#32047;&#31215;&#21644;&#20256;&#36882;&#22810;&#20010;&#22797;&#26434;&#35270;&#39057;&#25968;&#25454;&#30340;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#12290;&#20026;&#20102;&#20811;&#26381;NIR&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#28176;&#36827;&#20613;&#37324;&#21494;&#31070;&#32463;&#34920;&#31034;(PFNR)&#65292;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#33258;&#36866;&#24212;&#21644;&#32039;&#20945;&#30340;&#20613;&#37324;&#21494;&#31354;&#38388;&#23376;&#27169;&#22359;&#65292;&#20197;&#32534;&#30721;&#27599;&#20010;&#35757;&#32451;&#20250;&#35805;&#20013;&#30340;&#35270;&#39057;&#12290;&#36825;&#31181;&#31232;&#30095;&#30340;&#31070;&#32463;&#32534;&#30721;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25345;&#26377;&#33258;&#30001;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#21487;&#36845;&#20195;&#22320;&#32534;&#30721;&#21644;&#35299;&#30721;&#22810;&#20010;&#39034;&#24207;&#35270;&#39057;&#25968;&#25454;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit Representation (NIR) has recently gained significant attention due to its remarkable ability to encode complex and high-dimensional data into representation space and easily reconstruct it through a trainable mapping function. However, NIR methods assume a one-to-one mapping between the target data and representation models regardless of data relevancy or similarity. This results in poor generalization over multiple complex data and limits their efficiency and scalability. Motivated by continual learning, this work investigates how to accumulate and transfer neural implicit representations for multiple complex video data over sequential encoding sessions. To overcome the limitation of NIR, we propose a novel method, Progressive Fourier Neural Representation (PFNR), that aims to find an adaptive and compact sub-module in Fourier space to encode videos in each training session. This sparsified neural encoding allows the neural network to hold free weights, enabling an imp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24179;&#31227;&#23545;&#31216;&#24615;&#30340;&#20998;&#35010;&#24182;&#34892;&#21270;QCNN&#26550;&#26500;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#24179;&#31227;&#23545;&#31216;&#37327;&#23376;&#25968;&#25454;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;QCNN&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27979;&#37327;&#25928;&#29575;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.07331</link><description>&lt;p&gt;
&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#35010;&#21644;&#24182;&#34892;&#21270;&#29992;&#20110;&#23398;&#20064;&#24179;&#31227;&#23545;&#31216;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Splitting and Parallelizing of Quantum Convolutional Neural Networks for Learning Translationally Symmetric Data. (arXiv:2306.07331v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07331
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24179;&#31227;&#23545;&#31216;&#24615;&#30340;&#20998;&#35010;&#24182;&#34892;&#21270;QCNN&#26550;&#26500;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#24179;&#31227;&#23545;&#31216;&#37327;&#23376;&#25968;&#25454;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;QCNN&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27979;&#37327;&#25928;&#29575;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(QCNN)&#26159;&#19968;&#31181;&#26377;&#26395;&#22312;&#32463;&#20856;&#38590;&#39064;&#19978;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;(QML)&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;QCNN&#38656;&#35201;&#22823;&#37327;&#30340;&#27979;&#37327;&#29992;&#20110;&#25968;&#25454;&#23398;&#20064;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20998;&#35010;&#24182;&#34892;&#21270;QCNN(sp-QCNN)&#65292;&#23427;&#21033;&#29992;&#37327;&#23376;&#25968;&#25454;&#30340;&#20808;&#39564;&#30693;&#35782;&#35774;&#35745;&#39640;&#25928;&#30005;&#36335;&#12290;&#36825;&#31181;&#26550;&#26500;&#20174;&#20960;&#20309;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#38024;&#23545;&#20957;&#32858;&#24577;&#29289;&#29702;&#20013;&#24120;&#35265;&#30340;&#24179;&#31227;&#23545;&#31216;&#37327;&#23376;&#25968;&#25454;&#12290;&#36890;&#36807;&#22522;&#20110;&#24179;&#31227;&#23545;&#31216;&#24615;&#20998;&#35010;&#37327;&#23376;&#30005;&#36335;&#65292;sp-QCNN&#26497;&#22823;&#22320;&#24182;&#34892;&#21270;&#20102;&#20256;&#32479;&#30340;QCNN&#65292;&#32780;&#19981;&#22686;&#21152;&#37327;&#23376;&#27604;&#29305;&#25968;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27979;&#37327;&#25928;&#29575;&#65292;&#36798;&#21040;&#20102;&#37327;&#23376;&#30456;&#35782;&#21035;&#20219;&#21153;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A quantum convolutional neural network (QCNN) is a promising quantum machine learning (QML) model to achieve quantum advantages in classically intractable problems. However, QCNN requires a large number of measurements for data learning, limiting its practical applications for large-scale problems. To relieve this requirement, we propose a novel architecture called split-parallelizing QCNN (sp-QCNN), which exploits the prior knowledge of quantum data for designing efficient circuits. This architecture draws inspiration from geometric quantum machine learning and targets translationally symmetric quantum data commonly encountered in condensed matter physics. By splitting the quantum circuit based on translational symmetry, sp-QCNN substantially parallelizes conventional QCNN without increasing the number of qubits and further improves the measurement efficiency by an order of the number of qubits. To demonstrate its effectiveness, we apply sp-QCNN to a quantum phase recognition task and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22810;&#39046;&#22495;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;FedWon&#65292;&#36890;&#36807;&#28040;&#38500;&#35268;&#33539;&#21270;&#27493;&#39588;&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.05879</link><description>&lt;p&gt;
&#22810;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#26159;&#21542;&#31163;&#19981;&#24320;&#26631;&#20934;&#21270;?
&lt;/p&gt;
&lt;p&gt;
Is Normalization Indispensable for Multi-domain Federated Learning?. (arXiv:2306.05879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22810;&#39046;&#22495;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;FedWon&#65292;&#36890;&#36807;&#28040;&#38500;&#35268;&#33539;&#21270;&#27493;&#39588;&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#20998;&#25955;&#22312;&#23458;&#25143;&#31471;&#19978;&#30340;&#21327;&#20316;&#24335;&#20869;&#37096;&#35757;&#32451;&#22686;&#24378;&#20102;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65288;non-i.i.d&#65289;&#23548;&#33268;&#30340;&#28508;&#22312;&#24615;&#33021;&#19979;&#38477;&#21644;&#25910;&#25947;&#21463;&#38459;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;&#19968;&#20010;&#20851;&#38190;&#20294;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#8212;&#8212;&#22810;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23458;&#25143;&#31471;&#25968;&#25454;&#26469;&#28304;&#20110;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#30340;&#21508;&#31181;&#39046;&#22495;&#65292;&#32780;&#19981;&#26159;&#26631;&#31614;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22810;&#39046;&#22495;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#31216;&#20026;&#19981;&#20351;&#29992;&#35268;&#33539;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FedWon&#65289;&#12290;FedWon&#20174;&#19968;&#20010;&#35266;&#23519;&#20986;&#21457;&#65292;&#21363;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#22312;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#20010;&#39046;&#22495;&#30340;&#32479;&#35745;&#20449;&#24687;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#26367;&#20195;&#35268;&#33539;&#21270;&#25216;&#26415;&#20855;&#26377;&#33258;&#36523;&#30340;&#23616;&#38480;&#24615;&#12290;FedWon&#36890;&#36807;&#28040;&#38500;&#35268;&#33539;&#21270;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enhances data privacy with collaborative in-situ training on decentralized clients. Nevertheless, FL encounters challenges due to non-independent and identically distributed (non-i.i.d) data, leading to potential performance degradation and hindered convergence. While prior studies predominantly addressed the issue of skewed label distribution, our research addresses a crucial yet frequently overlooked problem known as multi-domain FL. In this scenario, clients' data originate from diverse domains with distinct feature distributions, as opposed to label distributions. To address the multi-domain problem in FL, we propose a novel method called Federated learning Without normalizations (FedWon). FedWon draws inspiration from the observation that batch normalization (BN) faces challenges in effectively modeling the statistics of multiple domains, while alternative normalization techniques possess their own limitations. In order to address these issues, FedWon elimi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;&#65288;OPER&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#26469;&#23545;&#39640;&#22238;&#25253;&#30340;&#36716;&#25442;&#36827;&#34892;&#20248;&#20808;&#22788;&#29702;&#65292;&#20174;&#32780;&#25913;&#21892;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#22312;&#27492;&#25913;&#36827;&#30340;&#31574;&#30053;&#32422;&#26463;&#19979;&#20248;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;OPER&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.05412</link><description>&lt;p&gt;
&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;
&lt;/p&gt;
&lt;p&gt;
Offline Prioritized Experience Replay. (arXiv:2306.05412v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;&#65288;OPER&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#26469;&#23545;&#39640;&#22238;&#25253;&#30340;&#36716;&#25442;&#36827;&#34892;&#20248;&#20808;&#22788;&#29702;&#65292;&#20174;&#32780;&#25913;&#21892;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#22312;&#27492;&#25913;&#36827;&#30340;&#31574;&#30053;&#32422;&#26463;&#19979;&#20248;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;OPER&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#23398;&#20064;&#31574;&#30053;&#21644;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32422;&#26463;&#36890;&#36807;&#22343;&#21248;&#37319;&#26679;&#31561;&#26041;&#24335;&#34987;&#24212;&#29992;&#21040;&#34920;&#29616;&#33391;&#22909;&#21644;&#34920;&#29616;&#24046;&#30340;&#34892;&#21160;&#19978;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#23398;&#20064;&#31574;&#30053;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;&#65288;OPER&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#29992;&#20110;&#23558;&#39640;&#22238;&#25253;&#30340;&#36716;&#25442;&#32622;&#20110;&#26356;&#39057;&#32321;&#30340;&#35775;&#38382;&#20013;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#33021;&#22815;&#24341;&#36215;&#34892;&#20026;&#31574;&#30053;&#30340;&#25913;&#21892;&#65292;&#24403;&#31574;&#30053;&#32422;&#26463;&#21040;&#36825;&#20010;&#25913;&#36827;&#30340;&#31574;&#30053;&#19978;&#26102;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24456;&#21487;&#33021;&#24471;&#21040;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#23454;&#29992;&#31574;&#30053;&#26469;&#33719;&#24471;&#22522;&#20110;&#25311;&#21512;&#20540;&#32593;&#32476;&#30340;&#20248;&#20808;&#26435;&#37325;&#65288;OPER-A&#65289;&#25110;&#32773;u
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is challenged by the distributional shift problem. To address this problem, existing works mainly focus on designing sophisticated policy constraints between the learned policy and the behavior policy. However, these constraints are applied equally to well-performing and inferior actions through uniform sampling, which might negatively affect the learned policy. To alleviate this issue, we propose Offline Prioritized Experience Replay (OPER), featuring a class of priority functions designed to prioritize highly-rewarding transitions, making them more frequently visited during training. Through theoretical analysis, we show that this class of priority functions induce an improved behavior policy, and when constrained to this improved policy, a policy-constrained offline RL algorithm is likely to yield a better solution. We develop two practical strategies to obtain priority weights by estimating advantages based on a fitted value network (OPER-A) or u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#24341;&#20837;&#32593;&#32476;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#26696;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#38469;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.02766</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#32593;&#32476;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Networked Communication for Decentralised Agents in Mean-Field Games. (arXiv:2306.02766v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#24341;&#20837;&#32593;&#32476;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#26696;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#38469;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#32593;&#32476;&#36890;&#20449;&#24341;&#20837;&#22343;&#22330;&#21338;&#24328;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;oracle&#30340;&#24773;&#20917;&#19979;&#65292;N&#20010;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#27839;&#30528;&#32463;&#36807;&#30340;&#32463;&#39564;&#31995;&#32479;&#30340;&#21333;&#19968;&#38750;&#21608;&#26399;&#28436;&#21270;&#36335;&#24452;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#21482;&#26377;&#19968;&#20123;&#20851;&#20110;&#32593;&#32476;&#32467;&#26500;&#30340;&#21512;&#29702;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#26679;&#26412;&#20445;&#35777;&#65292;&#22312;&#38598;&#20013;&#23398;&#20064;&#21644;&#29420;&#31435;&#23398;&#20064;&#24773;&#20917;&#20043;&#38388;&#26377;&#30028;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19977;&#20010;&#29702;&#35770;&#31639;&#27861;&#30340;&#26679;&#26412;&#20445;&#35777;&#23454;&#38469;&#19978;&#24182;&#19981;&#20250;&#23548;&#33268;&#23454;&#38469;&#25910;&#25947;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#65292;&#24403;&#29702;&#35770;&#21442;&#25968;&#26410;&#34987;&#35266;&#23519;&#21040;&#65288;&#23548;&#33268;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#19981;&#20934;&#30830;&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#36890;&#20449;&#26041;&#26696;&#26174;&#33879;&#21152;&#36895;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#19968;&#20010;&#19981;&#21487;&#21462;&#30340;&#38598;&#20013;&#24335;&#25511;&#21046;&#22120;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#29702;&#35770;&#31639;&#27861;&#36827;&#34892;&#20102;&#20960;&#31181;&#23454;&#38469;&#30340;&#25913;&#36827;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23637;&#31034;&#23427;&#20204;&#30340;&#31532;&#19968;&#20010;&#23454;&#35777;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce networked communication to the mean-field game framework, in particular to oracle-free settings where $N$ decentralised agents learn along a single, non-episodic evolution path of the empirical system. We prove that our architecture, with only a few reasonable assumptions about network structure, has sample guarantees bounded between those of the centralised- and independent-learning cases. We discuss how the sample guarantees of the three theoretical algorithms do not actually result in practical convergence. Accordingly, we show that in practical settings where the theoretical parameters are not observed (leading to poor estimation of the Q-function), our communication scheme significantly accelerates convergence over the independent case, without relying on the undesirable assumption of a centralised controller. We contribute several further practical enhancements to all three theoretical algorithms, allowing us to showcase their first empirical demonstrations. Our expe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#21333;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#20013;&#20801;&#35768;&#21333;&#20010;&#36755;&#20837;&#20855;&#26377;&#22810;&#20010;&#36755;&#20986;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#21487;&#29992;&#30340;&#36755;&#20986;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#12290;&#36890;&#36807;&#22312;speechocean762&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#35745;&#31639;&#20986;&#26356;&#25509;&#36817;&#22810;&#20010;&#20154;&#24037;&#35780;&#32423;&#22120;&#21442;&#32771;&#36755;&#20986;&#38598;&#30340;&#27979;&#35797;&#38598;&#36755;&#20986;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.02719</link><description>&lt;p&gt;
&#21333;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#21333;&#20010;&#36755;&#20837;&#22810;&#20010;&#36755;&#20986;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Multiple output samples per input in a single-output Gaussian process. (arXiv:2306.02719v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#21333;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#20013;&#20801;&#35768;&#21333;&#20010;&#36755;&#20837;&#20855;&#26377;&#22810;&#20010;&#36755;&#20986;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#21487;&#29992;&#30340;&#36755;&#20986;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#12290;&#36890;&#36807;&#22312;speechocean762&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#35745;&#31639;&#20986;&#26356;&#25509;&#36817;&#22810;&#20010;&#20154;&#24037;&#35780;&#32423;&#22120;&#21442;&#32771;&#36755;&#20986;&#38598;&#30340;&#27979;&#35797;&#38598;&#36755;&#20986;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#21482;&#32771;&#34385;&#35757;&#32451;&#38598;&#20013;&#27599;&#20010;&#36755;&#20837;&#30340;&#21333;&#20010;&#36755;&#20986;&#26679;&#26412;&#12290;&#38024;&#23545;&#20027;&#35266;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#21475;&#35821;&#35780;&#20272;&#65292;&#21487;&#20197;&#29992;&#22810;&#20010;&#20154;&#24037;&#35780;&#32423;&#22120;&#30340;&#36755;&#20986;&#26631;&#31614;&#23545;&#36755;&#20837;&#36827;&#34892;&#27880;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;GP&#25512;&#24191;&#20026;&#20801;&#35768;&#22312;&#35757;&#32451;&#38598;&#20013;&#26377;&#36825;&#20123;&#22810;&#20010;&#36755;&#20986;&#26679;&#26412;&#65292;&#24182;&#19988;&#21033;&#29992;&#21487;&#29992;&#30340;&#36755;&#20986;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#12290;&#36825;&#19982;&#22810;&#36755;&#20986;GP&#19981;&#21516;&#65292;&#22240;&#20026;&#36825;&#37324;&#25152;&#26377;&#30340;&#36755;&#20986;&#26679;&#26412;&#37117;&#26469;&#33258;&#21516;&#19968;&#20219;&#21153;&#12290;&#36755;&#20986;&#23494;&#24230;&#20989;&#25968;&#34987;&#24418;&#24335;&#21270;&#20026;&#35266;&#23519;&#21040;&#25152;&#26377;&#36755;&#20986;&#26679;&#26412;&#30340;&#32852;&#21512;&#20284;&#28982;&#24230;&#37327;&#65292;&#20026;&#20102;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#28508;&#22312;&#21464;&#37327;&#19981;&#20250;&#37325;&#22797;&#12290;&#27979;&#35797;&#38598;&#39044;&#27979;&#31867;&#20284;&#20110;&#26631;&#20934;GP&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#21807;&#19968;&#19981;&#21516;&#30340;&#26159;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#12290;&#36890;&#36807;&#22312;speechocean762&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20351;&#24471;GP&#33021;&#22815;&#35745;&#31639;&#20986;&#19982;&#22810;&#20010;&#20154;&#24037;&#35780;&#32423;&#22120;&#30340;&#21442;&#32771;&#36755;&#20986;&#38598;&#26356;&#30456;&#20284;&#30340;&#27979;&#35797;&#38598;&#36755;&#20986;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard Gaussian Process (GP) only considers a single output sample per input in the training set. Datasets for subjective tasks, such as spoken language assessment, may be annotated with output labels from multiple human raters per input. This paper proposes to generalise the GP to allow for these multiple output samples in the training set, and thus make use of available output uncertainty information. This differs from a multi-output GP, as all output samples are from the same task here. The output density function is formulated to be the joint likelihood of observing all output samples, and latent variables are not repeated to reduce computation cost. The test set predictions are inferred similarly to a standard GP, with a difference being in the optimised hyper-parameters. This is evaluated on speechocean762, showing that it allows the GP to compute a test set output distribution that is more similar to the collection of reference outputs from the multiple human raters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#22411;&#36229;&#22768;&#22270;&#20687;&#21069;&#21015;&#33146;&#20998;&#21106;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#27880;&#37322;&#24341;&#23548;&#30340;Transformer UNet&#27169;&#22411;&#21644;&#27880;&#37322;&#24341;&#23548;&#30340;&#20108;&#20998;&#31867;&#20132;&#21449;&#29109;&#25439;&#22833;&#35299;&#20915;&#20302;&#20998;&#36776;&#29575;&#21644;&#30028;&#38480;&#19981;&#28165;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20851;&#27880;&#38590;&#20197;&#20998;&#21106;&#30340;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.19956</link><description>&lt;p&gt;
MicroSegNet&#65306;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#22411;&#36229;&#22768;&#22270;&#20687;&#21069;&#21015;&#33146;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MicroSegNet: A Deep Learning Approach for Prostate Segmentation on Micro-Ultrasound Images. (arXiv:2305.19956v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#22411;&#36229;&#22768;&#22270;&#20687;&#21069;&#21015;&#33146;&#20998;&#21106;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#27880;&#37322;&#24341;&#23548;&#30340;Transformer UNet&#27169;&#22411;&#21644;&#27880;&#37322;&#24341;&#23548;&#30340;&#20108;&#20998;&#31867;&#20132;&#21449;&#29109;&#25439;&#22833;&#35299;&#20915;&#20302;&#20998;&#36776;&#29575;&#21644;&#30028;&#38480;&#19981;&#28165;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20851;&#27880;&#38590;&#20197;&#20998;&#21106;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#22411;&#36229;&#22768;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;29MHz&#36229;&#22768;&#25216;&#26415;&#65292;&#25552;&#20379;&#27604;&#20256;&#32479;&#36229;&#22768;&#39640;3-4&#20493;&#30340;&#20998;&#36776;&#29575;&#65292;&#22312;&#35786;&#26029;&#21069;&#21015;&#33146;&#30284;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#19982;MRI&#30456;&#24403;&#65292;&#20294;&#25104;&#26412;&#26356;&#20302;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20302;&#20998;&#36776;&#29575;&#21644;&#21069;&#21015;&#33146;&#12289;&#33152;&#33009;&#21644;&#23615;&#36947;&#20013;&#32447;&#20043;&#38388;&#30340;&#30028;&#38480;&#19981;&#28165;&#65292;&#22522;&#20110;&#24494;&#22411;&#36229;&#22768;&#30340;&#21069;&#21015;&#33146;&#20998;&#21106;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MicroSegNet&#65292;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#22810;&#23610;&#24230;&#27880;&#37322;&#24341;&#23548;&#30340;Transformer UNet&#27169;&#22411;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;MicroSegNet&#26356;&#21152;&#20851;&#27880;&#38590;&#20197;&#20998;&#21106;&#65288;&#38590;&#21306;&#22495;&#65289;&#30340;&#21306;&#22495;&#65292;&#36825;&#20123;&#21306;&#22495;&#20855;&#26377;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#27880;&#37322;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27880;&#37322;&#24341;&#23548;&#30340;&#20108;&#20998;&#31867;&#20132;&#21449;&#29109;&#65288;AG-BCE&#65289;&#25439;&#22833;&#65292;&#23427;&#22312;&#38590;&#21306;&#22495;&#20013;&#32473;&#39044;&#27979;&#35823;&#24046;&#20998;&#37197;&#26356;&#22823;&#30340;&#26435;&#37325;&#21644;&#36739;&#20302;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Micro-ultrasound (micro-US) is a novel 29-MHz ultrasound technique that provides 3-4 times higher resolution than traditional ultrasound, delivering comparable accuracy for diagnosing prostate cancer to MRI but at a lower cost. Accurate prostate segmentation is crucial for prostate volume measurement, cancer diagnosis, prostate biopsy, and treatment planning. However, prostate segmentation on microUS is challenging due to artifacts and indistinct borders between the prostate, bladder, and urethra in the midline. This paper presents MicroSegNet, a multi-scale annotation-guided transformer UNet model designed specifically to tackle these challenges. During the training process, MicroSegNet focuses more on regions that are hard to segment (hard regions), characterized by discrepancies between expert and non-expert annotations. We achieve this by proposing an annotation-guided binary cross entropy (AG-BCE) loss that assigns a larger weight to prediction errors in hard regions and a lower w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MT-SLVR &#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#19981;&#21464;&#24615;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17191</link><description>&lt;p&gt;
MT-SLVR: &#22810;&#20219;&#21153;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#21464;&#25442;&#34920;&#31034;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations. (arXiv:2305.17191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17191
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MT-SLVR &#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#19981;&#21464;&#24615;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#22240;&#20854;&#33021;&#20174;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21019;&#24314;&#39640;&#36136;&#37327;&#34920;&#31034;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#20123;&#24378;&#22823;&#30340;&#29305;&#24449;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#25552;&#20379;&#20102;&#25193;&#20805;&#19981;&#21464;&#24615;&#65292;&#36825;&#36890;&#24120;&#26159;&#19968;&#31181;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#20174;&#20808;&#39564;&#19978;&#19981;&#30693;&#36947;&#25152;&#38656;&#30340;&#19981;&#21464;&#24615;&#25968;&#37327;&#21644;&#31867;&#22411;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#33258;&#30417;&#30563;&#26694;&#26550;(MT-SLVR)&#65292;&#20197;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#21464;&#20307;&#21644;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#25552;&#20379;&#20102;&#24378;&#22823;&#21644;&#28789;&#27963;&#30340;&#29305;&#24449;&#65292;&#21487;&#20351;&#22810;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#21463;&#30410;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#21508;&#31181;&#38899;&#39057;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#22343;&#26377;&#25913;&#21892;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive self-supervised learning has gained attention for its ability to create high-quality representations from large unlabelled data sets. A key reason that these powerful features enable data-efficient learning of downstream tasks is that they provide augmentation invariance, which is often a useful inductive bias. However, the amount and type of invariances preferred is not known apriori, and varies across different downstream tasks. We therefore propose a multi-task self-supervised framework (MT-SLVR) that learns both variant and invariant features in a parameter-efficient manner. Our multi-task representation provides a strong and flexible feature that benefits diverse downstream tasks. We evaluate our approach on few-shot classification tasks drawn from a variety of audio domains and demonstrate improved classification performance on all of them
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21327;&#20316;&#19990;&#30028;&#27169;&#22411;&#65288;CoWorld&#65289;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#31163;&#32447;&#26465;&#20214;&#19979;&#35270;&#35273;RL&#30340;&#24615;&#33021;&#12290;&#20854;&#26680;&#24515;&#24819;&#27861;&#26159;&#20351;&#29992;&#26131;&#20110;&#20132;&#20114;&#30340;&#27169;&#25311;&#22120;&#26469;&#35757;&#32451;&#36741;&#21161;RL&#27169;&#22411;&#20316;&#20026;&#31163;&#32447;&#31574;&#30053;&#30340;&#22312;&#32447;&#8220;&#27979;&#35797;&#24202;&#8221;&#65292;&#24182;&#25191;&#34892;&#22495;&#21327;&#20316;&#34920;&#31034;&#23398;&#20064;&#21644;&#22495;&#21327;&#20316;&#34892;&#20026;&#23398;&#20064;&#65292;&#32531;&#35299;&#31163;&#32447;&#25968;&#25454;&#20998;&#24067;&#20043;&#22806;&#30340;&#20215;&#20540;&#20989;&#25968;&#36807;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15260</link><description>&lt;p&gt;
&#21327;&#20316;&#19990;&#30028;&#27169;&#22411;: &#19968;&#31181;&#22312;&#32447;&#31163;&#32447;&#36716;&#31227;RL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative World Models: An Online-Offline Transfer RL Approach. (arXiv:2305.15260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21327;&#20316;&#19990;&#30028;&#27169;&#22411;&#65288;CoWorld&#65289;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#31163;&#32447;&#26465;&#20214;&#19979;&#35270;&#35273;RL&#30340;&#24615;&#33021;&#12290;&#20854;&#26680;&#24515;&#24819;&#27861;&#26159;&#20351;&#29992;&#26131;&#20110;&#20132;&#20114;&#30340;&#27169;&#25311;&#22120;&#26469;&#35757;&#32451;&#36741;&#21161;RL&#27169;&#22411;&#20316;&#20026;&#31163;&#32447;&#31574;&#30053;&#30340;&#22312;&#32447;&#8220;&#27979;&#35797;&#24202;&#8221;&#65292;&#24182;&#25191;&#34892;&#22495;&#21327;&#20316;&#34920;&#31034;&#23398;&#20064;&#21644;&#22495;&#21327;&#20316;&#34892;&#20026;&#23398;&#20064;&#65292;&#32531;&#35299;&#31163;&#32447;&#25968;&#25454;&#20998;&#24067;&#20043;&#22806;&#30340;&#20215;&#20540;&#20989;&#25968;&#36807;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#30001;&#20110;&#34920;&#24449;&#23398;&#20064;&#20013;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#21644;&#20215;&#20540;&#20989;&#25968;&#20013;&#30340;&#36807;&#24230;&#20272;&#35745;&#38382;&#39064;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21327;&#20316;&#19990;&#30028;&#27169;&#22411;&#65288;CoWorld&#65289;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#31163;&#32447;&#26465;&#20214;&#19979;&#35270;&#35273;RL&#30340;&#24615;&#33021;&#12290;&#20854;&#26680;&#24515;&#24819;&#27861;&#26159;&#20351;&#29992;&#26131;&#20110;&#20132;&#20114;&#12289;&#29616;&#25104;&#30340;&#27169;&#25311;&#22120;&#26469;&#35757;&#32451;&#36741;&#21161;RL&#27169;&#22411;&#20316;&#20026;&#31163;&#32447;&#31574;&#30053;&#22312;&#30446;&#26631;&#22495;&#20013;&#23398;&#20064;&#30340;&#22312;&#32447;&#8220;&#27979;&#35797;&#24202;&#8221;&#65292;&#36825;&#20026;&#20215;&#20540;&#20989;&#25968;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#32422;&#26463;&#8212;&#8212;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#24819;&#22312;&#19981;&#22952;&#30861;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#30340;&#21160;&#20316;&#25506;&#32034;&#30340;&#24773;&#20917;&#19979;&#32531;&#35299;&#31163;&#32447;&#25968;&#25454;&#20998;&#24067;&#20043;&#22806;&#30340;&#20215;&#20540;&#20989;&#25968;&#36807;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CoWorld&#25191;&#34892;&#22495;&#21327;&#20316;&#34920;&#31034;&#23398;&#20064;&#20197;&#24357;&#21512;&#22312;&#32447;&#21644;&#31163;&#32447;&#38544;&#34255;&#29366;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#23427;&#25191;&#34892;&#22495;&#21327;&#20316;&#34892;&#20026;&#23398;&#20064;&#65292;&#20351;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#22806;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#22312;&#32447;&#34892;&#20026;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training visual reinforcement learning (RL) models in offline datasets is challenging due to overfitting issues in representation learning and overestimation problems in value function. In this paper, we propose a transfer learning method called Collaborative World Models (CoWorld) to improve the performance of visual RL under offline conditions. The core idea is to use an easy-to-interact, off-the-shelf simulator to train an auxiliary RL model as the online "test bed" for the offline policy learned in the target domain, which provides a flexible constraint for the value function -- Intuitively, we want to mitigate the overestimation problem of value functions outside the offline data distribution without impeding the exploration of actions with potential advantages. Specifically, CoWorld performs domain-collaborative representation learning to bridge the gap between online and offline hidden state distributions. Furthermore, it performs domain-collaborative behavior learning that enab
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#20302;&#31209;&#30697;&#38453;&#22635;&#34917;&#38382;&#39064;&#20026;&#25237;&#24433;&#30697;&#38453;&#30340;&#38750;&#20984;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#33021;&#22815;&#30830;&#23450;&#26368;&#20248;&#35299;&#30340;&#20998;&#31163;&#20998;&#25903;&#23450;&#30028;&#26041;&#26696;&#65292;&#24182;&#19988;&#36890;&#36807;&#26032;&#39062;&#21644;&#32039;&#23494;&#30340;&#20984;&#26494;&#24347;&#26041;&#27861;&#65292;&#20351;&#24471;&#26368;&#20248;&#24615;&#24046;&#36317;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20943;&#23569;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2305.12292</link><description>&lt;p&gt;
&#26368;&#20248;&#20302;&#31209;&#30697;&#38453;&#22635;&#34917;&#65306;&#21322;&#23450;&#26494;&#24347;&#21644;&#29305;&#24449;&#21521;&#37327;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Optimal Low-Rank Matrix Completion: Semidefinite Relaxations and Eigenvector Disjunctions. (arXiv:2305.12292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12292
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#20302;&#31209;&#30697;&#38453;&#22635;&#34917;&#38382;&#39064;&#20026;&#25237;&#24433;&#30697;&#38453;&#30340;&#38750;&#20984;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#33021;&#22815;&#30830;&#23450;&#26368;&#20248;&#35299;&#30340;&#20998;&#31163;&#20998;&#25903;&#23450;&#30028;&#26041;&#26696;&#65292;&#24182;&#19988;&#36890;&#36807;&#26032;&#39062;&#21644;&#32039;&#23494;&#30340;&#20984;&#26494;&#24347;&#26041;&#27861;&#65292;&#20351;&#24471;&#26368;&#20248;&#24615;&#24046;&#36317;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20943;&#23569;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#30697;&#38453;&#22635;&#34917;&#30340;&#30446;&#30340;&#26159;&#35745;&#31639;&#19968;&#20010;&#22797;&#26434;&#24230;&#26368;&#23567;&#30340;&#30697;&#38453;&#65292;&#20197;&#23613;&#21487;&#33021;&#20934;&#30830;&#22320;&#24674;&#22797;&#32473;&#23450;&#30340;&#19968;&#32452;&#35266;&#27979;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#26377;&#20247;&#22810;&#24212;&#29992;&#65292;&#22914;&#20135;&#21697;&#25512;&#33616;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#20302;&#31209;&#30697;&#38453;&#22635;&#34917;&#30340;&#26041;&#27861;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#34429;&#28982;&#39640;&#24230;&#21487;&#25193;&#23637;&#24182;&#19988;&#36890;&#24120;&#33021;&#22815;&#30830;&#23450;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#19981;&#20855;&#22791;&#20219;&#20309;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20302;&#31209;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#25237;&#24433;&#30697;&#38453;&#30340;&#38750;&#20984;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#19968;&#31181;&#20998;&#31163;&#20998;&#25903;&#23450;&#30028;&#26041;&#26696;&#26469;&#37325;&#26032;&#23457;&#35270;&#30697;&#38453;&#22635;&#34917;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26368;&#20248;&#24615;&#23548;&#21521;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#20026;&#19968;&#32452;&#31209;&#19968;&#30697;&#38453;&#30340;&#21644;&#65292;&#24182;&#36890;&#36807; Shor &#26494;&#24347;&#26469;&#28608;&#21169;&#27599;&#20010;&#31209;&#19968;&#30697;&#38453;&#20013;&#30340;&#27599;&#20010; 2*2 &#23567;&#30697;&#38453;&#30340;&#34892;&#21015;&#24335;&#20026;&#38646;&#65292;&#20174;&#32780;&#25512;&#23548;&#20986;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#24120;&#24456;&#32039;&#30340;&#20984;&#26494;&#24347;&#31867;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26032;&#20984;&#26494;&#24347;&#26041;&#27861;&#23558;&#26368;&#20248;&#24615;&#24046;&#36317;&#20943;&#23569;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank matrix completion consists of computing a matrix of minimal complexity that recovers a given set of observations as accurately as possible, and has numerous applications such as product recommendation. Unfortunately, existing methods for solving low-rank matrix completion are heuristics that, while highly scalable and often identifying high-quality solutions, do not possess any optimality guarantees. We reexamine matrix completion with an optimality-oriented eye, by reformulating low-rank problems as convex problems over the non-convex set of projection matrices and implementing a disjunctive branch-and-bound scheme that solves them to certifiable optimality. Further, we derive a novel and often tight class of convex relaxations by decomposing a low-rank matrix as a sum of rank-one matrices and incentivizing, via a Shor relaxation, that each two-by-two minor in each rank-one matrix has determinant zero. In numerical experiments, our new convex relaxations decrease the optimali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20266;&#26631;&#31614;&#30340;&#27867;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#65292;&#22312;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#24212;&#29992;&#25928;&#26524;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.01747</link><description>&lt;p&gt;
&#24102;&#26377;&#26377;&#38480;&#27880;&#37322;&#30340;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#20266;&#26631;&#31614;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Expectation Maximization Pseudo Labelling for Segmentation with Limited Annotations. (arXiv:2305.01747v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20266;&#26631;&#31614;&#30340;&#27867;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#65292;&#22312;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#24212;&#29992;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20266;&#26631;&#31614;&#21450;&#20854;&#25512;&#24191;&#65292;&#20266;&#26631;&#31614;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#21407;&#22987;&#25512;&#26029;&#20316;&#20026;&#33258;&#35757;&#32451;&#30340;&#20266;&#26631;&#31614;&#65292;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20266;&#26631;&#31614;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#37096;&#20998;&#35299;&#37322;&#20102;&#20854;&#23454;&#35777;&#25104;&#21151;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36125;&#21494;&#26031;&#21407;&#29702;&#19979;&#20266;&#26631;&#31614;&#30340;&#23436;&#20840;&#27867;&#21270;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21464;&#20998;&#26041;&#27861;&#26469;&#23398;&#20064;&#36924;&#36817;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#65292;&#36890;&#36807;&#23398;&#20064;&#36873;&#25321;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#38408;&#20540;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#20266;&#26631;&#31614;&#21644;&#20854;&#25512;&#24191;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study pseudo labelling and its generalisation for semi-supervised segmentation of medical images. Pseudo labelling has achieved great empirical successes in semi-supervised learning, by utilising raw inferences on unlabelled data as pseudo labels for self-training. In our paper, we build a connection between pseudo labelling and the Expectation Maximization algorithm which partially explains its empirical successes. We thereby realise that the original pseudo labelling is an empirical estimation of its underlying full formulation. Following this insight, we demonstrate the full generalisation of pseudo labels under Bayes' principle, called Bayesian Pseudo Labels. We then provide a variational approach to learn to approximate Bayesian Pseudo Labels, by learning a threshold to select good quality pseudo labels. In the rest of the paper, we demonstrate the applications of Pseudo Labelling and its generalisation Bayesian Psuedo Labelling in semi-supervised segmentation of medical images
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#21521;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;DynaVol&#65292;&#21487;&#20197;&#22312;&#22810;&#23454;&#20307;&#21160;&#24577;&#22330;&#26223;&#20013;&#23398;&#20064;&#26102;&#38388;&#21464;&#21270;&#30340;&#20307;&#31215;&#34920;&#31034;&#65292;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;3D&#26684;&#28857;&#21644;&#32852;&#21512;&#23398;&#20064;&#26684;&#28857;&#32423;&#23616;&#37096;&#21160;&#24577;&#12289;&#29289;&#20307;&#32423;&#20840;&#23616;&#21160;&#24577;&#21644;&#32452;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330;&#26469;&#22686;&#24378;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#20307;&#32032;&#21270;&#30340;&#26102;&#31354;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00393</link><description>&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#31070;&#32463;&#28210;&#26579;&#23545;&#21160;&#24577;&#22330;&#26223;&#36827;&#34892;&#29289;&#20307;&#20013;&#24515;&#20307;&#32032;&#21270;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Voxelization of Dynamic Scenes via Inverse Neural Rendering. (arXiv:2305.00393v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#21521;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;DynaVol&#65292;&#21487;&#20197;&#22312;&#22810;&#23454;&#20307;&#21160;&#24577;&#22330;&#26223;&#20013;&#23398;&#20064;&#26102;&#38388;&#21464;&#21270;&#30340;&#20307;&#31215;&#34920;&#31034;&#65292;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;3D&#26684;&#28857;&#21644;&#32852;&#21512;&#23398;&#20064;&#26684;&#28857;&#32423;&#23616;&#37096;&#21160;&#24577;&#12289;&#29289;&#20307;&#32423;&#20840;&#23616;&#21160;&#24577;&#21644;&#32452;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330;&#26469;&#22686;&#24378;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#20307;&#32032;&#21270;&#30340;&#26102;&#31354;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#30340;3D&#22330;&#26223;&#20013;&#29702;&#35299;&#19990;&#30028;&#30340;&#32452;&#25104;&#21160;&#24577;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#26410;&#33021;&#26377;&#25928;&#21033;&#29992;&#26102;&#38388;&#32447;&#32034;&#65292;&#35201;&#20040;&#24573;&#30053;&#20102;&#22330;&#26223;&#20998;&#35299;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DynaVol&#65292;&#19968;&#31181;&#36870;&#21521;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#20026;&#22810;&#23454;&#20307;&#65288;&#22914;&#29289;&#20307;&#65289;&#30340;&#21160;&#24577;&#22330;&#26223;&#23398;&#20064;&#26102;&#38388;&#21464;&#21270;&#30340;&#20307;&#31215;&#34920;&#31034;&#25552;&#20379;&#20102;&#19968;&#20010;&#23398;&#20064;&#26041;&#27861;&#12290;&#23427;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#23427;&#32500;&#25252;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;3D&#26684;&#28857;&#65292;&#21160;&#24577;&#32780;&#28789;&#27963;&#22320;&#23558;&#31354;&#38388;&#20301;&#32622;&#32465;&#23450;&#21040;&#19981;&#21516;&#30340;&#23454;&#20307;&#65292;&#20174;&#32780;&#22312;&#20195;&#34920;&#24615;&#27700;&#24179;&#19978;&#40723;&#21169;&#20449;&#24687;&#30340;&#20998;&#31163;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#32852;&#21512;&#23398;&#20064;&#26684;&#28857;&#32423;&#23616;&#37096;&#21160;&#24577;&#12289;&#29289;&#20307;&#32423;&#20840;&#23616;&#21160;&#24577;&#21644;&#32452;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#20307;&#32032;&#21270;&#30340;&#26102;&#31354;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;DynaVol&#35757;&#32451;&#26041;&#26696;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the compositional dynamics of the world in unsupervised 3D scenarios is challenging. Existing approaches either fail to make effective use of time cues or ignore the multi-view consistency of scene decomposition. In this paper, we propose DynaVol, an inverse neural rendering framework that provides a pilot study for learning time-varying volumetric representations for dynamic scenes with multiple entities (like objects). It has two main contributions. First, it maintains a time-dependent 3D grid, which dynamically and flexibly binds the spatial locations to different entities, thus encouraging the separation of information at a representational level. Second, our approach jointly learns grid-level local dynamics, object-level global dynamics, and the compositional neural radiance fields in an end-to-end architecture, thereby enhancing the spatiotemporal consistency of object-centric scene voxelization. We present a two-stage training scheme for DynaVol and validate its ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31163;&#21464;&#20998;&#23545;&#25239;&#35757;&#32451;&#30340;&#39118;&#38505;&#24863;&#30693;&#22411;&#32929;&#31080;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#25200;&#21160;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#39118;&#38505;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#36890;&#36807;&#21464;&#20998;&#25200;&#21160;&#29983;&#25104;&#22120;&#27169;&#25311;&#19981;&#21516;&#30340;&#39118;&#38505;&#22240;&#32032;&#24182;&#29983;&#25104;&#20195;&#34920;&#24615;&#30340;&#39118;&#38505;&#25351;&#26631;&#23545;&#25239;&#26679;&#26412;&#12290;&#22312;&#30495;&#23454;&#32929;&#31080;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#38477;&#20302;&#20102;&#25237;&#36164;&#39118;&#38505;&#21516;&#26102;&#20445;&#25345;&#39640;&#39044;&#26399;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2304.11043</link><description>&lt;p&gt;
&#25200;&#21160;&#26377;&#21161;&#20110;&#38477;&#20302;&#25237;&#36164;&#39118;&#38505;&#21527;&#65311; &#22522;&#20110;&#20998;&#31163;&#21464;&#20998;&#23545;&#25239;&#35757;&#32451;&#30340;&#39118;&#38505;&#24863;&#30693;&#22411;&#32929;&#31080;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Can Perturbations Help Reduce Investment Risks? Risk-Aware Stock Recommendation via Split Variational Adversarial Training. (arXiv:2304.11043v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31163;&#21464;&#20998;&#23545;&#25239;&#35757;&#32451;&#30340;&#39118;&#38505;&#24863;&#30693;&#22411;&#32929;&#31080;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#25200;&#21160;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#39118;&#38505;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#36890;&#36807;&#21464;&#20998;&#25200;&#21160;&#29983;&#25104;&#22120;&#27169;&#25311;&#19981;&#21516;&#30340;&#39118;&#38505;&#22240;&#32032;&#24182;&#29983;&#25104;&#20195;&#34920;&#24615;&#30340;&#39118;&#38505;&#25351;&#26631;&#23545;&#25239;&#26679;&#26412;&#12290;&#22312;&#30495;&#23454;&#32929;&#31080;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#38477;&#20302;&#20102;&#25237;&#36164;&#39118;&#38505;&#21516;&#26102;&#20445;&#25345;&#39640;&#39044;&#26399;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32929;&#31080;&#24066;&#22330;&#65292;&#25104;&#21151;&#30340;&#25237;&#36164;&#38656;&#35201;&#22312;&#21033;&#28070;&#21644;&#39118;&#38505;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#26368;&#36817;&#65292;&#22312;&#37327;&#21270;&#25237;&#36164;&#20013;&#24191;&#27867;&#30740;&#31350;&#20102;&#32929;&#31080;&#25512;&#33616;&#65292;&#20197;&#20026;&#25237;&#36164;&#32773;&#36873;&#25321;&#20855;&#26377;&#26356;&#39640;&#25910;&#30410;&#29575;&#30340;&#32929;&#31080;&#12290;&#23613;&#31649;&#22312;&#33719;&#21033;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#20173;&#28982;&#22312;&#39118;&#38505;&#25511;&#21046;&#26041;&#38754;&#36739;&#24369;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23454;&#38469;&#32929;&#31080;&#25237;&#36164;&#20013;&#38590;&#20197;&#25215;&#21463;&#30340;&#20111;&#25439;&#12290;&#20026;&#20102;&#26377;&#25928;&#38477;&#20302;&#39118;&#38505;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#24615;&#25200;&#21160;&#20013;&#33719;&#24471;&#21551;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#31163;&#21464;&#20998;&#23545;&#25239;&#35757;&#32451;&#65288;SVAT&#65289;&#26694;&#26550;&#30340;&#39118;&#38505;&#24863;&#30693;&#22411;&#32929;&#31080;&#25512;&#33616;&#26041;&#27861;&#12290;&#26412;&#36136;&#19978;&#65292;SVAT&#40723;&#21169;&#27169;&#22411;&#23545;&#39118;&#38505;&#32929;&#31080;&#26679;&#26412;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#25935;&#24863;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#25200;&#21160;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#39118;&#38505;&#24847;&#35782;&#12290;&#20026;&#20102;&#29983;&#25104;&#20195;&#34920;&#24615;&#30340;&#39118;&#38505;&#25351;&#26631;&#23545;&#25239;&#26679;&#26412;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21464;&#20998;&#25200;&#21160;&#29983;&#25104;&#22120;&#26469;&#27169;&#25311;&#19981;&#21516;&#30340;&#39118;&#38505;&#22240;&#32032;&#12290;&#29305;&#21035;&#22320;&#65292;&#21464;&#20998;&#32467;&#26500;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#38590;&#20197;&#26126;&#30830;&#37327;&#21270;&#21644;&#24314;&#27169;&#30340;&#21508;&#31181;&#39118;&#38505;&#22240;&#32032;&#12290;&#22312;&#30495;&#23454;&#32929;&#31080;&#25968;&#25454;&#19978;&#30340;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;SVAT&#22312;&#38477;&#20302;&#25237;&#36164;&#39118;&#38505;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#39044;&#26399;&#25910;&#30410;&#19978;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the stock market, a successful investment requires a good balance between profits and risks. Recently, stock recommendation has been widely studied in quantitative investment to select stocks with higher return ratios for investors. Despite the success in making profits, most existing recommendation approaches are still weak in risk control, which may lead to intolerable paper losses in practical stock investing. To effectively reduce risks, we draw inspiration from adversarial perturbations and propose a novel Split Variational Adversarial Training (SVAT) framework for risk-aware stock recommendation. Essentially, SVAT encourages the model to be sensitive to adversarial perturbations of risky stock examples and enhances the model's risk awareness by learning from perturbations. To generate representative adversarial examples as risk indicators, we devise a variational perturbation generator to model diverse risk factors. Particularly, the variational architecture enables our method
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#12289;&#32431;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#22312;$\{0,1\}^d$&#19978;&#20934;&#30830;&#20272;&#35745;&#20108;&#20803;&#31215;&#20998;&#24067;&#30340;&#22343;&#20540;&#65292;&#36798;&#21040;&#20102;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.06787</link><description>&lt;p&gt;
&#20108;&#20803;&#31215;&#20998;&#24067;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#32431;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions. (arXiv:2304.06787v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#12289;&#32431;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#22312;$\{0,1\}^d$&#19978;&#20934;&#30830;&#20272;&#35745;&#20108;&#20803;&#31215;&#20998;&#24067;&#30340;&#22343;&#20540;&#65292;&#36798;&#21040;&#20102;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#949;-&#24046;&#20998;&#38544;&#31169;&#12289;&#35745;&#31639;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#24635;&#21464;&#21270;&#36317;&#31163;&#19979;&#20934;&#30830;&#22320;&#20272;&#35745;$\{0,1\}^d$&#19978;&#30340;&#20056;&#31215;&#20998;&#24067;&#30340;&#22343;&#20540;&#65292;&#21516;&#26102;&#22312;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#20869;&#33719;&#24471;&#20102;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#22312;&#26356;&#24369;&#30340;&#38544;&#31169;&#27010;&#24565;&#19979;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35201;&#20040;&#22312;&#25351;&#25968;&#32423;&#36816;&#34892;&#26102;&#38388;&#20869;&#26368;&#20248;&#22320;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the first $\varepsilon$-differentially private, computationally efficient algorithm that estimates the means of product distributions over $\{0,1\}^d$ accurately in total-variation distance, whilst attaining the optimal sample complexity to within polylogarithmic factors. The prior work had either solved this problem efficiently and optimally under weaker notions of privacy, or had solved it optimally while having exponential running times.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#32500;&#21367;&#31215;&#23618;&#30340;&#32447;&#24615;&#32593;&#32476;&#30340;&#20989;&#25968;&#31354;&#38388;&#65292;&#20998;&#26512;&#20102;&#32593;&#32476;&#26550;&#26500;&#23545;&#20989;&#25968;&#31354;&#38388;&#30340;&#24433;&#21709;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#27493;&#24133;&#22823;&#20110;&#19968;&#19988;&#25968;&#25454;&#19968;&#33324;&#30340;&#26550;&#26500;&#65292;&#35813;&#20248;&#21270;&#38382;&#39064;&#30340;&#38750;&#38646;&#20020;&#30028;&#28857;&#26159;&#20989;&#25968;&#31354;&#38388;&#30340;&#24179;&#28369;&#20869;&#37096;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.05752</link><description>&lt;p&gt;
&#32447;&#24615;&#21367;&#31215;&#32593;&#32476;&#30340;&#20989;&#25968;&#31354;&#38388;&#21644;&#20020;&#30028;&#28857;
&lt;/p&gt;
&lt;p&gt;
Function Space and Critical Points of Linear Convolutional Networks. (arXiv:2304.05752v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05752
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#32500;&#21367;&#31215;&#23618;&#30340;&#32447;&#24615;&#32593;&#32476;&#30340;&#20989;&#25968;&#31354;&#38388;&#65292;&#20998;&#26512;&#20102;&#32593;&#32476;&#26550;&#26500;&#23545;&#20989;&#25968;&#31354;&#38388;&#30340;&#24433;&#21709;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#27493;&#24133;&#22823;&#20110;&#19968;&#19988;&#25968;&#25454;&#19968;&#33324;&#30340;&#26550;&#26500;&#65292;&#35813;&#20248;&#21270;&#38382;&#39064;&#30340;&#38750;&#38646;&#20020;&#30028;&#28857;&#26159;&#20989;&#25968;&#31354;&#38388;&#30340;&#24179;&#28369;&#20869;&#37096;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#32500;&#21367;&#31215;&#23618;&#30340;&#32447;&#24615;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#36825;&#20123;&#32593;&#32476;&#30340;&#20989;&#25968;&#31354;&#38388;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#31232;&#30095;&#22240;&#23376;&#20998;&#35299;&#30340;&#21322;&#20195;&#25968;&#22810;&#39033;&#24335;&#26063;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#32593;&#32476;&#26550;&#26500;&#23545;&#20989;&#25968;&#31354;&#38388;&#30340;&#32500;&#24230;&#12289;&#36793;&#30028;&#21644;&#22855;&#24322;&#28857;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#32593;&#32476;&#21442;&#25968;&#21270;&#26144;&#23556;&#30340;&#20020;&#30028;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#24179;&#26041;&#35823;&#24046;&#25439;&#22833;&#35757;&#32451;&#32593;&#32476;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#25152;&#26377;&#27493;&#24133;&#22823;&#20110;&#19968;&#19988;&#25968;&#25454;&#19968;&#33324;&#30340;&#26550;&#26500;&#65292;&#35813;&#20248;&#21270;&#38382;&#39064;&#30340;&#38750;&#38646;&#20020;&#30028;&#28857;&#26159;&#20989;&#25968;&#31354;&#38388;&#30340;&#24179;&#28369;&#20869;&#37096;&#28857;&#12290;&#23545;&#20110;&#31264;&#23494;&#30340;&#32447;&#24615;&#32593;&#32476;&#21644;&#27493;&#24133;&#20026;&#19968;&#30340;&#32447;&#24615;&#21367;&#31215;&#32593;&#32476;&#65292;&#36825;&#31181;&#29305;&#24615;&#34987;&#35748;&#20026;&#26159;&#38169;&#35823;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the geometry of linear networks with one-dimensional convolutional layers. The function spaces of these networks can be identified with semi-algebraic families of polynomials admitting sparse factorizations. We analyze the impact of the network's architecture on the function space's dimension, boundary, and singular points. We also describe the critical points of the network's parameterization map. Furthermore, we study the optimization problem of training a network with the squared error loss. We prove that for architectures where all strides are larger than one and generic data, the non-zero critical points of that optimization problem are smooth interior points of the function space. This property is known to be false for dense linear networks and linear convolutional networks with stride one.
&lt;/p&gt;</description></item><item><title>FDRL&#26159;&#19968;&#31181;&#22522;&#20110;&#27969;&#24341;&#23548;&#30340;&#23494;&#24230;&#27604;&#23398;&#20064;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23494;&#24230;&#27604;&#20272;&#35745;&#22120;&#20174;&#36880;&#28176;&#25913;&#36827;&#30340;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#32531;&#35299;&#20102;&#23494;&#24230;&#40511;&#27807;&#38382;&#39064;&#65292;&#24182;&#22312;&#29983;&#25104;&#39640;&#23610;&#23544;&#22270;&#20687;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.03714</link><description>&lt;p&gt;
&#20351;&#29992;&#27969;&#24341;&#23548;&#30340;&#23494;&#24230;&#27604;&#23398;&#20064;&#36827;&#34892;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling with Flow-Guided Density Ratio Learning. (arXiv:2303.03714v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03714
&lt;/p&gt;
&lt;p&gt;
FDRL&#26159;&#19968;&#31181;&#22522;&#20110;&#27969;&#24341;&#23548;&#30340;&#23494;&#24230;&#27604;&#23398;&#20064;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23494;&#24230;&#27604;&#20272;&#35745;&#22120;&#20174;&#36880;&#28176;&#25913;&#36827;&#30340;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#32531;&#35299;&#20102;&#23494;&#24230;&#40511;&#27807;&#38382;&#39064;&#65292;&#24182;&#22312;&#29983;&#25104;&#39640;&#23610;&#23544;&#22270;&#20687;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#31216;&#20026;&#27969;&#24341;&#23548;&#30340;&#23494;&#24230;&#27604;&#23398;&#20064;&#65288;FDRL&#65289;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;DGflow&#20013;&#24341;&#20837;&#30340;&#22522;&#20110;&#29109;&#27491;&#21017;&#21270;f-&#25955;&#24230;&#30340;&#26799;&#24230;&#27969;&#30340;&#36807;&#26102;&#65288;&#26102;&#38388;&#26080;&#20851;&#65289;&#36817;&#20284;&#65292;&#24182;&#19988;&#36890;&#36807;GAN&#37492;&#21035;&#22120;&#32473;&#20986;&#30340;&#36807;&#26102;&#20272;&#35745;&#22120;&#36817;&#20284;&#20102;&#19981;&#21487;&#35745;&#31639;&#30340;&#26102;&#38388;&#30456;&#20851;&#23494;&#24230;&#27604;&#12290;&#22312;&#26679;&#26412;&#32454;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#36817;&#20284;&#36275;&#22815;&#65292;&#22240;&#20026;&#27969;&#30340;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#26159;&#30456;&#36817;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#20551;&#35774;&#26159;&#26080;&#25928;&#30340;&#65292;&#32780;&#19988;&#36807;&#26102;&#20272;&#35745;&#22120;&#30340;&#26420;&#32032;&#24212;&#29992;&#30001;&#20110;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;&#22823;&#40511;&#27807;&#32780;&#22833;&#36133;&#12290;FDRL&#25552;&#20986;&#20102;&#35757;&#32451;&#23494;&#24230;&#27604;&#20272;&#35745;&#22120;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#36880;&#28176;&#25913;&#36827;&#30340;&#26679;&#26412;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#32531;&#35299;&#20102;&#23494;&#24230;&#40511;&#27807;&#38382;&#39064;&#65292;&#20351;&#24471;FDRL&#33021;&#22815;&#29983;&#25104;&#39640;&#36798;$128\times128$&#23610;&#23544;&#30340;&#22270;&#20687;&#65292;&#24182;&#19988;&#22312;&#36136;&#37327;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26799;&#24230;&#27969;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Flow-Guided Density Ratio Learning (FDRL), a simple and scalable approach to generative modeling which builds on the stale (time-independent) approximation of the gradient flow of entropy-regularized f-divergences introduced in DGflow. In DGflow, the intractable time-dependent density ratio is approximated by a stale estimator given by a GAN discriminator. This is sufficient in the case of sample refinement, where the source and target distributions of the flow are close to each other. However, this assumption is invalid for generation and a naive application of the stale estimator fails due to the large chasm between the two distributions. FDRL proposes to train a density ratio estimator such that it learns from progressively improving samples during the training process. We show that this simple method alleviates the density chasm problem, allowing FDRL to generate images of dimensions as high as $128\times128$, as well as outperform existing gradient flow baselines on qua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#35299;&#32806;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#36890;&#36807;&#37492;&#21035;&#24615;&#27491;&#21017;&#21270;&#26469;&#23454;&#29616;&#34920;&#24449;&#35299;&#32806;&#65292;&#35299;&#20915;&#20102;&#29616;&#20195;&#28145;&#24230;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#32416;&#32544;&#20559;&#24046;&#34892;&#20026;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.00128</link><description>&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;&#24615;&#27491;&#21017;&#21270;&#26469;&#23454;&#29616;&#34920;&#24449;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Representation Disentaglement via Regularization by Identification. (arXiv:2303.00128v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#35299;&#32806;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#36890;&#36807;&#37492;&#21035;&#24615;&#27491;&#21017;&#21270;&#26469;&#23454;&#29616;&#34920;&#24449;&#35299;&#32806;&#65292;&#35299;&#20915;&#20102;&#29616;&#20195;&#28145;&#24230;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#32416;&#32544;&#20559;&#24046;&#34892;&#20026;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#35299;&#32806;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;&#32473;&#23450;&#20174;$p(\mathbf{x}|\mathbf{y})$&#20013;&#29983;&#25104;&#30340;&#20855;&#26377;&#21508;&#33258;&#29983;&#25104;&#21464;&#37327;$\mathbf{y}_c$&#20998;&#35299;&#30340;&#20998;&#24067;$p(\mathbf{y}) = \prod_{c} p(\mathbf{y}_c )$&#30340;&#35266;&#27979;&#20540;${\mathbf{x}^{(i)}}$&#65292;&#25105;&#20204;&#23581;&#35797;&#23398;&#20064;&#19982;&#27599;&#20010;$c$&#30340;&#21518;&#39564;&#20998;&#24067;$p(\mathbf{z}| \mathbf{x}, \hat{\mathbf{y}}_c)$&#21305;&#37197;&#30340;&#35299;&#32806;&#34920;&#31034;&#26159;&#21542;&#21487;&#34892;&#12290;&#25105;&#20204;&#35748;&#20026;&#29616;&#20195;&#28145;&#24230;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#26080;&#27861;&#35299;&#20915;&#19982;&#29983;&#25104;&#21464;&#37327;&#20043;&#38388;&#20986;&#29616;&#30340;&#32416;&#32544;&#20559;&#24046;&#34892;&#20026;&#38382;&#39064;&#65292;&#36825;&#31181;&#34892;&#20026;&#19978;&#20135;&#29983;&#20559;&#35265;&#12290;&#22312;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#21487;&#35782;&#21035;&#24615;&#30340;&#26465;&#20214;&#19979;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#35299;&#37322;&#21644;&#35843;&#21644;&#65292;&#36825;&#19968;&#28857;&#21487;&#20197;&#22312;&#30417;&#30563;&#25110;&#24369;&#30417;&#30563;&#30340;&#26465;&#20214;&#19979;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37492;&#21035;&#24615;&#27491;&#21017;&#21270;&#65288;ReI&#65289;&#30340;&#27169;&#22359;&#21270;&#37325;&#26032;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work focuses on the problem of learning disentangled representations from observational data. Given observations ${\mathbf{x}^{(i)}}$ for $i=1,...,N $ drawn from $p(\mathbf{x}|\mathbf{y})$ with generative variables $\mathbf{y}$ admitting the distribution factorization $p(\mathbf{y}) = \prod_{c} p(\mathbf{y}_c )$, we ask whether learning disentangled representations matching the space of observations with identification guarantees on the posterior $p(\mathbf{z}| \mathbf{x}, \hat{\mathbf{y}}_c)$ for each $c$, is plausible. We argue modern deep representation learning models of data matching the distributed factorization property are ill-posed with collider bias behaviour; a source of bias producing entanglement between generating variables. Under the rubric of causality, we show this issue can be explained and reconciled under the condition of identifiability; attainable under supervision or a weak-form of it. For this, we propose regularization by identification (ReI), a modular re
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#35299;&#37322;&#20102;&#20960;&#31181;&#26368;&#26032;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21450;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#21452;&#37325;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;DIL&#65289;&#30452;&#25509;&#26368;&#23567;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08560</link><description>&lt;p&gt;
&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#65306;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#32479;&#19968;&#21644;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dual RL: Unification and New Methods for Reinforcement and Imitation Learning. (arXiv:2302.08560v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08560
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#35299;&#37322;&#20102;&#20960;&#31181;&#26368;&#26032;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21450;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#21452;&#37325;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;DIL&#65289;&#30452;&#25509;&#26368;&#23567;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#26399;&#26395;&#32047;&#31215;&#22238;&#25253;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20010;&#30446;&#26631;&#21487;&#20197;&#36890;&#36807;&#22312;&#32447;&#24615;&#32422;&#26463;&#19979;&#20248;&#21270;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#20998;&#24067;&#30340;&#20248;&#21270;&#38382;&#39064;&#26469;&#34920;&#31034;&#12290;&#36825;&#20010;&#34920;&#36848;&#30340;&#23545;&#20598;&#38382;&#39064;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#65292;&#26159;&#26080;&#32422;&#26463;&#30340;&#24182;&#19988;&#26356;&#23481;&#26131;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20197;&#21450;&#27169;&#20223;&#23398;&#20064;&#21487;&#20197;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#34987;&#35270;&#20026;&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#31181;&#32479;&#19968;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#22522;&#30784;&#65292;&#21487;&#20197;&#30740;&#31350;&#21644;&#35782;&#21035;&#36825;&#20123;&#26041;&#27861;&#25104;&#21151;&#30340;&#26500;&#25104;&#37096;&#20998;&#65292;&#24182;&#25581;&#31034;&#36825;&#20123;&#26041;&#27861;&#30340;&#20849;&#21516;&#32570;&#28857;&#21644;&#25913;&#36827;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20197;&#21069;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#19981;&#29616;&#23454;&#30340;&#35206;&#30422;&#29575;&#20551;&#35774;&#65292;&#24182;&#26368;&#23567;&#21270;&#20102;&#23398;&#20064;&#20195;&#29702;&#21644;&#19987;&#23478;&#35775;&#38382;&#20998;&#24067;&#20043;&#38388;&#30340;&#29305;&#23450;f-&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21452;&#37325;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;DIL&#65289;&#30452;&#25509;&#26368;&#23567;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#22312;&#21516;&#26679;&#30340;&#21452;&#37325;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#23545;&#20960;&#20010;&#22522;&#20934;&#20219;&#21153;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of reinforcement learning (RL) is to maximize the expected cumulative return. It has been shown that this objective can be represented by an optimization problem of the state-action visitation distribution under linear constraints. The dual problem of this formulation, which we refer to as dual RL, is unconstrained and easier to optimize. We show that several state-of-the-art off-policy deep reinforcement learning (RL) algorithms, under both online and offline, RL and imitation learning (IL) settings, can be viewed as dual RL approaches in a unified framework. This unification provides a common ground to study and identify the components that contribute to the success of these methods and also reveals the common shortcomings across methods with new insights for improvement. Our analysis shows that prior off-policy imitation learning methods are based on an unrealistic coverage assumption and are minimizing a particular f-divergence between the visitation distributions of the l
&lt;/p&gt;</description></item><item><title>&#22312;&#22240;&#26524;&#22270;&#20013;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2211.16468</link><description>&lt;p&gt;
&#22240;&#26524;&#22270;&#20013;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs. (arXiv:2211.16468v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16468
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#22270;&#20013;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#26159;&#23454;&#35777;&#31185;&#23398;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#24403;&#31995;&#32479;&#20013;&#28041;&#21450;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#26102;&#65292;&#36825;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#21069;&#38376;&#35843;&#25972;&#8212;&#8212;&#19968;&#31181;&#32463;&#20856;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;&#34429;&#28982;&#21069;&#38376;&#20272;&#35745;&#30340;&#32479;&#35745;&#29305;&#24615;&#24050;&#32463;&#24456;&#22909;&#22320;&#29702;&#35299;&#20102;&#65292;&#20294;&#23427;&#30340;&#31639;&#27861;&#26041;&#38754;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290;&#26368;&#36817;&#65292;Jeong&#65292;Tian&#21644;Barenboim [NeurIPS 2022]&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32473;&#23450;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#20013;&#25214;&#21040;&#28385;&#36275;&#21069;&#38376;&#20934;&#21017;&#30340;&#38598;&#21512;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$O&#65288;n^3&#65288;n+m&#65289;&#65289;$&#65292;&#20854;&#20013;$n$&#34920;&#31034;&#21464;&#37327;&#30340;&#25968;&#37327;&#65292;$m$&#34920;&#31034;&#22240;&#26524;&#22270;&#30340;&#36793;&#30340;&#25968;&#37327;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#21363;$O&#65288;n+m&#65289;$&#65292;&#29992;&#20110;&#36825;&#39033;&#20219;&#21153;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#28176;&#36817;&#26368;&#20248;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal effect estimation from observational data is a fundamental task in empirical sciences. It becomes particularly challenging when unobserved confounders are involved in a system. This paper focuses on front-door adjustment -- a classic technique which, using observed mediators allows to identify causal effects even in the presence of unobserved confounding. While the statistical properties of the front-door estimation are quite well understood, its algorithmic aspects remained unexplored for a long time. Recently, Jeong, Tian, and Barenboim [NeurIPS 2022] have presented the first polynomial-time algorithm for finding sets satisfying the front-door criterion in a given directed acyclic graph (DAG), with an $O(n^3(n+m))$ run time, where $n$ denotes the number of variables and $m$ the number of edges of the causal graph. In our work, we give the first linear-time, i.e., $O(n+m)$, algorithm for this task, which thus reaches the asymptotically optimal time complexity. This result impli
&lt;/p&gt;</description></item><item><title>&#22312;&#20247;&#21253;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#24314;&#27169;&#12289;&#27880;&#37322;&#30340;&#24402;&#19968;&#21270;&#20998;&#24067;&#21644;&#29109;&#27979;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#35782;&#21035;&#26631;&#31614;&#19981;&#30830;&#23450;&#30340;&#22270;&#20687;&#65292;&#24182;&#37327;&#21270;&#36825;&#20123;&#26679;&#26412;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.16380</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#24314;&#27169;&#12289;&#27880;&#37322;&#30340;&#24402;&#19968;&#21270;&#20998;&#24067;&#21644;&#29109;&#27979;&#37327;&#30340;&#19981;&#31934;&#30830;&#12289;&#20247;&#21253;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Approach for Noisy, Crowdsourced Datasets Utilizing Ensemble Modeling, Normalized Distributions of Annotations, and Entropic Measures of Uncertainty. (arXiv:2210.16380v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16380
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#21253;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#24314;&#27169;&#12289;&#27880;&#37322;&#30340;&#24402;&#19968;&#21270;&#20998;&#24067;&#21644;&#29109;&#27979;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#35782;&#21035;&#26631;&#31614;&#19981;&#30830;&#23450;&#30340;&#22270;&#20687;&#65292;&#24182;&#37327;&#21270;&#36825;&#20123;&#26679;&#26412;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#31934;&#30830;&#30340;&#12289;&#20247;&#21253;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20998;&#31867;&#23545;&#20110;&#26368;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#37117;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20004;&#20010;&#38382;&#39064;&#20351;&#24471;&#36825;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#26356;&#21152;&#22797;&#26434;&#65292;&#21363;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;AL-ALL&#21644;AL-PUB&#25968;&#25454;&#38598;--&#21253;&#21547;&#26469;&#33258;&#21476;&#24076;&#33098;&#32440;&#33609;&#30340;&#22270;&#20687;&#30340;&#32039;&#23494;&#35009;&#21098;&#30340;&#21333;&#20010;&#23383;&#31526;--&#21463;&#21040;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#20005;&#37325;&#24433;&#21709;&#12290;&#23558;&#38598;&#21512;&#24314;&#27169;&#24212;&#29992;&#20110;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#26631;&#31614;&#19981;&#30830;&#23450;&#30340;&#22270;&#20687;&#65292;&#24182;&#37327;&#21270;&#36825;&#20123;&#26679;&#26412;&#30340;&#21487;&#20449;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24212;&#29992;&#30001;&#20960;&#20046;&#30456;&#21516;&#30340;ResNets&#32452;&#25104;&#30340;&#22534;&#21472;&#27867;&#21270;&#65292;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65306;&#19968;&#20010;&#21033;&#29992;&#31232;&#30095;&#20132;&#21449;&#29109;&#65288;CXE&#65289;&#65292;&#21478;&#19968;&#20010;&#21033;&#29992;Kullback-Liebler&#25955;&#24230;&#65288;KLD&#65289;&#12290;&#20004;&#20010;&#32593;&#32476;&#37117;&#20351;&#29992;&#20174;&#20247;&#21253;&#19968;&#33268;&#24615;&#20013;&#24471;&#20986;&#30340;&#26631;&#31614;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#32593;&#32476;&#65292;KLD&#26159;&#30456;&#23545;&#20110;&#25152;&#25552;&#20986;&#30340;&#27880;&#37322;&#30340;&#24402;&#19968;&#21270;&#20998;&#24067;&#65288;NDA&#65289;&#35745;&#31639;&#30340;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#38598;&#21512;&#27169;&#22411;&#65292;&#25105;&#20204;&#24212;&#29992;k-&#36817;&#37051;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performing classification on noisy, crowdsourced image datasets can prove challenging even for the best neural networks. Two issues which complicate the problem on such datasets are class imbalance and ground-truth uncertainty in labeling. The AL-ALL and AL-PUB datasets -- consisting of tightly cropped, individual characters from images of ancient Greek papyri -- are strongly affected by both issues. The application of ensemble modeling to such datasets can help identify images where the ground-truth is questionable and quantify the trustworthiness of those samples. As such, we apply stacked generalization consisting of nearly identical ResNets with different loss functions: one utilizing sparse cross-entropy (CXE) and the other Kullback-Liebler Divergence (KLD). Both networks use labels drawn from the crowdsourced consensus. For the second network, the KLD is calculated with respect to the proposed Normalized Distribution of Annotations (NDA). For our ensemble model, we apply a k-near
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21333;&#26102;&#38388;&#23610;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#39532;&#23572;&#21487;&#22827;&#26679;&#26412;&#26356;&#26032;&#65292;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#25214;&#21040;&#20102;&#19968;&#20010;$\epsilon$-&#36817;&#20284;&#30340;&#31283;&#23450;&#28857;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\widetilde{\mathcal{O}}(\epsilon^{-2})$&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.09921</link><description>&lt;p&gt;
&#21333;&#26102;&#38388;&#23610;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Finite-time analysis of single-timescale actor-critic. (arXiv:2210.09921v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09921
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21333;&#26102;&#38388;&#23610;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#39532;&#23572;&#21487;&#22827;&#26679;&#26412;&#26356;&#26032;&#65292;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#25214;&#21040;&#20102;&#19968;&#20010;$\epsilon$-&#36817;&#20284;&#30340;&#31283;&#23450;&#28857;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\widetilde{\mathcal{O}}(\epsilon^{-2})$&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24212;&#29992;&#20013;&#65292;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#26368;&#23454;&#38469;&#30340;&#21333;&#26102;&#38388;&#23610;&#24230;&#24418;&#24335;&#19979;&#65292;&#20854;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#24615;&#20173;&#28982;&#19981;&#22815;&#29702;&#35299;&#12290;&#29616;&#26377;&#30340;&#21333;&#26102;&#38388;&#23610;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20998;&#26512;&#24037;&#20316;&#20165;&#38480;&#20110;&#31616;&#21270;&#30340;i.i.d.&#37319;&#26679;&#25110;&#34920;&#26684;&#35774;&#32622;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26356;&#23454;&#38469;&#30340;&#22312;&#32447;&#21333;&#26102;&#38388;&#23610;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#65292;&#35780;&#35770;&#23478;&#37319;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#65292;&#24182;&#22312;&#27599;&#20010;&#28436;&#21592;&#27493;&#39588;&#20013;&#20351;&#29992;&#21333;&#20010;&#39532;&#23572;&#21487;&#22827;&#26679;&#26412;&#36827;&#34892;&#26356;&#26032;&#12290;&#20808;&#21069;&#30340;&#20998;&#26512;&#26080;&#27861;&#22312;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#23454;&#29616;&#25910;&#25947;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#22312;&#32447;&#21333;&#26102;&#38388;&#23610;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#33021;&#22815;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\widetilde{\mathcal{O}}(\epsilon^{-2})$&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#19968;&#20010;$\epsilon$-&#36817;&#20284;&#30340;&#31283;&#23450;&#28857;&#65292;&#32780;&#22312;i.i.d.&#37319;&#26679;&#19979;&#65292;&#36825;&#20010;&#22797;&#26434;&#24230;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#20026;$\mathcal{O}(\epsilon^{-2})$&#12290;&#25105;&#20204;&#30340;&#26032;&#26694;&#26550;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Actor-critic methods have achieved significant success in many challenging applications. However, its finite-time convergence is still poorly understood in the most practical single-timescale form. Existing works on analyzing single-timescale actor-critic have been limited to i.i.d. sampling or tabular setting for simplicity. We investigate the more practical online single-timescale actor-critic algorithm on continuous state space, where the critic assumes linear function approximation and updates with a single Markovian sample per actor step. Previous analysis has been unable to establish the convergence for such a challenging scenario. We demonstrate that the online single-timescale actor-critic method provably finds an $\epsilon$-approximate stationary point with $\widetilde{\mathcal{O}}(\epsilon^{-2})$ sample complexity under standard assumptions, which can be further improved to $\mathcal{O}(\epsilon^{-2})$ under the i.i.d. sampling. Our novel framework systematically evaluates an
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24212;&#29992;&#30340;&#31614;&#21517;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#23398;&#27934;&#23519;&#21147;&#29702;&#35299;&#22797;&#26434;&#30340;&#27969;&#24335;&#25968;&#25454;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20110;&#20998;&#26512;&#38750;&#35268;&#21017;&#12289;&#38750;&#24179;&#31283;&#30340;&#27969;&#24335;&#25968;&#25454;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.14674</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31614;&#21517;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Signature Methods in Machine Learning. (arXiv:2206.14674v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24212;&#29992;&#30340;&#31614;&#21517;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#23398;&#27934;&#23519;&#21147;&#29702;&#35299;&#22797;&#26434;&#30340;&#27969;&#24335;&#25968;&#25454;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20110;&#20998;&#26512;&#38750;&#35268;&#21017;&#12289;&#38750;&#24179;&#31283;&#30340;&#27969;&#24335;&#25968;&#25454;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31614;&#21517;&#30340;&#25216;&#26415;&#20026;&#29702;&#35299;&#22797;&#26434;&#30340;&#27969;&#24335;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25552;&#20379;&#20102;&#25968;&#23398;&#27934;&#23519;&#21147;&#12290;&#36825;&#20123;&#27934;&#23519;&#21147;&#21487;&#20197;&#24456;&#33258;&#28982;&#22320;&#36716;&#21270;&#20026;&#29702;&#35299;&#27969;&#24335;&#25968;&#25454;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#20063;&#35768;&#26159;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25968;&#23398;&#30340;&#31934;&#30830;&#24615;&#65292;&#23427;&#20204;&#22312;&#20998;&#26512;&#38750;&#35268;&#21017;&#12289;&#38750;&#24179;&#31283;&#30340;&#27969;&#24335;&#25968;&#25454;&#20197;&#21450;&#25968;&#25454;&#32500;&#24230;&#21644;&#26679;&#26412;&#22823;&#23567;&#37117;&#36866;&#20013;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20102;&#24456;&#26377;&#29992;&#30340;&#24615;&#36136;&#12290;&#23545;&#20110;&#29702;&#35299;&#27969;&#24335;&#22810;&#27169;&#24577;&#25968;&#25454;&#26159;&#25351;&#25968;&#32423;&#30340;&#38382;&#39064;&#65306;&#38271;&#24230;&#20026;$n$&#30340;&#23383;&#27597;&#20018;&#65292;&#26469;&#33258;&#22823;&#23567;&#20026;$d$&#30340;&#23383;&#27597;&#34920;&#65292;&#21487;&#20197;&#26159;$d^n$&#31181;&#19981;&#21516;&#30340;&#28040;&#24687;&#12290;&#31614;&#21517;&#28040;&#38500;&#20102;&#30001;&#20110;&#37319;&#26679;&#19981;&#35268;&#21017;&#24615;&#32780;&#20135;&#29983;&#30340;&#25351;&#25968;&#32423;&#30340;&#22122;&#22768;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#25351;&#25968;&#32423;&#30340;&#20449;&#24687;&#37327;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#20445;&#25345;&#22312;&#21487;&#20197;&#30452;&#25509;&#31649;&#29702;&#36825;&#31181;&#25351;&#25968;&#32423;&#32553;&#25918;&#30340;&#39046;&#22495;&#20869;&#12290;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#26159;&#35768;&#22810;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#20294;&#38656;&#35201;&#21478;&#19968;&#31687;&#32508;&#36848;&#25991;&#31456;&#21644;&#36827;&#19968;&#27493;&#30340;&#24605;&#36335;&#12290;&#26412;&#32508;&#36848;&#25551;&#36848;&#20102;&#19968;&#31995;&#21015;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Signature-based techniques give mathematical insight into the interactions between complex streams of evolving data. These insights can be quite naturally translated into numerical approaches to understanding streamed data, and perhaps because of their mathematical precision, have proved useful in analysing streamed data in situations where the data is irregular, and not stationary, and the dimension of the data and the sample sizes are both moderate. Understanding streamed multi-modal data is exponential: a word in $n$ letters from an alphabet of size $d$ can be any one of $d^n$ messages. Signatures remove the exponential amount of noise that arises from sampling irregularity, but an exponential amount of information still remain. This survey aims to stay in the domain where that exponential scaling can be managed directly. Scalability issues are an important challenge in many problems but would require another survey article and further ideas. This survey describes a range of context
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#20844;&#24179;&#30340;&#38598;&#32676;&#32534;&#36753;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#22312;&#39030;&#28857;&#30528;&#33394;&#22270;&#20013;&#26631;&#20934;&#31639;&#27861;&#21487;&#33021;&#20135;&#29983;&#30340;&#20559;&#21521;&#23376;&#32452;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#21452;&#33394;&#22270;&#20013;&#65292;&#38382;&#39064;&#26159; NP-hard &#30340;&#65292;&#21363;&#20351;&#21482;&#20801;&#35768;&#22312;&#23376;&#32452;&#20869;&#37096;&#25554;&#20837;&#36793;&#32536;&#12290;</title><link>http://arxiv.org/abs/2112.03183</link><description>&lt;p&gt;
&#20462;&#25913;&#20844;&#24179;&#30340;&#38598;&#32676;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Modification-Fair Cluster Editing. (arXiv:2112.03183v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03183
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#20844;&#24179;&#30340;&#38598;&#32676;&#32534;&#36753;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#22312;&#39030;&#28857;&#30528;&#33394;&#22270;&#20013;&#26631;&#20934;&#31639;&#27861;&#21487;&#33021;&#20135;&#29983;&#30340;&#20559;&#21521;&#23376;&#32452;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#21452;&#33394;&#22270;&#20013;&#65292;&#38382;&#39064;&#26159; NP-hard &#30340;&#65292;&#21363;&#20351;&#21482;&#20801;&#35768;&#22312;&#23376;&#32452;&#20869;&#37096;&#25554;&#20837;&#36793;&#32536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#30340;&#38598;&#32676;&#32534;&#36753;&#38382;&#39064;&#65288;&#20063;&#31216;&#20026;&#30456;&#20851;&#32858;&#31867;&#65289;&#35201;&#27714;&#36890;&#36807;&#23569;&#37327;&#30340;&#36793;&#32536;&#20462;&#25913;&#23558;&#32473;&#23450;&#30340;&#22270;&#24418;&#36716;&#25442;&#20026;&#19981;&#30456;&#20132;&#30340;&#22242;&#65288;&#38598;&#32676;&#65289;&#12290;&#24403;&#24212;&#29992;&#20110;&#39030;&#28857;&#30528;&#33394;&#22270;&#26102;&#65288;&#39068;&#33394;&#34920;&#31034;&#23376;&#32452;&#65289;&#65292;&#23545;&#20110; NP-hard &#30340;&#38598;&#32676;&#32534;&#36753;&#38382;&#39064;&#30340;&#26631;&#20934;&#31639;&#27861;&#21487;&#33021;&#20250;&#20135;&#29983;&#20559;&#21521;&#25968;&#25454;&#23376;&#32452;&#65288;&#20363;&#22914;&#65292;&#20154;&#21475;&#32479;&#35745;&#23398;&#32452;&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#31181;&#20559;&#21521;&#26159;&#36890;&#36807;&#19982;&#23376;&#32452;&#25104;&#21592;&#30456;&#20851;&#30340;&#20462;&#25913;&#25968;&#37327;&#26469;&#34913;&#37327;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20462;&#25913;&#20844;&#24179;&#32422;&#26463;&#65292;&#30830;&#20445;&#27599;&#20010;&#23376;&#32452;&#30340;&#32534;&#36753;&#25968;&#37327;&#19982;&#20854;&#22823;&#23567;&#25104;&#27604;&#20363;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20004;&#31181;&#39030;&#28857;&#39068;&#33394;&#30340;&#22270;&#24418;&#30340;&#20462;&#25913;&#20844;&#24179;&#38598;&#32676;&#32534;&#36753;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#21482;&#20801;&#35768;&#22312;&#23376;&#32452;&#20869;&#37096;&#25554;&#20837;&#36793;&#32536;&#65292;&#35813;&#38382;&#39064;&#20173;&#28982;&#26159; NP-hard &#30340;&#65307;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#32463;&#20856;&#30340;&#8220;&#38750;&#20844;&#24179;&#8221;&#35774;&#32622;&#20013;&#65292;&#27492;&#24773;&#20917;&#21487;&#20197;&#36731;&#26494;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#22312;&#26356;&#19968;&#33324;&#30340;&#32534;&#36753;&#24418;&#24335;&#20013;&#65292;&#20462;&#25913;&#20844;&#24179;&#30340;&#21464;&#20307;&#20173;&#28982;&#26159;&#22266;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The classic Cluster Editing problem (also known as Correlation Clustering) asks to transform a given graph into a disjoint union of cliques (clusters) by a small number of edge modifications. When applied to vertex-colored graphs (the colors representing subgroups), standard algorithms for the NP-hard Cluster Editing problem may yield solutions that are biased towards subgroups of data (e.g., demographic groups), measured in the number of modifications incident to the members of the subgroups. We propose a modification fairness constraint which ensures that the number of edits incident to each subgroup is proportional to its size. To start with, we study Modification-Fair Cluster Editing for graphs with two vertex colors. We show that the problem is NP-hard even if one may only insert edges within a subgroup; note that in the classic "non-fair" setting, this case is trivially polynomial-time solvable. However, in the more general editing form, the modification-fair variant remains fixe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#21704;&#24076;&#20989;&#25968;&#23646;&#24615;&#36827;&#34892;&#25968;&#23398;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#20998;&#31867;&#23383;&#27597;&#34920;&#19978;&#30340;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#20351;&#29992;&#38543;&#26426;&#39640;&#26031;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#26368;&#22823;&#27744;&#21270;&#31561;&#20215;&#20110;&#36873;&#25321;&#19968;&#31181;&#26368;&#23567;&#21270;&#22120;&#25490;&#24207;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#19982;&#20854;&#20182;&#26368;&#23567;&#21270;&#22120;&#36317;&#31163;&#36739;&#36817;&#20294;&#19982;&#24207;&#21015;&#20013;&#30340;k-mer&#30456;&#36317;&#36739;&#36828;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2111.08452</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#23567;&#21270;&#22120;&#21644;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#29702;&#35770;&#36830;&#25509;&#21450;&#20854;&#22312;&#22522;&#22240;&#32452;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On minimizers and convolutional filters: theoretical connections and applications to genome analysis. (arXiv:2111.08452v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#21704;&#24076;&#20989;&#25968;&#23646;&#24615;&#36827;&#34892;&#25968;&#23398;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#20998;&#31867;&#23383;&#27597;&#34920;&#19978;&#30340;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#20351;&#29992;&#38543;&#26426;&#39640;&#26031;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#26368;&#22823;&#27744;&#21270;&#31561;&#20215;&#20110;&#36873;&#25321;&#19968;&#31181;&#26368;&#23567;&#21270;&#22120;&#25490;&#24207;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#19982;&#20854;&#20182;&#26368;&#23567;&#21270;&#22120;&#36317;&#31163;&#36739;&#36817;&#20294;&#19982;&#24207;&#21015;&#20013;&#30340;k-mer&#30456;&#36317;&#36739;&#36828;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#21270;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26159;&#20004;&#31181;&#23436;&#20840;&#19981;&#21516;&#30340;&#27969;&#34892;&#25216;&#26415;&#65292;&#22343;&#34987;&#29992;&#20110;&#20998;&#26512;&#29983;&#29289;&#24207;&#21015;&#12290;&#20174;&#34920;&#38754;&#19978;&#30475;&#65292;&#36825;&#20123;&#26041;&#27861;&#20284;&#20046;&#23436;&#20840;&#19981;&#21516;&#12290;&#26368;&#23567;&#21270;&#22120;&#20351;&#29992;&#28378;&#21160;&#31383;&#21475;&#30340;&#26368;&#23567;&#21704;&#24076;&#26041;&#27861;&#25552;&#21462;&#27599;&#20010;&#31383;&#21475;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;k-mer&#29305;&#24449;&#12290;CNN&#21017;&#20197;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#27744;&#21270;&#25805;&#20316;&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#22810;&#20010;&#31070;&#32463;&#23618;&#26469;&#23398;&#20064;&#28388;&#27874;&#22120;&#26412;&#36523;&#21450;&#20854;&#29992;&#20110;&#20998;&#31867;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20027;&#35201;&#32467;&#26524;&#26159;&#23545;&#21704;&#24076;&#20989;&#25968;&#23646;&#24615;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#26174;&#31034;&#23545;&#20110;&#20998;&#31867;&#23383;&#27597;&#34920;&#19978;&#30340;&#24207;&#21015;&#65292;&#20351;&#29992;&#38543;&#26426;&#39640;&#26031;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#26368;&#22823;&#27744;&#21270;&#31561;&#20215;&#20110;&#36873;&#25321;&#19968;&#20010;&#26368;&#23567;&#21270;&#22120;&#25490;&#24207;&#65292;&#20351;&#24471;&#36873;&#25321;&#30340;k-mer&#19982;&#24207;&#21015;&#20013;&#30340;k-mer&#65288;&#25353;&#27721;&#26126;&#36317;&#31163;&#65289;&#30456;&#36317;&#36739;&#36828;&#65292;&#20294;&#19982;&#20854;&#20182;&#26368;&#23567;&#21270;&#22120;&#30456;&#36317;&#36739;&#36817;&#12290;&#22312;&#23454;&#35777;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimizers and convolutional neural networks (CNNs) are two quite distinct popular techniques that have both been employed to analyze categorical biological sequences. At face value, the methods seem entirely dissimilar. Minimizers use min-wise hashing on a rolling window to extract a single important k-mer feature per window. CNNs start with a wide array of randomly initialized convolutional filters, paired with a pooling operation, and then multiple additional neural layers to learn both the filters themselves and how they can be used to classify the sequence.  Here, our main result is a careful mathematical analysis of hash function properties showing that for sequences over a categorical alphabet, random Gaussian initialization of convolutional filters with max-pooling is equivalent to choosing a minimizer ordering such that selected k-mers are (in Hamming distance) far from the k-mers within the sequence but close to other minimizers. In empirical experiments, we find that this pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#22495;&#36873;&#25321;&#26041;&#27861;&#30340;&#39640;&#32500;&#20989;&#25968;&#22270;&#27169;&#22411;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20989;&#25968;&#23545;&#20989;&#25968;&#22238;&#24402;&#20272;&#35745;&#33410;&#28857;&#37051;&#22495;&#65292;&#28982;&#21518;&#32467;&#21512;&#36825;&#20123;&#20272;&#35745;&#30340;&#37051;&#22495;&#24674;&#22797;&#25972;&#20010;&#22270;&#32467;&#26500;&#65292;&#20174;&#32780;&#30452;&#25509;&#20272;&#35745;&#26465;&#20214;&#29420;&#31435;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2105.02487</link><description>&lt;p&gt;
&#22522;&#20110;&#37051;&#22495;&#36873;&#25321;&#26041;&#27861;&#30340;&#39640;&#32500;&#20989;&#25968;&#22270;&#27169;&#22411;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Functional Graphical Model Structure Learning via Neighborhood Selection Approach. (arXiv:2105.02487v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.02487
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#22495;&#36873;&#25321;&#26041;&#27861;&#30340;&#39640;&#32500;&#20989;&#25968;&#22270;&#27169;&#22411;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20989;&#25968;&#23545;&#20989;&#25968;&#22238;&#24402;&#20272;&#35745;&#33410;&#28857;&#37051;&#22495;&#65292;&#28982;&#21518;&#32467;&#21512;&#36825;&#20123;&#20272;&#35745;&#30340;&#37051;&#22495;&#24674;&#22797;&#25972;&#20010;&#22270;&#32467;&#26500;&#65292;&#20174;&#32780;&#30452;&#25509;&#20272;&#35745;&#26465;&#20214;&#29420;&#31435;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#21521;&#22270;&#27169;&#22411;&#24191;&#27867;&#29992;&#20110;&#24314;&#27169;&#21521;&#37327;&#20540;&#25968;&#25454;&#30340;&#26465;&#20214;&#29420;&#31435;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#20195;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#28041;&#21450;EEG&#21644;fMRI&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#65292;&#35266;&#27979;&#26356;&#36866;&#21512;&#34987;&#24314;&#27169;&#20026;&#22810;&#21464;&#37327;&#38543;&#26426;&#20989;&#25968;&#32780;&#19981;&#26159;&#21521;&#37327;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20989;&#25968;&#22270;&#27169;&#22411;&#26469;&#24314;&#27169;&#36825;&#31181;&#20989;&#25968;&#25968;&#25454;&#30340;&#26465;&#20214;&#29420;&#31435;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#36873;&#25321;&#26041;&#27861;&#26469;&#20272;&#35745;&#39640;&#26031;&#20989;&#25968;&#22270;&#27169;&#22411;&#30340;&#32467;&#26500;&#65292;&#39318;&#20808;&#36890;&#36807;&#20989;&#25968;&#23545;&#20989;&#25968;&#22238;&#24402;&#20272;&#35745;&#27599;&#20010;&#33410;&#28857;&#30340;&#37051;&#22495;&#65292;&#28982;&#21518;&#36890;&#36807;&#32452;&#21512;&#20272;&#35745;&#30340;&#37051;&#22495;&#24674;&#22797;&#25972;&#20010;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#23545;&#38543;&#26426;&#20989;&#25968;&#30340;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#20551;&#35774;&#65292;&#24182;&#30452;&#25509;&#20272;&#35745;&#26465;&#20214;&#29420;&#31435;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#23545;&#21487;&#33021;&#19981;&#23384;&#22312;&#30340;&#31934;&#24230;&#31639;&#23376;&#36827;&#34892;&#26126;&#30830;&#23450;&#20041;&#30340;&#38656;&#35201;&#65292;&#23588;&#20854;&#26159;&#24403;&#20989;&#25968;&#20855;&#26377;&#26080;&#38480;&#32500;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Undirected graphical models are widely used to model the conditional independence structure of vector-valued data. However, in many modern applications, for example those involving EEG and fMRI data, observations are more appropriately modeled as multivariate random functions rather than vectors. Functional graphical models have been proposed to model the conditional independence structure of such functional data. We propose a neighborhood selection approach to estimate the structure of Gaussian functional graphical models, where we first estimate the neighborhood of each node via a function-on-function regression and subsequently recover the entire graph structure by combining the estimated neighborhoods. Our approach only requires assumptions on the conditional distributions of random functions, and we estimate the conditional independence structure directly. We thus circumvent the need for a well-defined precision operator that may not exist when the functions are infinite dimension
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#28369;&#21160;&#35270;&#35282;&#25511;&#21046;&#30340;&#31283;&#23450;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#21453;&#39304;&#32447;&#24615;&#21270;&#31995;&#32479;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;RHC&#30340;&#19968;&#38454;&#35299;&#21487;&#20197;&#20351;&#21487;&#32447;&#24615;&#21270;&#31995;&#32479;&#25351;&#25968;&#32423;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2103.15010</link><description>&lt;p&gt;
&#20851;&#20110;&#38750;&#32447;&#24615;&#28369;&#21160;&#35270;&#35282;&#25511;&#21046;&#31283;&#23450;&#24615;&#30340;&#20960;&#20309;&#35270;&#35282;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Stability of Nonlinear Receding Horizon Control: A Geometric Perspective. (arXiv:2103.15010v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.15010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#28369;&#21160;&#35270;&#35282;&#25511;&#21046;&#30340;&#31283;&#23450;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#21453;&#39304;&#32447;&#24615;&#21270;&#31995;&#32479;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;RHC&#30340;&#19968;&#38454;&#35299;&#21487;&#20197;&#20351;&#21487;&#32447;&#24615;&#21270;&#31995;&#32479;&#25351;&#25968;&#32423;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#28369;&#21160;&#35270;&#35282;&#25511;&#21046;&#65288;RHC&#65289;&#31574;&#30053;&#30340;&#24191;&#27867;&#24212;&#29992;&#24050;&#32463;&#20419;&#20351;&#20154;&#20204;&#22312;&#36825;&#20123;&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#26041;&#38754;&#36827;&#34892;&#20102;30&#22810;&#24180;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#29702;&#35770;&#20445;&#35777;&#35201;&#27714;&#27599;&#20010;&#65288;&#36890;&#24120;&#38750;&#20984;&#65289;&#35268;&#21010;&#38382;&#39064;&#37117;&#21487;&#20197;&#34987;&#27714;&#35299;&#20026;&#65288;&#36817;&#20284;&#65289;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#23454;&#26045;RHC&#30340;&#22522;&#20110;&#23548;&#25968;&#30340;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#26469;&#35828;&#26159;&#19981;&#29616;&#23454;&#30340;&#35201;&#27714;&#12290;&#26412;&#25991;&#22312;&#20869;&#37096;&#35268;&#21010;&#38382;&#39064;&#34987;&#35299;&#20915;&#20026;&#19968;&#38454;&#31283;&#23450;&#28857;&#32780;&#19981;&#19968;&#23450;&#26159;&#20840;&#23616;&#26368;&#20248;&#35299;&#26102;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;&#38750;&#32447;&#24615;RHC&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;&#29305;&#21035;&#20851;&#27880;&#21453;&#39304;&#32447;&#24615;&#21270;&#31995;&#32479;&#65292;&#24182;&#25552;&#20379;&#20102;&#27491;&#38754;&#21644;&#36127;&#38754;&#32467;&#26524;&#30340;&#28151;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#24378;&#26465;&#20214;&#19979;&#65292;RHC&#30340;&#19968;&#38454;&#35299;&#21487;&#20197;&#25351;&#25968;&#32423;&#22320;&#31283;&#23450;&#21487;&#32447;&#24615;&#21270;&#31995;&#32479;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;&#26465;&#20214;...
&lt;/p&gt;
&lt;p&gt;
%!TEX root = LCSS_main_max.tex  The widespread adoption of nonlinear Receding Horizon Control (RHC) strategies by industry has led to more than 30 years of intense research efforts to provide stability guarantees for these methods. However, current theoretical guarantees require that each (generally nonconvex) planning problem can be solved to (approximate) global optimality, which is an unrealistic requirement for the derivative-based local optimization methods generally used in practical implementations of RHC. This paper takes the first step towards understanding stability guarantees for nonlinear RHC when the inner planning problem is solved to first-order stationary points, but not necessarily global optima. Special attention is given to feedback linearizable systems, and a mixture of positive and negative results are provided. We establish that, under certain strong conditions, first-order solutions to RHC exponentially stabilize linearizable systems. Surprisingly, these conditio
&lt;/p&gt;</description></item></channel></rss>