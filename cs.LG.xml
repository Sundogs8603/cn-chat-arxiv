<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hFT-Transformer&#30340;&#33258;&#21160;&#38899;&#20048;&#36716;&#24405;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#20004;&#32423;&#20998;&#23618;&#39057;&#29575;-&#26102;&#38388;Transformer&#26550;&#26500;&#65292;&#24182;&#22312;MAPS&#21644;MAESTRO v3.0.0&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.04305</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#39057;&#29575;-&#26102;&#38388;Transformer&#30340;&#33258;&#21160;&#38050;&#29748;&#36716;&#24405;
&lt;/p&gt;
&lt;p&gt;
Automatic Piano Transcription with Hierarchical Frequency-Time Transformer. (arXiv:2307.04305v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04305
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hFT-Transformer&#30340;&#33258;&#21160;&#38899;&#20048;&#36716;&#24405;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#20004;&#32423;&#20998;&#23618;&#39057;&#29575;-&#26102;&#38388;Transformer&#26550;&#26500;&#65292;&#24182;&#22312;MAPS&#21644;MAESTRO v3.0.0&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#38271;&#26399;&#30340;&#39057;&#29575;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#23545;&#20110;&#33258;&#21160;&#38050;&#29748;&#36716;&#24405;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#30830;&#23450;&#22797;&#38899;&#38050;&#29748;&#20869;&#23481;&#20013;&#27599;&#20010;&#38899;&#31526;&#30340;&#31934;&#30830;&#36215;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#26102;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20381;&#38752;Transformer&#20013;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33021;&#21147;&#26469;&#25429;&#25417;&#36825;&#20123;&#39057;&#29575;&#21644;&#26102;&#38388;&#36724;&#19978;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;hFT-Transformer&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#20004;&#32423;&#20998;&#23618;&#39057;&#29575;-&#26102;&#38388;Transformer&#26550;&#26500;&#30340;&#33258;&#21160;&#38899;&#20048;&#36716;&#24405;&#26041;&#27861;&#12290;&#31532;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#21253;&#25324;&#26102;&#38388;&#36724;&#19978;&#30340;&#21367;&#31215;&#22359;&#65292;&#39057;&#29575;&#36724;&#19978;&#30340;Transformer&#32534;&#30721;&#22120;&#20197;&#21450;&#23558;&#39057;&#29575;&#36724;&#19978;&#30340;&#32500;&#24230;&#36716;&#25442;&#30340;Transformer&#35299;&#30721;&#22120;&#12290;&#28982;&#21518;&#23558;&#36755;&#20986;&#39304;&#36865;&#21040;&#31532;&#20108;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#21478;&#19968;&#20010;&#26102;&#38388;&#36724;&#19978;&#30340;Transformer&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#20351;&#29992;&#24191;&#27867;&#20351;&#29992;&#30340;MAPS&#21644;MAESTRO v3.0.0&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#25152;&#26377;&#27979;&#35797;&#25968;&#25454;&#19978;&#37117;&#23637;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Taking long-term spectral and temporal dependencies into account is essential for automatic piano transcription. This is especially helpful when determining the precise onset and offset for each note in the polyphonic piano content. In this case, we may rely on the capability of self-attention mechanism in Transformers to capture these long-term dependencies in the frequency and time axes. In this work, we propose hFT-Transformer, which is an automatic music transcription method that uses a two-level hierarchical frequency-time Transformer architecture. The first hierarchy includes a convolutional block in the time axis, a Transformer encoder in the frequency axis, and a Transformer decoder that converts the dimension in the frequency axis. The output is then fed into the second hierarchy which consists of another Transformer encoder in the time axis. We evaluated our method with the widely used MAPS and MAESTRO v3.0.0 datasets, and it demonstrated state-of-the-art performance on all t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20110;&#36793;&#32536;&#23384;&#20648;&#31649;&#29702;&#21644;&#36947;&#36335;&#24322;&#24120;&#26816;&#27979;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23384;&#20648;&#39640;&#20445;&#30495;&#38899;&#39057;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.04298</link><description>&lt;p&gt;
&#24102;&#26377;&#38646;-shot&#25968;&#25454;&#21387;&#32553;&#30340;&#36793;&#32536;&#23384;&#20648;&#31649;&#29702;&#37197;&#26041;&#29992;&#20110;&#36947;&#36335;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Edge Storage Management Recipe with Zero-Shot Data Compression for Road Anomaly Detection. (arXiv:2307.04298v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04298
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20110;&#36793;&#32536;&#23384;&#20648;&#31649;&#29702;&#21644;&#36947;&#36335;&#24322;&#24120;&#26816;&#27979;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23384;&#20648;&#39640;&#20445;&#30495;&#38899;&#39057;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#22522;&#20110;&#36793;&#32536;&#35745;&#31639;&#30340;&#36947;&#36335;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#21516;&#26102;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#12290;&#28982;&#32780;&#65292;&#36793;&#32536;&#35745;&#31639;&#26426;&#30340;&#25968;&#25454;&#23384;&#20648;&#31354;&#38388;&#24456;&#23567;&#65292;&#20294;&#25105;&#20204;&#38656;&#35201;&#38271;&#26102;&#38388;&#23384;&#20648;&#25910;&#38598;&#21040;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#20197;&#20415;&#26356;&#26032;&#29616;&#26377;&#27169;&#22411;&#25110;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24212;&#35813;&#32771;&#34385;&#19968;&#31181;&#22312;&#20445;&#25345;&#39640;&#20445;&#30495;&#38899;&#39057;&#30340;&#21516;&#26102;&#36827;&#34892;&#39640;&#25928;&#23384;&#20648;&#31649;&#29702;&#30340;&#26041;&#27861;&#12290;&#20174;&#30828;&#20214;&#35282;&#24230;&#26469;&#30475;&#65292;&#20351;&#29992;&#20302;&#20998;&#36776;&#29575;&#40614;&#20811;&#39118;&#21487;&#20197;&#30452;&#35266;&#22320;&#20943;&#23567;&#25991;&#20214;&#22823;&#23567;&#65292;&#20294;&#19981;&#25512;&#33616;&#36825;&#31181;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20250;&#20174;&#26681;&#26412;&#19978;&#21066;&#24369;&#39640;&#39057;&#32452;&#20214;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35745;&#31639;&#25991;&#20214;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#23558;&#25910;&#38598;&#21040;&#30340;&#39640;&#20998;&#36776;&#29575;&#38899;&#39057;&#32534;&#30721;&#20026;&#32039;&#20945;&#30340;&#20195;&#30721;&#65292;&#22240;&#27492;&#25512;&#33616;&#20351;&#29992;&#35813;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#36824;&#25552;&#20379;&#30456;&#24212;&#30340;&#35299;&#30721;&#26041;&#27861;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#30340;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show edge computing-based road anomaly detection systems which may also conduct data collection simultaneously. However, the edge computers will have small data storage but we need to store the collected audio samples for a long time in order to update existing models or develop a novel method. Therefore, we should consider an approach for efficient storage management methods while preserving high-fidelity audio. A hardware-perspective approach, such as using a low-resolution microphone, is an intuitive way to reduce file size but is not recommended because it fundamentally cuts off high-frequency components. On the other hand, a computational file compression approach that encodes collected high-resolution audio into a compact code should be recommended because it also provides a corresponding decoding method. Motivated by this, we propose a way of simple yet effective pre-trained autoencoder-based data compression method. The pre-trained autoencoder is trained for the 
&lt;/p&gt;</description></item><item><title>GG-ODE&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#29615;&#22659;&#23398;&#20064;&#22797;&#26434;&#31995;&#32479;&#21160;&#24577;&#65292;&#21033;&#29992;&#31070;&#32463;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.04287</link><description>&lt;p&gt;
&#24191;&#20041;&#22270;ODE&#65306;&#36328;&#29615;&#22659;&#23398;&#20064;&#22797;&#26434;&#31995;&#32479;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Generalizing Graph ODE for Learning Complex System Dynamics across Environments. (arXiv:2307.04287v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04287
&lt;/p&gt;
&lt;p&gt;
GG-ODE&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#29615;&#22659;&#23398;&#20064;&#22797;&#26434;&#31995;&#32479;&#21160;&#24577;&#65292;&#21033;&#29992;&#31070;&#32463;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21160;&#24577;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#22914;&#29983;&#29289;&#23398;&#20013;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#27169;&#22411;&#37117;&#26159;&#22522;&#20110;&#35266;&#27979;&#21040;&#30340;&#21382;&#21490;&#25968;&#25454;&#23398;&#20064;&#21333;&#19968;&#31995;&#32479;&#21160;&#24577;&#24182;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#21487;&#33021;&#35266;&#23519;&#21040;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#29983;&#25104;&#30340;&#22810;&#20010;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#22312;&#28201;&#24230;&#21644;&#37325;&#21147;&#31561;&#28508;&#22312;&#22806;&#22240;&#32032;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23398;&#20064;&#22810;&#20010;&#29615;&#22659;&#29305;&#23450;&#27169;&#22411;&#65292;&#20294;&#23427;&#26410;&#33021;&#21033;&#29992;&#36328;&#29615;&#22659;&#21160;&#24577;&#20013;&#30340;&#28508;&#22312;&#20849;&#24615;&#65292;&#24182;&#22312;&#27599;&#20010;&#29615;&#22659;&#25968;&#25454;&#31232;&#32570;&#25110;&#26377;&#38480;&#26102;&#25552;&#20379;&#36739;&#24046;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GG-ODE&#65288;&#24191;&#20041;&#22270;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#36328;&#29615;&#22659;&#23398;&#20064;&#36830;&#32493;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21160;&#24577;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#30001;&#22270;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#26469;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning multi-agent system dynamics has been extensively studied for various real-world applications, such as molecular dynamics in biology. Most of the existing models are built to learn single system dynamics from observed historical data and predict the future trajectory. In practice, however, we might observe multiple systems that are generated across different environments, which differ in latent exogenous factors such as temperature and gravity. One simple solution is to learn multiple environment-specific models, but it fails to exploit the potential commonalities among the dynamics across environments and offers poor prediction results where per-environment data is sparse or limited. Here, we present GG-ODE (Generalized Graph Ordinary Differential Equations), a machine learning framework for learning continuous multi-agent system dynamics across environments. Our model learns system dynamics using neural ordinary differential equations (ODE) parameterized by Graph Neural Netwo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;&#25945;&#24072;&#22238;&#31572;&#20013;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25945;&#24072;-&#23398;&#29983;&#32842;&#22825;&#23460;&#35821;&#26009;&#24211;&#23376;&#38598;&#19978;&#65292;GPT-4&#30340;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#30740;&#31350;&#36824;&#25351;&#20986;&#26679;&#26412;&#25277;&#26679;&#21644;&#20195;&#34920;&#24615;&#31561;&#25968;&#25454;&#38598;&#29305;&#24449;&#21487;&#33021;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.04274</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;&#25945;&#24072;&#22238;&#31572;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the efficacy of large language models in generating accurate teacher responses. (arXiv:2307.04274v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;&#25945;&#24072;&#22238;&#31572;&#20013;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25945;&#24072;-&#23398;&#29983;&#32842;&#22825;&#23460;&#35821;&#26009;&#24211;&#23376;&#38598;&#19978;&#65292;GPT-4&#30340;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#30740;&#31350;&#36824;&#25351;&#20986;&#26679;&#26412;&#25277;&#26679;&#21644;&#20195;&#34920;&#24615;&#31561;&#25968;&#25454;&#38598;&#29305;&#24449;&#21487;&#33021;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
(Tack&#31561;&#20154;&#65292;2023&#24180;)&#22312;&#31532;18&#23626;&#21019;&#26032;&#20351;&#29992;NLP&#26500;&#24314;&#25945;&#32946;&#24212;&#29992;&#30740;&#35752;&#20250;&#19978;&#32452;&#32455;&#20102;&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#25945;&#32946;&#23545;&#35805;&#20013;&#25945;&#24072;&#35821;&#35328;&#30340;&#29983;&#25104;&#12290;&#26412;&#30740;&#31350;&#25353;&#29031;&#20849;&#20139;&#20219;&#21153;&#30340;&#32467;&#26500;&#65292;&#35797;&#22270;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21521;&#23398;&#29983;&#25552;&#20379;&#20449;&#24687;&#24615;&#21644;&#26377;&#30410;&#27934;&#23519;&#21147;&#26041;&#38754;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20174;&#32780;&#27169;&#25311;&#30693;&#35782;&#28170;&#21338;&#30340;&#25945;&#24072;&#30340;&#35282;&#33394;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#20960;&#31181;&#22522;&#20934;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;GPT-4&#65288;&#23569;&#26679;&#26412;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#12289;&#24494;&#35843;&#30340;GPT-2&#21644;&#24494;&#35843;&#30340;DialoGPT&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20248;&#21270;&#25945;&#23398;&#36136;&#37327;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23545;Flan-T5&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#22312;&#25945;&#24072;-&#23398;&#29983;&#32842;&#22825;&#23460;&#35821;&#26009;&#24211;&#23376;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;BERTScore&#21644;DialogRPT&#24230;&#37327;&#65292;GPT-4&#27604;&#20854;&#20182;&#24494;&#35843;&#27169;&#22411;&#26356;&#20026;&#26377;&#25928;&#12290;&#25105;&#20204;&#20551;&#35774;&#26679;&#26412;&#25277;&#26679;&#21644;&#20195;&#34920;&#24615;&#31561;&#22810;&#20010;&#25968;&#25454;&#38598;&#29305;&#24449;&#21487;&#33021;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on Innovative Use of NLP for Building Educational Applications on generation of teacher language in educational dialogues. Following the structure of the shared task, in this study, we attempt to assess the generative abilities of large language models in providing informative and helpful insights to students, thereby simulating the role of a knowledgeable teacher. To this end, we present an extensive evaluation of several benchmarking generative models, including GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and fine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we fine-tuned the Flan-T5 model using reinforcement learning. Our experimental findings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of GPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT.  We hypothesize that several dataset characteristics, including sampling, representativen
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20844;&#20247;&#22823;&#35268;&#27169;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20114;&#21160;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20063;&#24341;&#21457;&#20102;&#31867;&#20284;&#25216;&#26415;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#24212;&#29992;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.04251</link><description>&lt;p&gt;
ChatGPT&#22312;&#29983;&#25104;&#24335;AI&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#24212;&#29992;&#65306;&#19968;&#20221;&#31616;&#27905;&#30340;&#35843;&#26597;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey. (arXiv:2307.04251v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04251
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20844;&#20247;&#22823;&#35268;&#27169;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20114;&#21160;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20063;&#24341;&#21457;&#20102;&#31867;&#20284;&#25216;&#26415;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#24212;&#29992;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#32463;&#36807;&#31934;&#24515;&#35757;&#32451;&#24182;&#20351;&#29992;&#20102;&#22823;&#37327;&#25968;&#25454;&#12290;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#65292;&#24182;&#25512;&#21160;&#20102;LLM&#33021;&#21147;&#30340;&#36793;&#30028;&#12290;ChatGPT&#22312;&#22823;&#35268;&#27169;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26222;&#36941;&#20844;&#20247;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#30340;&#20114;&#21160;&#65292;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#23427;&#36824;&#24341;&#21457;&#20102;&#24320;&#21457;&#31867;&#20284;&#25216;&#26415;&#21644;&#30740;&#31350;&#20854;&#24212;&#29992;&#21644;&#24433;&#21709;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;ChatGPT&#21450;&#20854;&#28436;&#21270;&#30340;&#24403;&#21069;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#31616;&#26126;&#35843;&#26597;&#12290;&#25105;&#20204;&#21516;&#26102;&#32771;&#34385;&#20102;ChatGPT&#30340;&#29627;&#29827;&#30418;&#21644;&#40657;&#30418;&#35270;&#35282;&#65292;&#21253;&#25324;&#25216;&#26415;&#30340;&#32452;&#25104;&#37096;&#20998;&#21644;&#22522;&#26412;&#35201;&#32032;&#65292;&#20197;&#21450;&#20854;&#24212;&#29992;&#12289;&#24433;&#21709;&#21644;&#24433;&#21709;&#12290;&#29627;&#29827;&#30418;&#26041;&#27861;&#30528;&#37325;&#20110;&#29702;&#35299;&#25216;&#26415;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#32780;&#40657;&#30418;&#26041;&#27861;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#22797;&#26434;&#31995;&#32479;&#65292;&#22240;&#27492;&#30740;&#31350;&#20854;&#36755;&#20837;&#65292;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a large language model (LLM) created by OpenAI that has been carefully trained on a large amount of data. It has revolutionized the field of natural language processing (NLP) and has pushed the boundaries of LLM capabilities. ChatGPT has played a pivotal role in enabling widespread public interaction with generative artificial intelligence (GAI) on a large scale. It has also sparked research interest in developing similar technologies and investigating their applications and implications. In this paper, our primary goal is to provide a concise survey on the current lines of research on ChatGPT and its evolution. We considered both the glass box and black box views of ChatGPT, encompassing the components and foundational elements of the technology, as well as its applications, impacts, and implications. The glass box approach focuses on understanding the inner workings of the technology, and the black box approach embraces it as a complex system, and thus examines its inputs,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#34892;&#31243;&#26102;&#38388;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#21033;&#29992;&#25935;&#24863;&#24615;&#20449;&#24687;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#21644;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#65292;&#20197;&#22788;&#29702;&#22320;&#36136;&#22797;&#26434;&#24615;&#20808;&#39564;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.04228</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#34892;&#31243;&#26102;&#38388;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#25935;&#24863;&#24615;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#21644;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#30340;&#22320;&#36136;&#22797;&#26434;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks. (arXiv:2307.04228v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#34892;&#31243;&#26102;&#38388;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#21033;&#29992;&#25935;&#24863;&#24615;&#20449;&#24687;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#21644;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#65292;&#20197;&#22788;&#29702;&#22320;&#36136;&#22797;&#26434;&#24615;&#20808;&#39564;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33945;&#29305;&#21345;&#27931;&#39532;&#23572;&#21487;&#22827;&#38142;&#65288;MCMC&#65289;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#20004;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#65306;&#20808;&#39564;&#20998;&#24067;&#30340;&#20934;&#30830;&#21051;&#30011;&#21644;&#20284;&#28982;&#20989;&#25968;&#30340;&#39640;&#25928;&#35780;&#20272;&#12290;&#22312;&#23618;&#26512;&#25104;&#20687;&#30340;&#36125;&#21494;&#26031;&#30740;&#31350;&#20013;&#65292;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#26041;&#20415;&#22320;&#23450;&#20041;&#20808;&#39564;&#20998;&#24067;&#65292;&#24182;&#21516;&#26102;&#20511;&#21161;&#22522;&#20110;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#65288;PCE&#65289;&#30340;&#20934;&#30830;&#20195;&#29702;&#27169;&#22411;&#26469;&#26367;&#20195;&#35745;&#31639;&#23494;&#38598;&#30340;&#20840;&#29289;&#29702;&#27491;&#21521;&#27714;&#35299;&#22120;&#12290;&#24403;PCA&#26080;&#27861;&#30452;&#25509;&#25552;&#20379;&#23450;&#20041;&#20808;&#39564;&#20998;&#24067;&#30340;&#26041;&#24335;&#26102;&#65292;&#21487;&#20197;&#37319;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65289;&#31561;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#20135;&#29983;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;VAE&#30340;&#28508;&#22312;&#21442;&#25968;&#19982;&#27491;&#21521;&#24314;&#27169;&#36755;&#20986;&#20043;&#38388;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#20195;&#29702;&#27169;&#22411;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo Markov Chain (MCMC) methods commonly confront two fundamental challenges: the accurate characterization of the prior distribution and the efficient evaluation of the likelihood. In the context of Bayesian studies on tomography, principal component analysis (PCA) can in some cases facilitate the straightforward definition of the prior distribution, while simultaneously enabling the implementation of accurate surrogate models based on polynomial chaos expansion (PCE) to replace computationally intensive full-physics forward solvers. When faced with scenarios where PCA does not offer a direct means of easily defining the prior distribution alternative methods like deep generative models (e.g., variational autoencoders (VAEs)), can be employed as viable options. However, accurately producing a surrogate capable of capturing the intricate non-linear relationship between the latent parameters of a VAE and the outputs of forward modeling presents a notable challenge. Indeed, while
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#33258;&#32534;&#30721;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#26174;&#33879;&#21387;&#32553;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#31185;&#23398;&#25968;&#25454;&#65292;&#24182;&#20445;&#25345;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.04216</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#33258;&#32534;&#30721;&#22120;&#30340;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#31185;&#23398;&#25968;&#25454;&#26377;&#25439;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Autoencoder-based Lossy Compression for Large-scale High-resolution Scientific Data. (arXiv:2307.04216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#33258;&#32534;&#30721;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#26174;&#33879;&#21387;&#32553;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#31185;&#23398;&#25968;&#25454;&#65292;&#24182;&#20445;&#25345;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25439;&#21387;&#32553;&#24050;&#25104;&#20026;&#35768;&#22810;&#39046;&#22495;&#20013;&#20943;&#23567;&#25968;&#25454;&#22823;&#23567;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#36825;&#31181;&#21387;&#32553;&#26041;&#27861;&#23545;&#20110;&#22823;&#23567;&#22312;&#20960;&#20010;PB&#33539;&#22260;&#20869;&#30340;&#22823;&#35268;&#27169;&#31185;&#23398;&#25968;&#25454;&#23588;&#20026;&#37325;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#24050;&#25104;&#21151;&#22320;&#29992;&#20110;&#21387;&#32553;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#20294;&#36825;&#31181;&#31070;&#32463;&#32593;&#32476;&#22312;&#31185;&#23398;&#25968;&#25454;&#39046;&#22495;&#23578;&#26410;&#24191;&#20026;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#19981;&#20165;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#22823;&#35268;&#27169;&#31185;&#23398;&#25968;&#25454;&#65292;&#36824;&#21487;&#20197;&#20445;&#25345;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20844;&#24320;&#30340;&#31185;&#23398;&#22522;&#20934;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#24212;&#29992;&#20110;&#19968;&#31181;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#30340;&#27668;&#20505;&#27169;&#25311;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;140&#30340;&#21387;&#32553;&#27604;&#65292;&#21516;&#26102;&#20445;&#25345;&#37325;&#24314;&#36136;&#37327;&#12290;&#39640;&#20998;&#36776;&#29575;&#31038;&#21306;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;(CESM) Version 1.3&#30340;&#27169;&#25311;&#25968;&#25454;&#22312;&#21387;&#32553;&#27604;&#36798;&#21040;200&#30340;&#21516;&#26102;&#36827;&#34892;&#20102;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lossy compression has become an important technique to reduce data size in many domains. This type of compression is especially valuable for large-scale scientific data, whose size ranges up to several petabytes. Although Autoencoder-based models have been successfully leveraged to compress images and videos, such neural networks have not widely gained attention in the scientific data domain. Our work presents a neural network that not only significantly compresses large-scale scientific data but also maintains high reconstruction quality. The proposed model is tested with scientific benchmark data available publicly and applied to a large-scale high-resolution climate modeling data set. Our model achieves a compression ratio of 140 on several benchmark data sets without compromising the reconstruction quality. Simulation data from the High-Resolution Community Earth System Model (CESM) Version 1.3 over 500 years are also being compressed with a compression ratio of 200 while the recon
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;360&#24230;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#22522;&#20110;&#21160;&#20316;&#30340;&#29699;&#26435;&#24674;&#22797;&#27169;&#22411;&#65292;&#25506;&#31350;&#29699;&#38431;&#22312;&#22833;&#21435;&#29699;&#26435;&#21518;&#22914;&#20309;&#23613;&#24555;&#24674;&#22797;&#29699;&#26435;&#65292;&#29699;&#38431;&#20301;&#32622;&#23545;&#29699;&#26435;&#24674;&#22797;&#30340;&#24433;&#21709;&#20197;&#21450;&#22312;&#21387;&#21147;&#19979;&#26356;&#26131;&#22833;&#35823;&#30340;&#29699;&#21592;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04215</link><description>&lt;p&gt;
&#24191;&#20041;&#30340;&#22522;&#20110;&#21160;&#20316;&#30340;&#29699;&#26435;&#24674;&#22797;&#27169;&#22411;&#20351;&#29992;360&#24230;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generalized Action-based Ball Recovery Model using 360$^\circ$ data. (arXiv:2307.04215v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;360&#24230;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#22522;&#20110;&#21160;&#20316;&#30340;&#29699;&#26435;&#24674;&#22797;&#27169;&#22411;&#65292;&#25506;&#31350;&#29699;&#38431;&#22312;&#22833;&#21435;&#29699;&#26435;&#21518;&#22914;&#20309;&#23613;&#24555;&#24674;&#22797;&#29699;&#26435;&#65292;&#29699;&#38431;&#20301;&#32622;&#23545;&#29699;&#26435;&#24674;&#22797;&#30340;&#24433;&#21709;&#20197;&#21450;&#22312;&#21387;&#21147;&#19979;&#26356;&#26131;&#22833;&#35823;&#30340;&#29699;&#21592;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25317;&#26377;&#26356;&#22810;&#30340;&#25511;&#29699;&#24182;&#19981;&#19968;&#23450;&#23548;&#33268;&#32988;&#21033;&#65292;&#20294;&#20687;&#26364;&#22478;&#12289;&#21033;&#29289;&#28006;&#21644;&#21033;&#20857;&#32852;&#36825;&#26679;&#30340;&#29699;&#38431;&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#19968;&#30452;&#35797;&#22270;&#22312;&#22833;&#21435;&#29699;&#21518;&#23613;&#24555;&#24674;&#22797;&#29699;&#26435;&#12290;&#29616;&#22312;&#65292;&#19990;&#30028;&#19978;&#19968;&#20123;&#39030;&#32423;&#25945;&#32451;&#24212;&#29992;&#39640;&#20301;&#36924;&#25250;&#30340;&#39118;&#26684;&#65292;&#20687;&#8220;&#20116;&#31186;&#35268;&#21017;&#8221;&#36825;&#26679;&#30340;&#27010;&#24565;&#36890;&#24120;&#24402;&#21151;&#20110;&#29916;&#36842;&#22885;&#25289;&#65292;&#27491;&#22312;&#25193;&#25955;&#24320;&#26469;&#65292;&#25104;&#20026;&#35768;&#22810;&#29699;&#38431;&#36807;&#21435;&#20960;&#24180;&#26469;&#25171;&#29699;&#30340;&#22522;&#26412;&#37096;&#20998;&#12290;&#23186;&#20307;&#32463;&#24120;&#21548;&#21040;&#8220;&#19981;&#35753;&#20182;&#20204;&#21912;&#24687;&#8221;&#21644;&#8220;&#23613;&#24555;&#25343;&#22238;&#29699;&#26435;&#8221;&#31561;&#35828;&#27861;&#65292;&#20294;&#26159;&#21738;&#20123;&#21160;&#20316;&#26368;&#26377;&#21487;&#33021;&#25913;&#21464;&#29699;&#26435;&#65311;&#29699;&#38431;&#30340;&#20301;&#32622;&#23545;&#29699;&#26435;&#24674;&#22797;&#26377;&#20160;&#20040;&#24433;&#21709;&#65311;&#21738;&#20123;&#29699;&#21592;&#22312;&#21387;&#21147;&#19979;&#26356;&#23481;&#26131;&#22833;&#35823;&#65311;&#25105;&#20204;&#33021;&#21542;&#35780;&#20272;&#37027;&#20123;&#24182;&#19981;&#20687;&#19978;&#36848;&#29699;&#38431;&#19968;&#26679;&#24378;&#28872;&#36924;&#25250;&#25511;&#29699;&#29699;&#21592;&#30340;&#29699;&#38431;&#30340;&#38450;&#23432;&#21160;&#24577;&#65311;&#25105;&#20204;&#35797;&#22270;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#21644;&#20854;&#20182;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even though having more possession does not necessarily lead to winning, teams like Manchester City, Liverpool, and Leeds United notably have tried to recover the ball quickly after they lost it over the past few years. Nowadays, some of the top managers in the world apply high-pressing styles, and concepts such as the five-second rule, usually credited to Guardiola, have been spreading out [9][10], becoming a fundamental part of how lots of teams have played over the recent years. Expressions like "don't let them breathe" and "get the ball back as soon as possible" are often heard in the media [4][5][6], but what are the actions that most lead to a change in possession? What is the influence of a team's positioning on the ball recovery? Which are the players that more often collapse when under pressure? Can we evaluate the defensive dynamics of teams that do not necessarily press the player in possession as intensely as those mentioned above? We try to answer those and other questions
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#65292;&#21457;&#29616;&#21363;&#20351;&#22312;&#38750;&#25919;&#31574;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#26377;&#21161;&#20110;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#20248;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.04210</link><description>&lt;p&gt;
&#25506;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Investigating the Edge of Stability Phenomenon in Reinforcement Learning. (arXiv:2307.04210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#65292;&#21457;&#29616;&#21363;&#20351;&#22312;&#38750;&#25919;&#31574;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#26377;&#21161;&#20110;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#20248;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#29702;&#35299;&#20840;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#21160;&#21147;&#23398;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36890;&#36807;&#26377;&#21160;&#37327;&#30340;&#20840;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#25581;&#31034;&#20102;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#12290;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#21457;&#29983;&#22312;Hessian&#30340;&#20027;&#23548;&#29305;&#24449;&#20540;&#36798;&#21040;&#20108;&#27425;&#25439;&#22833;&#30340;&#21457;&#25955;&#38408;&#20540;&#26102;&#65292;&#27492;&#21518;&#23427;&#24320;&#22987;&#22312;&#38408;&#20540;&#21608;&#22260;&#25391;&#33633;&#65292;&#24182;&#19988;&#25439;&#22833;&#24320;&#22987;&#34920;&#29616;&#20986;&#23616;&#37096;&#19981;&#31283;&#23450;&#24615;&#20294;&#22312;&#36739;&#38271;&#26102;&#38388;&#20869;&#36880;&#28176;&#20943;&#23567;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20307;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#25968;&#25454;&#22330;&#26223;&#20013;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#25919;&#31574;Q&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#65292;&#23613;&#31649;&#19982;&#30417;&#30563;&#23398;&#20064;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#22914;&#25968;&#25454;&#20998;&#24067;&#30340;&#38750;&#31283;&#23450;&#24615;&#21644;&#24341;&#23548;bootstrap&#30340;&#20351;&#29992;&#65292;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#20173;&#28982;&#21487;&#33021;&#22312;&#38750;&#25919;&#31574;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#12290;&#19982;&#30417;&#30563;&#23398;&#20064;&#19981;&#21516;&#30340;&#26159;&#65292;&#30417;&#30563;&#23398;&#20064;&#21024;&#38500;&#20102;&#35823;&#24046;&#20943;&#23567;&#30340;&#31283;&#24577;&#65288;&#35823;&#24046;&#20943;&#23567;&#38454;&#27573;&#19979;&#32654;&#22312;&#31283;&#23450;&#36793;&#30028;&#30340;&#24773;&#20917;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress has been made in understanding optimisation dynamics in neural networks trained with full-batch gradient descent with momentum with the uncovering of the edge of stability phenomenon in supervised learning. The edge of stability phenomenon occurs as the leading eigenvalue of the Hessian reaches the divergence threshold of the underlying optimisation algorithm for a quadratic loss, after which it starts oscillating around the threshold, and the loss starts to exhibit local instability but decreases over long time frames. In this work, we explore the edge of stability phenomenon in reinforcement learning (RL), specifically off-policy Q-learning algorithms across a variety of data regimes, from offline to online RL. Our experiments reveal that, despite significant differences to supervised learning, such as non-stationarity of the data distribution and the use of bootstrapping, the edge of stability phenomenon can be present in off-policy deep RL. Unlike supervised learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20225;&#19994;&#20013;&#37096;&#32626;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#20010;&#20154;&#21644;&#39640;&#24230;&#25935;&#24863;&#25968;&#25454;&#25152;&#24341;&#36215;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#37492;&#21035;&#20986;&#20102;40&#22810;&#20010;&#25361;&#25112;&#24182;&#23558;&#20854;&#31995;&#32479;&#21270;&#65292;&#25552;&#20986;&#20102;&#20225;&#19994;&#21487;&#20197;&#37319;&#29992;&#30340;&#25112;&#30053;&#21644;&#31995;&#32479;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.04208</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#20225;&#19994;&#20013;&#37096;&#32626;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
On the Challenges of Deploying Privacy-Preserving Synthetic Data in the Enterprise. (arXiv:2307.04208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20225;&#19994;&#20013;&#37096;&#32626;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#20010;&#20154;&#21644;&#39640;&#24230;&#25935;&#24863;&#25968;&#25454;&#25152;&#24341;&#36215;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#37492;&#21035;&#20986;&#20102;40&#22810;&#20010;&#25361;&#25112;&#24182;&#23558;&#20854;&#31995;&#32479;&#21270;&#65292;&#25552;&#20986;&#20102;&#20225;&#19994;&#21487;&#20197;&#37319;&#29992;&#30340;&#25112;&#30053;&#21644;&#31995;&#32479;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#27491;&#33719;&#24471;&#31354;&#21069;&#30340;&#26222;&#21450;&#65292;&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#33021;&#21147;&#24102;&#26469;&#20102;&#20852;&#22859;&#21644;&#24551;&#34385;&#30340;&#28151;&#21512;&#24863;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#32626;&#21512;&#25104;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20225;&#19994;&#37096;&#32626;&#19978;&#65292;&#29305;&#21035;&#20851;&#27880;&#20010;&#20154;&#21644;&#39640;&#24230;&#25935;&#24863;&#25968;&#25454;&#25152;&#24341;&#36215;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#37492;&#21035;&#20986;&#20102;40&#22810;&#20010;&#25361;&#25112;&#65292;&#24182;&#23558;&#23427;&#20204;&#31995;&#32479;&#21270;&#20026;&#20116;&#20010;&#20027;&#35201;&#32452;&#21035; - i&#65289;&#29983;&#25104;&#65292;ii&#65289;&#22522;&#30784;&#35774;&#26045;&#21644;&#26550;&#26500;&#65292;iii&#65289;&#27835;&#29702;&#65292;iv&#65289;&#21512;&#35268;&#21644;&#30417;&#31649;&#65292;&#21644;v&#65289;&#37319;&#32435;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20225;&#19994;&#21487;&#20197;&#37319;&#29992;&#30340;&#25112;&#30053;&#21644;&#31995;&#32479;&#26041;&#27861;&#26469;&#26377;&#25928;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#23545;&#23454;&#26045;&#35299;&#20915;&#26041;&#26696;&#30340;&#20449;&#20219;&#26469;&#23454;&#29616;&#20182;&#20204;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI technologies are gaining unprecedented popularity, causing a mix of excitement and apprehension through their remarkable capabilities. In this paper, we study the challenges associated with deploying synthetic data, a subfield of Generative AI. Our focus centers on enterprise deployment, with an emphasis on privacy concerns caused by the vast amount of personal and highly sensitive data. We identify 40+ challenges and systematize them into five main groups -- i) generation, ii) infrastructure &amp; architecture, iii) governance, iv) compliance &amp; regulation, and v) adoption. Additionally, we discuss a strategic and systematic approach that enterprises can employ to effectively address the challenges and achieve their goals by establishing trust in the implemented solutions.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25193;&#23637;&#20102;&#21069;&#21521;&#21069;&#21521;&#31639;&#27861;&#65292;&#39318;&#20808;&#22312;IMDb&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#20854;&#27425;&#24341;&#20837;&#20102;&#37329;&#23383;&#22612;&#20248;&#21270;&#31574;&#30053;&#26469;&#25913;&#36827;&#25439;&#22833;&#38408;&#20540;&#65292;&#26368;&#21518;&#36890;&#36807;&#21442;&#25968;&#21487;&#35270;&#21270;&#24471;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2307.04205</link><description>&lt;p&gt;
&#25193;&#23637;&#21069;&#21521;&#21069;&#21521;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Extending the Forward Forward Algorithm. (arXiv:2307.04205v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04205
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25193;&#23637;&#20102;&#21069;&#21521;&#21069;&#21521;&#31639;&#27861;&#65292;&#39318;&#20808;&#22312;IMDb&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#20854;&#27425;&#24341;&#20837;&#20102;&#37329;&#23383;&#22612;&#20248;&#21270;&#31574;&#30053;&#26469;&#25913;&#36827;&#25439;&#22833;&#38408;&#20540;&#65292;&#26368;&#21518;&#36890;&#36807;&#21442;&#25968;&#21487;&#35270;&#21270;&#24471;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#21521;&#21069;&#21521;&#31639;&#27861;&#26159;Geoffrey Hinton&#20110;2022&#24180;11&#26376;&#25552;&#20986;&#30340;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#20316;&#20026;&#23545;&#21453;&#21521;&#20256;&#25773;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#22797;&#21046;&#20102;Hinton&#30340;&#23454;&#39564;&#65292;&#24182;&#38543;&#21518;&#36890;&#36807;&#20004;&#20010;&#37325;&#35201;&#30340;&#36129;&#29486;&#25193;&#23637;&#20102;&#35813;&#26041;&#27861;&#30340;&#33539;&#22260;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20026;&#21069;&#21521;&#21069;&#21521;&#32593;&#32476;&#22312;IMDb&#30005;&#24433;&#35780;&#35770;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#19978;&#30340;&#32467;&#26524;&#26631;&#24535;&#30528;&#35813;&#31639;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20043;&#22806;&#30340;&#39318;&#27425;&#25193;&#23637;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37329;&#23383;&#22612;&#20248;&#21270;&#31574;&#30053;&#65292;&#29992;&#20110;&#25439;&#22833;&#38408;&#20540;&#65292;&#36825;&#26159;&#21069;&#21521;&#21069;&#21521;&#26041;&#27861;&#29305;&#26377;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#37329;&#23383;&#22612;&#26041;&#27861;&#34920;&#26126;&#65292;&#19968;&#20010;&#22909;&#30340;&#38408;&#20540;&#31574;&#30053;&#20250;&#23548;&#33268;&#27979;&#35797;&#38169;&#35823;&#29575;&#30340;&#24046;&#24322;&#39640;&#36798;8%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#35757;&#32451;&#21442;&#25968;&#36827;&#34892;&#21487;&#35270;&#21270;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#27934;&#23519;&#65292;&#20363;&#22914;&#26435;&#37325;&#30340;&#24179;&#22343;&#20540;&#21644;&#26041;&#24046;&#26174;&#33879;&#22686;&#21152;&#20102;10-20&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Forward Forward algorithm, proposed by Geoffrey Hinton in November 2022, is a novel method for training neural networks as an alternative to backpropagation. In this project, we replicate Hinton's experiments on the MNIST dataset, and subsequently extend the scope of the method with two significant contributions. First, we establish a baseline performance for the Forward Forward network on the IMDb movie reviews dataset. As far as we know, our results on this sentiment analysis task marks the first instance of the algorithm's extension beyond computer vision. Second, we introduce a novel pyramidal optimization strategy for the loss threshold - a hyperparameter specific to the Forward Forward method. Our pyramidal approach shows that a good thresholding strategy causes a difference of upto 8% in test error. 1 Lastly, we perform visualizations of the trained parameters and derived several significant insights, such as a notably larger (10-20x) mean and variance in the weights acquire
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#36712;&#36857;&#19978;&#30340;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#24182;&#19988;&#23545;&#20110;&#29305;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#20102;&#36712;&#36857;&#23545;&#40784;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#28176;&#36827;&#23574;&#38160;&#21270;&#21644;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#25193;&#23637;&#20102;&#24403;&#21069;&#25991;&#29486;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.04204</link><description>&lt;p&gt;
&#36712;&#36857;&#23545;&#40784;&#65306;&#36890;&#36807;&#20998;&#21449;&#29702;&#35770;&#29702;&#35299;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory. (arXiv:2307.04204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#36712;&#36857;&#19978;&#30340;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#24182;&#19988;&#23545;&#20110;&#29305;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#20102;&#36712;&#36857;&#23545;&#40784;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#28176;&#36827;&#23574;&#38160;&#21270;&#21644;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#25193;&#23637;&#20102;&#24403;&#21069;&#25991;&#29486;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cohen&#31561;&#20154;&#65288;2021&#65289;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#36712;&#36857;&#19978;&#25439;&#22833;Hessian&#30340;&#26368;&#22823;&#29305;&#24449;&#20540;&#65288;&#21363;&#38160;&#24230;&#65289;&#65292;&#35266;&#23519;&#21040;&#19968;&#31181;&#31216;&#20026;&#31283;&#23450;&#36793;&#32536;&#65288;EoS&#65289;&#30340;&#29616;&#35937;&#12290;&#38160;&#24230;&#22312;&#22521;&#35757;&#30340;&#26089;&#26399;&#38454;&#27573;&#22686;&#21152;&#65288;&#31216;&#20026;&#28176;&#36827;&#23574;&#38160;&#21270;&#65289;&#65292;&#26368;&#32456;&#25509;&#36817;&#38408;&#20540;$2/\text{(&#27493;&#38271;)}$&#38468;&#36817;&#20572;&#28382;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#39318;&#20808;&#35777;&#26126;&#20102;&#24403;EoS&#29616;&#35937;&#21457;&#29983;&#26102;&#65292;&#19981;&#21516;&#30340;GD&#36712;&#36857;&#65288;&#32463;&#36807;&#36866;&#24403;&#30340;&#21442;&#25968;&#21270;&#65289;&#22312;&#19968;&#20010;&#29305;&#23450;&#30340;&#20998;&#21449;&#22270;&#19978;&#23545;&#40784;&#65292;&#32780;&#19982;&#21021;&#22987;&#21270;&#26080;&#20851;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#20108;&#23618;&#20840;&#36830;&#25509;&#32447;&#24615;&#32593;&#32476;&#21644;&#19968;&#20010;&#20351;&#29992;&#21333;&#20010;&#25968;&#25454;&#28857;&#35757;&#32451;&#30340;&#21333;&#31070;&#32463;&#20803;&#38750;&#32447;&#24615;&#32593;&#32476;&#20005;&#26684;&#35777;&#26126;&#20102;&#36825;&#31181;&#36712;&#36857;&#23545;&#40784;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#36712;&#36857;&#23545;&#40784;&#20998;&#26512;&#24314;&#31435;&#20102;&#28176;&#36827;&#23574;&#38160;&#21270;&#21644;EoS&#29616;&#35937;&#65292;&#28085;&#30422;&#24182;&#25193;&#23637;&#20102;&#26368;&#36817;&#25991;&#29486;&#20013;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cohen et al. (2021) empirically study the evolution of the largest eigenvalue of the loss Hessian, also known as sharpness, along the gradient descent (GD) trajectory and observe a phenomenon called the Edge of Stability (EoS). The sharpness increases at the early phase of training (referred to as progressive sharpening), and eventually saturates close to the threshold of $2 / \text{(step size)}$. In this paper, we start by demonstrating through empirical studies that when the EoS phenomenon occurs, different GD trajectories (after a proper reparameterization) align on a specific bifurcation diagram independent of initialization. We then rigorously prove this trajectory alignment phenomenon for a two-layer fully-connected linear network and a single-neuron nonlinear network trained with a single data point. Our trajectory alignment analysis establishes both progressive sharpening and EoS phenomena, encompassing and extending recent findings in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#22312;&#26631;&#20934;&#27491;&#24577;&#21327;&#21464;&#37327;&#19979;&#30340;&#21442;&#25968;&#20272;&#35745;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#26679;&#26412;&#22797;&#26434;&#24230;&#26354;&#32447;&#22312;&#36870;&#28201;&#24230;&#26041;&#38754;&#26377;&#20004;&#20010;&#36716;&#25240;&#28857;&#65292;&#26126;&#30830;&#21010;&#20998;&#20102;&#20302;&#12289;&#20013;&#21644;&#39640;&#28201;&#24230;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2307.04191</link><description>&lt;p&gt;
&#20851;&#20110;&#36923;&#36753;&#22238;&#24402;&#20013;&#21442;&#25968;&#20272;&#35745;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the sample complexity of estimation in logistic regression. (arXiv:2307.04191v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#22312;&#26631;&#20934;&#27491;&#24577;&#21327;&#21464;&#37327;&#19979;&#30340;&#21442;&#25968;&#20272;&#35745;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#26679;&#26412;&#22797;&#26434;&#24230;&#26354;&#32447;&#22312;&#36870;&#28201;&#24230;&#26041;&#38754;&#26377;&#20004;&#20010;&#36716;&#25240;&#28857;&#65292;&#26126;&#30830;&#21010;&#20998;&#20102;&#20302;&#12289;&#20013;&#21644;&#39640;&#28201;&#24230;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#26159;&#22122;&#22768;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#20043;&#19968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26631;&#20934;&#27491;&#24577;&#21327;&#21464;&#37327;&#19979;&#65292;&#20197;$\ell_2$&#35823;&#24046;&#20026;&#38480;&#65292;&#20272;&#35745;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#21442;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#32771;&#34385;&#20102;&#32500;&#24230;&#21644;&#36870;&#28201;&#24230;&#30340;&#24433;&#21709;&#12290;&#36870;&#28201;&#24230;&#25511;&#21046;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#20449;&#22122;&#27604;&#12290;&#34429;&#28982;&#36923;&#36753;&#22238;&#24402;&#30340;&#24191;&#20041;&#30028;&#38480;&#21644;&#28176;&#36817;&#24615;&#33021;&#24050;&#32463;&#26377;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#20851;&#20110;&#21442;&#25968;&#20272;&#35745;&#30340;&#38750;&#28176;&#36817;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;&#20043;&#21069;&#30340;&#20998;&#26512;&#20013;&#27809;&#26377;&#35752;&#35770;&#20854;&#19982;&#35823;&#24046;&#21644;&#36870;&#28201;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#26354;&#32447;&#22312;&#36870;&#28201;&#24230;&#26041;&#38754;&#20855;&#26377;&#20004;&#20010;&#36716;&#25240;&#28857;&#65288;&#25110;&#20020;&#30028;&#28857;&#65289;&#65292;&#26126;&#30830;&#21010;&#20998;&#20102;&#20302;&#12289;&#20013;&#21644;&#39640;&#28201;&#24230;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The logistic regression model is one of the most popular data generation model in noisy binary classification problems. In this work, we study the sample complexity of estimating the parameters of the logistic regression model up to a given $\ell_2$ error, in terms of the dimension and the inverse temperature, with standard normal covariates. The inverse temperature controls the signal-to-noise ratio of the data generation process. While both generalization bounds and asymptotic performance of the maximum-likelihood estimator for logistic regression are well-studied, the non-asymptotic sample complexity that shows the dependence on error and the inverse temperature for parameter estimation is absent from previous analyses. We show that the sample complexity curve has two change-points (or critical points) in terms of the inverse temperature, clearly separating the low, moderate, and high temperature regimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#22270;&#27880;&#24847;&#21147;&#65288;LGA&#65289;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#20687;&#20013;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;LGA&#22312;&#35745;&#31639;&#19978;&#31616;&#27905;&#19988;&#31283;&#23450;&#65292;&#33021;&#22815;&#22312;&#23567;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#20013;&#23454;&#29616;&#25509;&#36817;&#22823;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#36731;&#37327;&#32423;&#20307;&#31995;&#32467;&#26500;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#26356;&#21152;&#23454;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.04149</link><description>&lt;p&gt;
&#22686;&#24378;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#28508;&#22312;&#22270;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Attention for Enhanced Spatial Context. (arXiv:2307.04149v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#22270;&#27880;&#24847;&#21147;&#65288;LGA&#65289;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#20687;&#20013;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;LGA&#22312;&#35745;&#31639;&#19978;&#31616;&#27905;&#19988;&#31283;&#23450;&#65292;&#33021;&#22815;&#22312;&#23567;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#20013;&#23454;&#29616;&#25509;&#36817;&#22823;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#36731;&#37327;&#32423;&#20307;&#31995;&#32467;&#26500;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#26356;&#21152;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20013;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#22312;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#38382;&#39064;&#20013;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#22270;&#27169;&#22411;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#25429;&#25417;&#21040;&#20102;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#35745;&#31639;&#19978;&#27604;&#36739;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#33021;&#23398;&#20064;&#22270;&#20687;&#20013;&#20219;&#24847;&#20004;&#20010;&#28857;&#20043;&#38388;&#30340;&#37197;&#23545;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#22270;&#27880;&#24847;&#21147;&#65288;LGA&#65289;&#30340;&#35745;&#31639;&#31616;&#27905;&#65288;&#19982;&#33410;&#28857;&#25968;&#37327;&#21576;&#32447;&#24615;&#20851;&#31995;&#65289;&#21644;&#31283;&#23450;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20840;&#23616;&#19978;&#19979;&#25991;&#32435;&#20837;&#29616;&#26377;&#20307;&#31995;&#32467;&#26500;&#20013;&#65292;&#29305;&#21035;&#26159;&#22686;&#24378;&#23567;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#36731;&#37327;&#32423;&#20307;&#31995;&#32467;&#26500;&#23545;&#20110;&#35745;&#31639;&#33021;&#21147;&#36739;&#20302;&#21644;&#33021;&#37327;&#38656;&#27714;&#36739;&#20302;&#30340;&#36793;&#32536;&#35774;&#22791;&#26356;&#21152;&#26377;&#29992;&#12290;LGA&#20351;&#29992;&#23616;&#37096;&#36830;&#25509;&#22270;&#32593;&#32476;&#26469;&#22312;&#31354;&#38388;&#19978;&#20256;&#25773;&#20449;&#24687;&#65292;&#20174;&#32780;&#26041;&#20415;&#22320;&#26500;&#24314;&#36828;&#36317;&#31163;&#30340;&#20004;&#20010;&#31354;&#38388;&#28857;&#20043;&#38388;&#30340;&#35821;&#20041;&#19968;&#33268;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global contexts in images are quite valuable in image-to-image translation problems. Conventional attention-based and graph-based models capture the global context to a large extent, however, these are computationally expensive. Moreover, the existing approaches are limited to only learning the pairwise semantic relation between any two points on the image. In this paper, we present Latent Graph Attention (LGA) a computationally inexpensive (linear to the number of nodes) and stable, modular framework for incorporating the global context in the existing architectures, especially empowering small-scale architectures to give performance closer to large size architectures, thus making the light-weight architectures more useful for edge devices with lower compute power and lower energy needs. LGA propagates information spatially using a network of locally connected graphs, thereby facilitating to construct a semantically coherent relation between any two spatially distant points that also 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#24403;&#21069;&#22270;&#34920;&#20998;&#31867;&#25216;&#26415;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#24182;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#24615;&#33021;&#20998;&#26512;&#65292;&#20197;&#25552;&#39640;&#23545;&#22270;&#34920;&#30340;&#33258;&#21160;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.04147</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#22270;&#34920;&#20998;&#31867;&#30340;&#35843;&#26597;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Survey and Approach to Chart Classification. (arXiv:2307.04147v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#24403;&#21069;&#22270;&#34920;&#20998;&#31867;&#25216;&#26415;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#24182;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#24615;&#33021;&#20998;&#26512;&#65292;&#20197;&#25552;&#39640;&#23545;&#22270;&#34920;&#30340;&#33258;&#21160;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#22312;&#25991;&#26723;&#20013;&#34920;&#31034;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#35270;&#35273;&#20449;&#24687;&#28304;&#65292;&#24182;&#20419;&#36827;&#20102;&#23545;&#36890;&#24120;&#20197;&#25968;&#23383;&#24418;&#24335;&#20256;&#36798;&#30340;&#20449;&#24687;&#30340;&#28145;&#20837;&#29702;&#35299;&#21644;&#35299;&#37322;&#12290;&#22312;&#31185;&#23398;&#25991;&#29486;&#20013;&#65292;&#26377;&#35768;&#22810;&#22270;&#34920;&#65292;&#27599;&#20010;&#22270;&#34920;&#37117;&#26377;&#20854;&#39118;&#26684;&#19978;&#30340;&#24046;&#24322;&#12290;&#26368;&#36817;&#65292;&#25991;&#26723;&#29702;&#35299;&#39046;&#22495;&#24320;&#22987;&#35299;&#20915;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#28041;&#21450;&#21040;&#22270;&#34920;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#34920;&#20998;&#31867;&#25216;&#26415;&#30340;&#35843;&#26597;&#65292;&#24182;&#35752;&#35770;&#20102;&#21487;&#29992;&#25968;&#25454;&#38598;&#21450;&#20854;&#25903;&#25345;&#30340;&#22270;&#34920;&#31867;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#36129;&#29486;&#20998;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#20256;&#32479;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;CHARTINFO UB-UNITECH PMC&#25968;&#25454;&#38598;&#19978;&#23545;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#24615;&#33021;&#20998;&#26512;&#65292;&#35813;&#25968;&#25454;&#38598;&#29992;&#20110;ICPR 2022&#30340;CHART-Infographics&#31454;&#36187;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;15&#20010;&#19981;&#21516;&#30340;&#22270;&#34920;&#31867;&#21035;&#65292;&#21253;&#25324;22923&#20010;&#35757;&#32451;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Charts represent an essential source of visual information in documents and facilitate a deep understanding and interpretation of information typically conveyed numerically. In the scientific literature, there are many charts, each with its stylistic differences. Recently the document understanding community has begun to address the problem of automatic chart understanding, which begins with chart classification. In this paper, we present a survey of the current state-of-the-art techniques for chart classification and discuss the available datasets and their supported chart types. We broadly classify these contributions as traditional approaches based on ML, CNN, and Transformers. Furthermore, we carry out an extensive comparative performance analysis of CNN-based and transformer-based approaches on the recently published CHARTINFO UB-UNITECH PMC dataset for the CHART-Infographics competition at ICPR 2022. The data set includes 15 different chart categories, including 22,923 training i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#38543;&#26426;&#24615;&#23545;&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#32676;&#20307;&#20844;&#24179;&#24615;&#25351;&#26631;&#30340;&#26041;&#24046;&#20027;&#35201;&#26469;&#33258;&#20110;&#22312;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#19978;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#39640;&#26131;&#21464;&#24615;&#65292;&#20854;&#20013;&#26368;&#20027;&#35201;&#30340;&#38543;&#26426;&#24615;&#26469;&#28304;&#26159;&#35757;&#32451;&#26399;&#38388;&#25968;&#25454;&#39034;&#24207;&#30340;&#38543;&#26426;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#25968;&#25454;&#39034;&#24207;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#32676;&#20307;&#32423;&#20934;&#30830;&#24615;&#65292;&#32780;&#20960;&#20046;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.04138</link><description>&lt;p&gt;
&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#38543;&#26426;&#24615;&#23545;&#32676;&#20307;&#20844;&#24179;&#24615;&#24433;&#21709;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
On The Impact of Machine Learning Randomness on Group Fairness. (arXiv:2307.04138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#38543;&#26426;&#24615;&#23545;&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#32676;&#20307;&#20844;&#24179;&#24615;&#25351;&#26631;&#30340;&#26041;&#24046;&#20027;&#35201;&#26469;&#33258;&#20110;&#22312;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#19978;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#39640;&#26131;&#21464;&#24615;&#65292;&#20854;&#20013;&#26368;&#20027;&#35201;&#30340;&#38543;&#26426;&#24615;&#26469;&#28304;&#26159;&#35757;&#32451;&#26399;&#38388;&#25968;&#25454;&#39034;&#24207;&#30340;&#38543;&#26426;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#25968;&#25454;&#39034;&#24207;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#32676;&#20307;&#32423;&#20934;&#30830;&#24615;&#65292;&#32780;&#20960;&#20046;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#29992;&#20110;&#34913;&#37327;&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#32479;&#35745;&#25351;&#26631;&#21453;&#26144;&#20102;&#19981;&#21516;&#32676;&#20307;&#31639;&#27861;&#24615;&#33021;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25351;&#26631;&#22312;&#19981;&#21516;&#35757;&#32451;&#23454;&#20363;&#20043;&#38388;&#23384;&#22312;&#24456;&#39640;&#30340;&#26041;&#24046;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#20844;&#24179;&#24615;&#30340;&#32463;&#39564;&#35780;&#20272;&#20013;&#19981;&#21487;&#38752;&#12290;&#26159;&#20160;&#20040;&#23548;&#33268;&#20102;&#36825;&#31181;&#39640;&#26041;&#24046;&#65311;&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#19981;&#21516;&#38543;&#26426;&#22240;&#32032;&#23545;&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#32676;&#20307;&#20844;&#24179;&#24615;&#25351;&#26631;&#30340;&#26041;&#24046;&#26681;&#26893;&#20110;&#22312;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#19978;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#39640;&#26131;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#38543;&#26426;&#24615;&#20027;&#35201;&#26469;&#28304;&#26159;&#35757;&#32451;&#26399;&#38388;&#25968;&#25454;&#39034;&#24207;&#30340;&#38543;&#26426;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#25913;&#21464;&#21333;&#20010;&#26102;&#26399;&#30340;&#25968;&#25454;&#39034;&#24207;&#26469;&#25511;&#21046;&#32676;&#20307;&#32423;&#20934;&#30830;&#24615;&#65288;&#21363;&#27169;&#22411;&#20844;&#24179;&#24615;&#65289;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#25972;&#20307;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical measures for group fairness in machine learning reflect the gap in performance of algorithms across different groups. These measures, however, exhibit a high variance between different training instances, which makes them unreliable for empirical evaluation of fairness. What causes this high variance? We investigate the impact on group fairness of different sources of randomness in training neural networks. We show that the variance in group fairness measures is rooted in the high volatility of the learning process on under-represented groups. Further, we recognize the dominant source of randomness as the stochasticity of data order during training. Based on these findings, we show how one can control group-level accuracy (i.e., model fairness), with high efficiency and negligible impact on the model's overall performance, by simply changing the data order for a single epoch.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#30899;&#25928;&#29575;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65288;CE-NAS&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#24179;&#34913;&#33021;&#25928;&#25277;&#26679;&#21644;&#33021;&#32791;&#35780;&#20272;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#30899;&#21644;&#25628;&#32034;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.04131</link><description>&lt;p&gt;
&#30899;&#25928;&#29575;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Carbon-Efficient Neural Architecture Search. (arXiv:2307.04131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#30899;&#25928;&#29575;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65288;CE-NAS&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#24179;&#34913;&#33021;&#25928;&#25277;&#26679;&#21644;&#33021;&#32791;&#35780;&#20272;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#30899;&#21644;&#25628;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#27169;&#22411;&#35774;&#35745;&#36807;&#31243;&#20013;&#38477;&#20302;&#33021;&#28304;&#25104;&#26412;&#24182;&#25552;&#39640;&#30899;&#25928;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#31216;&#20026;&#30899;&#25928;&#29575;NAS&#65288;CE-NAS&#65289;&#65292;&#23427;&#30001;&#20855;&#26377;&#19981;&#21516;&#33021;&#28304;&#38656;&#27714;&#30340;NAS&#35780;&#20272;&#31639;&#27861;&#12289;&#22810;&#30446;&#26631;&#20248;&#21270;&#22120;&#21644;&#21551;&#21457;&#24335;GPU&#20998;&#37197;&#31574;&#30053;&#32452;&#25104;&#12290;CE-NAS&#26681;&#25454;&#24403;&#21069;&#30340;&#30899;&#25490;&#25918;&#21160;&#24577;&#24179;&#34913;&#20102;&#33021;&#25928;&#25277;&#26679;&#21644;&#33021;&#32791;&#35780;&#20272;&#20219;&#21153;&#12290;&#20351;&#29992;&#26368;&#36817;&#30340;NAS&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#30899;&#36861;&#36394;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#36861;&#36394;&#39537;&#21160;&#27169;&#25311;&#34920;&#26126;CE-NAS&#22312;&#30899;&#21644;&#25628;&#32034;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#19977;&#20010;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a novel approach to neural architecture search (NAS) that aims to reduce energy costs and increase carbon efficiency during the model design process. The proposed framework, called carbon-efficient NAS (CE-NAS), consists of NAS evaluation algorithms with different energy requirements, a multi-objective optimizer, and a heuristic GPU allocation strategy. CE-NAS dynamically balances energy-efficient sampling and energy-consuming evaluation tasks based on current carbon emissions. Using a recent NAS benchmark dataset and two carbon traces, our trace-driven simulations demonstrate that CE-NAS achieves better carbon and search efficiency than the three baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36817;&#20284;&#35299;&#38750;&#32447;&#24615;PDEs&#65292;&#29305;&#21035;&#26159;&#20855;&#26377;&#25903;&#37197;&#24615;&#21452;&#26354;&#29305;&#24615;&#30340;PDEs&#12290;&#35813;&#26694;&#26550;&#22312;&#19981;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#22788;&#29702;&#20914;&#20987;&#25110;&#38388;&#26029;&#65292;&#24182;&#33021;&#22815;&#33258;&#28982;&#22788;&#29702;&#36793;&#30028;&#26465;&#20214;&#12289;&#29109;&#26465;&#20214;&#21644;&#27491;&#21017;&#24615;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.04121</link><description>&lt;p&gt;
&#29992;&#20110;&#27714;&#35299;&#21452;&#26354;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65306;&#31532;&#19968;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Framework for Solving Hyperbolic Partial Differential Equations: Part I. (arXiv:2307.04121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36817;&#20284;&#35299;&#38750;&#32447;&#24615;PDEs&#65292;&#29305;&#21035;&#26159;&#20855;&#26377;&#25903;&#37197;&#24615;&#21452;&#26354;&#29305;&#24615;&#30340;PDEs&#12290;&#35813;&#26694;&#26550;&#22312;&#19981;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#22788;&#29702;&#20914;&#20987;&#25110;&#38388;&#26029;&#65292;&#24182;&#33021;&#22815;&#33258;&#28982;&#22788;&#29702;&#36793;&#30028;&#26465;&#20214;&#12289;&#29109;&#26465;&#20214;&#21644;&#27491;&#21017;&#24615;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#23545;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#35299;&#30340;&#31283;&#20581;&#20934;&#30830;&#30340;&#36817;&#20284;&#12290;&#28982;&#32780;&#65292;&#22312;&#23581;&#35797;&#36817;&#20284;&#20855;&#26377;&#25903;&#37197;&#24615;&#21452;&#26354;&#29305;&#24615;&#30340;PDEs&#26102;&#65292;PINNs&#38754;&#20020;&#20005;&#37325;&#22256;&#38590;&#21644;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#24320;&#21457;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36817;&#20284;&#35299;&#38750;&#32447;&#24615;PDEs&#65292;&#36825;&#20123;PDEs&#21487;&#20197;&#22312;&#19981;&#20855;&#26377;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#20914;&#20987;&#25110;&#38388;&#26029;&#12290;&#35813;&#26694;&#26550;&#21463;&#21040;&#26377;&#38480;&#20803;&#27861;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#22312;&#31163;&#25955;&#21270;&#22495;&#20013;&#27714;&#35299;&#33410;&#28857;&#22788;&#30340;&#35299;&#20540;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#33410;&#28857;&#20540;&#33719;&#24471;&#20840;&#23616;&#23450;&#20041;&#30340;&#35299;&#22330;&#12290;&#22522;&#20110;&#38388;&#26029;Galerkin&#26041;&#27861;&#30340;&#20005;&#26684;&#25968;&#23398;&#22522;&#30784;&#65292;&#35813;&#26694;&#26550;&#33258;&#28982;&#22788;&#29702;&#36793;&#30028;&#26465;&#20214;&#65288;Neumann / Dirichlet&#65289;&#65292;&#29109;&#26465;&#20214;&#21644;&#27491;&#21017;&#24615;&#35201;&#27714;&#12290;&#36827;&#34892;&#20102;&#22810;&#20010;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics informed neural networks (PINNs) have emerged as a powerful tool to provide robust and accurate approximations of solutions to partial differential equations (PDEs). However, PINNs face serious difficulties and challenges when trying to approximate PDEs with dominant hyperbolic character. This research focuses on the development of a physics informed deep learning framework to approximate solutions to nonlinear PDEs that can develop shocks or discontinuities without any a-priori knowledge of the solution or the location of the discontinuities. The work takes motivation from finite element method that solves for solution values at nodes in the discretized domain and use these nodal values to obtain a globally defined solution field. Built on the rigorous mathematical foundations of the discontinuous Galerkin method, the framework naturally handles imposition of boundary conditions (Neumann/Dirichlet), entropy conditions, and regularity requirements. Several numerical experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#21644;&#24230;&#37327;&#27169;&#22359;&#26469;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04114</link><description>&lt;p&gt;
&#12298;FILM:&#22914;&#20309;&#35753;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#21463;&#30410;?&#12299;
&lt;/p&gt;
&lt;p&gt;
FILM: How can Few-Shot Image Classification Benefit from Pre-Trained Language Models?. (arXiv:2307.04114v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#21644;&#24230;&#37327;&#27169;&#22359;&#26469;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#26088;&#22312;&#35757;&#32451;&#33021;&#22815;&#21033;&#29992;&#23569;&#37327;&#26679;&#26412;&#25512;&#24191;&#21040;&#26032;&#31867;&#21035;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#24037;&#20316;&#25552;&#20986;&#21033;&#29992;&#21487;&#35775;&#38382;&#30340;&#31867;&#21035;&#21517;&#31216;&#35821;&#20041;&#20449;&#24687;&#26469;&#22686;&#24378;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#26631;&#20934;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#35270;&#35273;&#21407;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#22120;&#31561;&#29616;&#26377;&#27169;&#22359;&#65292;&#38480;&#21046;&#20102;&#35821;&#20041;&#20449;&#24687;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#12290;&#20026;&#20102;&#35299;&#20915;&#35270;&#35273;&#29305;&#24449;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#23545;&#40784;&#25361;&#25112;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#26694;&#26550;&#30340;&#25991;&#26412;&#20998;&#25903;&#65292;&#24182;&#24341;&#20837;&#20102;&#24230;&#37327;&#27169;&#22359;&#26469;&#25512;&#24191;&#20313;&#24358;&#30456;&#20284;&#24230;&#12290;&#20026;&#20102;&#26356;&#22909;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#25105;&#20204;&#35753;&#24230;&#37327;&#27169;&#22359;&#36866;&#24212;&#19981;&#21516;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;MAML&#36827;&#34892;&#21452;&#23618;&#20248;&#21270;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning aims to train models that can be generalized to novel classes with only a few samples. Recently, a line of works are proposed to enhance few-shot learning with accessible semantic information from class names. However, these works focus on improving existing modules such as visual prototypes and feature extractors of the standard few-shot learning framework. This limits the full potential use of semantic information. In this paper, we propose a novel few-shot learning framework that uses pre-trained language models based on contrastive learning. To address the challenge of alignment between visual features and textual embeddings obtained from text-based pre-trained language model, we carefully design the textual branch of our framework and introduce a metric module to generalize the cosine similarity. For better transferability, we let the metric module adapt to different few-shot tasks and adopt MAML to train the model via bi-level optimization. Moreover, we conduct 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#37096;&#20998;&#35266;&#27979;&#29366;&#24577;&#23398;&#20064;&#26102;&#31354;&#36830;&#32493;&#31070;&#32463;PDE&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#35774;&#35745;&#21644;&#39640;&#25928;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#30340;&#32593;&#26684;&#29420;&#31435;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2307.04110</link><description>&lt;p&gt;
&#20174;&#37096;&#20998;&#35266;&#27979;&#29366;&#24577;&#20013;&#23398;&#20064;&#26102;&#31354;&#36830;&#32493;&#31070;&#32463;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Space-Time Continuous Neural PDEs from Partially Observed States. (arXiv:2307.04110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04110
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#37096;&#20998;&#35266;&#27979;&#29366;&#24577;&#23398;&#20064;&#26102;&#31354;&#36830;&#32493;&#31070;&#32463;PDE&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#35774;&#35745;&#21644;&#39640;&#25928;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#30340;&#32593;&#26684;&#29420;&#31435;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29420;&#31435;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#20855;&#26377;&#22122;&#22768;&#21644;&#37096;&#20998;&#35266;&#27979;&#30340;&#19981;&#35268;&#21017;&#26102;&#31354;&#32593;&#26684;&#19978;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#39640;&#25928;&#27010;&#29575;&#26694;&#26550;&#21644;&#25913;&#36827;&#30340;&#32534;&#30721;&#22120;&#35774;&#35745;&#30340;&#26102;&#31354;&#36830;&#32493;&#28508;&#22312;&#31070;&#32463;PDE&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#32593;&#26684;&#29420;&#31435;&#24615;&#12290;&#28508;&#22312;&#29366;&#24577;&#21160;&#21147;&#23398;&#30001;&#32467;&#21512;&#20102;&#25554;&#20540;&#27861;&#21644;&#32447;&#26041;&#27861;&#30340;PDE&#27169;&#22411;&#25152;&#25511;&#21046;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22343;&#25674;&#21464;&#20998;&#25512;&#29702;&#26469;&#36817;&#20284;&#21518;&#39564;&#20272;&#35745;&#65292;&#24182;&#21033;&#29992;&#22810;&#37325;&#23556;&#20987;&#25216;&#26415;&#26469;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#65292;&#24182;&#26377;&#25928;&#22788;&#29702;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#30340;&#26041;&#27861;&#65292;&#26174;&#31034;&#20102;&#20854;&#25512;&#36827;&#25968;&#25454;&#39537;&#21160;PDE&#24314;&#27169;&#21644;&#23454;&#29616;&#22797;&#26434;&#37096;&#20998;&#35266;&#27979;&#30340;&#32593;&#26684;&#29420;&#31435;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel grid-independent model for learning partial differential equations (PDEs) from noisy and partial observations on irregular spatiotemporal grids. We propose a space-time continuous latent neural PDE model with an efficient probabilistic framework and a novel encoder design for improved data efficiency and grid independence. The latent state dynamics are governed by a PDE model that combines the collocation method and the method of lines. We employ amortized variational inference for approximate posterior estimation and utilize a multiple shooting technique for enhanced training speed and stability. Our model demonstrates state-of-the-art performance on complex synthetic and real-world datasets, overcoming limitations of previous approaches and effectively handling partially-observed data. The proposed model outperforms recent methods, showing its potential to advance data-driven PDE modeling and enabling robust, grid-independent modeling of complex partially-observe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20551;&#35774;&#30340;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#26174;&#31034;&#27495;&#35270;&#21644;&#19981;&#20844;&#24179;&#39044;&#27979;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04105</link><description>&lt;p&gt;
&#26080;&#20551;&#35774;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Assumption-free Bias Mitigation. (arXiv:2307.04105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20551;&#35774;&#30340;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#26174;&#31034;&#27495;&#35270;&#21644;&#19981;&#20844;&#24179;&#39044;&#27979;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23545;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#26174;&#31034;&#20986;&#27495;&#35270;&#65292;&#24182;&#19988;&#23384;&#22312;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#34892;&#20026;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#27495;&#35270;&#65292;&#24191;&#27867;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#22810;&#31181;&#26041;&#27861;&#28040;&#38500;&#25935;&#24863;&#23646;&#24615;&#30340;&#19981;&#24179;&#31561;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#30495;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#26080;&#27861;&#24471;&#21040;&#25110;&#32570;&#23569;&#25935;&#24863;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#29616;&#26377;&#30340;&#24037;&#20316;&#22312;&#26080;&#25935;&#24863;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#20943;&#36731;&#20102;&#20559;&#24046;&#12290;&#36825;&#20123;&#30740;&#31350;&#38754;&#20020;&#25361;&#25112;&#65292;&#35201;&#20040;&#26159;&#23545;&#25935;&#24863;&#23646;&#24615;&#30340;&#39044;&#27979;&#19981;&#20934;&#30830;&#65292;&#35201;&#20040;&#26159;&#38656;&#35201;&#20943;&#36731;&#19982;&#20559;&#24046;&#30456;&#20851;&#30340;&#20154;&#24037;&#23450;&#20041;&#30340;&#38750;&#25935;&#24863;&#23646;&#24615;&#30340;&#19981;&#24179;&#31561;&#20998;&#24067;&#12290;&#21518;&#32773;&#23545;&#25935;&#24863;&#21644;&#38750;&#25935;&#24863;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26377;&#30528;&#24378;&#22823;&#30340;&#20551;&#35774;&#12290;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#21644;&#20219;&#21153;&#30446;&#26631;&#30340;&#24046;&#24322;&#65292;&#23545;&#38750;&#25935;&#24863;&#23646;&#24615;&#30340;&#24378;&#20551;&#35774;&#21487;&#33021;&#26080;&#25928;&#65292;&#24182;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20551;&#35774;&#30340;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive prediction ability, machine learning models show discrimination towards certain demographics and suffer from unfair prediction behaviors. To alleviate the discrimination, extensive studies focus on eliminating the unequal distribution of sensitive attributes via multiple approaches. However, due to privacy concerns, sensitive attributes are often either unavailable or missing in real-world scenarios. Therefore, several existing works alleviate the bias without sensitive attributes. Those studies face challenges, either in inaccurate predictions of sensitive attributes or the need to mitigate unequal distribution of manually defined non-sensitive attributes related to bias. The latter requires strong assumptions about the correlation between sensitive and non-sensitive attributes. As data distribution and task goals vary, the strong assumption on non-sensitive attributes may not be valid and require domain expertise. In this work, we propose an assumption-free fra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#20915;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#26469;&#25551;&#36848;&#26465;&#20214;&#20998;&#24067;&#30340;&#38750;&#21442;&#25968;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#22359;&#19977;&#35282;&#36755;&#36816;&#26144;&#23556;&#23558;&#21442;&#32771;&#26679;&#26412;&#36845;&#20195;&#26144;&#23556;&#21040;&#30446;&#26631;&#26679;&#26412;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21442;&#25968;&#20559;&#24046;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.04102</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#26465;&#20214;&#37319;&#26679;&#30340;&#29983;&#25104;&#27969;
&lt;/p&gt;
&lt;p&gt;
A generative flow for conditional sampling via optimal transport. (arXiv:2307.04102v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#20915;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#26469;&#25551;&#36848;&#26465;&#20214;&#20998;&#24067;&#30340;&#38750;&#21442;&#25968;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#22359;&#19977;&#35282;&#36755;&#36816;&#26144;&#23556;&#23558;&#21442;&#32771;&#26679;&#26412;&#36845;&#20195;&#26144;&#23556;&#21040;&#30446;&#26631;&#26679;&#26412;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21442;&#25968;&#20559;&#24046;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#20998;&#24067;&#30340;&#37319;&#26679;&#26159;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#23494;&#24230;&#20272;&#35745;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#24402;&#19968;&#21270;&#27969;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#31616;&#21333;&#21442;&#32771;&#27169;&#22411;&#65288;&#22914;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#65289;&#25512;&#21521;&#30446;&#26631;&#20998;&#24067;&#30340;&#36755;&#36816;&#26144;&#23556;&#65292;&#26469;&#25551;&#36848;&#26465;&#20214;&#20998;&#24067;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#25104;&#21151;&#22320;&#25551;&#36848;&#20102;&#35768;&#22810;&#38750;&#39640;&#26031;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#36890;&#24120;&#21463;&#21040;&#21442;&#25968;&#20559;&#24046;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#65288;&#23545;&#25239;&#24615;&#65289;&#20248;&#21270;&#22120;&#23398;&#20064;&#36825;&#20123;&#36716;&#25442;&#30340;&#21487;&#38752;&#24615;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#23558;&#21442;&#32771;&#26679;&#26412;&#26144;&#23556;&#21040;&#30446;&#26631;&#26679;&#26412;&#26469;&#25551;&#36848;&#26465;&#20214;&#20998;&#24067;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#22359;&#19977;&#35282;&#36755;&#36816;&#26144;&#23556;&#65292;&#20854;&#32452;&#20214;&#34987;&#35777;&#26126;&#21487;&#20197;&#34920;&#24449;&#30446;&#26631;&#20998;&#24067;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#36825;&#20123;&#26144;&#23556;&#26159;&#36890;&#36807;&#35299;&#20915;&#24102;&#26435; $L^2$ &#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#24471;&#21040;&#30340;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;[Trigila and Tabak, 2016]&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#29992;&#20110;&#26465;&#20214;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling conditional distributions is a fundamental task for Bayesian inference and density estimation. Generative models, such as normalizing flows and generative adversarial networks, characterize conditional distributions by learning a transport map that pushes forward a simple reference (e.g., a standard Gaussian) to a target distribution. While these approaches successfully describe many non-Gaussian problems, their performance is often limited by parametric bias and the reliability of gradient-based (adversarial) optimizers to learn these transformations. This work proposes a non-parametric generative model that iteratively maps reference samples to the target. The model uses block-triangular transport maps, whose components are shown to characterize conditionals of the target distribution. These maps arise from solving an optimal transport problem with a weighted $L^2$ cost function, thereby extending the data-driven approach in [Trigila and Tabak, 2016] for conditional sampling
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#33539;&#25968;&#24809;&#32602;&#23454;&#29616;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25915;&#20987;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#19982;&#20854;&#20182;&#26799;&#24230;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.04099</link><description>&lt;p&gt;
GNP&#25915;&#20987;: &#36890;&#36807;&#26799;&#24230;&#33539;&#25968;&#24809;&#32602;&#23454;&#29616;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
GNP Attack: Transferable Adversarial Examples via Gradient Norm Penalty. (arXiv:2307.04099v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#33539;&#25968;&#24809;&#32602;&#23454;&#29616;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25915;&#20987;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#19982;&#20854;&#20182;&#26799;&#24230;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#33391;&#22909;&#21487;&#36716;&#31227;&#24615;&#30340;&#23545;&#25239;&#26679;&#26412;&#20351;&#24471;&#38024;&#23545;&#19981;&#21516;&#30446;&#26631;&#27169;&#22411;&#30340;&#40657;&#30418;&#25915;&#20987;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#20851;&#20110;&#30446;&#26631;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#29983;&#25104;&#20855;&#26377;&#24456;&#23569;&#25110;&#27809;&#26377;&#21487;&#36716;&#31227;&#24615;&#30340;&#23545;&#25239;&#26679;&#26412;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#23427;&#20204;&#23481;&#26131;&#36807;&#25311;&#21512;&#20110;&#28304;&#27169;&#22411;&#30340;&#29305;&#23450;&#26550;&#26500;&#21644;&#29305;&#24449;&#34920;&#31034;&#65292;&#29983;&#25104;&#30340;&#23545;&#25239;&#26679;&#26412;&#20960;&#20046;&#23545;&#30446;&#26631;&#40657;&#30418;&#27169;&#22411;&#27809;&#26377;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#33539;&#25968;&#24809;&#32602;&#65288;GNP&#65289;&#22686;&#24378;&#23545;&#25239;&#26679;&#26412;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#23427;&#39537;&#20351;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#36807;&#31243;&#25910;&#25947;&#21040;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#24179;&#22374;&#21306;&#22495;&#30340;&#23616;&#37096;&#26368;&#20248;&#28857;&#12290;&#36890;&#36807;&#25915;&#20987;11&#20010;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;6&#31181;&#39640;&#32423;&#38450;&#24481;&#26041;&#27861;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;GNP&#22312;&#29983;&#25104;&#20855;&#26377;&#39640;&#36716;&#31227;&#24615;&#30340;&#23545;&#25239;&#26679;&#26412;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#23427;&#38750;&#24120;&#28789;&#27963;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#20854;&#20182;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples (AE) with good transferability enable practical black-box attacks on diverse target models, where insider knowledge about the target models is not required. Previous methods often generate AE with no or very limited transferability; that is, they easily overfit to the particular architecture and feature representation of the source, white-box model and the generated AE barely work for target, black-box models. In this paper, we propose a novel approach to enhance AE transferability using Gradient Norm Penalty (GNP). It drives the loss function optimization procedure to converge to a flat region of local optima in the loss landscape. By attacking 11 state-of-the-art (SOTA) deep learning models and 6 advanced defense methods, we empirically show that GNP is very effective in generating AE with high transferability. We also demonstrate that it is very flexible in that it can be easily integrated with other gradient based methods for stronger transfer-based attacks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21333;&#31867;&#20998;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#20316;&#32773;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#26144;&#23556;&#65292;&#23558;&#35757;&#32451;&#25968;&#25454;&#30340;&#26410;&#30693;&#20998;&#24067;&#36716;&#21270;&#20026;&#24050;&#30693;&#30446;&#26631;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#23545;&#24322;&#24120;&#25968;&#25454;&#30340;&#21487;&#38752;&#21306;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.04097</link><description>&lt;p&gt;
&#38480;&#21046;&#29983;&#25104;&#25237;&#24433;&#22312;&#21333;&#31867;&#20998;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Restricted Generative Projection for One-Class Classification and Anomaly Detection. (arXiv:2307.04097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04097
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21333;&#31867;&#20998;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#20316;&#32773;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#26144;&#23556;&#65292;&#23558;&#35757;&#32451;&#25968;&#25454;&#30340;&#26410;&#30693;&#20998;&#24067;&#36716;&#21270;&#20026;&#24050;&#30693;&#30446;&#26631;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#23545;&#24322;&#24120;&#25968;&#25454;&#30340;&#21487;&#38752;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#29992;&#20110;&#21333;&#31867;&#20998;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#26144;&#23556;&#26469;&#23558;&#35757;&#32451;&#65288;&#27491;&#24120;&#65289;&#25968;&#25454;&#30340;&#26410;&#30693;&#20998;&#24067;&#36716;&#21270;&#20026;&#24050;&#30693;&#30446;&#26631;&#20998;&#24067;&#12290;&#30446;&#26631;&#20998;&#24067;&#24212;&#35813;&#36275;&#22815;&#31616;&#21333;&#12289;&#32039;&#20945;&#21644;&#20449;&#24687;&#20016;&#23500;&#12290;&#31616;&#27905;&#24615;&#26159;&#20026;&#20102;&#30830;&#20445;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#22320;&#20174;&#20998;&#24067;&#20013;&#36827;&#34892;&#21462;&#26679;&#65292;&#32039;&#20945;&#24615;&#26159;&#20026;&#20102;&#30830;&#20445;&#27491;&#24120;&#25968;&#25454;&#21644;&#24322;&#24120;&#25968;&#25454;&#20043;&#38388;&#30340;&#20915;&#31574;&#36793;&#30028;&#28165;&#26224;&#21487;&#38752;&#65292;&#20449;&#24687;&#20016;&#23500;&#24615;&#26159;&#20026;&#20102;&#30830;&#20445;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#12289;&#36229;&#29699;&#38754;&#19978;&#30340;&#22343;&#21248;&#20998;&#24067;&#12289;&#36229;&#29699;&#38754;&#20043;&#38388;&#30340;&#22343;&#21248;&#20998;&#24067;&#20316;&#20026;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#21516;&#26102;&#20351;&#24471;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#20998;&#24067;&#19982;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#30340;&#37325;&#24314;&#35823;&#24046;&#36275;&#22815;&#23567;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple framework for one-class classification and anomaly detection. The core idea is to learn a mapping to transform the unknown distribution of training (normal) data to a known target distribution. Crucially, the target distribution should be sufficiently simple, compact, and informative. The simplicity is to ensure that we can sample from the distribution easily, the compactness is to ensure that the decision boundary between normal data and abnormal data is clear and reliable, and the informativeness is to ensure that the transformed data preserve the important information of the original data. Therefore, we propose to use truncated Gaussian, uniform in hypersphere, uniform on hypersphere, or uniform between hyperspheres, as the target distribution. We then minimize the distance between the transformed data distribution and the target distribution while keeping the reconstruction error for the original data small enough. Comparative studies on multiple benchmark datas
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#23558;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#19982;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#30456;&#32467;&#21512;&#30340;&#31867;&#21035;&#22686;&#37327;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#22320;&#22312;&#28145;&#24230;&#29305;&#24449;&#25552;&#21462;&#22120;&#19979;&#36827;&#34892;&#20102;&#32852;&#21512;&#20248;&#21270;&#21644;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2307.04094</link><description>&lt;p&gt;
&#28145;&#24230;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#22686;&#37327;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Mixture of Gaussians for Deep Continual Learning. (arXiv:2307.04094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04094
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#23558;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#19982;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#30456;&#32467;&#21512;&#30340;&#31867;&#21035;&#22686;&#37327;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#22320;&#22312;&#28145;&#24230;&#29305;&#24449;&#25552;&#21462;&#22120;&#19979;&#36827;&#34892;&#20102;&#32852;&#21512;&#20248;&#21270;&#21644;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#38745;&#24577;&#25968;&#25454;&#30340;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#27880;&#37325;&#20197;&#39034;&#24207;&#26041;&#24335;&#23398;&#20064;&#21644;&#20445;&#30041;&#21040;&#36798;&#30340;&#27010;&#24565;&#12290;&#22312;&#26368;&#36890;&#29992;&#30340;&#31867;&#21035;&#22686;&#37327;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#24517;&#39035;&#20934;&#22791;&#22909;&#22788;&#29702;&#19968;&#20010;&#25509;&#19968;&#20010;&#21040;&#26469;&#30340;&#31867;&#21035;&#65292;&#32780;&#27809;&#26377;&#20219;&#20309;&#26356;&#39640;&#32423;&#21035;&#30340;&#20998;&#32452;&#12290;&#36825;&#20010;&#35201;&#27714;&#20351;&#24471;&#20043;&#21069;&#25552;&#20986;&#30340;&#35768;&#22810;&#26041;&#27861;&#26080;&#25928;&#65292;&#24182;&#36843;&#20351;&#30740;&#31350;&#32773;&#23547;&#25214;&#26356;&#28789;&#27963;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36981;&#24490;&#20197;&#36136;&#24515;&#39537;&#21160;&#30340;&#26041;&#27861;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#23558;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#23436;&#20840;&#25972;&#21512;&#21040;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#24819;&#27861;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#21644;&#35774;&#35745;&#33021;&#22815;&#23398;&#20064;&#26377;&#21306;&#20998;&#24615;&#29305;&#24449;&#24182;&#36991;&#20813;&#36864;&#21270;&#35299;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#28151;&#21512;&#27169;&#22411;&#19982;&#28145;&#24230;&#29305;&#24449;&#25552;&#21462;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#22312;&#28508;&#31354;&#38388;&#20013;&#30340;&#32852;&#21512;&#20248;&#21270;&#21644;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#22266;&#23450;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#19979;&#26377;&#25928;&#22320;&#22312;&#26080;&#20869;&#23384;&#30340;&#22330;&#26223;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning models for stationary data focus on learning and retaining concepts coming to them in a sequential manner. In the most generic class-incremental environment, we have to be ready to deal with classes coming one by one, without any higher-level grouping. This requirement invalidates many previously proposed methods and forces researchers to look for more flexible alternative approaches. In this work, we follow the idea of centroid-driven methods and propose end-to-end incorporation of the mixture of Gaussians model into the continual learning framework. By employing the gradient-based approach and designing losses capable of learning discriminative features while avoiding degenerate solutions, we successfully combine the mixture model with a deep feature extractor allowing for joint optimization and adjustments in the latent space. Additionally, we show that our model can effectively learn in memory-free scenarios with fixed extractors. In the conducted experiments, we
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26597;&#35810;&#27491;&#30830;&#23398;&#20064;&#20915;&#31574;&#26641;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;NP&#38590;&#38382;&#39064;&#65292;&#36825;&#23601;&#22635;&#34917;&#20102;&#23398;&#20064;&#29702;&#35770;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#19968;&#20010;&#31354;&#30333;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21270;&#21644;&#21152;&#24378;&#20915;&#31574;&#26641;&#26368;&#23567;&#21270;&#38382;&#39064;&#19979;&#30028;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.04093</link><description>&lt;p&gt;
&#36890;&#36807;&#26597;&#35810;&#27491;&#30830;&#23398;&#20064;&#20915;&#31574;&#26641;&#26159;NP&#38590;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Properly Learning Decision Trees with Queries Is NP-Hard. (arXiv:2307.04093v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04093
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26597;&#35810;&#27491;&#30830;&#23398;&#20064;&#20915;&#31574;&#26641;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;NP&#38590;&#38382;&#39064;&#65292;&#36825;&#23601;&#22635;&#34917;&#20102;&#23398;&#20064;&#29702;&#35770;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#19968;&#20010;&#31354;&#30333;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21270;&#21644;&#21152;&#24378;&#20915;&#31574;&#26641;&#26368;&#23567;&#21270;&#38382;&#39064;&#19979;&#30028;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#26597;&#35810;&#27491;&#30830;&#23398;&#20064;&#20915;&#31574;&#26641;&#26159;&#19968;&#20010;NP&#38590;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#29702;&#35770;&#20013;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#20110;&#20915;&#31574;&#26641;&#22797;&#26434;&#24615;&#30340;&#22256;&#38590;&#33976;&#39311;&#27010;&#24565;&#36827;&#34892;&#20102;&#24341;&#20837;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#30830;&#23450;&#23548;&#33268;&#22797;&#26434;&#24615;&#30340;&#19968;&#20010;&#23567;&#36755;&#20837;&#38598;&#21512;&#65292;&#20174;&#32780;&#31616;&#21270;&#21644;&#21152;&#24378;&#20102;&#23545;&#20915;&#31574;&#26641;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove that it is NP-hard to properly PAC learn decision trees with queries, resolving a longstanding open problem in learning theory (Bshouty 1993; Guijarro-Lavin-Raghavan 1999; Mehta-Raghavan 2002; Feldman 2016). While there has been a long line of work, dating back to (Pitt-Valiant 1988), establishing the hardness of properly learning decision trees from random examples, the more challenging setting of query learners necessitates different techniques and there were no previous lower bounds. En route to our main result, we simplify and strengthen the best known lower bounds for a different problem of Decision Tree Minimization (Zantema-Bodlaender 2000; Sieling 2003).  On a technical level, we introduce the notion of hardness distillation, which we study for decision tree complexity but can be considered for any complexity measure: for a function that requires large decision trees, we give a general method for identifying a small set of inputs that is responsible for its complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#65292;&#26377;&#25928;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#20013;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#24050;&#26377;&#25968;&#25454;&#38598;DebateSum&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20363;&#23376;&#21644;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#65292;&#21019;&#24314;&#21644;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#36866;&#21512;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.04090</link><description>&lt;p&gt;
DebateKG: &#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs. (arXiv:2307.04090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#65292;&#26377;&#25928;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#20013;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#24050;&#26377;&#25968;&#25454;&#38598;DebateSum&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20363;&#23376;&#21644;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#65292;&#21019;&#24314;&#21644;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#36866;&#21512;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30456;&#20851;&#24037;&#20316;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#22312;&#35299;&#20915;&#31454;&#36187;&#36777;&#35770;&#20013;&#30340;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#24615;&#12290;&#31454;&#36187;&#36777;&#35770;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#36777;&#25163;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#26500;&#24314;&#26377;&#25928;&#30340;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21517;&#20026;DebateSum&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#36825;&#31181;&#28508;&#21147;&#65292;&#35813;&#25968;&#25454;&#38598;&#38024;&#23545;&#30340;&#26159;&#19968;&#31181;&#21517;&#20026;&#25919;&#31574;&#36777;&#35770;&#30340;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#31867;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#21521;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;53180&#20010;&#26032;&#30340;&#20363;&#23376;&#65292;&#24182;&#20026;&#27599;&#20010;&#20363;&#23376;&#25552;&#20379;&#36827;&#19968;&#27493;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;DebateSum&#12290;&#25105;&#20204;&#21033;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#20135;&#29983;&#24182;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#22312;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work within the Argument Mining community has shown the applicability of Natural Language Processing systems for solving problems found within competitive debate. One of the most important tasks within competitive debate is for debaters to create high quality debate cases. We show that effective debate cases can be constructed using constrained shortest path traversals on Argumentative Semantic Knowledge Graphs. We study this potential in the context of a type of American Competitive Debate, called Policy Debate, which already has a large scale dataset targeting it called DebateSum. We significantly improve upon DebateSum by introducing 53180 new examples, as well as further useful metadata for every example, to the dataset. We leverage the txtai semantic search and knowledge graph toolchain to produce and contribute 9 semantic knowledge graphs built on this dataset. We create a unique method for evaluating which knowledge graphs are better in the context of producing policy deb
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26657;&#20934;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#26041;&#27861;&#25913;&#36827;&#22522;&#20110;&#35780;&#20998;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#20998;&#31867;&#22120;&#20316;&#20026;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#65292;&#24182;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#26657;&#20934;&#20998;&#31867;&#22120;&#65292;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04081</link><description>&lt;p&gt;
&#33258;&#26657;&#20934;&#20998;&#31867;&#22120;&#24341;&#23548;&#19979;&#65292;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#22522;&#20110;&#35780;&#20998;&#30340;&#26465;&#20214;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance. (arXiv:2307.04081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26657;&#20934;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#26041;&#27861;&#25913;&#36827;&#22522;&#20110;&#35780;&#20998;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#20998;&#31867;&#22120;&#20316;&#20026;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#65292;&#24182;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#26657;&#20934;&#20998;&#31867;&#22120;&#65292;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#65292;&#33021;&#22815;&#36798;&#21040;&#39046;&#20808;&#30340;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#24050;&#32463;&#23558;SGMs&#25193;&#23637;&#21040;&#22788;&#29702;&#31867;&#26465;&#20214;&#29983;&#25104;&#65292;&#36890;&#36807;&#23558;&#26080;&#26465;&#20214;&#30340;SGM&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30340;&#24341;&#23548;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;SGMs&#22312;&#35757;&#32451;&#25968;&#37327;&#36739;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#26102;&#24182;&#19981;&#24635;&#33021;&#23454;&#29616;&#20934;&#30830;&#30340;&#26465;&#20214;&#29983;&#25104;&#12290;&#25105;&#20204;&#35748;&#20026;&#38382;&#39064;&#26681;&#28304;&#22312;&#20110;&#20998;&#31867;&#22120;&#30340;&#19981;&#21487;&#38752;&#26799;&#24230;&#21644;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#35753;&#20998;&#31867;&#22120;&#33258;&#26657;&#20934;&#26469;&#25913;&#36827;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;SGMs&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#33021;&#37327;&#27169;&#22411;&#30340;&#21407;&#21017;&#23558;&#20998;&#31867;&#22120;&#36716;&#21270;&#20026;&#26080;&#26465;&#20214;SGM&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#12290;&#28982;&#21518;&#65292;&#21487;&#20197;&#37319;&#29992;&#29616;&#26377;&#30340;&#26080;&#26465;&#20214;SGM&#25439;&#22833;&#20989;&#25968;&#26469;&#20351;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#26657;&#20934;&#20998;&#31867;&#22120;&#12290;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#26465;&#20214;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based Generative Models (SGMs) are a popular family of deep generative models that achieves leading image generation quality. Earlier studies have extended SGMs to tackle class-conditional generation by coupling an unconditional SGM with the guidance of a trained classifier. Nevertheless, such classifier-guided SGMs do not always achieve accurate conditional generation, especially when trained with fewer labeled data. We argue that the issue is rooted in unreliable gradients of the classifier and the inability to fully utilize unlabeled data during training. We then propose to improve classifier-guided SGMs by letting the classifier calibrate itself. Our key idea is to use principles from energy-based models to convert the classifier as another view of the unconditional SGM. Then, existing loss for the unconditional SGM can be adopted to calibrate the classifier using both labeled and unlabeled data. Empirical results validate that the proposed approach significantly improves the
&lt;/p&gt;</description></item><item><title>&#31169;&#23494;&#25512;&#29702;&#26159;&#19968;&#31181;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#21508;&#31181;&#20445;&#25252;&#38544;&#31169;&#35745;&#31639;&#30340;&#24320;&#38144;&#38382;&#39064;&#65292;&#23454;&#29616;&#29992;&#25143;&#25968;&#25454;&#30340;&#26426;&#23494;&#24615;&#21644;&#25511;&#21046;&#24615;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2307.04077</link><description>&lt;p&gt;
&#36808;&#21521;&#24555;&#36895;&#21487;&#25193;&#23637;&#30340;&#31169;&#23494;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Fast and Scalable Private Inference. (arXiv:2307.04077v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04077
&lt;/p&gt;
&lt;p&gt;
&#31169;&#23494;&#25512;&#29702;&#26159;&#19968;&#31181;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#21508;&#31181;&#20445;&#25252;&#38544;&#31169;&#35745;&#31639;&#30340;&#24320;&#38144;&#38382;&#39064;&#65292;&#23454;&#29616;&#29992;&#25143;&#25968;&#25454;&#30340;&#26426;&#23494;&#24615;&#21644;&#25511;&#21046;&#24615;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#21644;&#23433;&#20840;&#36805;&#36895;&#25104;&#20026;&#20102;&#39318;&#35201;&#30340;&#35774;&#35745;&#38480;&#21046;&#26465;&#20214;&#12290;&#29616;&#22312;&#29992;&#25143;&#35201;&#27714;&#22312;&#35841;&#21487;&#20197;&#30475;&#21040;&#20182;&#20204;&#30340;&#25968;&#25454;&#65288;&#26426;&#23494;&#24615;&#65289;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#23427;&#65288;&#25511;&#21046;&#65289;&#26041;&#38754;&#33719;&#24471;&#26356;&#22810;&#30340;&#20445;&#25252;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#23433;&#20840;&#30340;&#21152;&#23494;&#25216;&#26415;&#19981;&#22815;&#23436;&#21892;&#65306;&#23427;&#20204;&#22312;&#23384;&#20648;&#25110;&#36890;&#20449;&#26102;&#20445;&#25252;&#25968;&#25454;&#65292;&#20294;&#24517;&#39035;&#35299;&#23494;&#26469;&#36827;&#34892;&#35745;&#31639;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#23384;&#22312;&#19968;&#31181;&#34987;&#31216;&#20026;&#20445;&#25252;&#38544;&#31169;&#35745;&#31639;&#65288;PPC&#65289;&#30340;&#26032;&#35745;&#31639;&#33539;&#24335;&#12290;&#26032;&#20852;&#30340;PPC&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#23433;&#20840;&#30340;&#22806;&#21253;&#35745;&#31639;&#65292;&#25110;&#20351;&#20004;&#20010;&#21442;&#19982;&#26041;&#33021;&#22815;&#36827;&#34892;&#35745;&#31639;&#32780;&#19981;&#27844;&#38706;&#29992;&#25143;&#30340;&#31192;&#23494;&#25968;&#25454;&#12290;&#23613;&#31649;&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#36825;&#20123;&#25216;&#26415;&#20855;&#26377;&#38761;&#21629;&#24615;&#30340;&#28508;&#21147;&#26469;&#25913;&#21464;&#29992;&#25143;&#30340;&#20445;&#25252;&#65292;&#20294;&#30001;&#20110;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#23384;&#20648;&#24320;&#38144;&#36807;&#39640;&#65292;&#23454;&#29616;&#19968;&#30452;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#26368;&#36817;&#20851;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31169;&#23494;&#25512;&#29702;&#65288;PI&#65289;&#26469;&#35299;&#20915;&#21508;&#31181;PPC&#24320;&#38144;&#30340;&#21162;&#21147;&#12290;&#39318;&#20808;&#65292;&#20171;&#32461;&#20102;&#38382;&#39064;&#21644;&#21508;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
Privacy and security have rapidly emerged as first order design constraints. Users now demand more protection over who can see their data (confidentiality) as well as how it is used (control). Here, existing cryptographic techniques for security fall short: they secure data when stored or communicated but must decrypt it for computation. Fortunately, a new paradigm of computing exists, which we refer to as privacy-preserving computation (PPC). Emerging PPC technologies can be leveraged for secure outsourced computation or to enable two parties to compute without revealing either users' secret data. Despite their phenomenal potential to revolutionize user protection in the digital age, the realization has been limited due to exorbitant computational, communication, and storage overheads.  This paper reviews recent efforts on addressing various PPC overheads using private inference (PI) in neural network as a motivating application. First, the problem and various technologies, including 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#24182;&#35782;&#21035;&#30284;&#30151;&#26032;&#20122;&#22411;&#12290;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#24182;&#36827;&#34892;&#20122;&#22411;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2307.04075</link><description>&lt;p&gt;
&#22522;&#20110;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#30284;&#30151;&#26032;&#20122;&#22411;&#21644;&#27835;&#30103;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data. (arXiv:2307.04075v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#24182;&#35782;&#21035;&#30284;&#30151;&#26032;&#20122;&#22411;&#12290;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#24182;&#36827;&#34892;&#20122;&#22411;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30284;&#30151;&#30340;&#39640;&#24322;&#36136;&#24615;&#21644;&#20020;&#24202;&#29305;&#24449;&#65292;&#19981;&#21516;&#30284;&#30151;&#20122;&#22411;&#20043;&#38388;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#21644;&#20020;&#24202;&#29305;&#24449;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#30284;&#30151;&#20122;&#22411;&#30340;&#35782;&#21035;&#21644;&#21457;&#29616;&#23545;&#20110;&#30284;&#30151;&#30340;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#39044;&#21518;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#65292;&#20174;&#32780;&#35782;&#21035;&#21644;&#34920;&#24449;&#30284;&#30151;&#20122;&#22411;&#12290;AMUCL&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#21644;&#32858;&#31867;&#65292;&#24182;&#35782;&#21035;&#26032;&#30340;&#30284;&#30151;&#20122;&#22411;&#12290;&#36825;&#31181;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#31354;&#38388;&#20013;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#32858;&#31867;&#20122;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the high heterogeneity and clinical characteristics of cancer, there are significant differences in multi-omics data and clinical features among subtypes of different cancers. Therefore, the identification and discovery of cancer subtypes are crucial for the diagnosis, treatment, and prognosis of cancer. In this study, we proposed a generalization framework based on attention mechanisms for unsupervised contrastive learning (AMUCL) to analyze cancer multi-omics data for the identification and characterization of cancer subtypes. AMUCL framework includes a unsupervised multi-head attention mechanism, which deeply extracts multi-omics data features. Importantly, a decoupled contrastive learning model (DMACL) based on a multi-head attention mechanism is proposed to learn multi-omics data features and clusters and identify new cancer subtypes. This unsupervised contrastive learning method clusters subtypes by calculating the similarity between samples in the feature space and sample
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#30340;&#38750;&#20984;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#28436;&#21270;&#32593;&#32476;&#36755;&#20986;&#20998;&#24067;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#22312;&#36229;&#39640;&#32500;&#24230;&#26223;&#35266;&#20013;&#30340;&#26377;&#25928;&#25628;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20215;&#27425;&#25968;&#19979;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.04065</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#39640;&#32500;&#38750;&#20984;&#26223;&#35266;&#30340;&#22823;&#35268;&#27169;&#20840;&#29699;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large-scale global optimization of ultra-high dimensional non-convex landscapes based on generative neural networks. (arXiv:2307.04065v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#30340;&#38750;&#20984;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#28436;&#21270;&#32593;&#32476;&#36755;&#20986;&#20998;&#24067;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#22312;&#36229;&#39640;&#32500;&#24230;&#26223;&#35266;&#20013;&#30340;&#26377;&#25928;&#25628;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20215;&#27425;&#25968;&#19979;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#35757;&#32451;&#30340;&#38750;&#20984;&#20248;&#21270;&#31639;&#27861;&#20803;&#21551;&#21457;&#24335;&#65292;&#33021;&#22815;&#22312;&#36830;&#32493;&#30340;&#36229;&#39640;&#32500;&#26223;&#35266;&#20013;&#36827;&#34892;&#26377;&#25928;&#25628;&#32034;&#12290;&#22312;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21033;&#29992;&#37319;&#26679;&#30340;&#23616;&#37096;&#26799;&#24230;&#32676;&#20307;&#65292;&#22312;&#23450;&#21046;&#25439;&#22833;&#20989;&#25968;&#20013;&#28436;&#21270;&#32593;&#32476;&#36755;&#20986;&#20998;&#24067;&#20989;&#25968;&#65292;&#20351;&#20854;&#36235;&#21521;&#20110;&#39640;&#24615;&#33021;&#26368;&#20248;&#28857;&#12290;&#28145;&#24230;&#32593;&#32476;&#32467;&#26500;&#32463;&#36807;&#23450;&#21046;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#27493;&#22686;&#38271;&#65292;&#20174;&#32780;&#20351;&#31639;&#27861;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#26223;&#35266;&#20013;&#30340;&#32500;&#24230;&#28798;&#38590;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#27010;&#24565;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#26631;&#20934;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#32500;&#24230;&#39640;&#36798;&#19968;&#21315;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20215;&#27425;&#25968;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#28145;&#24230;&#32593;&#32476;&#36807;&#21442;&#25968;&#21270;&#12289;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#21644;&#36866;&#24403;&#30340;&#32593;&#32476;&#26550;&#26500;&#36873;&#25321;&#22312;&#20248;&#21270;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a non-convex optimization algorithm metaheuristic, based on the training of a deep generative network, which enables effective searching within continuous, ultra-high dimensional landscapes. During network training, populations of sampled local gradients are utilized within a customized loss function to evolve the network output distribution function towards one peak at high-performing optima. The deep network architecture is tailored to support progressive growth over the course of training, which allows the algorithm to manage the curse of dimensionality characteristic of high-dimensional landscapes. We apply our concept to a range of standard optimization problems with dimensions as high as one thousand and show that our method performs better with fewer function evaluations compared to state-of-the-art algorithm benchmarks. We also discuss the role of deep network over-parameterization, loss function engineering, and proper network architecture selection in optimization,
&lt;/p&gt;</description></item><item><title>&#21452;&#21521;&#27880;&#24847;&#21147;&#27169;&#22411;&#20855;&#26377;&#28151;&#21512;&#19987;&#23478;&#26435;&#37325;&#65292;&#31867;&#20284;&#20110;&#36830;&#32493;&#35789;&#34955;&#27169;&#22411;&#65288;CBOW&#65289;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.04057</link><description>&lt;p&gt;
&#21452;&#21521;&#27880;&#24847;&#21147;&#20316;&#20026;&#36830;&#32493;&#35789;&#19987;&#23478;&#30340;&#28151;&#21512;&#29289;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Attention as a Mixture of Continuous Word Experts. (arXiv:2307.04057v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04057
&lt;/p&gt;
&lt;p&gt;
&#21452;&#21521;&#27880;&#24847;&#21147;&#27169;&#22411;&#20855;&#26377;&#28151;&#21512;&#19987;&#23478;&#26435;&#37325;&#65292;&#31867;&#20284;&#20110;&#36830;&#32493;&#35789;&#34955;&#27169;&#22411;&#65288;CBOW&#65289;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#21521;&#27880;&#24847;&#21147;&#30001;&#20301;&#32622;&#32534;&#30721;&#21644;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#30446;&#26631;&#32452;&#25104;&#30340;&#33258;&#27880;&#24847;&#21147;&#26500;&#25104;&#65292;&#24050;&#25104;&#20026;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#23613;&#31649;&#23427;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#23427;&#30340;&#32479;&#35745;&#22522;&#30784;&#65306;&#21452;&#21521;&#27880;&#24847;&#21147;&#38544;&#21547;&#22320;&#25311;&#21512;&#20102;&#20160;&#20040;&#32479;&#35745;&#27169;&#22411;&#65311;&#23427;&#19982;&#38750;&#27880;&#24847;&#26426;&#21046;&#30340;&#20808;&#39537;&#26377;&#20309;&#19981;&#21516;&#65311;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#21518;&#65292;&#25311;&#21512;&#21333;&#23618;&#21333;&#22836;&#21452;&#21521;&#27880;&#24847;&#21147;&#31561;&#20110;&#25311;&#21512;&#20855;&#26377;&#28151;&#21512;&#19987;&#23478;&#26435;&#37325;&#30340;&#36830;&#32493;&#35789;&#34955;&#65288;CBOW&#65289;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#22810;&#20010;&#22836;&#21644;&#22810;&#20010;&#23618;&#30340;&#21452;&#21521;&#27880;&#24847;&#21147;&#31561;&#20215;&#20110;&#22534;&#21472;&#30340;MoEs&#21644;MoEs&#30340;&#28151;&#21512;&#12290;&#36825;&#20010;&#32479;&#35745;&#35266;&#28857;&#25581;&#31034;&#20102;MoE&#22312;&#21452;&#21521;&#27880;&#24847;&#21147;&#20013;&#30340;&#29420;&#29305;&#29992;&#36884;&#65292;&#36825;&#19982;&#20854;&#22312;&#22788;&#29702;&#24322;&#26500;&#24615;&#26041;&#38754;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bidirectional attention $\unicode{x2013}$ composed of self-attention with positional encodings and the masked language model (MLM) objective $\unicode{x2013}$ has emerged as a key component of modern large language models (LLMs). Despite its empirical success, few studies have examined its statistical underpinnings: What statistical model is bidirectional attention implicitly fitting? What sets it apart from its non-attention predecessors? We explore these questions in this paper. The key observation is that fitting a single-layer single-head bidirectional attention, upon reparameterization, is equivalent to fitting a continuous bag of words (CBOW) model with mixture-of-experts (MoE) weights. Further, bidirectional attention with multiple heads and multiple layers is equivalent to stacked MoEs and a mixture of MoEs, respectively. This statistical viewpoint reveals the distinct use of MoE in bidirectional attention, which aligns with its practical effectiveness in handling heterogeneous
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31867;&#31216;&#20026;&#27969;&#24418;&#28388;&#27874;-&#32452;&#21512;&#32593;&#32476;&#30340;&#22823;&#22411;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#22270;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#31181;&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#21040;&#36830;&#32493;&#26497;&#38480;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#19981;&#20381;&#36182;&#20110;&#28388;&#27874;&#22120;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.04056</link><description>&lt;p&gt;
&#27969;&#24418;&#28388;&#27874;-&#32452;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Manifold Filter-Combine Networks. (arXiv:2307.04056v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04056
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31867;&#31216;&#20026;&#27969;&#24418;&#28388;&#27874;-&#32452;&#21512;&#32593;&#32476;&#30340;&#22823;&#22411;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#22270;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#31181;&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#21040;&#36830;&#32493;&#26497;&#38480;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#19981;&#20381;&#36182;&#20110;&#28388;&#27874;&#22120;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31867;&#22823;&#22411;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;(MNNs)&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#27969;&#24418;&#28388;&#27874;-&#32452;&#21512;&#32593;&#32476;&#12290;&#36825;&#20010;&#31867;&#21035;&#21253;&#25324;&#20102;Wang&#12289;Ruiz&#21644;Ribeiro&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#32771;&#34385;&#30340;MNNs&#65292;&#27969;&#24418;&#25955;&#23556;&#21464;&#25442;(&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;)&#65292;&#20197;&#21450;&#20854;&#20182;&#26377;&#36259;&#30340;&#20043;&#21069;&#22312;&#25991;&#29486;&#20013;&#26410;&#32771;&#34385;&#30340;&#31034;&#20363;&#65292;&#22914;Kipf&#21644;Welling&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#27969;&#24418;&#31561;&#25928;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#22522;&#20110;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#23545;&#27969;&#24418;&#26377;&#20840;&#23616;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36825;&#26679;&#30340;&#32593;&#32476;&#65292;&#32780;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;&#28857;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#32593;&#32476;&#22312;&#26679;&#26412;&#28857;&#25968;&#36235;&#20110;&#26080;&#31351;&#22823;&#26102;&#33021;&#22815;&#20445;&#35777;&#25910;&#25947;&#21040;&#20854;&#36830;&#32493;&#26497;&#38480;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;(&#20027;&#35201;&#20851;&#27880;&#29305;&#23450;&#30340;MNN&#32467;&#26500;&#21644;&#22270;&#26500;&#24314;)&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#19981;&#20381;&#36182;&#20110;&#20351;&#29992;&#30340;&#28388;&#27874;&#22120;&#25968;&#37327;&#12290;&#32780;&#19988;&#65292;&#23427;&#34920;&#29616;&#20986;&#32447;&#24615;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a large class of manifold neural networks (MNNs) which we call Manifold Filter-Combine Networks. This class includes as special cases, the MNNs considered in previous work by Wang, Ruiz, and Ribeiro, the manifold scattering transform (a wavelet-based model of neural networks), and other interesting examples not previously considered in the literature such as the manifold equivalent of Kipf and Welling's graph convolutional network. We then consider a method, based on building a data-driven graph, for implementing such networks when one does not have global knowledge of the manifold, but merely has access to finitely many sample points. We provide sufficient conditions for the network to provably converge to its continuum limit as the number of sample points tends to infinity. Unlike previous work (which focused on specific MNN architectures and graph constructions), our rate of convergence does not explicitly depend on the number of filters used. Moreover, it exhibits line
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#23558;&#20080;&#23478;&#30340;&#31574;&#30053;&#34892;&#20026;&#32435;&#20837;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#21334;&#26041;&#30340;&#32047;&#35745;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.04055</link><description>&lt;p&gt;
&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
Contextual Dynamic Pricing with Strategic Buyers. (arXiv:2307.04055v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#23558;&#20080;&#23478;&#30340;&#31574;&#30053;&#34892;&#20026;&#32435;&#20837;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#21334;&#26041;&#30340;&#32047;&#35745;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#23450;&#20215;&#26159;&#20225;&#19994;&#24120;&#29992;&#30340;&#19968;&#31181;&#38024;&#23545;&#20010;&#20307;&#29305;&#24449;&#21046;&#23450;&#20215;&#26684;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#20080;&#23478;&#20063;&#21487;&#20197;&#36890;&#36807;&#25805;&#32437;&#29305;&#24449;&#25968;&#25454;&#26469;&#33719;&#21462;&#26356;&#20302;&#30340;&#20215;&#26684;&#65292;&#20294;&#36825;&#20063;&#20250;&#23548;&#33268;&#29305;&#23450;&#30340;&#25805;&#20316;&#25104;&#26412;&#12290;&#36825;&#31181;&#31574;&#30053;&#34892;&#20026;&#21487;&#33021;&#20250;&#38459;&#30861;&#20225;&#19994;&#26368;&#22823;&#21270;&#21033;&#28070;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#12290;&#21334;&#26041;&#26080;&#27861;&#35266;&#23519;&#21040;&#20080;&#23478;&#30340;&#30495;&#23454;&#29305;&#24449;&#65292;&#32780;&#21482;&#33021;&#35266;&#23519;&#21040;&#20080;&#23478;&#26681;&#25454;&#31574;&#30053;&#34892;&#20026;&#25805;&#32437;&#21518;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#21334;&#26041;&#21482;&#33021;&#35266;&#23519;&#21040;&#20080;&#23478;&#23545;&#20135;&#21697;&#30340;&#20272;&#20540;&#65292;&#32780;&#26080;&#27861;&#30452;&#25509;&#33719;&#21462;&#20855;&#20307;&#25968;&#20540;&#65292;&#21482;&#33021;&#24471;&#21040;&#19968;&#20010;&#20108;&#36827;&#21046;&#30340;&#21709;&#24212;&#65292;&#34920;&#31034;&#26159;&#21542;&#21457;&#29983;&#38144;&#21806;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#23558;&#20080;&#23478;&#30340;&#31574;&#30053;&#34892;&#20026;&#32435;&#20837;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#21334;&#26041;&#30340;&#32047;&#35745;&#25910;&#30410;&#12290;&#39318;&#20808;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#19981;&#32771;&#34385;&#31574;&#30053;&#24615;&#30340;&#23450;&#20215;&#31574;&#30053;&#30340;&#23384;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized pricing, which involves tailoring prices based on individual characteristics, is commonly used by firms to implement a consumer-specific pricing policy. In this process, buyers can also strategically manipulate their feature data to obtain a lower price, incurring certain manipulation costs. Such strategic behavior can hinder firms from maximizing their profits. In this paper, we study the contextual dynamic pricing problem with strategic buyers. The seller does not observe the buyer's true feature, but a manipulated feature according to buyers' strategic behavior. In addition, the seller does not observe the buyers' valuation of the product, but only a binary response indicating whether a sale happens or not. Recognizing these challenges, we propose a strategic dynamic pricing policy that incorporates the buyers' strategic behavior into the online learning to maximize the seller's cumulative revenue. We first prove that existing non-strategic pricing policies that neglect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MolGroup&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#20219;&#21153;&#30456;&#20284;&#24615;&#30456;&#32467;&#21512;&#65292;&#39044;&#27979;&#27599;&#20010;&#36741;&#21161;&#20998;&#23376;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#20197;&#35299;&#20915;&#21512;&#20316;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#38598;&#26102;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04052</link><description>&lt;p&gt;
&#23398;&#20064;&#23558;&#36741;&#21161;&#25968;&#25454;&#38598;&#20998;&#32452;&#29992;&#20110;&#20998;&#23376;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning to Group Auxiliary Datasets for Molecule. (arXiv:2307.04052v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MolGroup&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#20219;&#21153;&#30456;&#20284;&#24615;&#30456;&#32467;&#21512;&#65292;&#39044;&#27979;&#27599;&#20010;&#36741;&#21161;&#20998;&#23376;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#20197;&#35299;&#20915;&#21512;&#20316;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#38598;&#26102;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#20998;&#23376;&#25968;&#25454;&#38598;&#20013;&#26377;&#38480;&#30340;&#27880;&#37322;&#21487;&#29992;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#31574;&#30053;&#26159;&#19982;&#39069;&#22806;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#25317;&#26377;&#26356;&#22810;&#30340;&#25968;&#25454;&#24182;&#19981;&#24635;&#26159;&#33021;&#20445;&#35777;&#25913;&#36827;&#12290;&#24403;&#30446;&#26631;&#25968;&#25454;&#38598;&#20013;&#30340;&#30693;&#35782;&#19982;&#36741;&#21161;&#20998;&#23376;&#25968;&#25454;&#38598;&#20013;&#30340;&#30693;&#35782;&#19981;&#21516;&#25110;&#30456;&#20114;&#30683;&#30462;&#26102;&#65292;&#36127;&#36801;&#31227;&#21487;&#33021;&#20250;&#21457;&#29983;&#12290;&#37492;&#20110;&#27492;&#65292;&#24403;&#20849;&#21516;&#35757;&#32451;&#26102;&#65292;&#30830;&#23450;&#21487;&#20197;&#20351;&#30446;&#26631;&#25968;&#25454;&#38598;&#21463;&#30410;&#30340;&#36741;&#21161;&#20998;&#23376;&#25968;&#25454;&#38598;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#32780;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23558;&#22270;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#20219;&#21153;&#30456;&#20284;&#24615;&#30456;&#32467;&#21512;&#21487;&#20197;&#20316;&#20026;&#30830;&#23450;&#39640;&#20146;&#21644;&#24615;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#26356;&#21487;&#38752;&#25351;&#26631;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MolGroup&#65292;&#23427;&#23558;&#25968;&#25454;&#38598;&#20146;&#21644;&#24615;&#20998;&#20026;&#20219;&#21153;&#20146;&#21644;&#24615;&#21644;&#32467;&#26500;&#20146;&#21644;&#24615;&#65292;&#20197;&#39044;&#27979;&#27599;&#20010;&#36741;&#21161;&#20998;&#23376;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;MolGroup&#36890;&#36807;&#21033;&#29992;&#36335;&#30001;&#26426;&#21046;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The limited availability of annotations in small molecule datasets presents a challenge to machine learning models. To address this, one common strategy is to collaborate with additional auxiliary datasets. However, having more data does not always guarantee improvements. Negative transfer can occur when the knowledge in the target dataset differs or contradicts that of the auxiliary molecule datasets. In light of this, identifying the auxiliary molecule datasets that can benefit the target dataset when jointly trained remains a critical and unresolved problem. Through an empirical analysis, we observe that combining graph structure similarity and task similarity can serve as a more reliable indicator for identifying high-affinity auxiliary datasets. Motivated by this insight, we propose MolGroup, which separates the dataset affinity into task and structure affinity to predict the potential benefits of each auxiliary molecule dataset. MolGroup achieves this by utilizing a routing mecha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#24555;&#36882;&#36816;&#33829;&#20013;&#30340;&#36127;&#36733;&#35268;&#21010;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#36127;&#36733;&#21644;&#27969;&#31243;&#35268;&#21010;&#30340;&#25361;&#25112;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#32593;&#32476;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#23545;&#31216;&#24615;&#65292;&#23548;&#33268;&#20248;&#21270;&#27714;&#35299;&#22120;&#36820;&#22238;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#38477;&#20302;&#20102;&#35268;&#21010;&#20154;&#21592;&#23545;&#20248;&#21270;&#27714;&#35299;&#30340;&#20449;&#20219;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.04050</link><description>&lt;p&gt;
&#22522;&#20110;&#20248;&#21270;&#30340;&#23398;&#20064;&#29992;&#20110;&#21345;&#36710;&#36816;&#36755;&#26381;&#21153;&#32593;&#32476;&#20013;&#30340;&#21160;&#24577;&#36127;&#36733;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Optimization-based Learning for Dynamic Load Planning in Trucking Service Networks. (arXiv:2307.04050v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#24555;&#36882;&#36816;&#33829;&#20013;&#30340;&#36127;&#36733;&#35268;&#21010;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#36127;&#36733;&#21644;&#27969;&#31243;&#35268;&#21010;&#30340;&#25361;&#25112;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#32593;&#32476;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#23545;&#31216;&#24615;&#65292;&#23548;&#33268;&#20248;&#21270;&#27714;&#35299;&#22120;&#36820;&#22238;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#38477;&#20302;&#20102;&#35268;&#21010;&#20154;&#21592;&#23545;&#20248;&#21270;&#27714;&#35299;&#30340;&#20449;&#20219;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36127;&#36733;&#35268;&#21010;&#38382;&#39064;&#26159;&#24555;&#36882;&#36816;&#33829;&#20013;&#26381;&#21153;&#32593;&#32476;&#35774;&#35745;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#23427;&#20915;&#23450;&#20102;&#22312;&#32456;&#31471;&#20043;&#38388;&#22914;&#20309;&#22312;&#26102;&#38388;&#19978;&#20998;&#37197;&#22810;&#23569;&#36742;&#25302;&#36710;&#65288;&#25110;&#36127;&#36733;&#65289;&#36827;&#34892;&#27966;&#36963;&#12290;&#21478;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30830;&#23450;&#19968;&#20010;&#27969;&#31243;&#35745;&#21010;&#65292;&#23427;&#25351;&#23450;&#20102;&#22914;&#20309;&#23558;&#21253;&#35065;&#20307;&#31215;&#20998;&#37197;&#32473;&#35745;&#21010;&#30340;&#36127;&#36733;&#12290;&#26412;&#25991;&#32771;&#34385;&#21040;&#20102;&#21160;&#24577;&#36127;&#36733;&#35268;&#21010;&#38382;&#39064;&#65288;DLPP&#65289;&#65292;&#23427;&#21516;&#26102;&#32771;&#34385;&#20102;&#36127;&#36733;&#21644;&#27969;&#31243;&#35268;&#21010;&#30340;&#25361;&#25112;&#65292;&#20197;&#22312;&#25805;&#20316;&#26085;&#20043;&#21069;&#38543;&#30528;&#38656;&#27714;&#39044;&#27979;&#30340;&#21464;&#21270;&#32780;&#35843;&#25972;&#36127;&#36733;&#21644;&#27969;&#31243;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#20026;&#32593;&#32476;&#20013;&#21508;&#20010;&#32456;&#31471;&#30340;&#35268;&#21010;&#20154;&#21592;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#12290;&#26412;&#25991;&#23558;DLPP&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;MIP&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#27599;&#20010;&#21830;&#21697;&#37117;&#21487;&#20197;&#36890;&#36807;&#20027;&#36335;&#24452;&#21644;&#22791;&#29992;&#36335;&#24452;&#36827;&#34892;&#36335;&#30001;&#30340;&#32593;&#32476;&#20013;&#26377;&#22823;&#37327;&#30340;&#23545;&#31216;&#24615;&#12290;&#22240;&#27492;&#65292;&#20248;&#21270;&#27714;&#35299;&#22120;&#21487;&#33021;&#20250;&#23545;&#23494;&#20999;&#30456;&#20851;&#30340;&#38382;&#39064;&#36820;&#22238;&#26681;&#26412;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#35268;&#21010;&#20154;&#21592;&#24863;&#21040;&#22256;&#24785;&#65292;&#38477;&#20302;&#23545;&#20248;&#21270;&#27714;&#35299;&#30340;&#20449;&#20219;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The load planning problem is a critical challenge in service network design for parcel carriers: it decides how many trailers (or loads) to assign for dispatch over time between pairs of terminals. Another key challenge is to determine a flow plan, which specifies how parcel volumes are assigned to planned loads. This paper considers the Dynamic Load Planning Problem (DLPP) that considers both flow and load planning challenges jointly to adjust loads and flows as the demand forecast changes over time before the day of operations. The paper aims at developing a decision-support tool to inform planners making these decisions at terminals across the network. The paper formulates the DLPP as a MIP and shows that it admits a large number of symmetries in a network where each commodity can be routed through primary and alternate paths. As a result, an optimization solver may return fundamentally different solutions to closely related problems, confusing planners and reducing trust in optimiz
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#39034;&#24207;&#31639;&#27861;&#30456;&#27604;&#65292;&#24182;&#34892;&#31639;&#27861;&#33021;&#20805;&#20998;&#21033;&#29992;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.04049</link><description>&lt;p&gt;
&#24182;&#34892;&#31639;&#27861;&#19982;&#31070;&#32463;&#25191;&#34892;&#21516;&#27493;
&lt;/p&gt;
&lt;p&gt;
Parallel Algorithms Align with Neural Execution. (arXiv:2307.04049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#39034;&#24207;&#31639;&#27861;&#30456;&#27604;&#65292;&#24182;&#34892;&#31639;&#27861;&#33021;&#20805;&#20998;&#21033;&#29992;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#26159;&#24182;&#34892;&#22788;&#29702;&#22120;&#12290;&#25945;&#25480;&#23427;&#20204;&#39034;&#24207;&#31639;&#27861;&#19982;&#20854;&#24615;&#36136;&#30456;&#30683;&#30462;&#65292;&#20351;&#24471;&#23427;&#20204;&#30340;&#35745;&#31639;&#20013;&#21253;&#21547;&#24456;&#22810;&#20887;&#20313;&#12290;&#28982;&#32780;&#24182;&#34892;&#31639;&#27861;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#22240;&#27492;&#21482;&#38656;&#25191;&#34892;&#36739;&#23569;&#30340;&#23618;&#27425;&#12290;&#36825;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;CLRS&#26694;&#26550;&#19978;&#39034;&#24207;&#23454;&#29616;&#30340;&#25628;&#32034;&#12289;&#25490;&#24207;&#21644;&#23547;&#25214;&#24378;&#36830;&#25509;&#32452;&#20214;&#30456;&#27604;&#65292;&#24182;&#34892;&#23454;&#29616;&#30340;&#35757;&#32451;&#26102;&#38388;&#22823;&#22823;&#32553;&#30701;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#24182;&#34892;&#29256;&#26412;&#30340;&#39044;&#27979;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural algorithmic reasoners are parallel processors. Teaching them sequential algorithms contradicts this nature, rendering a significant share of their computations redundant. Parallel algorithms however may exploit their full computational power, therefore requiring fewer layers to be executed. This drastically reduces training times, as we observe when comparing parallel implementations of searching, sorting and finding strongly connected components to their sequential counterparts on the CLRS framework. Additionally, parallel versions achieve strongly superior predictive performance in most cases.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#36229;&#33539;&#25968;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#26222;&#36890;&#30340;&#23545;&#25239;&#35757;&#32451;&#20351;&#24471;&#31070;&#32463;&#20272;&#35745;&#22120;&#19981;&#19968;&#33268;&#65292;&#20294;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#24102;&#20462;&#27491;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#22312;&#36229;&#33539;&#25968;&#24847;&#20041;&#19979;&#36798;&#21040;&#26368;&#20248;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.04042</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#36229;&#33539;&#25968;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sup-Norm Convergence of Deep Neural Network Estimator for Nonparametric Regression by Adversarial Training. (arXiv:2307.04042v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04042
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#36229;&#33539;&#25968;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#26222;&#36890;&#30340;&#23545;&#25239;&#35757;&#32451;&#20351;&#24471;&#31070;&#32463;&#20272;&#35745;&#22120;&#19981;&#19968;&#33268;&#65292;&#20294;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#24102;&#20462;&#27491;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#22312;&#36229;&#33539;&#25968;&#24847;&#20041;&#19979;&#36798;&#21040;&#26368;&#20248;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#26696;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#30340;&#36229;&#33539;&#25968;&#25910;&#25947;&#24615;&#12290;&#38024;&#23545;&#38750;&#21442;&#25968;&#22238;&#24402;&#38382;&#39064;&#65292;&#24050;&#32463;&#35777;&#26126;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#22120;&#22312;$L2$-&#33539;&#25968;&#24847;&#20041;&#19979;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#28145;&#24230;&#32467;&#26500;&#65292;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#27861;&#30340;&#31070;&#32463;&#20272;&#35745;&#22120;&#24456;&#38590;&#36798;&#21040;&#36229;&#33539;&#25968;&#25910;&#25947;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31181;&#23545;&#25239;&#35757;&#32451;&#26041;&#26696;&#65292;&#24182;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#30340;&#36229;&#33539;&#25968;&#25910;&#25947;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#26222;&#36890;&#30340;&#23545;&#25239;&#35757;&#32451;&#20351;&#24471;&#31070;&#32463;&#20272;&#35745;&#22120;&#19981;&#19968;&#33268;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#24102;&#20462;&#27491;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#22312;&#36229;&#33539;&#25968;&#24847;&#20041;&#19979;&#36798;&#21040;&#26368;&#20248;&#36895;&#29575;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#23545;&#25239;&#35757;&#32451;&#25193;&#23637;&#21040;&#20102;&#19968;&#33324;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#25968;&#25454;&#29983;&#25104;&#20989;&#25968;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25903;&#25345;&#20102;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show the sup-norm convergence of deep neural network estimators with a novel adversarial training scheme. For the nonparametric regression problem, it has been shown that an estimator using deep neural networks can achieve better performances in the sense of the $L2$-norm. In contrast, it is difficult for the neural estimator with least-squares to achieve the sup-norm convergence, due to the deep structure of neural network models. In this study, we develop an adversarial training scheme and investigate the sup-norm convergence of deep neural network estimators. First, we find that ordinary adversarial training makes neural estimators inconsistent. Second, we show that a deep neural network estimator achieves the optimal rate in the sup-norm sense by the proposed adversarial training with correction. We extend our adversarial training to general setups of a loss function and a data-generating function. Our experiments support the theoretical findings.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35774;&#35745;&#20102;DeepFuse&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#29992;&#25143;&#21644;CNN&#20043;&#38388;&#30452;&#25509;&#21453;&#39304;&#24490;&#29615;&#30340;&#20132;&#20114;&#35774;&#35745;&#65292;&#36890;&#36807;&#23616;&#37096;&#35299;&#37322;&#24110;&#21161;CNN&#24037;&#31243;&#24072;&#31995;&#32479;&#22320;&#35786;&#26029;&#21644;&#20462;&#25913;CNN&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04036</link><description>&lt;p&gt;
&#35774;&#35745;&#19968;&#20010;&#20154;&#31867;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#30452;&#25509;&#21453;&#39304;&#24490;&#29615;&#36890;&#36807;&#23616;&#37096;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Designing a Direct Feedback Loop between Humans and Convolutional Neural Networks through Local Explanations. (arXiv:2307.04036v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04036
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35774;&#35745;&#20102;DeepFuse&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#29992;&#25143;&#21644;CNN&#20043;&#38388;&#30452;&#25509;&#21453;&#39304;&#24490;&#29615;&#30340;&#20132;&#20114;&#35774;&#35745;&#65292;&#36890;&#36807;&#23616;&#37096;&#35299;&#37322;&#24110;&#21161;CNN&#24037;&#31243;&#24072;&#31995;&#32479;&#22320;&#35786;&#26029;&#21644;&#20462;&#25913;CNN&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#35299;&#37322;&#36890;&#36807;&#22270;&#20687;&#28909;&#22270;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26159;&#22914;&#20309;&#24471;&#20986;&#20182;&#20204;&#30340;&#36755;&#20986;&#32467;&#26524;&#30340;&#12290;&#30001;&#20110;&#20854;&#30452;&#35266;&#26126;&#20102;&#30340;&#29305;&#28857;&#65292;&#35813;&#26041;&#27861;&#24050;&#25104;&#20026;&#35786;&#26029;CNN&#30340;&#26368;&#21463;&#27426;&#36814;&#30340;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#65292;&#25105;&#20204;&#25429;&#25417;&#21040;ML&#24037;&#31243;&#24072;&#23545;&#23616;&#37096;&#35299;&#37322;&#30340;&#20215;&#20540;&#21644;&#19981;&#21487;&#25110;&#32570;&#30340;&#30475;&#27861;&#23384;&#22312;&#30683;&#30462;&#65306;&#19968;&#26041;&#38754;&#35748;&#20026;&#23616;&#37096;&#35299;&#37322;&#26159;&#26500;&#24314;CNN&#19981;&#21487;&#25110;&#32570;&#30340;&#24895;&#26223;&#65292;&#21478;&#19968;&#26041;&#38754;&#21364;&#21448;&#35748;&#20026;&#23427;&#26159;&#19968;&#20010;&#28040;&#32791;&#33021;&#37327;&#30340;&#36807;&#31243;&#65292;&#22240;&#20026;&#23427;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#33030;&#24369;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#35786;&#26029;&#25152;&#23398;&#21040;&#30340;&#33030;&#24369;&#24615;&#25351;&#23548;CNN&#30340;&#36807;&#31243;&#20063;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;DeepFuse&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#29992;&#25143;&#21644;CNN&#20043;&#38388;&#30452;&#25509;&#21453;&#39304;&#24490;&#29615;&#30340;&#20132;&#20114;&#35774;&#35745;&#65292;&#29992;&#20110;&#35786;&#26029;&#21644;&#20462;&#25913;CNN&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#20351;&#29992;&#23616;&#37096;&#35299;&#37322;&#24110;&#21161;CNN&#24037;&#31243;&#24072;&#31995;&#32479;&#22320;&#25628;&#32034;&#8220;&#19981;&#21512;&#29702;&#8221;&#30340;&#23616;&#37096;&#35299;&#37322;&#65292;&#24182;&#20026;&#37027;&#20123;&#34987;&#35782;&#21035;&#20026;&#19981;&#21512;&#29702;&#30340;&#35299;&#37322;&#26631;&#27880;&#26032;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
The local explanation provides heatmaps on images to explain how Convolutional Neural Networks (CNNs) derive their output. Due to its visual straightforwardness, the method has been one of the most popular explainable AI (XAI) methods for diagnosing CNNs. Through our formative study (S1), however, we captured ML engineers' ambivalent perspective about the local explanation as a valuable and indispensable envision in building CNNs versus the process that exhausts them due to the heuristic nature of detecting vulnerability. Moreover, steering the CNNs based on the vulnerability learned from the diagnosis seemed highly challenging. To mitigate the gap, we designed DeepFuse, the first interactive design that realizes the direct feedback loop between a user and CNNs in diagnosing and revising CNN's vulnerability using local explanations. DeepFuse helps CNN engineers to systemically search "unreasonable" local explanations and annotate the new boundaries for those identified as unreasonable 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#26102;&#39046;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#27010;&#29575;&#20266;&#26631;&#31614;&#21644;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#26469;&#25512;&#24191;&#28304;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04033</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#27979;&#35797;&#26102;&#39046;&#22495;&#27867;&#21270;&#30340;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Learning Variational Neighbor Labels for Test-Time Domain Generalization. (arXiv:2307.04033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#26102;&#39046;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#27010;&#29575;&#20266;&#26631;&#31614;&#21644;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#26469;&#25512;&#24191;&#28304;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#39046;&#22495;&#27867;&#21270;&#65292;&#22312;&#26410;&#30693;&#30340;&#30446;&#26631;&#39046;&#22495;&#20013;&#21482;&#22312;&#28304;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#28304;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30446;&#26631;&#22495;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#30446;&#26631;&#25968;&#25454;&#26412;&#36523;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30446;&#26631;&#26679;&#26412;&#30340;&#27010;&#29575;&#20266;&#26631;&#31614;&#65292;&#20197;&#22312;&#27979;&#35797;&#26102;&#23558;&#28304;&#39046;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#25512;&#24191;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#25105;&#20204;&#23558;&#27979;&#35797;&#26102;&#30340;&#25512;&#24191;&#24314;&#27169;&#20026;&#21464;&#20998;&#25512;&#29702;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20266;&#26631;&#31614;&#24314;&#27169;&#20026;&#20998;&#24067;&#65292;&#32771;&#34385;&#27867;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20943;&#36731;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#24615;&#24102;&#26469;&#30340;&#35823;&#23548;&#20449;&#21495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#65292;&#23558;&#37051;&#36817;&#30446;&#26631;&#26679;&#26412;&#30340;&#20449;&#24687;&#32435;&#20837;&#21040;&#29983;&#25104;&#26356;&#24378;&#40065;&#26834;&#20266;&#26631;&#31614;&#30340;&#36807;&#31243;&#20013;&#12290;&#31532;&#19977;&#65292;&#20026;&#20102;&#23398;&#20064;&#23558;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#30446;&#26631;&#20449;&#24687;&#32435;&#20837;&#21040;&#29983;&#25104;&#26356;&#20934;&#30830;&#12289;&#26356;&#24378;&#40065;&#26834;&#30340;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#30340;&#33021;&#21147;&#20013;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
This paper strives for domain generalization, where models are trained exclusively on source domains before being deployed at unseen target domains. We follow the strict separation of source training and target testing but exploit the value of the unlabeled target data itself during inference. We make three contributions. First, we propose probabilistic pseudo-labeling of target samples to generalize the source-trained model to the target domain at test time. We formulate the generalization at test time as a variational inference problem by modeling pseudo labels as distributions to consider the uncertainty during generalization and alleviate the misleading signal of inaccurate pseudo labels. Second, we learn variational neighbor labels that incorporate the information of neighboring target samples to generate more robust pseudo labels. Third, to learn the ability to incorporate more representative target information and generate more precise and robust variational neighbor labels, we 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#27169;&#20223;&#20154;&#31867;&#33402;&#26415;&#23478;&#26041;&#38754;&#30340;&#25104;&#21151;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#23558;&#29256;&#26435;&#36131;&#20219;&#19982;&#27169;&#22411;&#33021;&#21147;&#32852;&#31995;&#36215;&#26469;&#21487;&#33021;&#26159;&#26377;&#29992;&#30340;&#12290;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;Contrastive Language-Image Pretrained (CLIP)&#32534;&#30721;&#22120;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#26469;&#34913;&#37327;&#27169;&#22411;&#27169;&#20223;&#29305;&#23450;&#33402;&#26415;&#23478;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.04028</link><description>&lt;p&gt;
&#35780;&#20272;&#25193;&#25955;&#27169;&#22411;&#22312;&#27169;&#20223;&#20154;&#31867;&#33402;&#26415;&#23478;&#26041;&#38754;&#30340;&#25104;&#21151;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Measuring the Success of Diffusion Models at Imitating Human Artists. (arXiv:2307.04028v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04028
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#27169;&#20223;&#20154;&#31867;&#33402;&#26415;&#23478;&#26041;&#38754;&#30340;&#25104;&#21151;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#23558;&#29256;&#26435;&#36131;&#20219;&#19982;&#27169;&#22411;&#33021;&#21147;&#32852;&#31995;&#36215;&#26469;&#21487;&#33021;&#26159;&#26377;&#29992;&#30340;&#12290;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;Contrastive Language-Image Pretrained (CLIP)&#32534;&#30721;&#22120;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#26469;&#34913;&#37327;&#27169;&#22411;&#27169;&#20223;&#29305;&#23450;&#33402;&#26415;&#23478;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25193;&#25955;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#23427;&#20204;&#30340;&#25104;&#21151;&#37096;&#20998;&#26159;&#22240;&#20026;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#65292;&#20854;&#20013;&#32463;&#24120;&#21253;&#21547;&#26377;&#29256;&#26435;&#30340;&#20316;&#21697;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20174;&#20154;&#31867;&#33402;&#26415;&#23478;&#30340;&#20316;&#21697;&#20013;&#23398;&#20064;&#12289;&#27169;&#20223;&#25110;&#22797;&#21046;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#35748;&#20026;&#65292;&#23558;&#29256;&#26435;&#36131;&#20219;&#19982;&#27169;&#22411;&#30340;&#33021;&#21147;&#32852;&#31995;&#36215;&#26469;&#65292;&#21487;&#33021;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#19981;&#26029;&#21457;&#23637;&#30340;&#29615;&#22659;&#20013;&#26159;&#26377;&#29992;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20851;&#20110;&#29256;&#26435;&#21644;&#29983;&#25104;&#31995;&#32479;&#30340;&#27861;&#24459;&#20998;&#26512;&#24448;&#24448;&#20391;&#37325;&#20110;&#20351;&#29992;&#21463;&#20445;&#25252;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#12289;&#35757;&#32451;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#32852;&#31995;&#24120;&#24120;&#34987;&#25513;&#30422;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#31616;&#21333;&#30340;&#22270;&#20687;&#20998;&#31867;&#25216;&#26415;&#65292;&#20197;&#34913;&#37327;&#27169;&#22411;&#27169;&#20223;&#29305;&#23450;&#33402;&#26415;&#23478;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;(CLIP)&#32534;&#30721;&#22120;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#36807;&#31243;&#39318;&#20808;&#25552;&#31034;&#27169;&#22411;&#27169;&#20223;&#29305;&#23450;&#30340;&#33402;&#26415;&#23478;&#65292;&#28982;&#21518;&#27979;&#35797;CLIP&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern diffusion models have set the state-of-the-art in AI image generation. Their success is due, in part, to training on Internet-scale data which often includes copyrighted work. This prompts questions about the extent to which these models learn from, imitate, or copy the work of human artists. This work suggests that tying copyright liability to the capabilities of the model may be useful given the evolving ecosystem of generative models. Specifically, much of the legal analysis of copyright and generative systems focuses on the use of protected data for training. As a result, the connections between data, training, and the system are often obscured. In our approach, we consider simple image classification techniques to measure a model's ability to imitate specific artists. Specifically, we use Contrastive Language-Image Pretrained (CLIP) encoders to classify images in a zero-shot fashion. Our process first prompts a model to imitate a specific artist. Then, we test whether CLIP 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;R2ET&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#35299;&#37322;&#21402;&#24230;&#26469;&#34913;&#37327;&#25490;&#21517;&#31283;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20986;&#21487;&#26368;&#22823;&#21270;&#35299;&#37322;&#21402;&#24230;&#24182;&#38170;&#23450;&#25490;&#21517;&#38752;&#21069;&#29305;&#24449;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;R2ET&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#20855;&#26377;&#26356;&#39640;&#30340;&#35299;&#37322;&#40065;&#26834;&#24615;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.04024</link><description>&lt;p&gt;
&#24378;&#22823;&#30340;&#25490;&#21517;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust Ranking Explanations. (arXiv:2307.04024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04024
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;R2ET&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#35299;&#37322;&#21402;&#24230;&#26469;&#34913;&#37327;&#25490;&#21517;&#31283;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20986;&#21487;&#26368;&#22823;&#21270;&#35299;&#37322;&#21402;&#24230;&#24182;&#38170;&#23450;&#25490;&#21517;&#38752;&#21069;&#29305;&#24449;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;R2ET&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#20855;&#26377;&#26356;&#39640;&#30340;&#35299;&#37322;&#40065;&#26834;&#24615;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#23545;&#20110;&#24314;&#31435;&#20154;&#31867;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#35748;&#30693;&#33021;&#21147;&#26377;&#38480;&#65292;&#22823;&#22810;&#25968;&#20154;&#21482;&#33021;&#35299;&#37322;&#25490;&#21517;&#21069;&#20960;&#20010;&#26174;&#33879;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#23558;&#25490;&#21517;&#38752;&#21069;&#30340;&#26174;&#33879;&#29305;&#24449;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#23588;&#20026;&#20851;&#38190;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#38024;&#23545;&#26356;&#33030;&#24369;&#30340;&#26799;&#24230;&#35299;&#37322;&#12290;&#29616;&#26377;&#30340;&#38450;&#23432;&#25514;&#26045;&#20351;&#29992;l_p-&#33539;&#25968;&#26469;&#25552;&#20379;&#40065;&#26834;&#24615;&#65292;&#20294;&#20854;&#20445;&#25252;&#33021;&#21147;&#36739;&#24369;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#35299;&#37322;&#21402;&#24230;&#26469;&#34913;&#37327;&#25490;&#21517;&#31283;&#23450;&#24615;&#65292;&#24182;&#23548;&#20986;&#20102;&#21487;&#35745;&#31639;&#30340;&#26367;&#20195;&#19978;&#30028;&#26469;&#35774;&#35745;R2ET&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#26368;&#22823;&#21270;&#21402;&#24230;&#24182;&#38170;&#23450;&#25490;&#21517;&#38752;&#21069;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;R2ET&#19982;&#23545;&#25239;&#24615;&#35757;&#32451;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#12290;&#23454;&#39564;&#20013;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#25968;&#25454;&#27169;&#24335;&#65292;&#21253;&#25324;&#33041;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;R2ET&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#36798;&#21040;&#26356;&#39640;&#30340;&#35299;&#37322;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust explanations of machine learning models are critical to establish human trust in the models. Due to limited cognition capability, most humans can only interpret the top few salient features. It is critical to make top salient features robust to adversarial attacks, especially those against the more vulnerable gradient-based explanations. Existing defense measures robustness using $\ell_p$-norms, which have weaker protection power. We define explanation thickness for measuring salient features ranking stability, and derive tractable surrogate bounds of the thickness to design the \textit{R2ET} algorithm to efficiently maximize the thickness and anchor top salient features. Theoretically, we prove a connection between R2ET and adversarial training. Experiments with a wide spectrum of network architectures and data modalities, including brain networks, demonstrate that R2ET attains higher explanation robustness under stealthy attacks while retaining accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;PapillArray&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#26816;&#27979;&#26089;&#26399;&#28369;&#21160;&#65292;&#23454;&#29616;&#20102;95.6%&#30340;&#26816;&#27979;&#25104;&#21151;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20063;&#20445;&#25345;&#20102;&#40065;&#26834;&#30340;&#24615;&#33021;&#65292;&#25104;&#21151;&#29575;&#36798;&#21040;96.8%&#12290;</title><link>http://arxiv.org/abs/2307.04011</link><description>&lt;p&gt;
&#20351;&#29992;PapillArray&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#30340;&#40065;&#26834;&#23398;&#20064;&#22522;&#20110;&#26089;&#26399;&#28369;&#21160;&#26816;&#27979;&#30340;&#25913;&#36827;&#26426;&#22120;&#20154;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Robust Learning-Based Incipient Slip Detection using the PapillArray Optical Tactile Sensor for Improved Robotic Gripping. (arXiv:2307.04011v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;PapillArray&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#26816;&#27979;&#26089;&#26399;&#28369;&#21160;&#65292;&#23454;&#29616;&#20102;95.6%&#30340;&#26816;&#27979;&#25104;&#21151;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20063;&#20445;&#25345;&#20102;&#40065;&#26834;&#30340;&#24615;&#33021;&#65292;&#25104;&#21151;&#29575;&#36798;&#21040;96.8%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#28369;&#21160;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#26089;&#26399;&#28369;&#21160;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#31995;&#32479;&#33021;&#22815;&#37319;&#21462;&#32416;&#27491;&#25514;&#26045;&#65292;&#38450;&#27490;&#25235;&#21462;&#30340;&#29289;&#20307;&#25481;&#33853;&#12290;&#22240;&#27492;&#65292;&#28369;&#21160;&#26816;&#27979;&#21487;&#20197;&#22686;&#24378;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#25972;&#20307;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#26816;&#27979;&#26089;&#26399;&#28369;&#21160;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;PapillArray&#65288;Contactile&#65292;&#28595;&#22823;&#21033;&#20122;&#65289;&#35302;&#35273;&#20256;&#24863;&#22120;&#26469;&#26816;&#27979;&#26089;&#26399;&#28369;&#21160;&#12290;&#25152;&#24471;&#27169;&#22411;&#22312;&#35782;&#21035;&#19982;&#26089;&#26399;&#28369;&#21160;&#30456;&#20851;&#30340;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#65292;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#27979;&#35797;&#26102;&#23454;&#29616;&#20102;95.6%&#30340;&#26816;&#27979;&#25104;&#21151;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#22686;&#24378;&#25105;&#20204;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#24403;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36716;&#31227;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#37319;&#38598;&#29615;&#22659;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#25235;&#21462;&#29615;&#22659;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20445;&#25345;&#20102;&#40065;&#26834;&#30340;&#24615;&#33021;&#65292;&#25104;&#21151;&#29575;&#20026;96.8%&#65292;&#20026;&#31283;&#23450;&#20960;&#31181;&#23454;&#38469;&#25235;&#21462;&#25552;&#20379;&#21450;&#26102;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to detect slip, particularly incipient slip, enables robotic systems to take corrective measures to prevent a grasped object from being dropped. Therefore, slip detection can enhance the overall security of robotic gripping. However, accurately detecting incipient slip remains a significant challenge. In this paper, we propose a novel learning-based approach to detect incipient slip using the PapillArray (Contactile, Australia) tactile sensor. The resulting model is highly effective in identifying patterns associated with incipient slip, achieving a detection success rate of 95.6% when tested with an offline dataset. Furthermore, we introduce several data augmentation methods to enhance the robustness of our model. When transferring the trained model to a robotic gripping environment distinct from where the training data was collected, our model maintained robust performance, with a success rate of 96.8%, providing timely feedback for stabilizing several practical gripping 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;U-Net&#12289;U-Net + ViT&#21644;FNO&#31561;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22320;&#19979;&#27700;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#30456;&#20851;&#27491;&#21521;&#24314;&#27169;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#21457;&#29616;U-Net&#21644;U-Net + ViT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;FNO&#65292;&#23588;&#20854;&#26159;&#22312;&#31232;&#30095;&#25968;&#25454;&#22330;&#26223;&#20013;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;U-Net&#27169;&#22411;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#23454;&#38469;&#22320;&#19979;&#27700;&#24314;&#27169;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.04010</link><description>&lt;p&gt;
&#29702;&#35299;U-Net&#21644;Vision Transformer&#22312;&#22320;&#19979;&#27700;&#25968;&#20540;&#24314;&#27169;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding the Efficacy of U-Net &amp; Vision Transformer for Groundwater Numerical Modelling. (arXiv:2307.04010v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04010
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;U-Net&#12289;U-Net + ViT&#21644;FNO&#31561;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22320;&#19979;&#27700;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#30456;&#20851;&#27491;&#21521;&#24314;&#27169;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#21457;&#29616;U-Net&#21644;U-Net + ViT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;FNO&#65292;&#23588;&#20854;&#26159;&#22312;&#31232;&#30095;&#25968;&#25454;&#22330;&#26223;&#20013;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;U-Net&#27169;&#22411;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#23454;&#38469;&#22320;&#19979;&#27700;&#24314;&#27169;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#21253;&#25324;U-Net&#65292;&#38598;&#25104;&#20102;Vision Transformers&#65288;ViT&#65289;&#30340;U-Net&#21644;Fourier&#31070;&#32463;&#36816;&#31639;&#31526;&#65288;FNO&#65289;&#65292;&#29992;&#20110;&#22320;&#19979;&#27700;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#30456;&#20851;&#27491;&#21521;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;U-Net&#21644;U-Net + ViT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;FNO&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#25968;&#25454;&#22330;&#26223;&#20013;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;U-Net&#27169;&#22411;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#29992;&#20110;&#22320;&#19979;&#27700;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive comparison of various machine learning models, namely U-Net, U-Net integrated with Vision Transformers (ViT), and Fourier Neural Operator (FNO), for time-dependent forward modelling in groundwater systems. Through testing on synthetic datasets, it is demonstrated that U-Net and U-Net + ViT models outperform FNO in accuracy and efficiency, especially in sparse data scenarios. These findings underscore the potential of U-Net-based models for groundwater modelling in real-world applications where data scarcity is prevalent.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20004;&#31181;&#38598;&#21512;&#20803;&#32032;&#23884;&#20837;&#23618;&#30340;&#25506;&#32034;&#65292;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#23485;&#24230;&#23545;&#20110;&#39640;&#32500;&#29305;&#24449;&#30340;&#38598;&#21512;&#34920;&#31034;&#36275;&#22815;&#65292;&#24182;&#25581;&#31034;&#20102;&#20043;&#21069;&#20998;&#26512;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04001</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#23485;&#24230;&#23545;&#20110;&#20855;&#26377;&#39640;&#32500;&#29305;&#24449;&#30340;&#38598;&#21512;&#34920;&#31034;&#36275;&#22815;
&lt;/p&gt;
&lt;p&gt;
Polynomial Width is Sufficient for Set Representation with High-dimensional Features. (arXiv:2307.04001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20004;&#31181;&#38598;&#21512;&#20803;&#32032;&#23884;&#20837;&#23618;&#30340;&#25506;&#32034;&#65292;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#23485;&#24230;&#23545;&#20110;&#39640;&#32500;&#29305;&#24449;&#30340;&#38598;&#21512;&#34920;&#31034;&#36275;&#22815;&#65292;&#24182;&#25581;&#31034;&#20102;&#20043;&#21069;&#20998;&#26512;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#21512;&#34920;&#31034;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#65292;&#29992;&#20110;&#24314;&#27169;&#31070;&#32463;&#32593;&#32476;&#23545;&#36755;&#20837;&#39034;&#24207;&#19981;&#25935;&#24863;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;DeepSets&#26159;&#26368;&#24120;&#29992;&#30340;&#38598;&#21512;&#34920;&#31034;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#23558;&#27599;&#20010;&#38598;&#21512;&#20803;&#32032;&#23884;&#20837;&#21040;&#20855;&#26377;&#32500;&#24230;L&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#36827;&#34892;&#27714;&#21644;&#27744;&#21270;&#20197;&#33719;&#24471;&#25972;&#20010;&#38598;&#21512;&#30340;&#23884;&#20837;&#65292;&#26368;&#21518;&#23558;&#25972;&#20010;&#38598;&#21512;&#30340;&#23884;&#20837;&#26144;&#23556;&#21040;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32500;&#24230;L&#23545;DeepSets&#34920;&#36798;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#20043;&#21069;&#30340;&#20998;&#26512;&#35201;&#20040;&#23558;&#39640;&#32500;&#29305;&#24449;&#36807;&#20110;&#31616;&#21270;&#20026;&#19968;&#32500;&#29305;&#24449;&#65292;&#35201;&#20040;&#23616;&#38480;&#20110;&#20998;&#26512;&#28608;&#27963;&#20989;&#25968;&#65292;&#20174;&#32780;&#33073;&#31163;&#23454;&#38469;&#24212;&#29992;&#25110;&#23548;&#33268;L&#38543;&#30528;&#38598;&#21512;&#22823;&#23567;N&#21644;&#29305;&#24449;&#32500;&#24230;D&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#20026;&#20102;&#30740;&#31350;&#36798;&#21040;&#36275;&#22815;&#34920;&#36798;&#33021;&#21147;&#30340;&#26368;&#23567;L&#20540;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#38598;&#21512;&#20803;&#32032;&#23884;&#20837;&#23618;&#65306;&#65288;a&#65289;&#32447;&#24615;+&#24130;&#28608;&#27963;&#65288;LP&#65289;&#21644;&#65288;b&#65289;&#32447;&#24615;+&#25351;&#25968;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
Set representation has become ubiquitous in deep learning for modeling the inductive bias of neural networks that are insensitive to the input order. DeepSets is the most widely used neural network architecture for set representation. It involves embedding each set element into a latent space with dimension $L$, followed by a sum pooling to obtain a whole-set embedding, and finally mapping the whole-set embedding to the output. In this work, we investigate the impact of the dimension $L$ on the expressive power of DeepSets. Previous analyses either oversimplified high-dimensional features to be one-dimensional features or were limited to analytic activations, thereby diverging from practical use or resulting in $L$ that grows exponentially with the set size $N$ and feature dimension $D$. To investigate the minimal value of $L$ that achieves sufficient expressive power, we present two set-element embedding layers: (a) linear + power activation (LP) and (b) linear + exponential activatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#35745;&#31639;&#39640;&#25928;&#12289;&#26080;&#27169;&#22411;&#30340;&#20302;&#31209;MDPs&#25506;&#32034;&#31639;&#27861;&#65292;&#20801;&#35768;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#32467;&#26500;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2307.03997</link><description>&lt;p&gt;
&#20302;&#31209;MDP&#20013;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient Model-Free Exploration in Low-Rank MDPs. (arXiv:2307.03997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03997
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#35745;&#31639;&#39640;&#25928;&#12289;&#26080;&#27169;&#22411;&#30340;&#20302;&#31209;MDPs&#25506;&#32034;&#31639;&#27861;&#65292;&#20801;&#35768;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#32467;&#26500;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#22312;&#38656;&#35201;&#27867;&#21270;&#21644;&#20989;&#25968;&#36924;&#36817;&#30340;&#39640;&#32500;&#39046;&#22495;&#20013;&#24320;&#21457;&#20986;&#23454;&#29992;&#12289;&#26679;&#26412;&#39640;&#25928;&#30340;&#25506;&#32034;&#31639;&#27861;&#12290;&#20302;&#31209;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#8212;&#8212;&#20854;&#20013;&#36716;&#31227;&#27010;&#29575;&#21487;&#20197;&#22522;&#20110;&#26410;&#30693;&#29305;&#24449;&#23884;&#20837;&#36827;&#34892;&#20302;&#31209;&#20998;&#35299;&#8212;&#8212;&#20026;&#24102;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#31616;&#21333;&#32780;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#26694;&#26550;&#65292;&#20294;&#29616;&#26377;&#31639;&#27861;&#35201;&#20040;&#35745;&#31639;&#22797;&#26434;&#24230;&#24456;&#39640;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#38480;&#21046;&#24615;&#30340;&#32479;&#35745;&#20551;&#35774;&#65292;&#22914;&#28508;&#21464;&#37327;&#32467;&#26500;&#12289;&#23545;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#20989;&#25968;&#36924;&#36817;&#30340;&#35775;&#38382;&#24615;&#25110;&#21487;&#36798;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;&#20302;&#31209;MDPs&#25506;&#32034;&#30340;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26082;&#35745;&#31639;&#39640;&#25928;&#21448;&#26080;&#27169;&#22411;&#65292;&#20801;&#35768;&#36827;&#34892;&#36890;&#29992;&#30340;&#20989;&#25968;&#36924;&#36817;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#32467;&#26500;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;VoX&#20351;&#29992;&#20102;&#19968;&#20010;&#24191;&#20041;&#20248;&#21270;&#35774;&#35745;&#30340;&#29305;&#24449;&#23884;&#20837;&#27010;&#24565;&#20316;&#20026;&#25506;&#32034;&#30340;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in reinforcement learning is to develop practical, sample-efficient algorithms for exploration in high-dimensional domains where generalization and function approximation is required. Low-Rank Markov Decision Processes -- where transition probabilities admit a low-rank factorization based on an unknown feature embedding -- offer a simple, yet expressive framework for RL with function approximation, but existing algorithms are either (1) computationally intractable, or (2) reliant upon restrictive statistical assumptions such as latent variable structure, access to model-based function approximation, or reachability. In this work, we propose the first provably sample-efficient algorithm for exploration in Low-Rank MDPs that is both computationally efficient and model-free, allowing for general function approximation and requiring no additional structural assumptions. Our algorithm, VoX, uses the notion of a generalized optimal design for the feature embedding as an eff
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;EffUNet&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24314;&#31569;&#21644;&#36947;&#36335;&#20998;&#21106;&#30340;&#26032;&#26550;&#26500;&#12290;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#22312;&#39532;&#33832;&#35832;&#22622;&#24030;&#24314;&#31569;&#29289;&#21644;&#36947;&#36335;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.03980</link><description>&lt;p&gt;
&#20351;&#29992;EffUNet&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24314;&#31569;&#21644;&#36947;&#36335;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Building and Road Segmentation Using EffUNet and Transfer Learning Approach. (arXiv:2307.03980v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;EffUNet&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24314;&#31569;&#21644;&#36947;&#36335;&#20998;&#21106;&#30340;&#26032;&#26550;&#26500;&#12290;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#22312;&#39532;&#33832;&#35832;&#22622;&#24030;&#24314;&#31569;&#29289;&#21644;&#36947;&#36335;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#20013;&#65292;&#20102;&#35299;&#22478;&#24066;&#23545;&#35937;&#65288;&#22914;&#20379;&#27700;&#12289;&#38081;&#36335;&#32447;&#12289;&#30005;&#21147;&#32447;&#36335;&#12289;&#24314;&#31569;&#29289;&#12289;&#36947;&#36335;&#31561;&#65289;&#30340;&#20449;&#24687;&#23545;&#22478;&#24066;&#35268;&#21010;&#26159;&#24517;&#35201;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25919;&#31574;&#21046;&#23450;&#32773;&#38656;&#35201;&#20102;&#35299;&#36825;&#20123;&#23545;&#35937;&#30340;&#20998;&#24067;&#12289;&#20301;&#32622;&#21644;&#23481;&#37327;&#65292;&#20197;&#20570;&#20986;&#26377;&#24433;&#21709;&#21147;&#30340;&#20915;&#31574;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#21355;&#26143;&#21644;&#26080;&#20154;&#26426;&#25293;&#25668;&#30340;&#33322;&#31354;&#22270;&#20687;&#23545;&#24314;&#31569;&#29289;&#21644;&#36947;&#36335;&#36827;&#34892;&#20998;&#21106;&#12290;&#35768;&#22810;&#19981;&#21516;&#30340;&#26550;&#26500;&#24050;&#32463;&#34987;&#25552;&#20986;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#20854;&#20013;UNet&#26159;&#20854;&#20013;&#20043;&#19968;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;&#26032;&#25552;&#20986;&#30340;EfficientNetV2&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#30340;&#32534;&#30721;&#22120;&#21644;UNet&#35299;&#30721;&#22120;&#26500;&#24314;&#20998;&#21106;&#22270;&#30340;&#26032;&#26550;&#26500;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#39532;&#33832;&#35832;&#22622;&#24030;&#24314;&#31569;&#29289;&#21644;&#36947;&#36335;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#22522;&#20934;&#20998;&#25968;&#65292;&#20998;&#21035;&#20026;0.8365&#21644;0.9153&#30340;mIOU&#12290;
&lt;/p&gt;
&lt;p&gt;
In city, information about urban objects such as water supply, railway lines, power lines, buildings, roads, etc., is necessary for city planning. In particular, information about the spread of these objects, locations and capacity is needed for the policymakers to make impactful decisions. This thesis aims to segment the building and roads from the aerial image captured by the satellites and UAVs. Many different architectures have been proposed for the semantic segmentation task and UNet being one of them. In this thesis, we propose a novel architecture based on Google's newly proposed EfficientNetV2 as an encoder for feature extraction with UNet decoder for constructing the segmentation map. Using this approach we achieved a benchmark score for the Massachusetts Building and Road dataset with an mIOU of 0.8365 and 0.9153 respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26080;&#28304;&#20809;&#32593;&#32476;&#25925;&#38556;&#30417;&#27979;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.03945</link><description>&lt;p&gt;
&#26080;&#28304;&#20809;&#32593;&#32476;&#20013;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#25925;&#38556;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fault Monitoring in Passive Optical Networks using Machine Learning Techniques. (arXiv:2307.03945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26080;&#28304;&#20809;&#32593;&#32476;&#25925;&#38556;&#30417;&#27979;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#28304;&#20809;&#32593;&#32476;&#31995;&#32479;&#23481;&#26131;&#20986;&#29616;&#22810;&#31181;&#25925;&#38556;&#65292;&#21253;&#25324;&#20809;&#32420;&#20999;&#26029;&#21644;&#20809;&#32593;&#32476;&#21333;&#20803;&#65288;ONU&#65289;&#30340;&#21457;&#23556;&#26426;/&#25509;&#25910;&#26426;&#25925;&#38556;&#12290;&#20809;&#32420;&#20999;&#26029;&#36896;&#25104;&#30340;&#20219;&#20309;&#26381;&#21153;&#20013;&#26029;&#37117;&#21487;&#33021;&#32473;&#26381;&#21153;&#25552;&#20379;&#21830;&#25110;&#36816;&#33829;&#21830;&#24102;&#26469;&#24040;&#22823;&#30340;&#32463;&#27982;&#25439;&#22833;&#12290;&#22312;&#20960;&#20046;&#31561;&#36317;&#31163;&#30340;&#20998;&#25903;&#32456;&#27490;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#25925;&#38556;ONU&#21464;&#24471;&#22256;&#38590;&#65292;&#22240;&#20026;&#20998;&#25903;&#30340;&#21453;&#23556;&#37325;&#21472;&#65292;&#24456;&#38590;&#26681;&#25454;&#20840;&#23616;&#21453;&#23556;&#20449;&#21495;&#21306;&#21035;&#20986;&#25925;&#38556;&#30340;&#20998;&#25903;&#12290;&#38543;&#30528;&#32593;&#32476;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#26080;&#28304;&#20809;&#32593;&#32476;&#31995;&#32479;&#20013;&#25925;&#38556;&#30417;&#27979;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#23548;&#33268;&#30417;&#27979;&#19981;&#22815;&#21487;&#38752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#29992;&#20110;&#26080;&#28304;&#20809;&#32593;&#32476;&#31995;&#32479;&#30340;&#25925;&#38556;&#30417;&#27979;&#65292;&#24182;&#20351;&#29992;&#23454;&#39564;&#20809;&#26102;&#22495;&#21453;&#23556;&#35745;&#65288;OTDR&#65289;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Passive optical network (PON) systems are vulnerable to a variety of failures, including fiber cuts and optical network unit (ONU) transmitter/receiver failures. Any service interruption caused by a fiber cut can result in huge financial losses for service providers or operators. Identifying the faulty ONU becomes difficult in the case of nearly equidistant branch terminations because the reflections from the branches overlap, making it difficult to distinguish the faulty branch given the global backscattering signal. With increasing network size, the complexity of fault monitoring in PON systems increases, resulting in less reliable monitoring. To address these challenges, we propose in this paper various machine learning (ML) approaches for fault monitoring in PON systems, and we validate them using experimental optical time domain reflectometry (OTDR) data.
&lt;/p&gt;</description></item><item><title>Rosko&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;&#65288;SpMM&#65289;&#20869;&#26680;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#34892;&#36339;&#36807;&#22806;&#31215;&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#35775;&#38382;&#38656;&#27714;&#30340;&#20943;&#23569;&#12290;&#23427;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30828;&#20214;&#29305;&#24615;&#65292;&#24182;&#19982;&#20854;&#20182;&#22806;&#31215;&#35843;&#24230;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#32988;&#36807;&#33258;&#21160;&#35843;&#20248;&#21644;&#25628;&#32034;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#33021;&#22312;&#19981;&#21516;&#31232;&#30095;&#24230;&#30340;&#30697;&#38453;&#19978;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.03930</link><description>&lt;p&gt;
Rosko: &#20351;&#29992;&#34892;&#36339;&#36807;&#22806;&#31215;&#36827;&#34892;&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;&#20869;&#26680;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Rosko: Row Skipping Outer Products for Sparse Matrix Multiplication Kernels. (arXiv:2307.03930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03930
&lt;/p&gt;
&lt;p&gt;
Rosko&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;&#65288;SpMM&#65289;&#20869;&#26680;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#34892;&#36339;&#36807;&#22806;&#31215;&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#35775;&#38382;&#38656;&#27714;&#30340;&#20943;&#23569;&#12290;&#23427;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30828;&#20214;&#29305;&#24615;&#65292;&#24182;&#19982;&#20854;&#20182;&#22806;&#31215;&#35843;&#24230;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#32988;&#36807;&#33258;&#21160;&#35843;&#20248;&#21644;&#25628;&#32034;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#33021;&#22312;&#19981;&#21516;&#31232;&#30095;&#24230;&#30340;&#30697;&#38453;&#19978;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Rosko&#65292;&#21363;&#20351;&#29992;&#34892;&#36339;&#36807;&#22806;&#31215;&#65288;Row Skipping Outer Products&#65289;&#65292;&#29992;&#20110;&#22312;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#35775;&#38382;&#38656;&#27714;&#26041;&#38754;&#25512;&#23548;&#20986;&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;&#65288;SpMM&#65289;&#20869;&#26680;&#12290;Rosko&#20801;&#35768;&#22312;&#31243;&#24207;&#25191;&#34892;&#26399;&#38388;&#36339;&#36807;&#25972;&#34892;&#35745;&#31639;&#65292;&#24182;&#20855;&#26377;&#20302;&#31232;&#30095;&#31649;&#29702;&#24320;&#38144;&#12290;&#25105;&#20204;&#20174;&#20998;&#26512;&#19978;&#25512;&#23548;&#20986;&#36866;&#24212;&#32473;&#23450;&#30828;&#20214;&#29305;&#24615;&#30340;&#31232;&#30095;CPU&#20869;&#26680;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#22788;&#29702;&#22120;&#26680;&#24515;&#24182;&#26368;&#23567;&#21270;&#25968;&#25454;&#31227;&#21160;&#65292;&#32780;&#26080;&#38656;&#33258;&#21160;&#35843;&#20248;&#25110;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#12290;Rosko&#21487;&#20197;&#19982;&#20854;&#20182;&#22806;&#31215;&#35843;&#24230;&#26041;&#27861;&#38598;&#25104;&#65292;&#36890;&#36807;&#20351;&#29992;Rosko&#30340;&#25171;&#21253;&#26684;&#24335;&#26469;&#36339;&#36807;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#65292;&#20174;&#32780;&#21033;&#29992;&#34892;&#36339;&#36807;&#12290;&#22312;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#24037;&#20316;&#36127;&#36733;&#19979;&#65292;Rosko&#20869;&#26680;&#22312;&#30495;&#23454;&#30828;&#20214;&#19978;&#32988;&#36807;&#29616;&#26377;&#30340;&#33258;&#21160;&#35843;&#20248;&#21644;&#22522;&#20110;&#25628;&#32034;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#20379;&#24212;&#21830;&#20248;&#21270;&#24211;&#12290;&#23545;&#20110;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36890;&#24120;&#20986;&#29616;&#30340;&#31232;&#30095;&#24230;&#20174;65&#65285;&#21040;99.8&#65285;&#30340;&#30697;&#38453;&#65292;Rosko&#20869;&#26680;&#21487;&#20197;&#23454;&#29616;...&#65288;&#25688;&#35201;&#26410;&#23436;&#25972;&#65289;
&lt;/p&gt;
&lt;p&gt;
We propose Rosko -- row skipping outer products -- for deriving sparse matrix multiplication (SpMM) kernels in reducing computation and memory access requirements of deep neural networks (DNNs). Rosko allows skipping of entire row computations during program execution with low sparsity-management overheads. We analytically derive sparse CPU kernels that adapt to given hardware characteristics to effectively utilize processor cores and minimize data movement without the need for auto-tuning or search space exploration. Rosko can be integrated with other outer product scheduling methods, allowing them to leverage row skipping by using Rosko's packing format to skip unnecessary computation.  Rosko kernels outperform existing auto-tuning and search-based solutions as well as state-of-the-art vendor-optimized libraries on real hardware across a variety of neural network workloads. For matrices with sparsities ranging from 65% to 99.8% typically found in machine learning, Rosko kernels achie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#20844;&#24179;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35752;&#35770;&#20102;&#25552;&#39640;GNNs&#20844;&#24179;&#24615;&#30340;&#25216;&#26415;&#65292;&#24182;&#20171;&#32461;&#20102;&#20844;&#24179;&#24230;&#35780;&#20272;&#25351;&#26631;&#20998;&#31867;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03929</link><description>&lt;p&gt;
&#20844;&#24179;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fairness-Aware Graph Neural Networks: A Survey. (arXiv:2307.03929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03929
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#20844;&#24179;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35752;&#35770;&#20102;&#25552;&#39640;GNNs&#20844;&#24179;&#24615;&#30340;&#25216;&#26415;&#65292;&#24182;&#20171;&#32461;&#20102;&#20844;&#24179;&#24230;&#35780;&#20272;&#25351;&#26631;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20195;&#34920;&#33021;&#21147;&#21644;&#22312;&#35768;&#22810;&#22522;&#26412;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#39044;&#27979;&#24615;&#33021;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;GNNs&#30001;&#20110;&#22522;&#30784;&#22270;&#25968;&#25454;&#21644;&#24222;&#22823;&#30340;GNN&#27169;&#22411;&#20013;&#24515;&#30340;&#22522;&#26412;&#32858;&#21512;&#26426;&#21046;&#30340;&#32467;&#26524;&#65292;&#23384;&#22312;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#23519;&#24182;&#20998;&#31867;&#20102;&#25552;&#39640;GNNs&#20844;&#24179;&#24615;&#30340;&#20844;&#24179;&#25216;&#26415;&#12290;&#20808;&#21069;&#20851;&#20110;&#20844;&#24179;GNN&#27169;&#22411;&#21644;&#25216;&#26415;&#30340;&#24037;&#20316;&#22312;&#39044;&#22788;&#29702;&#27493;&#39588;&#12289;&#35757;&#32451;&#36807;&#31243;&#20013;&#25110;&#21518;&#22788;&#29702;&#38454;&#27573;&#26159;&#21542;&#20851;&#27880;&#25552;&#39640;&#20844;&#24179;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#25216;&#26415;&#22914;&#20309;&#22312;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#20351;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#21508;&#33258;&#30340;&#20248;&#21183;&#21644;&#30452;&#35273;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#20844;&#24179;&#24230;&#35780;&#20272;&#25351;&#26631;&#20998;&#31867;&#27861;&#65292;&#21253;&#25324;&#22270;&#32423;&#20844;&#24179;&#24615;&#12289;&#37051;&#22495;&#32423;&#20844;&#24179;&#24615;&#12289;&#23884;&#20837;&#32423;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#32423;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become increasingly important due to their representational power and state-of-the-art predictive performance on many fundamental learning tasks. Despite this success, GNNs suffer from fairness issues that arise as a result of the underlying graph data and the fundamental aggregation mechanism that lies at the heart of the large class of GNN models. In this article, we examine and categorize fairness techniques for improving the fairness of GNNs. Previous work on fair GNN models and techniques are discussed in terms of whether they focus on improving fairness during a preprocessing step, during training, or in a post-processing phase. Furthermore, we discuss how such techniques can be used together whenever appropriate, and highlight the advantages and intuition as well. We also introduce an intuitive taxonomy for fairness evaluation metrics including graph-level fairness, neighborhood-level fairness, embedding-level fairness, and prediction-level fair
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#24555;&#36895;&#30340;&#32463;&#39564;&#22330;&#26223;&#25552;&#21462;&#31639;&#27861;&#65292;&#19968;&#31181;&#35782;&#21035;&#20043;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#24182;&#25552;&#20379;&#22330;&#26223;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#31034;&#65292;&#21478;&#19968;&#31181;&#20174;&#24050;&#23454;&#29616;&#30340;&#19990;&#30028;&#29366;&#24577;&#20013;&#36873;&#25321;&#37325;&#35201;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#19982;&#39640;&#38454;&#26679;&#26412;&#30697;&#19968;&#33268;&#65292;&#36825;&#20123;&#31639;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#36866;&#29992;&#20110;&#19968;&#33268;&#30340;&#22522;&#20110;&#22330;&#26223;&#30340;&#24314;&#27169;&#21644;&#39640;&#32500;&#25968;&#20540;&#31215;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.03927</link><description>&lt;p&gt;
&#24555;&#36895;&#32463;&#39564;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Fast Empirical Scenarios. (arXiv:2307.03927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03927
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#24555;&#36895;&#30340;&#32463;&#39564;&#22330;&#26223;&#25552;&#21462;&#31639;&#27861;&#65292;&#19968;&#31181;&#35782;&#21035;&#20043;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#24182;&#25552;&#20379;&#22330;&#26223;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#31034;&#65292;&#21478;&#19968;&#31181;&#20174;&#24050;&#23454;&#29616;&#30340;&#19990;&#30028;&#29366;&#24577;&#20013;&#36873;&#25321;&#37325;&#35201;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#19982;&#39640;&#38454;&#26679;&#26412;&#30697;&#19968;&#33268;&#65292;&#36825;&#20123;&#31639;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#36866;&#29992;&#20110;&#19968;&#33268;&#30340;&#22522;&#20110;&#22330;&#26223;&#30340;&#24314;&#27169;&#21644;&#39640;&#32500;&#25968;&#20540;&#31215;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24076;&#26395;&#20174;&#22823;&#22411;&#21644;&#39640;&#32500;&#38754;&#26495;&#25968;&#25454;&#20013;&#25552;&#21462;&#19968;&#23567;&#37096;&#20998;&#19982;&#26679;&#26412;&#30697;&#19968;&#33268;&#30340;&#20195;&#34920;&#24615;&#22330;&#26223;&#12290;&#22312;&#20004;&#31181;&#26032;&#31639;&#27861;&#20013;&#65292;&#31532;&#19968;&#31181;&#31639;&#27861;&#35782;&#21035;&#20043;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#31034;&#12290;&#31532;&#20108;&#31181;&#31639;&#27861;&#20174;&#24050;&#23454;&#29616;&#30340;&#19990;&#30028;&#29366;&#24577;&#20013;&#36873;&#25321;&#37325;&#35201;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#19982;&#39640;&#38454;&#26679;&#26412;&#30697;&#20449;&#24687;&#19968;&#33268;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#21487;&#29992;&#20110;&#19968;&#33268;&#30340;&#22522;&#20110;&#22330;&#26223;&#30340;&#24314;&#27169;&#21644;&#39640;&#32500;&#25968;&#20540;&#31215;&#20998;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#21644;&#22312;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#25903;&#25345;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to extract a small number of representative scenarios from large and high-dimensional panel data that are consistent with sample moments. Among two novel algorithms, the first identifies scenarios that have not been observed before, and comes with a scenario-based representation of covariance matrices. The second proposal picks important data points from states of the world that have already realized, and are consistent with higher-order sample moment information. Both algorithms are efficient to compute, and lend themselves to consistent scenario-based modeling and high-dimensional numerical integration. Extensive numerical benchmarking studies and an application in portfolio optimization favor the proposed algorithms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#20248;&#21270;&#30340;PINN&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#20132;&#36890;&#23494;&#24230;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21019;&#24314;&#21644;&#35299;&#20915;&#22810;&#20010;&#36741;&#21161;&#20219;&#21153;&#26469;&#36741;&#21161;&#27714;&#35299;&#20027;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#26102;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03920</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20219;&#21153;&#20248;&#21270;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#29992;&#20110;&#20132;&#36890;&#23494;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Training Physics-Informed Neural Networks via Multi-Task Optimization for Traffic Density Prediction. (arXiv:2307.03920v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#20248;&#21270;&#30340;PINN&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#20132;&#36890;&#23494;&#24230;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21019;&#24314;&#21644;&#35299;&#20915;&#22810;&#20010;&#36741;&#21161;&#20219;&#21153;&#26469;&#36741;&#21161;&#27714;&#35299;&#20027;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#26102;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#23558;&#26576;&#20010;&#32473;&#23450;&#25968;&#25454;&#38598;&#25152;&#25551;&#36848;&#30340;&#29289;&#29702;&#23450;&#24459;&#65288;&#22914;&#20559;&#24494;&#20998;&#26041;&#31243;&#65289;&#32467;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#12290;PINNs&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#29992;&#20316;PDE&#30340;&#35299;&#36817;&#20284;&#22120;&#65292;&#32780;PDE&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#25351;&#23548;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#20197;&#24212;&#23545;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25439;&#22833;&#20989;&#25968;&#30001;&#31070;&#32463;&#32593;&#32476;&#21644;&#29289;&#29702;&#23450;&#24459;&#20004;&#37096;&#20998;&#32452;&#25104;&#65292;&#35757;&#32451;PINNs&#26159;&#19968;&#39033;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#20248;&#21270;&#65288;MTO&#65289;&#33539;&#24335;&#30340;&#26032;PINN&#35757;&#32451;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#38500;&#20102;&#32473;&#23450;&#30340;&#65288;&#20027;&#65289;&#20219;&#21153;&#22806;&#65292;&#36824;&#21019;&#24314;&#20102;&#22810;&#20010;&#36741;&#21161;&#20219;&#21153;&#24182;&#19968;&#36215;&#35299;&#20915;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#20256;&#36882;&#27714;&#35299;&#19968;&#20010;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#26377;&#29992;&#30693;&#35782;&#65292;&#26469;&#36741;&#21161;&#27714;&#35299;&#20027;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) are a newly emerging research frontier in machine learning, which incorporate certain physical laws that govern a given data set, e.g., those described by partial differential equations (PDEs), into the training of the neural network (NN) based on such a data set. In PINNs, the NN acts as the solution approximator for the PDE while the PDE acts as the prior knowledge to guide the NN training, leading to the desired generalization performance of the NN when facing the limited availability of training data. However, training PINNs is a non-trivial task largely due to the complexity of the loss composed of both NN and physical law parts. In this work, we propose a new PINN training framework based on the multi-task optimization (MTO) paradigm. Under this framework, multiple auxiliary tasks are created and solved together with the given (main) task, where the useful knowledge from solving one task is transferred in an adaptive mode to assist in solv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;Deep Q-Network&#19982;&#22810;&#31867;&#20998;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#39044;&#27979;&#36130;&#21153;&#22256;&#22659;&#31561;&#39046;&#22495;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.03908</link><description>&lt;p&gt;
&#23558;Deep Q-Network&#19982;&#22810;&#31867;&#20998;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Incorporating Deep Q -- Network with Multiclass Classification Algorithms. (arXiv:2307.03908v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;Deep Q-Network&#19982;&#22810;&#31867;&#20998;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#39044;&#27979;&#36130;&#21153;&#22256;&#22659;&#31561;&#39046;&#22495;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;Deep Q-Network (DQN)&#26469;&#25552;&#39640;&#22810;&#31867;&#20998;&#31867;&#31639;&#27861;&#30340;&#21151;&#33021;&#24615;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;&#26469;&#33258;Kaggle&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21019;&#24314;&#19968;&#20010;&#23558;DQN&#19982;&#29616;&#26377;&#26377;&#30417;&#30563;&#22810;&#31867;&#20998;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#30340;&#21457;&#29616;&#23558;&#24102;&#26469;&#23545;&#22914;&#20309;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#25552;&#39640;&#22810;&#31867;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#35265;&#35299;&#12290;&#36825;&#20123;&#31574;&#30053;&#24050;&#32463;&#22312;&#22270;&#20687;&#35782;&#21035;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#31561;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#36824;&#20391;&#37325;&#20110;&#39044;&#27979;&#20844;&#21496;&#30340;&#36130;&#21153;&#22256;&#22659;&#20197;&#21450;Deep Q-Network&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#35782;&#21035;&#21487;&#33021;&#36935;&#21040;&#36130;&#21153;&#22256;&#22659;&#30340;&#20225;&#19994;&#26159;&#37329;&#34701;&#21644;&#39118;&#38505;&#31649;&#29702;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#24403;&#20225;&#19994;&#38754;&#20020;&#20005;&#37325;&#25361;&#25112;&#26102;&#65292;&#26080;&#27861;&#32500;&#25345;&#36816;&#33829;&#24182;&#23653;&#34892;&#36130;&#21153;&#36131;&#20219;&#65292;&#23601;&#34987;&#35748;&#20026;&#22788;&#20110;&#36130;&#21153;&#22256;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore how Deep Q-Network (DQN) might improve the functionality of multiclass classification algorithms. We will use a benchmark dataset from Kaggle to create a framework incorporating DQN with existing supervised multiclass classification algorithms. The findings of this study will bring insight into how deep reinforcement learning strategies may be used to increase multiclass classification accuracy. They have been used in a number of fields, including image recognition, natural language processing, and bioinformatics. This study is focused on the prediction of financial distress in companies in addition to the wider application of Deep Q-Network in multiclass classification. Identifying businesses that are likely to experience financial distress is a crucial task in the fields of finance and risk management. Whenever a business experiences serious challenges keeping its operations going and meeting its financial responsibilities, it is said to be in financial dist
&lt;/p&gt;</description></item><item><title>ScriptWorld&#26159;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#25945;&#25480;&#26234;&#33021;&#20307;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#26085;&#24120;&#20219;&#21153;&#21644;&#24120;&#35782;&#30693;&#35782;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#33050;&#26412;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#20114;&#21160;&#24335;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#26694;&#26550;&#65292;&#22312;10&#20010;&#26085;&#24120;&#27963;&#21160;&#20013;&#25552;&#20379;&#20102;&#28216;&#25103;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#23545;&#29615;&#22659;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#24341;&#20837;&#33050;&#26412;&#24335;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#21487;&#20197;&#26377;&#25928;&#25552;&#20379;&#24120;&#35782;&#30693;&#35782;&#24182;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03906</link><description>&lt;p&gt;
ScriptWorld: &#29992;&#20110;&#23398;&#20064;&#36807;&#31243;&#24615;&#30693;&#35782;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
ScriptWorld: Text Based Environment For Learning Procedural Knowledge. (arXiv:2307.03906v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03906
&lt;/p&gt;
&lt;p&gt;
ScriptWorld&#26159;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#25945;&#25480;&#26234;&#33021;&#20307;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#26085;&#24120;&#20219;&#21153;&#21644;&#24120;&#35782;&#30693;&#35782;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#33050;&#26412;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#20114;&#21160;&#24335;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#26694;&#26550;&#65292;&#22312;10&#20010;&#26085;&#24120;&#27963;&#21160;&#20013;&#25552;&#20379;&#20102;&#28216;&#25103;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#23545;&#29615;&#22659;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#24341;&#20837;&#33050;&#26412;&#24335;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#21487;&#20197;&#26377;&#25928;&#25552;&#20379;&#24120;&#35782;&#30693;&#35782;&#24182;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#28216;&#25103;&#20026;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#20307;&#24320;&#21457;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#24120;&#35782;&#30693;&#35782;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#29616;&#26377;&#30340;&#25991;&#26412;&#29615;&#22659;&#36890;&#24120;&#20381;&#36182;&#20110;&#34394;&#26500;&#30340;&#24773;&#26223;&#21644;&#35282;&#33394;&#26469;&#21019;&#24314;&#28216;&#25103;&#26694;&#26550;&#65292;&#19982;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#30456;&#24046;&#29978;&#36828;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ScriptWorld&#65306;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#25945;&#25480;&#26234;&#33021;&#20307;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#26085;&#24120;&#20219;&#21153;&#21644;&#24120;&#35782;&#30693;&#35782;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#33050;&#26412;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#20114;&#21160;&#24335;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;10&#20010;&#26085;&#24120;&#27963;&#21160;&#30340;&#28216;&#25103;&#29615;&#22659;&#65292;&#24182;&#23545;&#25152;&#25552;&#20986;&#30340;&#29615;&#22659;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#27169;&#22411;/&#26234;&#33021;&#20307;&#26469;&#29609;ScriptWorld&#20013;&#30340;&#28216;&#25103;&#12290;&#20026;&#20102;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#21033;&#29992;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#29305;&#24449;&#26469;&#36741;&#21161;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;ScriptWorld&#20013;&#24341;&#20837;&#33050;&#26412;&#24335;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#21487;&#20197;&#26377;&#25928;&#25552;&#20379;&#24120;&#35782;&#30693;&#35782;&#21644;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based games provide a framework for developing natural language understanding and commonsense knowledge about the world in reinforcement learning based agents. Existing text-based environments often rely on fictional situations and characters to create a gaming framework and are far from real-world scenarios. In this paper, we introduce ScriptWorld: a text-based environment for teaching agents about real-world daily chores and hence imparting commonsense knowledge. To the best of our knowledge, it is the first interactive text-based gaming framework that consists of daily real-world human activities designed using scripts dataset. We provide gaming environments for 10 daily activities and perform a detailed analysis of the proposed environment. We develop RL-based baseline models/agents to play the games in Scriptworld. To understand the role of language models in such environments, we leverage features obtained from pre-trained language models in the RL agents. Our experiments sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#31867;&#21035;&#21028;&#21035;&#21644;&#32858;&#31867;&#32467;&#26500;&#20445;&#25345;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20855;&#26377;&#26174;&#33879;&#31867;&#21035;&#21644;&#32858;&#31867;&#32467;&#26500;&#24046;&#24322;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#33391;&#22909;&#30340;&#20998;&#31867;&#21644;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03902</link><description>&lt;p&gt;
&#21516;&#26102;&#20445;&#30041;&#31867;&#21035;&#21644;&#32858;&#31867;&#32467;&#26500;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Feature selection simultaneously preserving both class and cluster structures. (arXiv:2307.03902v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#31867;&#21035;&#21028;&#21035;&#21644;&#32858;&#31867;&#32467;&#26500;&#20445;&#25345;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20855;&#26377;&#26174;&#33879;&#31867;&#21035;&#21644;&#32858;&#31867;&#32467;&#26500;&#24046;&#24322;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#33391;&#22909;&#30340;&#20998;&#31867;&#21644;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#21644;&#32858;&#31867;&#32467;&#26500;&#20855;&#26377;&#26174;&#33879;&#24046;&#24322;&#26102;&#65292;&#20165;&#38024;&#23545;&#31867;&#21035;&#30340;&#29305;&#24449;&#36873;&#25321;&#20250;&#23548;&#33268;&#32858;&#31867;&#24615;&#33021;&#19981;&#20339;&#65292;&#21516;&#26679;&#22320;&#65292;&#20165;&#32771;&#34385;&#32858;&#31867;&#32467;&#26500;&#30340;&#29305;&#24449;&#36873;&#25321;&#20063;&#20250;&#23548;&#33268;&#20998;&#31867;&#24615;&#33021;&#19981;&#20339;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25991;&#29486;&#20013;&#23578;&#26080;&#21516;&#26102;&#32771;&#34385;&#31867;&#21035;&#21028;&#21035;&#21644;&#32858;&#31867;&#32467;&#26500;&#20445;&#25345;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#26088;&#22312;&#25972;&#21512;&#31867;&#21035;&#21028;&#21035;&#21644;&#32467;&#26500;&#20445;&#25345;&#20004;&#26041;&#38754;&#30340;&#30446;&#26631;&#26469;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#38500;&#20102;&#35780;&#20272;&#20856;&#22411;&#30340;&#20998;&#31867;&#38382;&#39064;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#30340;&#27874;&#27573;&#36873;&#25321;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#21487;&#20197;&#22768;&#31216;&#65292;&#25152;&#25552;&#20986;&#30340;&#29305;&#24449;/&#27874;&#27573;&#36873;&#25321;&#26041;&#27861;&#21487;&#20197;&#36873;&#25321;&#36866;&#29992;&#20110;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
When a data set has significant differences in its class and cluster structure, selecting features aiming only at the discrimination of classes would lead to poor clustering performance, and similarly, feature selection aiming only at preserving cluster structures would lead to poor classification performance. To the best of our knowledge, a feature selection method that simultaneously considers class discrimination and cluster structure preservation is not available in the literature. In this paper, we have tried to bridge this gap by proposing a neural network-based feature selection method that focuses both on class discrimination and structure preservation in an integrated manner. In addition to assessing typical classification problems, we have investigated its effectiveness on band selection in hyperspectral images. Based on the results of the experiments, we may claim that the proposed feature/band selection can select a subset of features that is good for both classification an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#29289;&#29702;&#23398;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#24182;&#36890;&#36807;&#19987;&#23478;&#30340;&#27880;&#37322;&#26469;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#35813;&#26041;&#27861;&#22312;&#29289;&#29702;&#23398;&#39046;&#22495;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21478;&#22806;&#65292;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#20027;&#21160;&#23398;&#20064;&#19982;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#25972;&#21512;&#65292;&#35774;&#24819;&#20102;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#21327;&#21516;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2307.03899</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#65306;&#20174;101&#21040;&#36827;&#23637;&#19982;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Active Learning in Physics: From 101, to Progress, and Perspective. (arXiv:2307.03899v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29289;&#29702;&#23398;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#24182;&#36890;&#36807;&#19987;&#23478;&#30340;&#27880;&#37322;&#26469;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#35813;&#26041;&#27861;&#22312;&#29289;&#29702;&#23398;&#39046;&#22495;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21478;&#22806;&#65292;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#20027;&#21160;&#23398;&#20064;&#19982;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#25972;&#21512;&#65292;&#35774;&#24819;&#20102;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#21327;&#21516;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26159;&#19968;&#31867;&#22312;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#20043;&#21069;&#23384;&#22312;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#19981;&#21516;&#65292;AL&#36890;&#36807;&#36845;&#20195;&#36873;&#25321;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#30001;&#19987;&#23478;&#36827;&#34892;&#27880;&#37322;&#12290;&#36825;&#20010;&#21327;&#35758;&#26088;&#22312;&#20248;&#20808;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#65292;&#20351;&#24471;&#27169;&#22411;&#24615;&#33021;&#30456;&#23545;&#20110;&#20351;&#29992;&#20840;&#37096;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#26469;&#35828;&#26377;&#25152;&#25552;&#21319;&#12290;&#36817;&#24180;&#26469;&#65292;AL&#22312;&#29289;&#29702;&#23398;&#39046;&#22495;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#23545;AL&#30340;&#29702;&#35770;&#36827;&#34892;&#20840;&#38754;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#20171;&#32461;&#65292;&#24182;&#22238;&#39038;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;AL&#19982;&#37327;&#23376;ML&#30340;&#28508;&#22312;&#25972;&#21512;&#65292;&#35774;&#24819;&#20102;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#21327;&#21516;&#34701;&#21512;&#65292;&#32780;&#19981;&#20165;&#20165;&#25226;AL&#35270;&#20026;&#32463;&#20856;ML&#22312;&#37327;&#23376;&#39046;&#22495;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Learning (AL) is a family of machine learning (ML) algorithms that predates the current era of artificial intelligence. Unlike traditional approaches that require labeled samples for training, AL iteratively selects unlabeled samples to be annotated by an expert. This protocol aims to prioritize the most informative samples, leading to improved model performance compared to training with all labeled samples. In recent years, AL has gained increasing attention, particularly in the field of physics. This paper presents a comprehensive and accessible introduction to the theory of AL reviewing the latest advancements across various domains. Additionally, we explore the potential integration of AL with quantum ML, envisioning a synergistic fusion of these two fields rather than viewing AL as a mere extension of classical ML into the quantum realm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03887</link><description>&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#28145;&#24230;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#28165;&#26970;&#22320;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#24402;&#22240;&#20110;&#25968;&#25454;&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#65288;ProtoPNet&#65289;&#65292;&#23427;&#22522;&#20110;&#36755;&#20837;&#30340;&#26377;&#24847;&#20041;&#37096;&#20998;&#26469;&#23581;&#35797;&#20998;&#31867;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32463;&#24120;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#21551;&#21457;&#65292;&#36890;&#36807;&#22312;CUB-200-2011&#25968;&#25454;&#38598;&#19978;&#25910;&#38598;&#20154;&#31867;&#21407;&#22411;&#36136;&#37327;&#30340;1-5&#20998;&#32423;&#27880;&#37322;&#65292;&#26500;&#24314;&#19968;&#20010;&#23398;&#20064;&#35782;&#21035;&#38750;&#34394;&#20551;&#21407;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#65288;R3-ProtoPNet&#65289;&#65292;&#35813;&#32593;&#32476;&#22312;ProtoPNet&#35757;&#32451;&#24490;&#29615;&#20013;&#22686;&#21152;&#20102;&#19977;&#20010;&#39069;&#22806;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the prototypical part network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, this method often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. In place of a full RL update, we propose the reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The fir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#23558;&#20808;&#39564;&#30693;&#35782;&#21644;&#31526;&#21495;&#35268;&#21017;&#20197;&#26631;&#31614;&#32422;&#26463;&#30340;&#24418;&#24335;&#34920;&#36798;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27604;&#36739;&#27491;&#21017;&#21270;&#21644;&#32422;&#26463;&#25512;&#29702;&#20004;&#31181;&#24120;&#35265;&#30340;&#32534;&#30721;&#26631;&#31614;&#32422;&#26463;&#30340;&#31574;&#30053;&#65292;&#21457;&#29616;&#27491;&#21017;&#21270;&#32553;&#23567;&#20102;&#27867;&#21270;&#24046;&#36317;&#20294;&#24341;&#20837;&#20102;&#23545;&#27425;&#20248;&#27169;&#22411;&#30340;&#20559;&#32622;&#65292;&#32780;&#32422;&#26463;&#25512;&#29702;&#36890;&#36807;&#32416;&#27491;&#27169;&#22411;&#30340;&#36829;&#35268;&#34892;&#20026;&#23558;&#36829;&#35268;&#34892;&#20026;&#36716;&#21270;&#20026;&#20248;&#21183;&#12290;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#30340;&#21487;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#32422;&#26463;&#25512;&#29702;&#26469;&#34917;&#20607;&#27491;&#21017;&#21270;&#24341;&#20837;&#30340;&#20559;&#32622;&#30340;&#26465;&#20214;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#26368;&#20248;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.03886</link><description>&lt;p&gt;
&#20851;&#20110;&#27491;&#21017;&#21270;&#21644;&#26631;&#31614;&#32422;&#26463;&#25512;&#29702;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Regularization and Inference with Label Constraints. (arXiv:2307.03886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#23558;&#20808;&#39564;&#30693;&#35782;&#21644;&#31526;&#21495;&#35268;&#21017;&#20197;&#26631;&#31614;&#32422;&#26463;&#30340;&#24418;&#24335;&#34920;&#36798;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27604;&#36739;&#27491;&#21017;&#21270;&#21644;&#32422;&#26463;&#25512;&#29702;&#20004;&#31181;&#24120;&#35265;&#30340;&#32534;&#30721;&#26631;&#31614;&#32422;&#26463;&#30340;&#31574;&#30053;&#65292;&#21457;&#29616;&#27491;&#21017;&#21270;&#32553;&#23567;&#20102;&#27867;&#21270;&#24046;&#36317;&#20294;&#24341;&#20837;&#20102;&#23545;&#27425;&#20248;&#27169;&#22411;&#30340;&#20559;&#32622;&#65292;&#32780;&#32422;&#26463;&#25512;&#29702;&#36890;&#36807;&#32416;&#27491;&#27169;&#22411;&#30340;&#36829;&#35268;&#34892;&#20026;&#23558;&#36829;&#35268;&#34892;&#20026;&#36716;&#21270;&#20026;&#20248;&#21183;&#12290;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#30340;&#21487;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#32422;&#26463;&#25512;&#29702;&#26469;&#34917;&#20607;&#27491;&#21017;&#21270;&#24341;&#20837;&#30340;&#20559;&#32622;&#30340;&#26465;&#20214;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#26368;&#20248;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#39564;&#30693;&#35782;&#21644;&#31526;&#21495;&#35268;&#21017;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36890;&#24120;&#20197;&#26631;&#31614;&#32422;&#26463;&#30340;&#24418;&#24335;&#34920;&#36798;&#65292;&#29305;&#21035;&#26159;&#22312;&#32467;&#26500;&#39044;&#27979;&#38382;&#39064;&#20013;&#12290;&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#20854;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#20004;&#31181;&#24120;&#35265;&#30340;&#32534;&#30721;&#26631;&#31614;&#32422;&#26463;&#30340;&#31574;&#30053;&#65306;&#24102;&#32422;&#26463;&#30340;&#27491;&#21017;&#21270;&#21644;&#32422;&#26463;&#25512;&#29702;&#12290;&#23545;&#20110;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#36890;&#36807;&#25490;&#38500;&#19982;&#32422;&#26463;&#19981;&#19968;&#33268;&#30340;&#27169;&#22411;&#26469;&#32553;&#23567;&#27867;&#21270;&#24046;&#36317;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#27491;&#21017;&#21270;&#23545;&#23567;&#36829;&#35268;&#30340;&#20559;&#22909;&#23548;&#33268;&#20102;&#23545;&#27425;&#20248;&#27169;&#22411;&#30340;&#20559;&#32622;&#12290;&#23545;&#20110;&#32422;&#26463;&#25512;&#29702;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#36890;&#36807;&#32416;&#27491;&#27169;&#22411;&#30340;&#36829;&#35268;&#34892;&#20026;&#26469;&#20943;&#23567;&#24635;&#20307;&#39118;&#38505;&#65292;&#24182;&#23558;&#36829;&#35268;&#34892;&#20026;&#36716;&#21270;&#20026;&#20248;&#21183;&#12290;&#37492;&#20110;&#36825;&#20123;&#24046;&#24322;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#32422;&#26463;&#25512;&#29702;&#26469;&#34917;&#20607;&#27491;&#21017;&#21270;&#24341;&#20837;&#30340;&#20559;&#32622;&#30340;&#26465;&#20214;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#26368;&#20248;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior knowledge and symbolic rules in machine learning are often expressed in the form of label constraints, especially in structured prediction problems. In this work, we compare two common strategies for encoding label constraints in a machine learning pipeline, regularization with constraints and constrained inference, by quantifying their impact on model performance. For regularization, we show that it narrows the generalization gap by precluding models that are inconsistent with the constraints. However, its preference for small violations introduces a bias toward a suboptimal model. For constrained inference, we show that it reduces the population risk by correcting a model's violation, and hence turns the violation into an advantage. Given these differences, we further explore the use of two approaches together and propose conditions for constrained inference to compensate for the bias introduced by regularization, aiming to improve both the model complexity and optimal risk.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22122;&#22768;&#24352;&#37327;&#29615;&#36924;&#36817;&#26041;&#27861;&#35745;&#31639;&#32452;&#21512;&#20248;&#21270;&#30340;&#21464;&#20998;&#37327;&#23376;&#26412;&#24449;&#27714;&#35299;&#22120;&#26799;&#24230;&#65292;&#35299;&#20915;&#20102;VQE&#31639;&#27861;&#21463;&#38480;&#20110;&#32463;&#20856;&#19981;&#21487;&#22788;&#29702;&#30340;&#26799;&#24230;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03884</link><description>&lt;p&gt;
&#20351;&#29992;&#22122;&#22768;&#24352;&#37327;&#29615;&#36924;&#36817;&#26041;&#27861;&#35745;&#31639;&#32452;&#21512;&#20248;&#21270;&#30340;&#21464;&#20998;&#37327;&#23376;&#26412;&#24449;&#27714;&#35299;&#22120;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Noisy Tensor Ring approximation for computing gradients of Variational Quantum Eigensolver for Combinatorial Optimization. (arXiv:2307.03884v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03884
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22122;&#22768;&#24352;&#37327;&#29615;&#36924;&#36817;&#26041;&#27861;&#35745;&#31639;&#32452;&#21512;&#20248;&#21270;&#30340;&#21464;&#20998;&#37327;&#23376;&#26412;&#24449;&#27714;&#35299;&#22120;&#26799;&#24230;&#65292;&#35299;&#20915;&#20102;VQE&#31639;&#27861;&#21463;&#38480;&#20110;&#32463;&#20856;&#19981;&#21487;&#22788;&#29702;&#30340;&#26799;&#24230;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#21644;&#21464;&#20998;&#37327;&#23376;&#26412;&#24449;&#27714;&#35299;&#22120;&#65288;VQE&#65289;&#65292;&#24050;&#32463;&#22312;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#28508;&#22312;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#21463;&#21040;&#32463;&#20856;&#19981;&#21487;&#22788;&#29702;&#30340;&#26799;&#24230;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#32463;&#20856;&#26799;&#24230;&#35745;&#31639;&#26041;&#27861;&#26469;&#35299;&#20915;VQE&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21442;&#25968;&#20559;&#31227;&#35268;&#21017;&#65292;&#20294;&#26159;&#20351;&#29992;&#24352;&#37327;&#29615;&#36924;&#36817;&#26469;&#35745;&#31639;&#30005;&#36335;&#30340;&#26399;&#26395;&#20540;&#12290;&#30005;&#36335;&#20013;&#30340;&#21442;&#25968;&#21270;&#38376;&#36890;&#36807;&#22312;&#24352;&#37327;&#29615;&#30340;&#33258;&#30001;&#36793;&#19978;&#25910;&#32553;&#30697;&#38453;&#26469;&#36716;&#25442;&#24352;&#37327;&#29615;&#12290;&#34429;&#28982;&#21333;&#27604;&#29305;&#38376;&#19981;&#20250;&#25913;&#21464;&#29615;&#30340;&#32467;&#26500;&#65292;&#20294;&#20004;&#27604;&#29305;&#26059;&#36716;&#30340;&#29366;&#24577;&#21464;&#25442;&#36890;&#36807;&#25130;&#26029;&#22855;&#24322;&#20540;&#26469;&#35780;&#20272;&#65292;&#20174;&#32780;&#20445;&#25345;&#20102;&#24352;&#37327;&#29615;&#30340;&#32467;&#26500;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational Quantum algorithms, especially Quantum Approximate Optimization and Variational Quantum Eigensolver (VQE) have established their potential to provide computational advantage in the realm of combinatorial optimization. However, these algorithms suffer from classically intractable gradients limiting the scalability. This work addresses the scalability challenge for VQE by proposing a classical gradient computation method which utilizes the parameter shift rule but computes the expected values from the circuits using a tensor ring approximation. The parametrized gates from the circuit transform the tensor ring by contracting the matrix along the free edges of the tensor ring. While the single qubit gates do not alter the ring structure, the state transformations from the two qubit rotations are evaluated by truncating the singular values thereby preserving the structure of the tensor ring and reducing the computational complexity. This variation of the Matrix product state app
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#35299;&#37322;&#21644;&#35299;&#35835;&#20379;&#24212;&#38142;&#20248;&#21270;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#36890;&#36807;&#23450;&#37327;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#24110;&#21161;&#20225;&#19994;&#36816;&#33829;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#20449;&#20219;&#20248;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03875</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20379;&#24212;&#38142;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Supply Chain Optimization. (arXiv:2307.03875v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03875
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#35299;&#37322;&#21644;&#35299;&#35835;&#20379;&#24212;&#38142;&#20248;&#21270;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#36890;&#36807;&#23450;&#37327;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#24110;&#21161;&#20225;&#19994;&#36816;&#33829;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#20449;&#20219;&#20248;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20379;&#24212;&#38142;&#25805;&#20316;&#28041;&#21450;&#21508;&#31181;&#22797;&#26434;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#20379;&#24212;&#38142;&#21463;&#30410;&#20110;&#35745;&#31639;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20174;&#25163;&#21160;&#22788;&#29702;&#36807;&#28193;&#21040;&#33258;&#21160;&#21270;&#21644;&#25104;&#26412;&#25928;&#30410;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#20225;&#19994;&#36816;&#33829;&#32773;&#20173;&#28982;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#31934;&#21147;&#26469;&#35299;&#37322;&#21644;&#35299;&#35835;&#20248;&#21270;&#32467;&#26524;&#32473;&#30456;&#20851;&#20154;&#22763;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26368;&#36817;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#39072;&#35206;&#24615;&#25216;&#26415;&#22914;&#20309;&#24110;&#21161;&#24357;&#21512;&#20379;&#24212;&#38142;&#33258;&#21160;&#21270;&#21644;&#20154;&#31867;&#29702;&#35299;&#19982;&#20449;&#20219;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;\name{}&#30340;&#26694;&#26550;&#65292;&#23427;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24182;&#27809;&#26377;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#65292;&#32780;&#26159;&#21033;&#29992;&#23427;&#26469;&#23450;&#37327;&#22320;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65288;&#20363;&#22914;&#65292;&#22914;&#26524;&#25105;&#20204;&#20351;&#29992;&#20379;&#24212;&#21830;B&#32780;&#19981;&#26159;&#20379;&#24212;&#21830;A&#65292;&#25104;&#26412;&#20250;&#22914;&#20309;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supply chain operations traditionally involve a variety of complex decision making problems. Over the last few decades, supply chains greatly benefited from advances in computation, which allowed the transition from manual processing to automation and cost-effective optimization. Nonetheless, business operators still need to spend substantial efforts in \emph{explaining} and interpreting the optimization outcomes to stakeholders. Motivated by the recent advances in Large Language Models (LLMs), we study how this disruptive technology can help bridge the gap between supply chain automation and human comprehension and trust thereof. We design \name{} -- a framework that accepts as input queries in plain text, and outputs insights about the underlying optimization outcomes. Our framework does not forgo the state-of-the-art combinatorial optimization technology, but rather leverages it to quantitatively answer what-if scenarios (e.g., how would the cost change if we used supplier B instead
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#27969;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#30340;&#38134;&#26631;&#20934;&#26631;&#31614;&#23558;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#19982;&#28304;&#39046;&#22495;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;Ki-67&#35780;&#20998;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#22312;&#39046;&#22495;&#22806;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03872</link><description>&lt;p&gt;
&#20351;&#29992;&#38134;&#26631;&#20934;&#26631;&#31614;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#36827;&#34892;Ki-67&#35780;&#20998;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;: &#36827;&#19968;&#27493;&#25509;&#36817;&#22823;&#35268;&#27169;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation using Silver Standard Labels for Ki-67 Scoring in Digital Pathology: A Step Closer to Widescale Deployment. (arXiv:2307.03872v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#27969;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#30340;&#38134;&#26631;&#20934;&#26631;&#31614;&#23558;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#19982;&#28304;&#39046;&#22495;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;Ki-67&#35780;&#20998;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#22312;&#39046;&#22495;&#22806;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#34987;&#25552;&#20986;&#26469;&#25552;&#39640;Ki-67 PI&#35780;&#20998;&#30340;&#23458;&#35266;&#24615;&#21644;&#25928;&#29575;&#12290;&#25361;&#25112;&#22312;&#20110;&#65292;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#38750;&#24120;&#20934;&#30830;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#39046;&#22495;&#22806;&#25968;&#25454;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#23545;&#20110;&#20020;&#24202;&#24212;&#29992;&#26469;&#35828;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#27169;&#22411;&#36890;&#24120;&#26159;&#20351;&#29992;&#20379;&#24212;&#21830;&#21487;&#29992;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#19981;&#26469;&#33258;&#30446;&#26631;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#39046;&#22495;&#33258;&#36866;&#24212;&#27969;&#31243;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#26694;&#26550;&#22312;&#30446;&#26631;&#39046;&#22495;&#29983;&#25104;&#38134;&#26631;&#20934;&#65288;&#20266;&#65289;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#29992;&#20110;&#22686;&#21152;&#40644;&#37329;&#26631;&#20934;&#65288;GS&#65289;&#28304;&#39046;&#22495;&#25968;&#25454;&#12290;&#30740;&#31350;&#22312;&#20004;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;Ki-67&#35780;&#20998;&#26550;&#26500;&#65288;UV-Net&#21644;piNET&#65289;&#19978;&#27979;&#35797;&#20102;&#20116;&#20010;&#35757;&#32451;&#31574;&#30053;&#65306;(1) &#20165;SS: &#22312;&#30446;&#26631;&#38134;&#26631;&#20934;&#65288;SS&#65289;&#26631;&#31614;&#19978;&#35757;&#32451;&#65292;(2) &#20165;GS: &#22312;&#28304;GS&#26631;&#31614;&#19978;&#35757;&#32451;&#65292;(3) &#28151;&#21512;: &#22312;&#30446;&#26631;SS&#21644;&#28304;GS&#26631;&#31614;&#19978;&#35757;&#32451;&#65292;(4) GS+SS: &#22312;&#28304;GS&#26631;&#31614;&#19978;&#35757;&#32451;&#24182;&#22312;&#30446;&#26631;SS&#26631;&#31614;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning systems have been proposed to improve the objectivity and efficiency of Ki- 67 PI scoring. The challenge is that while very accurate, deep learning techniques suffer from reduced performance when applied to out-of-domain data. This is a critical challenge for clinical translation, as models are typically trained using data available to the vendor, which is not from the target domain. To address this challenge, this study proposes a domain adaptation pipeline that employs an unsupervised framework to generate silver standard (pseudo) labels in the target domain, which is used to augment the gold standard (GS) source domain data. Five training regimes were tested on two validated Ki-67 scoring architectures (UV-Net and piNET), (1) SS Only: trained on target silver standard (SS) labels, (2) GS Only: trained on source GS labels, (3) Mixed: trained on target SS and source GS labels, (4) GS+SS: trained on source GS labels and fine-tuned on target SS labels, and our proposed met
&lt;/p&gt;</description></item><item><title>Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#26159;&#22686;&#24378;&#35760;&#24518;&#33021;&#21147;&#32780;&#19981;&#26159;&#25913;&#36827;&#20449;&#29992;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2307.03864</link><description>&lt;p&gt;
&#20309;&#26102;&#20351;&#29992;Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21457;&#20809;&#65311;&#20174;&#35760;&#24518;&#21644;&#20449;&#29992;&#20998;&#37197;&#20013;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment. (arXiv:2307.03864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03864
&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#26159;&#22686;&#24378;&#35760;&#24518;&#33021;&#21147;&#32780;&#19981;&#26159;&#25913;&#36827;&#20449;&#29992;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38754;&#20020;&#20004;&#20010;&#19981;&#21516;&#30340;&#25361;&#25112;&#65306;&#23398;&#20064;&#26377;&#25928;&#30340;&#36807;&#21435;&#21644;&#24403;&#21069;&#35266;&#27979;&#30340;&#34920;&#31034;&#65292;&#24182;&#30830;&#23450;&#34892;&#21160;&#22914;&#20309;&#24433;&#21709;&#26410;&#26469;&#30340;&#25910;&#30410;&#12290;&#36825;&#20004;&#20010;&#25361;&#25112;&#37117;&#28041;&#21450;&#21040;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;Transformer&#26550;&#26500;&#22312;&#35299;&#20915;&#28041;&#21450;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;Transformer&#22522;&#20110;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#24378;&#21170;&#30340;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#65306;&#26159;&#22240;&#20026;&#23427;&#20204;&#23398;&#20064;&#20102;&#26377;&#25928;&#30340;&#35760;&#24518;&#65292;&#36824;&#26159;&#22240;&#20026;&#23427;&#20204;&#25191;&#34892;&#20102;&#26377;&#25928;&#30340;&#20449;&#29992;&#20998;&#37197;&#65311;&#22312;&#24341;&#20837;&#35760;&#24518;&#38271;&#24230;&#21644;&#20449;&#29992;&#20998;&#37197;&#38271;&#24230;&#30340;&#24418;&#24335;&#23450;&#20041;&#20043;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31616;&#21333;&#30340;&#21487;&#37197;&#32622;&#20219;&#21153;&#26469;&#27979;&#37327;&#36825;&#20123;&#19981;&#21516;&#30340;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#21487;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#25193;&#23637;&#21040;&#38656;&#35201;&#35760;&#20303;1500&#27493;&#21069;&#35266;&#23519;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;Transformer&#26080;&#27861;&#25913;&#36827;&#38271;&#26399;&#30340;&#20449;&#29992;&#20998;&#37197;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35760;&#24518;&#21644;&#20449;&#29992;&#20998;&#37197;&#26041;&#38754;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) algorithms face two distinct challenges: learning effective representations of past and present observations, and determining how actions influence future returns. Both challenges involve modeling long-term dependencies. The transformer architecture has been very successful to solve problems that involve long-term dependencies, including in the RL domain. However, the underlying reason for the strong performance of Transformer-based RL methods remains unclear: is it because they learn effective memory, or because they perform effective credit assignment? After introducing formal definitions of memory length and credit assignment length, we design simple configurable tasks to measure these distinct quantities. Our empirical results reveal that Transformers can enhance the memory capacity of RL algorithms, scaling up to tasks that require memorizing observations $1500$ steps ago. However, Transformers do not improve long-term credit assignment. In summary, our
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23384;&#30340;&#21512;&#20316;&#25968;&#23383;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#20943;&#23569;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#20013;&#20256;&#32479;&#27169;&#25311;&#25968;&#23383;&#36716;&#25442;&#22120;(ADC)&#30340;&#38754;&#31215;&#24320;&#38144;&#65292;&#25552;&#21319;&#24182;&#34892;&#24615;&#21644;&#20943;&#23569;&#22806;&#37096;&#20869;&#23384;&#35775;&#38382;&#12290;&#35813;&#26041;&#26696;&#36890;&#36807;&#22312;&#20869;&#23384;&#20013;&#24418;&#25104;&#20869;&#23384;&#20869;&#30340;&#25968;&#23383;-&#27169;&#25968;&#36716;&#25442;&#22120;(DAC)&#26469;&#23454;&#29616;&#38754;&#31215;&#39640;&#25928;&#30340;&#25968;&#23383;&#21270;&#65292;&#24182;&#19988;&#21033;&#29992;&#21512;&#20316;&#25968;&#23383;&#21270;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.03863</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#23384;&#30340;&#21512;&#20316;&#25968;&#23383;&#21270;&#29992;&#20110;&#38754;&#31215;&#39640;&#25928;&#30340;&#35745;&#31639;&#20869;&#23384;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Memory-Immersed Collaborative Digitization for Area-Efficient Compute-in-Memory Deep Learning. (arXiv:2307.03863v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23384;&#30340;&#21512;&#20316;&#25968;&#23383;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#20943;&#23569;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#20013;&#20256;&#32479;&#27169;&#25311;&#25968;&#23383;&#36716;&#25442;&#22120;(ADC)&#30340;&#38754;&#31215;&#24320;&#38144;&#65292;&#25552;&#21319;&#24182;&#34892;&#24615;&#21644;&#20943;&#23569;&#22806;&#37096;&#20869;&#23384;&#35775;&#38382;&#12290;&#35813;&#26041;&#26696;&#36890;&#36807;&#22312;&#20869;&#23384;&#20013;&#24418;&#25104;&#20869;&#23384;&#20869;&#30340;&#25968;&#23383;-&#27169;&#25968;&#36716;&#25442;&#22120;(DAC)&#26469;&#23454;&#29616;&#38754;&#31215;&#39640;&#25928;&#30340;&#25968;&#23383;&#21270;&#65292;&#24182;&#19988;&#21033;&#29992;&#21512;&#20316;&#25968;&#23383;&#21270;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#20869;&#23384;&#30340;&#21512;&#20316;&#25968;&#23383;&#21270;&#22312;&#35745;&#31639;&#20869;&#23384;(CiM)&#38453;&#21015;&#20043;&#38388;&#30340;&#24212;&#29992;&#65292;&#20197;&#26368;&#23567;&#21270;&#20256;&#32479;&#27169;&#25311;&#25968;&#23383;&#36716;&#25442;&#22120;(ADC)&#22312;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#20013;&#30340;&#38754;&#31215;&#24320;&#38144;&#12290;&#36890;&#36807;&#20351;&#29992;&#25552;&#20986;&#30340;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#23610;&#23544;&#35774;&#35745;&#20013;&#23481;&#32435;&#26356;&#22810;&#30340;CiM&#38453;&#21015;&#65292;&#20197;&#25552;&#39640;&#24182;&#34892;&#24615;&#24182;&#20943;&#23569;&#22806;&#37096;&#20869;&#23384;&#35775;&#38382;&#12290;&#22312;&#25968;&#23383;&#21270;&#26041;&#26696;&#19979;&#65292;CiM&#38453;&#21015;&#21033;&#29992;&#20854;&#23492;&#29983;&#27604;&#29305;&#32447;&#24418;&#25104;&#20869;&#23384;&#20869;&#30340;&#30005;&#23481;&#25968;&#23383;-&#27169;&#25968;&#36716;&#25442;&#22120;(DAC)&#65292;&#20197;&#23454;&#29616;&#38754;&#31215;&#39640;&#25928;&#30340;&#36880;&#27425;&#36924;&#36817;(SA)&#25968;&#23383;&#21270;&#12290;&#24403;&#19968;&#20010;&#38453;&#21015;&#35745;&#31639;&#36755;&#20837;&#21644;&#26435;&#37325;&#30340;&#26631;&#37327;&#31215;&#26102;&#65292;CiM&#38453;&#21015;&#21327;&#21516;&#24037;&#20316;&#65292;&#20854;&#20013;&#19968;&#20010;&#30456;&#37051;&#30340;&#38453;&#21015;&#36827;&#34892;&#27169;&#25311;&#39046;&#22495;&#30340;&#20056;&#31215;&#25968;&#30340;&#25968;&#23383;&#21270;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;CiM&#38453;&#21015;&#20043;&#38388;&#30340;&#21508;&#31181;&#32593;&#32476;&#37197;&#32622;&#65292;&#20854;&#20013;&#21033;&#29992;&#25552;&#20986;&#30340;&#20869;&#23384;&#20869;&#26041;&#26696;&#21487;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;Flash&#12289;SA&#21644;&#20854;&#28151;&#21512;&#25968;&#23383;&#21270;&#27493;&#39588;&#12290;&#32467;&#26524;&#20351;&#29992;65&#32435;&#31859;CMO&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work discusses memory-immersed collaborative digitization among compute-in-memory (CiM) arrays to minimize the area overheads of a conventional analog-to-digital converter (ADC) for deep learning inference. Thereby, using the proposed scheme, significantly more CiM arrays can be accommodated within limited footprint designs to improve parallelism and minimize external memory accesses. Under the digitization scheme, CiM arrays exploit their parasitic bit lines to form a within-memory capacitive digital-to-analog converter (DAC) that facilitates area-efficient successive approximation (SA) digitization. CiM arrays collaborate where a proximal array digitizes the analog-domain product-sums when an array computes the scalar product of input and weights. We discuss various networking configurations among CiM arrays where Flash, SA, and their hybrid digitization steps can be efficiently implemented using the proposed memory-immersed scheme. The results are demonstrated using a 65 nm CMO
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#32500;&#25252;&#35268;&#21010;&#12289;&#35843;&#24230;&#31574;&#30053;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#36830;&#32493;&#30340;&#26465;&#20214;&#30417;&#25511;&#25968;&#25454;&#26469;&#24320;&#21457;&#26234;&#33021;&#32500;&#25252;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#38477;&#20302;&#25104;&#26412;&#12289;&#24310;&#38271;&#36164;&#20135;&#23551;&#21629;&#21644;&#30830;&#20445;&#24037;&#20316;&#22330;&#25152;&#23433;&#20840;&#31561;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.03860</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#32500;&#25252;&#35268;&#21010;&#12289;&#35843;&#24230;&#31574;&#30053;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Reinforcement and Deep Reinforcement Learning-based Solutions for Machine Maintenance Planning, Scheduling Policies, and Optimization. (arXiv:2307.03860v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03860
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#32500;&#25252;&#35268;&#21010;&#12289;&#35843;&#24230;&#31574;&#30053;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#36830;&#32493;&#30340;&#26465;&#20214;&#30417;&#25511;&#25968;&#25454;&#26469;&#24320;&#21457;&#26234;&#33021;&#32500;&#25252;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#38477;&#20302;&#25104;&#26412;&#12289;&#24310;&#38271;&#36164;&#20135;&#23551;&#21629;&#21644;&#30830;&#20445;&#24037;&#20316;&#22330;&#25152;&#23433;&#20840;&#31561;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#21644;&#26426;&#22120;&#20250;&#32463;&#21382;&#21508;&#31181;&#25925;&#38556;&#27169;&#24335;&#65292;&#23548;&#33268;&#26426;&#22120;&#20581;&#24247;&#31243;&#24230;&#19979;&#38477;&#65292;&#22240;&#27492;&#38656;&#35201;&#32500;&#25252;&#25805;&#20316;&#26469;&#23558;&#23427;&#20204;&#24674;&#22797;&#21040;&#33021;&#22815;&#25191;&#34892;&#39044;&#26399;&#21151;&#33021;&#30340;&#29366;&#24577;&#12290;&#30001;&#20110;&#32500;&#25252;&#20219;&#21153;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#25152;&#20197;&#32500;&#25252;&#35268;&#21010;&#23545;&#20110;&#30830;&#20445;&#29983;&#20135;&#31995;&#32479;&#21644;&#20854;&#20182;&#34892;&#19994;&#30340;&#24179;&#31283;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#32500;&#25252;&#35268;&#21010;&#26159;&#19968;&#20010;&#20915;&#31574;&#38382;&#39064;&#65292;&#26088;&#22312;&#21046;&#23450;&#26368;&#20248;&#30340;&#32500;&#25252;&#31574;&#30053;&#21644;&#35745;&#21010;&#65292;&#20197;&#24110;&#21161;&#38477;&#20302;&#32500;&#25252;&#25104;&#26412;&#12289;&#24310;&#38271;&#36164;&#20135;&#23551;&#21629;&#12289;&#26368;&#22823;&#21270;&#21487;&#29992;&#24615;&#65292;&#26368;&#32456;&#30830;&#20445;&#24037;&#20316;&#22330;&#25152;&#23433;&#20840;&#12290;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#31639;&#27861;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24212;&#29992;&#20110;&#24320;&#21457;&#21160;&#24577;&#32500;&#25252;&#35745;&#21010;&#65292;&#21516;&#26102;&#21033;&#29992;&#31995;&#32479;&#21644;&#26426;&#22120;&#29366;&#24577;&#30340;&#36830;&#32493;&#20449;&#24687;&#12290;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#21644;&#26426;&#22120;&#30340;&#26465;&#20214;&#30417;&#25511;&#25968;&#25454;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#24320;&#21457;&#26234;&#33021;&#30340;&#32500;&#25252;&#35268;&#21010;&#22120;&#65292;&#21487;&#20197;&#20248;&#21270;&#32500;&#25252;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systems and machines undergo various failure modes that result in machine health degradation, so maintenance actions are required to restore them back to a state where they can perform their expected functions. Since maintenance tasks are inevitable, maintenance planning is essential to ensure the smooth operations of the production system and other industries at large. Maintenance planning is a decision-making problem that aims at developing optimum maintenance policies and plans that help reduces maintenance costs, extend asset life, maximize their availability, and ultimately ensure workplace safety. Reinforcement learning is a data-driven decision-making algorithm that has been increasingly applied to develop dynamic maintenance plans while leveraging the continuous information from condition monitoring of the system and machine states. By leveraging the condition monitoring data of systems and machines with reinforcement learning, smart maintenance planners can be developed, which
&lt;/p&gt;</description></item><item><title>inTformer&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#20851;&#27880;&#26426;&#21046;Transformer&#65292;&#29992;&#20110;&#20351;&#29992;&#36830;&#25509;&#36710;&#36742;&#25968;&#25454;&#39044;&#27979;&#36335;&#21475;&#20107;&#25925;&#21487;&#33021;&#24615;&#12290;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;inTformer&#20855;&#26377;&#22788;&#29702;&#38271;&#26399;&#20381;&#36182;&#24615;&#12289;&#24182;&#34892;&#22788;&#29702;&#25968;&#25454;&#24207;&#21015;&#20197;&#21450;&#35299;&#20915;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.03854</link><description>&lt;p&gt;
inTformer: &#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#20851;&#27880;&#26426;&#21046;Transformer&#29992;&#20110;&#20351;&#29992;&#36830;&#25509;&#36710;&#36742;&#25968;&#25454;&#30340;&#36335;&#21475;&#20107;&#25925;&#21487;&#33021;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data. (arXiv:2307.03854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03854
&lt;/p&gt;
&lt;p&gt;
inTformer&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#20851;&#27880;&#26426;&#21046;Transformer&#65292;&#29992;&#20110;&#20351;&#29992;&#36830;&#25509;&#36710;&#36742;&#25968;&#25454;&#39044;&#27979;&#36335;&#21475;&#20107;&#25925;&#21487;&#33021;&#24615;&#12290;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;inTformer&#20855;&#26377;&#22788;&#29702;&#38271;&#26399;&#20381;&#36182;&#24615;&#12289;&#24182;&#34892;&#22788;&#29702;&#25968;&#25454;&#24207;&#21015;&#20197;&#21450;&#35299;&#20915;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#20107;&#25925;&#21487;&#33021;&#24615;&#39044;&#27979;&#27169;&#22411;&#26159;&#20027;&#21160;&#20132;&#36890;&#23433;&#20840;&#31649;&#29702;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#22810;&#24180;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#26500;&#24314;&#20107;&#25925;&#21487;&#33021;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#12290;&#22312;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#37319;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#26469;&#35782;&#21035;&#20107;&#25925;&#28508;&#22312;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;Transformer&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#28508;&#22312;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#22522;&#26412;&#21407;&#29702;&#26159;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#36827;&#34892;&#25805;&#20316;&#12290;Transformer&#22312;&#21151;&#33021;&#19978;&#27604;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;LSTM&#65292;CNN&#31561;&#65289;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;Transformer&#21487;&#20197;&#36731;&#26494;&#22788;&#29702;&#25968;&#25454;&#24207;&#21015;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#12290;&#20854;&#27425;&#65292;Transformer&#21487;&#20197;&#22312;&#35757;&#32451;&#26399;&#38388;&#24182;&#34892;&#22788;&#29702;&#25968;&#25454;&#24207;&#21015;&#20013;&#30340;&#25152;&#26377;&#20803;&#32032;&#12290;&#26368;&#21518;&#65292;Transformer&#19981;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#12290;&#35748;&#35782;&#21040;Transformer&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
The real-time crash likelihood prediction model is an essential component of the proactive traffic safety management system. Over the years, numerous studies have attempted to construct a crash likelihood prediction model in order to enhance traffic safety, but mostly on freeways. In the majority of the existing studies, researchers have primarily employed a deep learning-based framework to identify crash potential. Lately, Transformer has emerged as a potential deep neural network that fundamentally operates through attention-based mechanisms. Transformer has several functional benefits over extant deep learning models such as Long Short-Term Memory (LSTM), Convolution Neural Network (CNN), etc. Firstly, Transformer can readily handle long-term dependencies in a data sequence. Secondly, Transformer can parallelly process all elements in a data sequence during training. Finally, Transformer does not have the vanishing gradient issue. Realizing the immense possibility of Transformer, th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#23454;&#29616;&#22238;&#24402;&#38382;&#39064;&#30340;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2307.03848</link><description>&lt;p&gt;
&#21487;&#23454;&#29616;&#22238;&#24402;&#30340;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65306;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Learners for Realizable Regression: PAC Learning and Online Learning. (arXiv:2307.03848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#23454;&#29616;&#22238;&#24402;&#38382;&#39064;&#30340;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23545;&#21487;&#23454;&#29616;&#22238;&#24402;&#22312;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#36827;&#34892;&#21051;&#30011;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#26377;&#38480;&#30340;fat shattering&#32500;&#24230;&#23545;&#20110;PAC&#23398;&#20064;&#30340;&#20805;&#20998;&#24615;&#20197;&#21450;&#26377;&#38480;&#30340;scaled Natarajan&#32500;&#24230;&#23545;&#20110;&#24517;&#35201;&#24615;&#30340;&#23384;&#22312;&#65292;&#20294;&#33258;&#20174;Simon 1997&#65288;SICOMP '97&#65289;&#30340;&#24037;&#20316;&#20197;&#26469;&#65292;&#23545;&#20110;&#26356;&#23436;&#25972;&#30340;&#21051;&#30011;&#30340;&#36827;&#23637;&#29978;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26368;&#23567;&#21270;&#23454;&#20363;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#26469;&#23545;&#21487;&#23454;&#29616;&#22238;&#24402;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#23450;&#24615;&#21448;&#23450;&#37327;&#22320;&#21051;&#30011;&#20102;&#21738;&#20123;&#31867;&#30340;&#23454;&#25968;&#39044;&#27979;&#22120;&#21487;&#20197;&#34987;&#23398;&#20064;&#30340;&#26032;&#39062;&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#19982;&#22270;&#32500;&#24230;&#30456;&#20851;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#35813;&#32500;&#24230;&#21051;&#30011;&#20102;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#30340;ERM&#21487;&#23398;&#20064;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#19982;DS&#32500;&#24230;&#30456;&#20851;&#30340;&#32452;&#21512;&#32500;&#24230;&#24314;&#31435;&#20102;&#23398;&#20064;&#21487;&#34892;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#29468;&#27979;&#23427;&#20063;&#21487;&#33021;&#26159;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to characterize the statistical complexity of realizable regression both in the PAC learning setting and the online learning setting.  Previous work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since the work of Simon 1997 (SICOMP '97). To this end, we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable. We then identify a combinatorial dimension related to the Graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#20889;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03838</link><description>&lt;p&gt;
RADAR: &#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
RADAR: Robust AI-Text Detection via Adversarial Learning. (arXiv:2307.03838v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#20889;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20197;&#21450;ChatGPT&#31867;&#24212;&#29992;&#30340;&#26222;&#21450;&#24050;&#32463;&#27169;&#31946;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#39640;&#36136;&#37327;&#25991;&#26412;&#29983;&#25104;&#30340;&#30028;&#38480;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#23545;&#25105;&#20204;&#30340;&#25216;&#26415;&#21644;&#31038;&#20250;&#39044;&#26399;&#30340;&#38761;&#21629;&#24615;&#21464;&#21270;&#22806;&#65292;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#65288;AI&#25991;&#26412;&#65289;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#22256;&#38590;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#28389;&#29992;&#21644;&#20844;&#24179;&#24615;&#25361;&#25112;&#65292;&#20363;&#22914;&#34394;&#20551;&#20869;&#23481;&#29983;&#25104;&#65292;&#25220;&#34989;&#20197;&#21450;&#23545;&#26080;&#36764;&#20316;&#32773;&#30340;&#38169;&#35823;&#25351;&#25511;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#24403;&#21069;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#22522;&#20110;LLM&#30340;&#25913;&#20889;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#20849;&#21516;&#35757;&#32451;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#12290;RADAR&#22522;&#20110;&#19968;&#20010;&#25913;&#20889;&#22120;&#21644;&#19968;&#20010;&#26816;&#27979;&#22120;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#25913;&#20889;&#22120;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#36924;&#30495;&#30340;&#20869;&#23481;&#20197;&#35268;&#36991;AI&#25991;&#26412;&#26816;&#27979;&#12290;RADAR&#20351;&#29992;&#26469;&#33258;&#26816;&#27979;&#22120;&#30340;&#21453;&#39304;&#26469;&#26356;&#26032;&#25913;&#20889;&#22120;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusation of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a Robust AI-text Detector via Adversarial leaRning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic contents to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice vers
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;MRI&#24378;&#24230;&#26631;&#20934;&#21270;&#26041;&#27861;&#22312;&#22810;&#20013;&#24515;FLAIR MRI&#30333;&#36136;&#30149;&#21464;&#20998;&#21106;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;&#19981;&#21516;&#26041;&#27861;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32452;&#21512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#24230;&#26631;&#20934;&#21270;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#26426;&#26500;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03827</link><description>&lt;p&gt;
&#24378;&#24230;&#26631;&#20934;&#21270;&#23545;&#22810;&#20013;&#24515;FLAIR MRI&#20013;&#28145;&#24230;&#23398;&#20064;&#30333;&#36136;&#30149;&#21464;&#20998;&#21106;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effect of Intensity Standardization on Deep Learning for WML Segmentation in Multi-Centre FLAIR MRI. (arXiv:2307.03827v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;MRI&#24378;&#24230;&#26631;&#20934;&#21270;&#26041;&#27861;&#22312;&#22810;&#20013;&#24515;FLAIR MRI&#30333;&#36136;&#30149;&#21464;&#20998;&#21106;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;&#19981;&#21516;&#26041;&#27861;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32452;&#21512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#24230;&#26631;&#20934;&#21270;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#26426;&#26500;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;MRI&#20013;&#29992;&#20110;&#30333;&#36136;&#30149;&#21464;&#20998;&#21106;&#26102;&#65292;&#24403;&#24212;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#20043;&#22806;&#30340;&#25195;&#25551;&#20202;&#25110;&#20013;&#24515;&#30340;&#25968;&#25454;&#26102;&#65292;&#24615;&#33021;&#20250;&#38477;&#20302;&#12290;&#30001;&#20110;&#24403;&#21069;&#27169;&#22411;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#26032;&#26426;&#26500;&#30340;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#36716;&#21270;&#21644;&#24191;&#27867;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;MRI&#24378;&#24230;&#26631;&#20934;&#21270;&#26041;&#27861;&#20316;&#20026;&#22810;&#20013;&#24515;&#28082;&#20307;&#34928;&#20943;&#21453;&#36716;&#24674;&#22797;&#65288;FLAIR&#65289;MRI&#30333;&#36136;&#30149;&#21464;&#20998;&#21106;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;FLAIR MRI&#30340;&#26041;&#27861;IAMLAB&#65292;&#20197;&#21450;&#20854;&#20182;&#24120;&#35265;&#30340;&#26631;&#20934;&#21270;&#25216;&#26415;&#65292;&#22914;White-strip&#12289;Nyul&#21644;Z-score&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;&#27599;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32452;&#21512;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#36339;&#36291;&#36830;&#25509;UNet&#65288;SC UNet&#65289;&#27169;&#22411;&#65292;&#20351;&#29992;&#26631;&#20934;&#21270;&#22270;&#20687;&#21644;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#35780;&#20272;&#20102;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) methods for white matter lesion (WML) segmentation in MRI suffer a reduction in performance when applied on data from a scanner or centre that is out-of-distribution (OOD) from the training data. This is critical for translation and widescale adoption, since current models cannot be readily applied to data from new institutions. In this work, we evaluate several intensity standardization methods for MRI as a preprocessing step for WML segmentation in multi-centre Fluid-Attenuated Inversion Recovery (FLAIR) MRI. We evaluate a method specifically developed for FLAIR MRI called IAMLAB along with other popular normalization techniques such as White-strip, Nyul and Z-score. We proposed an Ensemble model that combines predictions from each of these models. A skip-connection UNet (SC UNet) was trained on the standardized images, as well as the original data and segmentation performance was evaluated over several dimensions. The training (in-distribution) data consists of a 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23610;&#24230;&#25935;&#24863;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#31216;&#20026;&#39034;&#24207;&#26497;&#23567;&#26497;&#22823;&#32500;&#24230;&#65292;&#24182;&#36890;&#36807;&#23545;&#26377;&#30028;&#25439;&#22833;&#30340;&#22312;&#32447;&#23398;&#20064;&#28216;&#25103;&#36827;&#34892;&#30740;&#31350;&#65292;&#32473;&#20986;&#20102;&#23545;&#21521;&#37327;&#20540;&#22238;&#24402;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#30340;&#32039;&#23494;&#23450;&#37327;&#21051;&#30011;&#12290;</title><link>http://arxiv.org/abs/2307.03816</link><description>&lt;p&gt;
&#22312;&#26377;&#30028;&#25439;&#22833;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#28216;&#25103;&#30340;&#32452;&#21512;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Combinatorial Characterization of Online Learning Games with Bounded Losses. (arXiv:2307.03816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03816
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23610;&#24230;&#25935;&#24863;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#31216;&#20026;&#39034;&#24207;&#26497;&#23567;&#26497;&#22823;&#32500;&#24230;&#65292;&#24182;&#36890;&#36807;&#23545;&#26377;&#30028;&#25439;&#22833;&#30340;&#22312;&#32447;&#23398;&#20064;&#28216;&#25103;&#36827;&#34892;&#30740;&#31350;&#65292;&#32473;&#20986;&#20102;&#23545;&#21521;&#37327;&#20540;&#22238;&#24402;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#30340;&#32039;&#23494;&#23450;&#37327;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20551;&#35774;&#31867;&#21035;&#23545;&#20219;&#24847;&#20294;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23610;&#24230;&#25935;&#24863;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#31216;&#20026;&#39034;&#24207;&#26497;&#23567;&#26497;&#22823;&#32500;&#24230;&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22815;&#32039;&#23494;&#23450;&#37327;&#22320;&#21051;&#30011;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23545;&#20004;&#20010;&#24120;&#35265;&#23398;&#20064;&#22330;&#26223;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#30340;&#39318;&#20010;&#23450;&#37327;&#21051;&#30011;&#65306;&#21521;&#37327;&#20540;&#22238;&#24402;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the online learnability of hypothesis classes with respect to arbitrary, but bounded, loss functions. We give a new scale-sensitive combinatorial dimension, named the sequential Minimax dimension, and show that it gives a tight quantitative characterization of online learnability. As applications, we give the first quantitative characterization of online learnability for two natural learning settings: vector-valued regression and multilabel classification.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#25511;&#21046;&#25216;&#26415;&#19982;&#20648;&#23618;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#22312;&#28151;&#27788;H\'enon&#26144;&#23556;&#19978;&#23637;&#31034;&#20102;&#25511;&#21046;&#22120;&#23545;&#20110;&#25511;&#21046;&#31995;&#32479;&#30340;&#31283;&#23450;&#12289;&#22266;&#23450;&#28857;&#30340;&#25511;&#21046;&#21644;&#20219;&#24847;&#26399;&#26395;&#29366;&#24577;&#30340;&#25511;&#21046;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21482;&#38656;10&#20010;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#65292;&#21333;&#27425;&#36845;&#20195;&#23601;&#33021;&#25511;&#21046;&#21040;&#26399;&#26395;&#36712;&#36857;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03813</link><description>&lt;p&gt;
&#20351;&#29992;&#19979;&#19968;&#20195;&#20648;&#23618;&#35745;&#31639;&#25511;&#21046;&#28151;&#27788;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Controlling Chaotic Maps using Next-Generation Reservoir Computing. (arXiv:2307.03813v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03813
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#25511;&#21046;&#25216;&#26415;&#19982;&#20648;&#23618;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#22312;&#28151;&#27788;H\'enon&#26144;&#23556;&#19978;&#23637;&#31034;&#20102;&#25511;&#21046;&#22120;&#23545;&#20110;&#25511;&#21046;&#31995;&#32479;&#30340;&#31283;&#23450;&#12289;&#22266;&#23450;&#28857;&#30340;&#25511;&#21046;&#21644;&#20219;&#24847;&#26399;&#26395;&#29366;&#24577;&#30340;&#25511;&#21046;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21482;&#38656;10&#20010;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#65292;&#21333;&#27425;&#36845;&#20195;&#23601;&#33021;&#25511;&#21046;&#21040;&#26399;&#26395;&#36712;&#36857;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#25511;&#21046;&#25216;&#26415;&#19982;&#26368;&#20808;&#36827;&#30340;&#20648;&#23618;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#20648;&#23618;&#35745;&#31639;&#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#28151;&#27788;H\'enon&#26144;&#23556;&#19978;&#23637;&#31034;&#20102;&#25511;&#21046;&#22120;&#22312;&#19968;&#31995;&#21015;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#22312;&#19981;&#31283;&#23450;&#30340;&#22266;&#23450;&#28857;&#20043;&#38388;&#25511;&#21046;&#31995;&#32479;&#65292;&#23558;&#31995;&#32479;&#31283;&#23450;&#21040;&#26356;&#39640;&#38454;&#21608;&#26399;&#36712;&#36947;&#65292;&#24182;&#25511;&#21046;&#31995;&#32479;&#21040;&#20219;&#24847;&#26399;&#26395;&#29366;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25511;&#21046;&#22120;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#25104;&#21151;&#65292;&#24182;&#19988;&#20165;&#38656;10&#20010;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#21333;&#27425;&#36845;&#20195;&#20013;&#23558;&#31995;&#32479;&#25511;&#21046;&#21040;&#26399;&#26395;&#36712;&#36857;&#65292;&#24182;&#19988;&#23545;&#22122;&#22768;&#21644;&#24314;&#27169;&#35823;&#24046;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we combine nonlinear system control techniques with next-generation reservoir computing, a best-in-class machine learning approach for predicting the behavior of dynamical systems. We demonstrate the performance of the controller in a series of control tasks for the chaotic H\'enon map, including controlling the system between unstable fixed-points, stabilizing the system to higher order periodic orbits, and to an arbitrary desired state. We show that our controller succeeds in these tasks, requires only 10 data points for training, can control the system to a desired trajectory in a single iteration, and is robust to noise and modeling error.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;Formulation Graph Convolution Network&#65288;F-GCN&#65289;&#65292;&#23427;&#21487;&#20197;&#23558;&#30005;&#27744;&#30005;&#35299;&#36136;&#30340;&#32467;&#26500;&#32452;&#25104;&#20851;&#31995;&#26144;&#23556;&#21040;&#25972;&#20010;&#28082;&#20307;&#37197;&#26041;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#21152;&#24555;&#26032;&#21270;&#21512;&#29289;&#30340;&#21457;&#29616;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.03811</link><description>&lt;p&gt;
&#23558;&#30005;&#27744;&#30005;&#35299;&#36136;&#30340;&#32467;&#26500;&#32452;&#25104;&#26144;&#23556;&#21040;&#22120;&#20214;&#24615;&#33021;&#30340;&#37197;&#26041;&#22270;
&lt;/p&gt;
&lt;p&gt;
Formulation Graphs for Mapping Structure-Composition of Battery Electrolytes to Device Performance. (arXiv:2307.03811v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;Formulation Graph Convolution Network&#65288;F-GCN&#65289;&#65292;&#23427;&#21487;&#20197;&#23558;&#30005;&#27744;&#30005;&#35299;&#36136;&#30340;&#32467;&#26500;&#32452;&#25104;&#20851;&#31995;&#26144;&#23556;&#21040;&#25972;&#20010;&#28082;&#20307;&#37197;&#26041;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#21152;&#24555;&#26032;&#21270;&#21512;&#29289;&#30340;&#21457;&#29616;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#22312;&#31215;&#26497;&#23547;&#27714;&#20808;&#36827;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#35299;&#20915;&#21457;&#29616;&#21644;&#24320;&#21457;&#26032;&#30340;&#32452;&#21512;&#26448;&#26009;&#65288;&#22914;&#37197;&#26041;&#65289;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#19968;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#26041;&#27861;&#26159;&#39046;&#22495;&#24863;&#30693;&#30340;&#39640;&#36890;&#37327;&#31579;&#36873;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#21508;&#20010;&#32452;&#20998;&#32452;&#21512;&#25104;&#37197;&#26041;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#21152;&#36895;&#23547;&#25214;&#30446;&#26631;&#24212;&#29992;&#30340;&#26032;&#21270;&#21512;&#29289;&#65292;&#20294;&#26159;&#22312;&#20174;&#31934;&#36873;&#21270;&#23398;&#31354;&#38388;&#20013;&#35782;&#21035;&#20986;&#21512;&#36866;&#30340;&#8220;&#37197;&#26041;&#8221;&#26041;&#38754;&#20173;&#28982;&#20027;&#35201;&#26159;&#23454;&#39564;&#39537;&#21160;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;Formulation Graph Convolution Network&#65288;F-GCN&#65289;&#65292;&#21487;&#20197;&#23558;&#21508;&#20010;&#32452;&#20998;&#30340;&#32467;&#26500;&#32452;&#25104;&#20851;&#31995;&#26144;&#23556;&#21040;&#28082;&#20307;&#37197;&#26041;&#30340;&#24615;&#36136;&#12290;&#22810;&#20010;GCN&#24182;&#34892;&#32452;&#35013;&#65292;&#24182;&#26681;&#25454;&#37197;&#26041;&#20013;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#25705;&#23572;&#30334;&#20998;&#27604;&#30452;&#35266;&#22320;&#23545;&#37197;&#26041;&#25104;&#20998;&#36827;&#34892;&#29305;&#24449;&#21270;&#12290;&#28982;&#21518;&#26681;&#25454;&#30456;&#24212;&#32452;&#25104;&#37096;&#20998;&#30340;&#25705;&#23572;&#30334;&#20998;&#27604;&#23545;&#25152;&#24471;&#30340;&#20998;&#23376;&#25551;&#36848;&#31526;&#36827;&#34892;&#32553;&#25918;&#65292;&#25509;&#19979;&#26469;&#36827;&#34892;...
&lt;/p&gt;
&lt;p&gt;
Advanced computational methods are being actively sought for addressing the challenges associated with discovery and development of new combinatorial material such as formulations. A widely adopted approach involves domain informed high-throughput screening of individual components that can be combined into a formulation. This manages to accelerate the discovery of new compounds for a target application but still leave the process of identifying the right 'formulation' from the shortlisted chemical space largely a laboratory experiment-driven process. We report a deep learning model, Formulation Graph Convolution Network (F-GCN), that can map structure-composition relationship of the individual components to the property of liquid formulation as whole. Multiple GCNs are assembled in parallel that featurize formulation constituents domain-intuitively on the fly. The resulting molecular descriptors are scaled based on respective constituent's molar percentage in the formulation, followed
&lt;/p&gt;</description></item><item><title>URL&#22522;&#20934;&#26159;&#19968;&#20010;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#21457;&#29616;&#19987;&#27880;&#20110;&#34920;&#31034;&#26412;&#36523;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03810</link><description>&lt;p&gt;
URL&#65306;&#19968;&#31181;&#21487;&#36716;&#31227;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#34920;&#31034;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates. (arXiv:2307.03810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03810
&lt;/p&gt;
&lt;p&gt;
URL&#22522;&#20934;&#26159;&#19968;&#20010;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#21457;&#29616;&#19987;&#27880;&#20110;&#34920;&#31034;&#26412;&#36523;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#23398;&#20064;&#26174;&#33879;&#25512;&#21160;&#20102;&#35813;&#39046;&#22495;&#21457;&#23637;&#20986;&#33021;&#22815;&#20316;&#20026;&#20174;&#38646;&#24320;&#22987;&#36801;&#31227;&#21040;&#26032;&#25968;&#25454;&#38598;&#26102;&#30340;&#26377;&#20215;&#20540;&#36215;&#28857;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#38543;&#30528;&#23545;&#21487;&#38752;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19981;&#20165;&#33021;&#25552;&#20379;&#23884;&#20837;&#21521;&#37327;&#65292;&#36824;&#33021;&#25552;&#20379;&#21487;&#36716;&#31227;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#20026;&#20102;&#24341;&#23548;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;URL&#65288;Uncertainty-aware Representation Learning&#65289;&#22522;&#20934;&#12290;&#38500;&#20102;&#34920;&#31034;&#30340;&#21487;&#36716;&#31227;&#24615;&#20043;&#22806;&#65292;&#23427;&#36824;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#38646;&#26679;&#26412;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#24212;&#29992;URL&#26469;&#35780;&#20272;11&#31181;&#22312;ImageNet&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#36716;&#31227;&#21040;8&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30528;&#37325;&#20110;&#34920;&#31034;&#26412;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#19978;&#28216;&#31867;&#21035;&#30340;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#21487;&#36716;&#31227;&#30340;&#19981;&#30830;&#23450;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning has significantly driven the field to develop pretrained models that can act as a valuable starting point when transferring to new datasets. With the rising demand for reliable machine learning and uncertainty quantification, there is a need for pretrained models that not only provide embeddings but also transferable uncertainty estimates. To guide the development of such models, we propose the Uncertainty-aware Representation Learning (URL) benchmark. Besides the transferability of the representations, it also measures the zero-shot transferability of the uncertainty estimate using a novel metric. We apply URL to evaluate eleven uncertainty quantifiers that are pretrained on ImageNet and transferred to eight downstream datasets. We find that approaches that focus on the uncertainty of the representation itself or estimate the prediction risk directly outperform those that are based on the probabilities of upstream classes. Yet, achieving transferable uncertaint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#23376;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#23545;&#25972;&#20010;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#30340;&#36129;&#29486;&#65292;&#24182;&#24341;&#20837;&#20102;&#21322;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#36827;&#34892;&#24230;&#37327;&#12290;&#36825;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#23545;&#25239;&#24615;&#25915;&#20987;&#24182;&#20419;&#36827;&#26356;&#39640;&#25928;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.03803</link><description>&lt;p&gt;
&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#23376;&#32593;&#32476;&#36129;&#29486;&#30340;&#29702;&#35770;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Perspective on Subnetwork Contributions to Adversarial Robustness. (arXiv:2307.03803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#23376;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#23545;&#25972;&#20010;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#30340;&#36129;&#29486;&#65292;&#24182;&#24341;&#20837;&#20102;&#21322;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#36827;&#34892;&#24230;&#37327;&#12290;&#36825;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#23545;&#25239;&#24615;&#25915;&#20987;&#24182;&#20419;&#36827;&#26356;&#39640;&#25928;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#25910;&#25947;&#65292;&#24182;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#23433;&#20840;&#24615;&#12290;&#23545;&#25239;&#35757;&#32451;&#26159;&#21152;&#24378;DNN&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20197;&#24212;&#29992;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#26041;&#27861;&#21040;&#25972;&#20010;&#27169;&#22411;&#30340;&#20195;&#20215;&#25552;&#20379;&#36825;&#31181;&#26041;&#27861;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#25915;&#20987;&#24182;&#20419;&#36827;&#26356;&#39640;&#25928;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#23376;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#23545;&#25972;&#20010;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#30340;&#36129;&#29486;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#24341;&#20837;&#20102;&#21322;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#65292;&#23427;&#26159;&#23376;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24230;&#37327;&#12290;&#22312;&#27492;&#27010;&#24565;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#22914;&#26524;&#23376;&#32593;&#32476;&#26159;&#21322;&#40065;&#26834;&#30340;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#36275;&#22815;&#22823;&#30340;&#27604;&#20363;
&lt;/p&gt;
&lt;p&gt;
The robustness of deep neural networks (DNNs) against adversarial attacks has been studied extensively in hopes of both better understanding how deep learning models converge and in order to ensure the security of these models in safety-critical applications. Adversarial training is one approach to strengthening DNNs against adversarial attacks, and has been shown to offer a means for doing so at the cost of applying computationally expensive training methods to the entire model. To better understand these attacks and facilitate more efficient adversarial training, in this paper we develop a novel theoretical framework that investigates how the adversarial robustness of a subnetwork contributes to the robustness of the entire network. To do so we first introduce the concept of semirobustness, which is a measure of the adversarial robustness of a subnetwork. Building on this concept, we then provide a theoretical analysis to show that if a subnetwork is semirobust and there is a suffici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25366;&#25496;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#25214;&#21040;&#27450;&#39575;&#20027;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#25552;&#31034;&#19979;&#33021;&#27450;&#39575;CLIP&#27169;&#22411;&#65292;&#32780;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#26080;&#27861;&#35748;&#20986;&#30340;&#12290;&#27450;&#39575;&#20027;&#22270;&#20687;&#22312;&#23569;&#37327;&#22270;&#20687;&#26631;&#39064;&#19978;&#30340;&#35757;&#32451;&#19978;&#21487;&#33021;&#36866;&#29992;&#20110;&#26356;&#22810;&#25968;&#37327;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#26631;&#39064;&#12290;&#20004;&#31181;&#21487;&#33021;&#30340;&#32531;&#35299;&#31574;&#30053;&#34987;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#33030;&#24369;&#24615;&#19982;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.03798</link><description>&lt;p&gt;
CLIPMasterPrints: &#20351;&#29992;&#28508;&#22312;&#21464;&#37327;&#28436;&#21270;&#27450;&#39575;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution. (arXiv:2307.03798v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25366;&#25496;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#25214;&#21040;&#27450;&#39575;&#20027;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#25552;&#31034;&#19979;&#33021;&#27450;&#39575;CLIP&#27169;&#22411;&#65292;&#32780;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#26080;&#27861;&#35748;&#20986;&#30340;&#12290;&#27450;&#39575;&#20027;&#22270;&#20687;&#22312;&#23569;&#37327;&#22270;&#20687;&#26631;&#39064;&#19978;&#30340;&#35757;&#32451;&#19978;&#21487;&#33021;&#36866;&#29992;&#20110;&#26356;&#22810;&#25968;&#37327;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#26631;&#39064;&#12290;&#20004;&#31181;&#21487;&#33021;&#30340;&#32531;&#35299;&#31574;&#30053;&#34987;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#33030;&#24369;&#24615;&#19982;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#20026;&#20195;&#34920;&#30340;&#21516;&#26102;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#27169;&#22411;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#22810;&#21151;&#33021;&#24615;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#25152;&#35859;&#30340;&#27450;&#39575;&#20027;&#22270;&#20687;&#26159;&#33030;&#24369;&#30340;&#12290;&#27450;&#39575;&#20027;&#22270;&#20687;&#33021;&#22815;&#26368;&#22823;&#21270;CLIP&#27169;&#22411;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#25552;&#31034;&#19979;&#30340;&#32622;&#20449;&#24230;&#35780;&#20998;&#65292;&#21516;&#26102;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#26159;&#26080;&#27861;&#35748;&#20986;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#28436;&#21270;&#31574;&#30053;&#25110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25628;&#32034;&#27450;&#39575;&#20027;&#22270;&#20687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25366;&#25496;&#20986;&#30340;&#27450;&#39575;&#20027;&#22270;&#20687;&#30340;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#23569;&#37327;&#22270;&#20687;&#26631;&#39064;&#19978;&#35757;&#32451;&#30340;&#22270;&#20687;&#21487;&#33021;&#36866;&#29992;&#20110;&#26356;&#22810;&#25968;&#37327;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#26631;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#21487;&#33021;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#23545;&#27450;&#39575;&#20027;&#20363;&#23376;&#30340;&#33030;&#24369;&#24615;&#19982;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models leveraging both visual and textual data such as Contrastive Language-Image Pre-training (CLIP), are increasingly gaining importance. In this work, we show that despite their versatility, such models are vulnerable to what we refer to as fooling master images. Fooling master images are capable of maximizing the confidence score of a CLIP model for a significant number of widely varying prompts, while being unrecognizable for humans. We demonstrate how fooling master images can be mined by searching the latent space of generative models by means of an evolution strategy or stochastic gradient descent. We investigate the properties of the mined fooling master images, and find that images trained on a small number of image captions potentially generalize to a much larger number of semantically related captions. Further, we evaluate two possible mitigation strategies and find that vulnerability to fooling master examples is closely related to a modality gap in contrastive pre-trained
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#26469;&#20943;&#36731;&#25277;&#35937;&#25216;&#26415;&#20013;&#39640;&#20869;&#23384;&#38656;&#27714;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#27491;&#30830;&#26500;&#24314;&#30340;&#25511;&#21046;&#22120;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#29992;&#20110;&#37096;&#32626;&#36825;&#20123;&#25511;&#21046;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.03783</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#25277;&#35937;&#30340;&#25511;&#21046;&#22120;&#21512;&#25104;&#21644;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Neural Abstraction-Based Controller Synthesis and Deployment. (arXiv:2307.03783v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#26469;&#20943;&#36731;&#25277;&#35937;&#25216;&#26415;&#20013;&#39640;&#20869;&#23384;&#38656;&#27714;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#27491;&#30830;&#26500;&#24314;&#30340;&#25511;&#21046;&#22120;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#29992;&#20110;&#37096;&#32626;&#36825;&#20123;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25277;&#35937;&#30340;&#25216;&#26415;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#28385;&#36275;&#39640;&#23618;&#26102;&#38388;&#35201;&#27714;&#30340;&#27491;&#30830;&#26500;&#24314;&#30340;&#25511;&#21046;&#22120;&#12290;&#25104;&#21151;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#20869;&#23384;&#38656;&#27714;&#65292;&#22312;&#25511;&#21046;&#22120;&#21512;&#25104;&#21644;&#25511;&#21046;&#22120;&#37096;&#32626;&#26399;&#38388;&#37117;&#23384;&#22312;&#20869;&#23384;&#38656;&#27714;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#26469;&#20943;&#36731;&#22522;&#20110;&#25277;&#35937;&#30340;&#25216;&#26415;&#30340;&#39640;&#20869;&#23384;&#38656;&#27714;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#12290;&#20026;&#20102;&#25191;&#34892;&#36798;&#21040;&#36991;&#20813;&#35268;&#33539;&#30340;&#21512;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20110;&#31995;&#32479;&#21069;&#21521;&#21644;&#21518;&#21521;&#21160;&#21147;&#23398;&#30340;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#21363;&#26102;&#31639;&#27861;&#12290;&#19982;&#36890;&#24120;&#30340;&#31070;&#32463;&#34920;&#31034;&#24212;&#29992;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#20445;&#25345;&#31471;&#21040;&#31471;&#36807;&#31243;&#30340;&#23436;&#25972;&#24615;&#12290;&#20026;&#20102;&#30830;&#20445;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#32416;&#27491;&#20102;&#35757;&#32451;&#21518;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#20351;&#24471;&#20462;&#27491;&#21518;&#30340;&#36755;&#20986;&#34920;&#31034;&#22312;&#26377;&#38480;&#25277;&#35937;&#26041;&#38754;&#26159;&#20934;&#30830;&#30340;&#12290;&#23545;&#20110;&#37096;&#32626;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#31639;&#27861;&#20197;&#23436;&#25104;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstraction-based techniques are an attractive approach for synthesizing correct-by-construction controllers to satisfy high-level temporal requirements. A main bottleneck for successful application of these techniques is the memory requirement, both during controller synthesis and in controller deployment.  We propose memory-efficient methods for mitigating the high memory demands of the abstraction-based techniques using neural network representations. To perform synthesis for reach-avoid specifications, we propose an on-the-fly algorithm that relies on compressed neural network representations of the forward and backward dynamics of the system. In contrast to usual applications of neural representations, our technique maintains soundness of the end-to-end process. To ensure this, we correct the output of the trained neural network such that the corrected output representations are sound with respect to the finite abstraction. For deployment, we provide a novel training algorithm to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#27874;&#26031;&#35821;Twitter&#35805;&#35821;&#30340;&#35745;&#31639;&#20998;&#26512;&#65292;&#20272;&#31639;&#20102;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#21435;&#19990;&#21518;&#20234;&#26391;&#31038;&#20250;&#23545;&#24615;&#21035;&#24179;&#31561;&#31435;&#22330;&#30340;&#36716;&#21464;&#12290;&#30740;&#31350;&#37319;&#29992;&#21442;&#19982;&#24335;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#28041;&#21450;&#20234;&#26391;&#22899;&#24615;&#22312;&#26631;&#27880;&#36807;&#31243;&#20013;&#30340;&#31215;&#26497;&#35282;&#33394;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#30340;&#27515;&#20129;&#24341;&#21457;&#20102;&#27874;&#26031;&#35821;&#35805;&#35821;&#30340;&#26497;&#21270;&#65292;&#31215;&#26497;&#25512;&#25991;&#25968;&#37327;&#30053;&#22810;&#20110;&#36127;&#38754;&#25512;&#25991;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#35266;&#23519;&#21040;&#25919;&#24220;&#21644;&#25239;&#35758;&#32773;&#22312;Twitter&#19978;&#30340;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.03764</link><description>&lt;p&gt;
&#23545;&#20110;&#22899;&#24615;&#32780;&#35328;&#65292;&#29983;&#27963;&#21644;&#33258;&#30001;&#65306;&#22522;&#20110;&#21442;&#19982;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#35770;&#25991;&#30740;&#31350;&#20234;&#26391;&#24615;&#21035;&#26007;&#20105;&#20013;&#30340;&#20998;&#27700;&#23725;&#26102;&#21051;
&lt;/p&gt;
&lt;p&gt;
For Women, Life, Freedom: A Participatory AI-Based Social Web Analysis of a Watershed Moment in Iran's Gender Struggles. (arXiv:2307.03764v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#27874;&#26031;&#35821;Twitter&#35805;&#35821;&#30340;&#35745;&#31639;&#20998;&#26512;&#65292;&#20272;&#31639;&#20102;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#21435;&#19990;&#21518;&#20234;&#26391;&#31038;&#20250;&#23545;&#24615;&#21035;&#24179;&#31561;&#31435;&#22330;&#30340;&#36716;&#21464;&#12290;&#30740;&#31350;&#37319;&#29992;&#21442;&#19982;&#24335;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#28041;&#21450;&#20234;&#26391;&#22899;&#24615;&#22312;&#26631;&#27880;&#36807;&#31243;&#20013;&#30340;&#31215;&#26497;&#35282;&#33394;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#30340;&#27515;&#20129;&#24341;&#21457;&#20102;&#27874;&#26031;&#35821;&#35805;&#35821;&#30340;&#26497;&#21270;&#65292;&#31215;&#26497;&#25512;&#25991;&#25968;&#37327;&#30053;&#22810;&#20110;&#36127;&#38754;&#25512;&#25991;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#35266;&#23519;&#21040;&#25919;&#24220;&#21644;&#25239;&#35758;&#32773;&#22312;Twitter&#19978;&#30340;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#27874;&#26031;&#35821;Twitter&#35805;&#35821;&#30340;&#35745;&#31639;&#20998;&#26512;&#65292;&#26088;&#22312;&#20272;&#31639;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#22312;&#35686;&#26041;&#25304;&#30041;&#26399;&#38388;&#21435;&#19990;&#21518;&#23545;&#24615;&#21035;&#24179;&#31561;&#31435;&#22330;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20027;&#21160;&#23398;&#20064;&#27969;&#31243;&#26469;&#35757;&#32451;&#19968;&#20010;&#31435;&#22330;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#22312;&#20110;&#20234;&#26391;&#22899;&#24615;&#20197;&#20027;&#21160;&#30340;&#35282;&#33394;&#21442;&#19982;&#22312;&#26500;&#24314;&#36825;&#20010;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#26631;&#27880;&#12290;&#25105;&#20204;&#30340;&#26631;&#27880;&#32773;&#19981;&#20165;&#25552;&#20379;&#26631;&#31614;&#65292;&#36824;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20851;&#38190;&#35789;&#20197;&#36827;&#34892;&#26356;&#26377;&#24847;&#20041;&#30340;&#35821;&#26009;&#24211;&#21019;&#24314;&#65292;&#24182;&#25552;&#20379;&#31616;&#30701;&#30340;&#31034;&#20363;&#25991;&#26723;&#20197;&#36827;&#34892;&#24341;&#23548;&#24335;&#25277;&#26679;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#30340;&#27515;&#20129;&#24341;&#21457;&#20102;&#26497;&#21270;&#30340;&#27874;&#26031;&#35821;&#35805;&#35821;&#65292;&#23545;&#24615;&#21035;&#24179;&#31561;&#30340;&#36127;&#38754;&#21644;&#31215;&#26497;&#25512;&#25991;&#25968;&#37327;&#37117;&#22686;&#21152;&#20102;&#12290;&#31215;&#26497;&#25512;&#25991;&#30340;&#22686;&#21152;&#30053;&#22823;&#20110;&#36127;&#38754;&#25512;&#25991;&#30340;&#22686;&#21152;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#23601;&#36134;&#21495;&#21019;&#24314;&#26102;&#38388;&#32780;&#35328;&#65292;&#19982;&#25919;&#24220;&#23545;&#40784;&#30340;Twitter&#36134;&#21495;&#21644;&#25903;&#25345;&#25239;&#35758;&#30340;Twitter&#36134;&#21495;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a computational analysis of the Persian language Twitter discourse with the aim to estimate the shift in stance toward gender equality following the death of Mahsa Amini in police custody. We present an ensemble active learning pipeline to train a stance classifier. Our novelty lies in the involvement of Iranian women in an active role as annotators in building this AI system. Our annotators not only provide labels, but they also suggest valuable keywords for more meaningful corpus creation as well as provide short example documents for a guided sampling step. Our analyses indicate that Mahsa Amini's death triggered polarized Persian language discourse where both fractions of negative and positive tweets toward gender equality increased. The increase in positive tweets was slightly greater than the increase in negative tweets. We also observe that with respect to account creation time, between the state-aligned Twitter accounts and pro-protest Twitter accounts
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DyGATAD&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35782;&#21035;&#38598;&#20307;&#24322;&#24120;&#34892;&#20026;&#65292;&#20854;&#20013;&#24322;&#24120;&#34892;&#20026;&#21487;&#33021;&#30001;&#20110;&#31995;&#32479;&#20869;&#37096;&#30456;&#20114;&#20851;&#31995;&#30340;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#30417;&#25511;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.03761</link><description>&lt;p&gt;
&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks. (arXiv:2307.03761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03761
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DyGATAD&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35782;&#21035;&#38598;&#20307;&#24322;&#24120;&#34892;&#20026;&#65292;&#20854;&#20013;&#24322;&#24120;&#34892;&#20026;&#21487;&#33021;&#30001;&#20110;&#31995;&#32479;&#20869;&#37096;&#30456;&#20114;&#20851;&#31995;&#30340;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#30417;&#25511;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#26102;&#20195;&#65292;&#30001;&#24037;&#19994;&#29289;&#32852;&#32593; (IIoT) &#30417;&#25511;&#30340;&#31995;&#32479;&#36890;&#36807;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#29983;&#25104;&#22823;&#37327;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015; (MTS) &#25968;&#25454;&#12290;&#34429;&#28982;&#36825;&#20123;&#25968;&#25454;&#26377;&#21161;&#20110;&#26465;&#20214;&#30417;&#25511;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#20294;&#26159;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#26085;&#30410;&#22797;&#26434;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20851;&#31995;&#20063;&#32473;&#24322;&#24120;&#26816;&#27979;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#24322;&#24120;&#21644;&#32972;&#26223;&#24322;&#24120;&#65292;&#23545;&#38598;&#20307;&#24322;&#24120;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#38598;&#20307;&#24322;&#24120;&#30340;&#19968;&#31181;&#24120;&#35265;&#21464;&#31181;&#26159;&#24322;&#24120;&#38598;&#20307;&#34892;&#20026;&#30001;&#31995;&#32479;&#20869;&#37096;&#30340;&#30456;&#20114;&#20851;&#31995;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#24322;&#24120;&#29615;&#22659;&#26465;&#20214;&#65288;&#22914;&#36807;&#28909;&#65289;&#12289;&#30001;&#20110;&#32593;&#32476;&#25915;&#20987;&#36896;&#25104;&#30340;&#19981;&#27491;&#30830;&#25805;&#20316;&#35774;&#32622;&#25110;&#31995;&#32479;&#32423;&#25925;&#38556;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; DyGATAD&#65288;&#19968;&#31181;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65289;&#65292;&#37319;&#29992;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of digital transformation, systems monitored by the Industrial Internet of Things (IIoTs) generate large amounts of Multivariate Time Series (MTS) data through heterogeneous sensor networks. While this data facilitates condition monitoring and anomaly detection, the increasing complexity and interdependencies within the sensor network pose significant challenges for anomaly detection. Despite progress in this field, much of the focus has been on point anomalies and contextual anomalies, with lesser attention paid to collective anomalies. A less addressed but common variant of collective anomalies is when the abnormal collective behavior is caused by shifts in interrelationships within the system. This can be due to abnormal environmental conditions like overheating, improper operational settings resulting from cyber-physical attacks, or system-level faults. To address these challenges, this paper proposes DyGATAD (Dynamic Graph Attention for Anomaly Detection), a graph-based
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#20171;&#32461;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#24102;&#26469;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.03759</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32508;&#36848;&#65306;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#25554;&#20540;&#21644;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection. (arXiv:2307.03759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#20171;&#32461;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#24102;&#26469;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#26159;&#35760;&#24405;&#21160;&#24577;&#31995;&#32479;&#27979;&#37327;&#32467;&#26524;&#30340;&#20027;&#35201;&#25968;&#25454;&#31867;&#22411;&#65292;&#36890;&#36807;&#29289;&#29702;&#20256;&#24863;&#22120;&#21644;&#22312;&#32447;&#36807;&#31243;&#65288;&#34394;&#25311;&#20256;&#24863;&#22120;&#65289;&#29983;&#25104;&#22823;&#37327;&#25968;&#25454;&#12290;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#23545;&#20110;&#25581;&#31034;&#21487;&#29992;&#25968;&#25454;&#20013;&#25152;&#34164;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22522;&#20110;GNN&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#20063;&#22823;&#24133;&#22686;&#21152;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;&#20256;&#32479;&#21644;&#20854;&#20182;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21017;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32508;&#36848;&#65288;GNN4TS&#65289;&#65292;&#21253;&#25324;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#65306;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#25105;&#20204;&#26088;&#22312;&#25351;&#23548;&#35774;&#35745;&#24072;&#21644;&#23454;&#36341;&#32773;&#20102;&#35299;&#12289;&#26500;&#24314;&#24212;&#29992;&#21644;&#25512;&#21160;GNN4TS&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;GNN4TS&#20998;&#31867;&#20307;&#31995;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. Approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: Forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#25509;&#20837;&#20013;&#30340;&#26080;&#32447;&#30005;&#36164;&#28304;&#31454;&#20105;&#26426;&#21046;&#30340;&#32593;&#32476;&#20869;&#22312;&#26041;&#27861;&#26469;&#23454;&#29616;&#20998;&#24067;&#24335;&#29992;&#25143;&#36873;&#25321;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#12290;&#36890;&#36807;&#25805;&#32437;&#31454;&#20105;&#31383;&#21475;&#22823;&#23567;&#21644;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#20559;&#24046;&#20316;&#20026;&#20248;&#20808;&#32423;&#20381;&#25454;&#65292;&#21487;&#20197;&#24555;&#36895;&#23454;&#29616;&#39640;&#25928;&#30340;&#25910;&#25947;&#65292;&#21516;&#26102;&#20445;&#35777;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03758</link><description>&lt;p&gt;
&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#36890;&#36807;&#38543;&#26426;&#25509;&#20837;&#23454;&#29616;&#20998;&#24067;&#24335;&#29992;&#25143;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Federated Learning over a Wireless Network: Distributed User Selection through Random Access. (arXiv:2307.03758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#25509;&#20837;&#20013;&#30340;&#26080;&#32447;&#30005;&#36164;&#28304;&#31454;&#20105;&#26426;&#21046;&#30340;&#32593;&#32476;&#20869;&#22312;&#26041;&#27861;&#26469;&#23454;&#29616;&#20998;&#24067;&#24335;&#29992;&#25143;&#36873;&#25321;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#12290;&#36890;&#36807;&#25805;&#32437;&#31454;&#20105;&#31383;&#21475;&#22823;&#23567;&#21644;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#20559;&#24046;&#20316;&#20026;&#20248;&#20808;&#32423;&#20381;&#25454;&#65292;&#21487;&#20197;&#24555;&#36895;&#23454;&#29616;&#39640;&#25928;&#30340;&#25910;&#25947;&#65292;&#21516;&#26102;&#20445;&#35777;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36873;&#25321;&#23545;&#20110;&#38477;&#20302;&#32852;&#37030;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#30340;&#36890;&#20449;&#25104;&#26412;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38598;&#20013;&#24335;&#29992;&#25143;&#36873;&#25321;&#20250;&#24341;&#36215;&#39069;&#22806;&#30340;&#31995;&#32479;&#22797;&#26434;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#25509;&#20837;&#20013;&#30340;&#26080;&#32447;&#30005;&#36164;&#28304;&#31454;&#20105;&#26426;&#21046;&#30340;&#32593;&#32476;&#20869;&#22312;&#26041;&#27861;&#26469;&#36827;&#34892;&#20998;&#24067;&#24335;&#29992;&#25143;&#36873;&#25321;&#12290;&#20197;&#36733;&#27874;&#24863;&#30693;&#22810;&#36335;&#35775;&#38382;&#65288;CSMA&#65289;&#26426;&#21046;&#20026;&#38543;&#26426;&#25509;&#20837;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#36890;&#36807;&#25805;&#32437;&#31454;&#20105;&#31383;&#21475;&#65288;CW&#65289;&#22823;&#23567;&#65292;&#22312;&#27599;&#36718;&#35757;&#32451;&#20013;&#20248;&#20808;&#20026;&#26576;&#20123;&#29992;&#25143;&#33719;&#21462;&#26080;&#32447;&#30005;&#36164;&#28304;&#12290;&#35757;&#32451;&#25968;&#25454;&#20559;&#24046;&#34987;&#29992;&#20316;&#24102;&#26377;&#29992;&#25143;&#36873;&#25321;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#30446;&#26631;&#22330;&#26223;&#12290;&#20248;&#20808;&#32423;&#22522;&#20110;&#26032;&#35757;&#32451;&#30340;&#23616;&#37096;&#27169;&#22411;&#19982;&#19978;&#19968;&#36718;&#30340;&#20840;&#23616;&#27169;&#22411;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#20026;&#20102;&#36991;&#20813;&#26576;&#20123;&#29992;&#25143;&#36807;&#24230;&#36129;&#29486;&#65292;&#20351;&#29992;&#35745;&#25968;&#26426;&#21046;&#26469;&#30830;&#20445;&#20844;&#24179;&#24615;&#12290;&#20351;&#29992;&#21508;&#31181;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#27169;&#25311;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#23454;&#29616;&#31867;&#20284;&#20110;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
User selection has become crucial for decreasing the communication costs of federated learning (FL) over wireless networks. However, centralized user selection causes additional system complexity. This study proposes a network intrinsic approach of distributed user selection that leverages the radio resource competition mechanism in random access. Taking the carrier sensing multiple access (CSMA) mechanism as an example of random access, we manipulate the contention window (CW) size to prioritize certain users for obtaining radio resources in each round of training. Training data bias is used as a target scenario for FL with user selection. Prioritization is based on the distance between the newly trained local model and the global model of the previous round. To avoid excessive contribution by certain users, a counting mechanism is used to ensure fairness. Simulations with various datasets demonstrate that this method can rapidly achieve convergence similar to that of the centralized 
&lt;/p&gt;</description></item><item><title>FITS&#26159;&#19968;&#31181;&#36731;&#37327;&#32780;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#39057;&#29575;&#22495;&#20013;&#36827;&#34892;&#25554;&#20540;&#25805;&#20316;&#65292;&#20002;&#24323;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24433;&#21709;&#24494;&#23567;&#30340;&#39640;&#39057;&#20998;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#12290;</title><link>http://arxiv.org/abs/2307.03756</link><description>&lt;p&gt;
FITS&#65306;&#27169;&#25311;&#20855;&#26377;10k&#20010;&#21442;&#25968;&#30340;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
FITS: Modeling Time Series with $10k$ Parameters. (arXiv:2307.03756v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03756
&lt;/p&gt;
&lt;p&gt;
FITS&#26159;&#19968;&#31181;&#36731;&#37327;&#32780;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#39057;&#29575;&#22495;&#20013;&#36827;&#34892;&#25554;&#20540;&#25805;&#20316;&#65292;&#20002;&#24323;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24433;&#21709;&#24494;&#23567;&#30340;&#39640;&#39057;&#20998;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FITS&#65292;&#19968;&#31181;&#36731;&#37327;&#32780;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#27169;&#22411;&#12290;&#19982;&#30452;&#25509;&#22788;&#29702;&#21407;&#22987;&#26102;&#38388;&#22495;&#25968;&#25454;&#30340;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;FITS&#22522;&#20110;&#22312;&#22797;&#26434;&#39057;&#29575;&#22495;&#20013;&#36827;&#34892;&#25554;&#20540;&#30340;&#21407;&#29702;&#25805;&#20316;&#26102;&#38388;&#24207;&#21015;&#12290;&#36890;&#36807;&#20002;&#24323;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24433;&#21709;&#24494;&#23567;&#30340;&#39640;&#39057;&#20998;&#37327;&#65292;FITS&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#36817;&#20284;10k&#20010;&#21442;&#25968;&#30340;&#26174;&#33879;&#32039;&#20945;&#22823;&#23567;&#12290;&#36825;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#22320;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#37096;&#32626;&#65292;&#20026;&#21508;&#31181;&#24212;&#29992;&#21019;&#36896;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce FITS, a lightweight yet powerful model for time series analysis. Unlike existing models that directly process raw time-domain data, FITS operates on the principle that time series can be manipulated through interpolation in the complex frequency domain. By discarding high-frequency components with negligible impact on time series data, FITS achieves performance comparable to state-of-the-art models for time series forecasting and anomaly detection tasks, while having a remarkably compact size of only approximately $10k$ parameters. Such a lightweight model can be easily trained and deployed in edge devices, creating opportunities for various applications. The anonymous code repo is available in: \url{https://anonymous.4open.science/r/FITS}
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;ProgSyn&#65292;&#31532;&#19968;&#20010;&#21487;&#32534;&#31243;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#65292;&#23427;&#20801;&#35768;&#23545;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#33258;&#23450;&#20041;&#65292;&#24182;&#19988;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#29983;&#25104;&#27169;&#22411;&#26469;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;&#36981;&#23432;&#33258;&#23450;&#20041;&#35268;&#33539;&#12290;</title><link>http://arxiv.org/abs/2307.03577</link><description>&lt;p&gt;
&#21487;&#32534;&#31243;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Programmable Synthetic Tabular Data Generation. (arXiv:2307.03577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03577
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;ProgSyn&#65292;&#31532;&#19968;&#20010;&#21487;&#32534;&#31243;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#65292;&#23427;&#20801;&#35768;&#23545;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#33258;&#23450;&#20041;&#65292;&#24182;&#19988;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#29983;&#25104;&#27169;&#22411;&#26469;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;&#36981;&#23432;&#33258;&#23450;&#20041;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38544;&#31169;&#12289;&#25968;&#25454;&#36136;&#37327;&#21644;&#25968;&#25454;&#20849;&#20139;&#30340;&#38480;&#21046;&#65292;&#22823;&#37327;&#30340;&#34920;&#26684;&#25968;&#25454;&#20173;&#28982;&#34987;&#20302;&#25928;&#21033;&#29992;&#12290;&#23613;&#31649;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#21407;&#22987;&#20998;&#24067;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#20294;&#22823;&#22810;&#25968;&#24212;&#29992;&#31243;&#24207;&#36824;&#38656;&#35201;&#39069;&#22806;&#30340;&#29983;&#25104;&#25968;&#25454;&#32422;&#26463;&#12290;&#29616;&#26377;&#30340;&#21512;&#25104;&#25968;&#25454;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#21482;&#22788;&#29702;&#29305;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#20363;&#22914;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25110;&#22686;&#21152;&#20844;&#24179;&#24615;&#65292;&#24182;&#19988;&#32570;&#20047;&#19968;&#20010;&#21487;&#35775;&#38382;&#30340;&#25509;&#21475;&#26469;&#22768;&#26126;&#19968;&#33324;&#35268;&#33539;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ProgSyn&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#32534;&#31243;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#65292;&#23427;&#20801;&#35768;&#23545;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#33258;&#23450;&#20041;&#12290;&#20026;&#20102;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#24182;&#36981;&#23432;&#33258;&#23450;&#20041;&#35268;&#33539;&#65292;ProgSyn&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#22312;&#25552;&#20379;&#30340;&#35268;&#33539;&#19978;&#33258;&#21160;&#25512;&#23548;&#20986;&#21487;&#24494;&#20998;&#25439;&#22833;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#20123;&#35268;&#33539;&#21487;&#20197;&#20351;&#29992;&#32479;&#35745;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large amounts of tabular data remain underutilized due to privacy, data quality, and data sharing limitations. While training a generative model producing synthetic data resembling the original distribution addresses some of these issues, most applications require additional constraints from the generated data. Existing synthetic data approaches are limited as they typically only handle specific constraints, e.g., differential privacy (DP) or increased fairness, and lack an accessible interface for declaring general specifications. In this work, we introduce ProgSyn, the first programmable synthetic tabular data generation algorithm that allows for comprehensive customization over the generated data. To ensure high data quality while adhering to custom specifications, ProgSyn pre-trains a generative model on the original dataset and fine-tunes it on a differentiable loss automatically derived from the provided specifications. These can be programmatically declared using statistical and
&lt;/p&gt;</description></item><item><title>ITA&#26159;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#30340;Transformer&#30340;&#33021;&#25928;&#39640;&#30340;Attention&#21644;Softmax&#21152;&#36895;&#22120;&#12290;&#36890;&#36807;&#21033;&#29992;8&#20301;&#37327;&#21270;&#21644;&#20165;&#22522;&#20110;&#25972;&#25968;&#20540;&#30340;&#21019;&#26032;Softmax&#23454;&#29616;&#65292;ITA&#23454;&#29616;&#20102;&#39640;&#33021;&#25928;&#30340;&#25512;&#29702;&#65292;&#22312;16.9 TOPS/W&#30340;&#33021;&#25928;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;Transformer&#21152;&#36895;&#22120;&#65292;&#24182;&#22312;5.93 TOPS/mm$^2$&#30340;&#38754;&#31215;&#25928;&#29575;&#19978;&#36229;&#36234;&#20102;&#23427;&#20204;&#12290;</title><link>http://arxiv.org/abs/2307.03493</link><description>&lt;p&gt;
ITA:&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#30340;Transformer&#30340;&#33021;&#25928;&#39640;&#30340;Attention&#21644;Softmax&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers. (arXiv:2307.03493v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03493
&lt;/p&gt;
&lt;p&gt;
ITA&#26159;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#30340;Transformer&#30340;&#33021;&#25928;&#39640;&#30340;Attention&#21644;Softmax&#21152;&#36895;&#22120;&#12290;&#36890;&#36807;&#21033;&#29992;8&#20301;&#37327;&#21270;&#21644;&#20165;&#22522;&#20110;&#25972;&#25968;&#20540;&#30340;&#21019;&#26032;Softmax&#23454;&#29616;&#65292;ITA&#23454;&#29616;&#20102;&#39640;&#33021;&#25928;&#30340;&#25512;&#29702;&#65292;&#22312;16.9 TOPS/W&#30340;&#33021;&#25928;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;Transformer&#21152;&#36895;&#22120;&#65292;&#24182;&#22312;5.93 TOPS/mm$^2$&#30340;&#38754;&#31215;&#25928;&#29575;&#19978;&#36229;&#36234;&#20102;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#38899;&#39057;&#22788;&#29702;&#31561;&#20854;&#20182;&#39046;&#22495;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;Transformer&#27169;&#22411;&#30340;&#39640;&#31639;&#26415;&#24378;&#24230;&#12289;&#22823;&#20869;&#23384;&#38656;&#27714;&#21644;&#22797;&#26434;&#25968;&#25454;&#27969;&#20381;&#36182;&#23548;&#33268;&#20102;&#20854;&#26377;&#25928;&#30828;&#20214;&#21152;&#36895;&#38754;&#20020;&#26032;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ITA&#65292;&#19968;&#31181;&#38024;&#23545;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#39640;&#25928;&#25512;&#29702;&#30340;Transformer&#21644;&#30456;&#20851;&#27169;&#22411;&#30340;&#26032;&#22411;&#21152;&#36895;&#22120;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;8&#20301;&#37327;&#21270;&#21644;&#20165;&#22522;&#20110;&#25972;&#25968;&#20540;&#30340;&#21019;&#26032;Softmax&#23454;&#29616;&#12290;&#36890;&#36807;&#22312;&#27969;&#27169;&#24335;&#19979;&#23454;&#26102;&#35745;&#31639;&#65292;&#25105;&#20204;&#30340;Softmax&#23454;&#29616;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#25968;&#25454;&#31227;&#21160;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;ITA&#22312;&#33021;&#25928;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;Transformer&#21152;&#36895;&#22120;&#20445;&#25345;&#31454;&#20105;&#21147;&#65292;&#36798;&#21040;&#20102;16.9 TOPS/W&#65292;&#21516;&#26102;&#22312;&#38754;&#31215;&#25928;&#29575;&#26041;&#38754;&#20197;5.93 TOPS/mm$^2$&#30340;&#25104;&#32489;&#36229;&#36234;&#20102;&#23427;&#20204;&#65292;&#22312;22&#32435;&#31859;&#23436;&#20840;&#32791;&#23613;&#30340;&#30789;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer networks have emerged as the state-of-the-art approach for natural language processing tasks and are gaining popularity in other domains such as computer vision and audio processing. However, the efficient hardware acceleration of transformer models poses new challenges due to their high arithmetic intensities, large memory requirements, and complex dataflow dependencies. In this work, we propose ITA, a novel accelerator architecture for transformers and related models that targets efficient inference on embedded systems by exploiting 8-bit quantization and an innovative softmax implementation that operates exclusively on integer values. By computing on-the-fly in streaming mode, our softmax implementation minimizes data movement and energy consumption. ITA achieves competitive energy efficiency with respect to state-of-the-art transformer accelerators with 16.9 TOPS/W, while outperforming them in area efficiency with 5.93 TOPS/mm$^2$ in 22 nm fully-depleted silicon-on-insu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23581;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#23558;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#36890;&#36807;&#28023;&#37327;&#30693;&#35782;&#26469;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#29983;&#25104;&#39044;&#27979;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.03393</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23581;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#23558;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#36890;&#36807;&#28023;&#37327;&#30693;&#35782;&#26469;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#29983;&#25104;&#39044;&#27979;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#22240;&#20854;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#20197;&#25991;&#26412;&#33410;&#28857;&#23646;&#24615;&#20026;&#20027;&#30340;&#22270;&#23398;&#20064;&#26368;&#27969;&#34892;&#30340;&#27969;&#31243;&#20027;&#35201;&#20381;&#36182;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#24182;&#21033;&#29992;&#27973;&#23618;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#21021;&#22987;&#33410;&#28857;&#34920;&#31034;&#65292;&#20294;&#23384;&#22312;&#36890;&#29992;&#30693;&#35782;&#21644;&#28145;&#21051;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35777;&#26126;&#20855;&#26377;&#24191;&#27867;&#30340;&#24120;&#35782;&#21644;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#24050;&#32463;&#39072;&#35206;&#20102;&#29616;&#26377;&#30340;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;LLMs&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20004;&#31181;&#21487;&#33021;&#30340;&#27969;&#31243;&#65306;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#21644;LLMs&#20316;&#20026;&#39044;&#27979;&#22120;&#12290;&#21069;&#32773;&#21033;&#29992;LLMs&#36890;&#36807;&#20854;&#28023;&#37327;&#30693;&#35782;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;GNNs&#29983;&#25104;&#39044;&#27979;&#12290;&#21518;&#32773;&#35797;&#22270;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#35299;&#37322;&#20013;&#65292;&#27867;&#21270;&#20102;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#26799;&#24230;&#22270;&#30340;&#21487;&#35299;&#37322;&#32479;&#35745;&#25968;&#25454;&#65292;&#22914;&#26368;&#39640;&#21152;&#26435;&#36335;&#24452;&#21644;&#29109;&#12290;&#20316;&#32773;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#21644;&#24212;&#29992;&#20110;BERT&#30340;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03056</link><description>&lt;p&gt;
&#27867;&#21270;&#21453;&#21521;&#20256;&#25773;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalizing Backpropagation for Gradient-Based Interpretability. (arXiv:2307.03056v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#35299;&#37322;&#20013;&#65292;&#27867;&#21270;&#20102;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#26799;&#24230;&#22270;&#30340;&#21487;&#35299;&#37322;&#32479;&#35745;&#25968;&#25454;&#65292;&#22914;&#26368;&#39640;&#21152;&#26435;&#36335;&#24452;&#21644;&#29109;&#12290;&#20316;&#32773;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#21644;&#24212;&#29992;&#20110;BERT&#30340;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#27169;&#22411;&#36755;&#20986;&#23545;&#36755;&#20837;&#30340;&#26799;&#24230;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25351;&#31034;&#21738;&#20123;&#36755;&#20837;&#29305;&#24449;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#39044;&#27979;&#24456;&#37325;&#35201;&#65292;&#20294;&#23427;&#20204;&#23545;&#27169;&#22411;&#26412;&#36523;&#30340;&#20869;&#37096;&#24037;&#20316;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;&#26799;&#24230;&#35745;&#31639;&#26159;&#20351;&#29992;&#21322;&#29615;&#30340;&#26356;&#19968;&#33324;&#24418;&#24335;&#30340;&#29305;&#20363;&#12290;&#36825;&#31181;&#35266;&#23519;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#27867;&#21270;&#65292;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#22270;&#30340;&#20854;&#20182;&#21487;&#35299;&#37322;&#32479;&#35745;&#25968;&#25454;&#65292;&#20363;&#22914;&#26368;&#39640;&#21152;&#26435;&#36335;&#24452;&#21644;&#29109;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#27867;&#21270;&#31639;&#27861;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#35745;&#31639;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30740;&#31350;BERT&#22312;&#20027;&#35859;&#25968;&#19968;&#33268;&#24615;&#20219;&#21153;&#65288;SVA&#65289;&#19978;&#30340;&#34892;&#20026;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#27169;&#22411;&#32452;&#20214;&#19978;&#36890;&#36807;&#30340;&#26799;&#24230;&#27969;&#37327;&#21453;&#26144;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model's output with respect to its inputs. While these methods can indicate which input features may be important for the model's prediction, they reveal little about the inner workings of the model itself. In this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings. This observation allows us to generalize the backpropagation algorithm to efficiently compute other interpretable statistics about the gradient graph of a neural network, such as the highest-weighted path and entropy. We implement this generalized algorithm, evaluate it on synthetic datasets to better understand the statistics it computes, and apply it to study BERT's behavior on the subject-verb number agreement task (SVA). With this method, we (a) validate that the amount of gradient flow through a component of a model reflects its impor
&lt;/p&gt;</description></item><item><title>PUFFIN&#26159;&#19968;&#31181;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#21644;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#24402;&#32435;&#20559;&#24046;&#33410;&#28857;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#33976;&#27773;&#21387;&#21147;&#39044;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#24402;&#32435;&#20559;&#24046;&#21644;&#22270;&#23884;&#20837;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;PUFFIN&#22312;&#39044;&#27979;&#20013;&#32988;&#36807;&#19981;&#20351;&#29992;&#24402;&#32435;&#20559;&#24046;&#25110;&#20351;&#29992;&#36890;&#29992;&#25551;&#36848;&#31526;&#30340;&#26367;&#20195;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.02903</link><description>&lt;p&gt;
PUFFIN: &#29992;&#20110;&#33976;&#27773;&#21387;&#21147;&#39044;&#27979;&#30340;&#36335;&#24452;&#32479;&#19968;&#21069;&#21521;&#25509;&#21475;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PUFFIN: A Path-Unifying Feed-Forward Interfaced Network for Vapor Pressure Prediction. (arXiv:2307.02903v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02903
&lt;/p&gt;
&lt;p&gt;
PUFFIN&#26159;&#19968;&#31181;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#21644;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#24402;&#32435;&#20559;&#24046;&#33410;&#28857;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#33976;&#27773;&#21387;&#21147;&#39044;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#24402;&#32435;&#20559;&#24046;&#21644;&#22270;&#23884;&#20837;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;PUFFIN&#22312;&#39044;&#27979;&#20013;&#32988;&#36807;&#19981;&#20351;&#29992;&#24402;&#32435;&#20559;&#24046;&#25110;&#20351;&#29992;&#36890;&#29992;&#25551;&#36848;&#31526;&#30340;&#26367;&#20195;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#33976;&#27773;&#21387;&#21147;&#23545;&#20110;&#24037;&#19994;&#21644;&#29615;&#22659;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#39564;&#30340;&#36164;&#28304;&#21644;&#21171;&#21160;&#24378;&#24230;&#65292;&#26080;&#27861;&#33719;&#24471;&#25152;&#26377;&#26377;&#20852;&#36259;&#30340;&#21270;&#21512;&#29289;&#30340;&#20934;&#30830;&#27979;&#37327;&#12290;&#24403;&#24076;&#26395;&#39044;&#27979;&#33976;&#27773;&#21387;&#21147;&#30340;&#28201;&#24230;&#30456;&#20851;&#20851;&#31995;&#26102;&#65292;&#36164;&#28304;&#21644;&#21171;&#21160;&#30340;&#38656;&#27714;&#36827;&#19968;&#27493;&#22686;&#21152;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PUFFIN&#65288;&#36335;&#24452;&#32479;&#19968;&#21069;&#21521;&#25509;&#21475;&#32593;&#32476;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#36801;&#31227;&#23398;&#20064;&#19982;&#21551;&#21457;&#20110;&#39046;&#22495;&#30693;&#35782;&#65288;&#23433;&#25176;&#19975;&#26041;&#31243;&#65289;&#30340;&#26032;&#24402;&#32435;&#20559;&#24046;&#33410;&#28857;&#32467;&#21512;&#36215;&#26469;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25913;&#21892;&#33976;&#27773;&#21387;&#21147;&#39044;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#24402;&#32435;&#20559;&#24046;&#21644;&#20351;&#29992;&#22270;&#23884;&#20837;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;PUFFIN&#20248;&#20110;&#19981;&#20351;&#29992;&#24402;&#32435;&#20559;&#24046;&#25110;&#20351;&#29992;&#36890;&#29992;&#25551;&#36848;&#31526;&#30340;&#26367;&#20195;&#31574;&#30053;&#12290;&#35813;&#26694;&#26550;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#34701;&#20837;&#20811;&#26381;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#19981;&#36275;&#30340;&#38480;&#21046;&#65292;&#23637;&#31034;&#20986;&#20102;&#20854;&#22312;&#26356;&#24191;&#27867;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting vapor pressure is vital for various industrial and environmental applications. However, obtaining accurate measurements for all compounds of interest is not possible due to the resource and labor intensity of experiments. The demand for resources and labor further multiplies when a temperature-dependent relationship for predicting vapor pressure is desired. In this paper, we propose PUFFIN (Path-Unifying Feed-Forward Interfaced Network), a machine learning framework that combines transfer learning with a new inductive bias node inspired by domain knowledge (the Antoine equation) to improve vapor pressure prediction. By leveraging inductive bias and transfer learning using graph embeddings, PUFFIN outperforms alternative strategies that do not use inductive bias or that use generic descriptors of compounds. The framework's incorporation of domain-specific knowledge to overcome the limitation of poor data availability shows its potential for broader applications in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#26041;&#27861;&#65288;IRDEC&#65289;&#65292;&#36890;&#36807;&#20869;&#22312;&#22870;&#21169;&#39537;&#21160;&#20197;&#21450;&#25506;&#32034;&#33719;&#24471;&#20808;&#21069;&#34892;&#20026;&#65292;&#24182;&#19982;&#31034;&#33539;&#20013;&#30340;&#20219;&#21153;&#29305;&#23450;&#34892;&#20026;&#36830;&#25509;&#65292;&#20174;&#32780;&#35299;&#20915;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19977;&#20010;&#23548;&#33322;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02889</link><description>&lt;p&gt;
&#23398;&#20064;&#25506;&#32034;&#20808;&#21069;&#34892;&#20026;&#26469;&#35299;&#20915;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Learning to Solve Tasks with Exploring Prior Behaviours. (arXiv:2307.02889v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#26041;&#27861;&#65288;IRDEC&#65289;&#65292;&#36890;&#36807;&#20869;&#22312;&#22870;&#21169;&#39537;&#21160;&#20197;&#21450;&#25506;&#32034;&#33719;&#24471;&#20808;&#21069;&#34892;&#20026;&#65292;&#24182;&#19982;&#31034;&#33539;&#20013;&#30340;&#20219;&#21153;&#29305;&#23450;&#34892;&#20026;&#36830;&#25509;&#65292;&#20174;&#32780;&#35299;&#20915;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19977;&#20010;&#23548;&#33322;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#31034;&#33539;&#24120;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#20219;&#21153;&#24448;&#24448;&#20855;&#26377;&#19982;&#31034;&#33539;&#19981;&#21516;&#30340;&#21021;&#22987;&#26465;&#20214;&#65292;&#36825;&#23601;&#38656;&#35201;&#39069;&#22806;&#30340;&#20808;&#21069;&#34892;&#20026;&#12290;&#20363;&#22914;&#65292;&#20551;&#35774;&#25105;&#20204;&#24471;&#21040;&#20102;&#8220;&#20174;&#25171;&#24320;&#25277;&#23625;&#20013;&#25343;&#21462;&#29289;&#20307;&#8221;&#30340;&#20219;&#21153;&#30340;&#31034;&#33539;&#65292;&#20294;&#22312;&#35757;&#32451;&#26102;&#25277;&#23625;&#26159;&#20851;&#38381;&#30340;&#12290;&#22914;&#26524;&#27809;&#26377;&#25484;&#25569;&#25171;&#24320;&#25277;&#23625;&#30340;&#20808;&#21069;&#34892;&#20026;&#65292;&#26426;&#22120;&#20154;&#24456;&#38590;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#22312;&#22870;&#21169;&#39537;&#21160;&#30340;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#26041;&#27861;&#65288;IRDEC&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36171;&#20104;&#26234;&#33021;&#20307;&#25506;&#32034;&#21644;&#33719;&#21462;&#25152;&#38656;&#30340;&#20808;&#21069;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#24182;&#19982;&#31034;&#33539;&#20013;&#30340;&#20219;&#21153;&#29305;&#23450;&#34892;&#20026;&#36830;&#25509;&#65292;&#20174;&#32780;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#23637;&#31034;&#20808;&#21069;&#34892;&#20026;&#31034;&#33539;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#23548;&#33322;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demonstrations are widely used in Deep Reinforcement Learning (DRL) for facilitating solving tasks with sparse rewards. However, the tasks in real-world scenarios can often have varied initial conditions from the demonstration, which would require additional prior behaviours. For example, consider we are given the demonstration for the task of \emph{picking up an object from an open drawer}, but the drawer is closed in the training. Without acquiring the prior behaviours of opening the drawer, the robot is unlikely to solve the task. To address this, in this paper we propose an Intrinsic Rewards Driven Example-based Control \textbf{(IRDEC)}. Our method can endow agents with the ability to explore and acquire the required prior behaviours and then connect to the task-specific behaviours in the demonstration to solve sparse-reward tasks without requiring additional demonstration of the prior behaviours. The performance of our method outperforms other baselines on three navigation tasks a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#23454;&#38469;&#32593;&#32476;&#25968;&#25454;&#23545;5G&#32593;&#32476;&#20013;&#30340;&#39044;&#27979;&#24615;&#24310;&#36831;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;Hypoexponential&#20998;&#24067;&#20026;&#22522;&#30784;&#30340;&#29992;&#25143;&#38754;&#24310;&#36831;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#21644;&#22270;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#27010;&#29575;&#22238;&#24402;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#24615;&#39044;&#27979;&#30340;&#23454;&#39564;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#39044;&#27979;&#26694;&#26550;&#36866;&#29992;&#20110;&#19981;&#21516;&#24773;&#26223;&#19979;&#30340;&#31227;&#21160;&#24615;&#12289;&#22478;&#24066;&#20132;&#36890;&#21644;&#31038;&#20132;&#32858;&#20250;&#12290;</title><link>http://arxiv.org/abs/2307.02329</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#27979;&#24615;5G&#24310;&#36831;&#65306;&#21033;&#29992;&#32593;&#32476;&#27979;&#37327;&#36827;&#34892;&#29702;&#35770;&#21644;&#23454;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Data-driven Predictive Latency for 5G: A Theoretical and Experimental Analysis Using Network Measurements. (arXiv:2307.02329v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#23454;&#38469;&#32593;&#32476;&#25968;&#25454;&#23545;5G&#32593;&#32476;&#20013;&#30340;&#39044;&#27979;&#24615;&#24310;&#36831;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;Hypoexponential&#20998;&#24067;&#20026;&#22522;&#30784;&#30340;&#29992;&#25143;&#38754;&#24310;&#36831;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#21644;&#22270;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#27010;&#29575;&#22238;&#24402;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#24615;&#39044;&#27979;&#30340;&#23454;&#39564;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#39044;&#27979;&#26694;&#26550;&#36866;&#29992;&#20110;&#19981;&#21516;&#24773;&#26223;&#19979;&#30340;&#31227;&#21160;&#24615;&#12289;&#22478;&#24066;&#20132;&#36890;&#21644;&#31038;&#20132;&#32858;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;5G&#26381;&#21153;&#21644;&#24212;&#29992;&#30340;&#20986;&#29616;&#65292;&#20855;&#26377;&#32465;&#23450;&#24310;&#36831;&#35201;&#27714;&#21644;&#20445;&#35777;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#30340;&#38656;&#27714;&#36843;&#20351;&#32593;&#32476;&#31649;&#29702;&#31243;&#24207;&#32435;&#20837;&#33258;&#20027;&#21644;&#20027;&#21160;&#20915;&#31574;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#31227;&#21160;&#32593;&#32476;&#36816;&#33829;&#21830;&#65288;MNOs&#65289;&#21487;&#20197;&#35775;&#38382;&#30340;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#65292;&#23545;5G&#32593;&#32476;&#20013;&#30340;&#39044;&#27979;&#24615;&#24310;&#36831;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#20316;&#20026;Hypoexponential&#20998;&#24067;&#30340;&#29992;&#25143;&#38754;&#24310;&#36831;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#36890;&#36807;&#19982;&#23454;&#35777;&#27979;&#37327;&#30340;&#27604;&#36739;&#20998;&#26512;&#36827;&#34892;&#39564;&#35777;&#65292;&#65288;ii&#65289;&#25105;&#20204;&#36827;&#34892;&#20102;&#27010;&#29575;&#22238;&#24402;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#24615;&#39044;&#27979;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#21033;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#22914;&#36125;&#21494;&#26031;&#23398;&#20064;&#65288;BL&#65289;&#21644;&#22270;&#26426;&#22120;&#23398;&#20064;&#65288;GML&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#36710;&#36742;&#31227;&#21160;&#12289;&#23494;&#38598;&#22478;&#21306;&#20132;&#36890;&#21644;&#31038;&#20132;&#32858;&#20250;&#22330;&#26223;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#27979;&#35797;&#25105;&#20204;&#30340;&#39044;&#27979;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of novel 5G services and applications with binding latency requirements and guaranteed Quality of Service (QoS) hastened the need to incorporate autonomous and proactive decision-making in network management procedures. The objective of our study is to provide a thorough analysis of predictive latency within 5G networks by utilizing real-world network data that is accessible to mobile network operators (MNOs). In particular, (i) we present an analytical formulation of the user-plane latency as a Hypoexponential distribution, which is validated by means of a comparative analysis with empirical measurements, and (ii) we conduct experimental results of probabilistic regression, anomaly detection, and predictive forecasting leveraging on emerging domains in Machine Learning (ML), such as Bayesian Learning (BL) and Machine Learning on Graphs (GML). We test our predictive framework using data gathered from scenarios of vehicular mobility, dense-urban traffic, and social gathering 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#21512;&#23398;&#20064;&#26041;&#27861;(OKO)&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#21644;&#26657;&#20934;&#19981;&#33391;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#38598;&#21512;&#30340;&#20132;&#21449;&#29109;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#25928;&#26524;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.02245</link><description>&lt;p&gt;
&#20934;&#30830;&#21644;&#26657;&#20934;&#27169;&#22411;&#30340;&#38598;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Set Learning for Accurate and Calibrated Models. (arXiv:2307.02245v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02245
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#21512;&#23398;&#20064;&#26041;&#27861;(OKO)&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#21644;&#26657;&#20934;&#19981;&#33391;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#38598;&#21512;&#30340;&#20132;&#21449;&#29109;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#25928;&#26524;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#21644;&#26657;&#20934;&#19981;&#33391;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24456;&#24120;&#35265;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#26631;&#20934;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26102;&#24456;&#38590;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22855;&#25968;-$k$-&#21435;&#38500;&#23398;&#20064;&#65288;OKO&#65289;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#21270;&#38598;&#21512;&#30340;&#20132;&#21449;&#29109;&#35823;&#24046;&#32780;&#19981;&#26159;&#21333;&#20010;&#31034;&#20363;&#30340;&#35823;&#24046;&#26469;&#23454;&#29616;&#12290;&#36825;&#33258;&#28982;&#22320;&#20351;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#31034;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#20351;&#29992;&#30828;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#24182;&#19988;&#19981;&#36827;&#34892;&#20219;&#20309;&#39069;&#22806;&#30340;&#26657;&#20934;&#21442;&#25968;&#35843;&#25972;&#65292;&#22914;&#28201;&#24230;&#32553;&#25918;&#65292;OKO&#36890;&#24120;&#20063;&#33021;&#33719;&#24471;&#26356;&#22909;&#30340;&#26657;&#20934;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;OKO&#33258;&#28982;&#22320;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#26657;&#20934;&#25928;&#26524;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;OKO&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36866;&#24212;&#35768;&#22810;&#19981;&#21516;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets rather than for single examples. This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes. Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling. We provide theoretical justification, establishing that OKO naturally yields better calibration, and provide extensive experimental analyses that corroborate our theoretical findings. We emphasize that OKO is a general framework that can be easily adapted to many settings and the
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ChiENN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#25163;&#24615;&#25935;&#24863;&#30340;&#20449;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#20351;&#24471;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#21306;&#20998;&#21270;&#23398;&#21270;&#21512;&#29289;&#21644;&#20854;&#23545;&#26144;&#20307;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#30340;&#25163;&#24615;&#25935;&#24863;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02198</link><description>&lt;p&gt;
ChiENN: &#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#23545;&#20998;&#23376;&#25163;&#24615;&#30340;&#25317;&#25265;
&lt;/p&gt;
&lt;p&gt;
ChiENN: Embracing Molecular Chirality with Graph Neural Networks. (arXiv:2307.02198v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02198
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ChiENN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#25163;&#24615;&#25935;&#24863;&#30340;&#20449;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#20351;&#24471;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#21306;&#20998;&#21270;&#23398;&#21270;&#21512;&#29289;&#21644;&#20854;&#23545;&#26144;&#20307;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#30340;&#25163;&#24615;&#25935;&#24863;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#20013;&#36215;&#30528;&#22522;&#30784;&#20316;&#29992;&#65292;&#23588;&#20854;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#19981;&#33021;&#25429;&#25417;&#25163;&#24615;&#30340;&#27010;&#24565;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#26080;&#27861;&#21306;&#20998;&#21270;&#23398;&#21270;&#21512;&#29289;&#30340;3D&#22270;&#19982;&#20854;&#23545;&#26144;&#20307;&#65288;&#23545;&#26144;&#24322;&#26500;&#20307;&#65289;&#30340;&#24046;&#24322;&#12290;&#21306;&#20998;&#23545;&#26144;&#24322;&#26500;&#20307;&#30340;&#33021;&#21147;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#23545;&#26144;&#24322;&#26500;&#20307;&#21487;&#33021;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#29983;&#29289;&#21270;&#23398;&#24615;&#36136;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#35777;&#26126;&#30340;&#20449;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#20351;&#24471;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#33410;&#28857;&#37051;&#23621;&#30340;&#39034;&#24207;&#25935;&#24863;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#36890;&#29992;&#27010;&#24565;&#24212;&#29992;&#22312;&#20998;&#23376;&#25163;&#24615;&#30340;&#32972;&#26223;&#19979;&#65292;&#26500;&#24314;&#20102;&#21487;&#20197;&#38468;&#21152;&#21040;&#20219;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25163;&#24615;&#36793;&#32536;&#31070;&#32463;&#32593;&#32476;&#65288;ChiENN&#65289;&#23618;&#65292;&#20197;&#23454;&#29616;&#23545;&#25163;&#24615;&#30340;&#24847;&#35782;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;ChiENN&#23618;&#28155;&#21152;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22312;&#23545;&#25163;&#24615;&#25935;&#24863;&#30340;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) play a fundamental role in many deep learning problems, in particular in cheminformatics. However, typical GNNs cannot capture the concept of chirality, which means they do not distinguish between the 3D graph of a chemical compound and its mirror image (enantiomer). The ability to distinguish between enantiomers is important especially in drug discovery because enantiomers can have very distinct biochemical properties. In this paper, we propose a theoretically justified message-passing scheme, which makes GNNs sensitive to the order of node neighbors. We apply that general concept in the context of molecular chirality to construct Chiral Edge Neural Network (ChiENN) layer which can be appended to any GNN model to enable chirality-awareness. Our experiments show that adding ChiENN layers to a GNN outperforms current state-of-the-art methods in chiral-sensitive molecular property prediction tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#38024;&#23545;&#21333;&#39046;&#22495;&#24191;&#20041;&#30446;&#26631;&#26816;&#27979;&#30340;&#26032;&#26694;&#26550;SRCD&#65292;&#36890;&#36807;&#23398;&#20064;&#21644;&#32500;&#25252;&#33258;&#22686;&#24191;&#20041;&#36328;&#39046;&#22495;&#26679;&#26412;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.01750</link><description>&lt;p&gt;
SRCD&#65306;&#22810;&#20010;&#22797;&#21512;&#39046;&#22495;&#30340;&#35821;&#20041;&#25512;&#29702;&#29992;&#20110;&#21333;&#39046;&#22495;&#24191;&#20041;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection. (arXiv:2307.01750v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#38024;&#23545;&#21333;&#39046;&#22495;&#24191;&#20041;&#30446;&#26631;&#26816;&#27979;&#30340;&#26032;&#26694;&#26550;SRCD&#65292;&#36890;&#36807;&#23398;&#20064;&#21644;&#32500;&#25252;&#33258;&#22686;&#24191;&#20041;&#36328;&#39046;&#22495;&#26679;&#26412;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#38024;&#23545;&#21333;&#39046;&#22495;&#24191;&#20041;&#30446;&#26631;&#26816;&#27979;&#65288;&#21363;Single-DGOD&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#20854;&#20013;&#25105;&#20204;&#23545;&#23398;&#20064;&#21644;&#32500;&#25252;&#33258;&#22686;&#24191;&#20041;&#36328;&#39046;&#22495;&#26679;&#26412;&#30340;&#35821;&#20041;&#32467;&#26500;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24863;&#20852;&#36259;&#12290;&#19982;&#22312;&#22810;&#20010;&#28304;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;DGOD&#19981;&#21516;&#65292;Single-DGOD&#35201;&#20165;&#20351;&#29992;&#19968;&#20010;&#21333;&#19968;&#28304;&#39046;&#22495;&#24456;&#38590;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#22810;&#20010;&#30446;&#26631;&#39046;&#22495;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#37319;&#29992;&#20102;&#19982;DGOD&#31867;&#20284;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#36890;&#36807;&#35299;&#32806;&#21512;&#25110;&#21387;&#32553;&#35821;&#20041;&#31354;&#38388;&#26469;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20004;&#20010;&#28508;&#22312;&#30340;&#38480;&#21046;&#65306;1&#65289;&#30001;&#20110;&#26497;&#24230;&#31232;&#32570;&#30340;&#21333;&#39046;&#22495;&#25968;&#25454;&#65292;&#20266;&#23646;&#24615;-&#26631;&#31614;&#30456;&#20851;&#24615;&#65307;2&#65289;&#36890;&#24120;&#24573;&#30053;&#35821;&#20041;&#32467;&#26500;&#20449;&#24687;&#65292;&#21363;&#25105;&#20204;&#21457;&#29616;&#31034;&#20363;&#20013;&#30340;&#23454;&#20363;&#32423;&#35821;&#20041;&#20851;&#31995;&#30340;&#30456;&#20284;&#24615;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;Single-DGOD&#30340;&#22797;&#21512;&#39046;&#22495;&#30340;&#35821;&#20041;&#25512;&#29702;&#65288;SRCD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a novel framework for single-domain generalized object detection (i.e., Single-DGOD), where we are interested in learning and maintaining the semantic structures of self-augmented compound cross-domain samples to enhance the model's generalization ability. Different from DGOD trained on multiple source domains, Single-DGOD is far more challenging to generalize well to multiple target domains with only one single source domain. Existing methods mostly adopt a similar treatment from DGOD to learn domain-invariant features by decoupling or compressing the semantic space. However, there may have two potential limitations: 1) pseudo attribute-label correlation, due to extremely scarce single-domain data; and 2) the semantic structural information is usually ignored, i.e., we found the affinities of instance-level semantic relations in samples are crucial to model generalization. In this paper, we introduce Semantic Reasoning with Compound Domains (SRCD) for Single-DGOD. 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20381;&#36182;ERM&#39044;&#35328;&#26426;&#35843;&#29992;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#38480;&#30340;&#36951;&#25022;&#65292;&#24182;&#22312;&#19981;&#21487;&#30693;&#24773;&#20917;&#19979;&#20855;&#26377;&#20122;&#32447;&#24615;&#22686;&#38271;&#30340;&#36951;&#25022;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20379;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#29992;&#20110;&#38750;&#21442;&#25968;&#21338;&#24328;&#29615;&#22659;&#20013;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;&#20165;&#20381;&#36182;&#26368;&#20339;&#21709;&#24212;&#39044;&#35328;&#26426;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25910;&#25947;&#21040;&#36817;&#20284;&#26497;&#23567;-&#26497;&#22823;&#22343;&#34913;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.01689</link><description>&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#21644;&#20351;&#29992;ERM&#39044;&#35328;&#26426;&#35299;&#20915;&#26080;&#31351;&#21338;&#24328;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Learning and Solving Infinite Games with an ERM Oracle. (arXiv:2307.01689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01689
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20381;&#36182;ERM&#39044;&#35328;&#26426;&#35843;&#29992;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#38480;&#30340;&#36951;&#25022;&#65292;&#24182;&#22312;&#19981;&#21487;&#30693;&#24773;&#20917;&#19979;&#20855;&#26377;&#20122;&#32447;&#24615;&#22686;&#38271;&#30340;&#36951;&#25022;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20379;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#29992;&#20110;&#38750;&#21442;&#25968;&#21338;&#24328;&#29615;&#22659;&#20013;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;&#20165;&#20381;&#36182;&#26368;&#20339;&#21709;&#24212;&#39044;&#35328;&#26426;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25910;&#25947;&#21040;&#36817;&#20284;&#26497;&#23567;-&#26497;&#22823;&#22343;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;ERM&#36275;&#20197;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#30340;&#30446;&#26631;&#65292;&#20294;&#22312;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#19979;&#24182;&#38750;&#22914;&#27492;&#65292;&#36890;&#24120;&#30340;&#27010;&#24565;&#31867;&#31639;&#27861;&#20381;&#36182;&#35745;&#31639;&#25928;&#29575;&#36739;&#20302;&#30340;&#39044;&#35328;&#26426;&#65292;&#22914;&#26631;&#20934;&#26368;&#20248;&#31639;&#27861;(SOA)&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20381;&#36182;ERM&#39044;&#35328;&#26426;&#35843;&#29992;&#30340;&#22312;&#32447;&#20108;&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#22312;&#21487;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#38480;&#30340;&#36951;&#25022;(regret)&#65292;&#22312;&#19981;&#21487;&#30693;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#20122;&#32447;&#24615;&#22686;&#38271;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#36890;&#36807;&#24213;&#23618;&#27010;&#24565;&#31867;&#30340;Littlestone&#21644;&#38408;&#20540;&#32500;&#24230;&#26469;&#38480;&#21046;&#36951;&#25022;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#29992;&#20110;&#38750;&#21442;&#25968;&#21338;&#24328;&#65292;&#20854;&#20013;ERM&#39044;&#35328;&#26426;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#26368;&#20339;&#21709;&#24212;&#39044;&#35328;&#26426;&#65292;&#26681;&#25454;&#20854;&#20182;&#29609;&#23478;&#30340;&#28216;&#25103;&#21382;&#21490;&#25214;&#21040;&#19968;&#20010;&#29609;&#23478;&#30340;&#26368;&#20339;&#21709;&#24212;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20165;&#20381;&#36182;&#26368;&#20339;&#21709;&#24212;&#39044;&#35328;&#26426;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25910;&#25947;&#21040;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#30340;&#36817;&#20284;&#26497;&#23567;-&#26497;&#22823;&#22343;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
While ERM suffices to attain near-optimal generalization error in the stochastic learning setting, this is not known to be the case in the online learning setting, where algorithms for general concept classes rely on computationally inefficient oracles such as the Standard Optimal Algorithm (SOA). In this work, we propose an algorithm for online binary classification setting that relies solely on ERM oracle calls, and show that it has finite regret in the realizable setting and sublinearly growing regret in the agnostic setting. We bound the regret in terms of the Littlestone and threshold dimensions of the underlying concept class.  We obtain similar results for nonparametric games, where the ERM oracle can be interpreted as a best response oracle, finding the best response of a player to a given history of play of the other players. In this setting, we provide learning algorithms that only rely on best response oracles and converge to approximate-minimax equilibria in two-player zero
&lt;/p&gt;</description></item><item><title>vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2307.01226</link><description>&lt;p&gt;
vONTSS&#65306;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01226
&lt;/p&gt;
&lt;p&gt;
vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21463;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21551;&#21457;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTM&#65289;&#24341;&#36215;&#20102;&#24456;&#22810;&#30740;&#31350;&#20852;&#36259;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;vONTSS&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;von Mises-Fisher&#65288;vMF&#65289;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#26368;&#20248;&#20256;&#36755;&#12290;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#24403;&#25552;&#20379;&#27599;&#20010;&#20027;&#39064;&#30340;&#23569;&#37327;&#20851;&#38190;&#35789;&#26102;&#65292;vONTSS&#29983;&#25104;&#28508;&#22312;&#20027;&#39064;&#24182;&#20248;&#21270;&#20027;&#39064;-&#20851;&#38190;&#35789;&#36136;&#37327;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;vONTSS&#36824;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;&#26368;&#36817;&#30340;NTM&#65306;vONTSS&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#39640;&#24230;&#32858;&#31867;&#21644;&#36830;&#36143;&#30340;&#20027;&#39064;&#12290;&#23427;&#20063;&#27604;&#29616;&#26377;-&#25163;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30001;&#26041;&#21521;&#30693;&#35782;&#33976;&#39311;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20849;&#21516;&#22686;&#38271;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21516;&#26102;&#35757;&#32451;&#20004;&#20010;&#36739;&#27973;&#30340;GNN&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#27969;&#21644;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2307.00534</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30001;&#26041;&#21521;&#30693;&#35782;&#33976;&#39311;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#20849;&#21516;&#22686;&#38271;
&lt;/p&gt;
&lt;p&gt;
Shared Growth of Graph Neural Networks via Free-direction Knowledge Distillation. (arXiv:2307.00534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30001;&#26041;&#21521;&#30693;&#35782;&#33976;&#39311;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20849;&#21516;&#22686;&#38271;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21516;&#26102;&#35757;&#32451;&#20004;&#20010;&#36739;&#27973;&#30340;GNN&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#27969;&#21644;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#25552;&#21319;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24615;&#33021;&#26377;&#25928;&#65292;&#20854;&#20013;&#20856;&#22411;&#30446;&#26631;&#26159;&#23558;&#28145;&#23618;&#25945;&#24072;GNN&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#26356;&#27973;&#30340;&#23398;&#29983;GNN&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#36807;&#21442;&#25968;&#21270;&#21644;&#36807;&#24179;&#28369;&#38382;&#39064;&#65292;&#35757;&#32451;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#28145;&#23618;GNN&#36890;&#24120;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#23548;&#33268;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26080;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#33258;&#30001;&#26041;&#21521;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#31216;&#20026;FreeKD&#65292;&#23427;&#19981;&#20877;&#38656;&#35201;&#25552;&#20379;&#19968;&#20010;&#26356;&#28145;&#23618;&#27425;&#30340;&#20248;&#21270;&#33391;&#22909;&#30340;&#25945;&#24072;GNN&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#21327;&#21516;&#23398;&#20064;&#20004;&#20010;&#36739;&#27973;&#30340;GNN&#65292;&#20197;&#20415;&#22312;&#23427;&#20204;&#20043;&#38388;&#20132;&#25442;&#30693;&#35782;&#12290;&#30001;&#20110;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#20856;&#22411;&#30340;GNN&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22312;&#19981;&#21516;&#33410;&#28857;&#34920;&#29616;&#20986;&#26356;&#22909;&#21644;&#26356;&#24046;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#21644;&#33258;&#30001;&#26041;&#21521;&#30340;&#30693;&#35782;&#20256;&#36882;&#31574;&#30053;&#65292;&#23427;&#28041;&#21450;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) has shown to be effective to boost the performance of graph neural networks (GNNs), where the typical objective is to distill knowledge from a deeper teacher GNN into a shallower student GNN. However, it is often quite challenging to train a satisfactory deeper GNN due to the well-known over-parametrized and over-smoothing issues, leading to invalid knowledge transfer in practical applications. In this paper, we propose the first Free-direction Knowledge Distillation framework via reinforcement learning for GNNs, called FreeKD, which is no longer required to provide a deeper well-optimized teacher GNN. Our core idea is to collaboratively learn two shallower GNNs in an effort to exchange knowledge between them via reinforcement learning in a hierarchical way. As we observe that one typical GNN model often exhibits better and worse performances at different nodes during training, we devise a dynamic and free-direction knowledge transfer strategy that involves 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;GenRec&#65292;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#25512;&#33616;&#39033;&#32780;&#19981;&#26159;&#35745;&#31639;&#25490;&#21517;&#20998;&#25968;&#65292;&#21033;&#29992;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#29702;&#35299;&#33021;&#21147;&#26469;&#29983;&#25104;&#30456;&#20851;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2307.00457</link><description>&lt;p&gt;
GenRec:&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#24335;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
GenRec: Large Language Model for Generative Recommendation. (arXiv:2307.00457v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;GenRec&#65292;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#25512;&#33616;&#39033;&#32780;&#19981;&#26159;&#35745;&#31639;&#25490;&#21517;&#20998;&#25968;&#65292;&#21033;&#29992;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#29702;&#35299;&#33021;&#21147;&#26469;&#29983;&#25104;&#30456;&#20851;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Model&#65292;LLM)&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#24335;&#25512;&#33616;&#33539;&#24335;&#19979;&#65292;&#23427;&#20204;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#30340;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#36827;&#34892;&#25512;&#33616;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#31995;&#32479;(GenRec)&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#25512;&#33616;&#39033;&#65292;&#32780;&#19981;&#26159;&#20687;&#20256;&#32479;&#30340;&#21028;&#21035;&#24335;&#25512;&#33616;&#31995;&#32479;&#19968;&#26679;&#36880;&#20010;&#35745;&#31639;&#27599;&#20010;&#20505;&#36873;&#39033;&#30340;&#25490;&#21517;&#20998;&#25968;&#12290;GenRec&#21033;&#29992;LLM&#30340;&#29702;&#35299;&#33021;&#21147;&#26469;&#35299;&#37322;&#19978;&#19979;&#25991;&#12289;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#24182;&#29983;&#25104;&#30456;&#20851;&#25512;&#33616;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#20016;&#23500;&#30693;&#35782;&#26469;&#23436;&#25104;&#25512;&#33616;&#20219;&#21153;&#12290;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#19987;&#38376;&#30340;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;LLM&#29702;&#35299;&#25512;&#33616;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLM) have emerged as powerful tools for diverse natural language processing tasks. However, their potential for recommender systems under the generative recommendation paradigm remains relatively unexplored. This paper presents an innovative approach to recommendation systems using large language models (LLMs) based on text data. In this paper, we present a novel LLM for generative recommendation (GenRec) that utilized the expressive power of LLM to directly generate the target item to recommend, rather than calculating ranking score for each candidate item one by one as in traditional discriminative recommendation. GenRec uses LLM's understanding ability to interpret context, learn user preferences, and generate relevant recommendation. Our proposed approach leverages the vast knowledge encoded in large language models to accomplish recommendation tasks. We first we formulate specialized prompts to enhance the ability of LLM to comprehend recomm
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#36830;&#32493;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#65288;TESGAN&#65289;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;GAN&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#21462;&#20195;&#31163;&#25955;&#30340;&#26631;&#35760;&#65292;&#20351;&#24471;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.17181</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#26080;&#30417;&#30563;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#29992;&#20110;&#25991;&#26412;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#36830;&#32493;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#65288;TESGAN&#65289;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;GAN&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#21462;&#20195;&#31163;&#25955;&#30340;&#26631;&#35760;&#65292;&#20351;&#24471;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#21512;&#25104;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#31454;&#20105;&#26469;&#21019;&#24314;&#36924;&#30495;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;GAN&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#30340;&#38480;&#21046;&#12290;&#22240;&#20026;&#33258;&#28982;&#35821;&#35328;&#30001;&#31163;&#25955;&#30340;&#26631;&#35760;&#32452;&#25104;&#65292;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#36935;&#21040;&#22256;&#38590;&#65307;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#25991;&#26412;-GAN&#30740;&#31350;&#20351;&#29992;&#22870;&#21169;&#31995;&#32479;&#20197;&#38543;&#26426;&#26631;&#35760;&#20026;&#22522;&#30784;&#29983;&#25104;&#21477;&#23376;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#30340;&#29983;&#25104;&#22120;&#22312;&#23545;&#25239;&#35757;&#32451;&#20043;&#21069;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23548;&#33268;&#21512;&#25104;&#30340;&#21477;&#23376;&#37325;&#22797;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#21407;&#22987;GAN&#30340;&#26694;&#26550;&#26469;&#21512;&#25104;&#21477;&#23376;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;TESGAN&#65289;&#65292;&#23427;&#29983;&#25104;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#26469;&#35299;&#20915;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20840;&#38754;&#22823;&#35268;&#27169;&#23454;&#39564;&#30740;&#31350;&#65292;&#31995;&#32479;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;TPP&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#20102;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2306.17066</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#26102;&#24577;&#28857;&#36807;&#31243;&#27169;&#22411;&#22312;&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Predictive Accuracy of Neural Temporal Point Process Models for Continuous-time Event Data. (arXiv:2306.17066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17066
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20840;&#38754;&#22823;&#35268;&#27169;&#23454;&#39564;&#30740;&#31350;&#65292;&#31995;&#32479;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;TPP&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#20102;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24577;&#28857;&#36807;&#31243;&#65288;TPP&#65289;&#26159;&#36830;&#32493;&#26102;&#38388;&#20013;&#24314;&#27169;&#24322;&#27493;&#20107;&#20214;&#24207;&#21015;&#30340;&#26631;&#20934;&#25968;&#23398;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;TPP&#27169;&#22411;&#24120;&#24120;&#21463;&#38480;&#20110;&#24378;&#20551;&#35774;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#22797;&#26434;&#30495;&#23454;&#19990;&#30028;&#20107;&#20214;&#21160;&#24577;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#31070;&#32463;TPP&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#26469;&#25552;&#20379;&#26356;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#24314;&#27169;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#31070;&#32463;TPP&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#32570;&#20047;&#32479;&#19968;&#30340;&#35774;&#32622;&#65292;&#20381;&#36182;&#19981;&#21516;&#30340;&#22522;&#32447;&#12289;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#37197;&#32622;&#12290;&#36825;&#20351;&#24471;&#38590;&#20197;&#30830;&#23450;&#25512;&#21160;&#39044;&#27979;&#20934;&#30830;&#24615;&#25913;&#36827;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#38459;&#30861;&#20102;&#30740;&#31350;&#36827;&#23637;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#30740;&#31350;&#65292;&#31995;&#32479;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;TPP&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Point Processes (TPPs) serve as the standard mathematical framework for modeling asynchronous event sequences in continuous time. However, classical TPP models are often constrained by strong assumptions, limiting their ability to capture complex real-world event dynamics. To overcome this limitation, researchers have proposed Neural TPPs, which leverage neural network parametrizations to offer more flexible and efficient modeling. While recent studies demonstrate the effectiveness of Neural TPPs, they often lack a unified setup, relying on different baselines, datasets, and experimental configurations. This makes it challenging to identify the key factors driving improvements in predictive accuracy, hindering research progress. To bridge this gap, we present a comprehensive large-scale experimental study that systematically evaluates the predictive accuracy of state-of-the-art neural TPP models. Our study encompasses multiple real-world and synthetic event sequence datasets, 
&lt;/p&gt;</description></item><item><title>OSP&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#38454;&#27573;&#21516;&#27493;&#21644;&#26412;&#22320;&#26799;&#24230;&#20462;&#27491;&#26469;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#65292;&#36991;&#20813;&#20102;&#31934;&#24230;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2306.16926</link><description>&lt;p&gt;
OSP: &#20351;&#29992;&#20004;&#38454;&#27573;&#21516;&#27493;&#25552;&#21319;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
OSP: Boosting Distributed Model Training with 2-stage Synchronization. (arXiv:2306.16926v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16926
&lt;/p&gt;
&lt;p&gt;
OSP&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#38454;&#27573;&#21516;&#27493;&#21644;&#26412;&#22320;&#26799;&#24230;&#20462;&#27491;&#26469;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#65292;&#36991;&#20813;&#20102;&#31934;&#24230;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#65288;DDL&#65289;&#26159;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#30340;&#25928;&#29575;&#12290;&#38543;&#30528;DDL&#33410;&#28857;&#30340;&#35745;&#31639;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#65292;&#33410;&#28857;&#20043;&#38388;&#30340;&#32593;&#32476;&#36830;&#25509;&#25104;&#20026;&#19968;&#20010;&#20027;&#35201;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#21442;&#25968;&#26381;&#21153;&#22120;&#24335;DDL&#20013;&#36825;&#20010;&#29942;&#39048;&#38382;&#39064;&#65292;&#20154;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#26799;&#24230;&#21387;&#32553;&#21644;&#25913;&#36827;&#30340;&#27169;&#22411;&#21516;&#27493;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#20002;&#22833;&#26799;&#24230;&#32780;&#20986;&#29616;&#31934;&#24230;&#25439;&#22833;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#21516;&#27493;&#30340;&#21534;&#21520;&#37327;&#30340;&#25552;&#21319;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21516;&#27493;&#26041;&#27861;&#65292;&#21517;&#20026;Overlapped Synchronization Parallel&#65288;OSP&#65289;&#65292;&#23427;&#37319;&#29992;&#20004;&#38454;&#27573;&#21516;&#27493;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#36890;&#20449;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#26412;&#22320;&#26799;&#24230;&#30340;&#21442;&#25968;&#20462;&#27491;&#65288;LGP&#65289;&#26469;&#36991;&#20813;&#30001;&#36807;&#26399;&#21442;&#25968;&#24341;&#36215;&#30340;&#31934;&#24230;&#25439;&#22833;&#12290;OSP&#30340;&#21407;&#22411;&#20351;&#29992;PyTo&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed deep learning (DDL) is a promising research area, which aims to increase the efficiency of training deep learning tasks with large size of datasets and models. As the computation capability of DDL nodes continues to increase, the network connection between nodes is becoming a major bottleneck. Various methods of gradient compression and improved model synchronization have been proposed to address this bottleneck in Parameter-Server-based DDL. However, these two types of methods can result in accuracy loss due to discarded gradients and have limited enhancement on the throughput of model synchronization, respectively. To address these challenges, we propose a new model synchronization method named Overlapped Synchronization Parallel (OSP), which achieves efficient communication with a 2-stage synchronization approach and uses Local-Gradient-based Parameter correction (LGP) to avoid accuracy loss caused by stale parameters. The prototype of OSP has been implemented using PyTo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#20540;&#25968;&#25454;&#22635;&#34917;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26368;&#36817;&#37051;&#20272;&#35745;&#21644;&#39640;&#26031;&#26680;&#23494;&#24230;&#20272;&#35745;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#24182;&#25552;&#20379;&#27604;&#24403;&#21069;&#26041;&#27861;&#26356;&#39640;&#30340;&#27010;&#29575;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.16906</link><description>&lt;p&gt;
&#25968;&#20540;&#25968;&#25454;&#22635;&#34917;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;:&#19968;&#31181;&#27010;&#29575;&#26368;&#36817;&#37051;&#26680;&#23494;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor Kernel Density Approach. (arXiv:2306.16906v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#20540;&#25968;&#25454;&#22635;&#34917;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26368;&#36817;&#37051;&#20272;&#35745;&#21644;&#39640;&#26031;&#26680;&#23494;&#24230;&#20272;&#35745;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#24182;&#25552;&#20379;&#27604;&#24403;&#21069;&#26041;&#27861;&#26356;&#39640;&#30340;&#27010;&#29575;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#25968;&#25454;&#22635;&#34917;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;&#26367;&#25442;&#32570;&#22833;&#30340;&#20540;&#20197;&#21033;&#29992;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#38598;&#12290;&#24403;&#21069;&#30340;&#22635;&#34917;&#26041;&#27861;&#35797;&#22270;&#26368;&#23567;&#21270;&#26410;&#35266;&#23519;&#21040;&#30340;&#30495;&#23454;&#20540;&#21644;&#22635;&#34917;&#20540;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;&#20294;&#26159;&#65292;&#22312;&#22810;&#27169;&#24577;&#25110;&#22797;&#26434;&#20998;&#24067;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#31574;&#30053;&#21487;&#33021;&#20250;&#20135;&#29983;&#20266;&#20687;&#65292;&#23548;&#33268;&#22635;&#34917;&#25928;&#26524;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;$k$NN$\times$KDE&#31639;&#27861;: &#19968;&#31181;&#23558;&#26368;&#36817;&#37051;&#20272;&#35745;($k$NN)&#21644;&#20351;&#29992;&#39640;&#26031;&#26680;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;(KDE)&#32467;&#21512;&#30340;&#25968;&#25454;&#22635;&#34917;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#24037;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20102;&#19982;&#20043;&#21069;&#25968;&#25454;&#22635;&#34917;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#28041;&#21450;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#32570;&#22833;&#24773;&#20917;&#21644;&#19981;&#21516;&#30340;&#25968;&#25454;&#32570;&#22833;&#29575;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#21407;&#22987;&#25968;&#25454;&#32467;&#26500;&#65292;&#20135;&#29983;&#26356;&#20302;&#30340;&#25968;&#25454;&#22635;&#34917;&#35823;&#24046;&#65292;&#24182;&#25552;&#20379;&#27604;&#24403;&#21069;&#26041;&#27861;&#26356;&#39640;&#30340;&#27010;&#29575;&#20272;&#35745;&#12290;&#25105;&#20204;&#23558;&#20195;&#30721;&#20197;&#24320;&#28304;&#24418;&#24335;&#21457;&#24067;&#32473;&#31038;&#21306;&#65306;https://github.com/DeltaFloflo/knnxkde
&lt;/p&gt;
&lt;p&gt;
Numerical data imputation algorithms replace missing values by estimates to leverage incomplete data sets. Current imputation methods seek to minimize the error between the unobserved ground truth and the imputed values. But this strategy can create artifacts leading to poor imputation in the presence of multimodal or complex distributions. To tackle this problem, we introduce the $k$NN$\times$KDE algorithm: a data imputation method combining nearest neighbor estimation ($k$NN) and density estimation with Gaussian kernels (KDE). We compare our method with previous data imputation methods using artificial and real-world data with different data missing scenarios and various data missing rates, and show that our method can cope with complex original data structure, yields lower data imputation errors, and provides probabilistic estimates with higher likelihood than current methods. We release the code in open-source for the community: https://github.com/DeltaFloflo/knnxkde
&lt;/p&gt;</description></item><item><title>DragDiffusion&#26159;&#19968;&#20010;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#28857;&#22522;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#28508;&#22312;&#23454;&#29616;&#31934;&#30830;&#30340;&#31354;&#38388;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14435</link><description>&lt;p&gt;
DragDiffusion: &#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#28857;&#22522;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing. (arXiv:2306.14435v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14435
&lt;/p&gt;
&lt;p&gt;
DragDiffusion&#26159;&#19968;&#20010;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#28857;&#22522;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#28508;&#22312;&#23454;&#29616;&#31934;&#30830;&#30340;&#31354;&#38388;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#21487;&#25511;&#30340;&#22270;&#20687;&#32534;&#36753;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;DragGAN&#23454;&#29616;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#26694;&#26550;&#65292;&#24182;&#20197;&#20687;&#32032;&#32423;&#31934;&#24230;&#23454;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32534;&#36753;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35813;&#26041;&#27861;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#20854;&#36890;&#29992;&#24615;&#21463;&#38480;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;GAN&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#32534;&#36753;&#26694;&#26550;&#25193;&#23637;&#21040;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;DragDiffusion&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#20132;&#20114;&#24335;&#22522;&#20110;&#28857;&#30340;&#32534;&#36753;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#22522;&#20110;&#25991;&#26412;&#23884;&#20837;&#65292;DragDiffusion&#20248;&#21270;&#25193;&#25955;&#28508;&#22312;&#26469;&#23454;&#29616;&#31934;&#30830;&#30340;&#31354;&#38388;&#25511;&#21046;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#20197;&#36845;&#20195;&#26041;&#24335;&#29983;&#25104;&#22270;&#20687;&#65292;&#20294;&#25105;&#20204;&#20973;&#32463;&#39564;&#34920;&#26126;&#65292;&#22312;&#19968;&#20010;&#21333;&#29420;&#30340;&#27493;&#39588;&#20013;&#20248;&#21270;&#25193;&#25955;&#28508;&#22312;&#24050;&#36275;&#20197;&#29983;&#25104;&#36830;&#36143;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#20351;&#24471;&#35813;&#26041;&#27861;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise and controllable image editing is a challenging task that has attracted significant attention. Recently, DragGAN enables an interactive point-based image editing framework and achieves impressive editing results with pixel-level precision. However, since this method is based on generative adversarial networks (GAN), its generality is upper-bounded by the capacity of the pre-trained GAN models. In this work, we extend such an editing framework to diffusion models and propose DragDiffusion. By leveraging large-scale pretrained diffusion models, we greatly improve the applicability of interactive point-based editing in real world scenarios. While most existing diffusion-based image editing methods work on text embeddings, DragDiffusion optimizes the diffusion latent to achieve precise spatial control. Although diffusion models generate images in an iterative manner, we empirically show that optimizing diffusion latent at one single step suffices to generate coherent results, enabl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA) &#26426;&#21046;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#21160;&#24577;&#22320;&#31232;&#30095;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#65292;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.11197</link><description>&lt;p&gt;
&#31232;&#30095;&#27169;&#22359;&#28608;&#27963;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Sparse Modular Activation for Efficient Sequence Modeling. (arXiv:2306.11197v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA) &#26426;&#21046;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#21160;&#24577;&#22320;&#31232;&#30095;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#65292;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411; (SSM) &#22312;&#21508;&#31181;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#24378;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#24490;&#29615;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#26356;&#32508;&#21512;&#30340;&#20219;&#21153;&#20013;&#65292;&#22914;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#20173;&#28982;&#20248;&#20110;SSM&#12290;&#21516;&#26102;&#20351;&#29992;SSM&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#28151;&#21512;&#27169;&#22411;&#36890;&#24120;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#23558;&#27880;&#24847;&#21147;&#27169;&#22359;&#38745;&#24577;&#19988;&#22343;&#21248;&#22320;&#24212;&#29992;&#20110;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#25152;&#26377;&#20803;&#32032;&#65292;&#23548;&#33268;&#20102;&#36136;&#37327;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#27425;&#20248;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA)&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#21046;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20197;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#31232;&#30095;&#22320;&#21160;&#24577;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#12290;&#36890;&#36807;&#20801;&#35768;&#27599;&#20010;&#20803;&#32032;&#36339;&#36807;&#38750;&#28608;&#27963;&#30340;&#23376;&#27169;&#22359;&#65292;SMA&#21487;&#20197;&#22312;&#24207;&#21015;&#24314;&#27169;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#38477;&#20302;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;&#20316;&#20026;SMA&#30340;&#19968;&#20010;&#29305;&#23450;&#23454;&#20363;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#30417;&#30563;&#23398;&#20064;&#30340;&#21160;&#30011;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25110;&#35821;&#38899;&#36755;&#20837;&#65292;&#23454;&#29616;&#22522;&#20110;&#30495;&#23454;&#21160;&#20316;&#34920;&#28436;&#30340;&#38754;&#37096;&#21160;&#30011;&#65292;&#24182;&#19988;&#21487;&#20197;&#19981;&#21516;&#31243;&#24230;&#22320;&#23398;&#20064;&#24182;&#21512;&#25104;&#19981;&#21516;&#30340;&#34920;&#28436;&#39118;&#26684;&#12290;</title><link>http://arxiv.org/abs/2306.10006</link><description>&lt;p&gt;
&#22522;&#20110;&#29616;&#23454;&#34920;&#28436;&#30340;&#38754;&#37096;&#21160;&#30011;&#39118;&#26684;&#24863;&#30693;&#38750;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances. (arXiv:2306.10006v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#30417;&#30563;&#23398;&#20064;&#30340;&#21160;&#30011;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25110;&#35821;&#38899;&#36755;&#20837;&#65292;&#23454;&#29616;&#22522;&#20110;&#30495;&#23454;&#21160;&#20316;&#34920;&#28436;&#30340;&#38754;&#37096;&#21160;&#30011;&#65292;&#24182;&#19988;&#21487;&#20197;&#19981;&#21516;&#31243;&#24230;&#22320;&#23398;&#20064;&#24182;&#21512;&#25104;&#19981;&#21516;&#30340;&#34920;&#28436;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#28151;&#21512;&#24418;&#29366;&#20960;&#20309;&#12289;&#21160;&#24577;&#32441;&#29702;&#21644;&#31070;&#32463;&#28210;&#26579;&#65292;&#29992;&#20110;&#20174;&#30495;&#23454;&#21160;&#20316;&#34920;&#28436;&#20013;&#39537;&#21160;&#38754;&#37096;&#27169;&#22411;&#30340;&#25991;&#26412;/&#35821;&#38899;&#21160;&#30011;&#12290;&#36890;&#36807;&#35757;&#32451;&#21253;&#25324;&#24418;&#29366;&#21644;&#32441;&#29702;&#30340;VAE&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#27169;&#22411;&#65292;&#20197;&#31934;&#30830;&#25429;&#25417;&#21644;&#36924;&#30495;&#21512;&#25104;&#28508;&#22312;&#29305;&#24449;&#21521;&#37327;&#20013;&#30340;&#38754;&#37096;&#34920;&#24773;&#12290;&#25105;&#20204;&#30340;&#21160;&#30011;&#26041;&#27861;&#22522;&#20110;&#26465;&#20214;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#25991;&#26412;&#25110;&#35821;&#38899;&#36716;&#25442;&#20026;&#19968;&#31995;&#21015;&#21160;&#30011;&#21442;&#25968;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#21160;&#30011;&#27169;&#22411;&#20197;&#38750;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#21306;&#20998;&#21644;&#21512;&#25104;&#19981;&#21516;&#30340;&#34920;&#28436;&#39118;&#26684;&#65292;&#21482;&#38656;&#35201;&#29992;&#20110;&#25551;&#36848;&#35757;&#32451;&#24207;&#21015;&#20869;&#23481;&#30340;&#35821;&#38899;&#26631;&#31614;&#12290;&#20026;&#20102;&#23454;&#29616;&#36924;&#30495;&#30340;&#23454;&#26102;&#28210;&#26579;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;U-Net&#65292;&#36890;&#36807;&#35745;&#31639;&#25913;&#36827;&#30340;&#20687;&#32032;&#39068;&#33394;&#21644;&#21069;&#26223;&#36974;&#32617;&#26469;&#25913;&#21892;&#26629;&#26684;&#21270;&#28210;&#26579;&#12290;&#25105;&#20204;&#23450;&#24615;/&#23450;&#37327;&#22320;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#26368;&#36817;&#30340;&#22836;&#37096;&#24314;&#27169;&#26041;&#27861;&#20197;&#21450;&#38754;&#37096;&#21160;&#30011;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35780;&#20272;&#24863;&#30693;&#28210;&#26579;/&#21160;&#30011;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach for text/speech-driven animation of a photo-realistic head model based on blend-shape geometry, dynamic textures, and neural rendering. Training a VAE for geometry and texture yields a parametric model for accurate capturing and realistic synthesis of facial expressions from a latent feature vector. Our animation method is based on a conditional CNN that transforms text or speech into a sequence of animation parameters. In contrast to previous approaches, our animation model learns disentangling/synthesizing different acting-styles in an unsupervised manner, requiring only phonetic labels that describe the content of training sequences. For realistic real-time rendering, we train a U-Net that refines rasterization-based renderings by computing improved pixel colors and a foreground matte. We compare our framework qualitatively/quantitatively against recent methods for head modeling as well as facial animation and evaluate the perceived rendering/ani
&lt;/p&gt;</description></item><item><title>Diff-TTSG&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#65292;&#30456;&#27604;&#20110;&#20808;&#21069;&#26368;&#26032;&#25216;&#26415;&#30340;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.09417</link><description>&lt;p&gt;
Diff-TTSG: &#21435;&#22122;&#27010;&#29575;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis. (arXiv:2306.09417v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09417
&lt;/p&gt;
&lt;p&gt;
Diff-TTSG&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#65292;&#30456;&#27604;&#20110;&#20808;&#21069;&#26368;&#26032;&#25216;&#26415;&#30340;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26391;&#35835;&#35821;&#38899;&#21512;&#25104;&#23454;&#29616;&#39640;&#33258;&#28982;&#24230;&#35780;&#20998;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#21512;&#25104;&#33258;&#28982;&#35328;&#35821;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#38754;&#23545;&#38754;&#30340;&#33258;&#21457;&#23545;&#35805;&#26082;&#26377;&#21475;&#22836;&#30340;&#65292;&#20063;&#26377;&#38750;&#35821;&#35328;&#30340;&#65288;&#20363;&#22914;&#65292;&#20849;&#21516;&#35328;&#35821;&#25163;&#21183;&#65289;&#12290;&#26368;&#36817;&#25165;&#24320;&#22987;&#30740;&#31350;&#32852;&#21512;&#21512;&#25104;&#36825;&#20004;&#31181;&#27169;&#24577;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#31995;&#32479;&#20013;&#30340;&#22909;&#22788;&#12290;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#20351;&#29992;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#26080;&#27861;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#24182;&#21487;&#33021;&#20135;&#29983;&#36807;&#24230;&#24179;&#28369;&#30340;&#20266;&#24433;&#21644;&#27425;&#20248;&#30340;&#21512;&#25104;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#31216;&#20026; Diff-TTSG&#65292;&#20849;&#21516;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#32452;&#23567;&#24515;&#30340;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#20027;&#35266;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#31995;&#32479;&#65292;&#24182;&#29992;&#23427;&#20204;&#26469;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#21512;&#25104;&#30340;&#26679;&#20363;&#32780;&#35328;&#65292;Diff-TTSG&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
With read-aloud speech synthesis achieving high naturalness scores, there is a growing research interest in synthesising spontaneous speech. However, human spontaneous face-to-face conversation has both spoken and non-verbal aspects (here, co-speech gestures). Only recently has research begun to explore the benefits of jointly synthesising these two modalities in a single system. The previous state of the art used non-probabilistic methods, which fail to capture the variability of human speech and motion, and risk producing oversmoothing artefacts and sub-optimal synthesis quality. We present the first diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to synthesise speech and gestures together. Our method can be trained on small datasets from scratch. Furthermore, we describe a set of careful uni- and multi-modal subjective tests for evaluating integrated speech and gesture synthesis systems, and use them to validate our proposed approach. For synthesised examp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#20195;&#29702;&#24066;&#22330;&#35746;&#21333;&#30340;&#34920;&#31034;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;K&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#23545;&#20195;&#29702;&#35746;&#21333;&#30340;&#23398;&#20064;&#34920;&#31034;&#21521;&#37327;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#31751;&#20013;&#30340;&#19981;&#21516;&#34892;&#20026;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05987</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#20195;&#29702;&#24066;&#22330;&#35746;&#21333;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Agent market orders representation through a contrastive learning approach. (arXiv:2306.05987v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05987
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#20195;&#29702;&#24066;&#22330;&#35746;&#21333;&#30340;&#34920;&#31034;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;K&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#23545;&#20195;&#29702;&#35746;&#21333;&#30340;&#23398;&#20064;&#34920;&#31034;&#21521;&#37327;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#31751;&#20013;&#30340;&#19981;&#21516;&#34892;&#20026;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35775;&#38382;Euronext&#30340;CAC40&#25968;&#25454;&#20013;&#30340;&#26631;&#35760;&#35746;&#21333;&#65292;&#20998;&#26512;&#20195;&#29702;&#22312;&#24066;&#22330;&#20013;&#26681;&#25454;&#20854;&#19979;&#36798;&#30340;&#35746;&#21333;&#30340;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#26469;&#26377;&#25928;&#22320;&#23398;&#20064;&#20195;&#29702;&#24066;&#22330;&#35746;&#21333;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#33719;&#21462;&#36825;&#20010;&#23398;&#20064;&#34920;&#31034;&#65292;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#21464;&#24471;&#21487;&#34892;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;K&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#23545;&#20195;&#29702;&#35746;&#21333;&#30340;&#23398;&#20064;&#34920;&#31034;&#21521;&#37327;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#31751;&#20013;&#30340;&#19981;&#21516;&#34892;&#20026;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the access to the labeled orders on the CAC40 data from Euronext, we are able to analyse agents' behaviours in the market based on their placed orders. In this study, we construct a self-supervised learning model using triplet loss to effectively learn the representation of agent market orders. By acquiring this learned representation, various downstream tasks become feasible. In this work, we utilise the K-means clustering algorithm on the learned representation vectors of agent orders to identify distinct behaviour types within each cluster.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31890;&#23376;&#27969;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#26080;&#30417;&#30563;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#26631;&#31614;&#32570;&#22833;&#12289;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#25968;&#25454;&#26102;&#38388;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#36719;&#27979;&#37327;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04919</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#36125;&#21494;&#26031;&#31890;&#23376;&#27969;&#26694;&#26550;&#30340;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#26080;&#30417;&#30563;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Cross-Domain Soft Sensor Modelling via A Deep Bayesian Particle Flow Framework. (arXiv:2306.04919v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31890;&#23376;&#27969;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#26080;&#30417;&#30563;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#26631;&#31614;&#32570;&#22833;&#12289;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#25968;&#25454;&#26102;&#38388;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#36719;&#27979;&#37327;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#36719;&#27979;&#37327;&#23545;&#20110;&#36890;&#36807;&#21487;&#38752;&#30340;&#29366;&#24577;&#25512;&#26029;&#23454;&#29616;&#31934;&#30830;&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#26631;&#31614;&#32570;&#22833;&#12289;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#25968;&#25454;&#26102;&#38388;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#24320;&#21457;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#36719;&#27979;&#37327;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31890;&#23376;&#27969;&#36125;&#21494;&#26031; (DPFB) &#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26080;&#30446;&#26631;&#29366;&#24577;&#26631;&#31614;&#24773;&#20917;&#19979;&#36827;&#34892;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#24314;&#27169;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#39034;&#24207;&#36125;&#21494;&#26031;&#30446;&#26631;&#65292;&#20197;&#25191;&#34892;&#28508;&#22312;&#30340;&#36328;&#39046;&#22495;&#36719;&#24863;&#30693;&#38382;&#39064;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;&#22312;&#26694;&#26550;&#26680;&#24515;&#65292;&#25105;&#20204;&#32467;&#21512;&#29289;&#29702;&#23398;&#21551;&#21457;&#30340;&#31890;&#23376;&#27969;&#65292;&#36890;&#36807;&#20248;&#21270;&#39034;&#24207;&#36125;&#21494;&#26031;&#30446;&#26631;&#26469;&#25191;&#34892;&#27169;&#22411;&#25552;&#21462;&#30340;&#28508;&#22312;&#21644;&#38544;&#34255;&#29305;&#24449;&#30340;&#31934;&#30830;&#36125;&#21494;&#26031;&#26356;&#26032;&#12290;&#30001;&#27492;&#65292;&#36825;&#20123;&#36129;&#29486;&#20351;&#24471;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;&#26377;&#26426;&#30340;&#36817;&#20284;&#21518;&#39564;&#29305;&#24449;&#34920;&#31034;&#65292;&#33021;&#22815;&#34920;&#24449;&#22797;&#26434;&#30340;&#36328;&#39046;&#22495;&#31995;&#32479;&#21160;&#21147;&#23398;&#24182;&#23454;&#29616;&#26377;&#25928;&#30340;&#36719;&#27979;&#37327;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven soft sensors are essential for achieving accurate perception through reliable state inference. However, developing representative soft sensor models is challenged by issues such as missing labels, domain adaptability, and temporal coherence in data. To address these challenges, we propose a deep Particle Flow Bayes (DPFB) framework for cross-domain soft sensor modeling in the absence of target state labels. In particular, a sequential Bayes objective is first formulated to perform the maximum likelihood estimation underlying the cross-domain soft sensing problem. At the core of the framework, we incorporate a physics-inspired particle flow that optimizes the sequential Bayes objective to perform an exact Bayes update of the model extracted latent and hidden features. As a result, these contributions enable the proposed framework to learn a cohesive approximate posterior feature representation capable of characterizing complex cross-domain system dynamics and performing effe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#39046;&#22495;&#20013;&#26368;&#36817;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20171;&#32461;&#20102;&#20854;&#29702;&#35770;&#22522;&#30784;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.04139</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Generative Diffusion Models for Structured Data. (arXiv:2306.04139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#39046;&#22495;&#20013;&#26368;&#36817;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20171;&#32461;&#20102;&#20854;&#29702;&#35770;&#22522;&#30784;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;generative diffusion models&#65289;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#25104;&#26524;&#65292;&#23637;&#29616;&#20102;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#21253;&#25324;&#34920;&#26684;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#30028;&#20013;&#21463;&#21040;&#30340;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#65292;&#23613;&#31649;&#20854;&#26080;&#22788;&#19981;&#22312;&#19988;&#24212;&#29992;&#24191;&#27867;&#12290;&#22240;&#27492;&#65292;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#20854;&#20182;&#25968;&#25454;&#24418;&#24335;&#30456;&#27604;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#24314;&#27169;&#30340;&#25991;&#29486;&#21450;&#20854;&#32508;&#36848;&#20173;&#28982;&#32570;&#20047;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#39046;&#22495;&#20013;&#26368;&#36817;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;&#39318;&#20808;&#65292;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#29702;&#35770;&#30340;&#31616;&#35201;&#27010;&#36848;&#65292;&#38543;&#21518;&#21448;&#35814;&#32454;&#25551;&#36848;&#20102;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#36890;&#29992;&#20219;&#21153;&#21644;&#29305;&#23450;&#39046;&#22495;&#24212;&#29992;&#20013;&#20351;&#29992;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#22823;&#37096;&#20998;&#24320;&#21019;&#24615;&#24037;&#20316;&#30340;&#25216;&#26415;&#25551;&#36848;&#12290;&#26368;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
In recent years, generative diffusion models have achieved a rapid paradigm shift in deep generative models by showing groundbreaking performance across various applications. Meanwhile, structured data, encompassing tabular and time series data, has been received comparatively limited attention from the deep learning research community, despite its omnipresence and extensive applications. Thus, there is still a lack of literature and its review on structured data modelling via diffusion models, compared to other data modalities such as computer vision and natural language processing. Hence, in this paper, we present a comprehensive review of recently proposed diffusion models in the field of structured data. First, this survey provides a concise overview of the score-based diffusion model theory, subsequently proceeding to the technical descriptions of the majority of pioneering works using structured data in both data-driven general tasks and domain-specific applications. Thereafter, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(LHVAE)&#65292;&#30740;&#31350;&#24773;&#24863;&#26465;&#20214;&#23545;&#26059;&#24459;&#21644;&#22768;&#32534;&#37197;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#21644;&#22768;&#36136;&#37327;&#24182;&#25429;&#25417;&#20102;&#20016;&#23500;&#30340;&#21644;&#24358;&#36827;&#34892;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24863;&#30693;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03718</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#24773;&#24863;&#26465;&#20214;&#26059;&#24459;&#21644;&#22768;&#32534;&#37197;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Emotion-Conditioned Melody Harmonization with Hierarchical Variational Autoencoder. (arXiv:2306.03718v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03718
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(LHVAE)&#65292;&#30740;&#31350;&#24773;&#24863;&#26465;&#20214;&#23545;&#26059;&#24459;&#21644;&#22768;&#32534;&#37197;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#21644;&#22768;&#36136;&#37327;&#24182;&#25429;&#25417;&#20102;&#20016;&#23500;&#30340;&#21644;&#24358;&#36827;&#34892;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24863;&#30693;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#26059;&#24459;&#21644;&#22768;&#32534;&#37197;&#27169;&#22411;&#22312;&#25552;&#39640;&#29983;&#25104;&#30340;&#21644;&#22768;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#24573;&#30053;&#20102;&#38899;&#20048;&#20013;&#30340;&#24773;&#24863;&#12290;&#21516;&#26102;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#25152;&#29983;&#25104;&#30340;&#21644;&#22768;&#21464;&#21270;&#24615;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LSTM&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(LHVAE)&#65292;&#30740;&#31350;&#24773;&#24863;&#26465;&#20214;&#23545;&#26059;&#24459;&#21644;&#22768;&#32534;&#37197;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#21644;&#22768;&#36136;&#37327;&#24182;&#25429;&#25417;&#20102;&#20016;&#23500;&#30340;&#21644;&#24358;&#36827;&#34892;&#21464;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;LHVAE&#22312;&#19981;&#21516;&#23618;&#27425;&#65288;&#20048;&#31456;&#21644;&#23567;&#33410;&#32423;&#21035;&#65289;&#19978;&#34701;&#21512;&#20102;&#28508;&#22312;&#21464;&#37327;&#21644;&#24773;&#24863;&#26465;&#20214;&#65292;&#20197;&#24314;&#27169;&#20840;&#23616;&#21644;&#23616;&#37096;&#38899;&#20048;&#23646;&#24615;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26059;&#24459;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#26059;&#24459;&#21644;&#21644;&#22768;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#23458;&#35266;&#35780;&#20272;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#32988;&#36807;&#20102;&#20854;&#20182;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#20027;&#35266;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#31526;&#21512;&#32473;&#23450;&#24773;&#24863;&#30340;&#21644;&#22768;&#36827;&#34892;&#65292;&#24182;&#22312;&#24863;&#30693;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing melody harmonization models have made great progress in improving the quality of generated harmonies, but most of them ignored the emotions beneath the music. Meanwhile, the variability of harmonies generated by previous methods is insufficient. To solve these problems, we propose a novel LSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the influence of emotional conditions on melody harmonization, while improving the quality of generated harmonies and capturing the abundant variability of chord progressions. Specifically, LHVAE incorporates latent variables and emotional conditions at different levels (piece- and bar-level) to model the global and local music properties. Additionally, we introduce an attention-based melody context vector at each step to better learn the correspondence between melodies and harmonies. Experimental results of the objective evaluation show that our proposed model outperforms other LSTM-based models. Through subjective evalu
&lt;/p&gt;</description></item><item><title>&#20998;&#25955;SGD&#21644;&#24179;&#22343;&#26041;&#21521;SAM&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#31561;&#20215;&#30340;&#65292;D-SGD&#34920;&#29616;&#20986;&#26799;&#24230;&#24179;&#28369;&#25928;&#24212;&#21644;&#38160;&#24230;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#39640;&#21518;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>http://arxiv.org/abs/2306.02913</link><description>&lt;p&gt;
&#20998;&#25955;&#21270;SGD&#21644;&#24179;&#22343;&#26041;&#21521;SAM&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#31561;&#20215;&#30340;
&lt;/p&gt;
&lt;p&gt;
Decentralized SGD and Average-direction SAM are Asymptotically Equivalent. (arXiv:2306.02913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02913
&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;SGD&#21644;&#24179;&#22343;&#26041;&#21521;SAM&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#31561;&#20215;&#30340;&#65292;D-SGD&#34920;&#29616;&#20986;&#26799;&#24230;&#24179;&#28369;&#25928;&#24212;&#21644;&#38160;&#24230;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#39640;&#21518;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;D-SGD&#65289;&#20801;&#35768;&#22312;&#27809;&#26377;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#25511;&#21046;&#19979;&#65292;&#22823;&#37327;&#35774;&#22791;&#21516;&#26102;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#29702;&#35770;&#35748;&#20026;&#65292;&#20998;&#25955;&#21270;&#19981;&#21487;&#36991;&#20813;&#22320;&#21066;&#24369;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25361;&#25112;&#20256;&#32479;&#20449;&#24565;&#65292;&#25552;&#20986;&#20102;&#23436;&#20840;&#26032;&#30340;&#35282;&#24230;&#26469;&#29702;&#35299;&#20998;&#25955;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#38750;&#20984;&#38750;-$\beta$-&#24179;&#28369;&#35774;&#32622;&#19979;&#65292;D-SGD&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#20102;&#24179;&#22343;&#26041;&#21521;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#31639;&#27861;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#36825;&#31181;&#24778;&#20154;&#30340;&#28176;&#36817;&#31561;&#20215;&#25581;&#31034;&#20102;&#20869;&#22312;&#30340;&#27491;&#21017;&#21270;-&#20248;&#21270;&#26435;&#34913;&#20197;&#21450;&#20998;&#25955;&#21270;&#30340;&#19977;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;D-SGD&#20013;&#23384;&#22312;&#19968;&#20010;&#33258;&#30001;&#30340;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#26426;&#21046;&#65292;&#21487;&#20197;&#25552;&#39640;&#21518;&#39564;&#20272;&#35745;&#65307;&#65288;2&#65289;D-SGD&#34920;&#29616;&#20986;&#26799;&#24230;&#24179;&#28369;&#25928;&#24212;&#65307;&#65288;3&#65289;D-SGD&#30340;&#38160;&#24230;&#27491;&#21017;&#21270;&#25928;&#24212;&#19981;&#20250;&#38543;&#30528;&#24635;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#36825;&#35777;&#26126;&#20102;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive devices simultaneously without the control of a central server. However, existing theories claim that decentralization invariably undermines generalization. In this paper, we challenge the conventional belief and present a completely new perspective for understanding decentralized learning. We prove that D-SGD implicitly minimizes the loss function of an average-direction Sharpness-aware minimization (SAM) algorithm under general non-convex non-$\beta$-smooth settings. This surprising asymptotic equivalence reveals an intrinsic regularization-optimization trade-off and three advantages of decentralization: (1) there exists a free uncertainty evaluation mechanism in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient smoothing effect; and (3) the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the potential generalization b
&lt;/p&gt;</description></item><item><title>&#39640;&#39118;&#38505;&#39046;&#22495;&#23545;&#20110;&#21069;&#25991;&#35299;&#37322;&#24615;&#30340;&#36861;&#27714;&#26159;&#37325;&#35201;&#30340;&#65292;&#20294;&#35813;&#27010;&#24565;&#33258;&#36523;&#21547;&#20041;&#27169;&#31946;&#19981;&#28165;&#65292;&#21462;&#20915;&#20110;&#25805;&#20316;&#29615;&#22659;&#21644;&#35266;&#23519;&#32773;&#21028;&#26029;&#12290;&#36879;&#26126;&#30340;&#39044;&#27979;&#27169;&#22411;&#21487;&#33021;&#20173;&#38656;&#21518;&#32493;&#22788;&#29702;&#25165;&#33021;&#25552;&#20379;&#21512;&#36866;&#30340;&#35299;&#37322;&#24615;&#27934;&#35265;&#65292;&#22240;&#27492;&#20256;&#32479;&#30340;&#21069;&#25991;&#35299;&#37322;&#24615;&#27010;&#24565;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#26126;&#30830;&#21644;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2306.02312</link><description>&lt;p&gt;
&#39640;&#39118;&#38505;&#39046;&#22495;&#23545;&#20110;&#21069;&#25991;&#35299;&#37322;&#24615;&#21560;&#24341;&#21147;&#30340;&#65288;&#19981;&#65289;&#21512;&#29702;&#21560;&#24341;&#21147;&#65306;&#36879;&#26126;&#24230;&#23545;&#20110;&#21487;&#29702;&#35299;&#24615;&#26469;&#35828;&#26159;&#24517;&#35201;&#20294;&#19981;&#36275;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
(Un)reasonable Allure of Ante-hoc Interpretability for High-stakes Domains: Transparency Is Necessary but Insufficient for Comprehensibility. (arXiv:2306.02312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02312
&lt;/p&gt;
&lt;p&gt;
&#39640;&#39118;&#38505;&#39046;&#22495;&#23545;&#20110;&#21069;&#25991;&#35299;&#37322;&#24615;&#30340;&#36861;&#27714;&#26159;&#37325;&#35201;&#30340;&#65292;&#20294;&#35813;&#27010;&#24565;&#33258;&#36523;&#21547;&#20041;&#27169;&#31946;&#19981;&#28165;&#65292;&#21462;&#20915;&#20110;&#25805;&#20316;&#29615;&#22659;&#21644;&#35266;&#23519;&#32773;&#21028;&#26029;&#12290;&#36879;&#26126;&#30340;&#39044;&#27979;&#27169;&#22411;&#21487;&#33021;&#20173;&#38656;&#21518;&#32493;&#22788;&#29702;&#25165;&#33021;&#25552;&#20379;&#21512;&#36866;&#30340;&#35299;&#37322;&#24615;&#27934;&#35265;&#65292;&#22240;&#27492;&#20256;&#32479;&#30340;&#21069;&#25991;&#35299;&#37322;&#24615;&#27010;&#24565;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#26126;&#30830;&#21644;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#25991;&#35299;&#37322;&#24615;&#24050;&#25104;&#20026;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#65289;&#20013;&#30340;&#36861;&#27714;&#65292;&#28982;&#32780;&#65292;&#36825;&#20010;&#27010;&#24565;&#24456;&#38590;&#25417;&#25720;&#65292;&#32570;&#20047;&#24191;&#27867;&#25509;&#21463;&#30340;&#23450;&#20041;&#65292;&#24182;&#19988;&#21462;&#20915;&#20110;&#25805;&#20316;&#29615;&#22659;&#12290;&#23427;&#21487;&#20197;&#25351;&#30340;&#26159;&#37027;&#20123;&#32467;&#26500;&#31526;&#21512;&#29305;&#23450;&#39046;&#22495;&#32422;&#26463;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#25110;&#32773;&#26159;&#26412;&#36136;&#19978;&#36879;&#26126;&#30340;&#27169;&#22411;&#12290;&#21518;&#19968;&#31181;&#35266;&#24565;&#20551;&#35774;&#35266;&#23519;&#32773;&#23545;&#27492;&#36136;&#37327;&#36827;&#34892;&#21028;&#26029;&#65292;&#32780;&#21069;&#19968;&#31181;&#35266;&#24565;&#21017;&#20551;&#35774;&#20182;&#20204;&#20855;&#26377;&#25216;&#26415;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65288;&#20174;&#32780;&#20351;&#20854;&#20182;&#35299;&#37322;&#23545;&#35937;&#32676;&#20307;&#24863;&#21040;&#38476;&#29983;&#65289;&#12290;&#27492;&#22806;&#65292;&#21069;&#25991;&#35299;&#37322;&#24615;&#19982;&#36739;&#19981;&#29702;&#24819;&#30340;&#21518;&#25991;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#21306;&#21035;&#27169;&#31946;&#65292;&#21518;&#32773;&#26159;&#25351;&#26500;&#24314;&#19968;&#20010;&#21333;&#29420;&#30340;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#32780;&#36879;&#26126;&#30340;&#39044;&#27979;&#27169;&#22411;&#20173;&#21487;&#33021;&#38656;&#35201;&#65288;&#21518;&#32493;&#65289;&#22788;&#29702;&#25165;&#33021;&#20135;&#29983;&#36866;&#24403;&#30340;&#35299;&#37322;&#24615;&#27934;&#35265;&#12290;&#22240;&#27492;&#65292;&#21069;&#25991;&#35299;&#37322;&#24615;&#26159;&#19968;&#20010;&#36229;&#36127;&#33655;&#30340;&#27010;&#24565;&#65292;&#23427;&#21253;&#21547;&#19968;&#31995;&#21015;&#38544;&#21547;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#23558;&#22312;&#26412;&#25991;&#20013;&#35814;&#32454;&#38416;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ante-hoc interpretability has become the holy grail of explainable artificial intelligence for high-stakes domains such as healthcare; however, this notion is elusive, lacks a widely-accepted definition and depends on the operational context. It can refer to predictive models whose structure adheres to domain-specific constraints, or ones that are inherently transparent. The latter conceptualisation assumes observers who judge this quality, whereas the former presupposes them to have technical and domain expertise (thus alienating other groups of explainees). Additionally, the distinction between ante-hoc interpretability and the less desirable post-hoc explainability, which refers to methods that construct a separate explanatory model, is vague given that transparent predictive models may still require (post-)processing to yield suitable explanatory insights. Ante-hoc interpretability is thus an overloaded concept that comprises a range of implicit properties, which we unpack in this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#26377;&#25928;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01505</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#20351;&#29992;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations. (arXiv:2306.01505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#26377;&#25928;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35782;&#21035;&#22312;&#23545;&#35805;&#20013;&#26159;&#25552;&#21462;&#27867;&#21270;&#21644;&#31283;&#20581;&#34920;&#31034;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#23545;&#27604;&#24863;&#30693;&#23545;&#25239;&#24615;&#35757;&#32451;&#20197;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#26679;&#26412;&#65292;&#24182;&#22312;&#21407;&#22987;&#21644;&#23545;&#25239;&#26679;&#26412;&#19978;&#20351;&#29992;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#12290;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#12290;&#20026;&#20102;&#36991;&#20813;&#23545;&#19978;&#19979;&#25991;&#30456;&#20851;&#25968;&#25454;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#23545;&#25239;&#24615;&#35757;&#32451;&#31574;&#30053;&#65292;&#20174;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#26356;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#23545;&#19978;&#19979;&#25991;&#30340;&#23481;&#38169;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;SACL-LSTM&#65292;&#29992;&#20110;&#23398;&#20064;&#38024;&#23545;ERC&#30340;&#26631;&#31614;&#19968;&#33268;&#21644;&#19978;&#19979;&#25991;&#31283;&#20581;&#30340;&#24773;&#24863;&#29305;&#24449;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SACL-LSTM&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations. The framework applies contrast-aware adversarial training to generate worst-case samples and uses a joint class-spread contrastive learning objective on both original and adversarial samples. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training strategy to learn more diverse features from context and enhance the model's context robustness. We develop a sequence-based method SACL-LSTM under this framework, to learn label-consistent and context-robust emotional features for ERC. Experiments on three datasets demonstrate that SACL-LSTM achieves state-of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.18451</link><description>&lt;p&gt;
&#20855;&#26377;&#22240;&#26524;&#20122;&#32467;&#26500;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Shift-Robust Molecular Relational Learning with Causal Substructure. (arXiv:2305.18451v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#24341;&#36215;&#20102;&#20998;&#23376;&#31185;&#23398;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20854;&#30446;&#26631;&#26159;&#39044;&#27979;&#20998;&#23376;&#23545;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#23427;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20551;&#23450;&#22522;&#20110;&#20998;&#23376;&#31185;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#26500;&#24314;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#26469;&#25581;&#31034;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22522;&#20110;SCM&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26465;&#20214;&#24178;&#39044;&#26694;&#26550;&#65292;&#20854;&#24178;&#39044;&#26159;&#22522;&#20110;&#25104;&#23545;&#20998;&#23376;&#26465;&#20214;&#30340;&#12290;&#20351;&#29992;&#26465;&#20214;&#24178;&#39044;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#20174;&#22240;&#26524;&#20122;&#32467;&#26500;&#20013;&#23398;&#20064;&#65292;&#24182;&#20943;&#36731;&#20102;&#19982;&#21270;&#23398;&#21453;&#24212;&#34394;&#20551;&#30456;&#20851;&#30340;&#24555;&#25463;&#20122;&#32467;&#26500;&#30340;&#28151;&#28102;&#25928;&#24212;&#12290;&#26412;&#25991;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, molecular relational learning, whose goal is to predict the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. In this work, we propose CMRL that is robust to the distributional shift in molecular relational learning by detecting the core substructure that is causally related to chemical reactions. To do so, we first assume a causal relationship based on the domain knowledge of molecular sciences and construct a structural causal model (SCM) that reveals the relationship between variables. Based on the SCM, we introduce a novel conditional intervention framework whose intervention is conditioned on the paired molecule. With the conditional intervention framework, our model successfully learns from the causal substructure and alleviates the confounding effect of shortcut substructures that are spuriously correlated to chemical reactions. Extensive experiments on various tasks with real-world and sy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.18404</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#26696;&#30830;&#35748;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24320;&#21457;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#20581;&#22766;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#23558;&#25104;&#20026;&#23427;&#20204;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#19979;&#23433;&#20840;&#37096;&#32626;&#30340;&#20851;&#38190;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#36825;&#31181;&#35266;&#23519;&#23545;&#20110;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#36873;&#25321;&#24615;&#20998;&#31867;&#21644;&#36807;&#28388;&#20302;&#36136;&#37327;&#39044;&#27979;&#65292;&#21487;&#33021;&#20250;&#26377;&#29992;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#23545;&#20110;&#36229;&#20986;&#20027;&#39064;&#30340;&#38382;&#39064;&#30340;&#20132;&#25442;&#24615;&#20551;&#35774;&#65292;&#36825;&#21487;&#33021;&#26159;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#26356;&#20026;&#29616;&#23454;&#30340;&#22330;&#26223;&#12290;&#26412;&#30740;&#31350;&#20026;&#22312;&#38656;&#35201;&#21487;&#38752;&#20445;&#35777;&#38169;&#35823;&#29575;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#20917;&#19979;&#26356;&#21152;&#20540;&#24471;&#20449;&#36182;&#21644;&#21487;&#38752;&#22320;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#20799;&#31461;&#33041;&#32959;&#30244;&#30340;BraTS&#25361;&#25112;&#65292;&#26088;&#22312;&#35780;&#20272;&#20799;&#31461;&#33041;&#33014;&#36136;&#30244;&#30340;&#20307;&#31215;&#20998;&#21106;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#20799;&#31461;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#32959;&#30244;&#26159;&#20799;&#31461;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#24182;&#19988;&#23545;&#36825;&#20123;&#23454;&#20307;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.17033</link><description>&lt;p&gt;
&#12298;&#22823;&#33041;&#32959;&#30244;&#20998;&#21106;&#65288;BraTS&#65289;&#25361;&#25112;2023&#65306;&#20851;&#27880;&#20799;&#31185;&#65288;CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs&#65289;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs). (arXiv:2305.17033v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17033
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#20799;&#31461;&#33041;&#32959;&#30244;&#30340;BraTS&#25361;&#25112;&#65292;&#26088;&#22312;&#35780;&#20272;&#20799;&#31461;&#33041;&#33014;&#36136;&#30244;&#30340;&#20307;&#31215;&#20998;&#21106;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#20799;&#31461;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#32959;&#30244;&#26159;&#20799;&#31461;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#24182;&#19988;&#23545;&#36825;&#20123;&#23454;&#20307;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#32959;&#30244;&#26159;&#20799;&#31461;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#26368;&#24120;&#35265;&#21407;&#22240;&#12290;&#20799;&#31461;&#39640;&#32423;&#21035;&#33014;&#36136;&#30244;&#30340;&#20116;&#24180;&#29983;&#23384;&#29575;&#19981;&#21040;20&#65285;&#12290;&#30001;&#20110;&#32597;&#35265;&#65292;&#23545;&#36825;&#20123;&#23454;&#20307;&#30340;&#35786;&#26029;&#36890;&#24120;&#20250;&#24310;&#36831;&#65292;&#20854;&#27835;&#30103;&#20027;&#35201;&#22522;&#20110;&#21382;&#21490;&#27835;&#30103;&#29702;&#24565;&#65292;&#24182;&#19988;&#20020;&#24202;&#35797;&#39564;&#38656;&#35201;&#22810;&#26426;&#26500;&#21512;&#20316;&#12290;MICCAI&#22823;&#33041;&#32959;&#30244;&#20998;&#21106;&#65288;BraTS&#65289;&#25361;&#25112;&#26159;&#19968;&#20010;&#37324;&#31243;&#30865;&#24335;&#30340;&#31038;&#21306;&#22522;&#20934;&#20107;&#20214;&#65292;&#24050;&#32463;&#25104;&#21151;&#21019;&#24314;&#36164;&#28304;12&#24180;&#65292;&#29992;&#20110;&#25104;&#20154;&#33014;&#36136;&#30244;&#30340;&#20998;&#21106;&#21644;&#20998;&#26512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023&#25361;&#25112;&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#20799;&#31461;&#33041;&#32959;&#30244;&#30340;BraTS&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;&#22810;&#20010;&#22269;&#38469;&#21512;&#20316;&#32452;&#32455;&#19987;&#27880;&#20110;&#20799;&#31185;&#31070;&#32463;&#32959;&#30244;&#21644;&#20020;&#24202;&#35797;&#39564;&#30340;&#25968;&#25454;&#12290;BraTS-PEDs 2023&#25361;&#25112;&#20391;&#37325;&#20110;&#35780;&#20272;&#29992;&#20110;&#20799;&#31461;&#33041;&#33014;&#36136;&#30244;&#30340;&#20307;&#31215;&#20998;&#21106;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pediatric tumors of the central nervous system are the most common cause of cancer-related death in children. The five-year survival rate for high-grade gliomas in children is less than 20\%. Due to their rarity, the diagnosis of these entities is often delayed, their treatment is mainly based on historic treatment concepts, and clinical trials require multi-institutional collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a landmark community benchmark event with a successful history of 12 years of resource creation for the segmentation and analysis of adult glioma. Here we present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which represents the first BraTS challenge focused on pediatric brain tumors with data acquired across multiple international consortia dedicated to pediatric neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on benchmarking the development of volumentric segmentation algorithms for pediatric brain gli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.16044</link><description>&lt;p&gt;
&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#23558;&#22122;&#22768;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#26159;&#22823;&#33041;&#38750;&#20961;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#30340;&#22522;&#30784;&#65292;&#24182;&#24050;&#25104;&#20026;&#31070;&#32463;&#24418;&#24577;&#26234;&#33021;&#30340;&#25903;&#26609;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#37319;&#29992;&#24102;&#26377;&#22122;&#22768;&#31070;&#32463;&#20803;&#21160;&#21147;&#23398;&#30340;&#33033;&#20914;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#26174;&#31034;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#29702;&#35770;&#19978;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;NDL&#20026;&#20195;&#29702;&#26799;&#24230;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#12290;&#36890;&#36807;&#23558;&#21508;&#31181;SNN&#26550;&#26500;&#21644;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#30830;&#23450;&#24615;SNNs&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Networks of spiking neurons underpin the extraordinary information-processing capabilities of the brain and have emerged as pillar models in neuromorphic intelligence. Despite extensive research on spiking neural networks (SNNs), most are established on deterministic models. Integrating noise into SNNs leads to biophysically more realistic neural dynamics and may benefit model performance. This work presents the noisy spiking neural network (NSNN) and the noise-driven learning rule (NDL) by introducing a spiking neuron model incorporating noisy neuronal dynamics. Our approach shows how noise may act as a resource for computation and learning and theoretically provides a framework for general SNNs. Moreover, NDL provides an insightful biological rationale for surrogate gradients. By incorporating various SNN architectures and algorithms, we show that our approach exhibits competitive performance and improved robustness against challenging perturbations than deterministic SNNs. Additiona
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#22823;&#35268;&#27169;&#30340;&#21333;&#27169;&#24577;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30005;&#21830;&#24212;&#29992;&#20013;&#35270;&#35273;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#28040;&#34701;&#30740;&#31350;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.13399</link><description>&lt;p&gt;
&#39640;&#25928;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Large-Scale Vision Representation Learning. (arXiv:2305.13399v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#22823;&#35268;&#27169;&#30340;&#21333;&#27169;&#24577;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30005;&#21830;&#24212;&#29992;&#20013;&#35270;&#35273;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#28040;&#34701;&#30740;&#31350;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#21333;&#27169;&#24577;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20102;&#35299;&#20135;&#21697;&#20869;&#23481;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#30005;&#21830;&#25512;&#33616;&#12289;&#25628;&#32034;&#21644;&#24191;&#21578;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#21644;&#23545;&#27604;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#26377;&#25928;&#24494;&#35843;&#22823;&#35268;&#27169;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#22810;&#31181;&#39044;&#35757;&#32451;&#30340;&#39592;&#24178;&#26550;&#26500;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#31995;&#21015;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#30340;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#20102;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#25552;&#20379;&#35270;&#35273;&#34920;&#31034;&#30340;&#21162;&#21147;&#12290;&#25105;&#20204;&#20026;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#21253;&#25324;&#25105;&#20204;&#30340;&#35270;&#35273;&#30456;&#20284;&#24191;&#21578;&#25512;&#33616;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#24471;&#35270;&#35273;&#34920;&#31034;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31163;&#32447;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#31163;&#32447;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#35273;&#30456;&#20284;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we present our approach to single-modality vision representation learning. Understanding vision representations of product content is vital for recommendations, search, and advertising applications in e-commerce. We detail and contrast techniques used to fine tune large-scale vision representation learning models in an efficient manner under low-resource settings, including several pretrained backbone architectures, both in the convolutional neural network as well as the vision transformer family. We highlight the challenges for e-commerce applications at-scale and highlight the efforts to more efficiently train, evaluate, and serve visual representations. We present ablation studies for several downstream tasks, including our visually similar ad recommendations. We evaluate the offline performance of the derived visual representations in downstream tasks. To this end, we present a novel text-to-image generative offline evaluation method for visually similar recommenda
&lt;/p&gt;</description></item><item><title>&#21512;&#25104;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#26159;&#19981;&#23436;&#32654;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#33021;&#20250;&#23548;&#33268;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#38169;&#35823;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#31181;&#24773;&#20917;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#28145;&#24230;&#29983;&#25104;&#38598;&#25104;&#65288;DGE&#65289;&#26694;&#26550;&#26469;&#36817;&#20284;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20197;&#25552;&#39640;&#19979;&#28216;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.09235</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#65292;&#30495;&#23454;&#35823;&#24046;&#65306;&#22914;&#20309;&#65288;&#19981;&#65289;&#21457;&#24067;&#21644;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Synthetic data, real errors: how (not) to publish and use synthetic data. (arXiv:2305.09235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09235
&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#26159;&#19981;&#23436;&#32654;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#33021;&#20250;&#23548;&#33268;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#38169;&#35823;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#31181;&#24773;&#20917;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#28145;&#24230;&#29983;&#25104;&#38598;&#25104;&#65288;DGE&#65289;&#26694;&#26550;&#26469;&#36817;&#20284;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20197;&#25552;&#39640;&#19979;&#28216;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#21644;&#20854;&#20182;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#36825;&#31181;&#26041;&#27861;&#25215;&#35834;&#23558;&#26469;&#21487;&#20197;&#26681;&#25454;&#20010;&#20307;&#38656;&#27714;&#23450;&#21046;&#25968;&#25454;&#38598;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21512;&#25104;&#25968;&#25454;&#36890;&#24120;&#24182;&#19981;&#23436;&#32654;&#65292;&#21487;&#33021;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#29983;&#25104;&#36807;&#31243;&#23545;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21333;&#32431;&#30340;&#21512;&#25104;&#25968;&#25454;&#26041;&#27861;&#8212;&#8212;&#23558;&#21512;&#25104;&#25968;&#25454;&#35270;&#20026;&#30495;&#23454;&#25968;&#25454;&#20351;&#29992;&#8212;&#8212;&#20250;&#23548;&#33268;&#19979;&#28216;&#27169;&#22411;&#21644;&#20998;&#26512;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#30495;&#23454;&#25968;&#25454;&#12290;&#20316;&#20026;&#21512;&#25104;&#25968;&#25454;&#29615;&#22659;&#19979;&#26356;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28145;&#24230;&#29983;&#25104;&#38598;&#25104;&#65288;DGE&#65289;&#8212;&#8212;&#21463;&#21040;&#28145;&#24230;&#38598;&#25104;&#21551;&#21457;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#38544;&#24335;&#22320;&#36817;&#20284;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;DGE&#25913;&#21892;&#20102;&#19979;&#28216;&#27169;&#22411;&#30340;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24179;&#22343;&#32780;&#35328;&#36828;&#36828;&#20248;&#20110;&#21333;&#32431;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#23569;&#25968;&#31867;&#21644;&#20302;&#23494;&#24230;&#21306;&#22495;&#65292;&#26368;&#22823;&#30340;&#25913;&#36827;&#25928;&#26524;&#24471;&#21040;&#20102;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating synthetic data through generative models is gaining interest in the ML community and beyond, promising a future where datasets can be tailored to individual needs. Unfortunately, synthetic data is usually not perfect, resulting in potential errors in downstream tasks. In this work we explore how the generative process affects the downstream ML task. We show that the naive synthetic data approach -- using synthetic data as if it is real -- leads to downstream models and analyses that do not generalize well to real data. As a first step towards better ML in the synthetic data regime, we introduce Deep Generative Ensemble (DGE) -- a framework inspired by Deep Ensembles that aims to implicitly approximate the posterior distribution over the generative process model parameters. DGE improves downstream model training, evaluation, and uncertainty quantification, vastly outperforming the naive approach on average. The largest improvements are achieved for minority classes and low-de
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#38750;&#23545;&#31216;&#26680;&#65288;asymmetric kernels&#65289;&#23454;&#29616;Toeplitz&#31070;&#32463;&#32593;&#32476;&#65288;TNNs&#65289;&#30340;&#21152;&#36895;&#65292;&#36890;&#36807;&#31232;&#30095;&#21152;&#20302;&#31209;Toeplitz&#30697;&#38453;&#20998;&#35299;&#12289;&#23567;&#22411;1D&#21367;&#31215;&#21644;&#26367;&#25442;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#22120;&#65288;RPE&#65289;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#23454;&#29616;O&#65288;n&#65289;&#22797;&#26434;&#24230;&#65292;&#38024;&#23545;&#22240;&#26524;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#8220;&#24555;&#36895;&#8221;&#22240;&#26524;&#23631;&#34109;&#26469;&#25269;&#28040;&#36825;&#31181;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.09028</link><description>&lt;p&gt;
SKI&#21152;&#36895;Toeplitz&#31070;&#32463;&#32593;&#32476;&#65306;&#36890;&#36807;&#38750;&#23545;&#31216;&#26680;&#23454;&#29616;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
SKI to go Faster: Accelerating Toeplitz Neural Networks via Asymmetric Kernels. (arXiv:2305.09028v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#38750;&#23545;&#31216;&#26680;&#65288;asymmetric kernels&#65289;&#23454;&#29616;Toeplitz&#31070;&#32463;&#32593;&#32476;&#65288;TNNs&#65289;&#30340;&#21152;&#36895;&#65292;&#36890;&#36807;&#31232;&#30095;&#21152;&#20302;&#31209;Toeplitz&#30697;&#38453;&#20998;&#35299;&#12289;&#23567;&#22411;1D&#21367;&#31215;&#21644;&#26367;&#25442;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#22120;&#65288;RPE&#65289;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#23454;&#29616;O&#65288;n&#65289;&#22797;&#26434;&#24230;&#65292;&#38024;&#23545;&#22240;&#26524;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#8220;&#24555;&#36895;&#8221;&#22240;&#26524;&#23631;&#34109;&#26469;&#25269;&#28040;&#36825;&#31181;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Toeplitz&#31070;&#32463;&#32593;&#32476;&#65288;TNNs&#65289;&#26159;&#26368;&#36817;&#20986;&#29616;&#24182;&#21462;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#32467;&#26524;&#30340;&#24207;&#21015;&#27169;&#22411;&#12290;&#23427;&#20204;&#38656;&#35201;O(n log n)&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;O(n)&#30340;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#22120;&#65288;RPE&#65289;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21644;&#34928;&#20943;&#20559;&#24046;&#35843;&#29992;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20943;&#23569;&#23427;&#20204;&#12290;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#65292;RPE&#26159;&#19968;&#20010;&#38750;&#23545;&#31216;&#27491;&#23450;&#26680;&#65292;&#32780;Toeplitz&#30697;&#38453;&#26159;&#20266;&#26684;&#25289;&#22982;&#30697;&#38453;&#12290;&#27492;&#22806;&#65306;1&#65289;&#23398;&#20064;&#30340;&#26680;&#22312;&#20027;&#23545;&#35282;&#32447;&#38468;&#36817;&#26174;&#31034;&#20986;&#21050;&#29366;&#34892;&#20026;&#65292;&#32780;&#22312;&#20854;&#20182;&#20301;&#32622;&#21017;&#34920;&#29616;&#20986;&#24179;&#28369;&#34892;&#20026;&#65307;2&#65289;RPE MLP&#36739;&#24930;&#12290;&#23545;&#20110;&#21452;&#21521;&#27169;&#22411;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#36827;&#34892;&#31232;&#30095;&#21152;&#20302;&#31209;Toeplitz&#30697;&#38453;&#20998;&#35299;&#12290;&#23545;&#20110;&#31232;&#30095;&#32452;&#20214;&#30340;&#25805;&#20316;&#65292;&#25105;&#20204;&#36827;&#34892;&#23567;&#22411;1D&#21367;&#31215;&#12290;&#23545;&#20110;&#20302;&#31209;&#32452;&#20214;&#65292;&#25105;&#20204;&#23558;RPE MLP&#26367;&#25442;&#20026;&#32447;&#24615;&#25554;&#20540;&#65292;&#24182;&#20351;&#29992;&#38750;&#23545;&#31216;&#26377;&#32467;&#26500;&#30340;&#20869;&#26680;&#25554;&#20540;&#65288;SKI&#65289;&#65288;Wilson&#31561;&#65292;2015&#65289;&#20197;&#23454;&#29616;O&#65288;n&#65289;&#22797;&#26434;&#24230;&#65306;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#35823;&#24046;&#20998;&#26512;&#12290;&#23545;&#20110;&#22240;&#26524;&#27169;&#22411;&#65292;&#8220;&#24555;&#36895;&#8221;&#22240;&#26524;&#23631;&#34109;&#65288;Katharopoulos&#31561;&#65292;2020&#65289;&#25269;&#28040;&#20102;SKI&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Toeplitz Neural Networks (TNNs) (Qin et. al. 2023) are a recent sequence model with impressive results. They require O(n log n) computational complexity and O(n) relative positional encoder (RPE) multi-layer perceptron (MLP) and decay bias calls. We aim to reduce both. We first note that the RPE is a non-SPD (symmetric positive definite) kernel and the Toeplitz matrices are pseudo-Gram matrices. Further 1) the learned kernels display spiky behavior near the main diagonals with otherwise smooth behavior; 2) the RPE MLP is slow. For bidirectional models, this motivates a sparse plus low-rank Toeplitz matrix decomposition. For the sparse component's action, we do a small 1D convolution. For the low rank component, we replace the RPE MLP with linear interpolation and use asymmetric Structured Kernel Interpolation (SKI) (Wilson et. al. 2015) for O(n) complexity: we provide rigorous error analysis. For causal models, "fast" causal masking (Katharopoulos et. al. 2020) negates SKI's benefits. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#31995;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;CGIB&#65292;&#36890;&#36807;&#26816;&#27979;&#20854;&#20013;&#30340;&#26680;&#24515;&#23376;&#22270;&#65292;&#39044;&#27979;&#23545;&#22270;&#23545;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.01520</link><description>&lt;p&gt;
&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#30340;&#26465;&#20214;&#22270;&#20449;&#24687;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Conditional Graph Information Bottleneck for Molecular Relational Learning. (arXiv:2305.01520v1 [q-bio.MN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#31995;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;CGIB&#65292;&#36890;&#36807;&#26816;&#27979;&#20854;&#20013;&#30340;&#26680;&#24515;&#23376;&#22270;&#65292;&#39044;&#27979;&#23545;&#22270;&#23545;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#26159;&#25351;&#23398;&#20064;&#20998;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#65292;&#22240;&#20854;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#23376;&#31185;&#23398;&#32780;&#24341;&#36215;&#26497;&#22823;&#20852;&#36259;&#12290;&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#20998;&#23376;&#24314;&#27169;&#20026;&#22270;&#32467;&#26500;&#65292;&#24182;&#32771;&#34385;&#20004;&#20010;&#20998;&#23376;&#20043;&#38388;&#30340;&#21407;&#23376;&#32423;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#24573;&#30053;&#20102;&#21270;&#23398;&#30340;&#26412;&#36136;&#12290;&#21363;&#21270;&#21512;&#29289;&#30001;&#22810;&#20010;&#20114;&#30456;&#20316;&#29992;&#30340;&#23376;&#32467;&#26500;&#32452;&#25104;&#65292;&#36825;&#20123;&#23376;&#32467;&#26500;&#20250;&#24341;&#36215;&#29420;&#29305;&#30340;&#21270;&#23398;&#21453;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#31995;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;CGIB&#65292;&#36890;&#36807;&#26816;&#27979;&#20854;&#20013;&#30340;&#26680;&#24515;&#23376;&#22270;&#65292;&#39044;&#27979;&#23545;&#22270;&#23545;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#65292;&#22312;&#32473;&#23450;&#19968;&#23545;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#19968;&#20010;&#22270;&#20013;&#25214;&#21040;&#21253;&#21547;&#26377;&#20851;&#25152;&#38656;&#20219;&#21153;&#30340;&#26368;&#23567;&#20805;&#20998;&#20449;&#24687;&#30340;&#23376;&#22270;&#65292;&#20197;&#27492;&#26469;&#39044;&#27979;&#36825;&#23545;&#22270;&#20043;&#38388;&#30340;&#20449;&#24687;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular relational learning, whose goal is to learn the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. Recently, graph neural networks have recently shown great success in molecular relational learning by modeling a molecule as a graph structure, and considering atom-level interactions between two molecules. Despite their success, existing molecular relational learning methods tend to overlook the nature of chemistry, i.e., a chemical compound is composed of multiple substructures such as functional groups that cause distinctive chemical reactions. In this work, we propose a novel relational learning framework, called CGIB, that predicts the interaction behavior between a pair of graphs by detecting core subgraphs therein. The main idea is, given a pair of graphs, to find a subgraph from a graph that contains the minimal sufficient information regarding the task at hand conditioned on the paired graph
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#21464;&#20998;&#36125;&#21494;&#26031;&#36817;&#20284;&#25512;&#26029;&#26041;&#27861;&#30340;&#25512;&#23548;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.14251</link><description>&lt;p&gt;
&#31616;&#21270;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#25512;&#23548;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Variational Bayes Made Easy. (arXiv:2304.14251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14251
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#21464;&#20998;&#36125;&#21494;&#26031;&#36817;&#20284;&#25512;&#26029;&#26041;&#27861;&#30340;&#25512;&#23548;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#36817;&#20284;&#25512;&#26029;&#26041;&#27861;&#65292;&#20294;&#20854;&#25512;&#23548;&#36807;&#31243;&#21487;&#33021;&#24456;&#32321;&#29712;&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#23547;&#25214;&#20851;&#20110;&#24050;&#30693;&#20998;&#24067;&#26399;&#26395;&#30340;&#32447;&#24615;&#24615;&#65292;&#26469;&#30830;&#23450;&#21518;&#39564;&#20998;&#24067;&#24418;&#24335;&#12290;&#28982;&#21518;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#36890;&#36807;&#8220;&#35835;&#21462;&#8221;&#36825;&#20123;&#26399;&#26395;&#21069;&#30340;&#39033;&#65292;&#20889;&#20986;&#26356;&#26032;&#12290;&#36825;&#20010;&#26041;&#27861;&#20351;&#24471;&#25512;&#23548;&#26356;&#21152;&#31616;&#21333;&#65292;&#24555;&#36895;&#65292;&#31616;&#30701;&#21644;&#36890;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational Bayes is a popular method for approximate inference but its derivation can be cumbersome. To simplify the process, we give a 3-step recipe to identify the posterior form by explicitly looking for linearity with respect to expectations of well-known distributions. We can then directly write the update by simply ``reading-off'' the terms in front of those expectations. The recipe makes the derivation easier, faster, shorter, and more general.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;S&#21442;&#25968;&#27169;&#24335;&#25104;&#21151;&#23454;&#29616;&#23545;Cu&#20114;&#36830;&#32570;&#38519;&#30340;&#38750;&#30772;&#22351;&#24615;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#22312;&#21516;&#26102;&#20998;&#26512;&#26681;&#26412;&#21407;&#22240;&#21644;&#20005;&#37325;&#24615;&#26041;&#38754;&#20855;&#26377;&#20808;&#36827;&#24615;&#65292;&#20855;&#22791;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#20934;&#30830;&#24230;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#31561;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.10207</link><description>&lt;p&gt;
SREL&#65306;&#22522;&#20110;S&#21442;&#25968;&#27169;&#24335;&#30340;&#38108;&#20114;&#36830;&#38750;&#30772;&#22351;&#25925;&#38556;&#35786;&#26029;&#30340;&#20005;&#37325;&#24615;&#35780;&#32423;&#38598;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SREL: Severity Rating Ensemble Learning for Non-Destructive Fault Diagnosis of Cu Interconnects using S-parameter Patterns. (arXiv:2304.10207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;S&#21442;&#25968;&#27169;&#24335;&#25104;&#21151;&#23454;&#29616;&#23545;Cu&#20114;&#36830;&#32570;&#38519;&#30340;&#38750;&#30772;&#22351;&#24615;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#22312;&#21516;&#26102;&#20998;&#26512;&#26681;&#26412;&#21407;&#22240;&#21644;&#20005;&#37325;&#24615;&#26041;&#38754;&#20855;&#26377;&#20808;&#36827;&#24615;&#65292;&#20855;&#22791;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#20934;&#30830;&#24230;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#31561;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22788;&#29702;&#22120;&#30340;&#24037;&#20316;&#39057;&#29575;&#21644;&#26102;&#38047;&#36895;&#24230;&#36880;&#24180;&#25552;&#39640;&#65292;&#20114;&#36830;&#23545;&#25972;&#20010;&#30005;&#23376;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#37117;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#26816;&#27979;&#21644;&#35786;&#26029;&#20114;&#36830;&#25925;&#38556;&#23545;&#30005;&#23376;&#20581;&#24247;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#30005;&#20449;&#21495;&#20316;&#20026;&#39044;&#27979;&#22240;&#23376;&#30340;&#29616;&#26377;&#30740;&#31350;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#26080;&#27861;&#21306;&#20998;&#32570;&#38519;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#26368;&#32456;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#30772;&#22351;&#24615;&#35780;&#20272;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#30340;&#24178;&#25200;&#32780;&#23548;&#33268;&#35823;&#35686;&#12290;&#26412;&#25991;&#23454;&#29616;&#20102;&#23545;Cu&#20114;&#36830;&#32570;&#38519;&#30340;&#38750;&#30772;&#22351;&#24615;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#23454;&#29616;&#20102;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#20934;&#30830;&#24230;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#30005;&#20449;&#21495;&#27169;&#24335;&#21516;&#26102;&#20998;&#26512;&#26681;&#26412;&#21407;&#22240;&#21644;&#20005;&#37325;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;S&#21442;&#25968;&#27169;&#24335;&#20855;&#26377;&#25925;&#38556;&#35786;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
As operating frequencies and clock speeds in processors have increased over the years, interconnects affect both the reliability and performance of entire electronic systems. Fault detection and diagnosis of the interconnects are crucial for prognostics and health management (PHM) of electronics. However, existing research works utilizing electrical signals as prognostic factors have limitations, such as the inability to distinguish the root cause of defects, which eventually requires additional destructive evaluation, and vulnerability to noise that results in a false alarm. Herein, we realize the non-destructive detection and diagnosis of defects in Cu interconnects, achieving early detection, high diagnostic accuracy, and noise robustness. To the best of our knowledge, this study first simultaneously analyzes the root cause and severity using electrical signal patterns. In this paper, we experimentally show that S-parameter patterns have the ability for fault diagnosis and they are 
&lt;/p&gt;</description></item><item><title>SimbaML&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#23427;&#36890;&#36807;&#26426;&#26800;&#27169;&#22411;&#34917;&#20805;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#20174;&#21512;&#25104;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#36716;&#31227;&#23398;&#20064;&#12289;&#25968;&#25454;&#22686;&#24378;&#12289;&#29289;&#29702;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#31561;&#22810;&#39033;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04000</link><description>&lt;p&gt;
SimbaML&#65306;&#20351;&#29992;&#22686;&#24378;&#25968;&#25454;&#36830;&#25509;&#26426;&#26800;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
SimbaML: Connecting Mechanistic Models and Machine Learning with Augmented Data. (arXiv:2304.04000v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04000
&lt;/p&gt;
&lt;p&gt;
SimbaML&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#23427;&#36890;&#36807;&#26426;&#26800;&#27169;&#22411;&#34917;&#20805;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#20174;&#21512;&#25104;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#36716;&#31227;&#23398;&#20064;&#12289;&#25968;&#25454;&#22686;&#24378;&#12289;&#29289;&#29702;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#31561;&#22810;&#39033;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#26469;&#35828;&#65292;&#25910;&#38598;&#36825;&#20123;&#25968;&#25454;&#38598;&#24448;&#24448;&#26159;&#22256;&#38590;&#25110;&#26114;&#36149;&#30340;&#12290;&#22914;&#26524;&#26377;&#31995;&#32479;&#21160;&#24577;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#21017;&#21487;&#20197;&#20351;&#29992;&#26426;&#26800;&#27169;&#22411;&#26469;&#34917;&#20805;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SimbaML&#65288;&#22522;&#20110;&#20223;&#30495;&#30340;&#26426;&#22120;&#23398;&#20064;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#65292;&#23427;&#36890;&#36807;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#30452;&#25509;&#23558;&#20854;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#30340;&#20998;&#26512;&#21644;&#21253;&#21547;&#12290; SimbaML&#26041;&#20415;&#22320;&#21551;&#29992;&#20102;&#20174;&#21512;&#25104;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#36716;&#31227;&#23398;&#20064;&#65292;&#25968;&#25454;&#22686;&#24378;&#65292;&#30830;&#23450;&#23545;&#25968;&#25454;&#25910;&#38598;&#30340;&#38656;&#27714;&#20197;&#21450;&#22522;&#20934;&#27979;&#35797;&#29289;&#29702;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;SimbaML&#21487;&#22312;https://pypi.org/project/simba-ml/&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training sophisticated machine learning (ML) models requires large datasets that are difficult or expensive to collect for many applications. If prior knowledge about system dynamics is available, mechanistic representations can be used to supplement real-world data. We present SimbaML (Simulation-Based ML), an open-source tool that unifies realistic synthetic dataset generation from ordinary differential equation-based models and the direct analysis and inclusion in ML pipelines. SimbaML conveniently enables investigating transfer learning from synthetic to real-world data, data augmentation, identifying needs for data collection, and benchmarking physics-informed ML approaches. SimbaML is available from https://pypi.org/project/simba-ml/.
&lt;/p&gt;</description></item><item><title>&#31232;&#32570;&#30693;&#35782;&#23545;&#33258;&#21160;&#21270;&#20915;&#31574;&#36896;&#25104;&#20102;&#38556;&#30861;&#65292;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#26159;&#36890;&#36807;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#32531;&#35299;&#19981;&#21516;&#39046;&#22495;&#30693;&#35782;&#24046;&#36317;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03452</link><description>&lt;p&gt;
&#22270;&#32467;&#26500;&#25903;&#25345;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Graph Enabled Cross-Domain Knowledge Transfer. (arXiv:2304.03452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03452
&lt;/p&gt;
&lt;p&gt;
&#31232;&#32570;&#30693;&#35782;&#23545;&#33258;&#21160;&#21270;&#20915;&#31574;&#36896;&#25104;&#20102;&#38556;&#30861;&#65292;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#26159;&#36890;&#36807;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#32531;&#35299;&#19981;&#21516;&#39046;&#22495;&#30693;&#35782;&#24046;&#36317;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20219;&#20309;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#24517;&#39035;&#23558;&#32473;&#23450;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#65292;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#65289;&#36716;&#21270;&#20026;&#21487;&#20197;&#34987;&#20854;&#20860;&#23481;&#35821;&#35328;&#21644;&#25968;&#25454;&#26684;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29702;&#35299;&#21644;&#22788;&#29702;&#30340;&#34920;&#31034;&#21521;&#37327;&#12290;&#28982;&#32780;&#65292;&#32463;&#24120;&#36935;&#21040;&#30340;&#22256;&#38590;&#26159;&#65292;&#39318;&#20808;&#32473;&#23450;&#30340;&#30693;&#35782;&#24182;&#19981;&#20805;&#20998;&#25110;&#21487;&#38752;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#20204;&#20250;&#23547;&#27714;&#34701;&#21512;&#26469;&#33258;&#21333;&#29420;&#39046;&#22495;&#30340;&#36741;&#21161;&#20449;&#24687;&#26469;&#32531;&#35299;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24863;&#20852;&#36259;&#39046;&#22495;&#30340;&#31232;&#32570;&#30693;&#35782;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36825;&#31181;&#26041;&#27861;&#34987;&#31216;&#20026;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#12290;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20174;&#22312;&#32447;&#21307;&#30103;&#24179;&#21488;&#20998;&#26512;&#21040;&#37329;&#34701;&#24066;&#22330;&#39118;&#38505;&#37327;&#21270;&#65292;&#37117;&#23384;&#22312;&#31232;&#32570;&#30693;&#35782;&#30340;&#20849;&#24615;&#65292;&#36825;&#20026;&#25105;&#20204;&#20174;&#33258;&#21160;&#21270;&#20915;&#31574;&#20013;&#21463;&#30410;&#30041;&#19979;&#20102;&#38556;&#30861;&#12290;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#21322;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#21033;&#29992;&#20102;&#36825;&#31181;&#36328;&#39046;&#22495;&#36716;&#31227;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
To leverage machine learning in any decision-making process, one must convert the given knowledge (for example, natural language, unstructured text) into representation vectors that can be understood and processed by machine learning model in their compatible language and data format. The frequently encountered difficulty is, however, the given knowledge is not rich or reliable enough in the first place. In such cases, one seeks to fuse side information from a separate domain to mitigate the gap between good representation learning and the scarce knowledge in the domain of interest. This approach is named Cross-Domain Knowledge Transfer. It is crucial to study the problem because of the commonality of scarce knowledge in many scenarios, from online healthcare platform analyses to financial market risk quantification, leaving an obstacle in front of us benefiting from automated decision making. From the machine learning perspective, the paradigm of semi-supervised learning takes advanta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00215</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#24402;&#32435;&#20851;&#31995;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#20851;&#31995;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#29616;&#26377;&#30340;&#23884;&#20837;&#24335;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36716;&#23548;&#35774;&#32622;&#65292;&#32570;&#20047;&#24402;&#32435;&#33021;&#21147;&#65292;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#23454;&#20307;&#19978;&#36827;&#34892;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#20998;&#23618;Transformer&#26694;&#26550;&#65292;&#21363;REPORT&#65292;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;REPORT&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#20004;&#20010;&#23436;&#20840;&#24402;&#32435;&#30340;&#25968;&#25454;&#38598;&#30340;&#20843;&#20010;&#29256;&#26412;&#23376;&#38598;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;REPORT&#33021;&#22815;&#23558;&#25512;&#29702;&#25512;&#24191;&#21040;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#27809;&#26377;&#20844;&#20849;&#23454;&#20307;&#30340;&#26032;&#23454;&#20307;&#19978;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation prediction on knowledge graphs (KGs) is a key research topic. Dominant embedding-based methods mainly focus on the transductive setting and lack the inductive ability to generalize to new entities for inference. Existing methods for inductive reasoning mostly mine the connections between entities, i.e., relational paths, without considering the nature of head and tail entities contained in the relational context. This paper proposes a novel method that captures both connections between entities and the intrinsic nature of entities, by simultaneously aggregating RElational Paths and cOntext with a unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely on relation semantics and can naturally generalize to the fully-inductive setting, where KGs for training and inference have no common entities. In the experiments, REPORT performs consistently better than all baselines on almost all the eight version subsets of two fully-inductive datasets. Moreover. REPO
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#22522;&#20110;&#36798;&#23572;&#25991;&#33258;&#28982;&#36873;&#25321;&#65292;&#32467;&#21512;&#20989;&#25968;&#36873;&#25321;&#21644;&#36816;&#31639;&#31526;&#36873;&#25321;&#20004;&#20010;&#36807;&#31243;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26500;&#24314;&#29702;&#35770;&#65292;&#21487;&#33258;&#21160;&#21457;&#29616;&#21644;&#34920;&#31034;&#33258;&#28982;&#23450;&#24459;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#25311;&#22810;&#39046;&#22495;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#26032;&#26041;&#27861;&#35299;&#20915;&#25551;&#36848;&#33258;&#28982;&#23450;&#24459;&#30340;&#20005;&#26684;&#25968;&#23398;&#27169;&#22411;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17607</link><description>&lt;p&gt;
&#21457;&#29616;&#33258;&#28982;&#23450;&#24459;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine learning for discovering laws of nature. (arXiv:2303.17607v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17607
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#22522;&#20110;&#36798;&#23572;&#25991;&#33258;&#28982;&#36873;&#25321;&#65292;&#32467;&#21512;&#20989;&#25968;&#36873;&#25321;&#21644;&#36816;&#31639;&#31526;&#36873;&#25321;&#20004;&#20010;&#36807;&#31243;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26500;&#24314;&#29702;&#35770;&#65292;&#21487;&#33258;&#21160;&#21457;&#29616;&#21644;&#34920;&#31034;&#33258;&#28982;&#23450;&#24459;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#25311;&#22810;&#39046;&#22495;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#26032;&#26041;&#27861;&#35299;&#20915;&#25551;&#36848;&#33258;&#28982;&#23450;&#24459;&#30340;&#20005;&#26684;&#25968;&#23398;&#27169;&#22411;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35266;&#31890;&#23376;&#36981;&#24490;&#37327;&#23376;&#21147;&#23398;&#30340;&#21407;&#29702;&#8212;&#8212;&#37027;&#20040;&#23439;&#35266;&#21644;&#24494;&#35266;&#19990;&#30028;&#20043;&#38388;&#30340;&#26126;&#30830;&#30028;&#38480;&#22312;&#21738;&#37324;&#21602;&#65311;&#27491;&#26159;&#36825;&#20010;&#8220;&#35299;&#37322;&#38382;&#39064;&#8221;&#20419;&#20351;&#34203;&#23450;&#35860;&#25552;&#20986;&#20102;&#20182;&#33879;&#21517;&#30340;&#24605;&#24819;&#23454;&#39564;&#65288;&#19968;&#21482;&#21516;&#26102;&#27515;&#20129;&#21644;&#27963;&#30528;&#30340;&#29483;&#65289;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#37327;&#23376;&#27979;&#37327;&#38382;&#39064;&#30340;&#28608;&#28872;&#20105;&#35770;&#65292;&#20294;&#33267;&#20170;&#20173;&#27809;&#26377;&#20196;&#20154;&#28385;&#24847;&#30340;&#31572;&#26696;&#12290;&#36825;&#27491;&#26159;&#25551;&#36848;&#33258;&#28982;&#23450;&#24459;&#30340;&#20005;&#26684;&#25968;&#23398;&#27169;&#22411;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36798;&#23572;&#25991;&#33258;&#28982;&#36873;&#25321;&#30340;&#35745;&#31639;&#27169;&#22411;&#26469;&#25551;&#36848;&#21644;&#29702;&#35299;&#33258;&#28982;&#23450;&#24459;&#12290;&#23454;&#38469;&#19978;&#65292;&#26080;&#35770;&#26159;&#23439;&#35266;&#31890;&#23376;&#12289;&#24494;&#35266;&#30005;&#23376;&#36824;&#26159;&#23433;&#20840;&#38382;&#39064;&#65292;&#23427;&#20204;&#37117;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#23454;&#20307;&#65292;&#36825;&#20010;&#23454;&#20307;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21464;&#21270;&#65292;&#21487;&#20197;&#29992;&#29366;&#24577;&#21644;&#20540;&#32452;&#25104;&#30340;&#25968;&#25454;&#24207;&#21015;&#26469;&#25551;&#36848;&#12290;&#35266;&#23519;&#32773;&#21487;&#20197;&#20174;&#36825;&#20010;&#25968;&#25454;&#24207;&#21015;&#20013;&#23398;&#20064;&#65292;&#26500;&#24314;&#29702;&#35770;&#65288;&#36890;&#24120;&#30001;&#20989;&#25968;&#21644;&#24494;&#20998;&#26041;&#31243;&#32452;&#25104;&#65289;&#12290;&#25105;&#20204;&#19981;&#20877;&#20351;&#29992;&#29992;&#25143;&#30340;&#32463;&#39564;&#25110;&#36923;&#36753;&#26469;&#24314;&#27169;&#65292;&#32780;&#26159;&#20351;&#29992;&#25968;&#25454;&#12290;&#35745;&#31639;&#27169;&#22411;&#30340;&#26680;&#24515;&#22522;&#20110;&#20004;&#20010;&#36807;&#31243;&#65306;&#20989;&#25968;&#36873;&#25321;&#21644;&#36816;&#31639;&#31526;&#36873;&#25321;&#12290;&#20989;&#25968;&#36873;&#25321;&#36807;&#31243;&#31867;&#20284;&#20110;&#36798;&#23572;&#25991;&#30340;&#36827;&#21270;&#65292;&#20801;&#35768;&#20855;&#26377;&#20248;&#21183;&#29305;&#24449;&#30340;&#20989;&#25968;&#29983;&#23384;&#21644;&#32321;&#27542;&#65307;&#32780;&#36816;&#31639;&#31526;&#36873;&#25321;&#36807;&#31243;&#25429;&#25417;&#20102;&#33258;&#28982;&#23450;&#24459;&#30340;&#30456;&#20114;&#20381;&#23384;&#24615;&#65292;&#21487;&#20197;&#24179;&#34913;&#33258;&#28982;&#30028;&#20013;&#19981;&#21516;&#20989;&#25968;&#30340;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#33258;&#21160;&#21457;&#29616;&#21644;&#34920;&#31034;&#33258;&#28982;&#23450;&#24459;&#65292;&#24182;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#25311;&#37327;&#23376;&#21147;&#23398;&#12289;&#32463;&#20856;&#21147;&#23398;&#21644;&#31995;&#32479;&#29983;&#29289;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
A microscopic particle obeys the principles of quantum mechanics -- so where is the sharp boundary between the macroscopic and microscopic worlds? It was this "interpretation problem" that prompted Schr\"odinger to propose his famous thought experiment (a cat that is simultaneously both dead and alive) and sparked a great debate about the quantum measurement problem, and there is still no satisfactory answer yet. This is precisely the inadequacy of rigorous mathematical models in describing the laws of nature. We propose a computational model to describe and understand the laws of nature based on Darwin's natural selection. In fact, whether it's a macro particle, a micro electron or a security, they can all be considered as an entity, the change of this entity over time can be described by a data series composed of states and values. An observer can learn from this data series to construct theories (usually consisting of functions and differential equations). We don't model with the us
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#39537;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;silent&#35270;&#39057;&#21098;&#36753;&#21512;&#25104;&#39640;&#20445;&#30495;&#30340;&#20914;&#20987;&#22768;&#65292;&#24182;&#20351;&#29992;&#39069;&#22806;&#30340;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#26469;&#25351;&#23548;&#20914;&#20987;&#22768;&#21512;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.16897</link><description>&lt;p&gt;
&#29289;&#29702;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#21512;&#25104;&#20914;&#20987;&#22768;
&lt;/p&gt;
&lt;p&gt;
Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos. (arXiv:2303.16897v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16897
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#39537;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;silent&#35270;&#39057;&#21098;&#36753;&#21512;&#25104;&#39640;&#20445;&#30495;&#30340;&#20914;&#20987;&#22768;&#65292;&#24182;&#20351;&#29992;&#39069;&#22806;&#30340;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#26469;&#25351;&#23548;&#20914;&#20987;&#22768;&#21512;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29289;&#20307;&#30456;&#20114;&#20316;&#29992;&#21457;&#20986;&#30340;&#22768;&#38899;&#36827;&#34892;&#24314;&#27169;&#23545;&#20110;&#23454;&#38469;&#19990;&#30028;&#21644;&#34394;&#25311;&#19990;&#30028;&#20013;&#30340;&#27785;&#28024;&#24335;&#24863;&#23448;&#20307;&#39564;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#20914;&#20987;&#22768;&#21512;&#25104;&#26041;&#27861;&#20351;&#29992;&#29289;&#29702;&#27169;&#25311;&#26469;&#33719;&#24471;&#19968;&#32452;&#33021;&#22815;&#34920;&#31034;&#21644;&#21512;&#25104;&#22768;&#38899;&#30340;&#29289;&#29702;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#29289;&#20307;&#30340;&#32454;&#33410;&#21644;&#20914;&#20987;&#20301;&#32622;&#65292;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#24456;&#23569;&#21487;&#29992;&#65292;&#24182;&#19988;&#26080;&#27861;&#24212;&#29992;&#20110;&#20174;&#26222;&#36890;&#35270;&#39057;&#20013;&#21512;&#25104;&#20914;&#20987;&#22768;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#35270;&#39057;&#39537;&#21160;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21482;&#33021;&#25429;&#25417;&#21040;&#35270;&#35273;&#20869;&#23481;&#21644;&#20914;&#20987;&#22768;&#20043;&#38388;&#30340;&#24369;&#23545;&#24212;&#20851;&#31995;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#29289;&#29702;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;&#38745;&#24577;&#35270;&#39057;&#21098;&#36753;&#21512;&#25104;&#39640;&#20445;&#30495;&#30340;&#20914;&#20987;&#22768;&#12290;&#38500;&#20102;&#35270;&#39057;&#20869;&#23481;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;&#39069;&#22806;&#30340;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#26469;&#25351;&#23548;&#20914;&#20987;&#22768;&#21512;&#25104;&#36807;&#31243;&#65292;&#36825;&#20123;&#20808;&#39564;&#21253;&#25324;&#26082;&#21487;&#25511;&#21046;&#29289;&#29702;&#21442;&#25968;&#65292;&#21516;&#26102;&#20063;&#33021;&#20445;&#35777;&#38899;&#25928;&#36136;&#37327;&#30340;&#22122;&#22768;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling sounds emitted from physical object interactions is critical for immersive perceptual experiences in real and virtual worlds. Traditional methods of impact sound synthesis use physics simulation to obtain a set of physics parameters that could represent and synthesize the sound. However, they require fine details of both the object geometries and impact locations, which are rarely available in the real world and can not be applied to synthesize impact sounds from common videos. On the other hand, existing video-driven deep learning-based approaches could only capture the weak correspondence between visual content and impact sounds since they lack of physics knowledge. In this work, we propose a physics-driven diffusion model that can synthesize high-fidelity impact sound for a silent video clip. In addition to the video content, we propose to use additional physics priors to guide the impact sound synthesis procedure. The physics priors include both physics parameters that are
&lt;/p&gt;</description></item><item><title>BlackVIP&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40657;&#30418;&#35270;&#35273;&#25552;&#31034;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#21442;&#25968;&#21487;&#35775;&#38382;&#24615;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36866;&#24212;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21327;&#35843;&#22120;&#21644;SPSA-GC&#32452;&#20214;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.14773</link><description>&lt;p&gt;
BlackVIP: &#38024;&#23545;&#31283;&#20581;&#36801;&#31227;&#23398;&#20064;&#30340;&#40657;&#30418;&#35270;&#35273;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning. (arXiv:2303.14773v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14773
&lt;/p&gt;
&lt;p&gt;
BlackVIP&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40657;&#30418;&#35270;&#35273;&#25552;&#31034;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#21442;&#25968;&#21487;&#35775;&#38382;&#24615;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36866;&#24212;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21327;&#35843;&#22120;&#21644;SPSA-GC&#32452;&#20214;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#30340;&#20852;&#36215;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#24494;&#35843;&#20026;&#20247;&#22810;&#19979;&#28216;&#20219;&#21153;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#22823;&#22411;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#36716;&#31227;&#23398;&#20064;&#65288;PETL&#65289;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;PETL&#26041;&#27861;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20048;&#35266;&#30340;&#20551;&#35774;&#65306;1&#65289;PTM&#30340;&#25972;&#20010;&#21442;&#25968;&#38598;&#26159;&#21487;&#29992;&#30340;&#65292;2&#65289;&#20855;&#22791;&#36275;&#22815;&#22823;&#30340;&#20869;&#23384;&#23481;&#37327;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;PTMs&#20316;&#20026;&#40657;&#30418;API&#25110;&#19987;&#26377;&#36719;&#20214;&#25552;&#20379;&#65292;&#27809;&#26377;&#26126;&#30830;&#21442;&#25968;&#21487;&#35775;&#38382;&#24615;&#12290;&#27492;&#22806;&#65292;&#28385;&#36275;&#29616;&#20195;PTMs&#30340;&#22823;&#20869;&#23384;&#35201;&#27714;&#20063;&#24456;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40657;&#30418;&#35270;&#35273;&#25552;&#31034;&#65288;BlackVIP&#65289;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;PTMs&#65292;&#32780;&#19981;&#38656;&#35201;&#20851;&#20110;&#27169;&#22411;&#26550;&#26500;&#21644;&#21442;&#25968;&#30340;&#30693;&#35782;&#12290;BlackVIP&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65306;1&#65289;&#21327;&#35843;&#22120;&#21644;2&#65289;&#24102;&#26799;&#24230;&#26657;&#27491;&#30340;&#21516;&#26102;&#25200;&#21160;&#38543;&#26426;&#36924;&#36817;&#65288;SPSA-GC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the surge of large-scale pre-trained models (PTMs), fine-tuning these models to numerous downstream tasks becomes a crucial problem. Consequently, parameter efficient transfer learning (PETL) of large models has grasped huge attention. While recent PETL methods showcase impressive performance, they rely on optimistic assumptions: 1) the entire parameter set of a PTM is available, and 2) a sufficiently large memory capacity for the fine-tuning is equipped. However, in most real-world applications, PTMs are served as a black-box API or proprietary software without explicit parameter accessibility. Besides, it is hard to meet a large memory requirement for modern PTMs. In this work, we propose black-box visual prompting (BlackVIP), which efficiently adapts the PTMs without knowledge about model architectures and parameters. BlackVIP has two components; 1) Coordinator and 2) simultaneous perturbation stochastic approximation with gradient correction (SPSA-GC). The Coordinator designs 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#26694;&#26550;&#65292;&#21487;&#28789;&#27963;&#22788;&#29702;&#20219;&#20309;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#36890;&#36807;&#27491;&#24577;&#36817;&#20284;&#25351;&#23548;&#21487;&#25193;&#23637;&#33258;&#36866;&#24212;&#35774;&#35745;&#65292;&#37319;&#29992;&#27531;&#20313;&#26102;&#38480;&#20248;&#21270;&#36873;&#25321;&#37319;&#26679;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11582</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#36866;&#24212;&#24615;&#23454;&#39564;&#65306;&#28789;&#27963;&#25209;&#22788;&#29702;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive Experimentation at Scale: Bayesian Algorithms for Flexible Batches. (arXiv:2303.11582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#26694;&#26550;&#65292;&#21487;&#28789;&#27963;&#22788;&#29702;&#20219;&#20309;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#36890;&#36807;&#27491;&#24577;&#36817;&#20284;&#25351;&#23548;&#21487;&#25193;&#23637;&#33258;&#36866;&#24212;&#35774;&#35745;&#65292;&#37319;&#29992;&#27531;&#20313;&#26102;&#38480;&#20248;&#21270;&#36873;&#25321;&#37319;&#26679;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#20551;&#23450;&#25345;&#32493;&#37325;&#26032;&#20998;&#37197;&#27979;&#37327;&#24037;&#20316;&#65292;&#36825;&#22312;&#23454;&#29616;&#36807;&#31243;&#20013;&#23384;&#22312;&#24310;&#36831;&#21453;&#39304;&#21644;&#22522;&#30784;&#35774;&#26045;/&#32452;&#32455;&#38590;&#39064;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;&#20165;&#26377;&#23569;&#25968;&#37325;&#26032;&#20998;&#37197;&#38454;&#27573;&#30340;&#23454;&#38469;&#24773;&#20917;&#65292;&#20854;&#20013;&#27979;&#37327;&#32467;&#26524;&#26159;&#20197;&#25209;&#22788;&#29702;&#24418;&#24335;&#27979;&#37327;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#24615;&#23454;&#39564;&#26694;&#26550;&#65292;&#21487;&#28789;&#27963;&#22788;&#29702;&#20219;&#20309;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#26159;&#65292;&#22312;&#32479;&#35745;&#25512;&#26029;&#20013;&#26222;&#36941;&#20351;&#29992;&#30340;&#27491;&#24577;&#36817;&#20284;&#20063;&#21487;&#20197;&#25351;&#23548;&#21487;&#25193;&#23637;&#33258;&#36866;&#24212;&#35774;&#35745;&#12290;&#36890;&#36807;&#25512;&#23548;&#28176;&#36827;&#39034;&#24207;&#23454;&#39564;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#65292;&#21487;&#20197;&#21033;&#29992;&#24179;&#22343;&#22238;&#25253;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#21160;&#24577;&#35268;&#21010;&#30340;&#29366;&#24577;&#36716;&#31227;&#30456;&#23545;&#20110;&#37319;&#26679;&#20998;&#37197;&#26159;&#21487;&#24494;&#30340;&#65292;&#20801;&#35768;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#35268;&#21010;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36845;&#20195;&#35268;&#21010;&#26041;&#27861;&#65292;&#21363;&#27531;&#20313;&#26102;&#38480;&#20248;&#21270;&#65292;&#36890;&#36807;&#20248;&#21270;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#35268;&#21010;&#30446;&#26631;&#26469;&#36873;&#25321;&#37319;&#26679;&#20998;&#37197;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#27169;&#22359;&#21270;&#21644;&#26131;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard bandit algorithms that assume continual reallocation of measurement effort are challenging to implement due to delayed feedback and infrastructural/organizational difficulties. Motivated by practical instances involving a handful of reallocation epochs in which outcomes are measured in batches, we develop a new adaptive experimentation framework that can flexibly handle any batch size. Our main observation is that normal approximations universal in statistical inference can also guide the design of scalable adaptive designs. By deriving an asymptotic sequential experiment, we formulate a dynamic program that can leverage prior information on average rewards. State transitions of the dynamic program are differentiable with respect to the sampling allocations, allowing the use of gradient-based methods for planning and policy optimization. We propose a simple iterative planning method, Residual Horizon Optimization, which selects sampling allocations by optimizing a planning obj
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#23545;&#35937;&#20013;&#24515;&#30340;&#27133;&#25193;&#25955;&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Latent Slot Diffusion (LSD)&#27169;&#22411;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#27133;&#35299;&#30721;&#22120;&#65292;&#21516;&#26102;&#20063;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26080;&#30417;&#30563;&#36827;&#34892;&#32452;&#21512;&#26465;&#20214;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.10834</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#35937;&#20013;&#24515;&#30340;&#27133;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Slot Diffusion. (arXiv:2303.10834v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10834
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#35937;&#20013;&#24515;&#30340;&#27133;&#25193;&#25955;&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Latent Slot Diffusion (LSD)&#27169;&#22411;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#27133;&#35299;&#30721;&#22120;&#65292;&#21516;&#26102;&#20063;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26080;&#30417;&#30563;&#36827;&#34892;&#32452;&#21512;&#26465;&#20214;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;Transformer&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#22330;&#26223;&#30340;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#36825;&#31361;&#26174;&#20102;&#24378;&#22823;&#30340;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#20013;&#30340;&#25972;&#21512;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20173;&#28982;&#36739;&#23569;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Latent Slot Diffusion (LSD)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#20004;&#20010;&#30446;&#26631;&#65306;&#39318;&#20808;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#23558;&#20256;&#32479;&#30340;&#27133;&#35299;&#30721;&#22120;&#26367;&#25442;&#20026;&#20197;&#23545;&#35937;&#27133;&#20026;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#27169;&#22411;&#65307;&#20854;&#27425;&#65292;&#23427;&#20063;&#26159;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#20687;&#25991;&#26412;&#36825;&#26679;&#30340;&#30417;&#30563;&#27880;&#37322;&#32780;&#33021;&#22815;&#26080;&#30417;&#30563;&#22320;&#36827;&#34892;&#32452;&#21512;&#26465;&#20214;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#23545;&#35937;&#20013;&#24515;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#39318;&#27425;&#22312;FFHQ&#25968;&#25454;&#38598;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in t
&lt;/p&gt;</description></item><item><title>NeuSE&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#35937;&#30340;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#31639;&#27861;&#65292;&#25903;&#25345;&#23454;&#29616;&#19982;&#38271;&#26399;&#22330;&#26223;&#21464;&#21270;&#19968;&#33268;&#30340;&#23545;&#35937;&#31354;&#38388;&#29702;&#35299;&#12290;&#23427;&#21487;&#20197;&#32534;&#30721;&#23436;&#25972;&#30340;&#24418;&#29366;&#20449;&#24687;&#65292;&#24182;&#19982;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#35937;&#21516;&#26102;&#21464;&#25442;&#65292;&#33021;&#22815;&#30452;&#25509;&#25512;&#26029;&#30456;&#23545;&#24103;&#21464;&#25442;&#21644;&#30456;&#26426;&#23039;&#24577;&#32422;&#26463;&#65292;&#24182;&#32500;&#25345;&#36866;&#24212;&#21464;&#21270;&#30340;&#36731;&#37327;&#32423;&#23545;&#35937;&#22320;&#22270;&#12290;</title><link>http://arxiv.org/abs/2303.07308</link><description>&lt;p&gt;
NeuSE: Neural SE(3)-&#31561;&#21464;&#23884;&#20837;&#29992;&#20110;&#19968;&#33268;&#30340;&#23545;&#35937;&#31354;&#38388;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects. (arXiv:2303.07308v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07308
&lt;/p&gt;
&lt;p&gt;
NeuSE&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#35937;&#30340;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#31639;&#27861;&#65292;&#25903;&#25345;&#23454;&#29616;&#19982;&#38271;&#26399;&#22330;&#26223;&#21464;&#21270;&#19968;&#33268;&#30340;&#23545;&#35937;&#31354;&#38388;&#29702;&#35299;&#12290;&#23427;&#21487;&#20197;&#32534;&#30721;&#23436;&#25972;&#30340;&#24418;&#29366;&#20449;&#24687;&#65292;&#24182;&#19982;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#35937;&#21516;&#26102;&#21464;&#25442;&#65292;&#33021;&#22815;&#30452;&#25509;&#25512;&#26029;&#30456;&#23545;&#24103;&#21464;&#25442;&#21644;&#30456;&#26426;&#23039;&#24577;&#32422;&#26463;&#65292;&#24182;&#32500;&#25345;&#36866;&#24212;&#21464;&#21270;&#30340;&#36731;&#37327;&#32423;&#23545;&#35937;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;NeuSE&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#35937;&#30340;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;SE(3)-&#31561;&#21464;&#23884;&#20837;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#25903;&#25345;&#23545;&#35937;SLAM&#20197;&#23454;&#29616;&#19982;&#38271;&#26399;&#22330;&#26223;&#21464;&#21270;&#19968;&#33268;&#30340;&#31354;&#38388;&#29702;&#35299;&#12290;NeuSE&#26159;&#20174;&#37096;&#20998;&#23545;&#35937;&#35266;&#27979;&#20013;&#21019;&#24314;&#30340;&#19968;&#32452;&#28508;&#22312;&#23545;&#35937;&#23884;&#20837;&#65292;&#22312;&#19982;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#35937;&#21516;&#26102;SE(3)-&#31561;&#21464;&#21464;&#25442;&#30340;&#21516;&#26102;&#65292;&#23427;&#20316;&#20026;&#19968;&#20010;&#32039;&#20945;&#30340;&#28857;&#20113;&#26367;&#20195;&#23436;&#25972;&#30340;&#23545;&#35937;&#27169;&#22411;&#65292;&#32534;&#30721;&#20102;&#23436;&#25972;&#30340;&#24418;&#29366;&#20449;&#24687;&#12290;&#36890;&#36807;NeuSE&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#25512;&#26029;&#30340;&#28508;&#22312;&#20195;&#30721;&#20013;&#33719;&#24471;&#30456;&#23545;&#24103;&#21464;&#25442;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20351;&#29992;NeuSE&#36827;&#34892;&#23545;&#35937;&#24418;&#29366;&#21644;&#23039;&#24577;&#29305;&#24449;&#21270;&#30340;SLAM&#33539;&#24335;&#21487;&#20197;&#29420;&#31435;&#36816;&#34892;&#25110;&#19982;&#20856;&#22411;&#30340;SLAM&#31995;&#32479;&#32467;&#21512;&#20351;&#29992;&#12290;&#23427;&#30452;&#25509;&#25512;&#26029;&#19982;&#26222;&#36890;SLAM&#23039;&#24577;&#22270;&#20248;&#21270;&#20860;&#23481;&#30340;SE(3)&#30456;&#26426;&#23039;&#24577;&#32422;&#26463;&#65292;&#21516;&#26102;&#36824;&#32500;&#25345;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#22320;&#22270;&#65292;&#33021;&#22815;&#36866;&#24212;&#29616;&#23454;&#19990;&#30028;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#21547;&#25913;&#21464;&#30340;&#23545;&#35937;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#24207;&#21015;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and illustrate how it supports object SLAM for consistent spatial understanding with long-term scene changes. NeuSE is a set of latent object embeddings created from partial object observations. It serves as a compact point cloud surrogate for complete object models, encoding full shape information while transforming SE(3)-equivariantly in tandem with the object in the physical world. With NeuSE, relative frame transforms can be directly derived from inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape and pose characterization, can operate independently or in conjunction with typical SLAM systems. It directly infers SE(3) camera pose constraints that are compatible with general SLAM pose graph optimization, while also maintaining a lightweight object-centric map that adapts to real-world changes. Our approach is evaluated on synthetic and real-world sequences featuring changed objects 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23616;&#37096;-&#20840;&#23616;&#26041;&#27861;&#36827;&#34892;&#24191;&#20041;&#22826;&#38451;&#36752;&#23556;&#39044;&#27979;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#21487;&#20197;&#26500;&#24314;&#19981;&#38656;&#35201;&#23454;&#26102;&#25968;&#25454;&#21644;&#21382;&#21490;&#35266;&#27979;&#25968;&#25454;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.06010</link><description>&lt;p&gt;
&#36890;&#36807;&#23616;&#37096;-&#20840;&#23616;&#26041;&#27861;&#36827;&#34892;&#24191;&#20041;&#22826;&#38451;&#36752;&#23556;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Local-Global Methods for Generalised Solar Irradiance Forecasting. (arXiv:2303.06010v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23616;&#37096;-&#20840;&#23616;&#26041;&#27861;&#36827;&#34892;&#24191;&#20041;&#22826;&#38451;&#36752;&#23556;&#39044;&#27979;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#21487;&#20197;&#26500;&#24314;&#19981;&#38656;&#35201;&#23454;&#26102;&#25968;&#25454;&#21644;&#21382;&#21490;&#35266;&#27979;&#25968;&#25454;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22826;&#38451;&#33021;&#20351;&#29992;&#37327;&#30340;&#22686;&#21152;&#65292;&#20934;&#30830;&#21450;&#26102;&#30340;&#39044;&#27979;&#23545;&#20110;&#36816;&#33829;&#24179;&#31283;&#30340;&#30005;&#32593;&#36816;&#33829;&#21830;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#26377;&#35768;&#22810;&#25552;&#20986;&#30340;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#22826;&#38451;&#36752;&#23556;/&#22826;&#38451;&#33021;&#20135;&#37327;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#26102;&#38388;&#24207;&#21015;&#65292;&#20381;&#36182;&#20110;&#23545;&#24863;&#20852;&#36259;&#20301;&#32622;&#38468;&#36817;&#35266;&#27979;&#25968;&#25454;&#30340;&#23454;&#26102;&#35775;&#38382;&#26469;&#29983;&#25104;&#39044;&#27979;&#12290;&#36825;&#35201;&#27714;&#21516;&#26102;&#20855;&#22791;&#23454;&#26102;&#25968;&#25454;&#27969;&#21644;&#36275;&#22815;&#30340;&#21382;&#21490;&#35266;&#27979;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#20840;&#23616;&#26041;&#27861;&#20197;&#19968;&#31181;&#27867;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20026;&#26410;&#30693;&#20301;&#32622;&#29983;&#25104;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;&#20998;&#24067;&#22312;&#33521;&#22269;&#22659;&#20869;&#30340;20&#20010;&#20301;&#32622;&#21644;&#24191;&#27867;&#21487;&#24471;&#30340;&#22825;&#27668;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#26500;&#24314;&#19981;&#38656;&#35201;&#35775;&#38382;&#27492;&#25968;&#25454;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#21516;&#26102;&#21033;&#29992;&#21644;&#27604;&#36739;&#20102;&#21355;&#26143;&#21644;&#22320;&#38754;&#35266;&#27979;&#25968;&#25454;&#65288;&#20363;&#22914;&#28201;&#24230;&#65292;&#21387;&#24378;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the use of solar power increases, having accurate and timely forecasts will be essential for smooth grid operators. There are many proposed methods for forecasting solar irradiance / solar power production. However, many of these methods formulate the problem as a time-series, relying on near real-time access to observations at the location of interest to generate forecasts. This requires both access to a real-time stream of data and enough historical observations for these methods to be deployed. In this paper, we propose the use of Global methods to train our models in a generalised way, enabling them to generate forecasts for unseen locations. We apply this approach to both classical ML and state of the art methods. Using data from 20 locations distributed throughout the UK and widely available weather data, we show that it is possible to build systems that do not require access to this data. We utilise and compare both satellite and ground observations (e.g. temperature, pressur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#39532;&#23572;&#21487;&#22827;&#21183;&#21338;&#24328;&#20013;&#30340;&#23616;&#37096;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#21270;&#21644;&#20989;&#25968;&#36924;&#36817;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#19981;&#20381;&#36182;&#20110;&#20195;&#29702;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.04865</link><description>&lt;p&gt;
&#32593;&#32476;&#39532;&#23572;&#21487;&#22827;&#21183;&#21338;&#24328;&#20013;&#23616;&#37096;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games. (arXiv:2303.04865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#39532;&#23572;&#21487;&#22827;&#21183;&#21338;&#24328;&#20013;&#30340;&#23616;&#37096;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#21270;&#21644;&#20989;&#25968;&#36924;&#36817;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#19981;&#20381;&#36182;&#20110;&#20195;&#29702;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#32593;&#32476;&#39532;&#23572;&#21487;&#22827;&#21183;&#21338;&#24328;&#20013;&#24341;&#20837;&#19968;&#31867;&#19982;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#30456;&#20851;&#30340;&#21183;&#21338;&#24328;&#12290;&#27599;&#20010;&#20195;&#29702;&#37117;&#26377;&#33258;&#24049;&#30340;&#23616;&#37096;&#21183;&#20989;&#25968;&#65292;&#27599;&#20010;&#20195;&#29702;&#30340;&#22870;&#21169;&#21482;&#21462;&#20915;&#20110;&#37051;&#22495;&#20013;&#20195;&#29702;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#20195;&#29702;&#21482;&#20351;&#29992;&#23616;&#37096;&#20449;&#24687;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#20840;&#23616;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#26469;&#20811;&#26381;&#32500;&#24230;&#28798;&#38590;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#22312;&#23616;&#37096;&#21270;&#35823;&#24046;&#21644;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#24179;&#22343;&#32435;&#20160;&#36951;&#25022;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;$\tilde{\mathcal {O}}(\tilde{\epsilon}^{-4})$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#19981;&#20381;&#36182;&#20110;&#20195;&#29702;&#25968;&#37327;&#30340;&#22810;&#26234;&#33021;&#20307;&#31454;&#20105;&#21338;&#24328;&#30340;&#26377;&#38480;&#26679;&#26412;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a class of networked Markov potential games in which agents are associated with nodes in a network. Each agent has its own local potential function, and the reward of each agent depends only on the states and actions of the agents within a neighborhood. In this context, we propose a localized actor-critic algorithm. The algorithm is scalable since each agent uses only local information and does not need access to the global state. Further, the algorithm overcomes the curse of dimensionality through the use of function approximation. Our main results provide finite-sample guarantees up to a localization error and a function approximation error. Specifically, we achieve an $\tilde{\mathcal{O}}(\tilde{\epsilon}^{-4})$ sample complexity measured by the averaged Nash regret. This is the first finite-sample bound for multi-agent competitive games that does not depend on the number of agents.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#26391;&#35835;&#21644;&#33258;&#30001;&#35828;&#35805;&#35821;&#38899;&#21512;&#25104;&#20013;&#20351;&#29992;&#30340;&#33258;&#25105;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#22312;12&#23618;wav2vec2.0&#65288;ASR&#24494;&#35843;&#65289;&#30340;&#31532;9&#23618;&#34920;&#29616;&#26368;&#20248;&#12290;&#36825;&#39033;&#24037;&#20316;&#25581;&#31034;&#20102;&#35821;&#38899;SSL&#22914;&#20309;&#25913;&#36827;TTS&#31995;&#32479;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;TTS&#29983;&#25104;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2303.02719</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#22312;&#26391;&#35835;&#21644;&#33258;&#30001;&#35828;&#35805;&#35821;&#38899;&#21512;&#25104;&#20013;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Self-Supervised Speech Representations in Read and Spontaneous TTS. (arXiv:2303.02719v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02719
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#26391;&#35835;&#21644;&#33258;&#30001;&#35828;&#35805;&#35821;&#38899;&#21512;&#25104;&#20013;&#20351;&#29992;&#30340;&#33258;&#25105;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#22312;12&#23618;wav2vec2.0&#65288;ASR&#24494;&#35843;&#65289;&#30340;&#31532;9&#23618;&#34920;&#29616;&#26368;&#20248;&#12290;&#36825;&#39033;&#24037;&#20316;&#25581;&#31034;&#20102;&#35821;&#38899;SSL&#22914;&#20309;&#25913;&#36827;TTS&#31995;&#32479;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;TTS&#29983;&#25104;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35821;&#38899;&#34920;&#31034;&#65292;&#22914;wav2vec2.0&#20316;&#20026;&#26631;&#20934;&#20004;&#38454;&#27573;&#35821;&#38899;&#21512;&#25104;&#65288;TTS&#65289;&#20013;&#30340;&#34920;&#31034;&#20171;&#36136;&#65292;&#20197;&#21462;&#20195;&#24815;&#24120;&#20351;&#29992;&#30340;mel&#39057;&#35889;&#22270;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#21738;&#31181;&#35821;&#38899;SSL&#36866;&#29992;&#20110;TTS&#65292;&#24182;&#19988;&#26391;&#35835;&#21644;&#33258;&#30001;&#35828;&#35805;TTS&#20043;&#38388;&#30340;&#24615;&#33021;&#26159;&#21542;&#26377;&#25152;&#19981;&#21516;&#65292;&#21518;&#32773;&#21487;&#33021;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22312;&#26391;&#35835;&#21644;&#33258;&#30001;&#35828;&#35805;&#35821;&#26009;&#24211;&#19978;&#27979;&#35797;&#20960;&#31181;&#35821;&#38899;SSL&#65292;&#21253;&#25324;&#21516;&#19968;&#20010;SSL&#30340;&#19981;&#21516;&#23618;&#65292;&#22312;&#20445;&#25345;TTS&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#35774;&#32622;&#24658;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#21548;&#27979;&#35797;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26391;&#35835;&#21644;&#33258;&#30001;&#35828;&#35805;&#30340;TTS&#20013;&#65292;12&#23618;wav2vec2.0&#65288;ASR&#24494;&#35843;&#65289;&#30340;&#31532;9&#23618;&#32988;&#36807;&#20854;&#20182;&#34987;&#27979;&#35797;&#30340;SSL&#21644;mel&#39057;&#35889;&#22270;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#35821;&#38899;SSL&#22914;&#20309;&#21487;&#20197;&#26041;&#20415;&#22320;&#25913;&#36827;&#24403;&#21069;&#30340;TTS&#31995;&#32479;&#65292;&#20197;&#21450;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;TTS&#29983;&#25104;&#20219;&#21153;&#20013;SSL&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has explored using self-supervised learning (SSL) speech representations such as wav2vec2.0 as the representation medium in standard two-stage TTS, in place of conventionally used mel-spectrograms. It is however unclear which speech SSL is the better fit for TTS, and whether or not the performance differs between read and spontaneous TTS, the later of which is arguably more challenging. This study aims at addressing these questions by testing several speech SSLs, including different layers of the same SSL, in two-stage TTS on both read and spontaneous corpora, while maintaining constant TTS model architecture and training settings. Results from listening tests show that the 9th layer of 12-layer wav2vec2.0 (ASR finetuned) outperforms other tested SSLs and mel-spectrogram, in both read and spontaneous TTS. Our work sheds light on both how speech SSL can readily improve current TTS systems, and how SSLs compare in the challenging generative task of TTS. Audio examples can be 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#21160;&#21147;&#31995;&#32479;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#36741;&#21161;&#20989;&#25968;&#20316;&#20026;Koopman&#21487;&#35266;&#27979;&#37327;&#65292;&#19981;&#38656;&#35201;&#26126;&#30830;&#30340;&#27169;&#22411;&#21457;&#29616;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.01483</link><description>&lt;p&gt;
&#36741;&#21161;&#20989;&#25968;&#20316;&#20026;Koopman&#21487;&#35266;&#27979;&#37327;&#65306;&#22522;&#20110;&#25968;&#25454;&#30340;&#21160;&#21147;&#31995;&#32479;&#22810;&#39033;&#24335;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Auxiliary Functions as Koopman Observables: Data-Driven Polynomial Optimization for Dynamical Systems. (arXiv:2303.01483v2 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01483
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#21160;&#21147;&#31995;&#32479;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#36741;&#21161;&#20989;&#25968;&#20316;&#20026;Koopman&#21487;&#35266;&#27979;&#37327;&#65292;&#19981;&#38656;&#35201;&#26126;&#30830;&#30340;&#27169;&#22411;&#21457;&#29616;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;&#21160;&#21147;&#31995;&#32479;&#20998;&#26512;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26126;&#30830;&#30340;&#27169;&#22411;&#21457;&#29616;&#12290;&#35813;&#26041;&#27861;&#28304;&#20110;&#20174;&#25968;&#25454;&#20013;&#36924;&#36817;Koopman&#31639;&#23376;&#30340;&#25216;&#26415;&#65292;&#24182;&#19988;&#20197;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#25968;&#20540;&#27714;&#35299;&#30340;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#19981;&#20851;&#24515;&#25968;&#25454;&#26159;&#36890;&#36807;&#30830;&#23450;&#24615;&#36824;&#26159;&#38543;&#26426;&#36807;&#31243;&#29983;&#25104;&#30340;&#65292;&#22240;&#27492;&#29992;&#25143;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#35843;&#25972;&#21363;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#20005;&#26684;&#30340;&#25910;&#25947;&#24615;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#23558;&#25991;&#29486;&#20013;&#31867;&#20284;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#25193;&#23637;&#21644;&#32479;&#19968;&#12290;&#36890;&#36807;&#23545;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21160;&#21147;&#23398;&#30340;&#21560;&#24341;&#23376;&#19978;&#21457;&#29616;Lyapunov&#20989;&#25968;&#12289;&#25191;&#34892;&#36941;&#21382;&#20248;&#21270;&#20197;&#21450;&#30028;&#23450;&#26497;&#20540;&#30340;&#31034;&#20363;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a flexible data-driven method for dynamical system analysis that does not require explicit model discovery. The method is rooted in well-established techniques for approximating the Koopman operator from data and is implemented as a semidefinite program that can be solved numerically. Furthermore, the method is agnostic of whether data is generated through a deterministic or stochastic process, so its implementation requires no prior adjustments by the user to accommodate these different scenarios. Rigorous convergence results justify the applicability of the method, while also extending and uniting similar results from across the literature. Examples on discovering Lyapunov functions, performing ergodic optimization, and bounding extrema over attractors for both deterministic and stochastic dynamics exemplify these convergence results and demonstrate the performance of the method.
&lt;/p&gt;</description></item><item><title>OmniForce&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#12289;&#22823;&#27169;&#22411;&#21644;&#20113;&#36793;&#21327;&#21516;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#24320;&#25918;&#29615;&#22659;&#19979;&#30340;AutoML&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;AutoML&#31995;&#32479;&#21644;&#24179;&#21488;&#20302;&#25928;&#19988;&#35745;&#31639;&#22797;&#26434;&#65292;&#32780;OmniForce&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#25216;&#26415;&#25913;&#36827;&#20102;&#36825;&#19968;&#29616;&#29366;&#12290;</title><link>http://arxiv.org/abs/2303.00501</link><description>&lt;p&gt;
OmniForce: &#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#12289;&#22823;&#27169;&#22411;&#21644;&#20113;&#36793;&#21327;&#21516;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
OmniForce: On Human-Centered, Large Model Empowered and Cloud-Edge Collaborative AutoML System. (arXiv:2303.00501v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00501
&lt;/p&gt;
&lt;p&gt;
OmniForce&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#12289;&#22823;&#27169;&#22411;&#21644;&#20113;&#36793;&#21327;&#21516;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#24320;&#25918;&#29615;&#22659;&#19979;&#30340;AutoML&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;AutoML&#31995;&#32479;&#21644;&#24179;&#21488;&#20302;&#25928;&#19988;&#35745;&#31639;&#22797;&#26434;&#65292;&#32780;OmniForce&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#25216;&#26415;&#25913;&#36827;&#20102;&#36825;&#19968;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064; (AutoML) &#26088;&#22312;&#20197;&#26368;&#23569;&#20154;&#21147;&#25237;&#20837;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;AutoML&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20197;&#20415;&#22312;&#26500;&#24314;&#20154;&#24037;&#26234;&#33021; (AI) &#24212;&#29992;&#31243;&#24207;&#26102;&#23558;&#20154;&#31867;&#25490;&#38500;&#22312;&#22806;&#65292;&#20294;&#40092;&#26377;&#25991;&#29486;&#20851;&#27880;AutoML&#22914;&#20309;&#22312;&#24320;&#25918;&#29615;&#22659;&#22330;&#26223;&#20013;&#24037;&#20316;&#65292;&#20363;&#22914;&#35757;&#32451;&#21644;&#26356;&#26032;&#22823;&#22411;&#27169;&#22411;&#12289;&#24037;&#19994;&#20379;&#24212;&#38142;&#25110;&#24037;&#19994;&#34394;&#25311;&#19990;&#30028;&#12290;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#65292;&#20154;&#20204;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#32463;&#24120;&#38754;&#20020;&#24320;&#25918;&#29615;&#36335;&#38382;&#39064;&#65306;&#20182;&#20204;&#24517;&#39035;&#19981;&#26029;&#25910;&#38598;&#25968;&#25454;&#12289;&#26356;&#26032;&#25968;&#25454;&#21644;&#27169;&#22411;&#12289;&#28385;&#36275;&#24320;&#21457;&#21644;&#37096;&#32626;&#29615;&#22659;&#30340;&#35201;&#27714;&#12289;&#25903;&#25345;&#22823;&#35268;&#27169;&#35774;&#22791;&#12289;&#20462;&#25913;&#35780;&#20272;&#25351;&#26631;&#31561;&#12290;&#20351;&#29992;&#32431;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35299;&#20915;&#24320;&#25918;&#29615;&#22659;&#38382;&#39064;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#12289;&#35745;&#31639;&#36164;&#28304;&#21644;&#26469;&#33258;&#19987;&#32844;&#25968;&#25454;&#24037;&#31243;&#24072;&#30340;&#21162;&#21147;&#65292;&#20351;&#24471;&#24403;&#21069;&#30340;AutoML&#31995;&#32479;&#21644;&#24179;&#21488;&#20302;&#25928;&#19988;&#35745;&#31639;&#22797;&#26434;&#12290;&#20154;&#26426;&#20132;&#20114;&#25216;&#26415;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated machine learning (AutoML) seeks to build ML models with minimal human effort. While considerable research has been conducted in the area of AutoML in general, aiming to take humans out of the loop when building artificial intelligence (AI) applications, scant literature has focused on how AutoML works well in open-environment scenarios such as the process of training and updating large models, industrial supply chains or the industrial metaverse, where people often face open-loop problems during the search process: they must continuously collect data, update data and models, satisfy the requirements of the development and deployment environment, support massive devices, modify evaluation metrics, etc. Addressing the open-environment issue with pure data-driven approaches requires considerable data, computing resources, and effort from dedicated data engineers, making current AutoML systems and platforms inefficient and computationally intractable. Human-computer interaction i
&lt;/p&gt;</description></item><item><title>&#24102;&#26377;&#32972;&#21253;&#30340;&#25504;&#22842;&#32773;&#38382;&#39064;&#22312;&#38543;&#26426;&#21644;&#25932;&#23545;&#24773;&#20917;&#19979;&#23384;&#22312;&#24040;&#22823;&#30340;&#24046;&#36317;&#65292;&#23588;&#20854;&#26159;&#22312;&#25932;&#23545;&#24773;&#20917;&#19979;&#65292;&#24403;&#39044;&#31639;&#26356;&#21152;&#32039;&#32570;&#26102;&#20445;&#35777;&#24615;&#33021;&#21464;&#24471;&#26356;&#24046;&#12290;</title><link>http://arxiv.org/abs/2302.14686</link><description>&lt;p&gt;
&#24102;&#26377;&#32972;&#21253;&#30340;&#36817;&#20284;&#31283;&#23450;&#25504;&#22842;&#32773;
&lt;/p&gt;
&lt;p&gt;
Approximately Stationary Bandits with Knapsacks. (arXiv:2302.14686v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14686
&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#32972;&#21253;&#30340;&#25504;&#22842;&#32773;&#38382;&#39064;&#22312;&#38543;&#26426;&#21644;&#25932;&#23545;&#24773;&#20917;&#19979;&#23384;&#22312;&#24040;&#22823;&#30340;&#24046;&#36317;&#65292;&#23588;&#20854;&#26159;&#22312;&#25932;&#23545;&#24773;&#20917;&#19979;&#65292;&#24403;&#39044;&#31639;&#26356;&#21152;&#32039;&#32570;&#26102;&#20445;&#35777;&#24615;&#33021;&#21464;&#24471;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#32972;&#21253;&#30340;&#25504;&#22842;&#32773;&#38382;&#39064;&#65288;BwK&#65289;&#26159;&#22312;&#20840;&#23616;&#39044;&#31639;&#32422;&#26463;&#19979;&#23558;&#25504;&#22842;&#32773;&#38382;&#39064;&#36827;&#34892;&#27867;&#21270;&#30340;&#30740;&#31350;&#65292;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#20004;&#20010;&#26497;&#31471;&#20043;&#19968;&#65306;&#38543;&#26426;BwK&#65292;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#22870;&#21169;&#21644;&#36164;&#28304;&#30340;&#28040;&#32791;&#20174;&#19968;&#20010;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#65307;&#32780;&#25932;&#23545;BwK&#65292;&#21017;&#30001;&#23545;&#25163;&#36873;&#25321;&#36825;&#20123;&#21442;&#25968;&#12290;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#21487;&#23454;&#29616;&#20445;&#35777;&#23384;&#22312;&#24040;&#22823;&#30340;&#24046;&#36317;&#65306;&#22312;&#38543;&#26426;&#24773;&#20917;&#19979;&#21487;&#20197;&#36798;&#21040;&#26080;&#24724;&#23398;&#20064;&#65292;&#32780;&#22312;&#25932;&#23545;&#24773;&#20917;&#19979;&#21482;&#33021;&#36798;&#21040;&#22522;&#20110;&#31454;&#20105;&#27604;&#30340;&#20445;&#35777;&#65292;&#20854;&#20013;&#31454;&#20105;&#27604;&#21462;&#20915;&#20110;&#39044;&#31639;&#25110;&#21516;&#26102;&#21462;&#20915;&#20110;&#26102;&#38388;&#21644;&#36164;&#28304;&#25968;&#37327;&#12290;&#36825;&#31181;&#24046;&#36317;&#20043;&#25152;&#20197;&#22914;&#27492;&#24040;&#22823;&#65292;&#22312;&#25932;&#23545;BwK&#30340;&#20856;&#22411;&#24773;&#20917;&#19979;&#65292;&#20445;&#35777;&#24615;&#33021;&#21464;&#24471;&#26356;&#24046;&#26102;&#65292;&#39044;&#31639;&#26356;&#21152;&#32039;&#32570;&#12290;&#34429;&#28982;&#24050;&#30693;&#23384;&#22312;&#8220;&#20004;&#20840;&#20854;&#32654;&#8221;&#31867;&#22411;&#30340;&#31639;&#27861;&#65288;&#21333;&#20010;&#31639;&#27861;&#21487;&#20197;&#22312;&#20004;&#20010;&#26497;&#31471;&#24773;&#20917;&#19979;&#25552;&#20379;&#26368;&#20339;&#30340;&#21487;&#23454;&#29616;&#20445;&#35777;&#65289;&#65292;&#23427;&#20204;&#30340;&#30028;&#38480;&#21017;&#20250;&#21464;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bandits with Knapsacks (BwK), the generalization of the Bandits problem under global budget constraints, has received a lot of attention in recent years. Previous work has focused on one of the two extremes: Stochastic BwK where the rewards and consumptions of the resources of each round are sampled from an i.i.d. distribution, and Adversarial BwK where these parameters are picked by an adversary. Achievable guarantees in the two cases exhibit a massive gap: No-regret learning is achievable in the stochastic case, but in the adversarial case only competitive ratio style guarantees are achievable, where the competitive ratio depends either on the budget or on both the time and the number of resources. What makes this gap so vast is that in Adversarial BwK the guarantees get worse in the typical case when the budget is more binding. While ``best-of-both-worlds'' type algorithms are known (single algorithms that provide the best achievable guarantee in each extreme case), their bounds deg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;CLIP&#30340;&#24555;&#36895;&#27867;&#21270;&#21644;&#20010;&#24615;&#21270;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36866;&#37197;&#22120;&#65292;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20449;&#24687;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;&#23458;&#25143;&#31471;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.13485</link><description>&lt;p&gt;
FedCLIP&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#29992;&#20110;CLIP&#30340;&#24555;&#36895;&#27867;&#21270;&#21644;&#20010;&#24615;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning. (arXiv:2302.13485v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;CLIP&#30340;&#24555;&#36895;&#27867;&#21270;&#21644;&#20010;&#24615;&#21270;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36866;&#37197;&#22120;&#65292;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20449;&#24687;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;&#23458;&#25143;&#31471;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#35745;&#31639;&#30340;&#26032;&#33539;&#24335;&#24050;&#32463;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;FL&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#38469;&#24615;&#33021;&#65306;&#25968;&#25454;&#20998;&#24067;&#24322;&#36136;&#24615;&#21644;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#24102;&#26469;&#30340;&#39640;&#36164;&#28304;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19981;&#21516;&#23458;&#25143;&#31471;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20351;&#24471;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#38590;&#20197;&#25910;&#25947;&#65292;&#32780;&#39640;&#36164;&#28304;&#25104;&#26412;&#65288;&#21253;&#25324;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#65289;&#22686;&#21152;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#38590;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#32780;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;FedCLIP&#65292;&#29992;&#20110;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#20013;CLIP&#30340;&#24555;&#36895;&#27867;&#21270;&#21644;&#20010;&#24615;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36866;&#37197;&#22120;&#26469;&#36866;&#24212;&#22823;&#22411;&#27169;&#22411;CLIP&#65292;&#20854;&#20313;&#25805;&#20316;&#20165;&#20381;&#36182;&#20110;&#36866;&#37197;&#22120;&#12290;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20449;&#24687;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#23458;&#25143;&#31471;&#20013;&#20855;&#26377;&#33258;&#36866;&#24212;&#24615;&#12290;&#21516;&#26102;&#65292;&#23567;&#35268;&#27169;&#25805;&#20316;&#21487;&#20197;&#32531;&#35299;&#35745;&#31639;&#36127;&#25285;&#21644;&#36890;&#20449;&#21387;&#21147;&#65292;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has emerged as a new paradigm for privacy-preserving computation in recent years. Unfortunately, FL faces two critical challenges that hinder its actual performance: data distribution heterogeneity and high resource costs brought by large foundation models. Specifically, the non-IID data in different clients make existing FL algorithms hard to converge while the high resource costs, including computational and communication costs that increase the deployment difficulty in real-world scenarios. In this paper, we propose an effective yet simple method, named FedCLIP, to achieve fast generalization and personalization for CLIP in federated learning. Concretely, we design an attention-based adapter for the large model, CLIP, and the rest operations merely depend on adapters. Lightweight adapters can make the most use of pretrained model information and ensure models be adaptive for clients in specific tasks. Simultaneously, small-scale operations can mitigate the co
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33258;&#36866;&#24212;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#65292;&#33021;&#22815;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#30340;&#25512;&#29702;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10325</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Adaptive Sparse Gaussian Process. (arXiv:2302.10325v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10325
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33258;&#36866;&#24212;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#65292;&#33021;&#22815;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#30340;&#25512;&#29702;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#23398;&#20064;&#23545;&#20110;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#30340;&#23398;&#20064;&#26426;&#22120;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#24536;&#35760;&#36807;&#21435;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#39640;&#25928;&#30340;&#31639;&#27861;&#38656;&#35201;&#32039;&#20945;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#20197;&#20415;&#19981;&#38543;&#30528;&#26032;&#25968;&#25454;&#30340;&#21040;&#26469;&#32780;&#22686;&#21152;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#20197;&#26368;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#36827;&#34892;&#22312;&#32447;&#21442;&#25968;&#26356;&#26032;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#21482;&#26159;&#37096;&#20998;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#30340;&#33258;&#36866;&#24212;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#36951;&#24536;&#22240;&#23376;&#37325;&#26032;&#23450;&#20041;&#20102;&#21464;&#20998;&#31232;&#30095;GP&#31639;&#27861;&#65292;&#20351;&#20854;&#20855;&#26377;&#33258;&#36866;&#24212;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#20351;&#27169;&#22411;&#25512;&#29702;&#23613;&#21487;&#33021;&#31616;&#21333;&#65292;&#25105;&#20204;&#24314;&#35758;&#27599;&#24403;&#20986;&#29616;&#26032;&#26679;&#26412;&#26102;&#21516;&#26102;&#26356;&#26032;&#31232;&#30095;GP&#27169;&#22411;&#30340;&#19968;&#20010;&#21333;&#20010;&#24341;&#23548;&#28857;&#21644;&#20854;&#20182;&#27169;&#22411;&#21442;&#25968;&#12290;&#32467;&#26524;&#65292;&#35813;&#31639;&#27861;&#21576;&#29616;&#20986;&#25512;&#29702;&#36807;&#31243;&#30340;&#24555;&#36895;&#25910;&#25947;&#24615;&#65292;&#21363;&#20351;&#22312;&#39640;&#24230;&#38750;&#24179;&#31283;&#30340;&#29615;&#22659;&#20013;&#20063;&#33021;&#36827;&#34892;&#39640;&#25928;&#30340;&#27169;&#22411;&#26356;&#26032;&#65288;&#21482;&#38656;&#19968;&#27425;&#25512;&#29702;&#36845;&#20195;&#65289;&#12290;&#35797;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive learning is necessary for non-stationary environments where the learning machine needs to forget past data distribution. Efficient algorithms require a compact model update to not grow in computational burden with the incoming data and with the lowest possible computational cost for online parameter updating. Existing solutions only partially cover these needs. Here, we propose the first adaptive sparse Gaussian Process (GP) able to address all these issues. We first reformulate a variational sparse GP algorithm to make it adaptive through a forgetting factor. Next, to make the model inference as simple as possible, we propose updating a single inducing point of the sparse GP model together with the remaining model parameters every time a new sample arrives. As a result, the algorithm presents a fast convergence of the inference process, which allows an efficient model update (with a single inference iteration) even in highly non-stationary environments. Experimental results d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#31639;&#23376;&#27169;&#25311;&#30913;&#27969;&#20307;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#20934;&#30830;&#25429;&#25417;&#20102;&#30913;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#30340;&#29289;&#29702;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.08332</link><description>&lt;p&gt;
&#30913;&#27969;&#20307;&#21147;&#23398;&#19982;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Magnetohydrodynamics with Physics Informed Neural Operators. (arXiv:2302.08332v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#31639;&#23376;&#27169;&#25311;&#30913;&#27969;&#20307;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#20934;&#30830;&#25429;&#25417;&#20102;&#30913;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#30340;&#29289;&#29702;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24314;&#27169;&#22810;&#23610;&#24230;&#21644;&#22810;&#29289;&#29702;&#22797;&#26434;&#31995;&#32479;&#65292;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#33021;&#22815;&#20805;&#20998;&#21457;&#25381;&#26497;&#31471;&#35268;&#27169;&#35745;&#31639;&#30340;&#31185;&#23398;&#36719;&#20214;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#27169;&#25311;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#21644;&#26102;&#38388;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26469;&#21152;&#36895;&#22797;&#26434;&#31995;&#32479;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#20854;&#35745;&#31639;&#25104;&#26412;&#20165;&#20026;&#20256;&#32479;&#26041;&#27861;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#24182;&#39318;&#27425;&#23558;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#31639;&#23376;&#24212;&#29992;&#20110;&#20108;&#32500;&#19981;&#21487;&#21387;&#32553;&#30913;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23558;&#24352;&#37327;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#20316;&#20026;&#20854;&#22522;&#30784;&#65292;&#24182;&#20351;&#29992;TensorLY&#36719;&#20214;&#21253;&#36827;&#34892;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#31639;&#23376;&#21487;&#20197;&#20934;&#30830;&#25429;&#25417;&#25551;&#36848;&#38647;&#35834;&#25968;$Re\leq250$&#30340;&#23618;&#27969;&#30913;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#30340;&#29289;&#29702;&#29305;&#24615;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23558;&#25105;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#29992;&#20110;&#28237;&#27969;&#27969;&#21160;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#22810;&#31181;&#21487;&#33021;&#30340;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The modeling of multi-scale and multi-physics complex systems typically involves the use of scientific software that can optimally leverage extreme scale computing. Despite major developments in recent years, these simulations continue to be computationally intensive and time consuming. Here we explore the use of AI to accelerate the modeling of complex systems at a fraction of the computational cost of classical methods, and present the first application of physics informed neural operators to model 2D incompressible magnetohydrodynamics simulations. Our AI models incorporate tensor Fourier neural operators as their backbone, which we implemented with the TensorLY package. Our results indicate that physics informed neural operators can accurately capture the physics of magnetohydrodynamics simulations that describe laminar flows with Reynolds numbers $Re\leq250$. We also explore the applicability of our AI surrogates for turbulent flows, and discuss a variety of methodologies that may
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#29983;&#26426;&#22120;&#30340;&#25945;&#24072;-&#23398;&#29983;&#35774;&#32622;&#65292;&#36890;&#36807;&#23398;&#29983;&#26426;&#22120;&#30340;&#38598;&#21512;&#26469;&#30740;&#31350;&#30001;&#20855;&#26377;&#22823;&#37327;&#21487;&#35843;&#21442;&#25968;&#30340;DNN&#30340;&#30417;&#30563;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;DNN&#30340;&#23398;&#20064;&#22312;&#32593;&#32476;&#31354;&#38388;&#20013;&#30456;&#24403;&#24322;&#36136;&#12290;</title><link>http://arxiv.org/abs/2302.07419</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#29983;&#26426;&#22120;&#23454;&#29616;&#31354;&#38388;&#24322;&#36136;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spatially heterogeneous learning by a deep student machine. (arXiv:2302.07419v3 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#29983;&#26426;&#22120;&#30340;&#25945;&#24072;-&#23398;&#29983;&#35774;&#32622;&#65292;&#36890;&#36807;&#23398;&#29983;&#26426;&#22120;&#30340;&#38598;&#21512;&#26469;&#30740;&#31350;&#30001;&#20855;&#26377;&#22823;&#37327;&#21487;&#35843;&#21442;&#25968;&#30340;DNN&#30340;&#30417;&#30563;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;DNN&#30340;&#23398;&#20064;&#22312;&#32593;&#32476;&#31354;&#38388;&#20013;&#30456;&#24403;&#24322;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21462;&#24471;&#20102;&#38750;&#20961;&#30340;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#20855;&#26377;&#22823;&#37327;&#21487;&#35843;&#21442;&#25968;&#65292;&#20854;&#20173;&#28982;&#26159;&#40657;&#21283;&#23376;&#12290;&#20026;&#20102;&#30740;&#31350;DNN&#30340;&#38544;&#34255;&#23618;&#65292;&#26412;&#25991;&#36890;&#36807;&#19968;&#31181;&#32479;&#35745;&#21147;&#23398;&#26041;&#27861;&#31216;&#20026;&#25945;&#24072;-&#23398;&#29983;&#35774;&#32622;&#65292;&#30740;&#31350;&#20102;&#30001;&#23485;&#24230;&#20026;N&#65292;&#28145;&#24230;&#20026;L&#65292;&#30001;&#20855;&#26377;c&#20010;&#36755;&#20837;&#30340;&#24863;&#30693;&#26426;&#32452;&#25104;&#30340;DNN&#30340;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#23398;&#29983;&#26426;&#22120;&#30340;&#38598;&#21512;&#65292;&#35813;&#38598;&#21512;&#21487;&#20197;&#31934;&#30830;&#37325;&#29616;&#30001;&#25945;&#24072;&#26426;&#22120;&#25552;&#20379;&#30340;M&#32452;N&#32500;&#36755;&#20837;/&#36755;&#20986;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#21103;&#26412;&#26041;&#27861;&#65288;H. Yoshino&#65288;2020&#65289;&#65289;&#29702;&#35770;&#20998;&#26512;&#20102;&#38598;&#21512;&#65292;&#24182;&#36827;&#34892;&#20102;&#36138;&#23146;&#30340;Monte Carlo&#27169;&#25311;&#12290;&#23545;&#20110;&#39640;&#32500;&#25968;&#25454;$N \gg 1$&#65292;&#29702;&#35770;&#22312;'&#23494;&#38598;&#26497;&#38480;' $N \gg c \gg 1$ &#21644; $M \gg 1$ &#19988;&#22266;&#23450;$\alpha=M/c$&#26102;&#21464;&#24471;&#31934;&#30830;&#12290;&#29702;&#35770;&#21644;&#27169;&#25311;&#37117;&#34920;&#26126;&#65292;DNN&#30340;&#23398;&#20064;&#22312;&#32593;&#32476;&#31354;&#38388;&#20013;&#30456;&#24403;&#24322;&#36136;&#65306;&#26426;&#22120;&#30340;&#37197;&#32622;&#22312;&#38752;&#36817;&#36755;&#20837;/&#36755;&#20986;&#30340;&#23618;&#20869;&#26356;&#21152;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the spectacular successes, deep neural networks (DNN) with a huge number of adjustable parameters remain largely black boxes. To shed light on the hidden layers of DNN, we study supervised learning by a DNN of width $N$ and depth $L$ consisting of perceptrons with $c$ inputs by a statistical mechanics approach called the teacher-student setting. We consider an ensemble of student machines that exactly reproduce $M$ sets of $N$ dimensional input/output relations provided by a teacher machine. We analyze the ensemble theoretically using a replica method (H. Yoshino (2020)) and numerically performing greedy Monte Carlo simulations. The replica theory which works on high dimensional data $N \gg 1$ becomes exact in 'dense limit' $N \gg c \gg 1$ and $M \gg 1$ with fixed $\alpha=M/c$. Both the theory and the simulation suggest learning by the DNN is quite heterogeneous in the network space: configurations of the machines are more correlated within the layers closer to the input/output
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#20381;&#36182;&#30340;&#20998;&#24418;&#32500;&#24230;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#19981;&#38656;&#35201;Lipschitz&#20551;&#35774;&#65292;&#24182;&#33021;&#25511;&#21046;&#27867;&#21270;&#35823;&#24046;&#21644;&#20114;&#20449;&#24687;&#39033;&#12290;</title><link>http://arxiv.org/abs/2302.02766</link><description>&lt;p&gt;
&#25968;&#25454;&#20381;&#36182;&#20998;&#24418;&#32500;&#24230;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Generalization Bounds with Data-dependent Fractal Dimensions. (arXiv:2302.02766v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02766
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#20381;&#36182;&#30340;&#20998;&#24418;&#32500;&#24230;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#19981;&#38656;&#35201;Lipschitz&#20551;&#35774;&#65292;&#24182;&#33021;&#25511;&#21046;&#27867;&#21270;&#35823;&#24046;&#21644;&#20114;&#20449;&#24687;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#23398;&#20064;&#20013;&#65292;&#20026;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#27867;&#21270;&#20445;&#35777;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#20998;&#24418;&#20960;&#20309;&#30340;&#24037;&#20855;&#26469;&#20998;&#26512;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#23613;&#31649;&#36825;&#20123;&#24037;&#20316;&#25104;&#21151;&#22320;&#24341;&#20837;&#20102;&#26032;&#30340;&#25968;&#23398;&#24037;&#20855;&#26469;&#29702;&#35299;&#27867;&#21270;&#65292;&#20294;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#65292;&#32780;&#36825;&#19968;&#20551;&#35774;&#36890;&#24120;&#19981;&#36866;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#21487;&#33021;&#20351;&#30028;&#38480;&#21464;&#24471;&#26080;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#19981;&#38656;&#35201;&#20219;&#20309;Lipschitz&#20551;&#35774;&#30340;&#22522;&#20110;&#20998;&#24418;&#20960;&#20309;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#23398;&#20064;&#29702;&#35770;&#20013;&#22522;&#20110;&#32463;&#20856;&#30340;&#35206;&#30422;&#35770;&#35777;&#65292;&#24182;&#24341;&#20837;&#20102;&#25968;&#25454;&#20381;&#36182;&#30340;&#20998;&#24418;&#32500;&#24230;&#12290;&#23613;&#31649;&#24341;&#20837;&#20102;&#22823;&#37327;&#30340;&#25216;&#26415;&#22797;&#26434;&#24615;&#65292;&#20294;&#36825;&#20010;&#26032;&#27010;&#24565;&#20351;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;&#27867;&#21270;&#35823;&#24046;&#65288;&#22312;&#22266;&#23450;&#25110;&#38543;&#26426;&#30340;&#20551;&#35774;&#31354;&#38388;&#19978;&#65289;&#20197;&#21450;&#29305;&#23450;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing generalization guarantees for modern neural networks has been a crucial task in statistical learning. Recently, several studies have attempted to analyze the generalization error in such settings by using tools from fractal geometry. While these works have successfully introduced new mathematical tools to apprehend generalization, they heavily rely on a Lipschitz continuity assumption, which in general does not hold for neural networks and might make the bounds vacuous. In this work, we address this issue and prove fractal geometry-based generalization bounds without requiring any Lipschitz assumption. To achieve this goal, we build up on a classical covering argument in learning theory and introduce a data-dependent fractal dimension. Despite introducing a significant amount of technical complications, this new notion lets us control the generalization error (over either fixed or random hypothesis spaces) along with certain mutual information (MI) terms. To provide a clearer
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#26102;&#24577;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#25552;&#20379;&#20102;&#23398;&#20064;&#35774;&#32622;&#21644;&#20219;&#21153;&#30340;&#20005;&#26684;&#35268;&#33539;&#21270;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#26368;&#30456;&#20851;&#30340;&#24320;&#25918;&#25361;&#25112;&#65292;&#20174;&#30740;&#31350;&#21644;&#24212;&#29992;&#35282;&#24230;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2302.01018</link><description>&lt;p&gt;
&#26102;&#24577;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities. (arXiv:2302.01018v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#26102;&#24577;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#25552;&#20379;&#20102;&#23398;&#20064;&#35774;&#32622;&#21644;&#20219;&#21153;&#30340;&#20005;&#26684;&#35268;&#33539;&#21270;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#26368;&#30456;&#20851;&#30340;&#24320;&#25918;&#25361;&#25112;&#65292;&#20174;&#30740;&#31350;&#21644;&#24212;&#29992;&#35282;&#24230;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#65288;&#38745;&#24577;&#65289;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20027;&#35201;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#26159;&#21160;&#24577;&#30340;&#65292;&#22240;&#20026;&#22270;&#21644;&#33410;&#28857;/&#36793;&#23646;&#24615;&#38543;&#30528;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;GNN&#30340;&#26102;&#24577;&#22270;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#25193;&#23637;GNN&#33021;&#21147;&#30340;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20851;&#20110;&#26102;&#24577;GNN&#30340;&#29616;&#29366;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#24341;&#20837;&#20102;&#23398;&#20064;&#35774;&#32622;&#21644;&#20219;&#21153;&#30340;&#20005;&#26684;&#35268;&#33539;&#21270;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#20197;&#34920;&#31034;&#21644;&#22788;&#29702;&#26102;&#24577;&#26041;&#38754;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#30740;&#31350;&#21644;&#24212;&#29992;&#35282;&#24230;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#26368;&#30456;&#20851;&#30340;&#24320;&#25918;&#25361;&#25112;&#65292;&#32467;&#26463;&#20102;&#36825;&#39033;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.
&lt;/p&gt;</description></item><item><title /><link>http://arxiv.org/abs/2302.00160</link><description>&lt;p&gt;
&#20174;&#19968;&#20010;&#25968;&#25454;&#31354;&#38388;&#21040;&#21478;&#19968;&#20010;&#25968;&#25454;&#31354;&#38388;&#30340;&#26412;&#22320;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Local transfer learning from one data space to another. (arXiv:2302.00160v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00160
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#22312;&#20174;&#25903;&#25345;&#22312;&#39640;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#20302;&#32500;&#23376;&#27969;&#24418;&#19978;&#38543;&#26426;&#36873;&#25321;&#30340;&#25968;&#25454;&#19978;&#36817;&#20284;&#19968;&#20010;&#20989;&#25968;&#20851;&#31995;&#12290;&#27969;&#24418;&#26412;&#36136;&#19978;&#30001;&#25968;&#25454;&#38598;&#26412;&#36523;&#23450;&#20041;&#65292;&#24182;&#19988;&#36890;&#24120;&#35774;&#35745;&#20026;&#25968;&#25454;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#22312;&#27969;&#24418;&#19978;&#31264;&#23494;&#12290;&#25968;&#25454;&#31354;&#38388;&#30340;&#27010;&#24565;&#26159;&#19968;&#20010;&#25277;&#35937;&#30340;&#27969;&#24418;&#65292;&#23553;&#35013;&#20102;&#20801;&#35768;&#36827;&#34892;&#20989;&#25968;&#36924;&#36817;&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#36801;&#31227;&#23398;&#20064;&#65288;&#20803;&#23398;&#20064;&#65289;&#38382;&#39064;&#26159;&#21033;&#29992;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#19968;&#20010;&#20989;&#25968;&#26469;&#23398;&#20064;&#22312;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#31867;&#20284;&#20989;&#25968;&#12290;&#22312;&#20989;&#25968;&#36924;&#36817;&#26041;&#38754;&#65292;&#36825;&#24847;&#21619;&#30528;&#23558;&#19968;&#20010;&#25968;&#25454;&#31354;&#38388;&#19978;&#30340;&#20989;&#25968;&#65288;&#22522;&#26412;&#25968;&#25454;&#31354;&#38388;&#65289;&#25552;&#21319;&#21040;&#21478;&#19968;&#20010;&#25968;&#25454;&#31354;&#38388;&#65288;&#30446;&#26631;&#25968;&#25454;&#31354;&#38388;&#65289;&#12290;&#36825;&#20010;&#35266;&#28857;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#24212;&#29992;&#25968;&#23398;&#20013;&#30340;&#19968;&#20123;&#36870;&#38382;&#39064;&#65288;&#22914;&#36870;Radon&#21464;&#25442;&#65289;&#19982;&#36801;&#31227;&#23398;&#20064;&#32852;&#31995;&#36215;&#26469;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#31181;&#25552;&#21319;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental problem in manifold learning is to approximate a functional relationship in a data chosen randomly from a probability distribution supported on a low dimensional sub-manifold of a high dimensional ambient Euclidean space. The manifold is essentially defined by the data set itself and, typically, designed so that the data is dense on the manifold in some sense. The notion of a data space is an abstraction of a manifold encapsulating the essential properties that allow for function approximation. The problem of transfer learning (meta-learning) is to use the learning of a function on one data set to learn a similar function on a new data set. In terms of function approximation, this means lifting a function on one data space (the base data space) to another (the target data space). This viewpoint enables us to connect some inverse problems in applied mathematics (such as inverse Radon transform) with transfer learning. In this paper we examine the question of such lifting w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffPreT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;-&#32467;&#26500;&#32852;&#21512;&#25193;&#25955;&#24314;&#27169;&#26469;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#32534;&#30721;&#22120;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;SiamDiff&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;DiffPreT&#65292;&#20197;&#25429;&#25417;&#34507;&#30333;&#36136;&#30340;&#19981;&#21516;&#26500;&#35937;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12068</link><description>&lt;p&gt;
&#36890;&#36807;&#23402;&#29983;&#24207;&#21015;-&#32467;&#26500;&#25193;&#25955;&#36712;&#36857;&#39044;&#27979;&#36827;&#34892;&#34507;&#30333;&#36136;&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pre-Training Protein Encoder via Siamese Sequence-Structure Diffusion Trajectory Prediction. (arXiv:2301.12068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffPreT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;-&#32467;&#26500;&#32852;&#21512;&#25193;&#25955;&#24314;&#27169;&#26469;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#32534;&#30721;&#22120;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;SiamDiff&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;DiffPreT&#65292;&#20197;&#25429;&#25417;&#34507;&#30333;&#36136;&#30340;&#19981;&#21516;&#26500;&#35937;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#34507;&#30333;&#36136;&#39046;&#22495;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#35201;&#20040;&#38598;&#20013;&#20110;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#35201;&#20040;&#38598;&#20013;&#20110;&#34507;&#30333;&#36136;&#32467;&#26500;&#65292;&#24573;&#35270;&#20102;&#23427;&#20204;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#25506;&#32034;&#65292;&#32780;&#32852;&#21512;&#20998;&#24067;&#23545;&#20110;&#20840;&#38754;&#20102;&#35299;&#34507;&#30333;&#36136;&#21151;&#33021;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#25972;&#21512;&#20849;&#21516;&#28436;&#21270;&#20449;&#24687;&#21644;&#32467;&#26500;&#29305;&#24615;&#12290;&#26412;&#30740;&#31350;&#21463;&#21040;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;DiffPreT&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;-&#32467;&#26500;&#32852;&#21512;&#25193;&#25955;&#24314;&#27169;&#26469;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#32534;&#30721;&#22120;&#12290;DiffPreT&#24341;&#23548;&#32534;&#30721;&#22120;&#20174;&#25200;&#21160;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#20013;&#24674;&#22797;&#20986;&#21407;&#22987;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#32771;&#34385;&#21040;&#34507;&#30333;&#36136;&#26500;&#35937;&#30340;&#37325;&#35201;&#21464;&#21270;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;&#23402;&#29983;&#25193;&#25955;&#36712;&#36857;&#39044;&#27979;&#65288;SiamDiff&#65289;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;DiffPreT&#65292;&#20197;&#25429;&#25417;&#34507;&#30333;&#36136;&#30340;&#19981;&#21516;&#26500;&#35937;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised pre-training methods on proteins have recently gained attention, with most approaches focusing on either protein sequences or structures, neglecting the exploration of their joint distribution, which is crucial for a comprehensive understanding of protein functions by integrating co-evolutionary information and structural characteristics. In this work, inspired by the success of denoising diffusion models in generative tasks, we propose the DiffPreT approach to pre-train a protein encoder by sequence-structure joint diffusion modeling. DiffPreT guides the encoder to recover the native protein sequences and structures from the perturbed ones along the joint diffusion trajectory, which acquires the joint distribution of sequences and structures. Considering the essential protein conformational variations, we enhance DiffPreT by a method called Siamese Diffusion Trajectory Prediction (SiamDiff) to capture the correlation between different conformers of a protein. SiamDiff 
&lt;/p&gt;</description></item><item><title>D-Adaptation&#26159;&#19968;&#31181;&#21487;&#20197;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#29992;&#20110;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#32780;&#26080;&#38656;&#36229;&#21442;&#25968;&#65292;&#20063;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.07733</link><description>&lt;p&gt;
&#36890;&#36807;D&#36866;&#24212;&#23454;&#29616;&#23398;&#20064;&#29575;&#33258;&#30001;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning-Rate-Free Learning by D-Adaptation. (arXiv:2301.07733v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07733
&lt;/p&gt;
&lt;p&gt;
D-Adaptation&#26159;&#19968;&#31181;&#21487;&#20197;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#29992;&#20110;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#32780;&#26080;&#38656;&#36229;&#21442;&#25968;&#65292;&#20063;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
D&#36866;&#24212;&#26159;&#19968;&#31181;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#28176;&#36817;&#22320;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#26080;&#38656;&#22238;&#28335;&#25110;&#32447;&#24615;&#25628;&#32034;&#65292;&#24182;&#19988;&#27599;&#27493;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#20989;&#25968;&#20540;&#25110;&#26799;&#24230;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36825;&#19968;&#31867;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#26080;&#36229;&#21442;&#25968;&#19988;&#25910;&#25947;&#36895;&#29575;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#38024;&#23545;SGD&#21644;Adam&#21464;&#20307;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20854;&#20013;&#35813;&#26041;&#27861;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#65292;&#22312;&#21313;&#22810;&#20010;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#24212;&#29992;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#38382;&#39064;&#12290;&#24320;&#28304;&#23454;&#29616;&#22312; \url{https://github.com/facebookresearch/dadaptation}.
&lt;/p&gt;
&lt;p&gt;
D-Adaptation is an approach to automatically setting the learning rate which asymptotically achieves the optimal rate of convergence for minimizing convex Lipschitz functions, with no back-tracking or line searches, and no additional function value or gradient evaluations per step. Our approach is the first hyper-parameter free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems.  An open-source implementation is available at \url{https://github.com/facebookresearch/dadaptation}.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#20351;&#29992;&#20102;&#30495;&#23454;&#21518;&#39564;&#65292;&#36825;&#31181;&#26041;&#27861;&#20173;&#26080;&#27861;&#38450;&#27490;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#21516;&#26102;&#65292;&#27169;&#22411;&#35823;&#24046;&#21644;&#20219;&#21153;&#25968;&#25454;&#19981;&#24179;&#34913;&#20063;&#20250;&#23548;&#33268;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2301.01828</link><description>&lt;p&gt;
&#20851;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
On Sequential Bayesian Inference for Continual Learning. (arXiv:2301.01828v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01828
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#20351;&#29992;&#20102;&#30495;&#23454;&#21518;&#39564;&#65292;&#36825;&#31181;&#26041;&#27861;&#20173;&#26080;&#27861;&#38450;&#27490;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#21516;&#26102;&#65292;&#27169;&#22411;&#35823;&#24046;&#21644;&#20219;&#21153;&#25968;&#25454;&#19981;&#24179;&#34913;&#20063;&#20250;&#23548;&#33268;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#21487;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#65292;&#20197;&#38450;&#27490;&#36807;&#21435;&#20219;&#21153;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#24182;&#27979;&#35797;&#26159;&#21542;&#26377;&#35775;&#38382;&#30495;&#23454;&#21518;&#39564;&#30340;&#20445;&#35777;&#21487;&#20197;&#38450;&#27490;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#27931;&#25191;&#34892;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#27931;&#26679;&#26412;&#25311;&#21512;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#23558;&#21518;&#39564;&#20256;&#25773;&#20026;&#26032;&#20219;&#21153;&#30340;&#20808;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#35777;&#26126;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#25191;&#34892;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#22256;&#38590;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#36830;&#32493;&#23398;&#20064;&#30340;&#31616;&#21333;&#20998;&#26512;&#31034;&#20363;&#65292;&#24182;&#24378;&#35843;&#20102;&#27169;&#22411;&#38169;&#35823;&#35828;&#26126;&#38382;&#39064;&#65292;&#21363;&#20351;&#36827;&#34892;&#20102;&#20934;&#30830;&#30340;&#25512;&#26029;&#65292;&#20063;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#30340;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20219;&#21153;&#25968;&#25454;&#19981;&#24179;&#34913;&#21487;&#33021;&#23548;&#33268;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential Bayesian inference can be used for continual learning to prevent catastrophic forgetting of past tasks and provide an informative prior when learning new tasks. We revisit sequential Bayesian inference and test whether having access to the true posterior is guaranteed to prevent catastrophic forgetting in Bayesian neural networks. To do this we perform sequential Bayesian inference using Hamiltonian Monte Carlo. We propagate the posterior as a prior for new tasks by fitting a density estimator on Hamiltonian Monte Carlo samples. We find that this approach fails to prevent catastrophic forgetting demonstrating the difficulty in performing sequential Bayesian inference in neural networks. From there we study simple analytical examples of sequential Bayesian inference and CL and highlight the issue of model misspecification which can lead to sub-optimal continual learning performance despite exact inference. Furthermore, we discuss how task data imbalances can cause forgetting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#35299;&#37322;&#21402;&#24230;&#21644;&#31283;&#23450;&#39030;&#37096;&#26174;&#33879;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102;&#35299;&#37322;&#30340;&#25968;&#20540;&#21644;&#32479;&#35745;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#21644;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.14106</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Provable Robust Saliency-based Explanations. (arXiv:2212.14106v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#35299;&#37322;&#21402;&#24230;&#21644;&#31283;&#23450;&#39030;&#37096;&#26174;&#33879;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102;&#35299;&#37322;&#30340;&#25968;&#20540;&#21644;&#32479;&#35745;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#21644;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#35299;&#37322;&#23545;&#20110;&#24314;&#31435;&#20154;&#31867;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#20351;&#29992;&#39030;&#37096;-k&#30340;&#20132;&#38598;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#40065;&#26834;&#24615;&#26159;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#37117;&#22522;&#20110;$\ell_p$&#33539;&#25968;&#65292;&#20174;&#32780;&#22312;&#35780;&#20272;&#21644;&#20248;&#21270;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#35299;&#37322;&#30340;&#21402;&#24230;&#26469;&#34913;&#37327;&#39030;&#37096;-k&#26174;&#33879;&#29305;&#24449;&#25490;&#21517;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#21487;&#34892;&#30340;&#26367;&#20195;&#30446;&#26631;&#30340;R2ET&#31639;&#27861;&#65292;&#20197;&#39640;&#25928;&#22320;&#26368;&#22823;&#21270;&#21402;&#24230;&#24182;&#31283;&#23450;&#39030;&#37096;&#26174;&#33879;&#29305;&#24449;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;R2ET&#21644;&#23545;&#25239;&#35757;&#32451;&#20043;&#38388;&#30340;&#32852;&#31995;&#65307;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#20844;&#24335;&#21644;&#27867;&#21270;&#35823;&#24046;&#30028;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#26367;&#20195;&#30446;&#26631;&#21487;&#20197;&#25913;&#36827;&#35299;&#37322;&#30340;&#25968;&#20540;&#21644;&#32479;&#35745;&#31283;&#23450;&#24615;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#21644;&#25968;&#25454;&#27169;&#24577;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;R2ET&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust explanations of machine learning models are critical to establishing human trust in the models. The top-$k$ intersection is widely used to evaluate the robustness of explanations. However, most existing attacking and defense strategies are based on $\ell_p$ norms, thus creating a mismatch between the evaluation and optimization objectives. To this end, we define explanation thickness for measuring top-$k$ salient features ranking stability, and design the \textit{R2ET} algorithm based on a novel tractable surrogate to maximize the thickness and stabilize the top salient features efficiently. Theoretically, we prove a connection between R2ET and adversarial training; using a novel multi-objective optimization formulation and a generalization error bound, we further prove that the surrogate objective can improve both the numerical and statistical stability of the explanations. Experiments with a wide spectrum of network architectures and data modalities demonstrate that R2ET attai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#19978;&#19982;MEV&#30456;&#20851;&#30340;&#35805;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;</title><link>http://arxiv.org/abs/2212.06951</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#19978;&#30340;AI&#20262;&#29702;: &#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#21306;&#22359;&#38142;&#23433;&#20840;&#20027;&#39064;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security. (arXiv:2212.06951v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#19978;&#19982;MEV&#30456;&#20851;&#30340;&#35805;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#20351;&#29992;&#20998;&#24067;&#24335;&#32593;&#32476;&#35753;&#35745;&#31639;&#26426;&#31995;&#32479;&#26356;&#23433;&#20840;&#65292;&#20294;&#24403;&#21069;&#30340;&#21306;&#22359;&#38142;&#35774;&#35745;&#22312;&#20132;&#26131;&#39034;&#24207;&#26041;&#38754;&#23384;&#22312;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#30719;&#24037;&#21487;&#20197;&#37325;&#26032;&#25490;&#24207;&#20132;&#26131;&#20197;&#29983;&#25104;&#21033;&#28070;&#65292;&#36825;&#34987;&#31216;&#20026;&#30719;&#24037;&#21487;&#25552;&#21462;&#20215;&#20540;&#65288;MEV&#65289;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#35748;&#20026;MEV&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#30693;&#21517;&#30340;Flashbots&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#22823;&#22810;&#20998;&#26512;&#20102;&#21306;&#22359;&#38142;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#21040;MEV&#22312;&#26356;&#24191;&#27867;&#30340;AI&#31038;&#20250;&#20013;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#20840;&#38754;&#20998;&#26512;&#20102;MEV&#25512;&#25991;&#20013;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#36229;&#36807;20,000&#20010;MEV&#21644;Flashbots&#26631;&#31614;&#30340;&#25512;&#25991;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#21306;&#22359;&#38142;&#19978;MEV&#27963;&#21160;&#30340;&#20849;&#21516;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchain has empowered computer systems to be more secure using a distributed network. However, the current blockchain design suffers from fairness issues in transaction ordering. Miners are able to reorder transactions to generate profits, the so-called miner extractable value (MEV). Existing research recognizes MEV as a severe security issue and proposes potential solutions, including prominent Flashbots. However, previous studies have mostly analyzed blockchain data, which might not capture the impacts of MEV in a much broader AI society. Thus, in this research, we applied natural language processing (NLP) methods to comprehensively analyze topics in tweets on MEV. We collected more than 20000 tweets with MEV and Flashbots hashtags and analyzed their topics. Our results show that the tweets discussed profound topics of ethical concern, including security, equity, emotional sentiments, and the desire for solutions to MEV. We also identify the co-movements of MEV activities on block
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#24314;&#27169;&#20026;&#22238;&#25253;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#36991;&#20813;&#21160;&#24577;&#35268;&#21010;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#32422;&#26463;&#21644;&#25216;&#33021;&#20316;&#20026;&#26465;&#20214;&#21464;&#37327;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.15657</link><description>&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#26159;&#21542;&#36275;&#20197;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Conditional Generative Modeling all you need for Decision-Making?. (arXiv:2211.15657v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15657
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#24314;&#27169;&#20026;&#22238;&#25253;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#36991;&#20813;&#21160;&#24577;&#35268;&#21010;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#32422;&#26463;&#21644;&#25216;&#33021;&#20316;&#20026;&#26465;&#20214;&#21464;&#37327;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#30340;&#25913;&#36827;&#20351;&#24471;&#20165;&#20973;&#35821;&#35328;&#25551;&#36848;&#23601;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#25506;&#35752;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#21487;&#20197;&#30452;&#25509;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#30340;&#35270;&#35282;&#26469;&#30475;&#24453;&#20915;&#31574;&#65292;&#32780;&#38750;&#24378;&#21270;&#23398;&#20064;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#21487;&#20197;&#20248;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#31574;&#30053;&#24314;&#27169;&#20026;&#22238;&#25253;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36991;&#20813;&#21160;&#24577;&#35268;&#21010;&#30340;&#38656;&#35201;&#65292;&#36827;&#32780;&#28040;&#38500;&#20256;&#32479;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25152;&#24102;&#26469;&#30340;&#35768;&#22810;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#32771;&#34385;&#20004;&#20010;&#20854;&#20182;&#30340;&#26465;&#20214;&#21464;&#37327;&#65306;&#32422;&#26463;&#21644;&#25216;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#23558;&#31574;&#30053;&#24314;&#27169;&#20026;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20165;&#23545;&#21333;&#20010;&#32422;&#26463;&#25110;&#25216;&#33021;&#36827;&#34892;&#26465;&#20214;&#35774;&#32622;&#65292;&#23548;&#33268;&#27979;&#35797;&#26102;&#30340;&#34892;&#20026;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#40723;&#21169;&#30446;&#26631;&#22495;&#20013;&#30340;&#39044;&#27979;&#19982;&#20854;&#21069;&#20960;&#20010;&#22855;&#24322;&#21521;&#37327;&#23545;&#40784;&#26469;&#23454;&#29616;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20010;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#20998;&#31867;&#22120;&#19982;&#26080;&#30417;&#30563;&#30446;&#26631;&#25968;&#25454;&#23545;&#40784;&#65292;&#32780;&#19981;&#26159;&#27491;&#21017;&#21270;&#34920;&#31034;&#12290;&#36890;&#36807;&#28040;&#38500;&#23545;&#26368;&#20248;&#32852;&#21512;&#39118;&#38505;&#20551;&#35774;&#30340;&#20381;&#36182;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.14960</link><description>&lt;p&gt;
&#20998;&#24067;&#20559;&#31227;&#30340;&#26631;&#31614;&#23545;&#40784;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Label Alignment Regularization for Distribution Shift. (arXiv:2211.14960v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14960
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#40723;&#21169;&#30446;&#26631;&#22495;&#20013;&#30340;&#39044;&#27979;&#19982;&#20854;&#21069;&#20960;&#20010;&#22855;&#24322;&#21521;&#37327;&#23545;&#40784;&#26469;&#23454;&#29616;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20010;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#20998;&#31867;&#22120;&#19982;&#26080;&#30417;&#30563;&#30446;&#26631;&#25968;&#25454;&#23545;&#40784;&#65292;&#32780;&#19981;&#26159;&#27491;&#21017;&#21270;&#34920;&#31034;&#12290;&#36890;&#36807;&#28040;&#38500;&#23545;&#26368;&#20248;&#32852;&#21512;&#39118;&#38505;&#20551;&#35774;&#30340;&#20381;&#36182;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#23545;&#40784;&#23646;&#24615;&#65288;LAP&#65289;&#65292;&#21363;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#26631;&#31614;&#30340;&#21521;&#37327;&#22823;&#37096;&#20998;&#22312;&#25968;&#25454;&#30697;&#38453;&#30340;&#21069;&#20960;&#20010;&#22855;&#24322;&#21521;&#37327;&#30340;&#24352;&#25104;&#31354;&#38388;&#20869;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#40723;&#21169;&#30446;&#26631;&#22495;&#20013;&#30340;&#39044;&#27979;&#19982;&#20854;&#21069;&#20960;&#20010;&#22855;&#24322;&#21521;&#37327;&#23545;&#40784;&#12290;&#19982;&#20256;&#32479;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#19987;&#27880;&#20110;&#27491;&#21017;&#21270;&#34920;&#31034;&#19981;&#21516;&#65292;&#25105;&#20204;&#30456;&#21453;&#65292;&#36890;&#36807;&#22312;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#20351;&#29992;LAP&#65292;&#29992;&#27491;&#21017;&#21270;&#20998;&#31867;&#22120;&#19982;&#26080;&#30417;&#30563;&#30446;&#26631;&#25968;&#25454;&#23545;&#40784;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20301;&#20110;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#21069;&#20960;&#20010;&#21491;&#22855;&#24322;&#21521;&#37327;&#30340;&#24352;&#25104;&#31354;&#38388;&#20869;&#65292;&#24182;&#19982;&#26368;&#20248;&#35299;&#23545;&#40784;&#12290;&#36890;&#36807;&#28040;&#38500;&#32463;&#20856;&#39046;&#22495;&#36866;&#24212;&#29702;&#35770;&#20013;&#24120;&#35265;&#30340;&#26368;&#20248;&#32852;&#21512;&#39118;&#38505;&#20551;&#35774;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has highlighted the label alignment property (LAP) in supervised learning, where the vector of all labels in the dataset is mostly in the span of the top few singular vectors of the data matrix. Drawing inspiration from this observation, we propose a regularization method for unsupervised domain adaptation that encourages alignment between the predictions in the target domain and its top singular vectors. Unlike conventional domain adaptation approaches that focus on regularizing representations, we instead regularize the classifier to align with the unsupervised target data, guided by the LAP in both the source and target domains. Theoretical analysis demonstrates that, under certain assumptions, our solution resides within the span of the top right singular vectors of the target domain data and aligns with the optimal solution. By removing the reliance on the commonly used optimal joint risk assumption found in classic domain adaptation theory, we showcase the effectivene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#21644;&#27979;&#35797;&#28508;&#22312;&#26641;&#29366;Ising&#27169;&#22411;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#24037;&#20316;&#30340;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#23637;&#31034;&#26641;&#29366;Ising&#27169;&#22411;&#21494;&#33410;&#28857;&#20998;&#24067;&#30340;&#26032;&#39062;&#23616;&#37096;&#21270;&#32467;&#26524;&#26469;&#23454;&#29616;&#36825;&#20123;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.13291</link><description>&lt;p&gt;
&#39640;&#25928;&#23398;&#20064;&#21644;&#27979;&#35797;&#28508;&#22312;&#26641;&#29366;Ising&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning and Testing Latent-Tree Ising Models Efficiently. (arXiv:2211.13291v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#21644;&#27979;&#35797;&#28508;&#22312;&#26641;&#29366;Ising&#27169;&#22411;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#24037;&#20316;&#30340;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#23637;&#31034;&#26641;&#29366;Ising&#27169;&#22411;&#21494;&#33410;&#28857;&#20998;&#24067;&#30340;&#26032;&#39062;&#23616;&#37096;&#21270;&#32467;&#26524;&#26469;&#23454;&#29616;&#36825;&#20123;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#23398;&#20064;&#21644;&#27979;&#35797;&#28508;&#22312;&#26641;&#29366;Ising&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21363;&#21482;&#33021;&#22312;&#20854;&#21494;&#33410;&#28857;&#22788;&#35266;&#27979;&#21040;&#30340;Ising&#27169;&#22411;&#12290;&#22312;&#23398;&#20064;&#26041;&#38754;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#20010;&#21494;&#33410;&#28857;&#20998;&#24067;&#22312;&#24635;&#21464;&#24322;&#36317;&#31163;&#19978;&#19982;&#20043;&#25509;&#36817;&#30340;&#26641;&#29366;Ising&#27169;&#22411;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#24037;&#20316;&#30340;&#32467;&#26524;&#12290;&#22312;&#27979;&#35797;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#26469;&#27979;&#35797;&#20004;&#20010;&#28508;&#22312;&#26641;&#29366;Ising&#27169;&#22411;&#30340;&#21494;&#33410;&#28857;&#20998;&#24067;&#26159;&#21542;&#22312;&#24635;&#21464;&#24322;&#36317;&#31163;&#19978;&#25509;&#36817;&#25110;&#36828;&#31163;&#12290;&#36890;&#36807;&#23637;&#31034;&#26641;&#29366;Ising&#27169;&#22411;&#21494;&#33410;&#28857;&#20998;&#24067;&#30340;&#24635;&#21464;&#24322;&#36317;&#31163;&#30340;&#26032;&#39062;&#23616;&#37096;&#21270;&#32467;&#26524;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide time- and sample-efficient algorithms for learning and testing latent-tree Ising models, i.e. Ising models that may only be observed at their leaf nodes. On the learning side, we obtain efficient algorithms for learning a tree-structured Ising model whose leaf node distribution is close in Total Variation Distance, improving on the results of prior work. On the testing side, we provide an efficient algorithm with fewer samples for testing whether two latent-tree Ising models have leaf-node distributions that are close or far in Total Variation distance. We obtain our algorithms by showing novel localization results for the total variation distance between the leaf-node distributions of tree-structured Ising models, in terms of their marginals on pairs of leaves.
&lt;/p&gt;</description></item><item><title>Transformers&#30456;&#23545;&#20110;&#24490;&#29615;&#27169;&#22411;&#26356;&#20559;&#21521;&#20110;&#23398;&#20064;&#20855;&#26377;&#20302;&#25935;&#24863;&#24230;&#30340;&#20989;&#25968;&#65292;&#23588;&#20854;&#22312;&#31232;&#30095;&#24067;&#23572;&#20989;&#25968;&#19978;&#65292;Transformers&#33021;&#22815;&#23454;&#29616;&#36817;&#20046;&#23436;&#32654;&#30340;&#27867;&#21270;&#65292;&#32780;LSTMs&#21017;&#34920;&#29616;&#20986;&#36807;&#25311;&#21512;&#21644;&#36739;&#20302;&#30340;&#27867;&#21270;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.12316</link><description>&lt;p&gt;
Transformers&#20013;&#30340;&#31616;&#27905;&#20559;&#35265;&#21450;&#20854;&#23398;&#20064;&#31232;&#30095;&#24067;&#23572;&#20989;&#25968;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions. (arXiv:2211.12316v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12316
&lt;/p&gt;
&lt;p&gt;
Transformers&#30456;&#23545;&#20110;&#24490;&#29615;&#27169;&#22411;&#26356;&#20559;&#21521;&#20110;&#23398;&#20064;&#20855;&#26377;&#20302;&#25935;&#24863;&#24230;&#30340;&#20989;&#25968;&#65292;&#23588;&#20854;&#22312;&#31232;&#30095;&#24067;&#23572;&#20989;&#25968;&#19978;&#65292;Transformers&#33021;&#22815;&#23454;&#29616;&#36817;&#20046;&#23436;&#32654;&#30340;&#27867;&#21270;&#65292;&#32780;LSTMs&#21017;&#34920;&#29616;&#20986;&#36807;&#25311;&#21512;&#21644;&#36739;&#20302;&#30340;&#27867;&#21270;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformers&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#24490;&#29615;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20204;&#22312;&#24314;&#27169;&#20960;&#31181;&#24418;&#24335;&#35821;&#35328;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#20026;&#20160;&#20040;Transformers&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#23427;&#20204;&#26159;&#21542;&#20855;&#26377;&#20219;&#20309;&#33021;&#20351;&#23427;&#20204;&#27604;&#24490;&#29615;&#27169;&#22411;&#26356;&#22909;&#22320;&#27867;&#21270;&#30340;&#23646;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#24067;&#23572;&#20989;&#25968;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#35777;&#26126;&#20197;&#19979;&#20869;&#23481;&#65306;(i) &#38543;&#26426;Transformers&#30456;&#23545;&#26356;&#20559;&#21521;&#20110;&#20855;&#26377;&#20302;&#25935;&#24863;&#24230;&#30340;&#20989;&#25968;&#12290;(ii) &#24403;&#35757;&#32451;&#24067;&#23572;&#20989;&#25968;&#26102;&#65292;Transformers&#21644;LSTMs&#37117;&#20248;&#20808;&#23398;&#20064;&#20855;&#26377;&#20302;&#25935;&#24863;&#24230;&#30340;&#20989;&#25968;&#65292;&#26368;&#32456;Transformers&#25910;&#25947;&#21040;&#20855;&#26377;&#26356;&#20302;&#25935;&#24863;&#24230;&#30340;&#20989;&#25968;&#12290;(iii) &#22312;&#20855;&#26377;&#20302;&#25935;&#24863;&#24230;&#30340;&#31232;&#30095;&#24067;&#23572;&#20989;&#25968;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;Transformers&#22312;&#23384;&#22312;&#22122;&#38899;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#36817;&#20046;&#23436;&#32654;&#22320;&#27867;&#21270;&#65292;&#32780;LSTMs&#36807;&#25311;&#21512;&#24182;&#19988;&#27867;&#21270;&#31934;&#24230;&#36739;&#20302;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#21487;&#37327;&#21270;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the widespread success of Transformers on NLP tasks, recent works have found that they struggle to model several formal languages when compared to recurrent models. This raises the question of why Transformers perform well in practice and whether they have any properties that enable them to generalize better than recurrent models. In this work, we conduct an extensive empirical study on Boolean functions to demonstrate the following: (i) Random Transformers are relatively more biased towards functions of low sensitivity. (ii) When trained on Boolean functions, both Transformers and LSTMs prioritize learning functions of low sensitivity, with Transformers ultimately converging to functions of lower sensitivity. (iii) On sparse Boolean functions which have low sensitivity, we find that Transformers generalize near perfectly even in the presence of noisy labels whereas LSTMs overfit and achieve poor generalization accuracy. Overall, our results provide strong quantifiable evidence
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36136;&#30097;&#20102;&#36882;&#24402;&#21010;&#20998;&#22312;&#20915;&#31574;&#26641;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35777;&#26126;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#19968;&#33268;&#33539;&#25968;&#30340;&#22810;&#39033;&#24335;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38543;&#26426;&#26862;&#26519;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#20302;&#24615;&#33021;&#30340;&#26641;&#36716;&#21270;&#20026;&#20960;&#20046;&#26368;&#20248;&#30340;&#36807;&#31243;&#65292;&#20294;&#20195;&#20215;&#26159;&#22833;&#21435;&#20102;&#35299;&#37322;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#39069;&#22806;&#30340;&#35843;&#25972;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2211.10805</link><description>&lt;p&gt;
&#20851;&#20110;&#36882;&#24402;&#21010;&#20998;&#30340;&#36880;&#28857;&#34892;&#20026;&#21450;&#20854;&#23545;&#24322;&#36136;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Pointwise Behavior of Recursive Partitioning and Its Implications for Heterogeneous Causal Effect Estimation. (arXiv:2211.10805v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36136;&#30097;&#20102;&#36882;&#24402;&#21010;&#20998;&#22312;&#20915;&#31574;&#26641;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35777;&#26126;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#19968;&#33268;&#33539;&#25968;&#30340;&#22810;&#39033;&#24335;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38543;&#26426;&#26862;&#26519;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#20302;&#24615;&#33021;&#30340;&#26641;&#36716;&#21270;&#20026;&#20960;&#20046;&#26368;&#20248;&#30340;&#36807;&#31243;&#65292;&#20294;&#20195;&#20215;&#26159;&#22833;&#21435;&#20102;&#35299;&#37322;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#39069;&#22806;&#30340;&#35843;&#25972;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#23398;&#20064;&#22312;&#36880;&#28857;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#12290;&#37325;&#35201;&#30340;&#24212;&#29992;&#21253;&#25324;&#24322;&#36136;&#22240;&#26524;&#27835;&#30103;&#25928;&#24212;&#21644;&#21160;&#24577;&#25919;&#31574;&#20915;&#31574;&#65292;&#20197;&#21450;&#26465;&#20214;&#20998;&#20301;&#25968;&#22238;&#24402;&#21644;&#23454;&#39564;&#35774;&#35745;&#65292;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#26641;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;&#26159;&#22312;&#29305;&#23450;&#30340;&#21327;&#21464;&#37327;&#20540;&#19978;&#36827;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;&#20915;&#31574;&#26641;&#65288;&#36890;&#36807;&#33258;&#36866;&#24212;&#36882;&#24402;&#21010;&#20998;&#35757;&#32451;&#65289;&#36827;&#34892;&#27492;&#31867;&#30446;&#30340;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#36890;&#36807;&#35777;&#26126;&#23427;&#20204;&#29978;&#33267;&#21487;&#20197;&#22312;&#20462;&#21098;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#23454;&#29616;&#19968;&#33268;&#33539;&#25968;&#30340;&#22810;&#39033;&#24335;&#25910;&#25947;&#36895;&#29575;&#12290;&#30456;&#21453;&#65292;&#25910;&#25947;&#36895;&#24230;&#21487;&#33021;&#26159;&#22810;&#39033;&#24335;&#23545;&#25968;&#32423;&#21035;&#30340;&#65292;&#25110;&#32773;&#22312;&#19968;&#20123;&#37325;&#35201;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#35802;&#23454;&#22238;&#24402;&#26641;&#65292;&#23436;&#20840;&#22833;&#36133;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#38543;&#26426;&#26862;&#26519;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#20302;&#24615;&#33021;&#30340;&#26641;&#36716;&#21270;&#20026;&#20960;&#20046;&#26368;&#20248;&#30340;&#36807;&#31243;&#65292;&#20294;&#20195;&#20215;&#26159;&#22833;&#21435;&#20102;&#35299;&#37322;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#39069;&#22806;&#30340;&#35843;&#25972;&#21442;&#25968;&#12290;&#38543;&#26426;&#26862;&#26519;&#30340;&#20004;&#20010;&#26631;&#24535;&#24615;&#29305;&#24449;&#26159;&#23376;&#37319;&#26679;&#21644;&#38543;&#26426;&#29305;&#24449;&#36873;&#25321;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision tree learning is increasingly being used for pointwise inference. Important applications include causal heterogenous treatment effects and dynamic policy decisions, as well as conditional quantile regression and design of experiments, where tree estimation and inference is conducted at specific values of the covariates. In this paper, we call into question the use of decision trees (trained by adaptive recursive partitioning) for such purposes by demonstrating that they can fail to achieve polynomial rates of convergence in uniform norm, even with pruning. Instead, the convergence may be poly-logarithmic or, in some important special cases, such as honest regression trees, fail completely. We show that random forests can remedy the situation, turning poor performing trees into nearly optimal procedures, at the cost of losing interpretability and introducing two additional tuning parameters. The two hallmarks of random forests, subsampling and the random feature selection mecha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AC-OSELM&#30340;&#39640;&#25928;RL&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#20272;&#35745;&#21512;&#36866;&#30340;&#21387;&#32553;&#27604;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#30340;&#39640;&#25928;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#25345;&#37325;&#26500;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.04284</link><description>&lt;p&gt;
&#22522;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#36793;&#32536;&#35745;&#31639;&#39640;&#25928;&#21387;&#32553;&#27604;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Compressed Ratio Estimation using Online Sequential Learning for Edge Computing. (arXiv:2211.04284v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AC-OSELM&#30340;&#39640;&#25928;RL&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#20272;&#35745;&#21512;&#36866;&#30340;&#21387;&#32553;&#27604;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#30340;&#39640;&#25928;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#25345;&#37325;&#26500;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22823;&#37327;&#30340;&#20256;&#24863;&#22120;&#20449;&#24687;&#27491;&#22312;&#23454;&#26102;&#37319;&#38598;&#12290;&#22240;&#27492;&#65292;&#20174;&#36793;&#32536;&#35774;&#22791;&#20256;&#36755;&#25968;&#25454;&#30340;&#36890;&#35759;&#25104;&#26412;&#19981;&#26029;&#22686;&#21152;&#12290;&#21387;&#32553;&#24863;&#30693;&#65288;CS&#65289;&#26159;&#19968;&#31181;&#21487;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#30340;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#65292;&#24182;&#22240;&#20854;&#21487;&#33410;&#30465;&#36890;&#35759;&#25104;&#26412;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#22312;&#21387;&#32553;&#24863;&#30693;&#20013;&#65292;&#20272;&#35745;&#21512;&#36866;&#30340;&#21387;&#32553;&#27604;&#26159;&#37325;&#35201;&#30340;&#12290;&#29616;&#26377;&#30340;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33258;&#36866;&#24212;&#20272;&#35745;&#33719;&#24471;&#25968;&#25454;&#30340;&#21387;&#32553;&#27604;&#30340;&#26041;&#27861;&#65292;&#20854;&#35745;&#31639;&#25104;&#26412;&#24120;&#24120;&#24456;&#39640;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;&#36793;&#32536;&#35774;&#22791;&#30340;&#39640;&#25928;RL&#26041;&#27861;&#65292;&#31216;&#20026;actor-critic&#22312;&#32447;&#36830;&#32493;&#26497;&#38480;&#23398;&#20064;&#26426;&#65288;AC-OSELM&#65289;&#65292;&#24182;&#21033;&#29992;AC-OSELM&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36890;&#36807;&#20272;&#35745;&#36866;&#24403;&#30340;&#21387;&#32553;&#27604;&#21387;&#32553;&#25968;&#25454;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#20351;&#29992;&#23454;&#38469;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#20272;&#35745;&#21387;&#32553;&#27604;&#21644;&#37325;&#26500;&#21387;&#32553;&#25968;&#25454;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#37325;&#26500;&#21387;&#32553;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#27604;&#24378;&#21270;&#23398;&#20064;&#21644;&#20256;&#32479;&#26041;&#27861;&#26356;&#39640;&#25928;&#30340;&#21387;&#32553;&#27604;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to the widespread adoption of the Internet of Things, a vast amount of sensor information is being acquired in real time. Accordingly, the communication cost of data from edge devices is increasing. Compressed sensing (CS), a data compression method that can be used on edge devices, has been attracting attention as a method to reduce communication costs. In CS, estimating the appropriate compression ratio is important. There is a method to adaptively estimate the compression ratio for the acquired data using reinforcement learning (RL). However, the computational costs associated with existing RL methods that can be utilized on edges are often high. In this study, we developed an efficient RL method for edge devices, referred to as the actor--critic online sequential extreme learning machine (AC-OSELM), and a system to compress data by estimating an appropriate compression ratio on the edge using AC-OSELM. The performance of the proposed method in estimating the compression ratio
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#39537;&#21160;&#27867;&#21270;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25928;&#30410;&#65292;&#36890;&#36807;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.02222</link><description>&lt;p&gt;
&#27169;&#22411;&#39537;&#21160;&#27867;&#21270;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25928;&#30410;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Model-Based Generalization in Reinforcement Learning. (arXiv:2211.02222v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02222
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#39537;&#21160;&#27867;&#21270;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25928;&#30410;&#65292;&#36890;&#36807;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#34987;&#24191;&#27867;&#35748;&#20026;&#20855;&#26377;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#20195;&#29702;&#22120;&#21512;&#25104;&#22823;&#37327;&#30340;&#24819;&#35937;&#32463;&#39564;&#12290;&#32463;&#39564;&#22238;&#25918;&#65288;ER&#65289;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#12290;&#29702;&#35770;&#19978;&#65292;&#19968;&#20010;&#23398;&#20064;&#30340;&#21442;&#25968;&#21270;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#32463;&#39564;&#20013;&#27867;&#21270;&#26469;&#22686;&#28155;&#25968;&#25454;&#38598;&#20013;&#30340;&#20854;&#20182;&#21487;&#20449;&#32463;&#39564;&#65292;&#20174;&#32780;&#25913;&#36827;ER&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#23398;&#20064;&#30340;&#20540;&#20989;&#25968;&#20063;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#27169;&#22411;&#27867;&#21270;&#20026;&#20309;&#26356;&#22909;&#24182;&#19981;&#26126;&#26174;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#25581;&#31034;&#20102;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#33021;&#22815;&#26377;&#25928;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23450;&#29702;&#65292;&#35299;&#37322;&#20102;&#36890;&#36807;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#27604;&#30452;&#25509;&#20351;&#29992;&#36125;&#23572;&#26364;&#26041;&#31243;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20540;&#20989;&#25968;&#22914;&#20309;&#32553;&#23567;&#21487;&#33021;&#20540;&#20989;&#25968;&#38598;&#21512;&#30340;&#33539;&#22260;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20379;&#23454;&#35777;&#32467;&#26524;&#65292;&#35777;&#26126;&#36890;&#36807;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-Based Reinforcement Learning (RL) is widely believed to have the potential to improve sample efficiency by allowing an agent to synthesize large amounts of imagined experience. Experience Replay (ER) can be considered a simple kind of model, which has proved effective at improving the stability and efficiency of deep RL. In principle, a learned parametric model could improve on ER by generalizing from real experience to augment the dataset with additional plausible experience. However, given that learned value functions can also generalize, it is not immediately obvious why model generalization should be better. Here, we provide theoretical and empirical insight into when, and how, we can expect data generated by a learned model to be useful. First, we provide a simple theorem motivating how learning a model as an intermediate step can narrow down the set of possible value functions more than learning a value function directly from data using the Bellman equation. Second, we prov
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24050;&#30693;&#36816;&#31639;&#31526;&#27880;&#20837;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#36229;&#22768;&#24377;&#24615;&#25104;&#20687;&#20013;&#27178;&#21521;&#24212;&#21464;&#25104;&#20687;&#30340;&#36136;&#37327;&#12290;&#30446;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21253;&#25324;&#20302;&#37319;&#26679;&#39057;&#29575;&#12289;&#36816;&#21160;&#21463;&#38480;&#21644;&#32570;&#20047;&#27178;&#21521;&#26041;&#21521;&#30340;&#30456;&#20301;&#20449;&#24687;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#30340;&#26080;&#30417;&#30563;&#27491;&#21017;&#21270;&#24377;&#24615;&#25104;&#20687;&#26041;&#27861;&#24050;&#21462;&#24471;&#23454;&#36136;&#24615;&#25913;&#36827;&#65292;&#20294;&#20165;&#22312;&#35757;&#32451;&#26399;&#38388;&#24212;&#29992;&#27491;&#21017;&#21270;&#65292;&#26080;&#27861;&#20445;&#35777;&#22312;&#27979;&#35797;&#26399;&#38388;&#27178;&#21521;&#24212;&#21464;&#22788;&#20110;&#21487;&#34892;&#33539;&#22260;&#20869;&#12290;</title><link>http://arxiv.org/abs/2211.00172</link><description>&lt;p&gt;
&#23558;&#24050;&#30693;&#36816;&#31639;&#31526;&#27880;&#20837;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36229;&#22768;&#24377;&#24615;&#25104;&#20687;&#20013;&#30340;&#27178;&#21521;&#24212;&#21464;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Infusing known operators in convolutional neural networks for lateral strain imaging in ultrasound elastography. (arXiv:2211.00172v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00172
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24050;&#30693;&#36816;&#31639;&#31526;&#27880;&#20837;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#36229;&#22768;&#24377;&#24615;&#25104;&#20687;&#20013;&#27178;&#21521;&#24212;&#21464;&#25104;&#20687;&#30340;&#36136;&#37327;&#12290;&#30446;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21253;&#25324;&#20302;&#37319;&#26679;&#39057;&#29575;&#12289;&#36816;&#21160;&#21463;&#38480;&#21644;&#32570;&#20047;&#27178;&#21521;&#26041;&#21521;&#30340;&#30456;&#20301;&#20449;&#24687;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#30340;&#26080;&#30417;&#30563;&#27491;&#21017;&#21270;&#24377;&#24615;&#25104;&#20687;&#26041;&#27861;&#24050;&#21462;&#24471;&#23454;&#36136;&#24615;&#25913;&#36827;&#65292;&#20294;&#20165;&#22312;&#35757;&#32451;&#26399;&#38388;&#24212;&#29992;&#27491;&#21017;&#21270;&#65292;&#26080;&#27861;&#20445;&#35777;&#22312;&#27979;&#35797;&#26399;&#38388;&#27178;&#21521;&#24212;&#21464;&#22788;&#20110;&#21487;&#34892;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#24050;&#34987;&#29992;&#20110;&#36229;&#22768;&#24377;&#24615;&#25104;&#20687;&#20013;&#30340;&#20301;&#31227;&#20272;&#35745;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#20272;&#35745;&#39640;&#36136;&#37327;&#30340;&#32437;&#21521;&#24212;&#21464;&#65288;&#22312;&#32437;&#21521;&#26041;&#21521;&#19978;&#30340;&#20301;&#31227;&#23548;&#25968;&#65289;&#12290;&#30456;&#27604;&#32437;&#21521;&#24212;&#21464;&#65292;&#27178;&#21521;&#24212;&#21464;&#22312;&#27850;&#26494;&#27604;&#25104;&#20687;&#21644;&#24377;&#24615;&#37325;&#24314;&#20013;&#20855;&#26377;&#36739;&#24046;&#30340;&#36136;&#37327;&#12290;&#20854;&#20027;&#35201;&#21407;&#22240;&#21253;&#25324;&#37319;&#26679;&#39057;&#29575;&#20302;&#12289;&#36816;&#21160;&#21463;&#38480;&#20197;&#21450;&#27178;&#21521;&#26041;&#21521;&#32570;&#20047;&#30456;&#20301;&#20449;&#24687;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#30340;&#26080;&#30417;&#30563;&#27491;&#21017;&#21270;&#24377;&#24615;&#25104;&#20687;&#65288;PICTURE&#65289;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#36816;&#21160;&#29289;&#29702;&#35268;&#21017;&#23450;&#20041;&#30340;&#21487;&#34892;&#27178;&#21521;&#24212;&#21464;&#33539;&#22260;&#65292;&#24182;&#20351;&#29992;&#27491;&#21017;&#21270;&#31574;&#30053;&#25913;&#21892;&#20102;&#27178;&#21521;&#24212;&#21464;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#25913;&#36827;&#65292;&#20294;&#27491;&#21017;&#21270;&#20165;&#22312;&#35757;&#32451;&#26399;&#38388;&#24212;&#29992;&#65292;&#22240;&#27492;&#26080;&#27861;&#20445;&#35777;&#22312;&#27979;&#35797;&#26399;&#38388;&#27178;&#21521;&#24212;&#21464;&#22788;&#20110;&#21487;&#34892;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (CNN) have been employed for displacement estimation in ultrasound elastography (USE). High-quality axial strains (derivative of the axial displacement in the axial direction) can be estimated by the proposed networks. In contrast to axial strain, lateral strain, which is highly required in Poisson's ratio imaging and elasticity reconstruction, has a poor quality. The main causes include low sampling frequency, limited motion, and lack of phase information in the lateral direction. Recently, physically inspired constraint in unsupervised regularized elastography (PICTURE) has been proposed. This method took into account the range of the feasible lateral strain defined by the rules of physics of motion and employed a regularization strategy to improve the lateral strains. Despite the substantial improvement, the regularization was only applied during the training; hence it did not guarantee during the test that the lateral strain is within the feasible rang
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#40657;&#30418;&#39564;&#35777;&#26041;&#27861;&#26469;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#33021;&#30340;&#33258;&#25105;&#25913;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21457;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#22833;&#36133;&#22330;&#26223;&#24182;&#37325;&#26032;&#35757;&#32451;&#65292;&#20197;&#25913;&#21892;&#22312;&#35757;&#32451;&#20013;&#32570;&#20047;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#30340;&#24615;&#33021;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.16575</link><description>&lt;p&gt;
&#22522;&#20110;&#40657;&#30418;&#39564;&#35777;&#31639;&#27861;&#30340;&#33258;&#25105;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#39550;&#39542;&#23433;&#20840;&#24615;&#33021;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Self-Improving Safety Performance of Reinforcement Learning Based Driving with Black-Box Verification Algorithms. (arXiv:2210.16575v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#40657;&#30418;&#39564;&#35777;&#26041;&#27861;&#26469;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#33021;&#30340;&#33258;&#25105;&#25913;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21457;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#22833;&#36133;&#22330;&#26223;&#24182;&#37325;&#26032;&#35757;&#32451;&#65292;&#20197;&#25913;&#21892;&#22312;&#35757;&#32451;&#20013;&#32570;&#20047;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#30340;&#24615;&#33021;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25913;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#21033;&#29992;&#40657;&#30418;&#39564;&#35777;&#26041;&#27861;&#26469;&#22686;&#24378;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#20027;&#39550;&#39542;&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#33258;&#20027;&#39550;&#39542;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#22330;&#26223;&#30340;&#22810;&#26679;&#24615;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#32570;&#20047;&#23433;&#20840;&#20851;&#38190;&#30340;&#22330;&#26223;&#21487;&#33021;&#23548;&#33268;&#22312;&#30495;&#23454;&#39550;&#39542;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#40657;&#30418;&#39564;&#35777;&#26041;&#27861;&#25506;&#32034;&#35757;&#32451;&#38598;&#30340;&#24369;&#28857;&#12290;&#21457;&#29616;&#33258;&#21160;&#39550;&#39542;&#22833;&#36133;&#22330;&#26223;&#21518;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#37325;&#26032;&#21551;&#21160;RL&#20195;&#29702;&#30340;&#35757;&#32451;&#65292;&#20197;&#25913;&#36827;&#20808;&#21069;&#19981;&#23433;&#20840;&#30340;&#22330;&#26223;&#30340;&#24615;&#33021;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#21457;&#29616;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#24212;&#29992;&#20013;&#34892;&#20026;&#20915;&#31574;&#30340;&#23433;&#20840;&#22833;&#36133;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a self-improving artificial intelligence system to enhance the safety performance of reinforcement learning (RL)-based autonomous driving (AD) agents using black-box verification methods. RL algorithms have become popular in AD applications in recent years. However, the performance of existing RL algorithms heavily depends on the diversity of training scenarios. A lack of safety-critical scenarios during the training phase could result in poor generalization performance in real-world driving applications. We propose a novel framework in which the weaknesses of the training set are explored through black-box verification methods. After discovering AD failure scenarios, the RL agent's training is re-initiated via transfer learning to improve the performance of previously unsafe scenarios. Simulation results demonstrate that our approach efficiently discovers safety failures of action decisions in RL-based adaptive cruise control (ACC) applications and significant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20559;&#22909;&#23376;&#37319;&#26679;&#30340;&#26041;&#27861;&#26469;&#23545;&#38543;&#26426;&#26799;&#24230;Langevin&#21160;&#21147;&#23398;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#22343;&#21248;&#27010;&#29575;&#20998;&#24067;&#23376;&#37319;&#26679;&#23545;&#20855;&#26377;&#26356;&#22823;&#24433;&#21709;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#21152;&#26435;&#65292;&#21516;&#26102;&#36824;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#23376;&#37319;&#26679;&#22823;&#23567;&#26469;&#25552;&#39640;&#26799;&#24230;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#20943;&#23569;&#23376;&#37319;&#26679;&#25968;&#30340;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#31934;&#24230;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2210.16189</link><description>&lt;p&gt;
&#20559;&#22909;&#23376;&#37319;&#26679;&#23545;&#20110;&#38543;&#26426;&#26799;&#24230;Langevin&#21160;&#21147;&#23398;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Preferential Subsampling for Stochastic Gradient Langevin Dynamics. (arXiv:2210.16189v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20559;&#22909;&#23376;&#37319;&#26679;&#30340;&#26041;&#27861;&#26469;&#23545;&#38543;&#26426;&#26799;&#24230;Langevin&#21160;&#21147;&#23398;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#22343;&#21248;&#27010;&#29575;&#20998;&#24067;&#23376;&#37319;&#26679;&#23545;&#20855;&#26377;&#26356;&#22823;&#24433;&#21709;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#21152;&#26435;&#65292;&#21516;&#26102;&#36824;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#23376;&#37319;&#26679;&#22823;&#23567;&#26469;&#25552;&#39640;&#26799;&#24230;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#20943;&#23569;&#23376;&#37319;&#26679;&#25968;&#30340;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#31934;&#24230;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;MCMC&#65288;SGMCMC&#65289;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#12289;&#22343;&#21248;&#21152;&#26435;&#30340;&#25968;&#25454;&#23376;&#26679;&#26412;&#26500;&#24314;&#23545;&#20110;&#23545;&#25968;&#21518;&#39564;&#26799;&#24230;&#30340;&#26080;&#20559;&#20272;&#35745;&#65292;&#20026;&#20256;&#32479;MCMC&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#34429;&#28982;&#35745;&#31639;&#39640;&#25928;&#65292;&#20294;&#30001;&#27492;&#20135;&#29983;&#30340;&#26799;&#24230;&#20272;&#35745;&#21487;&#33021;&#20855;&#26377;&#36739;&#39640;&#30340;&#26041;&#24046;&#65292;&#24182;&#19988;&#20250;&#24433;&#21709;&#37319;&#26679;&#22120;&#24615;&#33021;&#12290;&#20256;&#32479;&#19978;&#65292;&#26041;&#24046;&#25511;&#21046;&#38382;&#39064;&#36890;&#36807;&#26500;&#24314;&#26356;&#22909;&#30340;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#22120;&#26469;&#35299;&#20915;&#65292;&#36890;&#24120;&#20351;&#29992;&#25511;&#21046;&#21464;&#37327;&#12290;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#31163;&#25955;&#30340;&#38750;&#22343;&#21248;&#27010;&#29575;&#20998;&#24067;&#26469;&#20559;&#22909;&#22320;&#23376;&#37319;&#26679;&#23545;&#20110;&#23545;&#26799;&#24230;&#20135;&#29983;&#26356;&#22823;&#24433;&#21709;&#30340;&#25968;&#25454;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#22312;&#31639;&#27861;&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23376;&#37319;&#26679;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#22312;&#38590;&#20197;&#20272;&#35745;&#26799;&#24230;&#30340;&#26679;&#26412;&#31354;&#38388;&#20013;&#22686;&#21152;&#23376;&#37319;&#26679;&#22823;&#23567;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#22823;&#24133;&#20943;&#23569;&#24179;&#22343;&#23376;&#37319;&#26679;&#25968;&#30340;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#31934;&#24230;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient MCMC (SGMCMC) offers a scalable alternative to traditional MCMC, by constructing an unbiased estimate of the gradient of the log-posterior with a small, uniformly-weighted subsample of the data. While efficient to compute, the resulting gradient estimator may exhibit a high variance and impact sampler performance. The problem of variance control has been traditionally addressed by constructing a better stochastic gradient estimator, often using control variates. We propose to use a discrete, non-uniform probability distribution to preferentially subsample data points that have a greater impact on the stochastic gradient. In addition, we present a method of adaptively adjusting the subsample size at each iteration of the algorithm, so that we increase the subsample size in areas of the sample space where the gradient is harder to estimate. We demonstrate that such an approach can maintain the same level of accuracy while substantially reducing the average subsample s
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#22312;&#32447;&#36951;&#25022;&#20445;&#35777;&#30340;&#39118;&#38505;&#21388;&#24694;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#39564;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.13573</link><description>&lt;p&gt;
&#26377;&#26465;&#20214;&#39118;&#38505;&#21388;&#24694;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Conditionally Risk-Averse Contextual Bandits. (arXiv:2210.13573v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13573
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#22312;&#32447;&#36951;&#25022;&#20445;&#35777;&#30340;&#39118;&#38505;&#21388;&#24694;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#39564;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39118;&#38505;&#21388;&#24694;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#24179;&#22343;&#32479;&#35745;&#20445;&#35777;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#36890;&#36807;&#29306;&#29298;&#26368;&#22351;&#24773;&#20917;&#30340;&#34920;&#29616;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;&#35774;&#35745;&#19968;&#20010;&#39118;&#38505;&#21388;&#24694;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25506;&#32034;&#26159;&#24517;&#35201;&#30340;&#65292;&#20294;&#39118;&#38505;&#21388;&#24694;&#23545;&#25972;&#20010;&#22870;&#21169;&#20998;&#24067;&#37117;&#24456;&#25935;&#24863;&#65307;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#22312;&#32447;&#36951;&#25022;&#20445;&#35777;&#30340;&#39118;&#38505;&#21388;&#24694;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#36825;&#20123;&#22330;&#26223;&#20013;&#26368;&#22351;&#24773;&#20917;&#30340;&#32467;&#26524;&#24212;&#35813;&#34987;&#36991;&#20813;&#65292;&#21253;&#25324;&#21160;&#24577;&#23450;&#20215;&#12289;&#24211;&#23384;&#31649;&#29702;&#21644;&#33258;&#25105;&#35843;&#25972;&#36719;&#20214;&#65307;&#20854;&#20013;&#36824;&#21253;&#25324;&#19968;&#20010;&#29983;&#20135;&#32423;&#30340;&#25193;&#23637;&#25968;&#25454;&#22788;&#29702;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual bandits with average-case statistical guarantees are inadequate in risk-averse situations because they might trade off degraded worst-case behaviour for better average performance. Designing a risk-averse contextual bandit is challenging because exploration is necessary but risk-aversion is sensitive to the entire distribution of rewards; nonetheless we exhibit the first risk-averse contextual bandit algorithm with an online regret guarantee. We conduct experiments from diverse scenarios where worst-case outcomes should be avoided, from dynamic pricing, inventory management, and self-tuning software; including a production exascale data processing system.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#26041;&#27861;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#12289;&#38750;&#32447;&#24615;&#21644;&#28145;&#24230;&#23545;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#23545;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#21644;&#37051;&#25509;&#30697;&#38453;&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#25581;&#31034;&#20102;&#32447;&#24615;GNN&#19982;&#38750;&#32447;&#24615;ReLU-GNN&#24615;&#33021;&#30456;&#24403;&#30340;&#29616;&#35937;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2210.09809</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#20999;&#32447;&#26680;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#65292;&#38750;&#32447;&#24615;&#21644;&#28145;&#24230;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Convolutions, Non-linearity and Depth in Graph Neural Networks using Neural Tangent Kernel. (arXiv:2210.09809v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#26041;&#27861;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#12289;&#38750;&#32447;&#24615;&#21644;&#28145;&#24230;&#23545;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#23545;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#21644;&#37051;&#25509;&#30697;&#38453;&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#25581;&#31034;&#20102;&#32447;&#24615;GNN&#19982;&#38750;&#32447;&#24615;ReLU-GNN&#24615;&#33021;&#30456;&#24403;&#30340;&#29616;&#35937;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#36890;&#36807;&#20351;&#29992;&#8220;&#22270;&#21367;&#31215;&#8221;&#26469;&#32858;&#21512;&#30456;&#37051;&#33410;&#28857;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36873;&#25321;&#21512;&#36866;&#30340;&#32593;&#32476;&#26550;&#26500;&#65288;&#20363;&#22914;&#28145;&#24230;&#21644;&#28608;&#27963;&#20989;&#25968;&#65289;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#27599;&#20010;&#35774;&#35745;&#36873;&#25321;&#23545;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#30340;&#21367;&#31215;&#24050;&#25104;&#20026;&#20027;&#27969;&#36873;&#25321;&#65292;&#20854;&#20013;&#23545;&#37051;&#25509;&#30697;&#38453;&#36827;&#34892;&#23545;&#31216;&#24402;&#19968;&#21270;&#26159;&#26368;&#24191;&#27867;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#34892;&#24402;&#19968;&#21270;&#30340;&#37051;&#25509;&#30697;&#38453;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#23613;&#31649;GNN&#30340;&#20351;&#29992;&#38750;&#24120;&#24191;&#27867;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#20005;&#26684;&#30340;&#29702;&#35770;&#30740;&#31350;&#20851;&#20110;&#36825;&#20123;&#21367;&#31215;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#26080;&#27861;&#35299;&#37322;&#36825;&#31181;&#34892;&#20026;&#12290;&#21516;&#26679;&#65292;&#32447;&#24615;GNN&#30340;&#24615;&#33021;&#19982;&#38750;&#32447;&#24615;ReLU-GNN&#30340;&#24615;&#33021;&#30456;&#24403;&#30340;&#32463;&#39564;&#35266;&#23519;&#20063;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fundamental principle of Graph Neural Networks (GNNs) is to exploit the structural information of the data by aggregating the neighboring nodes using a `graph convolution' in conjunction with a suitable choice for the network architecture, such as depth and activation functions. Therefore, understanding the influence of each of the design choice on the network performance is crucial. Convolutions based on graph Laplacian have emerged as the dominant choice with the symmetric normalization of the adjacency matrix as the most widely adopted one. However, some empirical studies show that row normalization of the adjacency matrix outperforms it in node classification. Despite the widespread use of GNNs, there is no rigorous theoretical study on the representation power of these convolutions, that could explain this behavior. Similarly, the empirical observation of the linear GNNs performance being on par with non-linear ReLU GNNs lacks rigorous theory.  In this work, we theoretically a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;U-net-like&#32467;&#26500;&#21644;&#20107;&#20214;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#25104;&#21151;&#22320;&#23558;Baikal-GVD&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#21644;&#20449;&#21495;&#20998;&#31163;&#24320;&#26469;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.04653</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#32593;&#32476;&#25490;&#38500;Baikal-GVD&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Rejecting noise in Baikal-GVD data with neural networks. (arXiv:2210.04653v2 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;U-net-like&#32467;&#26500;&#21644;&#20107;&#20214;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#25104;&#21151;&#22320;&#23558;Baikal-GVD&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#21644;&#20449;&#21495;&#20998;&#31163;&#24320;&#26469;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Baikal-GVD&#26159;&#19968;&#20010;&#23433;&#35013;&#22312;&#36125;&#21152;&#23572;&#28246;&#28129;&#27700;&#20013;&#30340;&#22823;&#22411;&#65288;&#32422;1 km&#179;&#65289;&#27700;&#19979;&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;&#12290;&#28145;&#28246;&#27700;&#29615;&#22659;&#20805;&#26021;&#30528;&#32972;&#26223;&#20809;&#65292;&#21487;&#20197;&#34987;Baikal-GVD&#30340;&#20809;&#20256;&#24863;&#22120;&#26816;&#27979;&#21040;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#22122;&#22768;&#20987;&#20013;&#19982;&#36890;&#36807;&#25506;&#27979;&#22120;&#20256;&#25773;&#30340;&#30456;&#23545;&#35770;&#31890;&#23376;&#30340;&#20449;&#21495;&#20987;&#20013;&#20998;&#31163;&#24320;&#26469;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#31867;&#20284;U-net&#30340;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#20102;&#20107;&#20214;&#30340;&#26102;&#38388;&#65288;&#22240;&#26524;&#65289;&#32467;&#26500;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#26631;&#22312;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;99&#65285;&#30340;&#20449;&#21495;&#32431;&#24230;&#65288;&#31934;&#30830;&#24230;&#65289;&#21644;96&#65285;&#30340;&#29983;&#23384;&#25928;&#29575;&#65288;&#21484;&#22238;&#29575;&#65289;&#12290;&#25105;&#20204;&#23558;&#24320;&#21457;&#30340;&#26041;&#27861;&#19982;&#31639;&#27861;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20182;&#21487;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#22522;&#20110;&#22270;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Baikal-GVD is a large ($\sim$1 km$^3$) underwater neutrino telescope installed in the fresh waters of Lake Baikal. The deep lake water environment is pervaded by background light, which is detectable by Baikal-GVD's photosensors. We introduce a neural network for an efficient separation of these noise hits from the signal ones, stemming from the propagation of relativistic particles through the detector. The model has a U-net-like architecture and employs temporal (causal) structure of events. The neural network's metrics reach up to 99\% signal purity (precision) and 96\% survival efficiency (recall) on Monte-Carlo simulated dataset. We compare the developed method with the algorithmic approach to rejecting the noise and discuss other possible architectures of neural networks, including graph-based ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#26631;&#20934;&#21270;&#23545;&#33041;&#30005;&#39046;&#22495;&#36866;&#24212;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#19977;&#20010;EEG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2210.01081</link><description>&lt;p&gt;
&#25968;&#25454;&#26631;&#20934;&#21270;&#22312;&#33041;&#30005;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On The Effects Of Data Normalisation For Domain Adaptation On EEG Data. (arXiv:2210.01081v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#26631;&#20934;&#21270;&#23545;&#33041;&#30005;&#39046;&#22495;&#36866;&#24212;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#19977;&#20010;EEG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#65292;&#25968;&#25454;&#38598;&#36716;&#25442;&#38382;&#39064;&#19968;&#30452;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65292;&#19982;&#26426;&#22120;&#23398;&#20064;&#26631;&#20934;&#20551;&#35774;&#19981;&#21516;&#30340;&#26159;&#65292;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#30340;&#25968;&#25454;&#21487;&#33021;&#36981;&#24490;&#19981;&#21516;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#27867;&#21270;&#24615;&#33021;&#19981;&#20339;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#23588;&#20026;&#31361;&#20986;&#65292;&#22240;&#20026;&#29983;&#29289;&#20449;&#21495;&#22914;&#33041;&#30005;&#22270;&#20449;&#21495;&#36890;&#24120;&#34987;&#29992;&#20110;&#25968;&#25454;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#33041;&#30005;&#20449;&#21495;&#22312;&#26102;&#38388;&#21644;&#19981;&#21516;&#21463;&#35797;&#32773;&#20043;&#38388;&#37117;&#38750;&#24120;&#19981;&#31283;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35768;&#22810;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#27861;&#37117;&#22522;&#20110;&#26368;&#36817;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#39046;&#22495;&#36866;&#24212;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25913;&#36827;&#30340;&#23454;&#38469;&#21407;&#22240;&#20173;&#28982;&#27169;&#31946;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#25968;&#25454;&#26631;&#20934;&#21270;&#25110;&#26631;&#20934;&#21270;&#31574;&#30053;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#20351;&#29992;SEED&#12289;DEAP&#21644;BCI&#31454;&#36187;IV 2a EEG&#25968;&#25454;&#38598;&#65292;&#23545;&#25968;&#25454;&#26631;&#20934;&#21270;&#31574;&#30053;&#24212;&#29992;&#20110;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#20013;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the Machine Learning (ML) literature, a well-known problem is the Dataset Shift problem where, differently from the ML standard hypothesis, the data in the training and test sets can follow different probability distributions, leading ML systems toward poor generalisation performances. This problem is intensely felt in the Brain-Computer Interface (BCI) context, where bio-signals as Electroencephalographic (EEG) are often used. In fact, EEG signals are highly non-stationary both over time and between different subjects. To overcome this problem, several proposed solutions are based on recent transfer learning approaches such as Domain Adaption (DA). In several cases, however, the actual causes of the improvements remain ambiguous. This paper focuses on the impact of data normalisation, or standardisation strategies applied together with DA methods. In particular, using \textit{SEED}, \textit{DEAP}, and \textit{BCI Competition IV 2a} EEG datasets, we experimentally evaluated the impa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#35780;&#20998;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22522;&#20110;&#22810;&#20010;&#35266;&#27979;&#26465;&#20214;&#19979;&#24471;&#21040;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#32858;&#21512;&#22810;&#20010;&#35266;&#27979;&#20540;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2209.14249</link><description>&lt;p&gt;
&#22522;&#20110;&#32452;&#21512;&#35780;&#20998;&#24314;&#27169;&#30340;&#22522;&#20110;&#27169;&#25311;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Compositional Score Modeling for Simulation-based Inference. (arXiv:2209.14249v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#35780;&#20998;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22522;&#20110;&#22810;&#20010;&#35266;&#27979;&#26465;&#20214;&#19979;&#24471;&#21040;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#32858;&#21512;&#22810;&#20010;&#35266;&#27979;&#20540;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#25512;&#26029;&#30340;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#26041;&#27861;&#22312;&#22788;&#29702;&#22522;&#20110;&#22810;&#20010;&#35266;&#27979;&#26465;&#20214;&#19979;&#24471;&#21040;&#30340;&#21518;&#39564;&#20998;&#24067;&#26102;&#21487;&#33021;&#19981;&#22826;&#36866;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#20542;&#21521;&#20110;&#38656;&#35201;&#22823;&#37327;&#30340;&#27169;&#25311;&#22120;&#35843;&#29992;&#26469;&#23398;&#20064;&#20934;&#30830;&#30340;&#36817;&#20284;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31070;&#32463;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#21487;&#20197;&#22312;&#23398;&#20064;&#20102;&#21333;&#29420;&#35266;&#27979;&#20540;&#21518;&#65292;&#22788;&#29702;&#25512;&#26029;&#26102;&#30340;&#22810;&#20010;&#35266;&#27979;&#20540;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#26631;&#20934;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#22914;MCMC&#25110;&#21464;&#20998;&#25512;&#26029;&#65292;&#36825;&#20123;&#25512;&#26029;&#26041;&#27861;&#26377;&#19968;&#23450;&#30340;&#24615;&#33021;&#32570;&#38519;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#35780;&#20998;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#34701;&#21512;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#23545;&#30001;&#21333;&#20010;&#35266;&#27979;&#24341;&#36215;&#30340;&#65288;&#25193;&#25955;&#30340;&#65289;&#21518;&#39564;&#20998;&#24067;&#24314;&#27169;&#65292;&#28982;&#21518;&#24341;&#20837;&#20102;&#19968;&#31181;&#32452;&#21512;&#23398;&#20064;&#20998;&#25968;&#20197;&#36817;&#20284;&#20174;&#30446;&#26631;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20855;&#26377;&#20248;&#21183;&#65292;&#22312;&#25512;&#26029;&#26102;&#21487;&#20197;&#33258;&#28982;&#22320;&#32858;&#21512;&#22810;&#20010;&#35266;&#27979;&#20540;&#65292;&#24182;&#36991;&#20813;&#20102;&#20256;&#32479;&#25512;&#26029;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Posterior Estimation methods for simulation-based inference can be ill-suited for dealing with posterior distributions obtained by conditioning on multiple observations, as they tend to require a large number of simulator calls to learn accurate approximations. In contrast, Neural Likelihood Estimation methods can handle multiple observations at inference time after learning from individual observations, but they rely on standard inference methods, such as MCMC or variational inference, which come with certain performance drawbacks. We introduce a new method based on conditional score modeling that enjoys the benefits of both approaches. We model the scores of the (diffused) posterior distributions induced by individual observations, and introduce a way of combining the learned scores to approximately sample from the target posterior distribution. Our approach is sample-efficient, can naturally aggregate multiple observations at inference time, and avoids the drawbacks of standa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26925;&#29699;&#30340;&#38556;&#30861;&#29289;&#35782;&#21035;&#19982;&#27979;&#36895;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#22522;&#20110;&#26925;&#29699;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#24102;&#26377;&#38745;&#24577;&#21644;&#21160;&#24577;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#65292;&#20854;&#36816;&#34892;&#36895;&#24230;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#24555;&#19988;&#19981;&#38656;&#35201;&#39044;&#20808;&#30693;&#36947;&#32858;&#31867;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.14233</link><description>&lt;p&gt;
&#26410;&#30693;&#21160;&#24577;&#29615;&#22659;&#19979;&#24555;&#36895;&#36816;&#21160;&#35268;&#21010;&#30340;&#38556;&#30861;&#29289;&#35782;&#21035;&#19982;&#26925;&#29699;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Obstacle Identification and Ellipsoidal Decomposition for Fast Motion Planning in Unknown Dynamic Environments. (arXiv:2209.14233v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26925;&#29699;&#30340;&#38556;&#30861;&#29289;&#35782;&#21035;&#19982;&#27979;&#36895;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#22522;&#20110;&#26925;&#29699;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#24102;&#26377;&#38745;&#24577;&#21644;&#21160;&#24577;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#65292;&#20854;&#36816;&#34892;&#36895;&#24230;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#24555;&#19988;&#19981;&#38656;&#35201;&#39044;&#20808;&#30693;&#36947;&#32858;&#31867;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#20154;&#31995;&#32479;&#20013;&#65292;&#36991;&#20813;&#19982;&#26410;&#30693;&#21160;&#24577;&#38556;&#30861;&#29289;&#30340;&#30896;&#25758;&#26159;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26925;&#29699;&#26469;&#35782;&#21035;&#38556;&#30861;&#29289;&#65292;&#24182;&#20272;&#35745;&#20854;&#32447;&#24615;&#21644;&#35282;&#36895;&#24230;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#20219;&#20309;&#29289;&#20307;&#37117;&#21487;&#20197;&#36817;&#20284;&#34920;&#31034;&#20026;&#26925;&#29699;&#30340;&#29702;&#24565;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#20272;&#35745;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#12289;Kyachiyan&#31639;&#27861;&#21644;&#20248;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#30693;&#36947;&#32858;&#31867;&#25968;&#30446;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#26102;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#22522;&#20110;&#26925;&#29699;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#20197;&#21305;&#37197;&#32473;&#23450;&#30340;&#20004;&#20010;&#26102;&#38388;&#25509;&#36817;&#30340;&#28857;&#24103;&#30340;&#38556;&#30861;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#24102;&#26377;&#38745;&#24577;&#21644;&#21160;&#24577;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#65292;&#21253;&#25324;&#20855;&#26377;&#26059;&#36716;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#20854;&#20182;&#32858;&#31867;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collision avoidance in the presence of dynamic obstacles in unknown environments is one of the most critical challenges for unmanned systems. In this paper, we present a method that identifies obstacles in terms of ellipsoids to estimate linear and angular obstacle velocities. Our proposed method is based on the idea of any object can be approximately expressed by ellipsoids. To achieve this, we propose a method based on variational Bayesian estimation of Gaussian mixture model, the Kyachiyan algorithm, and a refinement algorithm. Our proposed method does not require knowledge of the number of clusters and can operate in real-time, unlike existing optimization-based methods. In addition, we define an ellipsoid-based feature vector to match obstacles given two timely close point frames. Our method can be applied to any environment with static and dynamic obstacles, including the ones with rotating obstacles. We compare our algorithm with other clustering methods and show that when coupl
&lt;/p&gt;</description></item><item><title>DynDepNet&#26159;&#19968;&#31181;&#23398;&#20064;fMRI&#25968;&#25454;&#20013;&#26102;&#21464;&#20381;&#36182;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#24615;&#21035;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.13513</link><description>&lt;p&gt;
DynDepNet:&#36890;&#36807;&#21160;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;&#20174;fMRI&#25968;&#25454;&#20013;&#23398;&#20064;&#26102;&#21464;&#30340;&#20381;&#36182;&#20851;&#31995;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
DynDepNet: Learning Time-Varying Dependency Structures from fMRI Data via Dynamic Graph Structure Learning. (arXiv:2209.13513v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13513
&lt;/p&gt;
&lt;p&gt;
DynDepNet&#26159;&#19968;&#31181;&#23398;&#20064;fMRI&#25968;&#25454;&#20013;&#26102;&#21464;&#20381;&#36182;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#24615;&#21035;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23398;&#20064;&#22522;&#20110;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#25968;&#25454;&#30340;&#22823;&#33041;&#22270;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#26041;&#27861;&#20551;&#35774;&#22823;&#33041;&#22270;&#22312;&#26102;&#38388;&#19978;&#26159;&#38745;&#24577;&#30340;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#24050;&#30693;&#22270;&#37051;&#25509;&#30697;&#38453;&#12290;&#36825;&#20123;&#20551;&#35774;&#19982;&#35777;&#25454;&#30456;&#30683;&#30462;&#65292;&#21363;&#22823;&#33041;&#22270;&#22312;&#26102;&#38388;&#19978;&#26159;&#21464;&#21270;&#30340;&#65292;&#24182;&#19988;&#20854;&#36830;&#25509;&#32467;&#26500;&#21462;&#20915;&#20110;&#21151;&#33021;&#36830;&#25509;&#27979;&#37327;&#30340;&#36873;&#25321;&#12290;&#29992;&#22122;&#22768;&#22823;&#33041;&#22270;&#38169;&#35823;&#22320;&#34920;&#31034;fMRI&#25968;&#25454;&#20250;&#23545;GNN&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DynDepNet&#65292;&#19968;&#31181;&#36890;&#36807;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#25152;&#24341;&#21457;&#30340;fMRI&#25968;&#25454;&#30340;&#26368;&#20248;&#26102;&#21464;&#20381;&#36182;&#32467;&#26500;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#23545;&#20110;&#24615;&#21035;&#20998;&#31867;&#20219;&#21153;&#65292;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;fMRI&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;DynDepNet&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20934;&#30830;&#29575;&#20998;&#21035;&#27604;&#26368;&#20339;&#22522;&#20934;&#32447;&#25552;&#39640;&#20102;&#32422;8&#20010;&#30334;&#20998;&#28857;&#21644;6&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have demonstrated success in learning representations of brain graphs derived from functional magnetic resonance imaging (fMRI) data. However, existing GNN methods assume brain graphs are static over time and the graph adjacency matrix is known prior to model training. These assumptions contradict evidence that brain graphs are time-varying with a connectivity structure that depends on the choice of functional connectivity measure. Incorrectly representing fMRI data with noisy brain graphs can adversely affect GNN performance. To address this, we propose DynDepNet, a novel method for learning the optimal time-varying dependency structure of fMRI data induced by downstream prediction tasks. Experiments on real-world fMRI datasets, for the task of sex classification, demonstrate that DynDepNet achieves state-of-the-art results, outperforming the best baseline in terms of accuracy by approximately 8 and 6 percentage points, respectively. Furthermore, analysis 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bayan&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#25110;&#36817;&#20284;&#20248;&#21270;&#27169;&#22359;&#24230;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36820;&#22238;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#20998;&#21306;&#65292;&#24182;&#19988;&#27604;&#20854;&#20182;&#31639;&#27861;&#24555;&#25968;&#20493;&#65292;&#24182;&#33021;&#22815;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#20934;&#30830;&#22320;&#25214;&#21040;&#22320;&#38754;&#30495;&#23454;&#31038;&#21306;&#12290;</title><link>http://arxiv.org/abs/2209.04562</link><description>&lt;p&gt;
Bayan&#31639;&#27861;&#65306;&#36890;&#36807;&#23545;&#27169;&#22359;&#24230;&#30340;&#31934;&#30830;&#21644;&#36817;&#20284;&#20248;&#21270;&#26469;&#26816;&#27979;&#32593;&#32476;&#20013;&#30340;&#31038;&#21306;
&lt;/p&gt;
&lt;p&gt;
The Bayan Algorithm: Detecting Communities in Networks Through Exact and Approximate Optimization of Modularity. (arXiv:2209.04562v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04562
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bayan&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#25110;&#36817;&#20284;&#20248;&#21270;&#27169;&#22359;&#24230;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36820;&#22238;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#20998;&#21306;&#65292;&#24182;&#19988;&#27604;&#20854;&#20182;&#31639;&#27861;&#24555;&#25968;&#20493;&#65292;&#24182;&#33021;&#22815;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#20934;&#30830;&#22320;&#25214;&#21040;&#22320;&#38754;&#30495;&#23454;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#26159;&#32593;&#32476;&#31185;&#23398;&#20013;&#30340;&#32463;&#20856;&#38382;&#39064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#20247;&#22810;&#26041;&#27861;&#20013;&#65292;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#26368;&#22823;&#21270;&#27169;&#22359;&#24230;&#12290;&#23613;&#31649;&#21551;&#21457;&#24335;&#27169;&#22359;&#24230;&#26368;&#22823;&#21270;&#31639;&#27861;&#35774;&#35745;&#29702;&#24565;&#21644;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#24456;&#23569;&#36820;&#22238;&#26368;&#20339;&#20998;&#21306;&#25110;&#31867;&#20284;&#20998;&#21306;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#31639;&#27861;Bayan&#65292;&#23427;&#36820;&#22238;&#20855;&#26377;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#20998;&#21306;&#20445;&#35777;&#30340;&#20998;&#21306;&#12290;Bayan&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#20998;&#25903;&#38480;&#30028;&#26041;&#26696;&#65292;&#23427;&#35299;&#20915;&#20102;&#38382;&#39064;&#30340;&#25972;&#25968;&#35268;&#21010;&#20844;&#24335;&#20197;&#36798;&#21040;&#26368;&#20248;&#25110;&#36817;&#20284;&#26368;&#20248;&#30340;&#30446;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;Bayan&#22312;&#21512;&#25104;&#22522;&#20934;&#21644;&#30495;&#23454;&#32593;&#32476;&#33410;&#28857;&#26631;&#31614;&#30340;&#26816;&#32034;&#22320;&#38754;&#30495;&#23454;&#31038;&#21306;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#27604;&#20854;&#20182;21&#31181;&#31639;&#27861;&#24555;&#25968;&#20493;&#65292;&#21487;&#20197;&#25214;&#21040;&#26368;&#20248;&#20998;&#21306;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection is a classic problem in network science with extensive applications in various fields. Among numerous approaches, the most common method is modularity maximization. Despite their design philosophy and wide adoption, heuristic modularity maximization algorithms rarely return an optimal partition or anything similar. We propose a specialized algorithm, Bayan, which returns partitions with a guarantee of either optimality or proximity to an optimal partition. At the core of the Bayan algorithm is a branch-and-cut scheme that solves an integer programming formulation of the problem to optimality or approximate it within a factor. We demonstrate Bayan's distinctive accuracy and stability over 21 other algorithms in retrieving ground-truth communities in synthetic benchmarks and node labels in real networks. Bayan is several times faster than open-source and commercial solvers for modularity maximization making it capable of finding optimal partitions for instances that c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26131;&#20110;&#23454;&#26045;&#30340;&#35821;&#38899;&#35748;&#35777;&#31995;&#32479;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#24378;&#22823;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#31216;&#20026;Guardian&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#37492;&#21035;&#22120;&#12290;</title><link>http://arxiv.org/abs/2209.04547</link><description>&lt;p&gt;
&#38450;&#24481;&#35821;&#38899;&#35748;&#35777;&#20013;&#30340;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defend Data Poisoning Attacks on Voice Authentication. (arXiv:2209.04547v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26131;&#20110;&#23454;&#26045;&#30340;&#35821;&#38899;&#35748;&#35777;&#31995;&#32479;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#24378;&#22823;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#31216;&#20026;Guardian&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#37492;&#21035;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#35828;&#35805;&#20154;&#39564;&#35777;&#24050;&#32463;&#21462;&#24471;&#20102;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#27491;&#22312;&#20316;&#20026;&#19968;&#31181;&#29983;&#29289;&#29305;&#24449;&#35748;&#35777;&#36873;&#39033;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#35768;&#22810;&#22330;&#26223;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#32593;&#32476;&#26381;&#21153;&#24066;&#22330;&#20013;&#12290;&#19982;&#20256;&#32479;&#23494;&#30721;&#30456;&#27604;&#65292;"&#22768;&#38899;&#23494;&#30721;"&#26356;&#26041;&#20415;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#20154;&#20204;&#19981;&#24517;&#35760;&#24518;&#19981;&#21516;&#30340;&#23494;&#30721;&#12290;&#28982;&#32780;&#65292;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#27491;&#22312;&#20351;&#36825;&#20123;&#35821;&#38899;&#35748;&#35777;&#31995;&#32479;&#38754;&#20020;&#39118;&#38505;&#12290;&#22312;&#27809;&#26377;&#24378;&#22823;&#30340;&#23433;&#20840;&#20445;&#35777;&#30340;&#24773;&#20917;&#19979;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27450;&#39575;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#26469;&#35775;&#38382;&#21512;&#27861;&#29992;&#25143;&#30340;&#32593;&#32476;&#36134;&#25143;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26131;&#20110;&#23454;&#26045;&#30340;&#35821;&#38899;&#35748;&#35777;&#31995;&#32479;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#65292;&#36825;&#31181;&#25915;&#20987;&#20960;&#20046;&#38590;&#20197;&#34987;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#25429;&#25417;&#21040;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#24378;&#22823;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#31216;&#20026;Guardian&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#37492;&#21035;&#22120;&#12290;Guardian&#37492;&#21035;&#22120;&#32467;&#21512;&#20102;&#19968;&#31995;&#21015;&#26032;&#25216;&#26415;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
With the advances in deep learning, speaker verification has achieved very high accuracy and is gaining popularity as a type of biometric authentication option in many scenes of our daily life, especially the growing market of web services. Compared to traditional passwords, "vocal passwords" are much more convenient as they relieve people from memorizing different passwords. However, new machine learning attacks are putting these voice authentication systems at risk. Without a strong security guarantee, attackers could access legitimate users' web accounts by fooling the deep neural network (DNN) based voice recognition models. In this paper, we demonstrate an easy-to-implement data poisoning attack to the voice authentication system, which can hardly be captured by existing defense mechanisms. Thus, we propose a more robust defense method, called Guardian, which is a convolutional neural network-based discriminator. The Guardian discriminator integrates a series of novel techniques i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39640;&#32500;(&#40065;&#26834;)Wasserstein&#23545;&#40784;&#30340;&#25968;&#25454;&#30456;&#20851;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#39640;&#32500;&#24230;&#30340;&#20960;&#20309;&#27169;&#24335;&#26469;&#38477;&#20302;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.02905</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#39640;&#32500;(&#40065;&#26834;)Wasserstein&#23545;&#40784;&#30340;&#25968;&#25454;&#30456;&#20851;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Data-dependent Approach for High Dimensional (Robust) Wasserstein Alignment. (arXiv:2209.02905v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39640;&#32500;(&#40065;&#26834;)Wasserstein&#23545;&#40784;&#30340;&#25968;&#25454;&#30456;&#20851;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#39640;&#32500;&#24230;&#30340;&#20960;&#20309;&#27169;&#24335;&#26469;&#38477;&#20302;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#21487;&#20197;&#34987;&#24402;&#32467;&#20026;&#20004;&#20010;&#20960;&#20309;&#27169;&#24335;&#30340;&#23545;&#40784;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#20197;&#24448;&#30340;&#35768;&#22810;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#23545;2D&#25110;3D&#27169;&#24335;&#30340;&#23545;&#40784;&#19978;&#12290;&#26368;&#36817;&#65292;&#22312;&#39640;&#32500;&#24230;&#30340;&#23545;&#40784;&#38382;&#39064;&#19978;&#21457;&#29616;&#20102;&#19968;&#20123;&#26032;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#31639;&#27861;&#26041;&#38754;&#30340;&#30740;&#31350;&#20173;&#28982;&#30456;&#23545;&#26377;&#38480;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#21482;&#26159;&#23545;2D&#21644;3D&#24773;&#20917;&#30340;&#31616;&#21333;&#25193;&#23637;&#65292;&#24182;&#19988;&#24448;&#24448;&#23384;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#39640;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#26694;&#26550;&#26469;&#21387;&#32553;&#39640;&#32500;&#24230;&#30340;&#20960;&#20309;&#27169;&#24335;&#12290;&#20219;&#20309;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#37117;&#21487;&#20197;&#24212;&#29992;&#20110;&#21387;&#32553;&#21518;&#30340;&#20960;&#20309;&#27169;&#24335;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#24605;&#36335;&#26469;&#28304;&#20110;&#39640;&#32500;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#20302;&#20869;&#22312;&#32500;&#24230;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#19968;&#31181;&#8220;&#25968;&#25454;&#30456;&#20851;&#8221;&#26041;&#27861;&#65292;&#20854;&#22797;&#26434;&#24230;&#21462;&#20915;&#20110;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world problems can be formulated as the alignment between two geometric patterns. Previously, a great amount of research focus on the alignment of 2D or 3D patterns in the field of computer vision. Recently, the alignment problem in high dimensions finds several novel applications in practice. However, the research is still rather limited in the algorithmic aspect. To the best of our knowledge, most existing approaches are just simple extensions of their counterparts for 2D and 3D cases, and often suffer from the issues such as high computational complexities. In this paper, we propose an effective framework to compress the high dimensional geometric patterns. Any existing alignment method can be applied to the compressed geometric patterns and the time complexity can be significantly reduced. Our idea is inspired by the observation that high dimensional data often has a low intrinsic dimension. Our framework is a ``data-dependent'' approach that has the complexity depending 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;DualNets&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#24555;&#36895;&#23398;&#20064;&#21644;&#24930;&#36895;&#23398;&#20064;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2209.02370</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#65292;&#24555;&#19982;&#24930;
&lt;/p&gt;
&lt;p&gt;
Continual Learning, Fast and Slow. (arXiv:2209.02370v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02370
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;DualNets&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#24555;&#36895;&#23398;&#20064;&#21644;&#24930;&#36895;&#23398;&#20064;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#34917;&#20805;&#23398;&#20064;&#31995;&#32479;&#65288;CLS&#65289;&#29702;&#35770;&#65292;&#20154;&#31867;&#36890;&#36807;&#20004;&#20010;&#20114;&#34917;&#31995;&#32479;&#26377;&#25928;&#22320;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65306;&#19968;&#20010;&#20197;&#28023;&#39532;&#20307;&#20026;&#20013;&#24515;&#30340;&#24555;&#36895;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#24555;&#36895;&#23398;&#20064;&#20855;&#20307;&#30340;&#20010;&#20307;&#32463;&#39564;&#65307;&#19968;&#20010;&#20301;&#20110;&#26032;&#30382;&#36136;&#30340;&#24930;&#36895;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#36880;&#28176;&#33719;&#21462;&#20851;&#20110;&#29615;&#22659;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;&#21463;&#21040;&#36825;&#19968;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DualNets&#65288;&#21452;&#32593;&#32476;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#30001;&#19968;&#20010;&#24555;&#36895;&#23398;&#20064;&#31995;&#32479;&#21644;&#19968;&#20010;&#24930;&#36895;&#23398;&#20064;&#31995;&#32479;&#32452;&#25104;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20174;&#29305;&#23450;&#20219;&#21153;&#20013;&#36827;&#34892;&#27169;&#24335;&#20998;&#31163;&#34920;&#31034;&#30340;&#30417;&#30563;&#23398;&#20064;&#21644;&#20174;&#20219;&#21153;&#26080;&#20851;&#30340;&#19968;&#33324;&#34920;&#31034;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;DualNets&#21487;&#20197;&#26080;&#32541;&#22320;&#23558;&#36825;&#20004;&#31181;&#34920;&#31034;&#31867;&#22411;&#21512;&#24182;&#21040;&#19968;&#20010;&#25972;&#20307;&#26694;&#26550;&#20013;&#65292;&#20197;&#20419;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
According to the Complementary Learning Systems (CLS) theory~\cite{mcclelland1995there} in neuroscience, humans do effective \emph{continual learning} through two complementary systems: a fast learning system centered on the hippocampus for rapid learning of the specifics, individual experiences; and a slow learning system located in the neocortex for the gradual acquisition of structured knowledge about the environment. Motivated by this theory, we propose \emph{DualNets} (for Dual Networks), a general continual learning framework comprising a fast learning system for supervised learning of pattern-separated representation from specific tasks and a slow learning system for representation learning of task-agnostic general representation via Self-Supervised Learning (SSL). DualNets can seamlessly incorporate both representation types into a holistic framework to facilitate better continual learning in deep neural networks. Via extensive experiments, we demonstrate the promising results 
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#21487;&#20197;&#23545;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#20135;&#29983;&#38750;&#21333;&#35843;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#23569;&#37327;&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;</title><link>http://arxiv.org/abs/2208.10967</link><description>&lt;p&gt;
&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
The Value of Out-of-Distribution Data. (arXiv:2208.10967v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10967
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#21487;&#20197;&#23545;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#20135;&#29983;&#38750;&#21333;&#35843;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#23569;&#37327;&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26399;&#26395;&#38543;&#30528;&#31867;&#20284;&#20219;&#21153;&#26679;&#26412;&#30340;&#22686;&#21152;&#65292;&#27867;&#21270;&#35823;&#24046;&#20250;&#20943;&#23567;&#65307;&#32780;&#38543;&#30528;&#26469;&#33258;&#19981;&#21516;&#20998;&#24067;&#65288;OOD&#65289;&#20219;&#21153;&#26679;&#26412;&#30340;&#22686;&#21152;&#65292;&#27867;&#21270;&#35823;&#24046;&#20250;&#22686;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#21453;&#30452;&#35273;&#30340;&#29616;&#35937;&#65306;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#21487;&#20197;&#26159;&#26679;&#26412;&#20174;OOD&#20219;&#21153;&#20013;&#30340;&#25968;&#37327;&#30340;&#38750;&#21333;&#35843;&#20989;&#25968;&#12290;&#38543;&#30528;OOD&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30446;&#26631;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#22312;&#36229;&#36807;&#19968;&#20010;&#38408;&#20540;&#20043;&#21069;&#20250;&#20808;&#20943;&#23567;&#21518;&#22686;&#22823;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#20351;&#29992;&#23569;&#37327;OOD&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;Fisher&#32447;&#24615;&#21028;&#21035;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#22914;MNIST&#12289;CIFAR-10&#12289;CINIC-10&#12289;PACS&#21644;DomainNet&#65289;&#19978;&#30340;&#28145;&#24230;&#32593;&#32476;&#26469;&#23637;&#31034;&#21644;&#20998;&#26512;&#36825;&#19968;&#29616;&#35937;&#12290;&#22312;&#25105;&#20204;&#30693;&#36947;&#21738;&#20123;&#26679;&#26412;&#23646;&#20110;OOD&#30340;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#21033;&#29992;&#30446;&#26631;&#21644;OOD&#32463;&#39564;&#39118;&#38505;&#30340;&#36866;&#24403;&#21152;&#26435;&#30446;&#26631;&#26469;&#21033;&#29992;&#36825;&#20123;&#38750;&#21333;&#35843;&#36235;&#21183;&#12290;&#23613;&#31649;&#23454;&#38469;&#24212;&#29992;&#26377;&#38480;&#65292;&#20294;&#36825;&#34920;&#26126;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#26816;&#27979;&#21040;OOD&#26679;&#26412;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expect the generalization error to improve with more samples from a similar task, and to deteriorate with more samples from an out-of-distribution (OOD) task. In this work, we show a counter-intuitive phenomenon: the generalization error of a task can be a non-monotonic function of the number of OOD samples. As the number of OOD samples increases, the generalization error on the target task improves before deteriorating beyond a threshold. In other words, there is value in training on small amounts of OOD data. We use Fisher's Linear Discriminant on synthetic datasets and deep networks on computer vision benchmarks such as MNIST, CIFAR-10, CINIC-10, PACS and DomainNet to demonstrate and analyze this phenomenon. In the idealistic setting where we know which samples are OOD, we show that these non-monotonic trends can be exploited using an appropriately weighted objective of the target and OOD empirical risk. While its practical utility is limited, this does suggest that if we can det
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#27979;&#35797;&#26041;&#27861;&#65292;&#31216;&#20026;&#22270;&#28789;&#23454;&#39564;&#65288;TE&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#33021;&#21147;&#12290;&#35770;&#25991;&#36890;&#36807;&#27169;&#25311;&#32463;&#27982;&#23398;&#12289;&#24515;&#29702;&#35821;&#35328;&#23398;&#21644;&#31038;&#20250;&#24515;&#29702;&#23398;&#23454;&#39564;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#24050;&#26377;&#30740;&#31350;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#31181;&#8220;&#36229;&#20934;&#30830;&#24230;&#25197;&#26354;&#8221;&#30340;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2208.10264</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#22810;&#20010;&#20154;&#24182;&#22797;&#21046;&#20154;&#31867;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies. (arXiv:2208.10264v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10264
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#27979;&#35797;&#26041;&#27861;&#65292;&#31216;&#20026;&#22270;&#28789;&#23454;&#39564;&#65288;TE&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#33021;&#21147;&#12290;&#35770;&#25991;&#36890;&#36807;&#27169;&#25311;&#32463;&#27982;&#23398;&#12289;&#24515;&#29702;&#35821;&#35328;&#23398;&#21644;&#31038;&#20250;&#24515;&#29702;&#23398;&#23454;&#39564;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#24050;&#26377;&#30740;&#31350;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#31181;&#8220;&#36229;&#20934;&#30830;&#24230;&#25197;&#26354;&#8221;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#22270;&#28789;&#23454;&#39564;&#65288;TE&#65289;&#30340;&#26032;&#22411;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#32473;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT&#27169;&#22411;&#65289;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;TE&#36824;&#21487;&#20197;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#29305;&#23450;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#25197;&#26354;&#12290;&#19982;&#22270;&#28789;&#27979;&#35797;&#19981;&#21516;&#65292;&#22270;&#28789;&#23454;&#39564;&#38656;&#35201;&#27169;&#25311;&#20195;&#34920;&#24615;&#21442;&#19982;&#20154;&#31867;&#21463;&#35797;&#32773;&#30740;&#31350;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#34892;TEs&#65292;&#35797;&#22270;&#22797;&#21046;&#20043;&#21069;&#30740;&#31350;&#20013;&#30830;&#31435;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#27169;&#25311;TEs&#30340;&#26041;&#27861;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#20854;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22810;&#22823;&#31243;&#24230;&#19978;&#22797;&#21046;&#32463;&#20856;&#32463;&#27982;&#23398;&#12289;&#24515;&#29702;&#35821;&#35328;&#23398;&#21644;&#31038;&#20250;&#24515;&#29702;&#23398;&#23454;&#39564;&#30340;&#33021;&#21147;&#65306;&#26368;&#32456;&#28216;&#25103;&#12289;&#22253;&#36335;&#21477;&#23376;&#12289;&#31859;&#23572;&#26684;&#25289;&#22982;&#30005;&#20987;&#23454;&#39564;&#21644;&#20247;&#20154;&#30340;&#26234;&#24935;&#12290;&#22312;&#21069;&#19977;&#20010;TEs&#20013;&#65292;&#20351;&#29992;&#26368;&#26032;&#27169;&#22411;&#25104;&#21151;&#22797;&#21046;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#32780;&#26368;&#21518;&#19968;&#20010;TE&#25581;&#31034;&#20102;&#23384;&#22312;&#20110;&#26368;&#26032;&#27169;&#22411;&#20013;&#30340;&#8220;&#36229;&#20934;&#30830;&#24230;&#25197;&#26354;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a "hyper-accuracy distortion" present in
&lt;/p&gt;</description></item><item><title>&#21452;&#25293;&#21334;&#24066;&#22330;&#20013;&#24341;&#20837;&#20102;&#21452;&#21521;&#20449;&#24687;&#21453;&#39304;&#65292;&#36890;&#36807;&#32622;&#20449;&#30028;&#31454;&#26631;&#21644;&#24179;&#22343;&#23450;&#20215;&#65292;&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#30340;&#20215;&#26684;&#21457;&#29616;&#21644;&#38477;&#20302;&#21442;&#19982;&#32773;&#30340;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2208.06536</link><description>&lt;p&gt;
&#21452;&#21521;&#20449;&#24687;&#21453;&#39304;&#30340;&#21452;&#25293;&#21334;
&lt;/p&gt;
&lt;p&gt;
Double Auctions with Two-sided Bandit Feedback. (arXiv:2208.06536v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06536
&lt;/p&gt;
&lt;p&gt;
&#21452;&#25293;&#21334;&#24066;&#22330;&#20013;&#24341;&#20837;&#20102;&#21452;&#21521;&#20449;&#24687;&#21453;&#39304;&#65292;&#36890;&#36807;&#32622;&#20449;&#30028;&#31454;&#26631;&#21644;&#24179;&#22343;&#23450;&#20215;&#65292;&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#30340;&#20215;&#26684;&#21457;&#29616;&#21644;&#38477;&#20302;&#21442;&#19982;&#32773;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#25293;&#21334;&#20351;&#24471;&#22810;&#20010;&#20080;&#23478;&#21644;&#21334;&#23478;&#22312;&#21435;&#20013;&#24515;&#21270;&#30340;&#29615;&#22659;&#19979;&#36827;&#34892;&#21830;&#21697;&#20132;&#26131;&#65292;&#20174;&#32780;&#25903;&#25745;&#35768;&#22810;&#22312;&#32447;&#24066;&#22330;&#30340;&#36816;&#20316;&#12290;&#22312;&#36825;&#20123;&#24066;&#22330;&#20013;&#65292;&#20080;&#23478;&#21644;&#21334;&#23478;&#36890;&#36807;&#31454;&#26631;&#26469;&#31454;&#20105;&#65292;&#20294;&#24448;&#24448;&#19981;&#20107;&#20808;&#30693;&#36947;&#33258;&#24049;&#30340;&#35780;&#20272;&#20215;&#20540;&#12290;&#30001;&#20110;&#20998;&#37197;&#21644;&#23450;&#20215;&#26159;&#36890;&#36807;&#31454;&#26631;&#36827;&#34892;&#30340;&#65292;&#21442;&#19982;&#32773;&#30340;&#30408;&#21033;&#33021;&#21147;&#20197;&#21450;&#24066;&#22330;&#30340;&#21487;&#25345;&#32493;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#36890;&#36807;&#37325;&#22797;&#20114;&#21160;&#26469;&#23398;&#20064;&#21508;&#33258;&#35780;&#20272;&#20215;&#20540;&#12290;&#25105;&#20204;&#22312;&#20080;&#23478;&#21644;&#21334;&#23478;&#21452;&#26041;&#30340;&#20449;&#24687;&#21453;&#39304;&#19978;&#23545;&#21452;&#25293;&#21334;&#24066;&#22330;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#32622;&#20449;&#30028;&#31454;&#26631;&#21644;&#8220;&#24179;&#22343;&#23450;&#20215;&#8221;&#30340;&#26377;&#25928;&#20215;&#26684;&#21457;&#29616;&#22312;&#21442;&#19982;&#32773;&#20043;&#38388;&#12290;&#29305;&#21035;&#22320;&#65292;&#20080;&#23478;&#21644;&#21334;&#23478;&#30340;&#32508;&#21512;&#35780;&#20272;&#20215;&#20540;&#65288;&#20063;&#31216;&#20026;&#31038;&#20250;&#36951;&#25022;&#65289;&#22312;T&#36718;&#20013;&#30340;&#36951;&#25022;&#20026;$O(\log(T)/\Delta)$&#65292;&#20854;&#20013;$\Delta$&#20026;&#26368;&#23567;&#20215;&#26684;&#24046;&#12290;&#27492;&#22806;&#65292;&#20132;&#25442;&#36135;&#29289;&#30340;&#20080;&#23478;&#21644;&#21334;&#23478;&#22312;&#20010;&#20307;&#19978;&#36798;&#21040;&#20102;$O(\sqrt{T})$&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
Double Auction enables decentralized transfer of goods between multiple buyers and sellers, thus underpinning functioning of many online marketplaces. Buyers and sellers compete in these markets through bidding, but do not often know their own valuation a-priori. As the allocation and pricing happens through bids, the profitability of participants, hence sustainability of such markets, depends crucially on learning respective valuations through repeated interactions. We initiate the study of Double Auction markets under bandit feedback on both buyers' and sellers' side. We show with confidence bound based bidding, and `Average Pricing' there is an efficient price discovery among the participants. In particular, the regret on combined valuation of the buyers and the sellers -- a.k.a. the social regret -- is $O(\log(T)/\Delta)$ in $T$ rounds, where $\Delta$ is the minimum price gap. Moreover, the buyers and sellers exchanging goods attain $O(\sqrt{T})$ regret, individually. The buyers an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#39046;&#22495;&#27867;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#32447;&#20105;&#35758;&#26368;&#23567;&#21270;&#23545;&#28304;&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#24615;&#20462;&#25913;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#39046;&#22495;&#27867;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.01996</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#39046;&#22495;&#27867;&#21270;&#36890;&#36807;&#22312;&#32447;&#20105;&#35758;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Domain Generalization via Online Disagreement Minimization. (arXiv:2208.01996v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#39046;&#22495;&#27867;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#32447;&#20105;&#35758;&#26368;&#23567;&#21270;&#23545;&#28304;&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#24615;&#20462;&#25913;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#39046;&#22495;&#27867;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#37096;&#32626;&#21644;&#35757;&#32451;&#20043;&#38388;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#26088;&#22312;&#36890;&#36807;&#20165;&#20381;&#36182;&#19968;&#32452;&#28304;&#39046;&#22495;&#23558;&#27169;&#22411;&#23433;&#20840;&#22320;&#36716;&#31227;&#21040;&#26410;&#30693;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;DG&#26041;&#27861;&#65292;&#20294;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#31216;&#20026;DomainBed&#25581;&#31034;&#20102;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#27809;&#26377;&#36229;&#36807;&#31616;&#21333;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#19982;&#29616;&#26377;&#30340;DG&#31639;&#27861;&#27491;&#20132;&#65292;&#24182;&#21487;&#20197;&#25345;&#32493;&#25913;&#36827;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#19982;&#20808;&#21069;&#30340;DG&#24037;&#20316;&#19981;&#21516;&#65292;&#35813;&#24037;&#20316;&#22522;&#20110;&#19968;&#20010;&#38745;&#24577;&#30340;&#28304;&#27169;&#22411;&#24076;&#26395;&#23427;&#26159;&#36890;&#29992;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;AdaODM&#22312;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#22320;&#20462;&#25913;&#28304;&#27169;&#22411;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#20849;&#20139;&#30340;&#39046;&#22495;&#36890;&#29992;&#29305;&#24449;&#25552;&#21462;&#22120;&#19978;&#21019;&#24314;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#20998;&#31867;&#22120;&#12290;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#29305;&#24449;&#25552;&#21462;&#22120;&#23884;&#20837;&#36755;&#20837;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks suffer from significant performance deterioration when there exists distribution shift between deployment and training. Domain Generalization (DG) aims to safely transfer a model to unseen target domains by only relying on a set of source domains. Although various DG approaches have been proposed, a recent study named DomainBed, reveals that most of them do not beat the simple Empirical Risk Minimization (ERM). To this end, we propose a general framework that is orthogonal to existing DG algorithms and could improve their performance consistently. Unlike previous DG works that stake on a static source model to be hopefully a universal one, our proposed AdaODM adaptively modifies the source model at test time for different target domains. Specifically, we create multiple domain-specific classifiers upon a shared domain-generic feature extractor. The feature extractor and classifiers are trained in an adversarial way, where the feature extractor embeds the input samp
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26799;&#24230;&#30340;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#20803;&#30693;&#35782;&#25552;&#21462;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#21452;&#23618;&#20248;&#21270;&#30340;&#23450;&#20041;&#12289;&#36866;&#29992;&#26631;&#20934;&#20197;&#21450;&#23454;&#29992;&#25351;&#21335;&#65292;&#23545;&#20110;&#21021;&#23398;&#32773;&#23588;&#20854;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2207.11719</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#21452;&#23618;&#20248;&#21270;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Gradient-based Bi-level Optimization for Deep Learning: A Survey. (arXiv:2207.11719v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11719
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#20803;&#30693;&#35782;&#25552;&#21462;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#21452;&#23618;&#20248;&#21270;&#30340;&#23450;&#20041;&#12289;&#36866;&#29992;&#26631;&#20934;&#20197;&#21450;&#23454;&#29992;&#25351;&#21335;&#65292;&#23545;&#20110;&#21021;&#23398;&#32773;&#23588;&#20854;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#20803;&#30693;&#35782;&#25552;&#21462;&#12290;&#21452;&#23618;&#20248;&#21270;&#23558;&#19968;&#20010;&#38382;&#39064;&#23884;&#22871;&#22312;&#21478;&#19968;&#20010;&#38382;&#39064;&#20043;&#20869;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#36229;&#26799;&#24230;&#26469;&#35299;&#20915;&#22806;&#23618;&#20219;&#21153;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#22914;&#36827;&#21270;&#31639;&#27861;&#26356;&#39640;&#25928;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#21452;&#23618;&#20248;&#21270;&#30340;&#24418;&#24335;&#21270;&#23450;&#20041;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30028;&#23450;&#20102;&#20915;&#23450;&#30740;&#31350;&#38382;&#39064;&#26159;&#21542;&#36866;&#21512;&#21452;&#23618;&#20248;&#21270;&#30340;&#26631;&#20934;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#25351;&#21335;&#65292;&#25351;&#23548;&#23558;&#36825;&#20123;&#38382;&#39064;&#32467;&#26500;&#21270;&#20026;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#65292;&#36825;&#23545;&#20110;&#26032;&#25163;&#26469;&#35828;&#23588;&#20026;&#26377;&#30410;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26377;&#20004;&#31181;&#24418;&#24335;&#65306;&#21333;&#20219;&#21153;&#24418;&#24335;&#29992;&#20110;&#20248;&#21270;&#27491;&#21017;&#21270;&#21442;&#25968;&#21644;&#32463;&#36807;&#25552;&#21462;&#30340;&#25968;&#25454;&#65292;&#22810;&#20219;&#21153;&#24418;&#24335;&#29992;&#20110;&#25552;&#21462;&#20803;&#30693;&#35782;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bi-level optimization, especially the gradient-based category, has been widely used in the deep learning community including hyperparameter optimization and meta-knowledge extraction. Bi-level optimization embeds one problem within another and the gradient-based category solves the outer-level task by computing the hypergradient, which is much more efficient than classical methods such as the evolutionary algorithm. In this survey, we first give a formal definition of the gradient-based bi-level optimization. Next, we delineate criteria to determine if a research problem is apt for bi-level optimization and provide a practical guide on structuring such problems into a bi-level optimization framework, a feature particularly beneficial for those new to this domain. More specifically, there are two formulations: the single-task formulation to optimize hyperparameters such as regularization parameters and the distilled data, and the multi-task formulation to extract meta-knowledge such as 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#22823;&#25968;&#25454;&#12289;&#36229;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#65292;BNN&#30340;&#21518;&#39564;&#20855;&#26377;&#26799;&#24230;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#23545;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#33030;&#24369;&#24615;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2207.06154</link><description>&lt;p&gt;
&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Bayesian Neural Networks to Adversarial Attacks. (arXiv:2207.06154v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#22823;&#25968;&#25454;&#12289;&#36229;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#65292;BNN&#30340;&#21518;&#39564;&#20855;&#26377;&#26799;&#24230;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#23545;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#33030;&#24369;&#24615;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#26159;&#28145;&#24230;&#23398;&#20064;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#21644;&#29702;&#35770;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#35757;&#32451;&#20986;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#25968;&#25454;&#12289;&#36229;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#20960;&#20309;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#20010;&#26497;&#38480;&#19979;&#65292;&#26799;&#24230;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#26159;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#36864;&#21270;&#23548;&#33268;&#30340;&#65292;&#20063;&#23601;&#26159;&#24403;&#25968;&#25454;&#20301;&#20110;&#29615;&#22659;&#31354;&#38388;&#30340;&#19968;&#20010;&#20302;&#32500;&#23376;&#27969;&#24418;&#19978;&#26102;&#12290;&#20316;&#20026;&#30452;&#25509;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36825;&#20010;&#26497;&#38480;&#19979;&#65292;BNN&#30340;&#21518;&#39564;&#23545;&#26799;&#24230;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#20174;&#21518;&#39564;&#20013;&#37319;&#26679;&#30340;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#23545;&#26799;&#24230;&#25915;&#20987;&#37117;&#20855;&#26377;&#33030;&#24369;&#24615;&#65292;&#25439;&#22833;&#20989;&#25968;&#23545;BNN&#21518;&#39564;&#20998;&#24067;&#30340;&#26399;&#26395;&#26799;&#24230;&#20173;&#28982;&#36235;&#20110;&#38646;&#12290;&#22312;t&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vulnerability to adversarial attacks is one of the principal hurdles to the adoption of deep learning in safety-critical applications. Despite significant efforts, both practical and theoretical, training deep learning models robust to adversarial attacks is still an open problem. In this paper, we analyse the geometry of adversarial attacks in the large-data, overparameterized limit for Bayesian Neural Networks (BNNs). We show that, in the limit, vulnerability to gradient-based attacks arises as a result of degeneracy in the data distribution, i.e., when the data lies on a lower-dimensional submanifold of the ambient space. As a direct consequence, we demonstrate that in this limit BNN posteriors are robust to gradient-based adversarial attacks. Crucially, we prove that the expected gradient of the loss with respect to the BNN posterior distribution is vanishing, even when each neural network sampled from the posterior is vulnerable to gradient-based attacks. Experimental results on t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#21363;&#29369;&#35947;&#26641;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#23398;&#20064;&#65292;&#25512;&#29702;&#21644;&#25552;&#20379;&#24378;&#20581;&#30340;&#26631;&#31614;&#20998;&#24067;&#65292;&#21487;&#29992;&#20110;&#20854;&#20182;&#25512;&#29702;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2206.12252</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#19979;&#23398;&#20064;&#22522;&#20110;&#35770;&#35777;&#30340;&#25512;&#29702;&#30340;&#29369;&#35947;&#26641;
&lt;/p&gt;
&lt;p&gt;
Indecision Trees: Learning Argument-Based Reasoning under Quantified Uncertainty. (arXiv:2206.12252v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#21363;&#29369;&#35947;&#26641;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#23398;&#20064;&#65292;&#25512;&#29702;&#21644;&#25552;&#20379;&#24378;&#20581;&#30340;&#26631;&#31614;&#20998;&#24067;&#65292;&#21487;&#29992;&#20110;&#20854;&#20182;&#25512;&#29702;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24448;&#24448;&#23384;&#22312;&#38382;&#39064;&#65292;&#27604;&#22914;&#19981;&#21487;&#35299;&#37322;&#30340;&#40657;&#30418;&#27169;&#22411;&#65292;&#19981;&#23436;&#32654;&#27979;&#37327;&#30340;&#30830;&#23450;&#24615;&#20551;&#35774;&#65292;&#25110;&#32773;&#21482;&#25552;&#20379;&#21333;&#19968;&#20998;&#31867;&#32780;&#19981;&#26159;&#27010;&#29575;&#20998;&#24067;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29369;&#35947;&#26641;&#65292;&#19968;&#31181;&#23545;&#20915;&#31574;&#26641;&#36827;&#34892;&#20462;&#25913;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#23398;&#20064;&#65292;&#33021;&#22815;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#25512;&#29702;&#65292;&#25552;&#20379;&#19968;&#20010;&#24378;&#20581;&#30340;&#26631;&#31614;&#20998;&#24067;&#65292;&#24182;&#21487;&#20998;&#35299;&#20026;&#19968;&#32452;&#36923;&#36753;&#35770;&#35777;&#29992;&#20110;&#20854;&#20182;&#25512;&#29702;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using Machine Learning systems in the real world can often be problematic, with inexplicable black-box models, the assumed certainty of imperfect measurements, or providing a single classification instead of a probability distribution.  This paper introduces Indecision Trees, a modification to Decision Trees which learn under uncertainty, can perform inference under uncertainty, provide a robust distribution over the possible labels, and can be disassembled into a set of logical arguments for use in other reasoning systems.
&lt;/p&gt;</description></item><item><title>Survival Kernets &#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#26680;&#29983;&#23384;&#20998;&#26512;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27169;&#22411;&#35299;&#37322;&#21644;&#29702;&#35770;&#20998;&#26512;&#12290;&#23427;&#21033;&#29992;&#26680;&#20989;&#25968;&#20272;&#35745;&#20010;&#20307;&#30340;&#29983;&#23384;&#20998;&#24067;&#65292;&#36890;&#36807;&#35757;&#32451;&#38598;&#21387;&#32553;&#26041;&#26696;&#36827;&#34892;&#25968;&#25454;&#20998;&#31751;&#65292;&#22240;&#27492;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#35270;&#21270;&#33021;&#21147;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20445;&#35777;&#12290;&#35813;&#27169;&#22411;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#30340;&#39044;&#27979;&#29983;&#23384;&#20998;&#24067;&#35823;&#24046;&#30028;&#38480;&#26368;&#20248;&#65292;&#19988;&#22312;&#27979;&#35797;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.10477</link><description>&lt;p&gt;
Survival Kernets: &#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#26680;&#29983;&#23384;&#20998;&#26512;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#20934;&#30830;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Survival Kernets: Scalable and Interpretable Deep Kernel Survival Analysis with an Accuracy Guarantee. (arXiv:2206.10477v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10477
&lt;/p&gt;
&lt;p&gt;
Survival Kernets &#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#26680;&#29983;&#23384;&#20998;&#26512;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27169;&#22411;&#35299;&#37322;&#21644;&#29702;&#35770;&#20998;&#26512;&#12290;&#23427;&#21033;&#29992;&#26680;&#20989;&#25968;&#20272;&#35745;&#20010;&#20307;&#30340;&#29983;&#23384;&#20998;&#24067;&#65292;&#36890;&#36807;&#35757;&#32451;&#38598;&#21387;&#32553;&#26041;&#26696;&#36827;&#34892;&#25968;&#25454;&#20998;&#31751;&#65292;&#22240;&#27492;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#35270;&#21270;&#33021;&#21147;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20445;&#35777;&#12290;&#35813;&#27169;&#22411;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#30340;&#39044;&#27979;&#29983;&#23384;&#20998;&#24067;&#35823;&#24046;&#30028;&#38480;&#26368;&#20248;&#65292;&#19988;&#22312;&#27979;&#35797;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#29983;&#23384;&#20998;&#26512;&#27169;&#22411;&#36890;&#36807;&#26680;&#20989;&#25968;&#26469;&#20272;&#35745;&#20010;&#20307;&#30340;&#29983;&#23384;&#20998;&#24067;&#65292;&#26680;&#20989;&#25968;&#24230;&#37327;&#20219;&#24847;&#20004;&#20010;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26680;&#29983;&#23384;&#27169;&#22411;&#8212;&#8212;&#29983;&#23384;kernet&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26131;&#20110;&#35299;&#37322;&#21644;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35757;&#32451;&#25968;&#25454;&#26681;&#25454;&#19968;&#31181;&#26368;&#36817;&#21457;&#23637;&#30340;&#29992;&#20110;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#35757;&#32451;&#38598;&#21387;&#32553;&#26041;&#26696;&#65288;&#31216;&#20026;&#26680;&#32676;&#65289;&#36827;&#34892;&#20998;&#31751;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#27599;&#20010;&#25968;&#25454;&#28857;&#34987;&#34920;&#31034;&#20026;&#36825;&#20123;&#31751;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#27599;&#20010;&#31751;&#21487;&#20197;&#36827;&#34892;&#21487;&#35270;&#21270;&#23637;&#31034;&#12290;&#23545;&#20110;&#29983;&#23384;kernet&#30340;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#30028;&#38480;&#65292;&#39044;&#27979;&#30340;&#29983;&#23384;&#20998;&#24067;&#22312;&#35813;&#30028;&#38480;&#19979;&#26159;&#26368;&#20248;&#30340;&#65288;&#38500;&#21435;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#22312;&#27979;&#35797;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel survival analysis models estimate individual survival distributions with the help of a kernel function, which measures the similarity between any two data points. Such a kernel function can be learned using deep kernel survival models. In this paper, we present a new deep kernel survival model called a survival kernet, which scales to large datasets in a manner that is amenable to model interpretation and also theoretical analysis. Specifically, the training data are partitioned into clusters based on a recently developed training set compression scheme for classification and regression called kernel netting that we extend to the survival analysis setting. At test time, each data point is represented as a weighted combination of these clusters, and each such cluster can be visualized. For a special case of survival kernets, we establish a finite-sample error bound on predicted survival distributions that is, up to a log factor, optimal. Whereas scalability at test time is achiev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MetaFed&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#19981;&#21516;&#30340;&#32852;&#37030;&#20043;&#38388;&#23454;&#29616;&#21487;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#36890;&#36807;&#24490;&#29615;&#30693;&#35782;&#33976;&#39311;&#65292;MetaFed&#33021;&#22815;&#33719;&#21462;&#27599;&#20010;&#32852;&#37030;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#26080;&#38656;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;MetaFed&#22312;&#31934;&#24230;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.08516</link><description>&lt;p&gt;
MetaFed: &#20010;&#24615;&#21270;&#21307;&#30103;&#20013;&#22522;&#20110;&#24490;&#29615;&#30693;&#35782;&#33976;&#39311;&#30340;&#36328;&#32852;&#37030;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MetaFed: Federated Learning among Federations with Cyclic Knowledge Distillation for Personalized Healthcare. (arXiv:2206.08516v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MetaFed&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#19981;&#21516;&#30340;&#32852;&#37030;&#20043;&#38388;&#23454;&#29616;&#21487;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#36890;&#36807;&#24490;&#29615;&#30693;&#35782;&#33976;&#39311;&#65292;MetaFed&#33021;&#22815;&#33719;&#21462;&#27599;&#20010;&#32852;&#37030;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#26080;&#38656;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;MetaFed&#22312;&#31934;&#24230;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#24314;&#31435;&#27169;&#22411;&#26102;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#19981;&#21516;&#30340;&#32852;&#37030;&#24456;&#23569;&#22240;&#20026;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#19981;&#20449;&#20219;/&#19981;&#23384;&#22312;&#31561;&#21407;&#22240;&#32780;&#20849;&#21516;&#24037;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MetaFed&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#20419;&#36827;&#19981;&#21516;&#32852;&#37030;&#20043;&#38388;&#21487;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#24490;&#29615;&#30693;&#35782;&#33976;&#39311;&#65292;MetaFed&#22312;&#27809;&#26377;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#24773;&#20917;&#19979;&#20026;&#27599;&#20010;&#32852;&#37030;&#33719;&#21462;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaFed&#23558;&#27599;&#20010;&#32852;&#37030;&#35270;&#20026;&#19968;&#20010;&#20803;&#20998;&#24067;&#65292;&#24182;&#20197;&#24490;&#29615;&#30340;&#26041;&#24335;&#32858;&#21512;&#27599;&#20010;&#32852;&#37030;&#30340;&#30693;&#35782;&#12290;&#35757;&#32451;&#20998;&#20026;&#20004;&#20010;&#37096;&#20998;&#65306;&#20849;&#21516;&#30693;&#35782;&#31215;&#32047;&#21644;&#20010;&#24615;&#21270;&#12290;&#23545;&#19977;&#20010;&#22522;&#20934;&#36827;&#34892;&#30340;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65288;&#20363;&#22914;&#65292;&#19982;&#22522;&#30784;&#26041;&#27861;&#30456;&#27604;&#65292;&#23545;&#20110;PAMAP2&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;10&#65285;+&#65289;&#65292;&#27809;&#26377;&#26381;&#21153;&#22120;&#30340;MetaFed&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has attracted increasing attention to building models without accessing the raw user data, especially in healthcare. In real applications, different federations can seldom work together due to possible reasons such as data heterogeneity and distrust/inexistence of the central server. In this paper, we propose a novel framework called MetaFed to facilitate trustworthy FL between different federations. MetaFed obtains a personalized model for each federation without a central server via the proposed Cyclic Knowledge Distillation. Specifically, MetaFed treats each federation as a meta distribution and aggregates knowledge of each federation in a cyclic manner. The training is split into two parts: common knowledge accumulation and personalization. Comprehensive experiments on three benchmarks demonstrate that MetaFed without a server achieves better accuracy compared to state-of-the-art methods (e.g., 10%+ accuracy improvement compared to the baseline for PAMAP2) with f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22235;&#20010;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#20248;&#21270;&#65288;MILO&#65289;&#27169;&#22411;&#65292;&#26088;&#22312;&#35774;&#35745;&#26368;&#20248;&#30340;&#20108;&#21449;&#20998;&#31867;&#26641;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#27491;&#30830;&#20998;&#31867;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#21644;&#26368;&#23567;&#21270;&#20998;&#25903;&#33410;&#28857;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.04857</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#20248;&#20108;&#21449;&#20998;&#31867;&#26641;&#30340;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#20248;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixed integer linear optimization formulations for learning optimal binary classification trees. (arXiv:2206.04857v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22235;&#20010;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#20248;&#21270;&#65288;MILO&#65289;&#27169;&#22411;&#65292;&#26088;&#22312;&#35774;&#35745;&#26368;&#20248;&#30340;&#20108;&#21449;&#20998;&#31867;&#26641;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#27491;&#30830;&#20998;&#31867;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#21644;&#26368;&#23567;&#21270;&#20998;&#25903;&#33410;&#28857;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#21560;&#24341;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#20915;&#31574;&#26641;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#28857;&#20043;&#19968;&#26159;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#24120;&#20248;&#20808;&#20110;&#20854;&#20182;&#20934;&#30830;&#29575;&#36739;&#39640;&#20294;&#38590;&#20197;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#20108;&#21449;&#20998;&#31867;&#26641;&#21253;&#25324;&#20004;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#65306;&#65288;i&#65289;&#20998;&#25903;&#33410;&#28857;&#65292;&#26377;&#20004;&#20010;&#23401;&#23376;&#33410;&#28857;&#65292;&#22312;&#19968;&#32452;&#31163;&#25955;&#29305;&#24449;&#19978;&#35780;&#20272;&#25968;&#25454;&#28857;&#65307;&#65288;ii&#65289;&#21494;&#33410;&#28857;&#65292;&#32473;&#20986;&#31163;&#25955;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#21452;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#65288;i&#65289;&#26368;&#22823;&#21270;&#27491;&#30830;&#20998;&#31867;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#65292;&#65288;ii&#65289;&#26368;&#23567;&#21270;&#20998;&#25903;&#33410;&#28857;&#25968;&#37327;&#65292;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#26368;&#20248;&#30340;&#20108;&#21449;&#20998;&#31867;&#26641;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#20248;&#21270;&#65288;MILO&#65289;&#27169;&#22411;&#26469;&#35774;&#35745;&#26368;&#20248;&#30340;&#20108;&#21449;&#20998;&#31867;&#26641;&#65306;&#20004;&#20010;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#21644;&#20004;&#20010;&#22522;&#20110;&#21106;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are powerful tools for classification and regression that attract many researchers working in the burgeoning area of machine learning. One advantage of decision trees over other methods is their interpretability, which is often preferred over other higher accuracy methods that are relatively uninterpretable. A binary classification tree has two types of vertices: (i) branching vertices which have exactly two children and where datapoints are assessed on a set of discrete features; and (ii) leaf vertices at which datapoints are given a discrete prediction. An optimal binary classification tree can be obtained by solving a biobjective optimization problem that seeks to (i) maximize the number of correctly classified datapoints and (ii) minimize the number of branching vertices. In this paper, we propose four mixed integer linear optimization (MILO) formulations for designing optimal binary classification trees: two flow-based formulations and two-cut based formulations. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORA&#30340;&#25968;&#25454;&#19981;&#21487;&#30693;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#24182;&#21487;&#20197;&#35782;&#21035;&#19981;&#31526;&#21512;&#20154;&#31867;&#30452;&#35266;&#35748;&#30693;&#30340;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2206.04530</link><description>&lt;p&gt;
DORA&#65306;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#20540;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DORA: Exploring outlier representations in Deep Neural Networks. (arXiv:2206.04530v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORA&#30340;&#25968;&#25454;&#19981;&#21487;&#30693;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#24182;&#21487;&#20197;&#35782;&#21035;&#19981;&#31526;&#21512;&#20154;&#31867;&#30452;&#35266;&#35748;&#30693;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#23398;&#20064;&#22797;&#26434;&#25277;&#35937;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#24847;&#22806;&#22320;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#34394;&#20551;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#30830;&#20445;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#65292;&#26816;&#26597;&#23398;&#20064;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#24847;&#22806;&#30340;&#27010;&#24565;&#24448;&#24448;&#34920;&#29616;&#20026;&#19982;&#25152;&#38656;&#30340;&#20219;&#21153;&#19981;&#31526;&#30340;&#24322;&#24120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DORA&#65288;Data-agnOstic Representation Analysis&#65289;&#65306;&#29992;&#20110;&#20998;&#26512;DNN&#34920;&#31034;&#31354;&#38388;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#19981;&#21487;&#30693;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#25152;&#25552;&#20986;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#26497;&#31471;&#28608;&#27963;&#65288;EA&#65289;&#36317;&#31163;&#24230;&#37327;&#65292;&#22312;&#19981;&#35775;&#38382;&#20219;&#20309;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#32593;&#32476;&#20869;&#33258;&#35828;&#26126;&#33021;&#21147;&#12290;&#25105;&#20204;&#23450;&#37327;&#39564;&#35777;&#20102;&#24230;&#37327;&#30340;&#27491;&#30830;&#24615;&#21644;&#19982;&#20154;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#36317;&#31163;&#30340;&#19968;&#33268;&#24615;&#12290;EA&#36317;&#31163;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#34920;&#24449;&#65292;&#20854;&#22522;&#26412;&#27010;&#24565;&#34987;&#35748;&#20026;&#26159;&#19981;&#33258;&#28982;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Deep Neural Networks (DNNs) are incredibly effective in learning complex abstractions, they are susceptible to unintentionally learning spurious artifacts from the training data. To ensure model transparency, it is crucial to examine the relationships between learned representations, as unintended concepts often manifest themselves to be anomalous to the desired task. In this work, we introduce DORA (Data-agnOstic Representation Analysis): the first data-agnostic framework for the analysis of the representation space of DNNs. Our framework employs the proposed Extreme-Activation (EA) distance measure between representations that utilizes self-explaining capabilities within the network without accessing any data. We quantitatively validate the metric's correctness and alignment with human-defined semantic distances. The coherence between the EA distance and human judgment enables us to identify representations whose underlying concepts would be considered unnatural by humans by
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cal-PIT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#27010;&#29575;-&#27010;&#29575;&#26144;&#23556;&#65292;&#35299;&#20915;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#35786;&#26029;&#21644;&#26657;&#20934;&#38382;&#39064;&#65292;&#26469;&#23454;&#29616;&#26377;&#26465;&#20214;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2205.14568</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;-&#27010;&#29575;&#26144;&#23556;&#23454;&#29616;&#26377;&#26465;&#20214;&#26657;&#20934;&#30340;&#39044;&#27979;&#20998;&#24067;&#65306;&#22312;&#38134;&#27827;&#32418;&#31227;&#20272;&#35745;&#21644;&#27010;&#29575;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conditionally Calibrated Predictive Distributions by Probability-Probability Map: Application to Galaxy Redshift Estimation and Probabilistic Forecasting. (arXiv:2205.14568v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cal-PIT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#27010;&#29575;-&#27010;&#29575;&#26144;&#23556;&#65292;&#35299;&#20915;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#35786;&#26029;&#21644;&#26657;&#20934;&#38382;&#39064;&#65292;&#26469;&#23454;&#29616;&#26377;&#26465;&#20214;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#35780;&#20272;AI&#31639;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25551;&#36848;&#30446;&#26631;&#21464;&#37327;$y \in \mathbb{R}$&#22312;&#32473;&#23450;&#22797;&#26434;&#36755;&#20837;&#29305;&#24449;$\mathbf{x} \in \mathcal{X}$&#30340;&#26465;&#20214;&#19979;&#30340;&#39044;&#27979;&#20998;&#24067;$F(y|\mathbf{x})$&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#27979;&#20998;&#24067;&#65288;&#20363;&#22914;&#65292;&#24402;&#19968;&#21270;&#27969;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65289;&#24448;&#24448;&#32570;&#20047;&#26465;&#20214;&#26657;&#20934;&#65292;&#21363;&#32473;&#23450;&#36755;&#20837;$\mathbf{x}$&#30340;&#20107;&#20214;&#21457;&#29983;&#30340;&#27010;&#29575;&#19982;&#39044;&#27979;&#27010;&#29575;&#26174;&#33879;&#19981;&#21516;&#12290;&#24403;&#21069;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#33021;&#23436;&#20840;&#35780;&#20272;&#21644;&#23454;&#26045;&#26377;&#26465;&#20214;&#26657;&#20934;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cal-PIT&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20174;&#26657;&#20934;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#27010;&#29575;-&#27010;&#29575;&#26144;&#23556;&#26469;&#21516;&#26102;&#35299;&#20915;&#39044;&#27979;&#20998;&#24067;&#30340;&#35786;&#26029;&#21644;&#26657;&#20934;&#38382;&#39064;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23545;&#27010;&#29575;&#31215;&#20998;&#21464;&#25442;&#20998;&#25968;&#36827;&#34892;$\mathbf{x}$&#30340;&#22238;&#24402;&#12290;&#20272;&#35745;&#30340;&#22238;&#24402;&#25552;&#20379;&#20102;&#23545;&#29305;&#24449;&#31354;&#38388;&#20013;&#26465;&#20214;&#35206;&#30422;&#30340;&#21487;&#35299;&#37322;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification is crucial for assessing the predictive ability of AI algorithms. Much research has been devoted to describing the predictive distribution (PD) $F(y|\mathbf{x})$ of a target variable $y \in \mathbb{R}$ given complex input features $\mathbf{x} \in \mathcal{X}$. However, off-the-shelf PDs (from, e.g., normalizing flows and Bayesian neural networks) often lack conditional calibration with the probability of occurrence of an event given input $\mathbf{x}$ being significantly different from the predicted probability. Current calibration methods do not fully assess and enforce conditionally calibrated PDs. Here we propose \texttt{Cal-PIT}, a method that addresses both PD diagnostics and recalibration by learning a single probability-probability map from calibration data. The key idea is to regress probability integral transform scores against $\mathbf{x}$. The estimated regression provides interpretable diagnostics of conditional coverage across the feature space. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeedGNN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#26377;&#30417;&#30563;&#31181;&#23376;&#22270;&#21305;&#37197;&#12290;&#30456;&#27604;&#20110;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;SeedGNN&#33021;&#22815;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#23569;&#37327;&#31181;&#23376;&#33410;&#28857;&#26469;&#21305;&#37197;&#26410;&#35265;&#22270;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#21644;&#21033;&#29992;&#19981;&#21516;&#36339;&#25968;&#30340;&#35777;&#20154;&#20449;&#24687;&#20197;&#21450;&#20351;&#29992;&#26131;&#21305;&#37197;&#30340;&#33410;&#28857;&#23545;&#25913;&#21892;&#21305;&#37197;&#25928;&#26524;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2205.13679</link><description>&lt;p&gt;
SeedGNN&#65306;&#29992;&#20110;&#26377;&#30417;&#30563;&#31181;&#23376;&#22270;&#21305;&#37197;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SeedGNN: Graph Neural Networks for Supervised Seeded Graph Matching. (arXiv:2205.13679v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeedGNN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#26377;&#30417;&#30563;&#31181;&#23376;&#22270;&#21305;&#37197;&#12290;&#30456;&#27604;&#20110;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;SeedGNN&#33021;&#22815;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#23569;&#37327;&#31181;&#23376;&#33410;&#28857;&#26469;&#21305;&#37197;&#26410;&#35265;&#22270;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#21644;&#21033;&#29992;&#19981;&#21516;&#36339;&#25968;&#30340;&#35777;&#20154;&#20449;&#24687;&#20197;&#21450;&#20351;&#29992;&#26131;&#21305;&#37197;&#30340;&#33410;&#28857;&#23545;&#25913;&#21892;&#21305;&#37197;&#25928;&#26524;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#35774;&#35745;&#29992;&#20110;&#26377;&#30417;&#30563;&#31181;&#23376;&#22270;&#21305;&#37197;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34920;&#31034;&#20852;&#36259;&#65292;&#36825;&#26088;&#22312;&#20165;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#21644;&#23569;&#37327;&#31181;&#23376;&#33410;&#28857;&#26469;&#21305;&#37197;&#20004;&#20010;&#26080;&#26631;&#31614;&#22270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#29992;&#20110;&#27492;&#20219;&#21153;&#30340;GNNs&#20351;&#29992;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#36825;&#35201;&#27714;&#22823;&#37327;&#31181;&#23376;&#24182;&#19981;&#33021;&#23398;&#20064;&#21487;&#36716;&#31227;&#21040;&#26410;&#35265;&#22270;&#30340;&#30693;&#35782;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#22914;&#20309;&#20165;&#29992;&#23569;&#37327;&#31181;&#23376;&#26469;&#21305;&#37197;&#26410;&#35265;&#22270;&#12290;&#25105;&#20204;&#30340;SeedGNN&#26550;&#26500;&#34701;&#21512;&#20102;&#20960;&#20010;&#26032;&#39062;&#30340;&#35774;&#35745;&#65292;&#21463;&#26377;&#20851;&#31181;&#23376;&#22270;&#21305;&#37197;&#30340;&#29702;&#35770;&#30740;&#31350;&#30340;&#21551;&#21457;&#65306;1&#65289;&#23427;&#21487;&#20197;&#23398;&#20064;&#35745;&#31639;&#21644;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#36339;&#25968;&#30340;&#31867;&#20284;&#35777;&#20154;&#20449;&#24687;&#65292;&#20197;&#19968;&#31181;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#22823;&#23567;&#30340;&#22270;&#30340;&#26041;&#24335;&#65307;2&#65289;&#23427;&#21487;&#20197;&#20351;&#29992;&#26131;&#21305;&#37197;&#30340;&#33410;&#28857;&#23545;&#20316;&#20026;&#26032;&#30340;&#31181;&#23376;&#65292;&#22312;&#21518;&#32493;&#23618;&#27425;&#20013;&#25913;&#21892;&#21305;&#37197;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#19978;&#35780;&#20272;&#20102;SeedGNN&#65292;&#24182;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in designing Graph Neural Networks (GNNs) for seeded graph matching, which aims to match two unlabeled graphs using only topological information and a small set of seed nodes. However, most previous GNNs for this task use a semi-supervised approach, which requires a large number of seeds and cannot learn knowledge that is transferable to unseen graphs. In contrast, this paper proposes a new supervised approach that can learn from a training set how to match unseen graphs with only a few seeds. Our SeedGNN architecture incorporates several novel designs, inspired by theoretical studies of seeded graph matching: 1) it can learn to compute and use witness-like information from different hops, in a way that can be generalized to graphs of different sizes; 2) it can use easily-matched node-pairs as new seeds to improve the matching in subsequent layers. We evaluate SeedGNN on synthetic and real-world graphs and demonstrate significant performance improvements ove
&lt;/p&gt;</description></item><item><title>DDAC-SpAM&#26159;&#19968;&#31181;&#22312;&#39640;&#32500;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#20013;&#21033;&#29992;&#29305;&#24449;&#21010;&#20998;&#21644;&#21435;&#30456;&#20851;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;&#21435;&#30456;&#20851;&#25805;&#20316;&#20351;&#24471;&#27599;&#20010;&#23616;&#37096;&#20272;&#35745;&#22120;&#33021;&#22815;&#24674;&#22797;&#27599;&#20010;&#21152;&#24615;&#32452;&#20998;&#30340;&#31232;&#30095;&#27169;&#24335;&#65292;&#21516;&#26102;&#19981;&#23545;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#32467;&#26500;&#26045;&#21152;&#20005;&#26684;&#30340;&#32422;&#26463;&#12290;&#35813;&#31639;&#27861;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#20026;&#25311;&#21512;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#25552;&#20379;&#20102;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2205.07932</link><description>&lt;p&gt;
DDAC-SpAM: &#19968;&#31181;&#29992;&#29305;&#24449;&#21010;&#20998;&#21644;&#21435;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#25311;&#21512;&#39640;&#32500;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DDAC-SpAM: A Distributed Algorithm for Fitting High-dimensional Sparse Additive Models with Feature Division and Decorrelation. (arXiv:2205.07932v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07932
&lt;/p&gt;
&lt;p&gt;
DDAC-SpAM&#26159;&#19968;&#31181;&#22312;&#39640;&#32500;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#20013;&#21033;&#29992;&#29305;&#24449;&#21010;&#20998;&#21644;&#21435;&#30456;&#20851;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;&#21435;&#30456;&#20851;&#25805;&#20316;&#20351;&#24471;&#27599;&#20010;&#23616;&#37096;&#20272;&#35745;&#22120;&#33021;&#22815;&#24674;&#22797;&#27599;&#20010;&#21152;&#24615;&#32452;&#20998;&#30340;&#31232;&#30095;&#27169;&#24335;&#65292;&#21516;&#26102;&#19981;&#23545;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#32467;&#26500;&#26045;&#21152;&#20005;&#26684;&#30340;&#32422;&#26463;&#12290;&#35813;&#31639;&#27861;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#20026;&#25311;&#21512;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#25552;&#20379;&#20102;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#32479;&#35745;&#23398;&#20064;&#24050;&#25104;&#20026;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#30340;&#24120;&#29992;&#25216;&#26415;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#26159;&#20851;&#27880;&#35266;&#23519;&#20540;&#30340;&#21010;&#20998;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;DDAC-SpAM&#65292;&#22312;&#39640;&#32500;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#20013;&#23545;&#29305;&#24449;&#36827;&#34892;&#21010;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;&#21010;&#20998;&#12289;&#21435;&#30456;&#20851;&#21644;&#24449;&#26381;&#12290;&#21435;&#30456;&#20851;&#25805;&#20316;&#20351;&#27599;&#20010;&#23616;&#37096;&#20272;&#35745;&#22120;&#33021;&#22815;&#24674;&#22797;&#27599;&#20010;&#21152;&#24615;&#32452;&#20998;&#30340;&#31232;&#30095;&#27169;&#24335;&#65292;&#32780;&#19981;&#23545;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#32467;&#26500;&#26045;&#21152;&#20005;&#26684;&#30340;&#32422;&#26463;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#29702;&#35770;&#32467;&#26524;&#21253;&#25324;&#19968;&#33268;&#30340;&#31232;&#30095;&#27169;&#24335;&#24674;&#22797;&#20197;&#21450;&#23545;&#27599;&#20010;&#21152;&#24615;&#20989;&#25968;&#32452;&#25104;&#37096;&#20998;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#25311;&#21512;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#25552;&#20379;&#20102;&#23454;&#38469;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed statistical learning has become a popular technique for large-scale data analysis. Most existing work in this area focuses on dividing the observations, but we propose a new algorithm, DDAC-SpAM, which divides the features under a high-dimensional sparse additive model. Our approach involves three steps: divide, decorrelate, and conquer. The decorrelation operation enables each local estimator to recover the sparsity pattern for each additive component without imposing strict constraints on the correlation structure among variables. The effectiveness and efficiency of the proposed algorithm are demonstrated through theoretical analysis and empirical results on both synthetic and real data. The theoretical results include both the consistent sparsity pattern recovery as well as statistical inference for each additive functional component. Our approach provides a practical solution for fitting sparse additive models, with promising applications in a wide range of domains.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;KenSwQuAD - &#19968;&#20221;&#36866;&#29992;&#20110;&#26031;&#29926;&#24076;&#37324;&#35821;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26426;&#22120;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#30340;&#38656;&#27714;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#20174;&#26031;&#29926;&#24076;&#37324;&#35821;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21407;&#22987;&#25925;&#20107;&#25991;&#26412;&#20013;&#36827;&#34892;&#27880;&#37322;&#24471;&#21040;&#30340;&#65292;&#21253;&#21547;7,526&#20010;QA&#23545;&#12290;</title><link>http://arxiv.org/abs/2205.02364</link><description>&lt;p&gt;
KenSwQuAD - &#19968;&#20221;&#36866;&#29992;&#20110;&#26031;&#29926;&#24076;&#37324;&#35821;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource Language. (arXiv:2205.02364v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;KenSwQuAD - &#19968;&#20221;&#36866;&#29992;&#20110;&#26031;&#29926;&#24076;&#37324;&#35821;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26426;&#22120;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#30340;&#38656;&#27714;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#20174;&#26031;&#29926;&#24076;&#37324;&#35821;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21407;&#22987;&#25925;&#20107;&#25991;&#26412;&#20013;&#36827;&#34892;&#27880;&#37322;&#24471;&#21040;&#30340;&#65292;&#21253;&#21547;7,526&#20010;QA&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#21160;&#26426;&#26159;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#24320;&#21457;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#26031;&#29926;&#24076;&#37324;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598;KenSwQuAD&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#26031;&#29926;&#24076;&#37324;&#35821;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21407;&#22987;&#25925;&#20107;&#25991;&#26412;&#20013;&#36827;&#34892;&#27880;&#37322;&#30340;&#65292;&#35813;&#35821;&#35328;&#20027;&#35201;&#22312;&#19996;&#38750;&#21644;&#19990;&#30028;&#20854;&#20182;&#22320;&#26041;&#20351;&#29992;&#12290;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#23545;&#20110;&#26426;&#22120;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#30340;&#20219;&#21153;&#65288;&#22914;&#20114;&#32852;&#32593;&#25628;&#32034;&#21644;&#23545;&#35805;&#31995;&#32479;&#65289;&#38750;&#24120;&#37325;&#35201;&#12290;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#65292;&#22914;&#26412;&#30740;&#31350;&#24320;&#21457;&#30340;&#40644;&#37329;&#26631;&#20934;&#38382;&#31572;&#38598;&#12290;&#26412;&#39033;&#30446;&#32856;&#35831;&#20102;&#27880;&#35299;&#21592;&#20174;Kencorpus&#39033;&#30446;&#25910;&#38598;&#30340;&#26031;&#29926;&#24076;&#37324;&#35821;&#25991;&#26412;&#20013;&#21046;&#23450;QA&#23545;&#12290;&#35813;&#39033;&#30446;&#23545;&#24635;&#20849;2,585&#20010;&#25991;&#26412;&#36827;&#34892;&#20102;1,445&#20010;&#25991;&#26412;&#30340;&#27880;&#37322;&#65292;&#27599;&#20010;&#27880;&#37322;&#33267;&#23569;&#26377;5&#20010;QA&#23545;&#65292;&#26368;&#32456;&#24418;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;7,526&#20010;QA&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#23545;12.5%&#30340;&#27880;&#37322;&#25991;&#26412;&#36827;&#34892;&#30340;&#36136;&#37327;&#20445;&#35777;&#30830;&#35748;&#20102;QA&#23545;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for Question Answering datasets in low resource languages is the motivation of this research, leading to the development of Kencorpus Swahili Question Answering Dataset, KenSwQuAD. This dataset is annotated from raw story texts of Swahili low resource language, which is a predominantly spoken in Eastern African and in other parts of the world. Question Answering (QA) datasets are important for machine comprehension of natural language for tasks such as internet search and dialog systems. Machine learning systems need training data such as the gold standard Question Answering set developed in this research. The research engaged annotators to formulate QA pairs from Swahili texts collected by the Kencorpus project, a Kenyan languages corpus. The project annotated 1,445 texts from the total 2,585 texts with at least 5 QA pairs each, resulting into a final dataset of 7,526 QA pairs. A quality assurance set of 12.5% of the annotated texts confirmed that the QA pairs were all correc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#35777;&#26126;&#20102;&#23398;&#20064;&#26410;&#30693;&#37193;&#30340;&#36234;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#20056;&#31215;&#24577;&#26469;&#23398;&#20064;&#37193;&#23545;&#32416;&#32544;&#24577;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#22312;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#19978;&#23398;&#20064;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#21069;&#26223;&#65292;&#24182;&#20026;&#32463;&#20856;&#21644;&#37327;&#23376;&#30005;&#36335;&#30340;&#32534;&#35793;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.10268</link><description>&lt;p&gt;
&#23398;&#20064;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#36234;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution generalization for learning quantum dynamics. (arXiv:2204.10268v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#35777;&#26126;&#20102;&#23398;&#20064;&#26410;&#30693;&#37193;&#30340;&#36234;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#20056;&#31215;&#24577;&#26469;&#23398;&#20064;&#37193;&#23545;&#32416;&#32544;&#24577;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#22312;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#19978;&#23398;&#20064;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#21069;&#26223;&#65292;&#24182;&#20026;&#32463;&#20856;&#21644;&#37327;&#23376;&#30005;&#36335;&#30340;&#32534;&#35793;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#36793;&#30028;&#26159;&#35780;&#20272;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#35757;&#32451;&#25968;&#25454;&#35201;&#27714;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24314;&#31435;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#22312;&#22495;&#20869;&#27867;&#21270;&#30340;&#20445;&#35777;&#65292;&#20854;&#20013;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#21516;&#19968;&#25968;&#25454;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#20851;&#20110;QML&#20013;&#22495;&#22806;&#27867;&#21270;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#25105;&#20204;&#35201;&#27714;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#22312;&#26469;&#33258;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#23398;&#20064;&#26410;&#30693;&#37193;&#30340;&#36234;&#22495;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#20165;&#20165;&#35757;&#32451;&#21040;&#20102;&#20056;&#31215;&#24577;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#23398;&#20064;&#21040;&#37193;&#23545;&#32416;&#32544;&#24577;&#30340;&#20316;&#29992;&#12290;&#30001;&#20110;&#20056;&#31215;&#24577;&#21482;&#38656;&#35201;&#20351;&#29992;&#21333;&#27604;&#29305;&#38376;&#23601;&#21487;&#20197;&#21046;&#22791;&#65292;&#36825;&#25512;&#21160;&#20102;&#22312;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#19978;&#23398;&#20064;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#21069;&#26223;&#65292;&#24182;&#36827;&#19968;&#27493;&#20026;&#32463;&#20856;&#21644;&#37327;&#23376;&#30005;&#36335;&#30340;&#32534;&#35793;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization bounds are a critical tool to assess the training data requirements of Quantum Machine Learning (QML). Recent work has established guarantees for in-distribution generalization of quantum neural networks (QNNs), where training and testing data are drawn from the same data distribution. However, there are currently no results on out-of-distribution generalization in QML, where we require a trained model to perform well even on data drawn from a different distribution to the training distribution. Here, we prove out-of-distribution generalization for the task of learning an unknown unitary. In particular, we show that one can learn the action of a unitary on entangled states having trained only product states. Since product states can be prepared using only single-qubit gates, this advances the prospects of learning quantum dynamics on near term quantum hardware, and further opens up new methods for both the classical and quantum compilation of quantum circuits.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22478;&#24066;&#27835;&#29702;&#20013;&#23621;&#27665;&#20247;&#21253;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#27979;&#37327;&#25253;&#36947;&#29575;&#30340;&#26041;&#27861;&#65292;&#20351;&#19981;&#21516;&#30340;&#25253;&#36947;&#29575;&#19981;&#20877;&#25104;&#20026;&#22478;&#24066;&#27835;&#29702;&#19979;&#28216;&#35299;&#20915;&#20107;&#20214;&#36895;&#24230;&#26041;&#38754;&#30340;&#19981;&#20844;&#24179;&#26681;&#28304;&#12290;</title><link>http://arxiv.org/abs/2204.08620</link><description>&lt;p&gt;
&#23621;&#27665;&#20247;&#21253;&#20013;&#31354;&#38388;&#27424;&#25253;&#21578;&#24046;&#24322;&#30340;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Quantifying Spatial Under-reporting Disparities in Resident Crowdsourcing. (arXiv:2204.08620v2 [stat.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22478;&#24066;&#27835;&#29702;&#20013;&#23621;&#27665;&#20247;&#21253;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#27979;&#37327;&#25253;&#36947;&#29575;&#30340;&#26041;&#27861;&#65292;&#20351;&#19981;&#21516;&#30340;&#25253;&#36947;&#29575;&#19981;&#20877;&#25104;&#20026;&#22478;&#24066;&#27835;&#29702;&#19979;&#28216;&#35299;&#20915;&#20107;&#20214;&#36895;&#24230;&#26041;&#38754;&#30340;&#19981;&#20844;&#24179;&#26681;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22478;&#24066;&#27835;&#29702;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20247;&#21253;(&#8220;&#21327;&#21516;&#29983;&#20135;&#8221;)&#65292;&#20197;&#35782;&#21035; downed trees &#21644; power lines &#31561;&#38382;&#39064;&#12290;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#65292;&#23621;&#27665;&#19981;&#20197;&#30456;&#21516;&#30340;&#36895;&#29575;&#25253;&#21578;&#38382;&#39064;&#65292;&#19981;&#21516;&#30340;&#25253;&#21578;&#24322;&#36136;&#24615;&#30452;&#25509;&#36716;&#21270;&#20026;&#19979;&#28216;&#22312;&#35299;&#20915;&#20107;&#20214;&#30340;&#36895;&#24230;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#27979;&#37327;&#36825;&#26679;&#30340;&#27424;&#25253;&#21578;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#32479;&#35745;&#20219;&#21153;&#65292;&#22240;&#20026;&#26681;&#25454;&#23450;&#20041;&#65292;&#25105;&#20204;&#19981;&#33021;&#35266;&#23519;&#21040;&#27809;&#26377;&#34987;&#25253;&#21578;&#30340;&#20107;&#20214;&#25110;&#25253;&#21578;&#20107;&#20214;&#31532;&#19968;&#27425;&#21457;&#29983;&#30340;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#19981;&#33021;&#21333;&#32431;&#22320;&#21306;&#20998;&#20302;&#25253;&#36947;&#29575;&#21644;&#20302;&#22522;&#20934;&#30495;&#23454;&#20107;&#20214;&#29575;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;(&#24322;&#36136;&#30340;)&#25253;&#36947;&#29575;&#65292;&#32780;&#19981;&#20351;&#29992;&#22806;&#37096;&#22522;&#20934;&#30495;&#23454;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#65292;&#22312;&#30456;&#21516;&#20107;&#20214;&#30340; $\textit{duplicate}$ &#25253;&#21578;&#20013;&#30340;&#27604;&#29575;&#21487;&#20197;&#21033;&#29992;&#26469;&#28040;&#38500;&#25253;&#21578;&#29575;&#38543;&#20107;&#20214;&#21457;&#29983;&#32780;&#21457;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20511;&#21161;&#36825;&#20010;&#24605;&#24819;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#26631;&#20934;&#30340;&#27850;&#26494;&#29575;&#20272;&#35745;&#20219;&#21153;&#65292;&#23613;&#31649;&#26631;&#39064;&#26377;&#24456;&#22810;&#25216;&#26415;&#26415;&#35821;&#65292;&#20294;&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#20247;&#21253;&#26469;&#24110;&#21161;&#22478;&#24066;&#27835;&#29702;&#65292;&#19981;&#21516;&#30340;&#25253;&#36947;&#29575;&#22914;&#20309;&#23548;&#33268;&#38382;&#39064;&#35299;&#20915;&#30340;&#24046;&#24322;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#27979;&#37327;&#25253;&#36947;&#29575;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#22806;&#37096;&#25968;&#25454;&#12290;&#20182;&#20204;&#21033;&#29992;&#22810;&#27425;&#25253;&#21578;&#30340;&#20107;&#20214;&#21487;&#20197;&#24110;&#21161;&#21306;&#20998;&#20107;&#20214;&#26159;&#21542;&#21457;&#29983;&#20197;&#21450;&#20854;&#25253;&#36947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern city governance relies heavily on crowdsourcing ("co-production") to identify problems such as downed trees and power lines. A major concern is that residents do not report problems at the same rates, with reporting heterogeneity directly translating to downstream disparities in how quickly incidents can be addressed. Measuring such under-reporting is a difficult statistical task, as, by definition, we do not observe incidents that are not reported or when reported incidents first occurred. Thus, low reporting rates and low ground-truth incident rates cannot be naively distinguished. We develop a method to identify (heterogeneous) reporting rates, without using external ground truth data. Our insight is that rates on $\textit{duplicate}$ reports about the same incident can be leveraged to disambiguate whether an incident has occurred with its reporting rate once it has occurred. Using this idea, we reduce the question to a standard Poisson rate estimation task -- even though the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20803;&#31639;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#12289;&#28436;&#31034;&#25110;&#29616;&#26377;&#31574;&#30053;&#26469;&#21021;&#22987;&#21270;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#19988;&#19982;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20860;&#23481;&#12290;&#36890;&#36807;&#20351;&#29992;&#23548;&#24341;&#31574;&#30053;&#26469;&#24418;&#25104;&#25506;&#32034;&#31574;&#30053;&#30340;&#21021;&#22987;&#29366;&#24577;&#35838;&#31243;&#65292;&#33021;&#22815;&#25552;&#39640;&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.02372</link><description>&lt;p&gt;
&#21551;&#21160;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Jump-Start Reinforcement Learning. (arXiv:2204.02372v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20803;&#31639;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#12289;&#28436;&#31034;&#25110;&#29616;&#26377;&#31574;&#30053;&#26469;&#21021;&#22987;&#21270;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#19988;&#19982;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20860;&#23481;&#12290;&#36890;&#36807;&#20351;&#29992;&#23548;&#24341;&#31574;&#30053;&#26469;&#24418;&#25104;&#25506;&#32034;&#31574;&#30053;&#30340;&#21021;&#22987;&#29366;&#24577;&#35838;&#31243;&#65292;&#33021;&#22815;&#25552;&#39640;&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36890;&#36807;&#35797;&#38169;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20174;&#32780;&#19981;&#26029;&#25913;&#36827;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20174;&#38646;&#24320;&#22987;&#39640;&#25928;&#22320;&#23398;&#20064;&#31574;&#30053;&#21487;&#33021;&#38750;&#24120;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#25506;&#32034;&#25361;&#25112;&#30340;&#20219;&#21153;&#26469;&#35828;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#29616;&#26377;&#31574;&#30053;&#12289;&#31163;&#32447;&#25968;&#25454;&#25110;&#28436;&#31034;&#26469;&#21021;&#22987;&#21270;RL&#21487;&#33021;&#26159;&#21487;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;RL&#36827;&#34892;&#36825;&#31181;&#21021;&#22987;&#21270;&#24120;&#24120;&#25928;&#26524;&#19981;&#20339;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22522;&#20110;&#20540;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#12289;&#28436;&#31034;&#25110;&#29616;&#26377;&#31574;&#30053;&#26469;&#21021;&#22987;&#21270;RL&#31574;&#30053;&#65292;&#24182;&#19988;&#19982;&#20219;&#20309;RL&#26041;&#27861;&#20860;&#23481;&#30340;&#20803;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Jump-Start Reinforcement Learning (JSRL)&#30340;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#20004;&#20010;&#31574;&#30053;&#26469;&#35299;&#20915;&#20219;&#21153;&#65306;&#19968;&#20010;&#23548;&#24341;&#31574;&#30053;&#21644;&#19968;&#20010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#20351;&#29992;&#23548;&#24341;&#31574;&#30053;&#20026;&#25506;&#32034;&#31574;&#30053;&#24418;&#25104;&#19968;&#20010;&#21021;&#22987;&#29366;&#24577;&#30340;&#35838;&#31243;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#19968;&#32452;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#39640;&#25928;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent's behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks with exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20851;&#32852;&#32447;&#24615;&#28909;&#26041;&#31243;&#30340;&#35299;&#65292;&#24471;&#21040;&#20102;&#23545;&#31216;&#21452;&#33218;&#20271;&#21162;&#21033;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;minmax&#26368;&#20248;&#36951;&#25022;&#21644;&#20266;&#36951;&#25022;&#30340;&#39046;&#20808;&#39033;&#12290;&#26032;&#30340;&#32467;&#26524;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#38750;&#28176;&#36817;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2202.05767</link><description>&lt;p&gt;
&#22522;&#20110;PDE&#30340;&#23545;&#31216;&#21452;&#33218;&#20271;&#21162;&#21033;&#36172;&#21338;&#26426;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A PDE-Based Analysis of the Symmetric Two-Armed Bernoulli Bandit. (arXiv:2202.05767v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20851;&#32852;&#32447;&#24615;&#28909;&#26041;&#31243;&#30340;&#35299;&#65292;&#24471;&#21040;&#20102;&#23545;&#31216;&#21452;&#33218;&#20271;&#21162;&#21033;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;minmax&#26368;&#20248;&#36951;&#25022;&#21644;&#20266;&#36951;&#25022;&#30340;&#39046;&#20808;&#39033;&#12290;&#26032;&#30340;&#32467;&#26524;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#38750;&#28176;&#36817;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#20010;&#29256;&#26412;&#30340;&#21452;&#33218;&#20271;&#21162;&#21033;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#20004;&#20010;&#33218;&#30340;&#24179;&#22343;&#20540;&#20043;&#21644;&#20026;1&#65288;&#21363;&#23545;&#31216;&#30340;&#21452;&#33218;&#20271;&#21162;&#21033;&#36172;&#21338;&#26426;&#65289;&#12290;&#22312;&#33218;&#20043;&#38388;&#30340;&#24046;&#36317;&#36235;&#36817;&#20110;&#38646;&#19988;&#39044;&#27979;&#26399;&#25968;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#27599;&#20010;&#35299;&#19982;&#32447;&#24615;&#28909;&#26041;&#31243;&#30340;&#35299;&#20851;&#32852;&#65292;&#24471;&#21040;&#20102;&#35813;&#38382;&#39064;&#30340;minmax&#26368;&#20248;&#36951;&#25022;&#21644;&#20266;&#36951;&#25022;&#30340;&#39046;&#20808;&#39033;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25913;&#36827;&#20102;&#20808;&#21069;&#24050;&#30693;&#30340;&#32467;&#26524;&#65307;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#24046;&#36317;&#32553;&#25918;&#27169;&#24335;&#19979;&#65292;&#25105;&#20204;&#26126;&#30830;&#35745;&#31639;&#20102;&#36825;&#20123;&#39046;&#20808;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24471;&#21040;&#20102;&#20219;&#20309;&#32473;&#23450;&#26102;&#38388;&#33539;&#22260;&#30340;&#26032;&#30340;&#38750;&#28176;&#36817;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses a version of the two-armed Bernoulli bandit problem where the sum of the means of the arms is one (the symmetric two-armed Bernoulli bandit). In a regime where the gap between these means goes to zero and the number of prediction periods approaches infinity, we obtain the leading order terms of the minmax optimal regret and pseudoregret for this problem by associating each of them with a solution of a linear heat equation. Our results improve upon the previously known results; specifically, we explicitly compute these leading order terms in three different scaling regimes for the gap. Additionally, we obtain new non-asymptotic bounds for any given time horizon.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24352;&#37327;&#20998;&#35299;&#26469;&#23454;&#29616;&#19968;&#33268;&#21327;&#21516;&#36807;&#28388;&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#25193;&#23637;&#20256;&#32479;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20559;&#22909;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#24471;&#22312;&#35780;&#20272;&#29289;&#21697;&#30456;&#23545;&#20559;&#22909;&#26102;&#20135;&#29983;&#29289;&#21697;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#38750;&#32447;&#24615;&#24577;&#24230;&#12290;</title><link>http://arxiv.org/abs/2201.11936</link><description>&lt;p&gt;
&#36890;&#36807;&#24352;&#37327;&#20998;&#35299;&#23454;&#29616;&#19968;&#33268;&#30340;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Consistent Collaborative Filtering via Tensor Decomposition. (arXiv:2201.11936v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24352;&#37327;&#20998;&#35299;&#26469;&#23454;&#29616;&#19968;&#33268;&#21327;&#21516;&#36807;&#28388;&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#25193;&#23637;&#20256;&#32479;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20559;&#22909;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#24471;&#22312;&#35780;&#20272;&#29289;&#21697;&#30456;&#23545;&#20559;&#22909;&#26102;&#20135;&#29983;&#29289;&#21697;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#38750;&#32447;&#24615;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#26159;&#20998;&#26512;&#29992;&#25143;&#27963;&#21160;&#21644;&#26500;&#24314;&#29289;&#21697;&#25512;&#33616;&#31995;&#32479;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#21453;&#39304;&#30340;&#21327;&#21516;&#36807;&#28388;&#26032;&#27169;&#22411;&#8212;&#8212;&#20999;&#21106;&#21453;&#23545;&#31216;&#20998;&#35299;&#65288;SAD&#65289;&#12290;&#19982;&#20256;&#32479;&#25216;&#26415;&#19981;&#21516;&#65292;SAD&#36890;&#36807;&#23545;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#26032;&#39062;&#19977;&#32500;&#24352;&#37327;&#35270;&#22270;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#29289;&#21697;&#30340;&#38544;&#21547;&#21521;&#37327;&#12290;&#35813;&#21521;&#37327;&#23558;&#36890;&#36807;&#26631;&#20934;&#28857;&#20056;&#35745;&#31639;&#20986;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20559;&#22909;&#25193;&#23637;&#21040;&#19968;&#33324;&#20869;&#31215;&#65292;&#20174;&#32780;&#22312;&#35780;&#20272;&#29289;&#21697;&#30340;&#30456;&#23545;&#20559;&#22909;&#26102;&#20135;&#29983;&#29289;&#21697;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#24403;&#21521;&#37327;&#25240;&#21472;&#20026;1&#26102;&#65292;SAD&#38477;&#20026;&#26368;&#20808;&#36827;&#30340;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#65288;SOTA&#65289;&#65292;&#32780;&#26412;&#25991;&#20801;&#35768;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#20854;&#20540;&#12290;&#20801;&#35768;&#26032;&#29289;&#21697;&#21521;&#37327;&#30340;&#20540;&#19982;1&#19981;&#21516;&#20855;&#26377;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#36825;&#34920;&#26126;&#29992;&#25143;&#21487;&#33021;&#20855;&#26377;&#38750;&#32447;&#24615;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering is the de facto standard for analyzing users' activities and building recommendation systems for items. In this work we develop Sliced Anti-symmetric Decomposition (SAD), a new model for collaborative filtering based on implicit feedback. In contrast to traditional techniques where a latent representation of users (user vectors) and items (item vectors) are estimated, SAD introduces one additional latent vector to each item, using a novel three-way tensor view of user-item interactions. This new vector extends user-item preferences calculated by standard dot products to general inner products, producing interactions between items when evaluating their relative preferences. SAD reduces to state-of-the-art (SOTA) collaborative filtering models when the vector collapses to 1, while in this paper we allow its value to be estimated from data. Allowing the values of the new item vector to be different from 1 has profound implications. It suggests users may have nonlin
&lt;/p&gt;</description></item><item><title>FIGS&#26159;&#19968;&#31181;&#24555;&#36895;&#21487;&#35299;&#37322;&#30340;&#36138;&#23146;&#26641;&#27714;&#21644;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#36923;&#36753;&#35268;&#21017;&#19982;&#21152;&#27861;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#36866;&#24212;&#21152;&#24615;&#32467;&#26500;&#21516;&#26102;&#20445;&#25345;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FIGS&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#22914;&#21307;&#23398;&#20013;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.11931</link><description>&lt;p&gt;
&#24555;&#36895;&#21487;&#35299;&#37322;&#30340;&#36138;&#23146;&#26641;&#27714;&#21644;
&lt;/p&gt;
&lt;p&gt;
Fast Interpretable Greedy-Tree Sums. (arXiv:2201.11931v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11931
&lt;/p&gt;
&lt;p&gt;
FIGS&#26159;&#19968;&#31181;&#24555;&#36895;&#21487;&#35299;&#37322;&#30340;&#36138;&#23146;&#26641;&#27714;&#21644;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#36923;&#36753;&#35268;&#21017;&#19982;&#21152;&#27861;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#36866;&#24212;&#21152;&#24615;&#32467;&#26500;&#21516;&#26102;&#20445;&#25345;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FIGS&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#22914;&#21307;&#23398;&#20013;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#36890;&#24120;&#20250;&#29306;&#29298;&#35299;&#37322;&#24615;&#65292;&#36825;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#22914;&#21307;&#23398;&#20013;&#26159;&#19968;&#20010;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20174;&#19994;&#32773;&#36890;&#24120;&#20351;&#29992;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#23545;&#21152;&#24615;&#32467;&#26500;&#23384;&#22312;&#24402;&#32435;&#20559;&#24046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#31181;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;&#21487;&#35299;&#37322;&#30340;&#36138;&#23146;&#26641;&#27714;&#21644;&#65288;FIGS&#65289;&#65292;&#23427;&#23558;CART&#31639;&#27861;&#25512;&#24191;&#21040;&#21516;&#26102;&#22686;&#38271;&#21487;&#21464;&#25968;&#37327;&#30340;&#26641;&#36827;&#34892;&#27714;&#21644;&#12290;&#36890;&#36807;&#23558;&#36923;&#36753;&#35268;&#21017;&#19982;&#21152;&#27861;&#30456;&#32467;&#21512;&#65292;FIGS&#33021;&#22815;&#36866;&#24212;&#21152;&#24615;&#32467;&#26500;&#21516;&#26102;&#20445;&#25345;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#12290;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;FIGS&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#23637;&#31034;FIGS&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;FIGS&#25913;&#36827;&#20026;&#23398;&#20064;&#20020;&#24202;&#20915;&#31574;&#24037;&#20855;&#65288;CDIs&#65289;&#65292;CDIs&#26159;&#25351;&#23548;&#20020;&#24202;&#20915;&#31574;&#30340;&#24037;&#20855;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FIGS&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#31216;&#20026;G-FIGS&#65292;&#23427;&#32771;&#34385;&#20102;&#21152;&#24615;&#32467;&#26500;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning has achieved impressive prediction performance, but often sacrifices interpretability, a critical consideration in high-stakes domains such as medicine. In such settings, practitioners often use highly interpretable decision tree models, but these suffer from inductive bias against additive structure. To overcome this bias, we propose Fast Interpretable Greedy-Tree Sums (FIGS), which generalizes the CART algorithm to simultaneously grow a flexible number of trees in summation. By combining logical rules with addition, FIGS is able to adapt to additive structure while remaining highly interpretable. Extensive experiments on real-world datasets show that FIGS achieves state-of-the-art prediction performance. To demonstrate the usefulness of FIGS in high-stakes domains, we adapt FIGS to learn clinical decision instruments (CDIs), which are tools for guiding clinical decision-making. Specifically, we introduce a variant of FIGS known as G-FIGS that accounts for the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20445;&#23432;&#20998;&#24067;&#26368;&#22823;&#21518;&#39564;&#31574;&#30053;&#20248;&#21270; (CDMPO) &#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20272;&#35745;Q&#20989;&#25968;&#21644;C&#20989;&#25968;&#65292;&#20197;&#21450;&#20351;&#29992;&#20445;&#23432;&#30340;&#20215;&#20540;&#20989;&#25968;&#25439;&#22833;&#26469;&#20943;&#23569;&#32422;&#26463;&#36829;&#21453;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2201.07286</link><description>&lt;p&gt;
&#20855;&#26377;&#23433;&#20840;&#32422;&#26463;&#30340;&#20445;&#23432;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conservative Distributional Reinforcement Learning with Safety Constraints. (arXiv:2201.07286v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20445;&#23432;&#20998;&#24067;&#26368;&#22823;&#21518;&#39564;&#31574;&#30053;&#20248;&#21270; (CDMPO) &#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20272;&#35745;Q&#20989;&#25968;&#21644;C&#20989;&#25968;&#65292;&#20197;&#21450;&#20351;&#29992;&#20445;&#23432;&#30340;&#20215;&#20540;&#20989;&#25968;&#25439;&#22833;&#26469;&#20943;&#23569;&#32422;&#26463;&#36829;&#21453;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#25506;&#32034;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#21463;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#39044;&#26399;&#30340;&#38271;&#26399;&#25104;&#26412;&#26159;&#21463;&#21040;&#32422;&#26463;&#30340;&#12290;&#20808;&#21069;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#25216;&#26415;&#65292;&#23558;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#36716;&#25442;&#20026;&#30456;&#24212;&#30340;&#26080;&#32422;&#26463;&#23545;&#20598;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19978;&#36848;&#31639;&#27861;&#30340;&#25104;&#26412;&#20989;&#25968;&#25552;&#20379;&#20102;&#19981;&#20934;&#30830;&#30340;&#20272;&#35745;&#65292;&#24182;&#23548;&#33268;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#23398;&#20064;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#20445;&#23432;&#20998;&#24067;&#26368;&#22823;&#21518;&#39564;&#31574;&#30053;&#20248;&#21270;(CDMPO)&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#20934;&#30830;&#21028;&#26029;&#24403;&#21069;&#24773;&#20917;&#26159;&#21542;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#65292;CDMPO&#37319;&#29992;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20272;&#35745;Q&#20989;&#25968;&#21644;C&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;CDMPO&#20351;&#29992;&#20445;&#23432;&#30340;&#20215;&#20540;&#20989;&#25968;&#25439;&#22833;&#26469;&#20943;&#23569;&#25506;&#32034;&#36807;&#31243;&#20013;&#36829;&#32422;&#27425;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#21152;&#26435;&#24179;&#22343;&#27604;&#20363;&#31215;&#20998;D&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety exploration can be regarded as a constrained Markov decision problem where the expected long-term cost is constrained. Previous off-policy algorithms convert the constrained optimization problem into the corresponding unconstrained dual problem by introducing the Lagrangian relaxation technique. However, the cost function of the above algorithms provides inaccurate estimations and causes the instability of the Lagrange multiplier learning. In this paper, we present a novel off-policy reinforcement learning algorithm called Conservative Distributional Maximum a Posteriori Policy Optimization (CDMPO). At first, to accurately judge whether the current situation satisfies the constraints, CDMPO adapts distributional reinforcement learning method to estimate the Q-function and C-function. Then, CDMPO uses a conservative value function loss to reduce the number of violations of constraints during the exploration process. In addition, we utilize Weighted Average Proportional Integral D
&lt;/p&gt;</description></item><item><title>&#38754;&#21521;&#31038;&#20132;&#24863;&#30693;&#26426;&#22120;&#20154;&#30340;&#32852;&#37030;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#36830;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#20998;&#25955;&#23398;&#20064;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#25552;&#21319;&#20102;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#20010;&#24615;&#21270;&#21644;&#29992;&#25143;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.05527</link><description>&lt;p&gt;
&#38754;&#21521;&#31038;&#20132;&#24863;&#30693;&#26426;&#22120;&#20154;&#30340;&#32852;&#37030;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Continual Learning for Socially Aware Robotics. (arXiv:2201.05527v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05527
&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#31038;&#20132;&#24863;&#30693;&#26426;&#22120;&#20154;&#30340;&#32852;&#37030;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#36830;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#20998;&#25955;&#23398;&#20064;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#25552;&#21319;&#20102;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#20010;&#24615;&#21270;&#21644;&#29992;&#25143;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23398;&#20064;&#36741;&#21161;&#21040;&#38506;&#20276;&#65292;&#31038;&#20132;&#26426;&#22120;&#20154;&#25215;&#35834;&#25552;&#21319;&#26085;&#24120;&#29983;&#27963;&#30340;&#35768;&#22810;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#26426;&#22120;&#20154;&#23578;&#26410;&#24191;&#27867;&#24212;&#29992;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#23427;&#20204;&#19981;&#20250;&#26681;&#25454;&#26032;&#29992;&#25143;&#33258;&#36866;&#24212;&#34892;&#20026;&#65292;&#24182;&#19988;&#26410;&#25552;&#20379;&#36275;&#22815;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#20013;&#24515;&#21270;&#23398;&#20064;&#36890;&#36807;&#38459;&#27490;&#22312;&#32447;&#23398;&#20064;&#26032;&#32463;&#39564;&#21644;&#35201;&#27714;&#23384;&#20648;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#26469;&#38480;&#21046;&#20102;&#36825;&#20123;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25913;&#21892;&#20102;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#38544;&#31169;&#21644;&#20010;&#24615;&#21270;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32852;&#37030;&#23398;&#20064;&#21644;&#36830;&#32493;&#23398;&#20064;&#65292;&#20197;&#25429;&#25417;&#20998;&#24067;&#22312;&#29289;&#29702;&#19978;&#30340;&#26426;&#22120;&#20154;&#20043;&#38388;&#21644;&#22312;&#26102;&#38388;&#19978;&#30340;&#37325;&#22797;&#26426;&#22120;&#20154;&#30456;&#36935;&#20013;&#30340;&#20132;&#20114;&#21160;&#24577;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#32452;&#22312;&#20998;&#25955;&#26426;&#22120;&#20154;&#23398;&#20064;&#22330;&#26223;&#20013;&#24212;&#35813;&#24179;&#34913;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#24377;&#24615;&#20256;&#36755;&#65288;Elastic Transfer&#65289;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#22312;&#32447;&#23398;&#20064;&#26469;&#24179;&#34913;&#20010;&#20307;&#23384;&#20648;&#21644;&#20840;&#23616;&#20449;&#24687;&#20849;&#20139;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
From learning assistance to companionship, social robots promise to enhance many aspects of daily life. However, social robots have not seen widespread adoption, in part because (1) they do not adapt their behavior to new users, and (2) they do not provide sufficient privacy protections. Centralized learning, whereby robots develop skills by gathering data on a server, contributes to these limitations by preventing online learning of new experiences and requiring storage of privacy-sensitive data. In this work, we propose a decentralized learning alternative that improves the privacy and personalization of social robots. We combine two machine learning approaches, Federated Learning and Continual Learning, to capture interaction dynamics distributed physically across robots and temporally across repeated robot encounters. We define a set of criteria that should be balanced in decentralized robot learning scenarios. We also develop a new algorithm -Elastic Transfer -- that leverages i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SCORE&#26694;&#26550;&#65292;&#22312;&#26080;&#32422;&#26463;&#26497;&#23567;&#21270;&#38382;&#39064;&#20013;&#21033;&#29992;&#33258;&#23545;&#20598;&#27491;&#21017;&#21270;&#36817;&#20284;&#26354;&#29575;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;GGN-SCORE&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#26356;&#26032;&#26497;&#23567;&#21270;&#21464;&#37327;&#26469;&#21152;&#36895;&#25910;&#25947;&#24182;&#25913;&#21892;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.07344</link><description>&lt;p&gt;
SCORE: &#22312;&#33258;&#23545;&#20598;&#27491;&#21017;&#21270;&#19979;&#36817;&#20284;&#26354;&#29575;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
SCORE: Approximating Curvature Information under Self-Concordant Regularization. (arXiv:2112.07344v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.07344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SCORE&#26694;&#26550;&#65292;&#22312;&#26080;&#32422;&#26463;&#26497;&#23567;&#21270;&#38382;&#39064;&#20013;&#21033;&#29992;&#33258;&#23545;&#20598;&#27491;&#21017;&#21270;&#36817;&#20284;&#26354;&#29575;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;GGN-SCORE&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#26356;&#26032;&#26497;&#23567;&#21270;&#21464;&#37327;&#26469;&#21152;&#36895;&#25910;&#25947;&#24182;&#25913;&#21892;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#21253;&#21547;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#32463;&#24120;&#34987;&#27714;&#35299;&#12290;&#24403;&#25105;&#20204;&#23547;&#27714;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#20108;&#38454;&#26041;&#27861;&#26102;&#65292;&#20026;&#20102;&#21152;&#36895;&#25910;&#25947;&#65292;&#32771;&#34385;&#21040;&#19968;&#20123;&#29305;&#23450;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#26354;&#29575;&#20449;&#24687;&#21487;&#33021;&#26159;&#20540;&#24471;&#21033;&#29992;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#26080;&#32422;&#26463;&#26497;&#23567;&#21270;&#38382;&#39064;&#30340;SCORE&#65288;&#33258;&#23545;&#20598;&#27491;&#21017;&#21270;&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#29275;&#39039;&#20943;&#37327;&#26694;&#26550;&#20013;&#32467;&#21512;&#20102;&#20108;&#38454;&#20449;&#24687;&#36827;&#34892;&#20984;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#39640;&#26031;-&#29275;&#39039;&#19982;&#33258;&#23545;&#20598;&#27491;&#21017;&#21270;&#65288;GGN-SCORE&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#27599;&#27425;&#25509;&#25910;&#21040;&#26032;&#30340;&#36755;&#20837;&#25209;&#27425;&#26102;&#26356;&#26032;&#26497;&#23567;&#21270;&#21464;&#37327;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;Hessian&#30697;&#38453;&#20013;&#20108;&#38454;&#20449;&#24687;&#30340;&#32467;&#26500;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;GGN-SCORE&#28436;&#31034;&#20102;&#22914;&#20309;&#21152;&#36895;&#25910;&#25947;&#65292;&#21516;&#26102;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization problems that include regularization functions in their objectives are regularly solved in many applications. When one seeks second-order methods for such problems, it may be desirable to exploit specific properties of some of these regularization functions when accounting for curvature information in the solution steps to speed up convergence. In this paper, we propose the SCORE (self-concordant regularization) framework for unconstrained minimization problems which incorporates second-order information in the Newton-decrement framework for convex optimization. We propose the generalized Gauss-Newton with Self-Concordant Regularization (GGN-SCORE) algorithm that updates the minimization variables each time it receives a new input batch. The proposed algorithm exploits the structure of the second-order information in the Hessian matrix, thereby reducing computational overhead. GGN-SCORE demonstrates how to speed up convergence while also improving model generalization for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#30456;&#21464;&#38382;&#39064;&#65292;&#26681;&#25454;&#26368;&#23567;&#26497;&#22823;MISE&#36895;&#29575;&#30340;&#19981;&#21516;&#24773;&#20917;&#65292;&#30830;&#23450;&#20102;&#19981;&#21516;&#33539;&#22260;&#20869;&#26368;&#20339;&#30340;&#24179;&#28369;&#24230;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#29992;&#20110;&#24179;&#28369;&#20989;&#25968;&#31867;&#21035;&#30340;&#24230;&#37327;&#29109;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2112.03626</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Phase transitions in nonparametric regressions. (arXiv:2112.03626v6 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#30456;&#21464;&#38382;&#39064;&#65292;&#26681;&#25454;&#26368;&#23567;&#26497;&#22823;MISE&#36895;&#29575;&#30340;&#19981;&#21516;&#24773;&#20917;&#65292;&#30830;&#23450;&#20102;&#19981;&#21516;&#33539;&#22260;&#20869;&#26368;&#20339;&#30340;&#24179;&#28369;&#24230;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#29992;&#20110;&#24179;&#28369;&#20989;&#25968;&#31867;&#21035;&#30340;&#24230;&#37327;&#29109;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24050;&#30693;&#21333;&#21464;&#37327;&#30340;&#26410;&#30693;&#22238;&#24402;&#20989;&#25968;&#30340;&#23548;&#25968;&#22312;&#32477;&#23545;&#20540;&#19978;&#30452;&#21040;$(\gamma+1)$&#38454;&#37117;&#21463;&#21040;&#19968;&#20010;&#20844;&#20849;&#24120;&#25968;&#30340;&#30028;&#38480;&#65288;&#21363;$(\gamma+1)$&#38454;&#24179;&#28369;&#24230;&#65289;&#65292;&#25991;&#29486;&#20013;&#32473;&#20986;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MISE&#65289;&#30340;&#26368;&#23567;&#26497;&#22823;&#36895;&#29575;&#20026;$\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$&#12290;&#26412;&#25991;&#34920;&#26126;&#65306;&#65288;i&#65289;&#22914;&#26524;$n\leq\left(\gamma+1\right)^{2\gamma+3}$&#65292;&#21017;&#26368;&#23567;&#26497;&#22823;MISE&#36895;&#29575;&#20026;$\frac{\log n}{n\log(\log n)}$&#65292;&#24182;&#19988;&#26368;&#20339;&#21033;&#29992;&#30340;&#24179;&#28369;&#24230;&#20026;&#22823;&#32422;$\max\left\{ \left\lfloor \frac{\log n}{2\log\left(\log n\right)}\right\rfloor ,\,1\right\}$&#65307;&#65288;ii&#65289;&#22914;&#26524;$n&gt;\left(\gamma+1\right)^{2\gamma+3}$&#65292;&#21017;&#26368;&#23567;&#26497;&#22823;MISE&#36895;&#29575;&#20026;$\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$&#65292;&#24182;&#19988;&#26368;&#20339;&#21033;&#29992;&#30340;&#24179;&#28369;&#24230;&#20026;$\gamma+1$&#12290;&#26412;&#25991;&#30340;&#22522;&#26412;&#36129;&#29486;&#26159;&#25105;&#20204;&#20026;&#24179;&#28369;&#20989;&#25968;&#31867;&#21035;&#21046;&#23450;&#30340;&#19968;&#32452;&#24230;&#37327;&#29109;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
When the unknown regression function of a single variable is known to have derivatives up to the $(\gamma+1)$th order bounded in absolute values by a common constant everywhere or a.e. (i.e., $(\gamma+1)$th degree of smoothness), the minimax optimal rate of the mean integrated squared error (MISE) is stated as $\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$ in the literature. This paper shows that: (i) if $n\leq\left(\gamma+1\right)^{2\gamma+3}$, the minimax optimal MISE rate is $\frac{\log n}{n\log(\log n)}$ and the optimal degree of smoothness to exploit is roughly $\max\left\{ \left\lfloor \frac{\log n}{2\log\left(\log n\right)}\right\rfloor ,\,1\right\} $; (ii) if $n&gt;\left(\gamma+1\right)^{2\gamma+3}$, the minimax optimal MISE rate is $\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$ and the optimal degree of smoothness to exploit is $\gamma+1$. The fundamental contribution of this paper is a set of metric entropy bounds we develop for smooth function classes. Some 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#36873;&#25321;&#19968;&#32452;&#22522;&#30784;&#33218;&#26469;&#26368;&#22823;&#21270;&#36229;&#32423;&#33218;&#22870;&#21169;&#65292;&#24182;&#21516;&#26102;&#28385;&#36275;&#32452;&#22870;&#21169;&#32422;&#26463;&#12290;&#31639;&#27861;&#21033;&#29992;&#20004;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#20026;&#27599;&#20010;&#22522;&#30784;&#33218;&#30340;&#32467;&#26524;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.14778</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#32452;&#21512;&#22810;&#36755;&#20986; GP Bandits &#24102;&#26377;&#32452;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Contextual Combinatorial Multi-output GP Bandits with Group Constraints. (arXiv:2111.14778v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.14778
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#36873;&#25321;&#19968;&#32452;&#22522;&#30784;&#33218;&#26469;&#26368;&#22823;&#21270;&#36229;&#32423;&#33218;&#22870;&#21169;&#65292;&#24182;&#21516;&#26102;&#28385;&#36275;&#32452;&#22870;&#21169;&#32422;&#26463;&#12290;&#31639;&#27861;&#21033;&#29992;&#20004;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#20026;&#27599;&#20010;&#22522;&#30784;&#33218;&#30340;&#32467;&#26524;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#65292;&#26368;&#22823;&#21270;&#20840;&#23616;&#22870;&#21169;&#21516;&#26102;&#28385;&#36275;&#20445;&#25252;&#23458;&#25143;&#30340;&#26368;&#20302;&#38544;&#31169;&#35201;&#27714;&#26159;&#20027;&#35201;&#30446;&#26631;&#12290;&#20026;&#20102;&#24314;&#31435;&#36825;&#26679;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;&#32452;&#21644;&#21464;&#21270;&#21160;&#20316;&#38598;&#30340;&#32452;&#21512;&#19978;&#19979;&#25991;&#36172;&#21338;&#35774;&#23450;&#65292;&#20854;&#20013;&#30456;&#20284;&#30340;&#22522;&#30784;&#33218;&#20197;&#32452;&#30340;&#24418;&#24335;&#20986;&#29616;&#65292;&#24182;&#19988;&#24517;&#39035;&#22312;&#27599;&#19968;&#36718;&#36873;&#25321;&#19968;&#32452;&#22522;&#30784;&#33218;&#65288;&#31216;&#20026;&#36229;&#32423;&#33218;&#65289;&#65292;&#20197;&#26368;&#22823;&#21270;&#36229;&#32423;&#33218;&#30340;&#22870;&#21169;&#21516;&#26102;&#28385;&#36275;&#26469;&#33258;&#36873;&#25321;&#22522;&#30784;&#33218;&#30340;&#32452;&#30340;&#22870;&#21169;&#32422;&#26463;&#12290;&#20026;&#20102;&#22686;&#21152;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#35753;&#27599;&#20010;&#22522;&#30784;&#33218;&#20855;&#26377;&#20004;&#20010;&#32467;&#26524;&#65292;&#34987;&#24314;&#27169;&#20026;&#20004;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#30340;&#36755;&#20986;&#65292;&#20854;&#20013;&#19968;&#20010;&#32467;&#26524;&#29992;&#20110;&#35745;&#31639;&#36229;&#32423;&#33218;&#30340;&#22870;&#21169;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#32452;&#30340;&#22870;&#21169;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;-UCB GP-bandit &#31639;&#27861;&#65292;&#31216;&#20026;&#38408;&#20540;&#32452;&#21512;&#39640;&#26031;&#36807;&#31243;&#19978;&#32622;&#20449;&#36793;&#30028;&#65288;TCGP-UCB&#65289;&#65292;&#23427;&#22312;&#26368;&#22823;&#21270;&#32047;&#31215;&#36229;&#32423;&#33218;&#22870;&#21169;&#21644;&#28385;&#36275;&#32452;&#22870;&#21169;&#32422;&#26463;&#20043;&#38388;&#25214;&#21040;&#24179;&#34913;&#65292;&#24182;&#19988;&#21487;&#20197;&#35843;&#25972;&#20026;&#20559;&#22909;&#19968;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated multi-armed bandit problems, maximizing global reward while satisfying minimum privacy requirements to protect clients is the main goal. To formulate such problems, we consider a combinatorial contextual bandit setting with groups and changing action sets, where similar base arms arrive in groups and a set of base arms, called a super arm, must be chosen in each round to maximize super arm reward while satisfying the constraints of the rewards of groups from which base arms were chosen. To allow for greater flexibility, we let each base arm have two outcomes, modeled as the output of a two-output Gaussian process (GP), where one outcome is used to compute super arm reward and the other for group reward. We then propose a novel double-UCB GP-bandit algorithm, called Thresholded Combinatorial Gaussian Process Upper Confidence Bounds (TCGP-UCB), which balances between maximizing cumulative super arm reward and satisfying group reward constraints and can be tuned to prefer one
&lt;/p&gt;</description></item><item><title>GFlowNets&#26159;&#19968;&#31181;&#29983;&#25104;&#27969;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20027;&#21160;&#23398;&#20064;&#29615;&#22659;&#20013;&#37319;&#26679;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#38598;&#12290;&#23427;&#20204;&#20855;&#26377;&#20272;&#35745;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#21644;&#36793;&#38469;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#34920;&#31034;&#20851;&#20110;&#22797;&#21512;&#23545;&#35937;&#65288;&#22914;&#38598;&#21512;&#21644;&#22270;&#65289;&#30340;&#20998;&#24067;&#12290;&#36890;&#36807;&#21333;&#27425;&#35757;&#32451;&#30340;&#29983;&#25104;&#20256;&#36882;&#65292;GFlowNets&#20998;&#25674;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;MCMC&#26041;&#27861;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2111.09266</link><description>&lt;p&gt;
GFlowNet&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
GFlowNet Foundations. (arXiv:2111.09266v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09266
&lt;/p&gt;
&lt;p&gt;
GFlowNets&#26159;&#19968;&#31181;&#29983;&#25104;&#27969;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20027;&#21160;&#23398;&#20064;&#29615;&#22659;&#20013;&#37319;&#26679;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#38598;&#12290;&#23427;&#20204;&#20855;&#26377;&#20272;&#35745;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#21644;&#36793;&#38469;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#34920;&#31034;&#20851;&#20110;&#22797;&#21512;&#23545;&#35937;&#65288;&#22914;&#38598;&#21512;&#21644;&#22270;&#65289;&#30340;&#20998;&#24067;&#12290;&#36890;&#36807;&#21333;&#27425;&#35757;&#32451;&#30340;&#29983;&#25104;&#20256;&#36882;&#65292;GFlowNets&#20998;&#25674;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;MCMC&#26041;&#27861;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#34987;&#24341;&#20837;&#20026;&#22312;&#20027;&#21160;&#23398;&#20064;&#29615;&#22659;&#20013;&#37319;&#26679;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#38598;&#30340;&#26041;&#27861;&#65292;&#20854;&#35757;&#32451;&#30446;&#26631;&#20351;&#20854;&#36817;&#20284;&#25353;&#29031;&#32473;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#37319;&#26679;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;GFlowNets&#30340;&#19968;&#20123;&#39069;&#22806;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#21644;&#30456;&#24212;&#30340;&#36793;&#38469;&#20998;&#24067;&#65292;&#20854;&#20013;&#19968;&#20123;&#21464;&#37327;&#26410;&#25351;&#23450;&#65292;&#29305;&#21035;&#26159;&#21487;&#20197;&#34920;&#31034;&#20851;&#20110;&#22797;&#21512;&#23545;&#35937;&#65288;&#22914;&#38598;&#21512;&#21644;&#22270;&#65289;&#30340;&#20998;&#24067;&#12290;GFlowNets&#36890;&#36807;&#21333;&#27425;&#35757;&#32451;&#30340;&#29983;&#25104;&#20256;&#36882;&#26469;&#20998;&#25674;&#36890;&#24120;&#30001;&#35745;&#31639;&#26114;&#36149;&#30340;MCMC&#26041;&#27861;&#23436;&#25104;&#30340;&#24037;&#20316;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#20998;&#21306;&#20989;&#25968;&#21644;&#33258;&#30001;&#33021;&#65292;&#32473;&#23450;&#19968;&#20010;&#23376;&#38598;&#65288;&#23376;&#22270;&#65289;&#30340;&#36229;&#38598;&#65288;&#36229;&#22270;&#65289;&#30340;&#26465;&#20214;&#27010;&#29575;&#65292;&#20197;&#21450;&#32473;&#23450;&#19968;&#20010;&#38598;&#21512;&#65288;&#22270;&#65289;&#30340;&#25152;&#26377;&#36229;&#38598;&#65288;&#36229;&#22270;&#65289;&#30340;&#36793;&#38469;&#20998;&#24067;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20123;&#21464;&#20307;&#65292;&#20351;&#24471;&#21487;&#20197;&#20272;&#35745;&#29109;&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets. They can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SCORE&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#38477;&#20302;&#12290;&#36890;&#36807;&#24341;&#20837;&#36864;&#28779;&#34892;&#20026;&#20811;&#38534;&#27491;&#21017;&#21270;&#22120;&#65292;SCORE&#23454;&#29616;&#20102;SoTA&#24615;&#33021;&#65292;&#24182;&#28040;&#38500;&#20102;&#27425;&#20248;&#24615;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.12468</link><description>&lt;p&gt;
SCORE&#65306;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
SCORE: Spurious COrrelation REduction for Offline Reinforcement Learning. (arXiv:2110.12468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.12468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SCORE&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#38477;&#20302;&#12290;&#36890;&#36807;&#24341;&#20837;&#36864;&#28779;&#34892;&#20026;&#20811;&#38534;&#27491;&#21017;&#21270;&#22120;&#65292;SCORE&#23454;&#29616;&#20102;SoTA&#24615;&#33021;&#65292;&#24182;&#28040;&#38500;&#20102;&#27425;&#20248;&#24615;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35299;&#20915;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#35770;&#25991;&#21482;&#35752;&#35770;&#20102;&#23545;&#25239;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#34892;&#20026;&#30340;&#38450;&#24481;&#65292;&#32780;&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#21363;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#19982;&#20915;&#31574;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#36825;&#26159;&#23548;&#33268;&#27425;&#20248;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#26377;&#25928;&#19988;&#29702;&#35770;&#19978;&#21487;&#35777;&#26126;&#30340;&#31639;&#27861;&#65306;&#29992;&#20110;&#31163;&#32447;RL&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#38477;&#20302;&#65288;SCORE&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SCORE&#22312;&#26631;&#20934;&#22522;&#20934;&#65288;D4RL&#65289;&#19978;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#20197;3.1&#20493;&#21152;&#36895;&#29575;&#23454;&#29616;&#20102;SoTA&#24615;&#33021;&#12290;&#25152;&#25552;&#31639;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#36864;&#28779;&#34892;&#20026;&#20811;&#38534;&#27491;&#21017;&#21270;&#22120;&#26469;&#24110;&#21161;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#23545;&#20110;&#28040;&#38500;&#27425;&#20248;&#24615;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#20854;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#27425;&#32447;&#24615;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) harnesses the power of massive datasets for resolving sequential decision problems. Most existing papers only discuss defending against out-of-distribution (OOD) actions while we investigate a broader issue, the spurious correlations between epistemic uncertainty and decision-making, an essential factor that causes suboptimality. In this paper, we propose Spurious COrrelation REduction (SCORE) for offline RL, a practically effective and theoretically provable algorithm. We empirically show that SCORE achieves the SoTA performance with 3.1x acceleration on various tasks in a standard benchmark (D4RL). The proposed algorithm introduces an annealing behavior cloning regularizer to help produce a high-quality estimation of uncertainty which is critical for eliminating spurious correlations from suboptimality. Theoretically, we justify the rationality of the proposed method and prove its convergence to the optimal policy with a sublinear rate under mild a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25253;&#36947;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#38754;&#30721;&#32508;&#21512;&#24449;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#35299;&#30721;&#20219;&#24847;&#24418;&#29366;&#21644;&#22823;&#23567;&#30340;&#34920;&#38754;&#30721;&#65292;&#24182;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#24615;&#25110;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#24778;&#20154;&#30340;&#21152;&#36895;&#20316;&#29992;&#65292;&#36825;&#26159;&#23454;&#29616;&#22823;&#35268;&#27169;&#37327;&#23376;&#32416;&#38169;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2110.05854</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#30340;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#38754;&#30721;&#32508;&#21512;&#24449;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A scalable and fast artificial neural network syndrome decoder for surface codes. (arXiv:2110.05854v4 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25253;&#36947;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#38754;&#30721;&#32508;&#21512;&#24449;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#35299;&#30721;&#20219;&#24847;&#24418;&#29366;&#21644;&#22823;&#23567;&#30340;&#34920;&#38754;&#30721;&#65292;&#24182;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#24615;&#25110;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#24778;&#20154;&#30340;&#21152;&#36895;&#20316;&#29992;&#65292;&#36825;&#26159;&#23454;&#29616;&#22823;&#35268;&#27169;&#37327;&#23376;&#32416;&#38169;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#38754;&#30721;&#32416;&#38169;&#25552;&#20379;&#20102;&#23454;&#29616;&#21487;&#25193;&#23637;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#30340;&#39640;&#24230;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#24403;&#20316;&#20026;&#31283;&#23450;&#30721;&#36827;&#34892;&#25805;&#20316;&#26102;&#65292;&#34920;&#38754;&#30721;&#35745;&#31639;&#21253;&#25324;&#19968;&#20010;&#32508;&#21512;&#24449;&#35299;&#30721;&#27493;&#39588;&#65292;&#20854;&#20013;&#20351;&#29992;&#27979;&#37327;&#30340;&#31283;&#23450;&#30721;&#31639;&#31526;&#26469;&#30830;&#23450;&#29289;&#29702;&#37327;&#23376;&#27604;&#29305;&#20013;&#30340;&#38169;&#35823;&#30340;&#36866;&#24403;&#32416;&#27491;&#12290;&#35793;&#30721;&#31639;&#27861;&#32463;&#21382;&#20102;&#23454;&#36136;&#24615;&#30340;&#21457;&#23637;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#32435;&#20837;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#12290;&#23613;&#31649;&#21021;&#27493;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65292;&#20294;&#22522;&#20110;ML&#30340;&#32508;&#21512;&#24449;&#35793;&#30721;&#22120;&#20173;&#28982;&#23616;&#38480;&#20110;&#20855;&#26377;&#20302;&#24310;&#36831;&#30340;&#23567;&#35268;&#27169;&#28436;&#31034;&#65292;&#24182;&#19988;&#26080;&#27861;&#22788;&#29702;&#38656;&#35201;&#36827;&#34892;&#26230;&#26684;&#25163;&#26415;&#21644;&#32534;&#32455;&#30340;&#36793;&#30028;&#26465;&#20214;&#21644;&#21508;&#31181;&#24418;&#29366;&#30340;&#34920;&#38754;&#30721;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#21487;&#25193;&#23637;&#24555;&#36895;&#32508;&#21512;&#24449;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#35299;&#30721;&#20219;&#24847;&#24418;&#29366;&#21644;&#22823;&#23567;&#30340;&#34920;&#38754;&#30721;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#25968;&#25454;&#37327;&#23376;&#27604;&#29305;&#21463;&#21040;&#26497;&#21270;&#35823;&#24046;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#23545;5000&#19975;&#20010;&#38543;&#26426;&#37327;&#23376;&#38169;&#35823;&#23454;&#20363;&#30340;&#20005;&#26684;&#35757;&#32451;&#65292;ANN&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#35793;&#30721;&#31639;&#27861;&#30456;&#27604;&#30340;&#31454;&#20105;&#24615;&#25110;&#20248;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#35299;&#30721;&#20934;&#30830;&#24615;&#65292;&#21576;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35793;&#30721;&#26041;&#27861;&#23454;&#29616;&#22823;&#35268;&#27169;&#37327;&#23376;&#32416;&#38169;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surface code error correction offers a highly promising pathway to achieve scalable fault-tolerant quantum computing. When operated as stabilizer codes, surface code computations consist of a syndrome decoding step where measured stabilizer operators are used to determine appropriate corrections for errors in physical qubits. Decoding algorithms have undergone substantial development, with recent work incorporating machine learning (ML) techniques. Despite promising initial results, the ML-based syndrome decoders are still limited to small scale demonstrations with low latency and are incapable of handling surface codes with boundary conditions and various shapes needed for lattice surgery and braiding. Here, we report the development of an artificial neural network (ANN) based scalable and fast syndrome decoder capable of decoding surface codes of arbitrary shape and size with data qubits suffering from the depolarizing error model. Based on rigorous training over 50 million random qu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21644;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#30340;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;E$^3$&#30340;&#39640;&#25928;&#31232;&#30095;MoEs&#38598;&#25104;&#26041;&#27861;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#21462;&#24471;&#20102;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2110.03360</link><description>&lt;p&gt;
&#31232;&#30095;MoEs&#28385;&#36275;&#39640;&#25928;&#30340;&#27169;&#22411;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Sparse MoEs meet Efficient Ensembles. (arXiv:2110.03360v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21644;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#30340;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;E$^3$&#30340;&#39640;&#25928;&#31232;&#30095;MoEs&#38598;&#25104;&#26041;&#27861;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#21462;&#24471;&#20102;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23376;&#27169;&#22411;&#30340;&#32858;&#21512;&#36755;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26080;&#35770;&#26159;&#22312;&#28608;&#27963;&#36824;&#26159;&#39044;&#27979;&#27700;&#24179;&#19978;&#65292;&#24448;&#24448;&#30456;&#23545;&#20110;&#21333;&#20010;&#27169;&#22411;&#26174;&#31034;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#27969;&#34892;&#27169;&#22411;&#30340;&#30456;&#20114;&#20316;&#29992;&#65306;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21644;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#65288;&#31232;&#30095;MoEs&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#20855;&#26377;&#20114;&#34917;&#30340;&#29305;&#28857;&#65292;&#23427;&#20204;&#30340;&#32467;&#21512;&#26159;&#26377;&#30410;&#30340;&#12290;&#36825;&#21253;&#25324;&#23545;&#31232;&#30095;MoEs&#22312;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#19987;&#23478;&#38598;&#25104;&#65288;E$^3$&#65289;&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#31616;&#21333;&#30340;&#31232;&#30095;MoEs&#38598;&#25104;&#26041;&#27861;&#65292;&#23427;&#20860;&#20855;&#20004;&#31867;&#27169;&#22411;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#28014;&#28857;&#36816;&#31639;&#65288;FLOPs&#65289;&#27604;&#28145;&#24230;&#38598;&#25104;&#23569;&#22810;&#36798;45&#65285;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;E$^3$&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20110;Transformer&#30340;&#35270;&#35273;&#22522;&#32447;&#27169;&#22411;&#19978;&#30340;&#20934;&#30830;&#24615;&#12289;&#23545;&#25968;&#20284;&#28982;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#40065;&#26834;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#25913;&#36827;&#12290;E$^3$&#22312;&#25193;&#23637;&#21040;&#20855;&#26377;&#39640;&#36798;27&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#26102;&#19981;&#20165;&#20445;&#25345;&#20854;&#25928;&#29575;&#65292;&#32780;&#19988;&#36824;&#33021;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models based on the aggregated outputs of submodels, either at the activation or prediction levels, often exhibit strong performance compared to individual models. We study the interplay of two popular classes of such models: ensembles of neural networks and sparse mixture of experts (sparse MoEs). First, we show that the two approaches have complementary features whose combination is beneficial. This includes a comprehensive evaluation of sparse MoEs in uncertainty related benchmarks. Then, we present Efficient Ensemble of Experts (E$^3$), a scalable and simple ensemble of sparse MoEs that takes the best of both classes of models, while using up to 45% fewer FLOPs than a deep ensemble. Extensive experiments demonstrate the accuracy, log-likelihood, few-shot learning, robustness, and uncertainty improvements of E$^3$ over several challenging vision Transformer-based baselines. E$^3$ not only preserves its efficiency while scaling to models with up to 2.7B parameters, b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#33258;&#21160;&#21512;&#25104;&#19968;&#23545;&#31243;&#24207;&#20043;&#38388;&#35821;&#20041;&#31561;&#20215;&#24615;&#30340;&#35777;&#26126;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#37325;&#20889;&#35268;&#21017;&#30340;&#24212;&#29992;&#24207;&#21015;&#23454;&#29616;&#31243;&#24207;&#30340;&#31561;&#20215;&#24615;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2109.10476</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#36890;&#36807;&#37325;&#20889;&#35268;&#21017;&#35777;&#26126;&#30452;&#32447;&#31243;&#24207;&#30340;&#31561;&#20215;&#24615;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning to Prove Equivalence Between Straight-Line Programs via Rewrite Rules. (arXiv:2109.10476v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.10476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#33258;&#21160;&#21512;&#25104;&#19968;&#23545;&#31243;&#24207;&#20043;&#38388;&#35821;&#20041;&#31561;&#20215;&#24615;&#30340;&#35777;&#26126;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#37325;&#20889;&#35268;&#21017;&#30340;&#24212;&#29992;&#24207;&#21015;&#23454;&#29616;&#31243;&#24207;&#30340;&#31561;&#20215;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33268;&#21147;&#20110;&#33258;&#21160;&#21512;&#25104;&#30001;&#19968;&#31995;&#21015;&#35821;&#21477;&#26500;&#25104;&#30340;&#31243;&#24207;&#20043;&#38388;&#35821;&#20041;&#31561;&#20215;&#24615;&#30340;&#35777;&#26126;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#34920;&#31034;&#31243;&#24207;&#65292;&#20854;&#20013;&#19968;&#32452;&#20445;&#25345;&#35821;&#20041;&#30340;&#37325;&#20889;&#35268;&#21017;&#21487;&#20197;&#24212;&#29992;&#20110;&#29305;&#23450;&#30340;AST&#27169;&#24335;&#65292;&#29983;&#25104;&#36716;&#25442;&#21518;&#30340;&#31243;&#24207;&#19982;&#21407;&#31243;&#24207;&#22312;&#35821;&#20041;&#19978;&#31561;&#20215;&#12290;&#22312;&#25105;&#20204;&#30340;&#31995;&#32479;&#20013;&#65292;&#22914;&#26524;&#23384;&#22312;&#19968;&#31995;&#21015;&#37325;&#20889;&#35268;&#21017;&#30340;&#24212;&#29992;&#24207;&#21015;&#23558;&#19968;&#20010;&#31243;&#24207;&#37325;&#20889;&#25104;&#21478;&#19968;&#20010;&#31243;&#24207;&#65292;&#37027;&#20040;&#36825;&#20004;&#20010;&#31243;&#24207;&#26159;&#31561;&#20215;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#29983;&#25104;&#31243;&#24207;&#23545;&#20043;&#38388;&#31561;&#20215;&#24615;&#30340;&#35777;&#26126;&#12290;&#31995;&#32479;&#36755;&#20986;&#19968;&#31995;&#21015;&#37325;&#20889;&#25805;&#20316;&#65292;&#39564;&#35777;&#35813;&#24207;&#21015;&#30340;&#26377;&#25928;&#24615;&#21482;&#38656;&#26816;&#26597;&#20854;&#26159;&#21542;&#21487;&#20197;&#24212;&#29992;&#12290;&#22914;&#26524;&#31070;&#32463;&#32593;&#32476;&#27809;&#26377;&#29983;&#25104;&#26377;&#25928;&#30340;&#24207;&#21015;&#65292;&#31995;&#32479;&#23558;&#25253;&#21578;&#36825;&#20004;&#20010;&#31243;&#24207;&#20026;&#38750;&#31561;&#20215;&#30340;&#65292;&#20174;&#35774;&#35745;&#19978;&#30830;&#20445;&#27809;&#26377;&#38169;&#35823;&#22320;&#23558;&#31243;&#24207;&#25253;&#21578;&#20026;&#31561;&#20215;&#30340;&#24773;&#20917;&#21457;&#29983;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#24050;&#32463;&#23436;&#20840;&#23454;&#29616;&#20110;&#21333;&#20010;&#25991;&#27861;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We target the problem of automatically synthesizing proofs of semantic equivalence between two programs made of sequences of statements. We represent programs using abstract syntax trees (AST), where a given set of semantics-preserving rewrite rules can be applied on a specific AST pattern to generate a transformed and semantically equivalent program. In our system, two programs are equivalent if there exists a sequence of application of these rewrite rules that leads to rewriting one program into the other. We propose a neural network architecture based on a transformer model to generate proofs of equivalence between program pairs. The system outputs a sequence of rewrites, and the validity of the sequence is simply checked by verifying it can be applied. If no valid sequence is produced by the neural network, the system reports the programs as non-equivalent, ensuring by design no programs may be incorrectly reported as equivalent. Our system is fully implemented for one single gramm
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#33258;&#22238;&#24402;&#21644;&#28508;&#21464;&#37327;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#23545;&#31070;&#32463;&#35270;&#39057;&#32534;&#30721;&#31639;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#26102;&#24207;&#33258;&#22238;&#24402;&#21464;&#25442;&#12289;&#25913;&#36827;&#30340;&#29109;&#27169;&#22411;&#20197;&#21450;&#21487;&#21464;&#30721;&#29575;&#29256;&#26412;&#31639;&#27861;&#65292;&#24182;&#22312;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2107.13136</link><description>&lt;p&gt;
&#20174;&#29983;&#25104;&#27169;&#22411;&#20013;&#33719;&#21462;&#31070;&#32463;&#35270;&#39057;&#21387;&#32553;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Insights from Generative Modeling for Neural Video Compression. (arXiv:2107.13136v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.13136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#33258;&#22238;&#24402;&#21644;&#28508;&#21464;&#37327;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#23545;&#31070;&#32463;&#35270;&#39057;&#32534;&#30721;&#31639;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#26102;&#24207;&#33258;&#22238;&#24402;&#21464;&#25442;&#12289;&#25913;&#36827;&#30340;&#29109;&#27169;&#22411;&#20197;&#21450;&#21487;&#21464;&#30721;&#29575;&#29256;&#26412;&#31639;&#27861;&#65292;&#24182;&#22312;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;VAEs&#65289;&#19982;&#23398;&#20064;&#21387;&#32553;&#20013;&#20351;&#29992;&#30340;&#29575;&#22833;&#30495;&#25439;&#22833;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20294;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#22270;&#20687;&#19978;&#12290;&#22312;&#30456;&#20284;&#30340;&#24605;&#36335;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#24230;&#33258;&#22238;&#24402;&#21644;&#28508;&#21464;&#37327;&#24314;&#27169;&#30340;&#35270;&#35282;&#26469;&#35266;&#23519;&#26368;&#36817;&#25552;&#20986;&#30340;&#31070;&#32463;&#35270;&#39057;&#32534;&#30721;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32534;&#30721;&#22120;&#35270;&#20026;&#24191;&#20041;&#38543;&#26426;&#26102;&#24207;&#33258;&#22238;&#24402;&#21464;&#25442;&#30340;&#23454;&#20363;&#65292;&#24182;&#25552;&#20986;&#20102;&#21463;&#24402;&#19968;&#21270;&#27969;&#21644;&#32467;&#26500;&#20808;&#39564;&#21551;&#21457;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26550;&#26500;&#65292;&#20351;&#24471;&#22312;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057;&#21387;&#32553;&#24615;&#33021;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#26435;&#34913;&#21644;&#28040;&#34701;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;i&#65289;&#25913;&#36827;&#30340;&#26102;&#24207;&#33258;&#22238;&#24402;&#21464;&#25442;&#65292;&#65288;ii&#65289;&#24102;&#26377;&#32467;&#26500;&#21644;&#26102;&#24207;&#20381;&#36182;&#24615;&#30340;&#25913;&#36827;&#29109;&#27169;&#22411;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#25105;&#20204;&#31639;&#27861;&#30340;&#21487;&#21464;&#30721;&#29575;&#29256;&#26412;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#25913;&#36827;&#19982;&#22823;&#37327;&#29616;&#26377;&#27169;&#22411;&#20860;&#23481;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#32467;&#26524;&#26469;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent machine learning research has revealed connections between deep generative models such as VAEs and rate-distortion losses used in learned compression, most of this work has focused on images. In a similar spirit, we view recently proposed neural video coding algorithms through the lens of deep autoregressive and latent variable modeling. We present these codecs as instances of a generalized stochastic temporal autoregressive transform, and propose new avenues for further improvements inspired by normalizing flows and structured priors. We propose several architectures that yield state-of-the-art video compression performance on high-resolution video and discuss their tradeoffs and ablations. In particular, we propose (i) improved temporal autoregressive transforms, (ii) improved entropy models with structured and temporal dependencies, and (iii) variable bitrate versions of our algorithms. Since our improvements are compatible with a large class of existing models, we prov
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20998;&#24067;&#24335;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21327;&#21516;&#23398;&#20064;&#19968;&#20010;&#20849;&#21516;&#30340;&#38745;&#24577;&#28508;&#22312;&#20989;&#25968;&#12290;&#36890;&#36807;&#29420;&#31435;&#36816;&#34892;&#22522;&#20110;&#20195;&#29702;&#30340;GPR&#21644;&#21327;&#21516;&#25191;&#34892;&#20998;&#24067;&#24335;GPR&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26377;&#38480;&#30340;&#20195;&#29702;&#38388;&#36890;&#20449;&#21487;&#20197;&#25913;&#21892;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#35780;&#20272;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2105.04738</link><description>&lt;p&gt;
&#36731;&#37327;&#32423;&#20998;&#24067;&#24335;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#29992;&#20110;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Lightweight Distributed Gaussian Process Regression for Online Machine Learning. (arXiv:2105.04738v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.04738
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20998;&#24067;&#24335;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21327;&#21516;&#23398;&#20064;&#19968;&#20010;&#20849;&#21516;&#30340;&#38745;&#24577;&#28508;&#22312;&#20989;&#25968;&#12290;&#36890;&#36807;&#29420;&#31435;&#36816;&#34892;&#22522;&#20110;&#20195;&#29702;&#30340;GPR&#21644;&#21327;&#21516;&#25191;&#34892;&#20998;&#24067;&#24335;GPR&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26377;&#38480;&#30340;&#20195;&#29702;&#38388;&#36890;&#20449;&#21487;&#20197;&#25913;&#21892;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#35780;&#20272;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#19968;&#32452;&#20195;&#29702;&#36890;&#36807;&#27969;&#25968;&#25454;&#21327;&#21516;&#23398;&#20064;&#19968;&#20010;&#20849;&#21516;&#30340;&#38745;&#24577;&#28508;&#22312;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20998;&#24067;&#24335;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#31639;&#27861;&#65292;&#32771;&#34385;&#21040;&#20195;&#29702;&#30340;&#26377;&#38480;&#36890;&#20449;&#12289;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12290;&#27599;&#20010;&#20195;&#29702;&#29420;&#31435;&#22320;&#21033;&#29992;&#26412;&#22320;&#27969;&#25968;&#25454;&#36816;&#34892;&#22522;&#20110;&#20195;&#29702;&#30340;GPR&#26469;&#39044;&#27979;&#24863;&#20852;&#36259;&#30340;&#27979;&#35797;&#28857;&#65307;&#28982;&#21518;&#20195;&#29702;&#20204;&#21327;&#21516;&#25191;&#34892;&#20998;&#24067;&#24335;GPR&#65292;&#24471;&#21040;&#22312;&#20849;&#21516;&#31232;&#30095;&#30340;&#19968;&#32452;&#27979;&#35797;&#28857;&#19978;&#30340;&#20840;&#23616;&#39044;&#27979;&#65307;&#26368;&#21518;&#65292;&#27599;&#20010;&#20195;&#29702;&#23558;&#20998;&#24067;&#24335;GPR&#30340;&#32467;&#26524;&#19982;&#22522;&#20110;&#20195;&#29702;&#30340;GPR&#30340;&#32467;&#26524;&#34701;&#21512;&#65292;&#20197;&#20248;&#21270;&#20854;&#39044;&#27979;&#12290;&#36890;&#36807;&#37327;&#21270;&#39044;&#27979;&#26041;&#24046;&#21644;&#35823;&#24046;&#30340;&#30636;&#24577;&#21644;&#31283;&#24577;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26377;&#38480;&#30340;&#20195;&#29702;&#38388;&#36890;&#20449;&#21487;&#20197;&#25913;&#21892;&#23398;&#20064;&#24615;&#33021;&#65292;&#28385;&#36275;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;&#21033;&#29992;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#35780;&#20272;&#20102;&#25152;&#24320;&#21457;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem where a group of agents aim to collaboratively learn a common static latent function through streaming data. We propose a lightweight distributed Gaussian process regression (GPR) algorithm that is cognizant of agents' limited capabilities in communication, computation and memory. Each agent independently runs agent-based GPR using local streaming data to predict test points of interest; then the agents collaboratively execute distributed GPR to obtain global predictions over a common sparse set of test points; finally, each agent fuses results from distributed GPR with agent-based GPR to refine its predictions. By quantifying the transient and steady-state performances in predictive variance and error, we show that limited inter-agent communication improves learning performances in the sense of Pareto. Monte Carlo simulation is conducted to evaluate the developed algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Riemannian Gauss-Newton&#26041;&#27861;&#36827;&#34892;&#20302;&#31209;&#24352;&#37327;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#23616;&#37096;&#20108;&#27425;&#25910;&#25947;&#24615;&#21644;&#32479;&#35745;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2104.12031</link><description>&lt;p&gt;
&#36890;&#36807;Riemannian Gauss-Newton&#36827;&#34892;&#20302;&#31209;&#24352;&#37327;&#20272;&#35745;&#65306;&#32479;&#35745;&#26368;&#20248;&#24615;&#21644;&#20108;&#38454;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Low-rank Tensor Estimation via Riemannian Gauss-Newton: Statistical Optimality and Second-Order Convergence. (arXiv:2104.12031v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.12031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Riemannian Gauss-Newton&#26041;&#27861;&#36827;&#34892;&#20302;&#31209;&#24352;&#37327;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#23616;&#37096;&#20108;&#27425;&#25910;&#25947;&#24615;&#21644;&#32479;&#35745;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20174;&#19968;&#31995;&#21015;&#26377;&#22122;&#32447;&#24615;&#27979;&#37327;&#20013;&#20272;&#35745;&#20302;Tucker&#31209;&#24352;&#37327;&#30340;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#28085;&#30422;&#20102;&#35768;&#22810;&#20855;&#20307;&#30340;&#24212;&#29992;&#31034;&#20363;&#65292;&#21253;&#25324;&#24352;&#37327;&#22238;&#24402;&#12289;&#24352;&#37327;&#34917;&#20840;&#21644;&#24352;&#37327;PCA / SVD&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;Riemannian Gauss-Newton&#65288;RGN&#65289;&#26041;&#27861;&#26469;&#20272;&#35745;&#20302;Tucker&#31209;&#24352;&#37327;&#12290;&#19982;&#25991;&#29486;&#20013;&#23545;RGN&#30340;&#65288;&#36229;&#65289;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#19981;&#21516;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22122;&#22768;&#29615;&#22659;&#19979;RGN&#22312;&#20302;&#31209;&#24352;&#37327;&#20272;&#35745;&#20013;&#30340;&#31532;&#19968;&#31181;&#23616;&#37096;&#20108;&#27425;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#20272;&#35745;&#35823;&#24046;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#30830;&#23450;&#24615;&#20272;&#35745;&#35823;&#24046;&#19979;&#30028;&#65292;&#35813;&#19979;&#30028;&#19982;&#19978;&#30028;&#30456;&#21305;&#37197;&#65292;&#35777;&#26126;&#20102;RGN&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65288;&#24352;&#37327;&#22238;&#24402;&#21644;&#24352;&#37327;SVD&#65289;&#26469;&#35828;&#26126;RGN&#30340;&#20248;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#25311;&#32467;&#26524;&#26469;&#35777;&#23454;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the estimation of a low Tucker rank tensor from a number of noisy linear measurements. The general problem covers many specific examples arising from applications, including tensor regression, tensor completion, and tensor PCA/SVD. We consider an efficient Riemannian Gauss-Newton (RGN) method for low Tucker rank tensor estimation. Different from the generic (super)linear convergence guarantee of RGN in the literature, we prove the first local quadratic convergence guarantee of RGN for low-rank tensor estimation in the noisy setting under some regularity conditions and provide the corresponding estimation error upper bounds. A deterministic estimation error lower bound, which matches the upper bound, is provided that demonstrates the statistical optimality of RGN. The merit of RGN is illustrated through two machine learning applications: tensor regression and tensor SVD. Finally, we provide the simulation results to corroborate our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#24182;&#20998;&#35299;&#24314;&#27169;&#20219;&#21153;&#65292;&#23454;&#29616;&#23545;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#26032;&#39062;&#24615;&#36827;&#34892;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#24314;&#27169;&#65292;&#20294;&#22312;&#27599;&#20010;&#32858;&#31867;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#36739;&#23569;&#65292;&#23545;&#20110;&#31163;&#25955;&#24207;&#21015;&#26469;&#35828;&#23588;&#20026;&#31361;&#20986;&#12290;</title><link>http://arxiv.org/abs/2103.03943</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#39537;&#21160;&#32858;&#31867;&#21644;&#24314;&#27169;&#23454;&#29616;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Novelty Detection in Sequential Data by Informed Clustering and Modeling. (arXiv:2103.03943v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.03943
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#24182;&#20998;&#35299;&#24314;&#27169;&#20219;&#21153;&#65292;&#23454;&#29616;&#23545;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#26032;&#39062;&#24615;&#36827;&#34892;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#24314;&#27169;&#65292;&#20294;&#22312;&#27599;&#20010;&#32858;&#31867;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#36739;&#23569;&#65292;&#23545;&#20110;&#31163;&#25955;&#24207;&#21015;&#26469;&#35828;&#23588;&#20026;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#25955;&#24207;&#21015;&#20013;&#36827;&#34892;&#26032;&#39062;&#24615;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#19982;&#29983;&#25104;&#27491;&#24120;&#25968;&#25454;&#30340;&#36807;&#31243;&#30456;&#27604;&#65292;&#20559;&#24046;&#36890;&#24120;&#24456;&#23567;&#25110;&#25925;&#24847;&#38544;&#34255;&#12290;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#27491;&#24120;&#24207;&#21015;&#24182;&#27979;&#37327;&#26032;&#24207;&#21015;&#19982;&#27169;&#22411;&#39044;&#27979;&#30340;&#20559;&#24046;&#26469;&#26816;&#27979;&#26032;&#39062;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#26159;&#30001;&#20960;&#20010;&#19981;&#21516;&#30340;&#36807;&#31243;&#29983;&#25104;&#30340;&#65292;&#22240;&#27492;&#22312;&#25152;&#26377;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#24448;&#24448;&#36807;&#24230;&#27867;&#21270;&#65292;&#26032;&#39062;&#24615;&#20173;&#28982;&#26410;&#34987;&#26816;&#27979;&#21040;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20998;&#35299;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65306;&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#65292;&#20174;&#32780;&#22312;&#27599;&#20010;&#32858;&#31867;&#20013;&#33719;&#24471;&#26356;&#31616;&#21333;&#30340;&#24314;&#27169;&#20219;&#21153;&#65292;&#20174;&#32780;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#36825;&#20250;&#24102;&#26469;&#19968;&#20010;&#26435;&#34913;&#65292;&#22240;&#20026;&#27599;&#20010;&#32858;&#31867;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#12290;&#36825;&#23545;&#20110;&#25968;&#25454;&#38656;&#27714;&#37327;&#36739;&#22823;&#30340;&#31163;&#25955;&#24207;&#21015;&#26469;&#35828;&#26159;&#19968;&#20010;&#29305;&#27530;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#32858;&#31867;&#30340;&#36136;&#37327;&#65292;&#21363;&#20010;&#20307;&#23398;&#20064;&#38382;&#39064;&#26159;&#21542;&#36275;&#22815;
&lt;/p&gt;
&lt;p&gt;
Novelty detection in discrete sequences is a challenging task, since deviations from the process generating the normal data are often small or intentionally hidden. Novelties can be detected by modeling normal sequences and measuring the deviations of a new sequence from the model predictions. However, in many applications data is generated by several distinct processes so that models trained on all the data tend to over-generalize and novelties remain undetected. We propose to approach this challenge through decomposition: by clustering the data we break down the problem, obtaining simpler modeling task in each cluster which can be modeled more accurately. However, this comes at a trade-off, since the amount of training data per cluster is reduced. This is a particular problem for discrete sequences where state-of-the-art models are data-hungry. The success of this approach thus depends on the quality of the clustering, i.e., whether the individual learning problems are sufficiently s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#32467;&#26500;&#37325;&#35201;&#24615;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#22797;&#21046;&#22312;&#36739;&#19981;&#32597;&#35265;&#30340;&#26679;&#26412;&#20013;&#35266;&#23519;&#21040;&#30340;&#27987;&#24230;&#29305;&#24615;&#65292;&#38544;&#24335;&#35825;&#23548;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;IS&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20272;&#35745;&#24615;&#33021;&#24230;&#37327;&#30340;&#20998;&#24067;&#23614;&#37096;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2102.07060</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#32467;&#26500;&#37325;&#35201;&#24615;&#37319;&#26679;&#23454;&#29616;&#40657;&#30418;&#27169;&#25311;&#20998;&#24067;&#23614;&#37096;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Achieving Efficiency in Black Box Simulation of Distribution Tails with Self-structuring Importance Samplers. (arXiv:2102.07060v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.07060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#32467;&#26500;&#37325;&#35201;&#24615;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#22797;&#21046;&#22312;&#36739;&#19981;&#32597;&#35265;&#30340;&#26679;&#26412;&#20013;&#35266;&#23519;&#21040;&#30340;&#27987;&#24230;&#29305;&#24615;&#65292;&#38544;&#24335;&#35825;&#23548;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;IS&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20272;&#35745;&#24615;&#33021;&#24230;&#37327;&#30340;&#20998;&#24067;&#23614;&#37096;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#35201;&#24615;&#37319;&#26679;&#65288;IS&#65289;&#26041;&#26696;&#65292;&#29992;&#20110;&#20272;&#35745;&#37319;&#29992;&#20016;&#23500;&#24037;&#20855;&#27169;&#25311;&#30340;&#24615;&#33021;&#24230;&#37327;&#30340;&#20998;&#24067;&#23614;&#37096;&#65292;&#36825;&#20123;&#24037;&#20855;&#21253;&#25324;&#32447;&#24615;&#35268;&#21010;&#12289;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#12289;&#20998;&#27573;&#32447;&#24615;/&#20108;&#27425;&#30446;&#26631;&#12289;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25351;&#23450;&#30340;&#29305;&#24449;&#26144;&#23556;&#31561;&#12290;&#20256;&#32479;&#30340;&#26126;&#30830;&#35782;&#21035;&#26377;&#25928;&#30340;&#27979;&#24230;&#21464;&#21270;&#30340;&#26041;&#27861;&#22312;&#39640;&#24230;&#26684;&#24335;&#21270;&#30340;&#27169;&#22411;&#20043;&#22806;&#21463;&#21040;&#21487;&#34892;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#19982;&#30446;&#26631;&#21644;&#27010;&#29575;&#20998;&#24067;&#31934;&#24515;&#35843;&#25972;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#20013;&#65292;&#36890;&#36807;&#19968;&#31181;&#22522;&#26412;&#36716;&#25442;&#20811;&#26381;&#20102;&#36825;&#20010;&#29942;&#39048;&#65292;&#35813;&#36716;&#25442;&#21487;&#20197;&#36890;&#36807;&#22797;&#21046;&#22312;&#36739;&#19981;&#32597;&#35265;&#30340;&#26679;&#26412;&#20013;&#35266;&#23519;&#21040;&#30340;&#27987;&#24230;&#29305;&#24615;&#26469;&#22312;&#21508;&#31181;&#27169;&#22411;&#20013;&#38544;&#24335;&#35825;&#23548;&#20986;&#19968;&#20010;&#26377;&#25928;&#30340;IS&#20998;&#24067;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#21457;&#23637;&#19968;&#20010;&#22823;&#20559;&#24046;&#21407;&#29702;&#26469;&#25351;&#23548;&#30340;&#65292;&#36825;&#31181;&#21407;&#29702;&#25581;&#31034;&#20102;&#26368;&#20248;IS&#20998;&#24067;&#30340;&#33258;&#30456;&#20284;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel Importance Sampling (IS) scheme for estimating distribution tails of performance measures modeled with a rich set of tools such as linear programs, integer linear programs, piecewise linear/quadratic objectives, feature maps specified with deep neural networks, etc. The conventional approach of explicitly identifying efficient changes of measure suffers from feasibility and scalability concerns beyond highly stylized models, due to their need to be tailored intricately to the objective and the underlying probability distribution. This bottleneck is overcome in the proposed scheme with an elementary transformation which is capable of implicitly inducing an effective IS distribution in a variety of models by replicating the concentration properties observed in less rare samples. This novel approach is guided by developing a large deviations principle that brings out the phenomenon of self-similarity of optimal IS distributions. The proposed sampler is the firs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#22122;&#22768;&#28304;&#30340;&#24773;&#20917;&#19979;&#30340;&#25968;&#25454;&#39537;&#21160;&#22788;&#26041;&#38382;&#39064;&#65292;&#24182;&#23548;&#20986;&#20102;&#22312;&#36825;&#20010;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#25968;&#25454;&#39537;&#21160;&#24418;&#24335;&#65292;&#24182;&#25351;&#20986;&#23427;&#20204;&#20855;&#26377;&#29109;&#26368;&#20248;&#20256;&#36755;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2102.04363</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#19982;&#26377;&#22122;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Efficient Data-Driven Optimization with Noisy Data. (arXiv:2102.04363v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.04363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#22122;&#22768;&#28304;&#30340;&#24773;&#20917;&#19979;&#30340;&#25968;&#25454;&#39537;&#21160;&#22788;&#26041;&#38382;&#39064;&#65292;&#24182;&#23548;&#20986;&#20102;&#22312;&#36825;&#20010;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#25968;&#25454;&#39537;&#21160;&#24418;&#24335;&#65292;&#24182;&#25351;&#20986;&#23427;&#20204;&#20855;&#26377;&#29109;&#26368;&#20248;&#20256;&#36755;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20915;&#31574;&#32773;&#38754;&#20020;&#30340;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21487;&#29992;&#20110;&#20915;&#31574;&#30340;&#25968;&#25454;&#37117;&#21463;&#21040;&#19968;&#23450;&#31243;&#24230;&#30340;&#27979;&#37327;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#22122;&#22768;&#28304;&#30340;&#24773;&#20917;&#19979;&#30340;&#25968;&#25454;&#39537;&#21160;&#22788;&#26041;&#38382;&#39064;&#65292;&#24182;&#23548;&#20986;&#20102;&#22312;&#36825;&#20010;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#25968;&#25454;&#39537;&#21160;&#24418;&#24335;&#65292;&#24182;&#25351;&#20986;&#23427;&#20204;&#20855;&#26377;&#29109;&#26368;&#20248;&#20256;&#36755;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#21033;&#29992;Strassen&#30340;&#32463;&#20856;&#34920;&#31034;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26377;&#25928;&#30340;&#40065;&#26834;&#24418;&#24335;&#22312;&#20960;&#31181;&#26377;&#36259;&#30340;&#35774;&#32622;&#19979;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical Kullback-Leibler or entropic distances are known to enjoy certain desirable statistical properties in the context of decision-making with noiseless data. However, in most practical situations the data available to a decision maker is subject to a certain amount of measurement noise. We hence study here data-driven prescription problems in which the data is corrupted by a known noise source. We derive efficient data-driven formulations in this noisy regime and indicate that they enjoy an entropic optimal transport interpretation. Finally, we show that these efficient robust formulations are tractable in several interesting settings by exploiting a classical representation result by Strassen.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#36830;&#20998;&#25968;&#26469;&#23398;&#20064;&#22806;&#25512;&#30340;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#36229;&#23548;&#20307;&#26448;&#26009;&#30340;&#20020;&#30028;&#28201;&#24230;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#38656;&#35201;&#22312;&#25193;&#23637;&#39046;&#22495;&#20869;&#20934;&#30830;&#36817;&#20284;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#23588;&#20854;&#22312;&#35774;&#35745;&#26032;&#32467;&#26500;&#26102;&#26368;&#20026;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2012.03774</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#36830;&#20998;&#25968;&#36827;&#34892;&#22806;&#25512;&#65306;&#39044;&#27979;&#36229;&#23548;&#20307;&#26448;&#26009;&#30340;&#20020;&#30028;&#28201;&#24230;
&lt;/p&gt;
&lt;p&gt;
Learning to extrapolate using continued fractions: Predicting the critical temperature of superconductor materials. (arXiv:2012.03774v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.03774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#36830;&#20998;&#25968;&#26469;&#23398;&#20064;&#22806;&#25512;&#30340;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#36229;&#23548;&#20307;&#26448;&#26009;&#30340;&#20020;&#30028;&#28201;&#24230;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#38656;&#35201;&#22312;&#25193;&#23637;&#39046;&#22495;&#20869;&#20934;&#30830;&#36817;&#20284;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#23588;&#20854;&#22312;&#35774;&#35745;&#26032;&#32467;&#26500;&#26102;&#26368;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#23454;&#20363;$S={(\mathbf{x^{(i)}},y^{(i)})}$&#26469;&#36817;&#20284;&#26410;&#30693;&#30446;&#26631;&#20989;&#25968;$y=f(\mathbf{x})$&#65292;&#20854;&#20013;$\mathbf{x^{(i)}}\in D$&#65292;$D$&#34920;&#31034;&#24863;&#20852;&#36259;&#30340;&#39046;&#22495;&#65292;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#23558;$S$&#31216;&#20026;&#35757;&#32451;&#38598;&#65292;&#24182;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#20302;&#22797;&#26434;&#24230;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#36825;&#20010;&#30446;&#26631;&#20989;&#25968;&#23545;&#20110;&#26032;&#30340;&#23454;&#20363;$\mathbf{x}$&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36890;&#36807;&#22312;&#19968;&#20010;&#21333;&#29420;&#30340;&#38598;&#21512;$T=\{\mathbf{x^{(j)}}\}\subset D$&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#20854;&#20013;$T\neq S$&#65292;&#32463;&#24120;&#26377;$T\cap S=\emptyset$&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#24212;&#29992;&#19981;&#20165;&#38656;&#35201;&#22312;&#21407;&#22987;&#39046;&#22495;$D$&#20869;&#20934;&#30830;&#36817;&#20284;&#65292;&#36824;&#38656;&#35201;&#22312;&#21253;&#21547;$D$&#30340;&#25193;&#23637;&#39046;&#22495;$D'$&#20869;&#20934;&#30830;&#36817;&#20284;&#12290;&#36825;&#22312;&#28041;&#21450;&#26032;&#32467;&#26500;&#35774;&#35745;&#30340;&#22330;&#26223;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#26368;&#23567;&#21270;&#36817;&#20284;&#35823;&#24046;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of Artificial Intelligence (AI) and Machine Learning (ML), the approximation of unknown target functions $y=f(\mathbf{x})$ using limited instances $S={(\mathbf{x^{(i)}},y^{(i)})}$, where $\mathbf{x^{(i)}} \in D$ and $D$ represents the domain of interest, is a common objective. We refer to $S$ as the training set and aim to identify a low-complexity mathematical model that can effectively approximate this target function for new instances $\mathbf{x}$. Consequently, the model's generalization ability is evaluated on a separate set $T=\{\mathbf{x^{(j)}}\} \subset D$, where $T \neq S$, frequently with $T \cap S = \emptyset$, to assess its performance beyond the training set. However, certain applications require accurate approximation not only within the original domain $D$ but also in an extended domain $D'$ that encompasses $D$. This becomes particularly relevant in scenarios involving the design of new structures, where minimizing errors in approximations is crucial. For e
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOTHIC&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21452;&#26680;&#26143;&#31995;&#65292;&#21457;&#29616;&#20102;&#35768;&#22810;&#21452;&#37325;&#27963;&#21160;&#26143;&#31995;&#26680;&#12290;&#25105;&#20204;&#22312;&#22823;&#26679;&#26412;&#20013;&#26816;&#27979;&#21040;&#20102;159&#20010;&#21452;&#37325;AGN&#65292;&#20854;&#20013;2&#20010;&#26159;&#19977;&#37325;AGN&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2011.12177</link><description>&lt;p&gt;
&#20351;&#29992;GOTHIC&#33258;&#21160;&#26816;&#27979;&#21452;&#26680;&#26143;&#31995;&#24182;&#21457;&#29616;&#19968;&#22823;&#25209;&#21452;&#37325;AGN
&lt;/p&gt;
&lt;p&gt;
Automated Detection of Double Nuclei Galaxies using GOTHIC and the Discovery of a Large Sample of Dual AGN. (arXiv:2011.12177v3 [astro-ph.GA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.12177
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOTHIC&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21452;&#26680;&#26143;&#31995;&#65292;&#21457;&#29616;&#20102;&#35768;&#22810;&#21452;&#37325;&#27963;&#21160;&#26143;&#31995;&#26680;&#12290;&#25105;&#20204;&#22312;&#22823;&#26679;&#26412;&#20013;&#26816;&#27979;&#21040;&#20102;159&#20010;&#21452;&#37325;AGN&#65292;&#20854;&#20013;2&#20010;&#26159;&#19977;&#37325;AGN&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;GOTHIC&#65288;&#22270;&#24418;&#22686;&#24378;&#36845;&#20195;&#29228;&#23665;&#31639;&#27861;&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#20855;&#26377;&#20004;&#20010;&#25110;&#22810;&#20010;&#23494;&#38598;&#20998;&#31163;&#26680;&#30340;&#26143;&#31995;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#26143;&#31995;&#20013;&#26816;&#27979;&#21452;&#37325;&#25110;&#22810;&#37325;&#27963;&#21160;&#26143;&#31995;&#26680;&#65288;AGN&#65289;&#30340;&#26679;&#26412;&#12290;&#23613;&#31649;&#26143;&#31995;&#21512;&#24182;&#24456;&#24120;&#35265;&#65292;&#20294;&#21452;&#37325;AGN&#30340;&#26816;&#27979;&#24456;&#32597;&#35265;&#12290;&#23427;&#20204;&#30340;&#26816;&#27979;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#36229;&#22823;&#36136;&#37327;&#40657;&#27934;&#65288;SMBH&#65289;&#21452;&#26143;&#30340;&#24418;&#25104;&#12289;SMBH&#30340;&#22686;&#38271;&#21644;&#22810;&#26680;&#31995;&#32479;&#20013;&#30340;AGN&#21453;&#39304;&#25928;&#24212;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#31639;&#27861;&#23545;&#29616;&#26377;&#30340;&#25104;&#20687;&#25968;&#25454;&#36827;&#34892;&#31995;&#32479;&#35843;&#26597;&#65292;&#20197;&#21457;&#29616;&#21452;&#26680;&#26143;&#31995;&#21644;&#21452;&#37325;AGN&#12290;&#25105;&#20204;&#22312;&#24050;&#30693;&#30340;DNG&#26679;&#26412;&#19978;&#27979;&#35797;&#20102;GOTHIC&#65292;&#24182;&#38543;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;SDSS DR16&#20013;&#22823;&#32422;100&#19975;&#20010;&#22788;&#20110;&#32418;&#31227;&#33539;&#22260;0&#33267;0.75&#30340;&#26143;&#31995;&#26679;&#26412;&#65292;&#24182;&#20855;&#26377;&#21487;&#29992;&#30340;&#20809;&#35889;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#26679;&#26412;&#20013;&#26816;&#27979;&#21040;159&#20010;&#21452;&#37325;AGN&#65292;&#20854;&#20013;2&#20010;&#26159;&#19977;&#37325;AGN&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel algorithm to detect double nuclei galaxies (DNG) called GOTHIC (Graph BOosted iterated HIll Climbing) - that detects whether a given image of a galaxy has two or more closely separated nuclei. Our aim is to detect samples of dual or multiple active galactic nuclei (AGN) in galaxies. Although galaxy mergers are common, the detection of dual AGN is rare. Their detection is very important as they help us understand the formation of supermassive black hole (SMBH) binaries, SMBH growth and AGN feedback effects in multiple nuclei systems. There is thus a need for an algorithm to do a systematic survey of existing imaging data for the discovery of DNGs and dual AGN. We have tested GOTHIC on a known sample of DNGs and subsequently applied it to a sample of a million SDSS DR16 galaxies lying in the redshift range of 0 to 0.75 approximately, and have available spectroscopic data. We have detected 159 dual AGN in this sample, of which 2 are triple AGN systems. Our results show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32467;&#26500;&#21270;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#22312;&#23384;&#22312;&#32467;&#26500;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#20197;&#26368;&#23567;&#21270;&#21518;&#24724;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2007.07302</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#26368;&#20248;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Learning for Structured Bandits. (arXiv:2007.07302v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.07302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32467;&#26500;&#21270;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#22312;&#23384;&#22312;&#32467;&#26500;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#20197;&#26368;&#23567;&#21270;&#21518;&#24724;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32467;&#26500;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#21363;&#22312;&#23384;&#22312;&#32467;&#26500;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#19979;&#36827;&#34892;&#22312;&#32447;&#20915;&#31574;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#20915;&#31574;&#32773;&#38656;&#35201;&#22312;&#35266;&#23519;&#21040;&#19981;&#30830;&#23450;&#30340;&#22870;&#21169;&#38543;&#26102;&#38388;&#21464;&#21270;&#26102;&#25214;&#20986;&#26368;&#20339;&#34892;&#21160;&#26041;&#38024;&#12290;&#20915;&#31574;&#32773;&#24050;&#32463;&#20102;&#35299;&#21040;&#22870;&#21169;&#20998;&#24067;&#23646;&#20110;&#19968;&#20010;&#20984;&#32039;&#33268;&#38598;&#21512;&#30340;&#26576;&#31181;&#20984;&#32467;&#26500;&#20449;&#24687;&#12290;&#22312;&#26377;&#36825;&#31181;&#32467;&#26500;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20915;&#31574;&#32773;&#24076;&#26395;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#26368;&#23567;&#21270;&#21518;&#24724;&#65288;&#19982;&#25552;&#21069;&#30693;&#36947;&#26368;&#20339;&#34892;&#21160;&#30340;&#22522;&#20934;&#31574;&#30053;&#30456;&#27604;&#30340;&#24615;&#33021;&#24046;&#24322;&#65289;&#12290;&#22312;&#27809;&#26377;&#32467;&#26500;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#20856;&#30340;&#19978;&#30028;&#32622;&#20449;&#21306;&#38388;&#65288;UCB&#65289;&#21644;&#27748;&#22982;&#36874;&#21462;&#26679;&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#26368;&#23567;&#21518;&#24724;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#25351;&#20986;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#26080;&#27861;&#21033;&#29992;&#22797;&#26434;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study structured multi-armed bandits, which is the problem of online decision-making under uncertainty in the presence of structural information. In this problem, the decision-maker needs to discover the best course of action despite observing only uncertain rewards over time. The decision-maker is aware of certain convex structural information regarding the reward distributions; that is, the decision-maker knows the reward distributions of the arms belong to a convex compact set. In the presence such structural information, they then would like to minimize their regret by exploiting this information, where the regret is its performance difference against a benchmark policy that knows the best action ahead of time. In the absence of structural information, the classical upper confidence bound (UCB) and Thomson sampling algorithms are well known to suffer minimal regret. As recently pointed out, neither algorithms are, however, capable of exploiting structural information that is com
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;18&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25915;&#20987;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#23545;&#21508;&#31181;&#26410;&#39044;&#26009;&#21040;&#30340;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#30340;&#26032;&#22522;&#20934;&#12290;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#19968;&#31995;&#21015;&#38450;&#24481;&#31574;&#30053;&#65292;&#21487;&#20197;&#24110;&#21161;&#20811;&#26381;&#35757;&#32451;&#26399;&#38388;&#26410;&#32771;&#34385;&#21040;&#30340;&#23545;&#25163;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#30340;&#32467;&#26524;&#23558;&#20026;&#30740;&#31350;&#29616;&#23454;&#19990;&#30028;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#25552;&#20379;&#26377;&#29992;&#24037;&#20855;&#65292;&#20419;&#36827;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/1908.08016</link><description>&lt;p&gt;
&#38024;&#23545;&#26410;&#39044;&#26009;&#21040;&#30340;&#23545;&#25163;&#27979;&#35797;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Testing Robustness Against Unforeseen Adversaries. (arXiv:1908.08016v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1908.08016
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;18&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25915;&#20987;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#23545;&#21508;&#31181;&#26410;&#39044;&#26009;&#21040;&#30340;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#30340;&#26032;&#22522;&#20934;&#12290;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#19968;&#31995;&#21015;&#38450;&#24481;&#31574;&#30053;&#65292;&#21487;&#20197;&#24110;&#21161;&#20811;&#26381;&#35757;&#32451;&#26399;&#38388;&#26410;&#32771;&#34385;&#21040;&#30340;&#23545;&#25163;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#30340;&#32467;&#26524;&#23558;&#20026;&#30740;&#31350;&#29616;&#23454;&#19990;&#30028;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#25552;&#20379;&#26377;&#29992;&#24037;&#20855;&#65292;&#20419;&#36827;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;&#29616;&#23454;&#19990;&#30028;&#30340;&#23545;&#25239;&#29615;&#22659;&#26102;&#65292;&#38450;&#24481;&#32773;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#22826;&#21487;&#33021;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#23545;&#25163;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#23545;&#25163;&#24456;&#21487;&#33021;&#20351;&#29992;&#36924;&#30495;&#30340;&#23545;&#25239;&#25197;&#26354;&#65292;&#32780;&#19981;&#38480;&#20110;&#23567;&#30340;L_p&#32422;&#26463;&#25200;&#21160;&#12290;&#20026;&#20102;&#32553;&#23567;&#30740;&#31350;&#21644;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;18&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#21019;&#24314;&#20102;ImageNet-UA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#21508;&#31181;&#26410;&#39044;&#26009;&#21040;&#30340;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#30340;&#26032;&#22522;&#20934;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#22522;&#20934;&#26469;&#35782;&#21035;&#19968;&#31995;&#21015;&#33021;&#22815;&#24110;&#21161;&#20811;&#26381;&#36825;&#31181;&#27867;&#21270;&#24046;&#36317;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#21457;&#29616;&#20102;&#21487;&#20197;&#25552;&#39640;&#23545;&#26410;&#39044;&#26009;&#21040;&#30340;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#30340;&#25216;&#26415;&#30340;&#20016;&#23500;&#31354;&#38388;&#12290;&#25105;&#20204;&#24076;&#26395;ImageNet-UA&#30340;&#26356;&#22810;&#26679;&#24615;&#21644;&#36924;&#30495;&#24615;&#23558;&#25104;&#20026;&#37027;&#20123;&#30740;&#31350;&#29616;&#23454;&#19990;&#30028;&#26368;&#22351;&#24773;&#20917;&#30340;&#40065;&#26834;&#24615;&#30340;&#20154;&#30340;&#26377;&#29992;&#24037;&#20855;&#65292;&#20174;&#32780;&#20419;&#36827;&#24320;&#21457;&#33021;&#22815;&#22312;&#35757;&#32451;&#26399;&#38388;&#30475;&#19981;&#21040;&#30340;&#25915;&#20987;&#20013;&#36827;&#34892;&#27867;&#21270;&#30340;&#26356;&#24378;&#22823;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
When considering real-world adversarial settings, defenders are unlikely to have access to the full range of deployment-time adversaries during training, and adversaries are likely to use realistic adversarial distortions that will not be limited to small L_p-constrained perturbations. To narrow in on this discrepancy between research and reality we introduce eighteen novel adversarial attacks, which we use to create ImageNet-UA, a new benchmark for evaluating model robustness against a wide range of unforeseen adversaries. We make use of our benchmark to identify a range of defense strategies which can help overcome this generalization gap, finding a rich space of techniques which can improve unforeseen robustness. We hope the greater variety and realism of ImageNet-UA will make it a useful tool for those working on real-world worst-case robustness, enabling development of more robust defenses which can generalize beyond attacks seen during training.
&lt;/p&gt;</description></item></channel></rss>