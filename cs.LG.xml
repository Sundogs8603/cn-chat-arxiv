<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>AutoRT&#26159;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#24182;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.12963</link><description>&lt;p&gt;
AutoRT&#65306;&#22823;&#35268;&#27169;&#32534;&#25490;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents. (arXiv:2401.12963v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12963
&lt;/p&gt;
&lt;p&gt;
AutoRT&#26159;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#24182;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#26377;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#34892;&#21160;&#31561;&#21151;&#33021;&#30340;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#21033;&#29992;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#26469;&#25512;&#29702;&#26377;&#29992;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#32570;&#20047;&#22522;&#20110;&#29289;&#29702;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoRT&#65292;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#23436;&#20840;&#26410;&#30693;&#22330;&#26223;&#20013;&#25805;&#20316;&#26426;&#22120;&#20154;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#21482;&#38656;&#35201;&#26368;&#23569;&#30340;&#20154;&#24037;&#30417;&#30563;&#12290;AutoRT&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#23454;&#29616;&#22330;&#26223;&#29702;&#35299;&#21644;&#22522;&#30784;&#32465;&#23450;&#65292;&#24182;&#36827;&#19968;&#27493;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#20379;&#19968;&#32452;&#26426;&#22120;&#20154;&#25191;&#34892;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#25351;&#23548;&#25968;&#25454;&#25910;&#38598;&#65292;AutoRT&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#65292;&#21516;&#26102;&#26174;&#33879;&#25193;&#22823;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;AutoRT&#21521;20&#22810;&#20010;&#26426;&#22120;&#20154;&#25552;&#35758;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multi
&lt;/p&gt;</description></item><item><title>&#32842;&#22825;&#23453;&#30418;&#65288;Chatterbox&#65289;&#26159;&#38024;&#23545;LLM Chatbots&#20013;&#30340;&#20196;&#29260;&#27969;&#38382;&#39064;&#25552;&#20986;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#36755;&#23618;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#26032;&#29983;&#25104;&#30340;&#20196;&#29260;&#21644;&#24403;&#21069;&#26410;&#30830;&#35748;&#30340;&#20196;&#29260;&#25918;&#20837;&#19979;&#19968;&#20010;&#21457;&#36865;&#30340;&#25968;&#25454;&#21253;&#20013;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#20256;&#36755;&#21644;&#28210;&#26579;&#65292;&#36991;&#20813;&#20102;&#22312;&#19981;&#31283;&#23450;&#32593;&#32476;&#29615;&#22659;&#19979;&#30340;&#20572;&#39039;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2401.12961</link><description>&lt;p&gt;
&#32842;&#22825;&#23453;&#30418;&#65306;&#19981;&#31283;&#23450;&#32593;&#32476;&#19979;LLM Token Streaming&#30340;&#31283;&#20581;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Chatterbox: Robust Transport for LLM Token Streaming under Unstable Network. (arXiv:2401.12961v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12961
&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#23453;&#30418;&#65288;Chatterbox&#65289;&#26159;&#38024;&#23545;LLM Chatbots&#20013;&#30340;&#20196;&#29260;&#27969;&#38382;&#39064;&#25552;&#20986;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#36755;&#23618;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#26032;&#29983;&#25104;&#30340;&#20196;&#29260;&#21644;&#24403;&#21069;&#26410;&#30830;&#35748;&#30340;&#20196;&#29260;&#25918;&#20837;&#19979;&#19968;&#20010;&#21457;&#36865;&#30340;&#25968;&#25454;&#21253;&#20013;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#20256;&#36755;&#21644;&#28210;&#26579;&#65292;&#36991;&#20813;&#20102;&#22312;&#19981;&#31283;&#23450;&#32593;&#32476;&#29615;&#22659;&#19979;&#30340;&#20572;&#39039;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#26102;&#28210;&#26579;&#29983;&#25104;&#30340;&#20196;&#29260;&#65292;LLM&#26381;&#21153;&#22120;&#36880;&#20010;&#29983;&#25104;&#21709;&#24212;&#20196;&#29260;&#65292;&#24182;&#36890;&#36807;&#32593;&#32476;&#23558;&#27599;&#20010;&#29983;&#25104;&#30340;&#20196;&#29260;&#65288;&#25110;&#23569;&#37327;&#20196;&#29260;&#32452;&#65289;&#27969;&#24335;&#20256;&#36755;&#21040;&#29992;&#25143;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;LLM&#20196;&#29260;&#27969;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#31283;&#23450;&#30340;&#32593;&#32476;&#26465;&#20214;&#19979;&#65292;LLM&#20196;&#29260;&#20256;&#36755;&#20307;&#39564;&#21487;&#33021;&#20250;&#21463;&#21040;&#26497;&#22823;&#30340;&#20572;&#39039;&#24433;&#21709;&#65292;&#22240;&#20026;&#19968;&#27425;&#25968;&#25454;&#21253;&#20002;&#22833;&#21487;&#33021;&#20250;&#38459;&#22622;&#21518;&#32493;&#25968;&#25454;&#21253;&#20013;&#21253;&#21547;&#30340;&#20196;&#29260;&#30340;&#28210;&#26579;&#65292;&#21363;&#20351;&#23427;&#20204;&#25353;&#26102;&#21040;&#36798;&#12290;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#37327;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#24212;&#29992;&#31243;&#24207;&#65288;&#21253;&#25324;ChatGPT&#65292;Claude&#21644;Bard&#65289;&#22312;&#19981;&#31283;&#23450;&#32593;&#32476;&#26465;&#20214;&#19979;&#37117;&#20250;&#36973;&#21463;&#20572;&#39039;&#38382;&#39064;&#30340;&#22686;&#21152;&#12290;&#38024;&#23545;LLM Chatbots&#20013;&#20986;&#29616;&#30340;&#36825;&#20010;&#26032;&#20852;&#30340;&#20196;&#29260;&#27969;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#36755;&#23618;&#26041;&#26696;&#65292;&#31216;&#20026;&#32842;&#22825;&#23453;&#30418;&#65288;Chatterbox&#65289;&#65292;&#23427;&#23558;&#26032;&#29983;&#25104;&#30340;&#20196;&#29260;&#21644;&#24403;&#21069;&#26410;&#30830;&#35748;&#30340;&#20196;&#29260;&#25918;&#20837;&#19979;&#19968;&#20010;&#21457;&#36865;&#30340;&#25968;&#25454;&#21253;&#20013;&#12290;&#36825;&#26679;&#65292;&#27599;&#20010;&#25968;&#25454;&#21253;&#37117;&#21253;&#21547;&#19968;&#20123;&#26032;&#30340;&#20196;&#29260;&#65292;&#24182;&#19988;&#22312;&#25509;&#25910;&#21040;&#26102;&#21487;&#20197;&#29420;&#31435;&#36827;&#34892;&#28210;&#26579;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20572;&#39039;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
To render each generated token in real time, the LLM server generates response tokens one by one and streams each generated token (or group of a few tokens) through the network to the user right after it is generated, which we refer to as LLM token streaming. However, under unstable network conditions, the LLM token streaming experience could suffer greatly from stalls since one packet loss could block the rendering of tokens contained in subsequent packets even if they arrive on time. With a real-world measurement study, we show that current applications including ChatGPT, Claude, and Bard all suffer from increased stall under unstable network.  For this emerging token streaming problem in LLM Chatbots, we propose a novel transport layer scheme, called Chatterbox, which puts new generated tokens as well as currently unacknowledged tokens in the next outgoing packet. This ensures that each packet contains some new tokens and can be independently rendered when received, thus avoiding af
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#36817;&#20284;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23376;&#31354;&#38388;&#25512;&#29702;&#26469;&#35299;&#20915;&#21322;&#32467;&#26500;&#22238;&#24402;&#27169;&#22411;&#20013;&#35299;&#37322;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#25512;&#26029;&#36755;&#20837;-&#36755;&#20986;&#20851;&#31995;&#21644;&#25429;&#25417;&#22810;&#37325;&#35201;&#32032;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12950</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#21322;&#32467;&#26500;&#23376;&#31354;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Bayesian Semi-structured Subspace Inference. (arXiv:2401.12950v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#36817;&#20284;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23376;&#31354;&#38388;&#25512;&#29702;&#26469;&#35299;&#20915;&#21322;&#32467;&#26500;&#22238;&#24402;&#27169;&#22411;&#20013;&#35299;&#37322;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#25512;&#26029;&#36755;&#20837;-&#36755;&#20986;&#20851;&#31995;&#21644;&#25429;&#25417;&#22810;&#37325;&#35201;&#32032;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#32467;&#26500;&#22238;&#24402;&#27169;&#22411;&#33021;&#22815;&#32852;&#21512;&#24314;&#27169;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#21270;&#29305;&#24449;&#25928;&#24212;&#21644;&#22797;&#26434;&#30340;&#38750;&#32467;&#26500;&#21270;&#29305;&#24449;&#25928;&#24212;&#12290;&#32467;&#26500;&#21270;&#27169;&#22411;&#37096;&#20998;&#21463;&#32479;&#35745;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#21487;&#29992;&#20110;&#25512;&#26029;&#37325;&#35201;&#29305;&#24449;&#30340;&#36755;&#20837;-&#36755;&#20986;&#20851;&#31995;&#12290;&#38750;&#32467;&#26500;&#21270;&#37096;&#20998;&#23450;&#20041;&#20102;&#19968;&#20010;&#20219;&#24847;&#28145;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#28789;&#27963;&#24615;&#65292;&#20197;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#20063;&#21487;&#20197;&#35299;&#37322;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#22312;&#35299;&#37322;&#24615;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#20173;&#28982;&#32570;&#20047;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#23376;&#31354;&#38388;&#25512;&#29702;&#65292;&#38024;&#23545;&#21322;&#32467;&#26500;&#22238;&#24402;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#36817;&#20284;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#23545;&#32467;&#26500;&#21270;&#25928;&#24212;&#30340;&#23436;&#25972;&#21442;&#25968;&#31354;&#38388;&#21644;&#38750;&#32467;&#26500;&#21270;&#25928;&#24212;&#30340;&#23376;&#31354;&#38388;&#30340;&#32852;&#21512;&#21518;&#39564;&#37319;&#26679;&#30340;&#23376;&#31354;&#38388;&#25512;&#29702;&#12290;&#38500;&#20102;&#36825;&#31181;&#28151;&#21512;&#37319;&#26679;&#26041;&#26696;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#20801;&#35768;&#23376;&#31354;&#38388;&#30340;&#21487;&#35843;&#22797;&#26434;&#24615;&#65292;&#24182;&#33021;&#25429;&#25417;&#22810;&#37325;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-structured regression models enable the joint modeling of interpretable structured and complex unstructured feature effects. The structured model part is inspired by statistical models and can be used to infer the input-output relationship for features of particular importance. The complex unstructured part defines an arbitrary deep neural network and thereby provides enough flexibility to achieve competitive prediction performance. While these models can also account for aleatoric uncertainty, there is still a lack of work on accounting for epistemic uncertainty. In this paper, we address this problem by presenting a Bayesian approximation for semi-structured regression models using subspace inference. To this end, we extend subspace inference for joint posterior sampling from a full parameter space for structured effects and a subspace for unstructured effects. Apart from this hybrid sampling scheme, our method allows for tunable complexity of the subspace and can capture multip
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#20915;&#31574;&#29702;&#35770;&#29615;&#22659;&#20013;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#30456;&#20851;&#24615;&#36807;&#28388;&#30340;&#26041;&#27861;&#65292;&#23558;&#29366;&#24577;-&#21160;&#20316;&#20540;&#20989;&#25968;&#30340;&#20272;&#35745;&#38480;&#21046;&#22312;&#31232;&#30095;&#32452;&#20214;&#19978;&#65292;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#19988;&#26679;&#26412;&#22797;&#26434;&#24230;&#20165;&#21462;&#20915;&#20110;&#31232;&#30095;&#32452;&#20214;&#30340;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2401.12934</link><description>&lt;p&gt;
&#22522;&#20110;&#22870;&#21169;&#30456;&#20851;&#24615;&#36807;&#28388;&#30340;&#32447;&#24615;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reward-Relevance-Filtered Linear Offline Reinforcement Learning. (arXiv:2401.12934v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#20915;&#31574;&#29702;&#35770;&#29615;&#22659;&#20013;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#30456;&#20851;&#24615;&#36807;&#28388;&#30340;&#26041;&#27861;&#65292;&#23558;&#29366;&#24577;-&#21160;&#20316;&#20540;&#20989;&#25968;&#30340;&#20272;&#35745;&#38480;&#21046;&#22312;&#31232;&#30095;&#32452;&#20214;&#19978;&#65292;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#19988;&#26679;&#26412;&#22797;&#26434;&#24230;&#20165;&#21462;&#20915;&#20110;&#31232;&#30095;&#32452;&#20214;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#20915;&#31574;&#29702;&#35770;&#29615;&#22659;&#20013;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#20013;&#20551;&#35774;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20855;&#26377;&#20915;&#31574;&#29702;&#35770;&#31232;&#30095;&#24615;&#32780;&#19981;&#26159;&#20272;&#35745;&#31232;&#30095;&#24615;&#12290;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#32467;&#26500;&#24615;&#38480;&#21046;&#39044;&#35774;&#20102;&#36716;&#31227;&#21487;&#20197;&#20998;&#35299;&#20026;&#19968;&#20010;&#24433;&#21709;&#22870;&#21169;&#30340;&#31232;&#30095;&#32452;&#20214;&#65292;&#24182;&#19988;&#21487;&#33021;&#24433;&#21709;&#19981;&#24433;&#21709;&#22870;&#21169;&#30340;&#20854;&#20182;&#22806;&#29983;&#21160;&#21147;&#23398;&#12290;&#34429;&#28982;&#29992;&#20110;&#20272;&#35745;&#20840;&#29366;&#24577;&#36807;&#28193;&#23646;&#24615;&#30340;&#26368;&#23567;&#21487;&#35843;&#25972;&#38598;&#21512;&#21462;&#20915;&#20110;&#25972;&#20010;&#29366;&#24577;&#65292;&#20294;&#26368;&#20248;&#31574;&#30053;&#65292;&#22240;&#27492;&#29366;&#24577;-&#21160;&#20316;&#20540;&#20989;&#25968;&#21482;&#20381;&#36182;&#20110;&#31232;&#30095;&#32452;&#20214;&#65306;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#22240;&#26524;/&#20915;&#31574;&#35770;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;&#38408;&#20540;&#23725;&#22238;&#24402;&#22312;&#26368;&#23567;&#20108;&#20056;&#31574;&#30053;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#28388;&#22870;&#21169;&#30340;&#26041;&#27861;&#65292;&#23558;&#29366;&#24577;-&#21160;&#20316;&#20540;&#20989;&#25968;&#30340;&#20272;&#35745;&#38480;&#21046;&#22312;&#31232;&#30095;&#32452;&#20214;&#19978;&#12290;&#25105;&#20204;&#20026;&#22870;&#21169;&#36807;&#28388;&#30340;&#32447;&#24615;&#25311;&#21512;Q-&#36845;&#20195;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20165;&#21462;&#20915;&#20110;&#31232;&#30095;&#32452;&#20214;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies offline reinforcement learning with linear function approximation in a setting with decision-theoretic, but not estimation sparsity. The structural restrictions of the data-generating process presume that the transitions factor into a sparse component that affects the reward and could affect additional exogenous dynamics that do not affect the reward. Although the minimally sufficient adjustment set for estimation of full-state transition properties depends on the whole state, the optimal policy and therefore state-action value function depends only on the sparse component: we call this causal/decision-theoretic sparsity. We develop a method for reward-filtering the estimation of the state-action value function to the sparse component by a modification of thresholded lasso in least-squares policy evaluation. We provide theoretical guarantees for our reward-filtered linear fitted-Q-iteration, with sample complexity depending only on the size of the sparse component.
&lt;/p&gt;</description></item><item><title>pyAKI&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23454;&#26045;KDIGO&#26631;&#20934;&#23545;&#24613;&#24615;&#32958;&#25439;&#20260;&#36827;&#34892;&#20998;&#31867;&#12290;&#19982;&#19987;&#23478;&#26631;&#27880;&#30456;&#27604;&#65292;pyAKI&#23637;&#29616;&#20986;&#20102;&#26356;&#20248;&#36136;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12930</link><description>&lt;p&gt;
pyAKI - &#19968;&#31181;&#33258;&#21160;&#21270;KDIGO&#20998;&#31867;&#30340;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
pyAKI - An Open Source Solution to Automated KDIGO classification. (arXiv:2401.12930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12930
&lt;/p&gt;
&lt;p&gt;
pyAKI&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23454;&#26045;KDIGO&#26631;&#20934;&#23545;&#24613;&#24615;&#32958;&#25439;&#20260;&#36827;&#34892;&#20998;&#31867;&#12290;&#19982;&#19987;&#23478;&#26631;&#27880;&#30456;&#27604;&#65292;pyAKI&#23637;&#29616;&#20986;&#20102;&#26356;&#20248;&#36136;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#24615;&#32958;&#25439;&#20260;&#65288;AKI&#65289;&#26159;&#21361;&#37325;&#30149;&#24739;&#32773;&#20013;&#24120;&#35265;&#30340;&#24182;&#21457;&#30151;&#65292;&#24433;&#21709;&#30528;&#39640;&#36798;50%&#30340;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#24739;&#32773;&#12290;&#32570;&#20047;&#26631;&#20934;&#21270;&#21644;&#24320;&#28304;&#24037;&#20855;&#65292;&#20197;&#24212;&#29992;&#32958;&#33039;&#30142;&#30149;&#25913;&#21892;&#20840;&#29699;&#32467;&#26524;&#65288;KDIGO&#65289;&#26631;&#20934;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#23545;&#24037;&#20316;&#37327;&#21644;&#30740;&#31350;&#36136;&#37327;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#39033;&#30446;&#20171;&#32461;&#20102;pyAKI&#65292;&#19968;&#20010;&#24320;&#28304;&#27969;&#31243;&#65292;&#36890;&#36807;&#25552;&#20379;&#20840;&#38754;&#30340;KDIGO&#26631;&#20934;&#19968;&#33268;&#24615;&#23454;&#26045;&#26041;&#26696;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#21033;&#29992;&#24120;&#29992;&#30340;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#30740;&#31350;&#25968;&#25454;&#24211;Medical Information Mart for Intensive Care (MIMIC)-IV&#30340;&#23376;&#38598;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#25968;&#25454;&#27169;&#22411;&#65292;&#20197;&#30830;&#20445;&#21487;&#37325;&#22797;&#24615;&#12290;&#19982;&#19987;&#23478;&#27880;&#37322;&#30340;&#39564;&#35777;&#34920;&#26126;&#65292;pyAKI&#22312;&#23454;&#26045;KDIGO&#26631;&#20934;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;&#27604;&#36739;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#33021;&#22815;&#36229;&#36234;&#20154;&#24037;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;&#26412;&#24037;&#20316;&#23558;pyAKI&#20316;&#20026;&#19968;&#31181;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#24341;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acute Kidney Injury (AKI) is a frequent complication in critically ill patients, affecting up to 50% of patients in the intensive care units. The lack of standardized and open-source tools for applying the Kidney Disease Improving Global Outcomes (KDIGO) criteria to time series data has a negative impact on workload and study quality. This project introduces pyAKI, an open-source pipeline addressing this gap by providing a comprehensive solution for consistent KDIGO criteria implementation.  The pyAKI pipeline was developed and validated using a subset of the Medical Information Mart for Intensive Care (MIMIC)-IV database, a commonly used database in critical care research. We defined a standardized data model in order to ensure reproducibility. Validation against expert annotations demonstrated pyAKI's robust performance in implementing KDIGO criteria. Comparative analysis revealed its ability to surpass the quality of human labels.  This work introduces pyAKI as an open-source soluti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#24863;&#30693;&#30340;&#25968;&#25454;&#38598;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#36873;&#25321;&#35270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#26469;&#35299;&#20915;&#65292;&#24182;&#26126;&#30830;&#22320;&#24314;&#27169;&#20102;&#23398;&#20064;&#36807;&#31243;&#22914;&#20309;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#28857;&#26469;&#39044;&#27979;&#30446;&#26631;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.12926</link><description>&lt;p&gt;
DsDm&#65306;&#20855;&#26377;&#25968;&#25454;&#27169;&#22411;&#30340;&#27169;&#22411;&#24863;&#30693;&#25968;&#25454;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
DsDm: Model-Aware Dataset Selection with Datamodels. (arXiv:2401.12926v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12926
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#24863;&#30693;&#30340;&#25968;&#25454;&#38598;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#36873;&#25321;&#35270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#26469;&#35299;&#20915;&#65292;&#24182;&#26126;&#30830;&#22320;&#24314;&#27169;&#20102;&#23398;&#20064;&#36807;&#31243;&#22914;&#20309;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#28857;&#26469;&#39044;&#27979;&#30446;&#26631;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#25968;&#25454;&#26102;&#65292;&#26631;&#20934;&#20570;&#27861;&#26159;&#26681;&#25454;&#20154;&#31867;&#23545;&#25968;&#25454;&#36136;&#37327;&#30340;&#35748;&#30693;&#36827;&#34892;&#31579;&#36873;&#12290;&#36825;&#31181;&#31579;&#36873;&#21487;&#20197;&#24471;&#21040;&#30452;&#35266;&#19978;&#33021;&#25552;&#39640;&#27169;&#22411;&#34892;&#20026;&#30340;&#25968;&#25454;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#30456;&#21453;&#30340;&#24773;&#20917;&#21487;&#33021;&#21457;&#29983;&#65306;&#25105;&#20204;&#21457;&#29616;&#26681;&#25454;&#19982;&#8220;&#39640;&#36136;&#37327;&#8221;&#25968;&#25454;&#28304;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;&#36873;&#25321;&#21487;&#33021;&#19981;&#20250;&#22686;&#21152;&#65288;&#29978;&#33267;&#21487;&#33021;&#21066;&#24369;&#65289;&#19982;&#38543;&#26426;&#36873;&#25321;&#25968;&#25454;&#30456;&#27604;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#24320;&#21457;&#26356;&#22909;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#25968;&#25454;&#38598;&#36873;&#25321;&#20316;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#26469;&#35299;&#20915;&#65306;&#32473;&#23450;&#30446;&#26631;&#20219;&#21153;&#12289;&#23398;&#20064;&#31639;&#27861;&#21644;&#20505;&#36873;&#25968;&#25454;&#65292;&#36873;&#25321;&#26368;&#22823;&#21270;&#27169;&#22411;&#24615;&#33021;&#30340;&#23376;&#38598;&#12290;&#36825;&#20010;&#26694;&#26550;&#36991;&#20813;&#20102;&#25163;&#21160;&#36873;&#25321;&#25968;&#25454;&#36136;&#37327;&#30340;&#27010;&#24565;&#65292;&#24182;&#26126;&#30830;&#22320;&#24314;&#27169;&#20102;&#23398;&#20064;&#36807;&#31243;&#22914;&#20309;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#28857;&#26469;&#39044;&#27979;&#30446;&#26631;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#39044;&#20808;&#25351;&#23450;&#30340;&#20219;&#21153;&#21644;&#20197;&#21069;&#19981;&#21253;&#25324;&#30340;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with "high quality" data sources may not increase (and can even hurt) performance compared to randomly selecting data.  To develop better methods for selecting data, we start by framing dataset selection as an optimization problem that we can directly solve for: given target tasks, a learning algorithm, and candidate data, select the subset that maximizes model performance. This framework thus avoids handpicked notions of data quality, and instead models explicitly how the learning process uses train datapoints to predict on the target tasks. Our resulting method greatly improves language model (LM) performance on both pre-specified tasks and previously
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#20351;&#29992;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#36827;&#34892;&#20102;&#24615;&#33021;&#20998;&#26512;&#65292;&#24182;&#30740;&#31350;&#20102;&#20851;&#38190;&#22240;&#32032;&#22914;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#21161;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.12924</link><description>&lt;p&gt;
&#23545;&#20110;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#30340;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Performance Analysis of Support Vector Machine (SVM) on Challenging Datasets for Forest Fire Detection. (arXiv:2401.12924v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#20351;&#29992;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#36827;&#34892;&#20102;&#24615;&#33021;&#20998;&#26512;&#65292;&#24182;&#30740;&#31350;&#20102;&#20851;&#38190;&#22240;&#32032;&#22914;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#21161;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#20351;&#29992;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#30340;&#24615;&#33021;&#21644;&#21033;&#29992;&#24773;&#20917;&#12290;&#38543;&#30528;&#26862;&#26519;&#28779;&#28798;&#23545;&#29983;&#24577;&#31995;&#32479;&#21644;&#20154;&#31867;&#23450;&#23621;&#28857;&#30340;&#23041;&#32961;&#26085;&#30410;&#22686;&#21152;&#65292;&#36805;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#31995;&#32479;&#30340;&#38656;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;SVM&#20197;&#20854;&#24378;&#22823;&#30340;&#20998;&#31867;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#22312;&#22270;&#20687;&#20013;&#35782;&#21035;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#27169;&#24335;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#24230;&#12290;&#36890;&#36807;&#22312;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;SVM&#33719;&#24471;&#20102;&#35782;&#21035;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#29420;&#29305;&#23646;&#24615;&#30340;&#33021;&#21147;&#65292;&#22914;&#28779;&#28976;&#12289;&#28895;&#38654;&#25110;&#26862;&#26519;&#21306;&#22495;&#35270;&#35273;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20351;&#29992;SVM&#30340;&#21508;&#20010;&#35201;&#32032;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;&#20005;&#26684;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#23454;&#38469;&#36866;&#29992;&#24615;&#31561;&#21442;&#25968;&#12290;&#20174;&#36825;&#39033;&#30740;&#31350;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#26377;&#21161;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article delves into the analysis of performance and utilization of Support Vector Machines (SVMs) for the critical task of forest fire detection using image datasets. With the increasing threat of forest fires to ecosystems and human settlements, the need for rapid and accurate detection systems is of utmost importance. SVMs, renowned for their strong classification capabilities, exhibit proficiency in recognizing patterns associated with fire within images. By training on labeled data, SVMs acquire the ability to identify distinctive attributes associated with fire, such as flames, smoke, or alterations in the visual characteristics of the forest area. The document thoroughly examines the use of SVMs, covering crucial elements like data preprocessing, feature extraction, and model training. It rigorously evaluates parameters such as accuracy, efficiency, and practical applicability. The knowledge gained from this study aids in the development of efficient forest fire detection sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26576;&#20123;&#38590;&#20197;&#27169;&#25311;&#24213;&#23618;&#29366;&#24577;&#21464;&#37327;&#30340;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20351;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12923</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#20915;&#19968;&#20123;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep multitask neural networks for solving some stochastic optimal control problems. (arXiv:2401.12923v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26576;&#20123;&#38590;&#20197;&#27169;&#25311;&#24213;&#23618;&#29366;&#24577;&#21464;&#37327;&#30340;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20351;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#29992;&#20110;&#20351;&#29992;&#30456;&#20851;&#30340;&#21453;&#21521;&#21160;&#24577;&#35268;&#21010;&#21407;&#29702;&#35299;&#20915;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#27169;&#25311;&#24213;&#23618;&#29366;&#24577;&#21464;&#37327;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#38382;&#39064;&#20013;&#65292;&#36825;&#31181;&#27169;&#25311;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#23548;&#33268;&#29366;&#24577;&#21464;&#37327;&#31354;&#38388;&#30340;&#31163;&#25955;&#21270;&#21644;&#38656;&#35201;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#12290;&#24403;&#22788;&#29702;&#22823;&#30340;&#29366;&#24577;&#21464;&#37327;&#31354;&#38388;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#21464;&#24471;&#20302;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#36825;&#31181;&#31867;&#22411;&#30340;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35757;&#32451;&#25105;&#20204;&#30340;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#22312;&#20219;&#21153;&#20043;&#38388;&#21160;&#24577;&#24179;&#34913;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#34893;&#29983;&#21697;&#23450;&#20215;&#38382;&#39064;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing neural network-based approaches for solving stochastic optimal control problems using the associated backward dynamic programming principle rely on the ability to simulate the underlying state variables. However, in some problems, this simulation is infeasible, leading to the discretization of state variable space and the need to train one neural network for each data point. This approach becomes computationally inefficient when dealing with large state variable spaces. In this paper, we consider a class of this type of stochastic optimal control problems and introduce an effective solution employing multitask neural networks. To train our multitask neural network, we introduce a novel scheme that dynamically balances the learning across tasks. Through numerical experiments on real-world derivatives pricing problems, we prove that our method outperforms state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38459;&#23612;&#29275;&#39039;&#26041;&#27861;&#30340;&#26080;&#27169;&#22411;$\delta$&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26410;&#30693;&#36830;&#32493;&#26102;&#38388;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;H&#8734;&#36319;&#36394;&#25511;&#21046;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#36845;&#20195;&#27714;&#35299;&#24191;&#20041;&#36319;&#36394;Bellman&#26041;&#31243;&#65292;&#21487;&#20197;&#23547;&#25214;&#20986;&#36319;&#36394;Hamilton-Jacobi-Isaac (HJI)&#26041;&#31243;&#30340;&#26368;&#20248;&#35299;&#12290;&#25552;&#20379;&#20102;&#22522;&#20110;&#31574;&#30053;&#23398;&#20064;&#21644;&#31163;&#31574;&#30053;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#31163;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#26159;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12882</link><description>&lt;p&gt;
&#22522;&#20110;&#38459;&#23612;&#29275;&#39039;&#27861;&#30340;&#26080;&#27169;&#22411;$\delta$&#31574;&#30053;&#36845;&#20195;&#29992;&#20110;&#38750;&#32447;&#24615;&#36830;&#32493;&#26102;&#38388;H&#8734;&#36319;&#36394;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Model-Free $\delta$-Policy Iteration Based on Damped Newton Method for Nonlinear Continuous-Time H$\infty$ Tracking Control. (arXiv:2401.12882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12882
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38459;&#23612;&#29275;&#39039;&#26041;&#27861;&#30340;&#26080;&#27169;&#22411;$\delta$&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26410;&#30693;&#36830;&#32493;&#26102;&#38388;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;H&#8734;&#36319;&#36394;&#25511;&#21046;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#36845;&#20195;&#27714;&#35299;&#24191;&#20041;&#36319;&#36394;Bellman&#26041;&#31243;&#65292;&#21487;&#20197;&#23547;&#25214;&#20986;&#36319;&#36394;Hamilton-Jacobi-Isaac (HJI)&#26041;&#31243;&#30340;&#26368;&#20248;&#35299;&#12290;&#25552;&#20379;&#20102;&#22522;&#20110;&#31574;&#30053;&#23398;&#20064;&#21644;&#31163;&#31574;&#30053;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#31163;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#26159;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38459;&#23612;&#29275;&#39039;&#27861;&#30340;{\delta}-PI&#31639;&#27861;&#65292;&#29992;&#20110;&#26410;&#30693;&#36830;&#32493;&#26102;&#38388;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;H&#8734;&#36319;&#36394;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#25240;&#29616;&#24615;&#33021;&#20989;&#25968;&#21644;&#22686;&#24191;&#31995;&#32479;&#26469;&#24471;&#21040;&#36319;&#36394;Hamilton-Jacobi-Isaac (HJI)&#26041;&#31243;&#12290;&#36319;&#36394;HJI&#26041;&#31243;&#26159;&#19968;&#20010;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#36319;&#36394;HJI&#26041;&#31243;&#20027;&#35201;&#22522;&#20110;&#29275;&#39039;&#27861;&#65292;&#36890;&#24120;&#21482;&#33021;&#28385;&#36275;&#23616;&#37096;&#25910;&#25947;&#24182;&#19988;&#38656;&#35201;&#19968;&#20010;&#33391;&#22909;&#30340;&#21021;&#22987;&#29468;&#27979;&#12290;&#22312;&#38459;&#23612;&#29275;&#39039;&#36845;&#20195;&#31639;&#23376;&#26041;&#31243;&#30340;&#22522;&#30784;&#19978;&#65292;&#39318;&#20808;&#25512;&#23548;&#20986;&#20102;&#24191;&#20041;&#30340;&#36319;&#36394;Bellman&#26041;&#31243;&#12290;{\delta}-PI&#31639;&#27861;&#36890;&#36807;&#36845;&#20195;&#27714;&#35299;&#24191;&#20041;&#36319;&#36394;Bellman&#26041;&#31243;&#26469;&#23547;&#25214;&#36319;&#36394;HJI&#26041;&#31243;&#30340;&#26368;&#20248;&#35299;&#12290;&#20998;&#21035;&#25552;&#20379;&#20102;&#22522;&#20110;&#31574;&#30053;&#23398;&#20064;&#21644;&#31163;&#31574;&#30053;&#23398;&#20064;&#30340;{\delta}-PI&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#22522;&#20110;&#31163;&#31574;&#30053;&#23398;&#20064;&#30340;{\delta}-PI&#31639;&#27861;&#26159;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a {\delta}-PI algorithm which is based on damped Newton method for the H{\infty} tracking control problem of unknown continuous-time nonlinear system. A discounted performance function and an augmented system are used to get the tracking Hamilton-Jacobi-Isaac (HJI) equation. Tracking HJI equation is a nonlinear partial differential equation, traditional reinforcement learning methods for solving the tracking HJI equation are mostly based on the Newton method, which usually only satisfies local convergence and needs a good initial guess. Based upon the damped Newton iteration operator equation, a generalized tracking Bellman equation is derived firstly. The {\delta}-PI algorithm can seek the optimal solution of the tracking HJI equation by iteratively solving the generalized tracking Bellman equation. On-policy learning and off-policy learning {\delta}-PI reinforcement learning methods are provided, respectively. Off-policy version {\delta}-PI algorithm is a model-fr
&lt;/p&gt;</description></item><item><title>&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#20013;&#20219;&#21153;&#20998;&#37197;&#21644;&#36136;&#37327;&#38382;&#39064;&#24341;&#36215;&#20102;&#30740;&#31350;&#32773;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#27969;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#20219;&#21153;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#20219;&#21153;&#36716;&#31227;&#26469;&#35299;&#20915;&#20219;&#21153;&#20998;&#37197;&#20013;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#26426;&#21046;&#26377;&#21161;&#20110;&#25552;&#39640;&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.12866</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#27969;&#25903;&#25345;&#30340;&#31227;&#21160;&#20247;&#21253;&#21327;&#35843;&#20013;&#35780;&#20272;&#21327;&#20316;&#21644;&#33258;&#27835;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported Coordination of Mobile Crowdsourcing. (arXiv:2401.12866v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12866
&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#20013;&#20219;&#21153;&#20998;&#37197;&#21644;&#36136;&#37327;&#38382;&#39064;&#24341;&#36215;&#20102;&#30740;&#31350;&#32773;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#27969;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#20219;&#21153;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#20219;&#21153;&#36716;&#31227;&#26469;&#35299;&#20915;&#20219;&#21153;&#20998;&#37197;&#20013;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#26426;&#21046;&#26377;&#21161;&#20110;&#25552;&#39640;&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20247;&#21253;&#26159;&#25351;&#23436;&#25104;&#20219;&#21153;&#38656;&#35201;&#20247;&#21253;&#24037;&#20316;&#32773;&#22312;&#25353;&#38656;&#21171;&#21160;&#21147;&#20013;&#36827;&#34892;&#29289;&#29702;&#31227;&#21160;&#30340;&#31995;&#32479;&#12290;Evidence suggests that in such systems, tasks often get assigned to crowdworkers who struggle to complete those tasks successfully, resulting in high failure rates and low service quality. &#25552;&#39640;&#26381;&#21153;&#36136;&#37327;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#19981;&#26029;&#36866;&#24212;&#20219;&#21153;&#20998;&#37197;&#65292;&#24182;&#22312;&#20986;&#29616;&#23548;&#33268;&#22833;&#36133;&#30340;&#20107;&#20214;&#26102;&#23558;&#20219;&#21153;&#36716;&#31227;&#32473;&#26356;&#21512;&#36866;&#30340;&#24037;&#20316;&#32773;&#65292;&#20182;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#36335;&#32447;&#25110;&#36710;&#36742;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#20247;&#21253;&#20013;&#23454;&#29616;&#20219;&#21153;&#36716;&#31227;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#24037;&#20316;&#32773;&#26159;&#33258;&#27835;&#30340;&#65292;&#21487;&#33021;&#25298;&#32477;&#36716;&#31227;&#35831;&#27714;&#12290;&#27492;&#22806;&#65292;&#20219;&#21153;&#32467;&#26524;&#26159;&#19981;&#30830;&#23450;&#30340;&#65292;&#38656;&#35201;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26426;&#21046;&#26469;&#23454;&#29616;&#31227;&#21160;&#20247;&#21253;&#20013;&#30340;&#32467;&#26524;&#39044;&#27979;&#21644;&#20219;&#21153;&#21327;&#35843;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#27969;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#20219;&#21153;&#32467;&#26524;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;p
&lt;/p&gt;
&lt;p&gt;
Mobile crowdsourcing refers to systems where the completion of tasks necessarily requires physical movement of crowdworkers in an on-demand workforce. Evidence suggests that in such systems, tasks often get assigned to crowdworkers who struggle to complete those tasks successfully, resulting in high failure rates and low service quality. A promising solution to ensure higher quality of service is to continuously adapt the assignment and respond to failure-causing events by transferring tasks to better-suited workers who use different routes or vehicles. However, implementing task transfers in mobile crowdsourcing is difficult because workers are autonomous and may reject transfer requests. Moreover, task outcomes are uncertain and need to be predicted. In this paper, we propose different mechanisms to achieve outcome prediction and task coordination in mobile crowdsourcing. First, we analyze different data stream learning approaches for the prediction of task outcomes. Second, based on
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#20154;&#26426;&#39640;&#20809;&#35889;&#25104;&#20687;&#23545;17&#31181;&#32418;&#30333;&#33889;&#33796;&#21697;&#31181;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#30772;&#22351;&#24615;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#39640;&#25928;&#21644; less prohibitive&#65292;&#33021;&#22815;&#32416;&#27491;&#21644;&#38477;&#20302;&#22823;&#37327;&#25968;&#25454;&#30340;&#37319;&#26679;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#33889;&#33796;&#21697;&#31181;&#39640;&#24230;&#30456;&#20284;&#30340;&#39640;&#20809;&#35889;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.12851</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#20154;&#26426;&#39640;&#20809;&#35889;&#25104;&#20687;&#23545;&#33889;&#33796;&#21697;&#31181;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of grapevine varieties using UAV hyperspectral imaging. (arXiv:2401.12851v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12851
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#20154;&#26426;&#39640;&#20809;&#35889;&#25104;&#20687;&#23545;17&#31181;&#32418;&#30333;&#33889;&#33796;&#21697;&#31181;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#30772;&#22351;&#24615;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#39640;&#25928;&#21644; less prohibitive&#65292;&#33021;&#22815;&#32416;&#27491;&#21644;&#38477;&#20302;&#22823;&#37327;&#25968;&#25454;&#30340;&#37319;&#26679;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#33889;&#33796;&#21697;&#31181;&#39640;&#24230;&#30456;&#20284;&#30340;&#39640;&#20809;&#35889;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#33889;&#33796;&#21697;&#31181;&#30340;&#20998;&#31867;&#26159;&#31934;&#20934;&#33889;&#33796;&#26685;&#22521;&#20013;&#19968;&#20010;&#30456;&#20851;&#30340;&#34920;&#22411;&#20998;&#26512;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20272;&#35745;&#19981;&#21516;&#21697;&#31181;&#30340;&#33889;&#33796;&#22253;&#34892;&#30340;&#29983;&#38271;&#24773;&#20917;&#65292;&#24182;&#28041;&#21450;&#21040;&#33889;&#33796;&#37202;&#34892;&#19994;&#30340;&#20854;&#20182;&#24212;&#29992;&#12290;&#36825;&#39033;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#30772;&#22351;&#24615;&#30340;&#26041;&#27861;&#26469;&#23436;&#25104;&#65292;&#38656;&#35201;&#32791;&#26102;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#23454;&#39564;&#23460;&#20998;&#26512;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26080;&#20154;&#26426;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#12289; less prohibitive&#30340;&#26041;&#27861;&#26469;&#25910;&#38598;&#39640;&#20809;&#35889;&#25968;&#25454;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#33719;&#24471;&#26356;&#22024;&#26434;&#30340;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#31532;&#19968;&#20010;&#20219;&#21153;&#26159;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#20197;&#32416;&#27491;&#21644;&#38477;&#20302;&#22823;&#37327;&#25968;&#25454;&#30340;&#37319;&#26679;&#29575;&#12290;&#27492;&#22806;&#65292;&#33889;&#33796;&#21697;&#31181;&#30340;&#39640;&#20809;&#35889;&#29305;&#24449;&#38750;&#24120;&#30456;&#20284;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#29992;&#20110;&#23545;17&#31181;&#32418;&#30333;&#33889;&#33796;&#21697;&#31181;&#36827;&#34892;&#20998;&#31867;&#12290;&#19982;&#20998;&#31867;&#21333;&#20010;&#26679;&#26412;&#19981;&#21516;&#65292;&#36825;&#20123;&#26679;&#26412;&#19982;&#20854;&#21608;&#22260;&#29615;&#22659;&#19968;&#36215;&#36827;&#34892;&#22788;&#29702;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#36827;&#34892;&#31354;&#38388;&#29305;&#24449;&#30340;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The classification of different grapevine varieties is a relevant phenotyping task in Precision Viticulture since it enables estimating the growth of vineyard rows dedicated to different varieties, among other applications concerning the wine industry. This task can be performed with destructive methods that require time-consuming tasks, including data collection and analysis in the laboratory. However, Unmanned Aerial Vehicles (UAV) provide a more efficient and less prohibitive approach to collecting hyperspectral data, despite acquiring noisier data. Therefore, the first task is the processing of these data to correct and downsample large amounts of data. In addition, the hyperspectral signatures of grape varieties are very similar. In this work, a Convolutional Neural Network (CNN) is proposed for classifying seventeen varieties of red and white grape variants. Rather than classifying single samples, these are processed together with their neighbourhood. Hence, the extraction of spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#23433;&#20840;&#23646;&#24615;&#30340;&#20108;&#36827;&#21046;&#23433;&#20840;&#35780;&#35770;&#23478;&#31639;&#23376;&#23398;&#20064;&#36991;&#20813;&#36798;&#21040;&#19981;&#23433;&#20840;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#29305;&#24615;&#21644;&#22266;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.12849</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#21387;&#32553;&#20108;&#36827;&#21046;Bellman&#31639;&#23376;&#23398;&#20064;&#23433;&#20840;&#35780;&#35770;&#23478;
&lt;/p&gt;
&lt;p&gt;
Learning safety critics via a non-contractive binary bellman operator. (arXiv:2401.12849v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#23433;&#20840;&#23646;&#24615;&#30340;&#20108;&#36827;&#21046;&#23433;&#20840;&#35780;&#35770;&#23478;&#31639;&#23376;&#23398;&#20064;&#36991;&#20813;&#36798;&#21040;&#19981;&#23433;&#20840;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#29305;&#24615;&#21644;&#22266;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#33258;&#28982;&#22320;&#24378;&#21046;&#25191;&#34892;&#26377;&#38480;&#22833;&#36133;&#30340;&#23433;&#20840;&#24615;&#26159;&#19968;&#20010;&#38459;&#30861;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#23454;&#38469;&#24847;&#20041;&#30340;&#23433;&#20840;&#24615;&#27010;&#24565;&#26159;&#36991;&#20813;&#65288;&#19981;&#23433;&#20840;&#30340;&#65289;&#29366;&#24577;&#31354;&#38388;&#30340;&#21306;&#22495;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#23433;&#20840;&#30446;&#26631;&#21487;&#20197;&#36890;&#36807;&#31867;&#20284;&#21160;&#20316;&#20540;&#20989;&#25968;&#30340;&#23433;&#20840;&#35780;&#35770;&#23478;&#26469;&#25429;&#25417;&#65292;&#20294;&#30456;&#20851;&#30340;&#31639;&#23376;&#32570;&#20047;&#21476;&#20856;Bellman&#31639;&#23376;&#25152;&#20855;&#26377;&#30340;&#25910;&#32553;&#21644;&#21807;&#19968;&#24615;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#23433;&#20840;&#26159;&#20108;&#36827;&#21046;&#23646;&#24615;&#26469;&#20811;&#26381;&#23433;&#20840;&#35780;&#35770;&#23478;&#31639;&#23376;&#30340;&#38750;&#25910;&#32553;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#23547;&#27714;&#36991;&#20813;&#36798;&#21040;&#19981;&#23433;&#20840;&#21306;&#22495;&#30340;&#30830;&#23450;&#24615;&#21160;&#24577;&#31995;&#32479;&#30456;&#20851;&#30340;&#20108;&#36827;&#21046;&#23433;&#20840;&#35780;&#35770;&#23478;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#30456;&#24212;&#30340;&#23433;&#20840;&#24615;&#20108;&#36827;&#21046;&#36125;&#23572;&#26364;&#26041;&#31243;&#65288;B2E&#65289;&#24182;&#30740;&#31350;&#20102;&#20854;&#29305;&#24615;&#12290;&#34429;&#28982;&#32467;&#26524;&#31639;&#23376;&#20173;&#28982;&#26159;&#38750;&#21387;&#32553;&#30340;&#65292;&#20294;&#25105;&#20204;&#23436;&#20840;&#34920;&#24449;&#20102;&#20854;&#34920;&#31034;&#30340;&#22266;&#23450;&#28857;&#65292;&#38500;&#20102;...
&lt;/p&gt;
&lt;p&gt;
The inability to naturally enforce safety in Reinforcement Learning (RL), with limited failures, is a core challenge impeding its use in real-world applications. One notion of safety of vast practical relevance is the ability to avoid (unsafe) regions of the state space. Though such a safety goal can be captured by an action-value-like function, a.k.a. safety critics, the associated operator lacks the desired contraction and uniqueness properties that the classical Bellman operator enjoys. In this work, we overcome the non-contractiveness of safety critic operators by leveraging that safety is a binary property. To that end, we study the properties of the binary safety critic associated with a deterministic dynamical system that seeks to avoid reaching an unsafe region. We formulate the corresponding binary Bellman equation (B2E) for safety and study its properties. While the resulting operator is still non-contractive, we fully characterize its fixed points representing--except for a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23884;&#20837;&#30340;&#26102;&#38388;&#22270;&#36317;&#31163;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#21644;&#26102;&#38388;&#23646;&#24615;&#30340;&#22270;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#22270;&#12290;</title><link>http://arxiv.org/abs/2401.12843</link><description>&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#36317;&#31163;&#35745;&#31639;&#30340;&#26102;&#38388;&#22270;
&lt;/p&gt;
&lt;p&gt;
An embedding-based distance for temporal graphs. (arXiv:2401.12843v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23884;&#20837;&#30340;&#26102;&#38388;&#22270;&#36317;&#31163;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#21644;&#26102;&#38388;&#23646;&#24615;&#30340;&#22270;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22522;&#20110;&#20351;&#29992;&#26102;&#38388;&#23562;&#37325;&#30340;&#38543;&#26426;&#28216;&#36208;&#26500;&#24314;&#30340;&#22270;&#23884;&#20837;&#26469;&#23450;&#20041;&#20102;&#19968;&#31181;&#26102;&#38388;&#22270;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21305;&#37197;&#22270;&#21644;&#19981;&#21305;&#37197;&#22270;&#30340;&#24773;&#20917;&#65292;&#24403;&#23384;&#22312;&#24050;&#30693;&#30340;&#33410;&#28857;&#20851;&#31995;&#26102;&#65292;&#20197;&#21450;&#24403;&#19981;&#23384;&#22312;&#35813;&#20851;&#31995;&#24182;&#19988;&#22270;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#22823;&#23567;&#26102;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#21644;&#21512;&#25104;&#30340;&#26102;&#38388;&#32593;&#32476;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#36317;&#31163;&#23450;&#20041;&#30340;&#20248;&#21183;&#65292;&#34920;&#26126;&#23427;&#33021;&#22815;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#21644;&#26102;&#38388;&#23646;&#24615;&#30340;&#22270;&#12290;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#22270;&#30340;&#36317;&#31163;&#35745;&#31639;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We define a distance between temporal graphs based on graph embeddings built using time-respecting random walks. We study both the case of matched graphs, when there exists a known relation between the nodes, and the unmatched case, when such a relation is unavailable and the graphs may be of different sizes. We illustrate the interest of our distance definition, using both real and synthetic temporal network data, by showing its ability to discriminate between graphs with different structural and temporal properties. Leveraging state-of-the-art machine learning techniques, we propose an efficient implementation of distance computation that is viable for large-scale temporal graphs.
&lt;/p&gt;</description></item><item><title>&#36845;&#20195;&#30456;&#20851;&#30697;&#38453;&#20998;&#26512;&#65288;IRMA&#65289;&#36890;&#36807;&#36845;&#20195;&#35782;&#21035;&#31867;&#21035;&#29305;&#23450;&#20449;&#24687;&#30340;&#32447;&#24615;&#23376;&#31354;&#38388;&#24182;&#29992;&#20110;&#38477;&#32500;&#21644;&#35757;&#32451;&#40065;&#26834;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2401.12842</link><description>&lt;p&gt;
&#36845;&#20195;&#30456;&#20851;&#30697;&#38453;&#20998;&#26512;&#65288;IRMA&#65289;&#29992;&#20110;&#35782;&#21035;&#20855;&#26377;&#31867;&#21035;&#21306;&#20998;&#33021;&#21147;&#30340;&#23376;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Iterated Relevance Matrix Analysis (IRMA) for the identification of class-discriminative subspaces. (arXiv:2401.12842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12842
&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#30456;&#20851;&#30697;&#38453;&#20998;&#26512;&#65288;IRMA&#65289;&#36890;&#36807;&#36845;&#20195;&#35782;&#21035;&#31867;&#21035;&#29305;&#23450;&#20449;&#24687;&#30340;&#32447;&#24615;&#23376;&#31354;&#38388;&#24182;&#29992;&#20110;&#38477;&#32500;&#21644;&#35757;&#32451;&#40065;&#26834;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#24191;&#20041;&#30697;&#38453;&#23398;&#20064;&#30690;&#37327;&#37327;&#21270;&#30340;&#36845;&#20195;&#24212;&#29992;&#65292;&#29992;&#20110;&#20998;&#26512;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#26500;&#24314;&#20855;&#26377;&#31867;&#21035;&#21306;&#20998;&#33021;&#21147;&#30340;&#23376;&#31354;&#38388;&#12290;&#24314;&#35758;&#30340;&#36845;&#20195;&#30456;&#20851;&#30697;&#38453;&#20998;&#26512;&#65288;IRMA&#65289;&#20351;&#29992;&#24191;&#20041;&#30697;&#38453;&#23398;&#20064;&#30690;&#37327;&#37327;&#21270;&#65288;GMLVQ&#65289;&#26469;&#35782;&#21035;&#34920;&#31034;&#25152;&#32771;&#34385;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#29305;&#23450;&#20449;&#24687;&#30340;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#36890;&#36807;&#36845;&#20195;&#30830;&#23450;&#19968;&#20010;&#26032;&#30340;&#20855;&#26377;&#21306;&#20998;&#33021;&#21147;&#30340;&#23376;&#31354;&#38388;&#65292;&#21516;&#26102;&#25237;&#24433;&#20986;&#25152;&#26377;&#20808;&#21069;&#35782;&#21035;&#20986;&#30340;&#23376;&#31354;&#38388;&#65292;&#21487;&#20197;&#25214;&#21040;&#19968;&#20010;&#21253;&#21547;&#25152;&#26377;&#31867;&#21035;&#29305;&#23450;&#20449;&#24687;&#30340;&#32452;&#21512;&#23376;&#31354;&#38388;&#12290;&#36825;&#26377;&#21161;&#20110;&#23545;&#29305;&#24449;&#30456;&#20851;&#24615;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#23454;&#29616;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#25913;&#36827;&#20302;&#32500;&#34920;&#31034;&#21644;&#21487;&#35270;&#21270;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;IRMA&#30340;&#20855;&#26377;&#31867;&#21035;&#21306;&#20998;&#33021;&#21147;&#30340;&#23376;&#31354;&#38388;&#21487;&#29992;&#20110;&#38477;&#32500;&#21644;&#35757;&#32451;&#20855;&#26377;&#28508;&#22312;&#25913;&#36827;&#24615;&#33021;&#30340;&#40065;&#26834;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce and investigate the iterated application of Generalized Matrix Learning Vector Quantizaton for the analysis of feature relevances in classification problems, as well as for the construction of class-discriminative subspaces. The suggested Iterated Relevance Matrix Analysis (IRMA) identifies a linear subspace representing the classification specific information of the considered data sets using Generalized Matrix Learning Vector Quantization (GMLVQ). By iteratively determining a new discriminative subspace while projecting out all previously identified ones, a combined subspace carrying all class-specific information can be found. This facilitates a detailed analysis of feature relevances, and enables improved low-dimensional representations and visualizations of labeled data sets. Additionally, the IRMA-based class-discriminative subspace can be used for dimensionality reduction and the training of robust classifiers with potentially improved performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#20934;&#30830;&#25429;&#25417;&#26053;&#34892;&#25968;&#25454;&#20013;&#30340;&#24207;&#21015;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20154;&#26053;&#34892;&#32773;&#26410;&#26469;&#30446;&#30340;&#22320;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#35268;&#27169;&#21644;&#24615;&#33021;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#25552;&#21319;&#30446;&#30340;&#22320;&#39044;&#27979;&#26041;&#27861;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#20351;&#20844;&#21496;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#20248;&#21270;&#23458;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2401.12830</link><description>&lt;p&gt;
&#25552;&#21319;&#19979;&#19968;&#20010;&#30446;&#30340;&#22320;&#39044;&#27979;&#65306;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#33322;&#31354;&#25968;&#25454;&#30340;&#26032;&#39062;LSTM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Next Destination Prediction: A Novel LSTM Approach Using Real-World Airline Data. (arXiv:2401.12830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12830
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#20934;&#30830;&#25429;&#25417;&#26053;&#34892;&#25968;&#25454;&#20013;&#30340;&#24207;&#21015;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20154;&#26053;&#34892;&#32773;&#26410;&#26469;&#30446;&#30340;&#22320;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#35268;&#27169;&#21644;&#24615;&#33021;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#25552;&#21319;&#30446;&#30340;&#22320;&#39044;&#27979;&#26041;&#27861;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#20351;&#20844;&#21496;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#20248;&#21270;&#23458;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#20132;&#36890;&#34892;&#19994;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#26053;&#34892;&#32773;&#30340;&#19979;&#19968;&#20010;&#30446;&#30340;&#22320;&#20026;&#20844;&#21496;&#24102;&#26469;&#24456;&#22810;&#22909;&#22788;&#65292;&#20363;&#22914;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#21644;&#23450;&#21521;&#33829;&#38144;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#20934;&#30830;&#25429;&#25417;&#26053;&#34892;&#25968;&#25454;&#20013;&#30340;&#24207;&#21015;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#20010;&#20154;&#26053;&#34892;&#32773;&#26410;&#26469;&#30446;&#30340;&#22320;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#30340;&#26032;&#39062;&#27169;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#20132;&#36890;&#19994;&#20013;&#30340;&#30446;&#30340;&#22320;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#35268;&#27169;&#21644;&#24615;&#33021;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#34920;&#29616;&#21644;&#39640;&#20998;&#25968;&#12290;&#26412;&#30740;&#31350;&#22312;&#25512;&#36827;&#30446;&#30340;&#22320;&#39044;&#27979;&#26041;&#27861;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#20351;&#20844;&#21496;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#24182;&#20248;&#21270;&#21160;&#24577;&#26053;&#34892;&#29615;&#22659;&#20013;&#30340;&#23458;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the modern transportation industry, accurate prediction of travelers' next destinations brings multiple benefits to companies, such as customer satisfaction and targeted marketing. This study focuses on developing a precise model that captures the sequential patterns and dependencies in travel data, enabling accurate predictions of individual travelers' future destinations. To achieve this, a novel model architecture with a sliding window approach based on Long Short-Term Memory (LSTM) is proposed for destination prediction in the transportation industry. The experimental results highlight satisfactory performance and high scores achieved by the proposed model across different data sizes and performance metrics. This research contributes to advancing destination prediction methods, empowering companies to deliver personalized recommendations and optimize customer experiences in the dynamic travel landscape.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#25935;&#24863;&#20449;&#24687;&#27844;&#38706;&#30340;&#21435;&#20559;&#32622;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20844;&#24179;&#33410;&#28857;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22270;&#32467;&#26500;&#20013;&#30340;&#25299;&#25169;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#21435;&#20559;&#32622;&#26694;&#26550;&#65292;&#20197;&#38450;&#27490;&#19979;&#28216;&#35823;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12824</link><description>&lt;p&gt;
MAPPING: &#20351;&#29992;&#26377;&#38480;&#25935;&#24863;&#20449;&#24687;&#27844;&#38706;&#30340;&#21435;&#20559;&#32622;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20844;&#24179;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MAPPING: Debiasing Graph Neural Networks for Fair Node Classification with Limited Sensitive Information Leakage. (arXiv:2401.12824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#25935;&#24863;&#20449;&#24687;&#27844;&#38706;&#30340;&#21435;&#20559;&#32622;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20844;&#24179;&#33410;&#28857;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22270;&#32467;&#26500;&#20013;&#30340;&#25299;&#25169;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#21435;&#20559;&#32622;&#26694;&#26550;&#65292;&#20197;&#38450;&#27490;&#19979;&#28216;&#35823;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21508;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#32487;&#25215;&#24182;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#21382;&#21490;&#19978;&#30340;&#20559;&#35265;&#21644;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#65292;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#22312;&#32447;&#20020;&#24202;&#35786;&#26029;&#12289;&#37329;&#34701;&#20449;&#36151;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#65292;&#24182;&#19981;&#33021;&#31616;&#21333;&#22320;&#22797;&#21046;&#21040;&#20855;&#26377;&#25299;&#25169;&#20381;&#36182;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22270;&#32467;&#26500;&#20013;&#12290;&#29616;&#26377;&#30340;&#20844;&#24179;&#22270;&#23398;&#20064;&#36890;&#24120;&#20559;&#22909;&#20110;&#20351;&#29992;&#25104;&#23545;&#32422;&#26463;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#20294;&#26080;&#27861;&#20811;&#26381;&#32500;&#24230;&#38480;&#21046;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#65307;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#22788;&#29702;&#25216;&#26415;&#19978;&#26469;&#24378;&#21046;&#24182;&#35843;&#25972;&#20844;&#24179;&#24615;&#65292;&#22312;&#39044;&#22788;&#29702;&#38454;&#27573;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#21435;&#20559;&#32622;GNN&#26694;&#26550;&#65292;&#20197;&#38450;&#27490;&#19979;&#28216;&#35823;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#30340;&#21487;&#38752;&#24615;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;GNN&#24448;&#24448;&#20542;&#21521;&#20110;&#22686;&#24378;&#20844;&#24179;&#24615;&#25110;&#22686;&#21152;&#39044;&#27979;&#24615;&#33021;&#65292;&#22240;&#27492;&#22312;&#20108;&#32773;&#20043;&#38388;&#36827;&#34892;&#20840;&#38754;&#26435;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable success in diverse web-based applications, Graph Neural Networks(GNNs) inherit and further exacerbate historical discrimination and social stereotypes, which critically hinder their deployments in high-stake domains such as online clinical diagnosis, financial crediting, etc. However, current fairness research that primarily craft on i.i.d data, cannot be trivially replicated to non-i.i.d. graph structures with topological dependence among samples. Existing fair graph learning typically favors pairwise constraints to achieve fairness but fails to cast off dimensional limitations and generalize them into multiple sensitive attributes; besides, most studies focus on in-processing techniques to enforce and calibrate fairness, constructing a model-agnostic debiasing GNN framework at the pre-processing stage to prevent downstream misuses and improve training reliability is still largely under-explored. Furthermore, previous work on GNNs tend to enhance either fairness or 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21019;&#24314;&#20102;&#30967;&#21435;&#38500;&#36807;&#31243;&#25511;&#21046;&#30340;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#35797;&#38169;&#26469;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#22120;&#30340;&#24615;&#33021;&#22312;&#26356;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#21463;&#21040;&#27169;&#22411;&#35823;&#24046;&#32047;&#31215;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.12822</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#25311;&#22120;&#29992;&#20110;&#24223;&#27700;&#22788;&#29702;&#20013;&#30967;&#21435;&#38500;&#36807;&#31243;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Simulators for the Phosphorus Removal Process Control in Wastewater Treatment via Deep Reinforcement Learning Algorithms. (arXiv:2401.12822v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21019;&#24314;&#20102;&#30967;&#21435;&#38500;&#36807;&#31243;&#25511;&#21046;&#30340;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#35797;&#38169;&#26469;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#22120;&#30340;&#24615;&#33021;&#22312;&#26356;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#21463;&#21040;&#27169;&#22411;&#35823;&#24046;&#32047;&#31215;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30967;&#21435;&#38500;&#23545;&#20110;&#24223;&#27700;&#22788;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#20943;&#23569;&#23545;&#26377;&#38480;&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#35797;&#38169;&#26469;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#20248;&#21270;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;&#24223;&#27700;&#22788;&#29702;&#21378;&#30340;&#22788;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#21270;&#23398;&#21644;&#29983;&#29289;&#36807;&#31243;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#38656;&#35201;&#20934;&#30830;&#30340;&#27169;&#25311;&#22120;&#12290;&#26412;&#30740;&#31350;&#35757;&#32451;&#20102;&#20845;&#20010;&#27169;&#22411;&#26469;&#35782;&#21035;&#30967;&#21435;&#38500;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#30340;&#27169;&#25311;&#22120;&#12290;&#34429;&#28982;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24456;&#39640;&#65288;&gt;97%&#65289;&#65292;&#20294;&#19981;&#30830;&#23450;&#24615;&#21644;&#38169;&#35823;&#39044;&#27979;&#34892;&#20026;&#38480;&#21046;&#20102;&#23427;&#20204;&#20316;&#20026;&#27169;&#25311;&#22120;&#22312;&#26356;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#24615;&#33021;&#12290;&#27169;&#22411;&#39044;&#27979;&#35823;&#24046;&#30340;&#32047;&#31215;&#34987;&#30830;&#23450;&#20026;&#36825;&#20010;&#38382;&#39064;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#36825;&#31181;&#25913;&#36827;&#36807;&#31243;&#25511;&#21046;&#30340;&#26041;&#27861;&#28041;&#21450;&#20026;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21019;&#24314;&#27169;&#25311;&#29615;&#22659;&#65292;&#20351;&#29992;&#30417;&#25511;&#19982;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#65288;SCADA&#65289;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phosphorus removal is vital in wastewater treatment to reduce reliance on limited resources. Deep reinforcement learning (DRL) is a machine learning technique that can optimize complex and nonlinear systems, including the processes in wastewater treatment plants, by learning control policies through trial and error. However, applying DRL to chemical and biological processes is challenging due to the need for accurate simulators. This study trained six models to identify the phosphorus removal process and used them to create a simulator for the DRL environment. Although the models achieved high accuracy (&gt;97%), uncertainty and incorrect prediction behavior limited their performance as simulators over longer horizons. Compounding errors in the models' predictions were identified as one of the causes of this problem. This approach for improving process control involves creating simulation environments for DRL algorithms, using data from supervisory control and data acquisition (SCADA) sys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65288;DatUS^2&#65289;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#29983;&#25104;&#35821;&#20041;&#19968;&#33268;&#19988;&#23494;&#38598;&#30340;&#20266;&#26631;&#27880;&#20998;&#21106;&#25513;&#27169;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#29305;&#24449;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.12820</link><description>&lt;p&gt;
DatUS^2: &#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#35270;&#35273;Transformer&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#30340;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained Self-supervised Vision Transformer. (arXiv:2401.12820v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65288;DatUS^2&#65289;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#29983;&#25104;&#35821;&#20041;&#19968;&#33268;&#19988;&#23494;&#38598;&#30340;&#20266;&#26631;&#27880;&#20998;&#21106;&#25513;&#27169;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#29305;&#24449;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#25552;&#20986;&#20102;&#20960;&#31181;&#33258;&#30417;&#30563;&#35757;&#32451;&#26041;&#26696;&#30340;&#24314;&#35758;&#65292;&#20351;&#24471;&#24320;&#21457;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#26356;&#36817;&#20102;&#19968;&#27493;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#26080;&#30417;&#30563;&#19979;&#28216;&#20219;&#21153;&#34987;&#35748;&#20026;&#26159;&#39564;&#35777;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#26041;&#26696;&#23398;&#20064;&#21040;&#30340;&#35270;&#35273;&#29305;&#24449;&#36136;&#37327;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#25506;&#32034;&#26080;&#30417;&#30563;&#30340;&#23494;&#38598;&#35821;&#20041;&#20998;&#21106;&#20316;&#20026;&#19968;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#21644;&#35780;&#20272;&#33258;&#30417;&#30563;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#35821;&#20041;&#20449;&#24687;&#22312;&#34917;&#19969;&#32423;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65288;DatUS^2&#65289;&#20316;&#20026;&#19968;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;DatUS^2&#20026;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#29983;&#25104;&#20102;&#35821;&#20041;&#19968;&#33268;&#19988;&#23494;&#38598;&#30340;&#20266;&#26631;&#27880;&#20998;&#21106;&#25513;&#27169;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#35270;&#35273;&#20808;&#39564;&#25110;&#21516;&#27493;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#20266;&#26631;&#27880;&#20998;&#21106;&#25513;&#27169;&#19982;&#30495;&#23454;&#25513;&#27169;&#36827;&#34892;&#20102;&#27604;&#36739;&#20197;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successive proposals of several self-supervised training schemes continue to emerge, taking one step closer to developing a universal foundation model. In this process, the unsupervised downstream tasks are recognized as one of the evaluation methods to validate the quality of visual features learned with a self-supervised training scheme. However, unsupervised dense semantic segmentation has not been explored as a downstream task, which can utilize and evaluate the quality of semantic information introduced in patch-level feature representations during self-supervised training of a vision transformer. Therefore, this paper proposes a novel data-driven approach for unsupervised semantic segmentation (DatUS^2) as a downstream task. DatUS^2 generates semantically consistent and dense pseudo annotate segmentation masks for the unlabeled image dataset without using any visual-prior or synchronized data. We compare these pseudo-annotated segmentation masks with ground truth masks for evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#23618;&#32465;&#23450;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21160;&#24577;&#36873;&#25321;&#23618;&#24182;&#23558;&#23427;&#20204;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#26469;&#20943;&#23569;&#28145;&#24230;Transformer&#32593;&#32476;&#20013;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12819</link><description>&lt;p&gt;
&#21160;&#24577;&#23618;&#32465;&#23450;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Dynamic Layer Tying for Parameter-Efficient Transformers. (arXiv:2401.12819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#23618;&#32465;&#23450;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21160;&#24577;&#36873;&#25321;&#23618;&#24182;&#23558;&#23427;&#20204;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#26469;&#20943;&#23569;&#28145;&#24230;Transformer&#32593;&#32476;&#20013;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20943;&#23569;&#28145;&#24230;Transformer&#32593;&#32476;&#20013;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#35757;&#32451;&#26399;&#38388;&#21160;&#24577;&#36873;&#25321;&#23618;&#24182;&#23558;&#23427;&#20204;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#27599;&#38548;&#19968;&#27573;&#26102;&#38388;&#65292;RL agent&#20250;&#34987;&#35810;&#38382;&#26159;&#21542;&#29420;&#31435;&#35757;&#32451;&#27599;&#20010;&#23618;$i$&#65292;&#36824;&#26159;&#22797;&#21046;&#21069;&#19968;&#20010;&#23618;$j&lt;i$&#30340;&#26435;&#37325;&#12290;&#36825;&#26679;&#20570;&#26377;&#21161;&#20110;&#20849;&#20139;&#26435;&#37325;&#65292;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20063;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#23454;&#39564;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#30053;&#20248;&#20110;&#22522;&#20934;Transformer&#27169;&#22411;&#65292;&#24182;&#19988;&#26174;&#33879;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#30340;&#20869;&#23384;&#28040;&#32791;&#27604;&#24120;&#35268;&#35757;&#32451;&#26041;&#27861;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer $i$ independently or to copy the weights of a previous layer $j&lt;i$. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#26469;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#35299;&#30340;&#26041;&#31243;&#26102;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.12806</link><description>&lt;p&gt;
&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#35299;&#30340;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Binary structured physics-informed neural networks for solving equations with rapidly changing solutions. (arXiv:2401.12806v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#26469;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#35299;&#30340;&#26041;&#31243;&#26102;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24050;&#25104;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;PDEs&#25551;&#36848;&#30340;&#29289;&#29702;&#20449;&#24687;&#23884;&#20837;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;PINNs&#34987;&#35757;&#32451;&#20026;&#26367;&#20195;&#27169;&#22411;&#65292;&#20197;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#32780;&#26080;&#38656;&#26631;&#31614;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;PINNs&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#35299;&#30340;&#26041;&#31243;&#26102;&#21487;&#33021;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#36825;&#20123;&#22256;&#38590;&#21253;&#25324;&#25910;&#25947;&#36895;&#24230;&#24930;&#12289;&#26131;&#38519;&#20837;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#35299;&#20915;&#31934;&#24230;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(BsPINN)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;(BsNN)&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12290;&#36890;&#36807;&#21033;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#65292;BsPINNs&#22312;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs), rooted in deep learning, have emerged as a promising approach for solving partial differential equations (PDEs). By embedding the physical information described by PDEs into feedforward neural networks, PINNs are trained as surrogate models to approximate solutions without the need for label data. Nevertheless, even though PINNs have shown remarkable performance, they can face difficulties, especially when dealing with equations featuring rapidly changing solutions. These difficulties encompass slow convergence, susceptibility to becoming trapped in local minima, and reduced solution accuracy. To address these issues, we propose a binary structured physics-informed neural network (BsPINN) framework, which employs binary structured neural network (BsNN) as the neural network component. By leveraging a binary structure that reduces inter-neuron connections compared to fully connected neural networks, BsPINNs excel in capturing the local features 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;5G NR PRACH&#25509;&#25910;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#20998;&#21035;&#20272;&#35745;RAPID&#21644;TA&#65292;&#19982;&#20256;&#32479;&#30456;&#20851;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.12803</link><description>&lt;p&gt;
5G NR PRACH&#25509;&#25910;&#30340;&#22686;&#24378;&#65306;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancements for 5G NR PRACH Reception: An AI/ML Approach. (arXiv:2401.12803v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12803
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;5G NR PRACH&#25509;&#25910;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#20998;&#21035;&#20272;&#35745;RAPID&#21644;TA&#65292;&#19982;&#20256;&#32479;&#30456;&#20851;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#25509;&#20837;&#26159;&#20351;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#19982;&#22522;&#31449;&#65288;gNB&#65289;&#36827;&#34892;&#26368;&#21021;&#36830;&#25509;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;UE&#36890;&#36807;&#23884;&#20837;&#22312;&#24050;&#30693;&#22522;&#24207;&#21015;&#30340;&#30456;&#20301;&#26059;&#36716;&#20013;&#30340;&#21069;&#23548;&#32034;&#24341;&#65288;RAPID&#65289;&#26469;&#36827;&#34892;&#33258;&#25105;&#35782;&#21035;&#65292;&#24182;&#23558;&#20854;&#21457;&#36865;&#21040;&#29289;&#29702;&#38543;&#26426;&#25509;&#20837;&#36890;&#36947;&#65288;PRACH&#65289;&#12290;PRACH&#19978;&#30340;&#20449;&#21495;&#36824;&#33021;&#22815;&#20272;&#35745;&#30001;UE&#20301;&#32622;&#24341;&#36215;&#30340;&#20256;&#25773;&#26102;&#24310;&#65288;&#24120;&#31216;&#20026;&#26102;&#38388;&#25552;&#21069;&#65292;TA&#65289;&#12290;&#20256;&#32479;&#25509;&#25910;&#22120;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;RAPID&#21644;TA&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;AI/ML&#27169;&#22411;&#30340;&#26367;&#20195;&#25509;&#25910;&#22120;&#26041;&#27861;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#19968;&#20010;&#29992;&#20110;RAPID&#65292;&#19968;&#20010;&#29992;&#20110;TA&#12290;&#19982;&#20854;&#20182;&#30740;&#31350;&#19981;&#21516;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#21487;&#20197;&#24182;&#34892;&#36816;&#34892;&#65292;&#32780;&#19981;&#26159;&#39034;&#24207;&#36816;&#34892;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#21644;&#23454;&#38469;&#30828;&#20214;&#25429;&#33719;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;AI/ML&#30340;&#25216;&#26415;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#30456;&#20851;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random Access is an important step in enabling the initial attachment of a User Equipment (UE) to a Base Station (gNB). The UE identifies itself by embedding a Preamble Index (RAPID) in the phase rotation of a known base sequence, which it transmits on the Physical Random Access Channel (PRACH). The signal on the PRACH also enables the estimation of propagation delay, often known as Timing Advance (TA), which is induced by virtue of the UE's position. Traditional receivers estimate the RAPID and TA using correlation-based techniques. This paper presents an alternative receiver approach that uses AI/ML models, wherein two neural networks are proposed, one for the RAPID and one for the TA. Different from other works, these two models can run in parallel as opposed to sequentially. Experiments with both simulated data and over-the-air hardware captures highlight the improved performance of the proposed AI/ML-based techniques compared to conventional correlation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#31995;&#32479;&#20013;&#23558;&#38647;&#36798;&#30446;&#26631;&#19982;&#36890;&#20449;&#29992;&#25143;&#35774;&#22791;&#36827;&#34892;&#20851;&#32852;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#32852;&#21512;&#22810;&#30446;&#26631;&#26816;&#27979;&#21644;&#27874;&#26463;&#25512;&#29702;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#20027;&#21160;&#20999;&#25442;&#21644;&#27874;&#26463;&#39044;&#27979;&#31561;&#36890;&#20449;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.12801</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#31995;&#32479;&#20013;&#22522;&#20110;&#30446;&#26631;&#21040;&#29992;&#25143;&#20851;&#32852;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-based Target-To-User Association in Integrated Sensing and Communication Systems. (arXiv:2401.12801v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#31995;&#32479;&#20013;&#23558;&#38647;&#36798;&#30446;&#26631;&#19982;&#36890;&#20449;&#29992;&#25143;&#35774;&#22791;&#36827;&#34892;&#20851;&#32852;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#32852;&#21512;&#22810;&#30446;&#26631;&#26816;&#27979;&#21644;&#27874;&#26463;&#25512;&#29702;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#20027;&#21160;&#20999;&#25442;&#21644;&#27874;&#26463;&#39044;&#27979;&#31561;&#36890;&#20449;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#65288;ISAC&#65289;&#31995;&#32479;&#20013;&#65292;&#23558;&#38647;&#36798;&#30446;&#26631;&#19982;&#36890;&#20449;&#29992;&#25143;&#35774;&#22791;&#65288;UEs&#65289;&#36827;&#34892;&#21305;&#37197;&#23545;&#20110;&#20960;&#31181;&#36890;&#20449;&#20219;&#21153;&#26159;&#26377;&#24847;&#20041;&#30340;&#65292;&#22914;&#20027;&#21160;&#20999;&#25442;&#21644;&#27874;&#26463;&#39044;&#27979;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#38647;&#36798;&#36741;&#21161;&#36890;&#20449;&#31995;&#32479;&#65292;&#19968;&#20010;&#22522;&#31449;&#65288;BS&#65289;&#37197;&#22791;&#26377;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#38647;&#36798;&#65292;&#38647;&#36798;&#20855;&#26377;&#21452;&#37325;&#30446;&#26631;&#65306;&#65288;i&#65289;&#23558;&#36710;&#36742;&#38647;&#36798;&#30446;&#26631;&#19982;&#36890;&#20449;&#27874;&#26463;&#31354;&#38388;&#20013;&#30340;&#36710;&#36742;&#35774;&#22791;&#65288;VEs&#65289;&#20851;&#32852;&#36215;&#26469;&#65292;&#65288;ii&#65289;&#26681;&#25454;&#38647;&#36798;&#25968;&#25454;&#39044;&#27979;&#27599;&#20010;VE&#30340;&#27874;&#26463;&#24418;&#25104;&#30690;&#37327;&#12290;&#25552;&#20986;&#30340;&#30446;&#26631;&#21040;&#29992;&#25143;&#65288;T2U&#65289;&#20851;&#32852;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#36317;&#31163;-&#35282;&#24230;&#22270;&#20687;&#26816;&#27979;&#36710;&#36742;&#38647;&#36798;&#30446;&#26631;&#65292;&#24182;&#20026;&#27599;&#20010;&#30446;&#26631;&#20272;&#35745;&#19968;&#20010;&#27874;&#26463;&#24418;&#25104;&#30690;&#37327;&#12290;&#28982;&#21518;&#65292;&#23558;&#25512;&#26029;&#24471;&#21040;&#30340;&#27599;&#20010;&#30446;&#26631;&#30340;&#27874;&#26463;&#24418;&#25104;&#30690;&#37327;&#19982;BS&#29992;&#20110;&#36890;&#20449;&#30340;&#27874;&#26463;&#24418;&#25104;&#30690;&#37327;&#36827;&#34892;&#21305;&#37197;&#65292;&#20197;&#25191;&#34892;&#30446;&#26631;&#21040;&#29992;&#25143;&#65288;T2U&#65289;&#20851;&#32852;&#12290;&#36890;&#36807;&#20462;&#25913;&#20320;&#21482;&#30475;&#33080;&#37096;&#32593;&#32476;&#65288;YOLO&#65289;&#31639;&#27861;&#23454;&#29616;&#20102;&#32852;&#21512;&#22810;&#30446;&#26631;&#26816;&#27979;&#21644;&#27874;&#26463;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Integrated Sensing and Communication (ISAC) systems, matching the radar targets with communication user equipments (UEs) is functional to several communication tasks, such as proactive handover and beam prediction. In this paper, we consider a radar-assisted communication system where a base station (BS) is equipped with a multiple-input-multiple-output (MIMO) radar that has a double aim: (i) associate vehicular radar targets to vehicular equipments (VEs) in the communication beamspace and (ii) predict the beamforming vector for each VE from radar data. The proposed target-to-user (T2U) association consists of two stages. First, vehicular radar targets are detected from range-angle images, and, for each, a beamforming vector is estimated. Then, the inferred per-target beamforming vectors are matched with the ones utilized at the BS for communication to perform target-to-user (T2U) association. Joint multi-target detection and beam inference is obtained by modifying the you only look
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#29289;&#29702;&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#23427;&#20204;&#25152;&#25903;&#25345;&#30340;&#35821;&#20041;&#24212;&#29992;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#36825;&#20123;&#31995;&#32479;&#34920;&#29616;&#20986;&#20102;&#22686;&#24378;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#65292;&#33021;&#22815;&#29702;&#35299;&#21644;&#36866;&#24212;&#25968;&#25454;&#20256;&#36755;&#30340;&#19978;&#19979;&#25991;&#21644;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2401.12800</link><description>&lt;p&gt;
&#29289;&#29702;&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#65306;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#36890;&#20449;&#31995;&#32479;&#21450;&#20854;&#25903;&#25345;&#30340;&#35821;&#20041;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Learning in Physical Layer: Review on Data Driven End-to-End Communication Systems and their Enabling Semantic Applications. (arXiv:2401.12800v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12800
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#29289;&#29702;&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#23427;&#20204;&#25152;&#25903;&#25345;&#30340;&#35821;&#20041;&#24212;&#29992;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#36825;&#20123;&#31995;&#32479;&#34920;&#29616;&#20986;&#20102;&#22686;&#24378;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#65292;&#33021;&#22815;&#29702;&#35299;&#21644;&#36866;&#24212;&#25968;&#25454;&#20256;&#36755;&#30340;&#19978;&#19979;&#25991;&#21644;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#21644;&#20248;&#21270;&#29289;&#29702;&#23618;&#23454;&#29616;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#22312;&#22797;&#26434;&#30340;&#26080;&#32447;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20102;&#22686;&#24378;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#65292;&#28385;&#36275;&#20102;5G&#21450;&#20854;&#20197;&#19978;&#32593;&#32476;&#31995;&#32479;&#21644;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#22312;&#29289;&#29702;&#23618;&#30340;&#21457;&#23637;&#20351;&#24471;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#22810;&#27169;&#24577;&#20256;&#36755;&#31561;&#21508;&#31181;&#27169;&#24577;&#19979;&#23454;&#29616;&#20102;&#39640;&#32423;&#30340;&#35821;&#20041;&#24212;&#29992;&#12290;&#36825;&#20123;&#24212;&#29992;&#20174;&#20256;&#32479;&#30340;&#27604;&#29305;&#32423;&#36890;&#20449;&#36716;&#21464;&#20026;&#35821;&#20041;&#32423;&#26234;&#33021;&#36890;&#20449;&#31995;&#32479;&#65292;&#33021;&#22815;&#29702;&#35299;&#21644;&#36866;&#24212;&#25968;&#25454;&#20256;&#36755;&#30340;&#19978;&#19979;&#25991;&#21644;&#24847;&#22270;&#12290;&#34429;&#28982;&#29289;&#29702;&#23618;&#20316;&#20026;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#36890;&#20449;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26159;&#23454;&#29616;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36817;&#24180;&#26469;&#26377;&#21508;&#31181;&#30740;&#31350;&#20998;&#21035;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) has enabled a paradigm shift in wireless communication system with data driven end-to-end (E2E) learning and optimization of the Physical Layer (PHY). By leveraging the representation learning of DL, E2E systems exhibit enhanced adaptability and performance in complex wireless environments, fulfilling the demands of 5G and beyond network systems and applications. The evolution of data-driven techniques in the PHY has enabled advanced semantic applications across various modalities including text, image, audio, video, and multi-modal transmissions. These applications transcend from traditional bit-level communication to semantic-level intelligent communication systems, which are capable of understanding and adapting to the context and intent of the data transmission. Although PHY as a DL architecture for data-driven E2E communication is a key factor in enabling semantic communication systems (SemCom), and various studies in recent years have surveyed them separately, 
&lt;/p&gt;</description></item><item><title>MORPH&#26159;&#19968;&#31181;&#19987;&#20026;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#26377;&#25928;&#30340;&#22522;&#20110;&#20266;&#26631;&#31614;&#30340;&#27010;&#24565;&#28418;&#31227;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#65292;&#23427;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#33021;&#22815;&#26377;&#25928;&#32531;&#35299;&#27010;&#24565;&#28418;&#31227;&#65292;&#24182;&#19988;&#33021;&#22815;&#38477;&#20302;&#27880;&#37322;&#24037;&#20316;&#37327;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.12790</link><description>&lt;p&gt;
MORPH&#65306;&#20026;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#25552;&#20379;&#33258;&#21160;&#27010;&#24565;&#28418;&#31227;&#36866;&#24212;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MORPH: Towards Automated Concept Drift Adaptation for Malware Detection. (arXiv:2401.12790v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12790
&lt;/p&gt;
&lt;p&gt;
MORPH&#26159;&#19968;&#31181;&#19987;&#20026;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#26377;&#25928;&#30340;&#22522;&#20110;&#20266;&#26631;&#31614;&#30340;&#27010;&#24565;&#28418;&#31227;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#65292;&#23427;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#33021;&#22815;&#26377;&#25928;&#32531;&#35299;&#27010;&#24565;&#28418;&#31227;&#65292;&#24182;&#19988;&#33021;&#22815;&#38477;&#20302;&#27880;&#37322;&#24037;&#20316;&#37327;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#28418;&#31227;&#23545;&#20110;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#19979;&#38477;&#65292;&#20351;&#20854;&#21464;&#24471;&#19981;&#23454;&#29992;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#24694;&#24847;&#36719;&#20214;&#27010;&#24565;&#28418;&#31227;&#36866;&#24212;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20027;&#21160;&#23398;&#20064;&#19978;&#65292;&#21363;&#36890;&#36807;&#36873;&#25321;&#20195;&#34920;&#24615;&#26679;&#26412;&#26469;&#26356;&#26032;&#27169;&#22411;&#65292;&#20294;&#33258;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#32531;&#35299;&#27010;&#24565;&#28418;&#31227;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#33258;&#35757;&#32451;&#26159;&#25351;&#20351;&#29992;&#20266;&#26631;&#31614;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MORPH&#30340;&#26377;&#25928;&#22522;&#20110;&#20266;&#26631;&#31614;&#30340;&#27010;&#24565;&#28418;&#31227;&#36866;&#24212;&#26041;&#27861;&#65292;&#19987;&#38376;&#20026;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#12290;&#36890;&#36807;&#23545;Android&#21644;Windows&#24694;&#24847;&#36719;&#20214;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32531;&#35299;&#27010;&#24565;&#28418;&#31227;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19982;&#20027;&#21160;&#23398;&#20064;&#30456;&#32467;&#21512;&#26102;&#65292;&#38477;&#20302;&#20102;&#27880;&#37322;&#24037;&#20316;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept drift is a significant challenge for malware detection, as the performance of trained machine learning models degrades over time, rendering them impractical. While prior research in malware concept drift adaptation has primarily focused on active learning, which involves selecting representative samples to update the model, self-training has emerged as a promising approach to mitigate concept drift. Self-training involves retraining the model using pseudo labels to adapt to shifting data distributions. In this research, we propose MORPH -- an effective pseudo-label-based concept drift adaptation method specifically designed for neural networks. Through extensive experimental analysis of Android and Windows malware datasets, we demonstrate the efficacy of our approach in mitigating the impact of concept drift. Our method offers the advantage of reducing annotation efforts when combined with active learning. Furthermore, our method significantly improves over existing works in au
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#33258;2017&#24180;&#33267;2023&#24180;&#26399;&#38388;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22788;&#29702;&#20809;&#30005;&#23481;&#31215;&#27861;&#25968;&#25454;&#30340;&#35770;&#25991;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20010;&#20154;&#20581;&#24247;&#31649;&#29702;&#21644;&#20854;&#20182;&#24212;&#29992;&#20013;&#20855;&#26377;&#26174;&#33879;&#25104;&#26524;&#12290;&#26681;&#25454;&#20219;&#21153;&#30340;&#19981;&#21516;&#65292;&#36825;&#20123;&#35770;&#25991;&#34987;&#20998;&#20026;&#21307;&#23398;&#30456;&#20851;&#21644;&#38750;&#21307;&#23398;&#30456;&#20851;&#20004;&#22823;&#31867;&#21035;&#65292;&#21307;&#23398;&#30456;&#20851;&#21448;&#32454;&#20998;&#20026;&#19971;&#20010;&#23376;&#32452;&#65292;&#21253;&#25324;&#34880;&#21387;&#20998;&#26512;...</title><link>http://arxiv.org/abs/2401.12783</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20809;&#30005;&#23481;&#31215;&#27861;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Deep Learning Methods for Photoplethysmography Data. (arXiv:2401.12783v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#33258;2017&#24180;&#33267;2023&#24180;&#26399;&#38388;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22788;&#29702;&#20809;&#30005;&#23481;&#31215;&#27861;&#25968;&#25454;&#30340;&#35770;&#25991;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20010;&#20154;&#20581;&#24247;&#31649;&#29702;&#21644;&#20854;&#20182;&#24212;&#29992;&#20013;&#20855;&#26377;&#26174;&#33879;&#25104;&#26524;&#12290;&#26681;&#25454;&#20219;&#21153;&#30340;&#19981;&#21516;&#65292;&#36825;&#20123;&#35770;&#25991;&#34987;&#20998;&#20026;&#21307;&#23398;&#30456;&#20851;&#21644;&#38750;&#21307;&#23398;&#30456;&#20851;&#20004;&#22823;&#31867;&#21035;&#65292;&#21307;&#23398;&#30456;&#20851;&#21448;&#32454;&#20998;&#20026;&#19971;&#20010;&#23376;&#32452;&#65292;&#21253;&#25324;&#34880;&#21387;&#20998;&#26512;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#23481;&#31215;&#27861;&#65288;PPG&#65289;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#21069;&#26223;&#30340;&#35774;&#22791;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#20415;&#25658;&#24615;&#12289;&#29992;&#25143;&#21451;&#22909;&#25805;&#20316;&#21644;&#38750;&#20405;&#20837;&#24615;&#27979;&#37327;&#22810;&#31181;&#29983;&#29702;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36890;&#36807;&#21033;&#29992;PPG&#20449;&#21495;&#65292;&#23637;&#31034;&#20102;&#22312;&#20010;&#20154;&#20581;&#24247;&#31649;&#29702;&#21644;&#20854;&#20182;&#22810;&#26041;&#38754;&#24212;&#29992;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#33258;2017&#24180;1&#26376;1&#26085;&#33267;2023&#24180;7&#26376;31&#26085;&#26399;&#38388;&#22312;Google&#23398;&#26415;&#12289;PubMed&#21644;Dimensions&#21457;&#34920;&#30340;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22788;&#29702;PPG&#25968;&#25454;&#30340;&#35770;&#25991;&#12290;&#27599;&#31687;&#35770;&#25991;&#20174;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#19977;&#20010;&#20851;&#38190;&#35282;&#24230;&#36827;&#34892;&#20998;&#26512;&#12290;&#26368;&#32456;&#25552;&#21462;&#20102;193&#31687;&#35770;&#25991;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#22788;&#29702;PPG&#20449;&#21495;&#12290;&#26681;&#25454;&#36825;&#20123;&#35770;&#25991;&#25152;&#28041;&#21450;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#20998;&#20026;&#20004;&#22823;&#31867;&#21035;&#65306;&#21307;&#23398;&#30456;&#20851;&#21644;&#38750;&#21307;&#23398;&#30456;&#20851;&#12290;&#21307;&#23398;&#30456;&#20851;&#20219;&#21153;&#36827;&#19968;&#27493;&#20998;&#20026;&#19971;&#20010;&#23376;&#32452;&#65292;&#21253;&#25324;&#34880;&#21387;&#20998;&#26512;...
&lt;/p&gt;
&lt;p&gt;
Photoplethysmography (PPG) is a highly promising device due to its advantages in portability, user-friendly operation, and non-invasive capabilities to measure a wide range of physiological information. Recent advancements in deep learning have demonstrated remarkable outcomes by leveraging PPG signals for tasks related to personal health management and other multifaceted applications. In this review, we systematically reviewed papers that applied deep learning models to process PPG data between January 1st of 2017 and July 31st of 2023 from Google Scholar, PubMed and Dimensions. Each paper is analyzed from three key perspectives: tasks, models, and data. We finally extracted 193 papers where different deep learning frameworks were used to process PPG signals. Based on the tasks addressed in these papers, we categorized them into two major groups: medical-related, and non-medical-related. The medical-related tasks were further divided into seven subgroups, including blood pressure anal
&lt;/p&gt;</description></item><item><title>DeepRicci&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#22270;&#32467;&#26500;-&#29305;&#24449;&#21327;&#21516;&#31934;&#21270;&#25216;&#26415;&#65292;&#26088;&#22312;&#32531;&#35299;&#20856;&#22411;GNNs&#20013;&#30340;&#36807;&#24230;&#25380;&#21387;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#32771;&#34385;Ricci&#26354;&#29575;&#26469;&#25913;&#36827;GNNs&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#40654;&#26364;&#27169;&#22411;DeepRicci&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.12780</link><description>&lt;p&gt;
DeepRicci:&#29992;&#20110;&#32531;&#35299;&#36807;&#24230;&#25380;&#21387;&#30340;&#33258;&#30417;&#30563;&#22270;&#32467;&#26500;-&#29305;&#24449;&#21327;&#21516;&#31934;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
DeepRicci: Self-supervised Graph Structure-Feature Co-Refinement for Alleviating Over-squashing. (arXiv:2401.12780v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12780
&lt;/p&gt;
&lt;p&gt;
DeepRicci&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#22270;&#32467;&#26500;-&#29305;&#24449;&#21327;&#21516;&#31934;&#21270;&#25216;&#26415;&#65292;&#26088;&#22312;&#32531;&#35299;&#20856;&#22411;GNNs&#20013;&#30340;&#36807;&#24230;&#25380;&#21387;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#32771;&#34385;Ricci&#26354;&#29575;&#26469;&#25913;&#36827;GNNs&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#40654;&#26364;&#27169;&#22411;DeepRicci&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#24418;&#23398;&#20064;&#21644;&#25366;&#25496;&#26041;&#38754;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#32780;&#22270;&#32467;&#26500;&#23398;&#20064;&#65288;GSL&#65289;&#22312;&#25552;&#21319;GNNs&#30340;&#24615;&#33021;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#22823;&#22810;&#25968;GSL&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#20027;&#35201;&#20851;&#27880;&#20855;&#26377;&#20219;&#21153;&#29305;&#23450;&#30417;&#30563;&#65288;&#22914;&#33410;&#28857;&#20998;&#31867;&#65289;&#30340;&#32467;&#26500;&#31934;&#21270;&#38382;&#39064;&#65292;&#35201;&#20040;&#24573;&#35270;GNNs&#33258;&#36523;&#30340;&#22266;&#26377;&#32570;&#38519;&#65288;&#22914;&#36807;&#24230;&#25380;&#21387;&#65289;&#65292;&#23548;&#33268;&#23613;&#31649;&#37319;&#29992;&#20102;&#31934;&#33268;&#35774;&#35745;&#65292;&#20294;&#24615;&#33021;&#20122;&#20248;&#12290;&#37492;&#20110;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30740;&#31350;&#29992;&#20110;&#26377;&#25928;&#32531;&#35299;&#20856;&#22411;GNNs&#20013;&#30340;&#36807;&#24230;&#25380;&#21387;&#38382;&#39064;&#30340;&#33258;&#30417;&#30563;&#22270;&#32467;&#26500;-&#29305;&#24449;&#21327;&#21516;&#31934;&#21270;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#40654;&#26364;&#20960;&#20309;&#20013;&#22522;&#26412;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#32771;&#34385;&#40654;&#26364;&#26354;&#29575;&#65292;&#20854;&#20013;&#25105;&#20204;&#36935;&#21040;&#20102;&#24314;&#27169;&#12289;&#21033;&#29992;&#21644;&#35745;&#31639;&#40654;&#26364;&#26354;&#29575;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#40654;&#26364;&#27169;&#22411;&#65292;DeepRicci&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have shown great power for learning and mining on graphs, and Graph Structure Learning (GSL) plays an important role in boosting GNNs with a refined graph. In the literature, most GSL solutions either primarily focus on structure refinement with task-specific supervision (i.e., node classification), or overlook the inherent weakness of GNNs themselves (e.g., over-squashing), resulting in suboptimal performance despite sophisticated designs. In light of these limitations, we propose to study self-supervised graph structure-feature co-refinement for effectively alleviating the issue of over-squashing in typical GNNs. In this paper, we take a fundamentally different perspective of the Ricci curvature in Riemannian geometry, in which we encounter the challenges of modeling, utilizing and computing Ricci curvature. To tackle these challenges, we present a self-supervised Riemannian model, DeepRicci. Specifically, we introduce a latent Riemannian space of heterog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#32806;&#21512;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#26681;&#65292;&#24182;&#19988;&#22312;&#24378;&#21333;&#35843;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21270;&#25910;&#25947;&#36895;&#29575;&#20026;$\mathcal{O}(1/k)$&#12290;</title><link>http://arxiv.org/abs/2401.12764</link><description>&lt;p&gt;
&#24555;&#36895;&#38750;&#32447;&#24615;&#30340;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#65306;&#23454;&#29616;$\mathcal{O}(1/k)$&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fast Nonlinear Two-Time-Scale Stochastic Approximation: Achieving $\mathcal{O}(1/k)$ Finite-Sample Complexity. (arXiv:2401.12764v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#32806;&#21512;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#26681;&#65292;&#24182;&#19988;&#22312;&#24378;&#21333;&#35843;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21270;&#25910;&#25947;&#36895;&#29575;&#20026;$\mathcal{O}(1/k)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#32806;&#21512;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#26681;&#65292;&#20165;&#20551;&#35774;&#21487;&#20197;&#35266;&#27979;&#21040;&#36825;&#20123;&#31639;&#23376;&#30340;&#22122;&#22768;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#32463;&#20856;&#30340;Ruppert-Polyak&#24179;&#22343;&#25216;&#26415;&#36890;&#36807;&#26679;&#26412;&#21160;&#24577;&#20272;&#35745;&#31639;&#23376;&#30340;&#20540;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#24179;&#22343;&#27493;&#39588;&#30340;&#20272;&#35745;&#20540;&#23558;&#29992;&#20110;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#26356;&#26032;&#20197;&#25214;&#21040;&#25152;&#38656;&#30340;&#35299;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#26159;&#22312;&#24213;&#23618;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#24378;&#21333;&#35843;&#26465;&#20214;&#19979;&#65292;&#25152;&#25552;&#20986;&#26041;&#27861;&#20135;&#29983;&#30340;&#36845;&#20195;&#30340;&#22343;&#26041;&#35823;&#24046;&#20197;&#20248;&#21270;&#30340;&#36895;&#29575;$\mathcal{O}(1/k)$&#25910;&#25947;&#20110;&#38646;&#65292;&#20854;&#20013;$k$&#20026;&#36845;&#20195;&#27425;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#33879;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#32467;&#26524;&#65292;&#26368;&#20339;&#24050;&#30693;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#36895;&#29575;&#20026;$\mathcal{O}(1/k^{2/3})$&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes to develop a new variant of the two-time-scale stochastic approximation to find the roots of two coupled nonlinear operators, assuming only noisy samples of these operators can be observed. Our key idea is to leverage the classic Ruppert-Polyak averaging technique to dynamically estimate the operators through their samples. The estimated values of these averaging steps will then be used in the two-time-scale stochastic approximation updates to find the desired solution. Our main theoretical result is to show that under the strongly monotone condition of the underlying nonlinear operators the mean-squared errors of the iterates generated by the proposed method converge to zero at an optimal rate $\mathcal{O}(1/k)$, where $k$ is the number of iterations. Our result significantly improves the existing result of two-time-scale stochastic approximation, where the best known finite-time convergence rate is $\mathcal{O}(1/k^{2/3})$.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#23454;&#20363;&#36827;&#34892;&#30701;&#26399;&#27714;&#35299;&#24471;&#21040;&#30340;&#25506;&#27979;&#36712;&#36857;&#26469;&#25551;&#36848;&#23454;&#20363;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;&#31639;&#27861;&#36873;&#25321;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#21069;&#26223;&#21644;&#26032;&#39062;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12745</link><description>&lt;p&gt;
&#20851;&#20110;&#25506;&#27979;&#36712;&#36857;&#22312;&#31639;&#27861;&#36873;&#25321;&#20013;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Utility of Probing Trajectories for Algorithm-Selection. (arXiv:2401.12745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#23454;&#20363;&#36827;&#34892;&#30701;&#26399;&#27714;&#35299;&#24471;&#21040;&#30340;&#25506;&#27979;&#36712;&#36857;&#26469;&#25551;&#36848;&#23454;&#20363;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;&#31639;&#27861;&#36873;&#25321;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#21069;&#26223;&#21644;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31639;&#27861;&#36873;&#25321;&#20013;&#36890;&#24120;&#23558;&#25551;&#36848;&#23454;&#20363;&#30340;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#12290;&#36755;&#20837;&#25968;&#25454;&#21487;&#20197;&#26159;&#20174;&#23454;&#20363;&#25551;&#36848;&#25110;&#36866;&#24212;&#24230;&#26223;&#35266;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#20063;&#21487;&#20197;&#26159;&#23454;&#20363;&#26412;&#36523;&#30340;&#30452;&#25509;&#34920;&#31034;&#65292;&#21363;&#22270;&#20687;&#25110;&#25991;&#26412;&#25551;&#36848;&#12290;&#26080;&#35770;&#36873;&#25321;&#30340;&#36755;&#20837;&#26159;&#20160;&#20040;&#65292;&#37117;&#23384;&#22312;&#36825;&#26679;&#19968;&#31181;&#38544;&#21547;&#20551;&#35774;&#65306;&#30456;&#20284;&#30340;&#23454;&#20363;&#20250;&#24341;&#36215;&#31639;&#27861;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#36825;&#31181;&#20851;&#31995;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20165;&#20174;&#23454;&#20363;&#35282;&#24230;&#26469;&#30475;&#31639;&#27861;&#36873;&#25321;&#21487;&#33021;&#20250;&#35823;&#23548;&#65292;&#22240;&#20026;&#23427;&#26410;&#33021;&#32771;&#34385;&#31639;&#27861;&#23545;&#23454;&#20363;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;`&#30475;&#27861;'&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;"&#31639;&#27861;&#20013;&#24515;"&#26041;&#27861;&#26469;&#25551;&#36848;&#23454;&#20363;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#31639;&#27861;&#36873;&#25321;&#27169;&#22411;&#65306;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#23558;&#19968;&#20010;&#27714;&#35299;&#22120;&#24212;&#29992;&#20110;&#23454;&#20363;&#19968;&#23567;&#27573;&#26102;&#38388;&#21518;&#35745;&#31639;&#24471;&#21040;&#30340;&#30701;&#25506;&#27979;&#36712;&#36857;&#12290;&#36825;&#31181;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#26377;&#21069;&#36884;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learning approaches to algorithm-selection typically take data describing an instance as input. Input data can take the form of features derived from the instance description or fitness landscape, or can be a direct representation of the instance itself, i.e. an image or textual description. Regardless of the choice of input, there is an implicit assumption that instances that are similar will elicit similar performance from algorithm, and that a model is capable of learning this relationship. We argue that viewing algorithm-selection purely from an instance perspective can be misleading as it fails to account for how an algorithm `views' similarity between instances. We propose a novel `algorithm-centric' method for describing instances that can be used to train models for algorithm-selection: specifically, we use short probing trajectories calculated by applying a solver to an instance for a very short period of time. The approach is demonstrated to be promising, providing co
&lt;/p&gt;</description></item><item><title>TNANet&#26159;&#19968;&#31181;&#19987;&#20026;&#20998;&#26512;&#24102;&#22122;&#22768;&#29983;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32780;&#35774;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#24182;&#20808;&#36827;&#30340;&#32534;&#30721;&#25216;&#26415;&#21644;&#32622;&#20449;&#24230;&#23398;&#20064;&#26469;&#25552;&#39640;&#33258;&#26432;&#20542;&#21521;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12733</link><description>&lt;p&gt;
TNANet: &#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#22122;&#22768;&#29983;&#29702;&#25968;&#25454;&#30340;&#33258;&#26432;&#20542;&#21521;&#39044;&#27979;&#30340;&#26102;&#38388;&#22122;&#22768;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TNANet: A Temporal-Noise-Aware Neural Network for Suicidal Ideation Prediction with Noisy Physiological Data. (arXiv:2401.12733v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12733
&lt;/p&gt;
&lt;p&gt;
TNANet&#26159;&#19968;&#31181;&#19987;&#20026;&#20998;&#26512;&#24102;&#22122;&#22768;&#29983;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32780;&#35774;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#24182;&#20808;&#36827;&#30340;&#32534;&#30721;&#25216;&#26415;&#21644;&#32622;&#20449;&#24230;&#23398;&#20064;&#26469;&#25552;&#39640;&#33258;&#26432;&#20542;&#21521;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23384;&#22312;&#22266;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#20581;&#27867;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#23588;&#20854;&#24403;&#26631;&#31614;&#26159;&#20027;&#35266;&#30340;&#24182;&#19988;&#22122;&#22768;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#24456;&#38590;&#36776;&#21035;&#26102;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#29305;&#21035;&#26126;&#26174;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#30417;&#27979;&#33258;&#26432;&#20542;&#21521;&#30340;&#29305;&#27530;&#19988;&#37325;&#35201;&#30340;&#24773;&#26223;&#65292;&#20854;&#20013;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22914;&#20809;&#30005;&#23481;&#31215;&#22270;&#65288;PPG&#65289;&#65292;&#26131;&#21463;&#27492;&#31867;&#22122;&#22768;&#24433;&#21709;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#25110;&#22788;&#29702;&#20154;&#20026;&#24341;&#20837;&#30340;&#22122;&#22768;&#19978;&#65292;&#24573;&#35270;&#20102;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#33258;&#28982;&#22122;&#22768;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#20998;&#26512;&#24102;&#26377;&#22122;&#22768;&#30340;&#29983;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#31216;&#20026;TNANet&#65292;&#23427;&#23558;&#20808;&#36827;&#30340;&#32534;&#30721;&#25216;&#26415;&#19982;&#32622;&#20449;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#21478;&#19968;&#20010;&#36129;&#29486;&#26159;&#25910;&#38598;&#20102;&#19968;&#20010;&#19987;&#38376;&#20174;&#29616;&#23454;&#29615;&#22659;&#20013;&#33719;&#21462;&#30340;PPG&#20449;&#21495;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#33258;&#26432;&#20542;&#21521;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robust generalization of deep learning models in the presence of inherent noise remains a significant challenge, especially when labels are subjective and noise is indiscernible in natural settings. This problem is particularly pronounced in many practical applications. In this paper, we address a special and important scenario of monitoring suicidal ideation, where time-series data, such as photoplethysmography (PPG), is susceptible to such noise. Current methods predominantly focus on image and text data or address artificially introduced noise, neglecting the complexities of natural noise in time-series analysis. To tackle this, we introduce a novel neural network model tailored for analyzing noisy physiological time-series data, named TNANet, which merges advanced encoding techniques with confidence learning, enhancing prediction accuracy. Another contribution of our work is the collection of a specialized dataset of PPG signals derived from real-world environments for suicidal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#26410;&#30693;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#19979;&#30340;SHAP&#35780;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2401.12731</link><description>&lt;p&gt;
SHAP&#35780;&#20998;&#22312;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Distributional Uncertainty of the SHAP score in Explainable Machine Learning. (arXiv:2401.12731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#26410;&#30693;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#19979;&#30340;SHAP&#35780;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#23646;&#20998;&#25968;&#21453;&#26144;&#20102;&#36755;&#20837;&#23454;&#20307;&#20013;&#30340;&#29305;&#24449;&#20540;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#20854;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#35780;&#20998;&#20043;&#19968;&#26159;SHAP&#35780;&#20998;&#65292;&#23427;&#26159;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#20013;Shapley&#20540;&#30340;&#20855;&#20307;&#23454;&#20363;&#12290;&#35813;&#35780;&#20998;&#30340;&#23450;&#20041;&#20381;&#36182;&#20110;&#23454;&#20307;&#32676;&#20307;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#30001;&#20110;&#36890;&#24120;&#19981;&#30693;&#36947;&#31934;&#30830;&#30340;&#20998;&#24067;&#65292;&#22240;&#27492;&#38656;&#35201;&#20027;&#35266;&#22320;&#36827;&#34892;&#20998;&#37197;&#25110;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#20272;&#35745;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#29305;&#24449;&#35780;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19981;&#30693;&#36947;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#30340;SHAP&#35780;&#20998;&#25512;&#29702;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21253;&#21547;&#28508;&#22312;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#32780;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#25104;&#20026;&#22312;&#35813;&#21306;&#22495;&#19978;&#23450;&#20041;&#30340;&#19968;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25214;&#21040;&#35813;&#20989;&#25968;&#30340;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attribution scores reflect how important the feature values in an input entity are for the output of a machine learning model. One of the most popular attribution scores is the SHAP score, which is an instantiation of the general Shapley value used in coalition game theory. The definition of this score relies on a probability distribution on the entity population. Since the exact distribution is generally unknown, it needs to be assigned subjectively or be estimated from data, which may lead to misleading feature scores. In this paper, we propose a principled framework for reasoning on SHAP scores under unknown entity population distributions. In our framework, we consider an uncertainty region that contains the potential distributions, and the SHAP score of a feature becomes a function defined over this region. We study the basic problems of finding maxima and minima of this function, which allows us to determine tight ranges for the SHAP scores of all features. In particular, we pinp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#21644;&#27604;&#20363;&#31867;&#24179;&#34913;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#38024;&#23545;&#23567;&#30446;&#26631;&#30340;&#29289;&#20307;&#26816;&#27979;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#24037;&#19994;&#22330;&#26223;&#20013;&#25910;&#38598;&#21644;&#27880;&#37322;&#23567;&#30446;&#26631;&#25968;&#25454;&#30340;&#38590;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#27604;&#20363;&#31867;&#24179;&#34913;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.12729</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#21644;&#27604;&#20363;&#31867;&#24179;&#34913;&#25216;&#26415;&#25552;&#39640;&#23567;&#30446;&#26631;&#30340;&#29289;&#20307;&#26816;&#27979;&#24615;&#33021;&#65306;&#22312;&#24037;&#19994;&#22330;&#26223;&#20013;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios. (arXiv:2401.12729v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12729
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#21644;&#27604;&#20363;&#31867;&#24179;&#34913;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#38024;&#23545;&#23567;&#30446;&#26631;&#30340;&#29289;&#20307;&#26816;&#27979;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#24037;&#19994;&#22330;&#26223;&#20013;&#25910;&#38598;&#21644;&#27880;&#37322;&#23567;&#30446;&#26631;&#25968;&#25454;&#30340;&#38590;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#27604;&#20363;&#31867;&#24179;&#34913;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#22312;&#25552;&#21462;&#23616;&#37096;&#31867;&#21035;&#20449;&#24687;&#26041;&#38754;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#65292;&#24182;&#22312;&#24037;&#19994;&#20013;&#26377;&#22810;&#31181;&#24212;&#29992;&#12290;&#23613;&#31649;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#22312;&#20013;&#31561;&#21644;&#22823;&#22411;&#30446;&#26631;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#22312;&#23567;&#30446;&#26631;&#19978;&#34920;&#29616;&#19981;&#36275;&#12290;&#22312;&#22823;&#22810;&#25968;&#24037;&#19994;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#25910;&#38598;&#21644;&#27880;&#37322;&#23567;&#30446;&#26631;&#25968;&#25454;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#19988;&#23481;&#26131;&#20986;&#29616;&#20154;&#20026;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#24448;&#24448;&#19981;&#24179;&#34913;&#65292;&#32463;&#24120;&#23548;&#33268;&#27169;&#22411;&#25910;&#25947;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27880;&#20837;&#39069;&#22806;&#30340;&#25968;&#25454;&#28857;&#26469;&#25913;&#21892;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#25910;&#38598;&#21644;&#27880;&#37322;&#23567;&#30446;&#26631;&#25968;&#25454;&#28857;&#30340;&#22256;&#38590;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#24179;&#34913;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#27604;&#20363;&#31867;&#24179;&#34913;&#25216;&#26415;&#30340;&#25928;&#26524;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#30340;&#26377;&#25928;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object Detection (OD) has proven to be a significant computer vision method in extracting localized class information and has multiple applications in the industry. Although many of the state-of-the-art (SOTA) OD models perform well on medium and large sized objects, they seem to under perform on small objects. In most of the industrial use cases, it is difficult to collect and annotate data for small objects, as it is time-consuming and prone to human errors. Additionally, those datasets are likely to be unbalanced and often result in an inefficient model convergence. To tackle this challenge, this study presents a novel approach that injects additional data points to improve the performance of the OD models. Using synthetic data generation, the difficulties in data collection and annotations for small object data points can be minimized and to create a dataset with balanced distribution. This paper discusses the effects of a simple proportional class-balancing technique, to enable be
&lt;/p&gt;</description></item><item><title>Falcon&#26159;&#19968;&#20010;&#20351;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#20844;&#24179;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#26679;&#26412;&#36873;&#25321;&#26469;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#23427;&#36890;&#36807;&#35782;&#21035;&#23545;&#20110;&#25552;&#39640;&#20844;&#24179;&#24615;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#8220;&#30446;&#26631;&#32676;&#20307;&#8221;&#26679;&#26412;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#35797;&#38169;&#26041;&#27861;&#26469;&#35299;&#20915;&#26679;&#26412;&#36873;&#25321;&#20013;&#27809;&#26377;ground truth&#26631;&#31614;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.12722</link><description>&lt;p&gt;
Falcon: &#20351;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#36827;&#34892;&#20844;&#24179;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Falcon: Fair Active Learning using Multi-armed Bandits. (arXiv:2401.12722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12722
&lt;/p&gt;
&lt;p&gt;
Falcon&#26159;&#19968;&#20010;&#20351;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#20844;&#24179;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#26679;&#26412;&#36873;&#25321;&#26469;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#23427;&#36890;&#36807;&#35782;&#21035;&#23545;&#20110;&#25552;&#39640;&#20844;&#24179;&#24615;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#8220;&#30446;&#26631;&#32676;&#20307;&#8221;&#26679;&#26412;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#35797;&#38169;&#26041;&#27861;&#26469;&#35299;&#20915;&#26679;&#26412;&#36873;&#25321;&#20013;&#27809;&#26377;ground truth&#26631;&#31614;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#20506;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24378;&#35843;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#24320;&#22987;&#38454;&#27573;&#23884;&#20837;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#38598;&#30340;&#31579;&#36873;&#21644;&#26631;&#23450;&#36807;&#31243;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20844;&#24179;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;Falcon&#12290;Falcon&#37319;&#29992;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#26679;&#26412;&#36873;&#25321;&#26469;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#32473;&#23450;&#29992;&#25143;&#25351;&#23450;&#30340;&#32676;&#20307;&#20844;&#24179;&#24230;&#37327;&#65292;Falcon&#30830;&#23450;&#20102;&#23545;&#25552;&#39640;&#20844;&#24179;&#24615;&#26368;&#26377;&#20449;&#24687;&#37327;&#30340;&#8220;&#30446;&#26631;&#32676;&#20307;&#8221;&#26679;&#26412;&#65288;&#20363;&#22914;&#65288;&#23646;&#24615;=&#22899;&#24615;&#65292;&#26631;&#31614;=&#27491;&#38754;&#65289;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#26679;&#26412;&#36873;&#25321;&#36807;&#31243;&#20013;&#19981;&#21487;&#29992;ground truth&#26631;&#31614;&#26469;&#23450;&#20041;&#36825;&#20123;&#30446;&#26631;&#32676;&#20307;&#65292;&#20986;&#29616;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35797;&#38169;&#26041;&#27861;&#65292;&#22312;&#39044;&#27979;&#26631;&#31614;&#19982;&#26399;&#26395;&#26631;&#31614;&#19981;&#21516;&#26102;&#24182;&#33853;&#22312;&#30446;&#26631;&#32676;&#20307;&#20043;&#22806;&#26102;&#65292;&#25105;&#20204;&#25512;&#36831;&#20351;&#29992;&#35813;&#26679;&#26412;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#36825;&#26679;&#20570;&#20250;&#20135;&#29983;&#26435;&#34913;&#65292;&#36873;&#25321;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#20250;&#22686;&#21152;&#26679;&#26412;&#36827;&#20837;&#30446;&#26631;&#32676;&#20307;&#20043;&#22806;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biased data can lead to unfair machine learning models, highlighting the importance of embedding fairness at the beginning of data analysis, particularly during dataset curation and labeling. In response, we propose Falcon, a scalable fair active learning framework. Falcon adopts a data-centric approach that improves machine learning model fairness via strategic sample selection. Given a user-specified group fairness measure, Falcon identifies samples from "target groups" (e.g., (attribute=female, label=positive)) that are the most informative for improving fairness. However, a challenge arises since these target groups are defined using ground truth labels that are not available during sample selection. To handle this, we propose a novel trial-and-error method, where we postpone using a sample if the predicted label is different from the expected one and falls outside the target group. We also observe the trade-off that selecting more informative samples results in higher likelihood o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#24050;&#24314;&#31435;&#30340;&#27668;&#20307;&#39281;&#21644;&#24230;&#21644;&#36807;&#28388;&#29305;&#24615;&#30340;&#20307;&#31215;&#65292;&#20351;&#29992;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#27668;&#20307;&#22280;&#38381;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#25928;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.12717</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#19977;&#32500;&#22320;&#38663;&#21644;&#20117;&#27979;&#35797;&#25968;&#25454;&#20013;&#39044;&#27979;&#27668;&#20307;&#22280;&#38381;
&lt;/p&gt;
&lt;p&gt;
Gas trap prediction from 3D seismic and well test data using machine learning. (arXiv:2401.12717v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#24050;&#24314;&#31435;&#30340;&#27668;&#20307;&#39281;&#21644;&#24230;&#21644;&#36807;&#28388;&#29305;&#24615;&#30340;&#20307;&#31215;&#65292;&#20351;&#29992;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#27668;&#20307;&#22280;&#38381;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21019;&#24314;&#21644;&#24212;&#29992;&#19968;&#31181;&#26041;&#27861;&#35770;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#24050;&#24314;&#31435;&#30340;&#27668;&#20307;&#39281;&#21644;&#24230;&#21644;&#36807;&#28388;&#29305;&#24615;&#30340;&#20307;&#31215;&#26469;&#39044;&#27979;&#27668;&#20307;&#22280;&#38381;&#12290;&#35813;&#35770;&#25991;&#23558;&#21019;&#24314;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#29992;&#20110;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#21644;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39034;&#24207;&#24212;&#29992;&#36807;&#31243;&#22534;&#26632;&#20013;&#12290;&#32467;&#26524;&#24471;&#21040;&#20102;&#19968;&#20010;&#26657;&#20934;&#30340;&#21487;&#33021;&#23646;&#20110;&#27668;&#30000;&#30340;&#30740;&#31350;&#31354;&#38388;&#30340;&#27010;&#29575;&#31435;&#26041;&#20307;&#12290;&#35813;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#22312;&#19977;&#21475;&#30450;&#20117;&#30340;&#24310;&#36831;&#27979;&#35797;&#26679;&#26412;&#19978;&#24471;&#21040;&#20102;&#23637;&#31034;&#12290;&#27668;&#20307;&#20648;&#23618;&#39044;&#27979;&#36136;&#37327;&#24230;&#37327;f1&#24471;&#20998;&#30340;&#26368;&#32456;&#20540;&#20026;0.893846.
&lt;/p&gt;
&lt;p&gt;
The aim of this work is to create and apply a methodological approach for predicting gas traps from 3D seismic data and gas well testing. The paper formalizes the approach to creating a training dataset by selecting volumes with established gas saturation and filtration properties within the seismic wavefield. The training dataset thus created is used in a process stack of sequential application of data processing methods and ensemble machine learning algorithms. As a result, a cube of calibrated probabilities of belonging of the study space to gas reservoirs was obtained. The high efficiency of this approach is shown on a delayed test sample of three wells (blind wells). The final value of the gas reservoir prediction quality metric f1 score was 0.893846.
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#26426;&#22120;&#25945;&#23398;&#20013;&#65292;&#27010;&#24565;&#21487;&#20197;&#26377;&#35768;&#22810;&#31561;&#25928;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#36825;&#31181;&#20887;&#20313;&#24615;&#23545;&#25628;&#32034;&#31354;&#38388;&#26377;&#24378;&#28872;&#24433;&#21709;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#25945;&#25480;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#25945;&#23398;&#27169;&#24335;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#34920;&#31034;&#35821;&#35328;&#19979;&#30340;&#25945;&#23398;&#26377;&#25928;&#24615;&#25552;&#21319;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Greedy&#27169;&#24335;&#33021;&#26356;&#22909;&#22320;&#22788;&#29702;&#20887;&#20313;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.12711</link><description>&lt;p&gt;
&#24403;&#20887;&#20313;&#24615;&#24456;&#37325;&#35201;&#65306;&#34920;&#31034;&#30340;&#26426;&#22120;&#25945;&#23398;
&lt;/p&gt;
&lt;p&gt;
When Redundancy Matters: Machine Teaching of Representations. (arXiv:2401.12711v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12711
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#25945;&#23398;&#20013;&#65292;&#27010;&#24565;&#21487;&#20197;&#26377;&#35768;&#22810;&#31561;&#25928;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#36825;&#31181;&#20887;&#20313;&#24615;&#23545;&#25628;&#32034;&#31354;&#38388;&#26377;&#24378;&#28872;&#24433;&#21709;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#25945;&#25480;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#25945;&#23398;&#27169;&#24335;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#34920;&#31034;&#35821;&#35328;&#19979;&#30340;&#25945;&#23398;&#26377;&#25928;&#24615;&#25552;&#21319;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Greedy&#27169;&#24335;&#33021;&#26356;&#22909;&#22320;&#22788;&#29702;&#20887;&#20313;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#26426;&#22120;&#25945;&#23398;&#20013;&#65292;&#25945;&#24072;&#24819;&#35201;&#36890;&#36807;&#19968;&#20010;&#26377;&#38480;&#30340;&#31034;&#20363;&#38598;&#21512;&#26469;&#21521;&#23398;&#20064;&#32773;&#25945;&#25480;&#19968;&#20010;&#27010;&#24565;&#65292;&#21363;&#35777;&#26126;&#38598;&#12290;&#20294;&#26159;&#27010;&#24565;&#21487;&#20197;&#26377;&#35768;&#22810;&#31561;&#25928;&#30340;&#34920;&#31034;&#26041;&#24335;&#12290;&#36825;&#31181;&#20887;&#20313;&#24615;&#24378;&#28872;&#24433;&#21709;&#20102;&#25628;&#32034;&#31354;&#38388;&#65292;&#20197;&#33267;&#20110;&#25945;&#24072;&#21644;&#23398;&#20064;&#32773;&#21487;&#33021;&#26080;&#27861;&#36731;&#26131;&#30830;&#23450;&#27599;&#20010;&#34920;&#31034;&#30340;&#31561;&#20215;&#31867;&#12290;&#22312;&#36825;&#31181;&#24120;&#35265;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#25945;&#25480;&#34920;&#31034;&#30340;&#24819;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20960;&#31181;&#21033;&#29992;&#34920;&#31034;&#21644;&#35777;&#26126;&#38598;&#22823;&#23567;&#30340;&#25945;&#23398;&#27169;&#24335;&#65288;Eager&#12289;Greedy&#21644;Optimal&#65289;&#65292;&#24182;&#20998;&#26512;&#20102;&#23545;&#26576;&#20123;&#34920;&#31034;&#35821;&#35328;&#65288;DNF&#34920;&#36798;&#24335;&#21644;&#22270;&#28789;&#23436;&#22791;&#30340;P3&#31243;&#24207;&#65289;&#30340;&#25945;&#23398;&#26377;&#25928;&#24615;&#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23384;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#20887;&#20313;&#65292;&#36890;&#36807;&#36825;&#37324;&#24341;&#20837;&#30340;Greedy&#27169;&#24335;&#27604;Eager&#27169;&#24335;&#26356;&#22909;&#22320;&#22788;&#29702;&#65292;&#23613;&#31649;&#20004;&#32773;&#37117;&#21487;&#33021;&#36828;&#31163;&#26368;&#20248;&#12290;&#23545;&#20110;P3&#31243;&#24207;&#65292;&#25105;&#20204;&#21457;&#29616;&#35777;&#26126;&#38598;&#36890;&#24120;&#26159;&#19981;&#21516;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traditional machine teaching, a teacher wants to teach a concept to a learner, by means of a finite set of examples, the witness set. But concepts can have many equivalent representations. This redundancy strongly affects the search space, to the extent that teacher and learner may not be able to easily determine the equivalence class of each representation. In this common situation, instead of teaching concepts, we explore the idea of teaching representations. We work with several teaching schemas that exploit representation and witness size (Eager, Greedy and Optimal) and analyze the gains in teaching effectiveness for some representational languages (DNF expressions and Turing-complete P3 programs). Our theoretical and experimental results indicate that there are various types of redundancy, handled better by the Greedy schema introduced here than by the Eager schema, although both can be arbitrarily far away from the Optimal. For P3 programs we found that witness sets are usuall
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#30340;&#26159;&#35774;&#35745;&#19968;&#31181;&#36873;&#25321;&#26426;&#21046;&#26469;&#24179;&#34913;&#34987;&#25298;&#32477;&#30340;&#39044;&#27979;&#27604;&#20363;&#21644;&#25152;&#36873;&#39044;&#27979;&#30340;&#39044;&#27979;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.12708</link><description>&lt;p&gt;
&#29992;&#20110;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Network Benchmarks for Selective Classification. (arXiv:2401.12708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#30340;&#26159;&#35774;&#35745;&#19968;&#31181;&#36873;&#25321;&#26426;&#21046;&#26469;&#24179;&#34913;&#34987;&#25298;&#32477;&#30340;&#39044;&#27979;&#27604;&#20363;&#21644;&#25152;&#36873;&#39044;&#27979;&#30340;&#39044;&#27979;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#20855;&#26377;&#31038;&#20250;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#30340;&#37096;&#32626;&#22686;&#21152;&#65292;&#23545;&#21487;&#38752;&#21644;&#21487;&#20449;&#39044;&#27979;&#30340;&#38656;&#27714;&#20063;&#26085;&#30410;&#22686;&#38271;&#12290;&#23454;&#29616;&#36825;&#20123;&#35201;&#27714;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20801;&#35768;&#27169;&#22411;&#22312;&#23384;&#22312;&#39640;&#38169;&#35823;&#39118;&#38505;&#26102;&#25918;&#24323;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#38656;&#35201;&#20026;&#27169;&#22411;&#28155;&#21152;&#36873;&#25321;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#36873;&#25321;&#27169;&#22411;&#23558;&#25552;&#20379;&#39044;&#27979;&#30340;&#20363;&#23376;&#12290;&#36873;&#25321;&#24615;&#20998;&#31867;&#26694;&#26550;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#24179;&#34913;&#34987;&#25298;&#32477;&#39044;&#27979;&#27604;&#20363;&#65288;&#21363;&#27169;&#22411;&#19981;&#36827;&#34892;&#39044;&#27979;&#30340;&#20363;&#23376;&#27604;&#20363;&#65289;&#19982;&#22312;&#25152;&#36873;&#39044;&#27979;&#19978;&#30340;&#39044;&#27979;&#24615;&#33021;&#25913;&#36827;&#20043;&#38388;&#30340;&#26426;&#21046;&#12290;&#23384;&#22312;&#22810;&#20010;&#36873;&#25321;&#24615;&#20998;&#31867;&#26694;&#26550;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20381;&#36182;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#23454;&#35777;&#35780;&#20272;&#20173;&#23616;&#38480;&#20110;&#37096;&#20998;&#26041;&#27861;&#21644;&#35774;&#32622;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#32473;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#24456;&#23569;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing deployment of machine learning models in many socially-sensitive tasks, there is a growing demand for reliable and trustworthy predictions. One way to accomplish these requirements is to allow a model to abstain from making a prediction when there is a high risk of making an error. This requires adding a selection mechanism to the model, which selects those examples for which the model will provide a prediction. The selective classification framework aims to design a mechanism that balances the fraction of rejected predictions (i.e., the proportion of examples for which the model does not make a prediction) versus the improvement in predictive performance on the selected predictions. Multiple selective classification frameworks exist, most of which rely on deep neural network architectures. However, the empirical evaluation of the existing approaches is still limited to partial comparisons among methods and settings, providing practitioners with little insight into 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12689</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#20381;&#36182;&#20110;&#26631;&#35760;&#30340;&#12289;&#20551;&#35774;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#24182;&#19981;&#24120;&#35265;&#12290;&#33258;&#21160;&#27169;&#22411;&#35780;&#20272;&#65288;AutoEval&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20256;&#32479;&#24037;&#20316;&#27969;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#25104;&#19968;&#20010;&#25509;&#36817;&#39044;&#27979;&#24615;&#33021;&#30340;&#27979;&#35797;&#31649;&#32447;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#26631;&#31614;&#30340;&#23384;&#22312;&#12290;&#23613;&#31649;AutoEval&#26694;&#26550;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26041;&#24335;&#8212;&#8212;&#20803;&#20998;&#24067;&#33021;&#37327;&#65288;MDE&#65289;&#65292;&#23427;&#21487;&#20197;&#20351;AutoEval&#26694;&#26550;&#26356;&#21152;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;MDE&#30340;&#26680;&#24515;&#26159;&#24314;&#31435;&#19968;&#20010;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#65288;&#33021;&#37327;&#65289;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#33021;&#37327;&#30340;&#23398;&#20064;&#25552;&#20379;&#26356;&#24179;&#28369;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;MDE&#19982;&#20998;&#31867;&#25439;&#22833;&#30456;&#36830;&#25509;&#65292;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#25454;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36827;&#34892;DVL&#26657;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#26102;&#38388;&#19978;&#20998;&#21035;&#27604;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;35%&#21644;80%&#12290;</title><link>http://arxiv.org/abs/2401.12687</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36827;&#34892;DVL&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
DVL Calibration using Data-driven Methods. (arXiv:2401.12687v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36827;&#34892;DVL&#26657;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#26102;&#38388;&#19978;&#20998;&#21035;&#27604;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;35%&#21644;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#27700;&#19979;&#36733;&#20855;(AUVs)&#22312;&#21508;&#31181;&#27700;&#19979;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20174;&#28023;&#24213;&#22320;&#22270;&#32472;&#21046;&#21040;&#24037;&#19994;&#25805;&#20316;&#12290;&#22312;&#27700;&#19979;&#65292;AUV&#30340;&#23548;&#33322;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#20381;&#36182;&#24815;&#24615;&#20256;&#24863;&#22120;&#21644;&#22810;&#26222;&#21202;&#36895;&#24230;&#26085;&#24535;(DVL)&#30340;&#34701;&#21512;&#12290;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;DVL&#27979;&#37327;&#32467;&#26524;&#65292;&#38656;&#35201;&#22312;&#20219;&#21153;&#24320;&#22987;&#21069;&#36827;&#34892;&#26657;&#20934;&#36807;&#31243;&#12290;&#22522;&#20110;&#27169;&#22411;&#30340;&#26657;&#20934;&#26041;&#27861;&#21253;&#25324;&#21033;&#29992;&#20840;&#29699;&#23548;&#33322;&#21355;&#26143;&#31995;&#32479;&#20449;&#21495;&#30340;&#28388;&#27874;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#26657;&#20934;&#36807;&#31243;&#12290;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25152;&#38656;&#26657;&#20934;&#26102;&#38388;&#19978;&#27604;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;35%&#21644;80%&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous underwater vehicles (AUVs) are used in a wide range of underwater applications, ranging from seafloor mapping to industrial operations. While underwater, the AUV navigation solution commonly relies on the fusion between inertial sensors and Doppler velocity logs (DVL). To achieve accurate DVL measurements a calibration procedure should be conducted before the mission begins. Model-based calibration approaches include filtering approaches utilizing global navigation satellite system signals. In this paper, we propose an end-to-end deep-learning framework for the calibration procedure. Using stimulative data, we show that our proposed approach outperforms model-based approaches by 35% in accuracy and 80% in the required calibration time.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22270;&#19978;&#23398;&#20064;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#24418;&#25193;&#23637;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#31232;&#30095;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.12686</link><description>&lt;p&gt;
&#22312;&#31232;&#30095;&#22270;&#19978;&#23398;&#20064;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#65306;&#19968;&#31181;&#28151;&#21512;&#22270;&#24418;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach. (arXiv:2401.12686v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12686
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22270;&#19978;&#23398;&#20064;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#24418;&#25193;&#23637;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#31232;&#30095;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22823;&#35268;&#27169;&#20195;&#29702;&#32676;&#20307;&#30340;&#34892;&#20026;&#26159;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#34429;&#28982;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#22312;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#20195;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#19988;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#12290;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#65288;MFGs&#65289;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25193;&#23637;&#21040;&#21253;&#25324;&#20195;&#29702;&#20043;&#38388;&#30340;&#32593;&#32476;&#32467;&#26500;&#30340;&#22270;&#24418;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#65288;GMFGs&#65289;&#12290;&#23613;&#31649;&#20855;&#26377;&#35832;&#22810;&#20248;&#28857;&#65292;&#20294;GMFGs&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#21463;&#21040;&#22270;&#24418;&#21482;&#33021;&#25429;&#25417;&#23494;&#38598;&#22270;&#30340;&#38480;&#21046;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#23454;&#39564;&#35777;&#26126;&#30340;&#32593;&#32476;&#26174;&#31034;&#20986;&#19968;&#23450;&#31243;&#24230;&#30340;&#31232;&#30095;&#24615;&#65292;&#20363;&#22914;&#24130;&#24459;&#22270;&#65292;&#22240;&#27492;GMFG&#26694;&#26550;&#26080;&#27861;&#25429;&#25417;&#36825;&#20123;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#23545;&#23616;&#21338;&#24328;&#65288;GXMFGs&#65289;&#30340;&#27010;&#24565;&#65292;&#23427;&#24314;&#31435;&#22312;&#22270;&#35770;&#27010;&#24565;&#22270;&#24418;&#25193;&#23637;&#65288;graphexes&#65289;&#22522;&#30784;&#19978;&#12290;&#22270;&#24418;&#25193;&#23637;&#26159;&#31232;&#30095;&#22270;&#24207;&#21015;&#30340;&#26497;&#38480;&#23545;&#35937;&#65292;&#36824;&#20855;&#26377;&#20854;&#20182;&#19968;&#20123;&#29702;&#24819;&#29305;&#24615;&#65292;&#22914;sma
&lt;/p&gt;
&lt;p&gt;
Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the sma
&lt;/p&gt;</description></item><item><title>LLpowershap&#26159;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#25439;&#22833;&#30340;&#33258;&#21160;Shapley&#20540;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36873;&#25321;&#30340;&#29305;&#24449;&#38598;&#21512;&#20013;&#35782;&#21035;&#20986;&#20855;&#26377;&#26368;&#23567;&#22122;&#22768;&#30340;&#20449;&#24687;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.12683</link><description>&lt;p&gt;
LLpowershap: &#22522;&#20110;&#36923;&#36753;&#25439;&#22833;&#30340;&#33258;&#21160;Shapley&#20540;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLpowershap: Logistic Loss-based Automated Shapley Values Feature Selection Method. (arXiv:2401.12683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12683
&lt;/p&gt;
&lt;p&gt;
LLpowershap&#26159;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#25439;&#22833;&#30340;&#33258;&#21160;Shapley&#20540;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36873;&#25321;&#30340;&#29305;&#24449;&#38598;&#21512;&#20013;&#35782;&#21035;&#20986;&#20855;&#26377;&#26368;&#23567;&#22122;&#22768;&#30340;&#20449;&#24687;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley&#20540;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#19981;&#20165;&#21487;&#20197;&#35299;&#37322;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#27169;&#22411;&#35843;&#35797;&#12289;&#25935;&#24863;&#24615;&#21644;&#20844;&#24179;&#24615;&#20998;&#26512;&#65292;&#20197;&#21450;&#36873;&#25321;&#37325;&#35201;&#29305;&#24449;&#36827;&#34892;&#31283;&#20581;&#24314;&#27169;&#21644;&#36827;&#19968;&#27493;&#20998;&#26512;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#21033;&#29992;Shapley&#20540;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;LLpowershap&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#25439;&#22833;&#30340;Shapley&#20540;&#22312;&#36873;&#25321;&#30340;&#29305;&#24449;&#38598;&#21512;&#20013;&#35782;&#21035;&#20986;&#20855;&#26377;&#26368;&#23567;&#22122;&#22768;&#30340;&#20449;&#24687;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;LLpowershap&#19981;&#20165;&#35782;&#21035;&#20986;&#26356;&#22810;&#30340;&#20449;&#24687;&#29305;&#24449;&#65292;&#32780;&#19988;&#36755;&#20986;&#30340;&#22122;&#22768;&#29305;&#24449;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shapley values have been used extensively in machine learning, not only to explain black box machine learning models, but among other tasks, also to conduct model debugging, sensitivity and fairness analyses and to select important features for robust modelling and for further follow-up analyses. Shapley values satisfy certain axioms that promote fairness in distributing contributions of features toward prediction or reducing error, after accounting for non-linear relationships and interactions when complex machine learning models are employed. Recently, a number of feature selection methods utilising Shapley values have been introduced. Here, we present a novel feature selection method, LLpowershap, which makes use of loss-based Shapley values to identify informative features with minimal noise among the selected sets of features. Our simulation results show that LLpowershap not only identifies higher number of informative features but outputs fewer noise features compared to other st
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#24615;&#20856;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;Kriging&#36807;&#31243;&#20013;&#37051;&#23621;&#21644;&#38750;&#37051;&#23621;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#23646;&#24615;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12681</link><description>&lt;p&gt;
Kriging &#30340;&#25193;&#23637;:&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#24615;&#20856;&#22411;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Non-Neighbors Also Matter to Kriging: A New Contrastive-Prototypical Learning. (arXiv:2401.12681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12681
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#24615;&#20856;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;Kriging&#36807;&#31243;&#20013;&#37051;&#23621;&#21644;&#38750;&#37051;&#23621;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#23646;&#24615;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kriging&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#31354;&#38388;&#37051;&#36817;&#25110;&#29289;&#29702;&#36830;&#25509;&#20013;&#30340;&#35266;&#27979;&#20540;&#26469;&#20272;&#35745;&#26410;&#37319;&#26679;&#22320;&#29702;&#20301;&#32622;&#30340;&#23646;&#24615;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#20943;&#36731;&#30001;&#20110;&#37096;&#32626;&#19981;&#36275;&#30340;&#20256;&#24863;&#22120;&#24341;&#36215;&#30340;&#30417;&#27979;&#20559;&#24046;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20551;&#35774;&#37051;&#23621;&#30340;&#20449;&#24687;&#20026;&#20272;&#35745;&#26410;&#35266;&#23519;&#21040;&#30340;&#30446;&#26631;&#30340;&#23646;&#24615;&#25552;&#20379;&#22522;&#30784;&#65292;&#32780;&#24573;&#30053;&#20102;&#38750;&#37051;&#23621;&#12290;&#28982;&#32780;&#65292;&#38750;&#37051;&#23621;&#20063;&#21487;&#20197;&#25552;&#20379;&#26377;&#30410;&#30340;&#20449;&#24687;&#65292;&#32780;&#37051;&#23621;&#20063;&#21487;&#33021;&#26159;&#35823;&#23548;&#24615;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23545;&#27604;&#24615;&#20856;&#22411;&#8221;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#37051;&#23621;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#24182;&#22238;&#25910;&#38750;&#37051;&#23621;&#30340;&#20449;&#24687;&#12290;&#20316;&#20026;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#25105;&#20204;&#20174;&#34920;&#31034;&#30340;&#26032;&#35270;&#35282;&#36827;&#34892;Kriging&#20219;&#21153;&#65306;&#25105;&#20204;&#39318;&#20808;&#23398;&#20064;&#24378;&#22823;&#32780;&#36890;&#29992;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#20174;&#34920;&#31034;&#20013;&#24674;&#22797;&#23646;&#24615;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#37051;&#23621;&#23545;&#27604;&#27169;&#22359;&#65292;&#36890;&#36807;&#32553;&#23567;&#30446;&#26631;&#19982;&#37051;&#23621;&#30340;&#34920;&#31034;&#36317;&#31163;&#65292;&#31895;&#30053;&#22320;&#23398;&#20064;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kriging aims at estimating the attributes of unsampled geo-locations from observations in the spatial vicinity or physical connections, which helps mitigate skewed monitoring caused by under-deployed sensors. Existing works assume that neighbors' information offers the basis for estimating the attributes of the unobserved target while ignoring non-neighbors. However, non-neighbors could also offer constructive information, and neighbors could also be misleading. To this end, we propose ``Contrastive-Prototypical'' self-supervised learning for Kriging (KCP) to refine valuable information from neighbors and recycle the one from non-neighbors. As a pre-trained paradigm, we conduct the Kriging task from a new perspective of representation: we aim to first learn robust and general representations and then recover attributes from representations. A neighboring contrastive module is designed that coarsely learns the representations by narrowing the representation distance between the target a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#22522;&#22240;&#34920;&#36798;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#31283;&#20581;&#21152;&#26435;&#20998;&#25968;&#65288;ROWSU&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20013;&#39640;&#24230;&#20542;&#26012;&#30340;&#31867;&#21035;&#20998;&#24067;&#23545;&#20998;&#31867;&#31639;&#27861;&#24615;&#33021;&#30340;&#19981;&#21033;&#24433;&#21709;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12667</link><description>&lt;p&gt;
&#20351;&#29992;&#31283;&#20581;&#21152;&#26435;&#20998;&#25968;&#36827;&#34892;&#39640;&#32500;&#20108;&#20998;&#31867;&#19981;&#24179;&#34913;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Feature Selection via Robust Weighted Score for High Dimensional Binary Class-Imbalanced Gene Expression Data. (arXiv:2401.12667v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#22522;&#22240;&#34920;&#36798;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#31283;&#20581;&#21152;&#26435;&#20998;&#25968;&#65288;ROWSU&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20013;&#39640;&#24230;&#20542;&#26012;&#30340;&#31867;&#21035;&#20998;&#24067;&#23545;&#20998;&#31867;&#31639;&#27861;&#24615;&#33021;&#30340;&#19981;&#21033;&#24433;&#21709;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#22522;&#22240;&#34920;&#36798;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#31283;&#20581;&#21152;&#26435;&#20998;&#25968;&#65288;ROWSU&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#26368;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20013;&#39640;&#24230;&#20542;&#26012;&#30340;&#31867;&#21035;&#20998;&#24067;&#23545;&#20998;&#31867;&#31639;&#27861;&#24615;&#33021;&#30340;&#19981;&#21033;&#24433;&#21709;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20174;&#23569;&#25968;&#31867;&#21035;&#35266;&#27979;&#25968;&#25454;&#20013;&#21512;&#25104;&#29983;&#25104;&#25968;&#25454;&#28857;&#26469;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#37319;&#29992;&#36138;&#23146;&#25628;&#32034;&#26041;&#27861;&#36873;&#25321;&#26368;&#23567;&#30340;&#22522;&#22240;&#23376;&#38598;&#12290;&#28982;&#21518;&#65292;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#21152;&#26435;&#31283;&#20581;&#20998;&#25968;&#65292;&#20854;&#20013;&#26435;&#37325;&#30001;&#25903;&#25345;&#21521;&#37327;&#35745;&#31639;&#65292;&#20197;&#33719;&#24471;&#19968;&#32452;&#31934;&#28860;&#30340;&#22522;&#22240;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#24471;&#21040;&#30340;&#26368;&#39640;&#20998;&#25968;&#22522;&#22240;&#19982;&#36138;&#23146;&#25628;&#32034;&#26041;&#27861;&#36873;&#25321;&#30340;&#26368;&#23567;&#22522;&#22240;&#23376;&#38598;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#26368;&#32456;&#30340;&#22522;&#22240;&#38598;&#21512;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#30830;&#20445;&#22312;&#23384;&#22312;&#20559;&#26012;&#31867;&#21035;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#26368;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#22522;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a robust weighted score for unbalanced data (ROWSU) is proposed for selecting the most discriminative feature for high dimensional gene expression binary classification with class-imbalance problem. The method addresses one of the most challenging problems of highly skewed class distributions in gene expression datasets that adversely affect the performance of classification algorithms. First, the training dataset is balanced by synthetically generating data points from minority class observations. Second, a minimum subset of genes is selected using a greedy search approach. Third, a novel weighted robust score, where the weights are computed by support vectors, is introduced to obtain a refined set of genes. The highest-scoring genes based on this approach are combined with the minimum subset of genes selected by the greedy search approach to form the final set of genes. The novel method ensures the selection of the most discriminative genes, even in the presence of ske
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#21644;&#24341;&#20837;&#26032;&#30340;&#25910;&#30410;&#20989;&#25968;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12662</link><description>&lt;p&gt;
&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#34701;&#20837;&#36830;&#32493;&#31354;&#38388;: &#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#20559;&#22909;&#39044;&#26399;&#25913;&#21892;&#30340;&#20132;&#20114;&#24335;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Integrating Human Expertise in Continuous Spaces: A Novel Interactive Bayesian Optimization Framework with Preference Expected Improvement. (arXiv:2401.12662v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12662
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#21644;&#24341;&#20837;&#26032;&#30340;&#25910;&#30410;&#20989;&#25968;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;IML&#65289;&#26088;&#22312;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#19982;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#19981;&#33021;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#29366;&#24577;&#31354;&#38388;&#21644;/&#25110;&#34892;&#20026;&#31354;&#38388;&#20165;&#38480;&#20110;&#31163;&#25955;&#20540;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#20043;&#38388;&#30340;&#20132;&#20114;&#21463;&#21040;&#22312;&#22810;&#20010;&#24314;&#35758;&#20043;&#38388;&#20570;&#20986;&#20915;&#31574;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#20132;&#20114;&#24335;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;IBO&#65289;&#23454;&#29616;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;&#35813;&#26694;&#26550;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#25552;&#20379;&#30028;&#38754;&#32473;&#29992;&#25143;&#25163;&#21160;&#35843;&#25972;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21152;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#30410;&#20989;&#25968;&#65292;&#20559;&#22909;&#39044;&#26399;&#25913;&#21892;&#65288;PEI&#65289;&#65292;&#36890;&#36807;&#29992;&#25143;&#20559;&#22909;&#30340;&#27010;&#29575;&#27169;&#22411;&#26469;&#25552;&#39640;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#30830;&#20445;&#26426;&#22120;&#33021;&#20174;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#20013;&#21463;&#30410;&#65292;&#20197;&#23454;&#29616;&#26356;&#21152;&#21327;&#35843;&#21644;&#26377;&#25928;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive Machine Learning (IML) seeks to integrate human expertise into machine learning processes. However, most existing algorithms cannot be applied to Realworld Scenarios because their state spaces and/or action spaces are limited to discrete values. Furthermore, the interaction of all existing methods is restricted to deciding between multiple proposals. We therefore propose a novel framework based on Bayesian Optimization (BO). Interactive Bayesian Optimization (IBO) enables collaboration between machine learning algorithms and humans. This framework captures user preferences and provides an interface for users to shape the strategy by hand. Additionally, we've incorporated a new acquisition function, Preference Expected Improvement (PEI), to refine the system's efficiency using a probabilistic model of the user preferences. Our approach is geared towards ensuring that machines can benefit from human expertise, aiming for a more aligned and effective learning process. In the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#22686;&#24378;&#30340;&#28145;&#24230;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65288;CCEC&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#36830;&#25509;&#22359;&#24182;&#20837;&#29305;&#24449;&#34920;&#31034;&#20013;&#65292;&#20197;&#20445;&#25345;&#22810;&#20010;&#35270;&#22270;&#38388;&#30340;&#19968;&#33268;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35889;&#32858;&#31867;&#25913;&#21892;&#32858;&#31867;&#30340;&#34920;&#31034;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12648</link><description>&lt;p&gt;
&#22522;&#20110;&#19968;&#33268;&#24615;&#22686;&#24378;&#30340;&#28145;&#24230;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Consistency Enhancement-Based Deep Multiview Clustering via Contrastive Learning. (arXiv:2401.12648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#22686;&#24378;&#30340;&#28145;&#24230;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65288;CCEC&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#36830;&#25509;&#22359;&#24182;&#20837;&#29305;&#24449;&#34920;&#31034;&#20013;&#65292;&#20197;&#20445;&#25345;&#22810;&#20010;&#35270;&#22270;&#38388;&#30340;&#19968;&#33268;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35889;&#32858;&#31867;&#25913;&#21892;&#32858;&#31867;&#30340;&#34920;&#31034;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#32858;&#31867;&#65288;MVC&#65289;&#36890;&#36807;&#32508;&#21512;&#22810;&#20010;&#35270;&#22270;&#30340;&#20449;&#24687;&#65292;&#23558;&#25968;&#25454;&#26679;&#26412;&#20998;&#20026;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;&#32780;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;MVC&#22330;&#26223;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#27867;&#21270;&#29305;&#24449;&#34920;&#31034;&#24182;&#20445;&#25345;&#19968;&#33268;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#29616;&#26377;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#22312;&#32858;&#31867;&#36807;&#31243;&#20013;&#24573;&#30053;&#20102;&#32858;&#31867;&#34920;&#31034;&#30340;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;&#19968;&#33268;&#22686;&#24378;&#22411;&#28145;&#24230;MVC&#26041;&#27861;&#65288;CCEC&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23558;&#35821;&#20041;&#36830;&#25509;&#22359;&#24182;&#20837;&#29305;&#24449;&#34920;&#31034;&#20013;&#65292;&#20197;&#20445;&#25345;&#22810;&#20010;&#35270;&#22270;&#38388;&#30340;&#19968;&#33268;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35889;&#32858;&#31867;&#25913;&#21892;&#32858;&#31867;&#30340;&#34920;&#31034;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#20102;&#22810;&#20010;&#35270;&#22270;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiview clustering (MVC) segregates data samples into meaningful clusters by synthesizing information across multiple views. Moreover, deep learning-based methods have demonstrated their strong feature learning capabilities in MVC scenarios. However, effectively generalizing feature representations while maintaining consistency is still an intractable problem. In addition, most existing deep clustering methods based on contrastive learning overlook the consistency of the clustering representations during the clustering process. In this paper, we show how the above problems can be overcome and propose a consistent enhancement-based deep MVC method via contrastive learning (CCEC). Specifically, semantic connection blocks are incorporated into a feature representation to preserve the consistent information among multiple views. Furthermore, the representation process for clustering is enhanced through spectral clustering, and the consistency across multiple views is improved. Experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#31526;&#21495;&#26816;&#27979;&#22120;&#22312;&#23545;&#21464;&#21160;&#26465;&#20214;&#21644;&#19981;&#23436;&#20840;&#20449;&#36947;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#31639;&#27861;&#22312;&#23398;&#20064;&#22122;&#22768;&#20449;&#36947;&#25968;&#25454;&#21644;&#19981;&#23436;&#20840;&#20449;&#36947;&#34928;&#20943;&#29305;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#65292;&#20294;&#20854;&#25345;&#32493;&#24615;&#21644;&#36866;&#29992;&#24615;&#26377;&#24453;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.12645</link><description>&lt;p&gt;
&#23545;&#20110;&#21464;&#21160;&#26465;&#20214;&#21644;&#19981;&#23436;&#20840;&#20449;&#36947;&#30693;&#35782;&#30340;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#31526;&#21495;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Deep Learning-aided Symbol Detectors to Varying Conditions and Imperfect Channel Knowledge. (arXiv:2401.12645v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#31526;&#21495;&#26816;&#27979;&#22120;&#22312;&#23545;&#21464;&#21160;&#26465;&#20214;&#21644;&#19981;&#23436;&#20840;&#20449;&#36947;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#31639;&#27861;&#22312;&#23398;&#20064;&#22122;&#22768;&#20449;&#36947;&#25968;&#25454;&#21644;&#19981;&#23436;&#20840;&#20449;&#36947;&#34928;&#20943;&#29305;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#65292;&#20294;&#20854;&#25345;&#32493;&#24615;&#21644;&#36866;&#29992;&#24615;&#26377;&#24453;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#20855;&#26377;&#31526;&#21495;&#38388;&#24178;&#25200;&#30340;&#20449;&#36947;&#30340;&#25968;&#25454;&#39537;&#21160;&#22411;Bahl-Cocke-Jelinek-Raviv&#65288;BCJR&#65289;&#31639;&#27861;&#12290;&#36825;&#31181;&#34987;&#31216;&#20026;BCJRNet&#31639;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#20449;&#36947;&#30340;&#20284;&#28982;&#27010;&#29575;&#12290;BCJRNet&#22312;&#24212;&#29992;&#20110;&#20855;&#26377;&#29702;&#24819;&#25351;&#25968;&#34928;&#20943;&#29305;&#24615;&#30340;&#23450;&#24577;&#20449;&#36947;&#26102;&#34920;&#29616;&#20986;&#23545;&#20449;&#36947;&#34928;&#20943;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23454;&#38469;&#20013;&#24120;&#35265;&#30340;&#26102;&#21464;&#20449;&#36947;&#20197;&#21450;&#25509;&#25910;&#26426;&#21482;&#33021;&#33719;&#21462;&#21040;&#38169;&#35823;&#30340;&#20449;&#36947;&#21442;&#25968;&#30340;&#24773;&#20917;&#65292;BCJRNet&#30340;&#27867;&#21270;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#32467;&#26524;&#36827;&#34892;&#25193;&#23637;&#65292;&#28085;&#30422;&#20102;&#23454;&#38469;&#20256;&#36755;&#20013;&#20986;&#29616;&#30340;&#21508;&#31181;&#19981;&#23436;&#20840;&#20449;&#36947;&#30693;&#35782;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23398;&#20064;&#22122;&#22768;&#20449;&#36947;&#25968;&#25454;&#21644;&#19981;&#23436;&#20840;&#20449;&#36947;&#34928;&#20943;&#29305;&#24615;&#65292;&#22312;&#23450;&#24577;&#20256;&#36755;&#22330;&#26223;&#19979;&#65292;BCJRNet&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;BCJR&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20248;&#21183;&#26159;&#21542;&#21487;&#25345;&#32493;&#20197;&#21450;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a data-driven Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm tailored to channels with intersymbol interference has been introduced. This so-called BCJRNet algorithm utilizes neural networks to calculate channel likelihoods. BCJRNet has demonstrated resilience against inaccurate channel tap estimations when applied to a time-invariant channel with ideal exponential decay profiles. However, its generalization capabilities for practically-relevant time-varying channels, where the receiver can only access incorrect channel parameters, remain largely unexplored. The primary contribution of this paper is to expand upon the results from existing literature to encompass a variety of imperfect channel knowledge cases that appear in real-world transmissions. Our findings demonstrate that BCJRNet significantly outperforms the conventional BCJR algorithm for stationary transmission scenarios when learning from noisy channel data and with imperfect channel decay profiles. However, this advant
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#23631;&#34109;&#26041;&#27861;&#26469;&#28040;&#38500;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#20174;&#25968;&#25454;&#38598;&#20013;&#31227;&#38500;&#23427;&#20204;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#32508;&#21512;&#32771;&#34385;&#29305;&#24449;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#20026;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.12644</link><description>&lt;p&gt;
&#20108;&#36827;&#21046;&#29305;&#24449;&#23631;&#34109;&#20248;&#21270;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Binary Feature Mask Optimization for Feature Selection. (arXiv:2401.12644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#23631;&#34109;&#26041;&#27861;&#26469;&#28040;&#38500;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#20174;&#25968;&#25454;&#38598;&#20013;&#31227;&#38500;&#23427;&#20204;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#32508;&#21512;&#32771;&#34385;&#29305;&#24449;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#20026;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#36873;&#25321;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#23631;&#34109;&#26041;&#27861;&#65292;&#22312;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#20013;&#28040;&#38500;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#20174;&#25968;&#25454;&#38598;&#20013;&#23436;&#20840;&#31227;&#38500;&#23427;&#20204;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#20013;&#20351;&#29992;&#30456;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20687;&#20854;&#20182;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#37027;&#26679;&#38656;&#35201;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#30340;&#32500;&#24230;&#19981;&#21516;&#12290;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#33719;&#21462;&#23631;&#34109;&#25805;&#20316;&#31526;&#65292;&#36825;&#20026;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#25552;&#20379;&#20102;&#23545;&#29305;&#24449;&#23376;&#38598;&#30340;&#20840;&#38754;&#35266;&#23519;&#12290;&#29305;&#24449;&#36873;&#25321;&#25991;&#29486;&#20013;&#23384;&#22312;&#21508;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#30740;&#31350;&#24341;&#20837;&#19968;&#20010;&#38024;&#23545;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#20197;&#25972;&#20307;&#32771;&#34385;&#29305;&#24449;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#19981;&#26159;&#21482;&#20851;&#27880;&#21333;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate feature selection problem for generic machine learning (ML) models. We introduce a novel framework that selects features considering the predictions of the model. Our framework innovates by using a novel feature masking approach to eliminate the features during the selection process, instead of completely removing them from the dataset. This allows us to use the same ML model during feature selection, unlike other feature selection methods where we need to train the ML model again as the dataset has different dimensions on each iteration. We obtain the mask operator using the predictions of the ML model, which offers a comprehensive view on the subsets of the features essential for the predictive performance of the model. A variety of approaches exist in the feature selection literature. However, no study has introduced a training-free framework for a generic ML model to select features while considering the importance of the feature subsets as a whole, instead of focusi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#24212;&#20102;Makelov&#31561;&#20154;(2023)&#30340;&#35770;&#25991;&#65292;&#35813;&#35770;&#25991;&#35780;&#36848;&#20102;&#23376;&#31354;&#38388;&#20132;&#25442;&#24178;&#39044;&#26041;&#27861;&#30340;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#38382;&#39064;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#25152;&#35859;&#30340;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#21487;&#20197;&#21253;&#25324;&#30452;&#35266;&#21644;&#21487;&#21462;&#30340;&#35299;&#37322;&#65292;&#32780;Makelov&#31561;&#20154;(2023)&#21457;&#29616;&#30340;"&#38169;&#35273;"&#26159;&#20182;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#33539;&#20363;&#30340;&#20135;&#29289;&#12290;&#23613;&#31649;&#25105;&#20204;&#19981;&#21516;&#24847;&#20182;&#20204;&#30340;&#26680;&#24515;&#34920;&#36848;&#65292;&#20294;&#20182;&#20204;&#30340;&#20363;&#23376;&#21644;&#35752;&#35770;&#25512;&#21160;&#20102;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.12631</link><description>&lt;p&gt;
&#23545;Makelov&#31561;&#20154;(2023)&#30340;&#12298;&#21487;&#35299;&#37322;&#24615;&#38169;&#35273;&#12299;&#35770;&#28857;&#30340;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
A Reply to Makelov et al. (2023)'s "Interpretability Illusion" Arguments. (arXiv:2401.12631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#24212;&#20102;Makelov&#31561;&#20154;(2023)&#30340;&#35770;&#25991;&#65292;&#35813;&#35770;&#25991;&#35780;&#36848;&#20102;&#23376;&#31354;&#38388;&#20132;&#25442;&#24178;&#39044;&#26041;&#27861;&#30340;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#38382;&#39064;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#25152;&#35859;&#30340;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#21487;&#20197;&#21253;&#25324;&#30452;&#35266;&#21644;&#21487;&#21462;&#30340;&#35299;&#37322;&#65292;&#32780;Makelov&#31561;&#20154;(2023)&#21457;&#29616;&#30340;"&#38169;&#35273;"&#26159;&#20182;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#33539;&#20363;&#30340;&#20135;&#29289;&#12290;&#23613;&#31649;&#25105;&#20204;&#19981;&#21516;&#24847;&#20182;&#20204;&#30340;&#26680;&#24515;&#34920;&#36848;&#65292;&#20294;&#20182;&#20204;&#30340;&#20363;&#23376;&#21644;&#35752;&#35770;&#25512;&#21160;&#20102;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22238;&#24212;&#20102;Makelov&#31561;&#20154;(2023)&#30340;&#26368;&#26032;&#35770;&#25991;&#65292;&#35813;&#35770;&#25991;&#35780;&#36848;&#20102;&#35832;&#22914;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;(DAS; Geiger&#31561;&#20154;&#65292;2023)&#36825;&#26679;&#30340;&#23376;&#31354;&#38388;&#20132;&#25442;&#24178;&#39044;&#26041;&#27861;&#65292;&#24182;&#22768;&#31216;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#24341;&#36215;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;Makelov&#31561;&#20154;(2023)&#23545;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#30340;&#25216;&#26415;&#27010;&#24565;&#65292;&#28982;&#21518;&#23637;&#31034;&#20102;&#21363;&#20351;&#30452;&#35266;&#21644;&#21487;&#21462;&#30340;&#35299;&#37322;&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#20063;&#21487;&#33021;&#25104;&#20026;&#38169;&#35273;&#12290;&#22240;&#27492;&#65292;&#20182;&#20204;&#21457;&#29616;"&#38169;&#35273;"&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#25298;&#32477;&#20182;&#20204;&#35748;&#20026;"&#38750;&#38169;&#35273;"&#30340;&#35299;&#37322;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#35748;&#20026;Makelov&#31561;&#20154;(2023)&#22312;&#23454;&#36341;&#20013;&#30475;&#21040;&#30340;"&#38169;&#35273;"&#26159;&#20182;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#33539;&#20363;&#30340;&#20135;&#29289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#65292;&#23613;&#31649;&#25105;&#20204;&#19981;&#21516;&#24847;&#20182;&#20204;&#30340;&#26680;&#24515;&#34920;&#36848;&#65292;&#20294;Makelov&#31561;&#20154;(2023)&#30340;&#20363;&#23376;&#21644;&#35752;&#35770;&#26080;&#30097;&#25512;&#21160;&#20102;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We respond to the recent paper by Makelov et al. (2023), which reviews subspace interchange intervention methods like distributed alignment search (DAS; Geiger et al. 2023) and claims that these methods potentially cause "interpretability illusions". We first review Makelov et al. (2023)'s technical notion of what an "interpretability illusion" is, and then we show that even intuitive and desirable explanations can qualify as illusions in this sense. As a result, their method of discovering "illusions" can reject explanations they consider "non-illusory". We then argue that the illusions Makelov et al. (2023) see in practice are artifacts of their training and evaluation paradigms. We close by emphasizing that, though we disagree with their core characterization, Makelov et al. (2023)'s examples and discussion have undoubtedly pushed the field of interpretability forward.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;CAM&#21482;&#26377;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#20840;&#26632;&#20248;&#21270;&#65292;&#36890;&#36807;&#31639;&#27861;&#20248;&#21270;&#21644;&#20851;&#32852;&#22788;&#29702;&#22120;&#30340;&#35774;&#35745;&#65292;&#20197;&#21450;&#20351;&#29992;&#36187;&#36710;&#30913;&#35760;&#24518;&#26469;&#23454;&#29616;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#33021;&#37327;&#28040;&#32791;&#21644;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12630</link><description>&lt;p&gt;
CAM&#21482;&#26377;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#20840;&#26632;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Full-Stack Optimization for CAM-Only DNN Inference. (arXiv:2401.12630v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;CAM&#21482;&#26377;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#20840;&#26632;&#20248;&#21270;&#65292;&#36890;&#36807;&#31639;&#27861;&#20248;&#21270;&#21644;&#20851;&#32852;&#22788;&#29702;&#22120;&#30340;&#35774;&#35745;&#65292;&#20197;&#21450;&#20351;&#29992;&#36187;&#36710;&#30913;&#35760;&#24518;&#26469;&#23454;&#29616;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#33021;&#37327;&#28040;&#32791;&#21644;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#31934;&#24230;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26085;&#30410;&#22686;&#38271;&#30340;&#22797;&#26434;&#24615;&#23548;&#33268;&#20911;&#183;&#35834;&#20234;&#26364;&#31995;&#32479;&#22312;&#33021;&#28304;&#38656;&#27714;&#21644;&#24310;&#36831;&#26041;&#38754;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#12290;&#26368;&#36817;&#26377;&#20960;&#20010;&#35745;&#31639;&#20869;&#23384;&#65288;CIM&#65289;&#31995;&#32479;&#34987;&#25552;&#20986;&#26469;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#12289;&#30828;&#20214;&#21487;&#38752;&#24615;&#21644;&#22823;&#22411;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#30528;&#26435;&#34913;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#19968;&#20123;CIM&#35774;&#35745;&#65292;&#28608;&#27963;&#30340;&#31227;&#21160;&#20173;&#28982;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#26102;&#38388;&#21644;&#33021;&#37327;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#20803;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#21644;&#20851;&#32852;&#22788;&#29702;&#22120;&#65288;AP&#65289;&#30340;&#31639;&#27861;&#20248;&#21270;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#20851;&#32852;&#22788;&#29702;&#22120;&#20351;&#29992;&#20102;&#36187;&#36710;&#30913;&#35760;&#24518;&#65288;RTM&#65289;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#35793;&#27969;&#31243;&#65292;&#36890;&#36807;&#20943;&#23569;AP&#20013;&#30340;&#31639;&#26415;&#24378;&#24230;&#26469;&#20248;&#21270;&#21367;&#31215;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;RTM&#30340;AP&#30340;&#20248;&#21183;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22788;&#29702;&#20934;&#30830;&#24615;&#12289;&#33021;&#28304;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#38382;&#39064;&#30340;&#21516;&#26102;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#20869;&#23384;&#20013;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accuracy of neural networks has greatly improved across various domains over the past years. Their ever-increasing complexity, however, leads to prohibitively high energy demands and latency in von Neumann systems. Several computing-in-memory (CIM) systems have recently been proposed to overcome this, but trade-offs involving accuracy, hardware reliability, and scalability for large models remain a challenge. Additionally, for some CIM designs, the activation movement still requires considerable time and energy. This paper explores the combination of algorithmic optimizations for ternary weight neural networks and associative processors (APs) implemented using racetrack memory (RTM). We propose a novel compilation flow to optimize convolutions on APs by reducing their arithmetic intensity. By leveraging the benefits of RTM-based APs, this approach substantially reduces data transfers within the memory while addressing accuracy, energy efficiency, and reliability concerns. Concretel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#21464;&#32447;&#24615;&#24178;&#25200;&#20449;&#36947;&#19978;&#22522;&#20110;&#22240;&#23376;&#22270;&#30340;&#30450;&#20449;&#36947;&#20272;&#35745;&#21644;&#32852;&#21512;&#31526;&#21495;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#30456;&#20114;&#20132;&#32455;&#30340;&#36845;&#20195;&#65292;&#21487;&#20197;&#38477;&#20302;&#22797;&#26434;&#24230;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#31639;&#27861;&#22312;&#31163;&#32447;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.12627</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#22240;&#23376;&#22270;&#30340;&#30450;&#20449;&#36947;&#20272;&#35745;&#21644;&#32852;&#21512;&#31526;&#21495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Blind Channel Estimation and Joint Symbol Detection with Data-Driven Factor Graphs. (arXiv:2401.12627v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#21464;&#32447;&#24615;&#24178;&#25200;&#20449;&#36947;&#19978;&#22522;&#20110;&#22240;&#23376;&#22270;&#30340;&#30450;&#20449;&#36947;&#20272;&#35745;&#21644;&#32852;&#21512;&#31526;&#21495;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#30456;&#20114;&#20132;&#32455;&#30340;&#36845;&#20195;&#65292;&#21487;&#20197;&#38477;&#20302;&#22797;&#26434;&#24230;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#31639;&#27861;&#22312;&#31163;&#32447;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26102;&#21464;&#32447;&#24615;&#24178;&#25200;&#20449;&#36947;&#19978;&#30450;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#21644;&#31526;&#21495;&#26816;&#27979;&#30340;&#22240;&#23376;&#22270;&#26694;&#26550;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#24120;&#30001;&#20110;&#38656;&#35201;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35745;&#31639;&#36880;&#31526;&#21495;&#21518;&#39564;&#20998;&#24067;&#32780;&#23548;&#33268;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#36866;&#24403;&#30340;&#22240;&#23376;&#22270;&#19978;&#20351;&#29992;&#32622;&#20449;&#20256;&#25773;&#65288;BP&#65289;&#31639;&#27861;&#26469;&#26377;&#25928;&#22320;&#36924;&#36817;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#20132;&#32455;BP&#21644;EM&#30340;&#36845;&#20195;&#65292;&#26816;&#27979;&#22797;&#26434;&#24230;&#36827;&#19968;&#27493;&#20943;&#23569;&#21040;&#27599;&#20010;EM&#27493;&#39588;&#21482;&#38656;&#35201;&#19968;&#27425;BP&#36845;&#20195;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#25968;&#25454;&#39537;&#21160;&#29256;&#26412;&#65292;&#23427;&#24341;&#20837;&#20102;BP&#26356;&#26032;&#30340;&#21160;&#37327;&#65292;&#24182;&#23398;&#20064;&#20102;&#36866;&#24403;&#30340;EM&#21442;&#25968;&#26356;&#26032;&#35745;&#21010;&#65292;&#20174;&#32780;&#22312;&#20165;&#26377;&#23569;&#37327;&#31163;&#32447;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25913;&#21892;&#20102;&#24615;&#33021;-&#22797;&#26434;&#24230;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the application of the factor graph framework for blind joint channel estimation and symbol detection on time-variant linear inter-symbol interference channels. In particular, we consider the expectation maximization (EM) algorithm for maximum likelihood estimation, which typically suffers from high complexity as it requires the computation of the symbol-wise posterior distributions in every iteration. We address this issue by efficiently approximating the posteriors using the belief propagation (BP) algorithm on a suitable factor graph. By interweaving the iterations of BP and EM, the detection complexity can be further reduced to a single BP iteration per EM step. In addition, we propose a data-driven version of our algorithm that introduces momentum in the BP updates and learns a suitable EM parameter update schedule, thereby significantly improving the performance-complexity tradeoff with a few offline training samples. Our numerical experiments demonstrate the excel
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23558;&#35821;&#35328;&#23548;&#21521;&#30340;&#35821;&#20041;&#36890;&#20449;&#19982;&#26032;&#20852;&#36890;&#20449;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#26234;&#33021;&#20307;&#36828;&#31243;&#25511;&#21046;&#30340;&#26032;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#34892;&#31243;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.12624</link><description>&lt;p&gt;
&#38754;&#21521;&#22810;&#26234;&#33021;&#20307;&#36828;&#31243;&#25511;&#21046;&#30340;&#22522;&#20110;&#35821;&#35328;&#21040;&#26032;&#20852;&#36890;&#20449;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control. (arXiv:2401.12624v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12624
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23558;&#35821;&#35328;&#23548;&#21521;&#30340;&#35821;&#20041;&#36890;&#20449;&#19982;&#26032;&#20852;&#36890;&#20449;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#26234;&#33021;&#20307;&#36828;&#31243;&#25511;&#21046;&#30340;&#26032;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#34892;&#31243;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MADRL&#65289;&#30340;&#26032;&#20852;&#36890;&#20449;&#65288;EC&#65289;&#21644;&#30001;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20351;&#29992;&#20154;&#31867;&#35821;&#35328;&#30340;&#38754;&#21521;&#35821;&#35328;&#30340;&#35821;&#20041;&#36890;&#20449;&#65288;LSC&#65289;&#12290;&#22312;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#36828;&#31243;&#23548;&#33322;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#21253;&#21547;&#20301;&#32622;&#21644;&#36890;&#36947;&#22320;&#22270;&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#25968;&#25454;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;EC&#22312;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#26102;&#20250;&#20135;&#29983;&#39640;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#22256;&#38590;&#65292;&#32780;LSC&#30001;&#20110;LLM&#23610;&#23544;&#36739;&#22823;&#65292;&#20250;&#23548;&#33268;&#39640;&#30340;&#25512;&#29702;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#23427;&#20204;&#21508;&#33258;&#30340;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#24341;&#23548;EC&#35757;&#32451;&#20351;&#29992;LSC&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;&#35821;&#35328;&#24341;&#23548;&#30340;EC&#65288;LEC&#65289;&#12290;&#27169;&#25311;&#39564;&#35777;&#20102;LEC&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#34892;&#31243;&#26102;&#38388;&#65292;&#36991;&#20813;&#20102;&#20449;&#36947;&#36136;&#37327;&#24046;&#30340;&#21306;&#22495;&#65292;&#24182;&#19988;&#22312;&#19982;EC&#30456;&#27604;&#33021;&#22815;&#21152;&#36895;MADRL&#35757;&#32451;&#25910;&#25947;&#36798;&#21040;61.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we compare emergent communication (EC) built upon multi-agent deep reinforcement learning (MADRL) and language-oriented semantic communication (LSC) empowered by a pre-trained large language model (LLM) using human language. In a multi-agent remote navigation task, with multimodal input data comprising location and channel maps, it is shown that EC incurs high training cost and struggles when using multimodal data, whereas LSC yields high inference computing cost due to the LLM's large size. To address their respective bottlenecks, we propose a novel framework of language-guided EC (LEC) by guiding the EC training using LSC via knowledge distillation (KD). Simulations corroborate that LEC achieves faster travel time while avoiding areas with poor channel conditions, as well as speeding up the MADRL training convergence by up to 61.8% compared to EC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#21644;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#32852;&#21512;&#24433;&#21709;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#65292;&#20013;&#31561;&#20219;&#21153;&#30456;&#20284;&#24615;&#23548;&#33268;&#26368;&#22810;&#30340;&#36951;&#24536;&#65292;&#32780;&#22312;&#25554;&#20540;&#38408;&#20540;&#38468;&#36817;&#65292;&#36951;&#24536;&#38543;&#26399;&#26395;&#20219;&#21153;&#30456;&#20284;&#24615;&#21333;&#35843;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2401.12617</link><description>&lt;p&gt;
&#20219;&#21153;&#30456;&#20284;&#24615;&#21644;&#36807;&#21442;&#25968;&#21270;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#32852;&#21512;&#24433;&#21709; - &#19968;&#31181;&#20998;&#26512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting -- An Analytical Model. (arXiv:2401.12617v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#21644;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#32852;&#21512;&#24433;&#21709;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#65292;&#20013;&#31561;&#20219;&#21153;&#30456;&#20284;&#24615;&#23548;&#33268;&#26368;&#22810;&#30340;&#36951;&#24536;&#65292;&#32780;&#22312;&#25554;&#20540;&#38408;&#20540;&#38468;&#36817;&#65292;&#36951;&#24536;&#38543;&#26399;&#26395;&#20219;&#21153;&#30456;&#20284;&#24615;&#21333;&#35843;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#21463;&#21040;&#22810;&#20010;&#20219;&#21153;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20998;&#21035;&#20998;&#26512;&#20102;&#36951;&#24536;&#21463;&#20219;&#21153;&#30456;&#20284;&#24615;&#25110;&#36807;&#21442;&#25968;&#21270;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#22312;&#21487;&#20998;&#26512;&#30340;&#27169;&#22411;&#20013;&#30740;&#31350;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#21644;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#20849;&#21516;&#24433;&#21709;&#36951;&#24536;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#21452;&#20219;&#21153;&#36830;&#32493;&#32447;&#24615;&#22238;&#24402;&#65292;&#20854;&#20013;&#31532;&#20108;&#20010;&#20219;&#21153;&#26159;&#20219;&#24847;&#31532;&#19968;&#20010;&#20219;&#21153;&#30340;&#38543;&#26426;&#27491;&#20132;&#21464;&#25442;&#65288;&#38543;&#26426;&#25490;&#21015;&#20219;&#21153;&#30340;&#25277;&#35937;&#65289;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#26399;&#26395;&#36951;&#24536;&#30340;&#31934;&#30830;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20010;&#24494;&#22937;&#30340;&#27169;&#24335;&#12290;&#22312;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#65292;&#20013;&#31561;&#20219;&#21153;&#30456;&#20284;&#24615;&#23548;&#33268;&#26368;&#22810;&#30340;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#22312;&#25554;&#20540;&#38408;&#20540;&#38468;&#36817;&#65292;&#36951;&#24536;&#38543;&#26399;&#26395;&#20219;&#21153;&#30456;&#20284;&#24615;&#21333;&#35843;&#20943;&#23569;&#12290;&#25105;&#20204;&#29992;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#32447;&#24615;&#22238;&#24402;&#21644;&#24050;&#24314;&#31435;&#30340;&#25490;&#21015;&#20219;&#21153;&#22522;&#20934;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning, catastrophic forgetting is affected by multiple aspects of the tasks. Previous works have analyzed separately how forgetting is affected by either task similarity or overparameterization. In contrast, our paper examines how task similarity and overparameterization jointly affect forgetting in an analyzable model. Specifically, we focus on two-task continual linear regression, where the second task is a random orthogonal transformation of an arbitrary first task (an abstraction of random permutation tasks). We derive an exact analytical expression for the expected forgetting - and uncover a nuanced pattern. In highly overparameterized models, intermediate task similarity causes the most forgetting. However, near the interpolation threshold, forgetting decreases monotonically with the expected task similarity. We validate our findings with linear regression on synthetic data, and with neural networks on established permutation task benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#27010;&#24565;&#65292;&#20998;&#21035;&#26159;&#23545;GenAI&#36755;&#20986;&#21487;&#21462;&#24615;&#30340;&#23450;&#20041;&#21644;"&#25552;&#31034;&#27668;&#21619;"&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#35299;&#20915;GenAI&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.12611</link><description>&lt;p&gt;
Prompt Smells: AI &#29983;&#25104;&#32467;&#26524;&#30340;&#19981;&#21463;&#27426;&#36814;&#30340;&#24449;&#20806;
&lt;/p&gt;
&lt;p&gt;
Prompt Smells: An Omen for Undesirable Generative AI Outputs. (arXiv:2401.12611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#27010;&#24565;&#65292;&#20998;&#21035;&#26159;&#23545;GenAI&#36755;&#20986;&#21487;&#21462;&#24615;&#30340;&#23450;&#20041;&#21644;"&#25552;&#31034;&#27668;&#21619;"&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#35299;&#20915;GenAI&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#36235;&#21183;&#20851;&#27880;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#21019;&#20316;&#25925;&#20107;&#12289;&#25554;&#22270;&#12289;&#35799;&#27468;&#12289;&#25991;&#31456;&#12289;&#35745;&#31639;&#26426;&#20195;&#30721;&#12289;&#38899;&#20048;&#20316;&#21697;&#21644;&#35270;&#39057;&#12290;&#22806;&#22312;&#24187;&#35273;&#26159;&#36825;&#31181;GenAI&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#21487;&#33021;&#23548;&#33268;&#22312;&#23454;&#29616;&#21644;&#32500;&#25252;GenAI&#30340;&#21487;&#20449;&#24230;&#26041;&#38754;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#27010;&#24565;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#23558;&#26377;&#21161;&#20110;&#30740;&#31350;&#31038;&#21306;&#35299;&#20915;&#19982;GenAI&#27169;&#22411;&#24212;&#29992;&#30456;&#20851;&#30340;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;GenAI&#36755;&#20986;"&#21487;&#21462;&#24615;"&#30340;&#23450;&#20041;&#65292;&#20197;&#21450;&#35266;&#23519;&#21040;&#24433;&#21709;&#20854;&#30340;&#19977;&#20010;&#22240;&#32032;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20511;&#37492;&#39532;&#19969;&#183;&#31119;&#21202;&#65288;Martin Fowler&#65289;&#30340;"&#20195;&#30721;&#27668;&#21619;"&#65292;&#25552;&#20986;&#20102;"&#25552;&#31034;&#27668;&#21619;"&#30340;&#27010;&#24565;&#65292;&#20197;&#21450;&#23427;&#20204;&#23545;GenAI&#36755;&#20986;&#30340;&#21487;&#21462;&#24615;&#25152;&#20135;&#29983;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#25105;&#20204;&#26399;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#20026;&#20851;&#20110;GenAI&#36755;&#20986;&#21487;&#21462;&#24615;&#30340;&#25345;&#32493;&#35752;&#35770;&#20570;&#20986;&#36129;&#29486;&#65292;&#24182;&#22312;&#26377;&#24847;&#20041;&#30340;&#26041;&#24335;&#19978;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent Generative Artificial Intelligence (GenAI) trends focus on various applications, including creating stories, illustrations, poems, articles, computer code, music compositions, and videos. Extrinsic hallucinations are a critical limitation of such GenAI, which can lead to significant challenges in achieving and maintaining the trustworthiness of GenAI. In this paper, we propose two new concepts that we believe will aid the research community in addressing limitations associated with the application of GenAI models. First, we propose a definition for the "desirability" of GenAI outputs and three factors which are observed to influence it. Second, drawing inspiration from Martin Fowler's code smells, we propose the concept of "prompt smells" and the adverse effects they are observed to have on the desirability of GenAI outputs. We expect our work will contribute to the ongoing conversation about the desirability of GenAI outputs and help advance the field in a meaningful way.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#21452;&#23792;&#29616;&#35937;&#65292;&#21457;&#29616;&#39640;&#24230;&#36807;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#21487;&#20197;&#36991;&#20813;&#36807;&#25311;&#21512;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#19982;&#20256;&#32479;&#30340;&#20559;&#24046;-&#26041;&#24046;&#25240;&#34935;&#27861;&#21017;&#19981;&#21516;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#24067;&#23572;&#22343;&#20540;&#32500;&#24230;&#65288;BMD&#65289;&#19982;&#32593;&#32476;&#22797;&#26434;&#24615;&#21644;&#25935;&#24863;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24471;&#21040;&#20102;&#22312;&#39640;&#32500;&#24230;&#33539;&#22260;&#20869;BMD&#30340;&#21487;&#35299;&#37322;&#34920;&#36798;&#24335;&#65292;&#21457;&#29616;BMD&#22312;&#32593;&#32476;&#36807;&#21442;&#25968;&#21270;&#31243;&#24230;&#22686;&#21152;&#26102;&#36798;&#21040;&#26497;&#20540;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.12610</link><description>&lt;p&gt;
&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#21452;&#23792;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
The twin peaks of learning neural networks. (arXiv:2401.12610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12610
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#21452;&#23792;&#29616;&#35937;&#65292;&#21457;&#29616;&#39640;&#24230;&#36807;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#21487;&#20197;&#36991;&#20813;&#36807;&#25311;&#21512;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#19982;&#20256;&#32479;&#30340;&#20559;&#24046;-&#26041;&#24046;&#25240;&#34935;&#27861;&#21017;&#19981;&#21516;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#24067;&#23572;&#22343;&#20540;&#32500;&#24230;&#65288;BMD&#65289;&#19982;&#32593;&#32476;&#22797;&#26434;&#24615;&#21644;&#25935;&#24863;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24471;&#21040;&#20102;&#22312;&#39640;&#32500;&#24230;&#33539;&#22260;&#20869;BMD&#30340;&#21487;&#35299;&#37322;&#34920;&#36798;&#24335;&#65292;&#21457;&#29616;BMD&#22312;&#32593;&#32476;&#36807;&#21442;&#25968;&#21270;&#31243;&#24230;&#22686;&#21152;&#26102;&#36798;&#21040;&#26497;&#20540;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#23384;&#22312;&#21452;&#23792;&#29616;&#35937;&#65292;&#21363;&#39640;&#24230;&#36807;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#21487;&#20197;&#36991;&#20813;&#36807;&#25311;&#21512;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#19982;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#25551;&#36848;&#30340;&#26631;&#20934;&#20559;&#24046;-&#26041;&#24046;&#25240;&#34935;&#27861;&#21017;&#19981;&#31526;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#19968;&#29616;&#35937;&#19982;&#31070;&#32463;&#32593;&#32476;&#25152;&#34920;&#31034;&#30340;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#21644;&#25935;&#24863;&#24615;&#22686;&#21152;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24067;&#23572;&#22343;&#20540;&#32500;&#24230;&#65288;BMD&#65289;&#65292;&#36825;&#26159;&#22312;&#24067;&#23572;&#20989;&#25968;&#20998;&#26512;&#32972;&#26223;&#19979;&#21457;&#23637;&#36215;&#26469;&#30340;&#19968;&#31181;&#24230;&#37327;&#12290;&#38024;&#23545;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#31616;&#21333;&#25945;&#24072;-&#23398;&#29983;&#35774;&#32622;&#65292;&#25105;&#20204;&#22522;&#20110;&#21103;&#26412;&#26041;&#27861;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;BMD&#34920;&#36798;&#24335;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#12289;&#29305;&#24449;&#30340;&#25968;&#37327;&#21644;&#36755;&#20837;&#22823;&#23567;&#22312;&#39640;&#32500;&#24230;&#33539;&#22260;&#20869;&#19981;&#26029;&#22686;&#38271;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#32593;&#32476;&#36807;&#21442;&#25968;&#21270;&#31243;&#24230;&#30340;&#22686;&#21152;&#65292;BMD&#36798;&#21040;&#19968;&#20010;&#26497;&#20540;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works demonstrated the existence of a double-descent phenomenon for the generalization error of neural networks, where highly overparameterized models escape overfitting and achieve good test performance, at odds with the standard bias-variance trade-off described by statistical learning theory. In the present work, we explore a link between this phenomenon and the increase of complexity and sensitivity of the function represented by neural networks. In particular, we study the Boolean mean dimension (BMD), a metric developed in the context of Boolean function analysis. Focusing on a simple teacher-student setting for the random feature model, we derive a theoretical analysis based on the replica method that yields an interpretable expression for the BMD, in the high dimensional regime where the number of data points, the number of features, and the input size grow to infinity. We find that, as the degree of overparameterization of the network is increased, the BMD reaches an ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#28151;&#21512;&#30340;&#24555;&#36895;&#21322;&#30417;&#30563;&#38750;&#20984;&#20248;&#21270;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#24211;&#19981;&#21305;&#37197;&#21644;&#20016;&#24230;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21322;&#30417;&#30563;&#35299;&#28151;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23454;&#26045;&#20984;&#24615;&#32422;&#26463;&#20248;&#20110;&#31232;&#30095;&#20808;&#39564;&#23545;&#20110;&#31471;&#20803;&#24211;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.12609</link><description>&lt;p&gt;
&#24555;&#36895;&#21322;&#30417;&#30563;&#38750;&#20984;&#20248;&#21270;&#35299;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Fast Semi-supervised Unmixing using Non-convex Optimization. (arXiv:2401.12609v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#28151;&#21512;&#30340;&#24555;&#36895;&#21322;&#30417;&#30563;&#38750;&#20984;&#20248;&#21270;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#24211;&#19981;&#21305;&#37197;&#21644;&#20016;&#24230;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21322;&#30417;&#30563;&#35299;&#28151;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23454;&#26045;&#20984;&#24615;&#32422;&#26463;&#20248;&#20110;&#31232;&#30095;&#20808;&#39564;&#23545;&#20110;&#31471;&#20803;&#24211;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21322;&#30417;&#30563;/&#22522;&#20110;&#24211;&#30340;&#35299;&#28151;&#21512;&#35774;&#35745;&#30340;&#26032;&#22411;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#24211;&#19981;&#21305;&#37197;&#65292;&#24182;&#33021;&#22815;&#23454;&#26045;&#20016;&#24230;&#21644;&#31561;&#20110;&#19968;&#30340;&#32422;&#26463;&#12290;&#19982;&#20256;&#32479;&#30340;&#31232;&#30095;&#35299;&#28151;&#21512;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#28041;&#21450;&#21040;&#38750;&#20984;&#20248;&#21270;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20132;&#26367;&#20056;&#27861;&#22120;&#26041;&#27861;&#65288;ADMM&#65289;&#22312;&#24490;&#29615;&#27714;&#35299;&#36825;&#20123;&#22797;&#26434;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21322;&#30417;&#30563;&#35299;&#28151;&#21512;&#26041;&#27861;&#65292;&#27599;&#31181;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#24212;&#29992;&#20110;&#26032;&#27169;&#22411;&#30340;&#19981;&#21516;&#20808;&#39564;&#20197;&#21450;&#20016;&#24230;&#21644;&#31561;&#20110;&#19968;&#30340;&#32422;&#26463;&#65306;&#31232;&#30095;&#20808;&#39564;&#21644;&#20984;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23454;&#26045;&#20984;&#24615;&#32422;&#26463;&#20248;&#20110;&#31232;&#30095;&#20808;&#39564;&#23545;&#20110;&#31471;&#20803;&#24211;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#32467;&#26524;&#22312;&#19977;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#65288;&#32771;&#34385;&#20102;&#20809;&#35889;&#21464;&#21270;&#21644;&#19981;&#21516;&#20687;&#32032;&#32431;&#24230;&#27700;&#24179;&#65289;&#21644;Cuprite&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel linear model tailored for semisupervised/library-based unmixing. Our model incorporates considerations for library mismatch while enabling the enforcement of the abundance sum-to-one constraint (ASC). Unlike conventional sparse unmixing methods, this model involves nonconvex optimization, presenting significant computational challenges. We demonstrate the efficacy of Alternating Methods of Multipliers (ADMM) in cyclically solving these intricate problems. We propose two semisupervised unmixing approaches, each relying on distinct priors applied to the new model in addition to the ASC: sparsity prior and convexity constraint. Our experimental results validate that enforcing the convexity constraint outperforms the sparsity prior for the endmember library. These results are corroborated across three simulated datasets (accounting for spectral variability and varying pixel purity levels) and the Cuprite dataset. Additionally, our comparison with convent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28508;&#22312;&#34920;&#31034;&#30340;&#31561;&#21464;&#24615;&#20197;&#21450;&#22312;&#20351;&#29992;&#20013;&#32771;&#34385;&#31561;&#21464;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#36873;&#25321;&#19981;&#21464;&#25237;&#24433;&#30340;&#21407;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#20004;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.12588</link><description>&lt;p&gt;
&#35299;&#35835;&#31561;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Interpreting Equivariant Representations. (arXiv:2401.12588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28508;&#22312;&#34920;&#31034;&#30340;&#31561;&#21464;&#24615;&#20197;&#21450;&#22312;&#20351;&#29992;&#20013;&#32771;&#34385;&#31561;&#21464;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#36873;&#25321;&#19981;&#21464;&#25237;&#24433;&#30340;&#21407;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#20004;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35270;&#21270;&#12289;&#25554;&#20540;&#25110;&#29305;&#24449;&#25552;&#21462;&#31561;&#19979;&#28216;&#20219;&#21153;&#65292;&#28508;&#22312;&#34920;&#31034;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#19981;&#21464;&#21644;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#24378;&#21046;&#25191;&#34892;&#24402;&#32435;&#20559;&#24046;&#30340;&#24378;&#22823;&#19988;&#24050;&#24314;&#31435;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#28508;&#22312;&#34920;&#31034;&#26102;&#65292;&#24517;&#39035;&#21516;&#26102;&#32771;&#34385;&#31561;&#21464;&#27169;&#22411;&#26045;&#21152;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#32771;&#34385;&#24402;&#32435;&#20559;&#24046;&#20250;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#21453;&#65292;&#36890;&#36807;&#20351;&#29992;&#28508;&#22312;&#34920;&#31034;&#30340;&#19981;&#21464;&#25237;&#24433;&#21487;&#20197;&#26377;&#25928;&#22320;&#32771;&#34385;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36873;&#25321;&#36825;&#26679;&#19968;&#20010;&#25237;&#24433;&#30340;&#21407;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#24120;&#35265;&#20363;&#23376;&#20013;&#20351;&#29992;&#36825;&#20123;&#21407;&#21017;&#30340;&#24433;&#21709;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#22270;&#29983;&#25104;&#30340;&#32622;&#25442;&#31561;&#21464;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65307;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#35774;&#35745;&#20986;&#19981;&#20135;&#29983;&#20449;&#24687;&#25439;&#22833;&#30340;&#19981;&#21464;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent representations are used extensively for downstream tasks, such as visualization, interpolation or feature extraction of deep learning models. Invariant and equivariant neural networks are powerful and well-established models for enforcing inductive biases. In this paper, we demonstrate that the inductive bias imposed on the by an equivariant model must also be taken into account when using latent representations. We show how not accounting for the inductive biases leads to decreased performance on downstream tasks, and vice versa, how accounting for inductive biases can be done effectively by using an invariant projection of the latent representations. We propose principles for how to choose such a projection, and show the impact of using these principles in two common examples: First, we study a permutation equivariant variational auto-encoder trained for molecule graph generation; here we show that invariant projections can be designed that incur no loss of information in the
&lt;/p&gt;</description></item><item><title>LLMCheckup&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65292;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19982;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#65292;&#29983;&#25104;&#33258;&#25105;&#35299;&#37322;&#24182;&#25552;&#20379;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2401.12576</link><description>&lt;p&gt;
LLMCheckup&#65306;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#24335;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools. (arXiv:2401.12576v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12576
&lt;/p&gt;
&lt;p&gt;
LLMCheckup&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65292;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19982;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#65292;&#29983;&#25104;&#33258;&#25105;&#35299;&#37322;&#24182;&#25552;&#20379;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20197;&#23545;&#35805;&#24418;&#24335;&#36827;&#34892;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#24050;&#32463;&#35777;&#26126;&#22312;&#22686;&#24378;&#29992;&#25143;&#29702;&#35299;&#26041;&#38754;&#20855;&#26377;&#25928;&#26524;&#65292;&#22240;&#20026;&#19968;&#27425;&#24615;&#35299;&#37322;&#26377;&#26102;&#26080;&#27861;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#32473;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;&#23545;&#35805;&#30340;&#35299;&#37322;&#26041;&#26696;&#38656;&#35201;&#35768;&#22810;&#20381;&#36182;&#39033;&#65292;&#24182;&#19988;&#19981;&#23481;&#26131;&#36716;&#31227;&#21040;&#23427;&#20204;&#26410;&#35774;&#35745;&#30340;&#20219;&#21153;&#19978;&#12290;&#36890;&#36807;LLMCheckup&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#24037;&#20855;&#65292;&#20801;&#35768;&#29992;&#25143;&#19982;&#20219;&#20309;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#23545;&#35805;&#20197;&#20102;&#35299;&#20854;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;LLMs&#33021;&#22815;&#33258;&#34892;&#29983;&#25104;&#25152;&#26377;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#19982;&#19968;&#31995;&#21015;&#21487;&#35299;&#37322;&#24615;AI&#65288;XAI&#65289;&#24037;&#20855;&#65288;&#20363;&#22914;&#29305;&#24449;&#24402;&#22240;&#12289;&#22522;&#20110;&#23884;&#20837;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#21453;&#20107;&#23454;&#21644;&#22522;&#20110;&#29702;&#30001;&#29983;&#25104;&#30340;&#25552;&#31034;&#31574;&#30053;&#65289;&#36830;&#25509;&#65292;&#20197;&#23436;&#25104;&#24847;&#22270;&#35782;&#21035;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;LLM&#65288;&#33258;&#25105;&#65289;&#35299;&#37322;&#20197;&#20132;&#20114;&#23545;&#35805;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#25903;&#25345;&#21518;&#32493;&#38382;&#39064;&#21644;&#29983;&#25104;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability tools that offer explanations in the form of a dialogue have demonstrated their efficacy in enhancing users' understanding, as one-off explanations may occasionally fall short in providing sufficient information to the user. Current solutions for dialogue-based explanations, however, require many dependencies and are not easily transferable to tasks they were not designed for. With LLMCheckup, we present an easily accessible tool that allows users to chat with any state-of-the-art large language model (LLM) about its behavior. We enable LLMs to generate all explanations by themselves and take care of intent recognition without fine-tuning, by connecting them with a broad spectrum of Explainable AI (XAI) tools, e.g. feature attributions, embedding-based similarity, and prompting strategies for counterfactual and rationale generation. LLM (self-)explanations are presented as an interactive dialogue that supports follow-up questions and generates suggestions. LLMCheckup p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22270;&#23545;&#27604;&#19981;&#21464;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GCL&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35889;&#22270;&#25193;&#22686;&#21644;&#35774;&#35745;&#19981;&#21464;&#24615;&#30446;&#26631;&#21644;&#29420;&#31435;&#24615;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.12564</link><description>&lt;p&gt;
&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#30475;&#22270;&#23545;&#27604;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Invariant Learning from the Causal Perspective. (arXiv:2401.12564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22270;&#23545;&#27604;&#19981;&#21464;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GCL&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35889;&#22270;&#25193;&#22686;&#21644;&#35774;&#35745;&#19981;&#21464;&#24615;&#30446;&#26631;&#21644;&#29420;&#31435;&#24615;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#20004;&#20010;&#25193;&#22686;&#22270;&#26469;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24050;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;GCL&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29702;&#35299;&#22312;&#23454;&#36341;&#20013;&#24635;&#26159;&#27491;&#30830;&#30340;&#21527;&#65311;&#26412;&#25991;&#39318;&#20808;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;GCL&#12290;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#20998;&#26512;GCL&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;GCL&#30001;&#20110;&#22270;&#20013;&#21253;&#21547;&#30340;&#38750;&#22240;&#26524;&#20449;&#24687;&#65292;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#12290;&#37027;&#20040;&#65292;&#25105;&#20204;&#22914;&#20309;&#20462;&#22797;&#24182;&#20419;&#20351;&#24403;&#21069;&#30340;GCL&#23398;&#20064;&#26356;&#22909;&#30340;&#19981;&#21464;&#34920;&#31034;&#21602;&#65311;SCM&#25552;&#20379;&#20102;&#20004;&#20010;&#35201;&#27714;&#24182;&#28608;&#21169;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GCL&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35889;&#22270;&#25193;&#22686;&#26469;&#27169;&#25311;&#23545;&#38750;&#22240;&#26524;&#22240;&#32032;&#30340;&#24178;&#39044;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19981;&#21464;&#24615;&#30446;&#26631;&#21644;&#29420;&#31435;&#24615;&#30446;&#26631;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#22240;&#26524;&#22240;&#32032;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#65288;i&#65289;&#19981;&#21464;&#24615;&#30446;&#26631;&#40723;&#21169;e
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning (GCL), learning the node representation by contrasting two augmented graphs in a self-supervised way, has attracted considerable attention. GCL is usually believed to learn the invariant representation. However, does this understanding always hold in practice? In this paper, we first study GCL from the perspective of causality. By analyzing GCL with the structural causal model (SCM), we discover that traditional GCL may not well learn the invariant representations due to the non-causal information contained in the graph. How can we fix it and encourage the current GCL to learn better invariant representations? The SCM offers two requirements and motives us to propose a novel GCL method. Particularly, we introduce the spectral graph augmentation to simulate the intervention upon non-causal factors. Then we design the invariance objective and independence objective to better capture the causal factors. Specifically, (i) the invariance objective encourages the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UR4NNV&#39564;&#35777;&#26694;&#26550;&#65292;&#21033;&#29992;&#27424;&#20272;&#35745;&#21487;&#36798;&#24615;&#20998;&#26512;&#36827;&#34892;DNN&#39564;&#35777;&#12290;&#35813;&#26694;&#26550;&#23545;&#20855;&#26377;ReLU&#28608;&#27963;&#30340;DNN&#36827;&#34892;&#27424;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#35797;&#38169;&#26041;&#27861;&#26377;&#25928;&#22320;&#35777;&#20266;DNN&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12550</link><description>&lt;p&gt;
UR4NNV: &#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#65292;&#22522;&#20110;&#27424;&#20272;&#35745;&#21487;&#36798;&#24615;&#30340;&#26041;&#27861;&#65281;
&lt;/p&gt;
&lt;p&gt;
UR4NNV: Neural Network Verification, Under-approximation Reachability Works!. (arXiv:2401.12550v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UR4NNV&#39564;&#35777;&#26694;&#26550;&#65292;&#21033;&#29992;&#27424;&#20272;&#35745;&#21487;&#36798;&#24615;&#20998;&#26512;&#36827;&#34892;DNN&#39564;&#35777;&#12290;&#35813;&#26694;&#26550;&#23545;&#20855;&#26377;ReLU&#28608;&#27963;&#30340;DNN&#36827;&#34892;&#27424;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#35797;&#38169;&#26041;&#27861;&#26377;&#25928;&#22320;&#35777;&#20266;DNN&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24418;&#24335;&#39564;&#35777;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22522;&#20110;&#36807;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#22240;&#20854;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31574;&#30053;&#22312;&#35299;&#20915;&#28041;&#21450;&#30830;&#20999;&#36755;&#20986;&#21306;&#22495;&#25110;&#24341;&#20837;&#30340;&#36817;&#20284;&#35823;&#24046;&#26159;&#21542;&#36829;&#21453;&#25152;&#35752;&#35770;&#23646;&#24615;&#30340;&#8220;&#26410;&#30693;&#22256;&#22659;&#8221;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#20102;UR4NNV&#39564;&#35777;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#27424;&#20272;&#35745;&#21487;&#36798;&#24615;&#20998;&#26512;&#36827;&#34892;DNN&#39564;&#35777;&#12290;UR4NNV&#19987;&#27880;&#20110;&#20855;&#26377;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#28608;&#27963;&#30340;DNN&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20108;&#21449;&#26641;&#20998;&#25903;&#30340;&#27424;&#20272;&#35745;&#31639;&#27861;&#12290;&#22312;&#27599;&#20010;&#21608;&#26399;&#20013;&#65292;UR4NNV&#23545;&#21487;&#36798;&#38598;&#21512;&#30340;&#23376;&#22810;&#38754;&#20307;&#36827;&#34892;&#27424;&#20272;&#35745;&#65292;&#24182;&#38024;&#23545;&#32473;&#23450;&#30340;&#23646;&#24615;&#39564;&#35777;&#35813;&#22810;&#38754;&#20307;&#12290;&#36890;&#36807;&#35797;&#38169;&#26041;&#27861;&#65292;UR4NNV&#22312;&#36798;&#21040;&#39564;&#35777;&#21608;&#26399;&#36793;&#30028;&#21644;&#22833;&#36133;&#26102;&#25552;&#20379;&#20102;&#26377;&#25928;&#22320;&#35777;&#20266;DNN&#23646;&#24615;&#30340;&#20449;&#24515;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, formal verification of deep neural networks (DNNs) has garnered considerable attention, and over-approximation based methods have become popular due to their effectiveness and efficiency. However, these strategies face challenges in addressing the "unknown dilemma" concerning whether the exact output region or the introduced approximation error violates the property in question. To address this, this paper introduces the UR4NNV verification framework, which utilizes under-approximation reachability analysis for DNN verification for the first time. UR4NNV focuses on DNNs with Rectified Linear Unit (ReLU) activations and employs a binary tree branch-based under-approximation algorithm. In each epoch, UR4NNV under-approximates a sub-polytope of the reachable set and verifies this polytope against the given property. Through a trial-and-error approach, UR4NNV effectively falsifies DNN properties while providing confidence levels when reaching verification epoch bounds and failing
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26500;&#24314;&#36817;&#35270;MPC&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#26368;&#20248;&#20540;&#20989;&#25968;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#22312;&#32447;&#35745;&#31639;&#36127;&#25285;&#65292;&#32780;&#19981;&#24433;&#21709;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12546</link><description>&lt;p&gt;
&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26500;&#24314;&#36817;&#35270;MPC&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
On Building Myopic MPC Policies using Supervised Learning. (arXiv:2401.12546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26500;&#24314;&#36817;&#35270;MPC&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#26368;&#20248;&#20540;&#20989;&#25968;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#22312;&#32447;&#35745;&#31639;&#36127;&#25285;&#65292;&#32780;&#19981;&#24433;&#21709;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#20013;&#65292;&#32467;&#21512;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#22312;&#36817;&#20284;&#26174;&#24335;MPC&#39046;&#22495;&#65292;&#20854;&#20013;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#36890;&#36807;&#31163;&#32447;&#29983;&#25104;&#30340;&#26368;&#20339;&#29366;&#24577;-&#21160;&#20316;&#23545;&#26469;&#23398;&#20064;MPC&#31574;&#30053;&#12290;&#34429;&#28982;&#36817;&#20284;&#26174;&#24335;MPC&#30340;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#20934;&#30830;&#22320;&#22797;&#21046;MPC&#31574;&#30053;&#65292;&#29992;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#22312;&#32447;&#20248;&#21270;&#65292;&#20294;&#36890;&#24120;&#20250;&#22833;&#21435;&#35299;&#20915;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#25152;&#24102;&#26469;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#31574;&#30053;&#65292;&#21363;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#26368;&#20248;&#20540;&#20989;&#25968;&#32780;&#19981;&#26159;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#22312;&#19968;&#20010;&#38750;&#24120;&#30701;&#30340;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#23558;&#20854;&#20316;&#20026;&#36817;&#35270;MPC&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#22312;&#32447;&#35745;&#31639;&#36127;&#25285;&#65292;&#19981;&#24433;&#21709;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#19982;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of supervised learning techniques in combination with model predictive control (MPC) has recently generated significant interest, particularly in the area of approximate explicit MPC, where function approximators like deep neural networks are used to learn the MPC policy via optimal state-action pairs generated offline. While the aim of approximate explicit MPC is to closely replicate the MPC policy, substituting online optimization with a trained neural network, the performance guarantees that come with solving the online optimization problem are typically lost. This paper considers an alternative strategy, where supervised learning is used to learn the optimal value function offline instead of learning the optimal policy. This can then be used as the cost-to-go function in a myopic MPC with a very short prediction horizon, such that the online computation burden reduces significantly without affecting the controller performance. This approach differs from existing wor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;k&#20013;&#24515;&#32858;&#31867;&#19978;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#24471;&#21040;&#20102;&#25928;&#29575;&#39640;&#19988;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12533</link><description>&lt;p&gt;
&#26377;&#25928;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;k&#20013;&#24515;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Efficient Constrained $k$-Center Clustering with Background Knowledge. (arXiv:2401.12533v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;k&#20013;&#24515;&#32858;&#31867;&#19978;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#24471;&#21040;&#20102;&#25928;&#29575;&#39640;&#19988;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#24515;&#20026;&#22522;&#30784;&#30340;&#32858;&#31867;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#37117;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36755;&#20837;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#32858;&#31867;&#32467;&#26524;&#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#24191;&#27867;&#37319;&#29992;&#30340;k&#20013;&#24515;&#32858;&#31867;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#30340;&#32972;&#26223;&#30693;&#35782;&#24314;&#27169;&#20026;&#24517;&#36830;&#65288;ML&#65289;&#21644;&#19981;&#36830;&#65288;CL&#65289;&#32422;&#26463;&#38598;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21253;&#25324;k&#20013;&#24515;&#22312;&#20869;&#30340;&#32858;&#31867;&#38382;&#39064;&#26412;&#36136;&#19978;&#37117;&#26159;NP&#22256;&#38590;&#30340;&#65292;&#32780;&#26356;&#22797;&#26434;&#30340;&#21463;&#32422;&#26463;&#21464;&#20307;&#34987;&#35748;&#20026;&#21463;&#21040;&#26356;&#20005;&#37325;&#30340;&#36817;&#20284;&#21644;&#35745;&#31639;&#38556;&#30861;&#30340;&#38480;&#21046;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#21253;&#25324;&#21453;&#25903;&#37197;&#38598;&#65292;&#32447;&#24615;&#35268;&#21010;&#65288;LP&#65289;&#25972;&#25968;&#24179;&#38754;&#21644;LP&#23545;&#20598;&#24615;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#32422;&#26463;k&#20013;&#24515;&#30340;&#39640;&#25928;&#36817;&#20284;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#31454;&#20105;&#22522;&#20934;&#31639;&#27861;&#65292;&#24182;&#23545;&#25105;&#20204;&#30340;&#36817;&#20284;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Center-based clustering has attracted significant research interest from both theory and practice. In many practical applications, input data often contain background knowledge that can be used to improve clustering results. In this work, we build on widely adopted $k$-center clustering and model its input background knowledge as must-link (ML) and cannot-link (CL) constraint sets. However, most clustering problems including $k$-center are inherently $\mathcal{NP}$-hard, while the more complex constrained variants are known to suffer severer approximation and computation barriers that significantly limit their applicability. By employing a suite of techniques including reverse dominating sets, linear programming (LP) integral polyhedron, and LP duality, we arrive at the first efficient approximation algorithm for constrained $k$-center with the best possible ratio of 2. We also construct competitive baseline algorithms and empirically evaluate our approximation algorithm against them o
&lt;/p&gt;</description></item><item><title>DAFA&#36890;&#36807;&#32771;&#34385;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#25439;&#22833;&#26435;&#37325;&#21644;&#23545;&#25239;&#36793;&#30028;&#65292;&#24182;&#35843;&#25972;&#23427;&#20204;&#20197;&#25552;&#39640;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12532</link><description>&lt;p&gt;
DAFA&#65306;&#36317;&#31163;&#24863;&#30693;&#20844;&#24179;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DAFA: Distance-Aware Fair Adversarial Training. (arXiv:2401.12532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12532
&lt;/p&gt;
&lt;p&gt;
DAFA&#36890;&#36807;&#32771;&#34385;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#25439;&#22833;&#26435;&#37325;&#21644;&#23545;&#25239;&#36793;&#30028;&#65292;&#24182;&#35843;&#25972;&#23427;&#20204;&#20197;&#25552;&#39640;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#35757;&#32451;&#20013;&#31867;&#21035;&#38388;&#30340;&#20934;&#30830;&#24615;&#24046;&#24322;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#34987;&#25918;&#22823;&#65292;&#36825;&#34987;&#31216;&#20026;&#40065;&#26834;&#20844;&#24179;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20026;&#22686;&#24378;&#40065;&#26834;&#20844;&#24179;&#24615;&#32780;&#29306;&#29298;&#27169;&#22411;&#23545;&#26131;&#31867;&#21035;&#30340;&#24615;&#33021;&#65292;&#20197;&#25913;&#21892;&#23545;&#38590;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#65292;&#27169;&#22411;&#23545;&#26368;&#24046;&#31867;&#21035;&#26679;&#26412;&#30340;&#39044;&#27979;&#22823;&#22810;&#20559;&#21521;&#20110;&#19982;&#26368;&#24046;&#31867;&#21035;&#30456;&#20284;&#30340;&#31867;&#21035;&#65292;&#32780;&#19981;&#26159;&#26131;&#31867;&#21035;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38543;&#30528;&#31867;&#21035;&#20043;&#38388;&#36317;&#31163;&#30340;&#20943;&#23567;&#65292;&#40065;&#26834;&#20844;&#24179;&#24615;&#20250;&#24694;&#21270;&#12290;&#21463;&#21040;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36317;&#31163;&#24863;&#30693;&#20844;&#24179;&#23545;&#25239;&#35757;&#32451;&#65288;DAFA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#35299;&#20915;&#40065;&#26834;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#27599;&#20010;&#31867;&#21035;&#20998;&#37197;&#19981;&#21516;&#30340;&#25439;&#22833;&#26435;&#37325;&#21644;&#23545;&#25239;&#36793;&#30028;&#65292;&#24182;&#35843;&#25972;&#23427;&#20204;&#20197;&#20419;&#36827;&#19968;&#31181;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The disparity in accuracy between classes in standard training is amplified during adversarial training, a phenomenon termed the robust fairness problem. Existing methodologies aimed to enhance robust fairness by sacrificing the model's performance on easier classes in order to improve its performance on harder ones. However, we observe that under adversarial attacks, the majority of the model's predictions for samples from the worst class are biased towards classes similar to the worst class, rather than towards the easy classes. Through theoretical and empirical analysis, we demonstrate that robust fairness deteriorates as the distance between classes decreases. Motivated by these insights, we introduce the Distance-Aware Fair Adversarial training (DAFA) methodology, which addresses robust fairness by taking into account the similarities between classes. Specifically, our method assigns distinct loss weights and adversarial margins to each class and adjusts them to encourage a trade-
&lt;/p&gt;</description></item><item><title>BiTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#35843;&#25972;&#23454;&#29616;&#20102;&#26080;&#25439;&#21152;&#36895;&#12290;&#23427;&#37319;&#29992;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#21516;&#26102;&#36827;&#34892;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2401.12522</link><description>&lt;p&gt;
BiTA: &#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#25439;&#21152;&#36895;&#30340;&#21452;&#21521;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models. (arXiv:2401.12522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12522
&lt;/p&gt;
&lt;p&gt;
BiTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#35843;&#25972;&#23454;&#29616;&#20102;&#26080;&#25439;&#21152;&#36895;&#12290;&#23427;&#37319;&#29992;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#21516;&#26102;&#36827;&#34892;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#23548;&#33268;&#39640;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#21644;&#24310;&#36831;&#24310;&#38271;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#8212;&#8212;&#21452;&#21521;&#35843;&#25972;&#20197;&#23454;&#29616;&#26080;&#25439;&#21152;&#36895;&#65288;BiTA&#65289;&#65292;&#36890;&#36807;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#26469;&#21152;&#36895;LLMs&#12290;&#21463;&#21551;&#21457;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#35774;&#35745;&#65292;&#31216;&#20026;&#21452;&#21521;&#35843;&#25972;&#65292;&#26469;&#22686;&#24378;LLMs&#22312;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#37319;&#29992;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#65292;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#36827;&#34892;&#33609;&#31295;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#30830;&#20445;&#36755;&#20986;&#32467;&#26524;&#19982;&#23427;&#20204;&#30340;&#33258;&#22238;&#24402;&#23545;&#24212;&#29289;&#22312;&#36138;&#23146;&#25277;&#26679;&#19979;&#23436;&#20840;&#30456;&#21516;&#12290;BiTA&#20316;&#20026;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25554;&#20214;&#27169;&#22359;&#65292;&#21487;&#20197;&#26080;&#32541;&#22686;&#24378;&#29616;&#26377;LLMs&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#25215;&#25285;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#25552;&#20986;&#30340;BiTA&#65292;LLaMA-2-70B-Chat&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) commonly employ autoregressive generation during inference, leading to high memory bandwidth demand and consequently extended latency. To mitigate this inefficiency, we present Bi-directional Tuning for lossless Acceleration (BiTA), an innovative method expediting LLMs via streamlined semi-autoregressive generation and draft verification. Inspired by the concept of prompt tuning, we enhance LLMs with a parameter-efficient design called bi-directional tuning for the capability in semi-autoregressive generation. Employing efficient tree-based decoding, the models perform draft candidate generation and verification in parallel, ensuring outputs identical to their autoregressive counterparts under greedy sampling. BiTA serves as a lightweight plug-in module, seamlessly boosting the inference efficiency of existing LLMs without requiring additional assistance models or incurring significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat achieve
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#25991;&#26412;&#20998;&#31867;&#21644;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#25216;&#26415;&#23545;&#38271;&#25991;&#26412;&#36827;&#34892;&#21387;&#32553;&#65292;&#28982;&#21518;&#37319;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;Transformers&#30340;&#23884;&#20837;&#26041;&#27861;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#20248;&#24800;&#36152;&#26131;&#21327;&#23450;&#30340;&#38271;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.12520</link><description>&lt;p&gt;
&#23545;&#20248;&#24800;&#36152;&#26131;&#21327;&#23450;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20869;&#23481;&#36827;&#34892;&#20851;&#38190;&#20449;&#24687;&#26816;&#32034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Key Information Retrieval to Classify the Unstructured Data Content of Preferential Trade Agreements. (arXiv:2401.12520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#25991;&#26412;&#20998;&#31867;&#21644;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#25216;&#26415;&#23545;&#38271;&#25991;&#26412;&#36827;&#34892;&#21387;&#32553;&#65292;&#28982;&#21518;&#37319;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;Transformers&#30340;&#23884;&#20837;&#26041;&#27861;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#20248;&#24800;&#36152;&#26131;&#21327;&#23450;&#30340;&#38271;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#25968;&#25454;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#39044;&#27979;&#38271;&#25991;&#26412;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#25991;&#26412;&#39044;&#27979;&#26041;&#27861;&#22312;&#22788;&#29702;&#38271;&#25991;&#26412;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20887;&#20313;&#21644;&#26080;&#20851;&#20449;&#24687;&#30340;&#23384;&#22312;&#65292;&#36825;&#24433;&#21709;&#20102;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25429;&#25417;&#37325;&#35201;&#35265;&#35299;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38271;&#25991;&#26412;&#20998;&#31867;&#21644;&#39044;&#27979;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#23884;&#20837;&#25216;&#26415;&#26469;&#23545;&#38271;&#25991;&#26412;&#36827;&#34892;&#21387;&#32553;&#65292;&#20197;&#20943;&#23569;&#20854;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;Transformers&#65288;BERT&#65289;&#30340;&#23884;&#20837;&#26041;&#27861;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20248;&#24800;&#36152;&#26131;&#21327;&#23450;&#30340;&#38271;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23884;&#20837;&#26041;&#27861;&#23545;&#25991;&#26412;&#36827;&#34892;&#21387;&#32553;&#19981;&#20165;&#22686;&#24378;&#20102;&#39044;&#27979;&#34920;&#29616;&#65292;&#32780;&#19988;&#26377;&#21161;&#20110;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid proliferation of textual data, predicting long texts has emerged as a significant challenge in the domain of natural language processing. Traditional text prediction methods encounter substantial difficulties when grappling with long texts, primarily due to the presence of redundant and irrelevant information, which impedes the model's capacity to capture pivotal insights from the text. To address this issue, we introduce a novel approach to long-text classification and prediction. Initially, we employ embedding techniques to condense the long texts, aiming to diminish the redundancy therein. Subsequently,the Bidirectional Encoder Representations from Transformers (BERT) embedding method is utilized for text classification training. Experimental outcomes indicate that our method realizes considerable performance enhancements in classifying long texts of Preferential Trade Agreements. Furthermore, the condensation of text through embedding methods not only augments predic
&lt;/p&gt;</description></item><item><title>DDMI&#26159;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#26080;&#20851;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#33258;&#36866;&#24212;&#20301;&#32622;&#23884;&#20837;&#32780;&#19981;&#26159;&#32593;&#32476;&#26435;&#37325;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#29983;&#25104;&#36136;&#37327;&#36739;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12517</link><description>&lt;p&gt;
DDMI: &#38754;&#21521;&#39046;&#22495;&#26080;&#20851;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing High-Quality Implicit Neural Representations. (arXiv:2401.12517v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12517
&lt;/p&gt;
&lt;p&gt;
DDMI&#26159;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#26080;&#20851;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#33258;&#36866;&#24212;&#20301;&#32622;&#23884;&#20837;&#32780;&#19981;&#26159;&#32593;&#32476;&#26435;&#37325;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#29983;&#25104;&#36136;&#37327;&#36739;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31867;&#29992;&#20110;&#21512;&#25104;&#21508;&#20010;&#39046;&#22495;&#20013;&#20219;&#24847;&#36830;&#32493;&#20449;&#21495;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#29983;&#25104;&#27169;&#22411;&#65292;&#20026;&#39046;&#22495;&#26080;&#20851;&#30340;&#29983;&#25104;&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#65292;&#20294;&#24448;&#24448;&#26080;&#27861;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#21442;&#25968;&#21270;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#22266;&#23450;&#30340;&#20301;&#32622;&#23884;&#20837;&#26469;&#35780;&#20272;&#32593;&#32476;&#12290;&#21487;&#20197;&#35828;&#65292;&#36825;&#31181;&#26550;&#26500;&#38480;&#21046;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23548;&#33268;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#29983;&#25104;&#30340;&#36136;&#37327;&#36739;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#26080;&#20851;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; (DDMI)&#65292;&#20854;&#29983;&#25104;&#33258;&#36866;&#24212;&#20301;&#32622;&#23884;&#20837;&#32780;&#19981;&#26159;&#32593;&#32476;&#26435;&#37325;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31163;&#25955;&#21040;&#36830;&#32493;&#31354;&#38388;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120; (D2C-VAE)&#65292;&#23427;&#22312;&#20849;&#20139;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#32541;&#36830;&#25509;&#31163;&#25955;&#25968;&#25454;&#21644;&#36830;&#32493;&#20449;&#21495;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Recent studies have introduced a new class of generative models for synthesizing implicit neural representations (INRs) that capture arbitrary continuous signals in various domains. These models opened the door for domain-agnostic generative models, but they often fail to achieve high-quality generation. We observed that the existing methods generate the weights of neural networks to parameterize INRs and evaluate the network with fixed positional embeddings (PEs). Arguably, this architecture limits the expressive power of generative models and results in low-quality INR generation. To address this limitation, we propose Domain-agnostic Latent Diffusion Model for INRs (DDMI) that generates adaptive positional embeddings instead of neural networks' weights. Specifically, we develop a Discrete-to-continuous space Variational AutoEncoder (D2C-VAE), which seamlessly connects discrete data and the continuous signal functions in the shared latent space. Additionally, we introduce a novel con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#24314;&#27169;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#20223;&#30495;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#35823;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#25968;&#23383;&#20811;&#38534;&#24050;&#30693;&#30340;&#35823;&#20449;&#24687;&#20849;&#20139;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#25311;&#30340;&#30495;&#23454;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#24182;&#19988;&#32771;&#34385;&#21040;&#20102;&#35752;&#35770;&#20027;&#39064;&#12289;&#29992;&#25143;&#20559;&#22909;&#21644;&#22312;&#32447;&#31038;&#21306;&#21160;&#24577;&#31561;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2401.12509</link><description>&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#25968;&#23383;&#20811;&#38534;&#30340;&#35821;&#35328;&#25935;&#24863;&#20195;&#29702;&#24314;&#27169;&#30740;&#31350;&#35823;&#20449;&#24687;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Digital cloning of online social networks for language-sensitive agent-based modeling of misinformation spread. (arXiv:2401.12509v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#24314;&#27169;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#20223;&#30495;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#35823;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#25968;&#23383;&#20811;&#38534;&#24050;&#30693;&#30340;&#35823;&#20449;&#24687;&#20849;&#20139;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#25311;&#30340;&#30495;&#23454;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#24182;&#19988;&#32771;&#34385;&#21040;&#20102;&#35752;&#35770;&#20027;&#39064;&#12289;&#29992;&#25143;&#20559;&#22909;&#21644;&#22312;&#32447;&#31038;&#21306;&#21160;&#24577;&#31561;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20223;&#30495;&#26694;&#26550;&#65292;&#23558;&#20195;&#29702;&#24314;&#27169;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#35823;&#20449;&#24687;&#20256;&#25773;&#12290;&#34429;&#28982;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#23384;&#22312;&#35768;&#22810;&#20854;&#20182;&#30340;&#20195;&#29702;&#24314;&#27169;&#20223;&#30495;&#65292;&#20294;&#23427;&#20204;&#22312;&#25552;&#20379;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#26041;&#38754;&#21463;&#38480;&#20110;&#20854;&#23545;&#29616;&#26377;&#32593;&#32476;&#30340;&#30495;&#23454;&#24615;&#21644;&#26222;&#36866;&#24615;&#30340;&#19981;&#36275;&#12290;&#20026;&#20102;&#37096;&#20998;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#19979;&#36733;&#36229;&#36807;&#19968;&#19975;&#21517;&#29992;&#25143;&#30340;&#31038;&#20132;&#23186;&#20307;&#21382;&#21490;&#35760;&#24405;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#24050;&#30693;&#35823;&#20449;&#24687;&#20849;&#20139;&#32593;&#32476;&#30340;&#8220;&#25968;&#23383;&#20811;&#38534;&#8221;&#12290;&#25105;&#20204;&#35299;&#26512;&#36825;&#20123;&#21382;&#21490;&#35760;&#24405;&#65292;&#25552;&#21462;&#20986;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#24182;&#23545;&#25104;&#21592;&#20043;&#38388;&#20449;&#24687;&#20998;&#20139;&#21644;&#20256;&#25773;&#30340;&#24494;&#22937;&#26041;&#24335;&#36827;&#34892;&#24314;&#27169;&#12290;&#19982;&#36825;&#20010;&#39046;&#22495;&#20013;&#35768;&#22810;&#20854;&#20182;&#30340;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#26694;&#26550;&#20013;&#29992;&#25143;&#20043;&#38388;&#30340;&#20449;&#24687;&#20998;&#20139;&#23545;&#35752;&#35770;&#20027;&#39064;&#12289;&#29992;&#25143;&#20559;&#22909;&#21644;&#22312;&#32447;&#31038;&#21306;&#21160;&#24577;&#37117;&#38750;&#24120;&#25935;&#24863;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#30495;&#23454;&#24615;&#65292;&#25105;&#20204;&#29992;&#19968;&#32452;&#35760;&#24405;&#22312; t &#20013;&#30340;&#24086;&#23376;&#23545;&#20811;&#38534;&#32593;&#32476;&#36827;&#34892;&#20102;&#31181;&#23376;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a simulation framework for studying misinformation spread within online social networks that blends agent-based modeling and natural language processing techniques. While many other agent-based simulations exist in this space, their ability to provide actionable insights in in part limited by their lack of fidelity and generalizability to existing networks. To partially address these concerns, we create a 'digital clone' of a known misinformation sharing network by downloading social media histories for over ten thousand of its users. We parse these histories to both extract the structure of the network and model the nuanced ways in which information is shared and spread among its members. Unlike many other agent-based methods in this space, information sharing between users in our framework is sensitive to topic of discussion, user preferences, and online community dynamics. To evaluate the fidelity of our method, we seed our cloned network with a set of posts recorded in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#27491;&#21017;&#21270;&#39044;&#26399;&#22870;&#21169;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26041;&#24046;&#20943;&#23567;&#30340;&#25216;&#26415;&#20197;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#36798;&#21040;$O(\epsilon^{-3})$&#12290;</title><link>http://arxiv.org/abs/2401.12508</link><description>&lt;p&gt;
&#20851;&#20110;&#27491;&#21017;&#21270;&#39044;&#26399;&#22870;&#21169;&#20248;&#21270;&#30340;&#38543;&#26426;&#65288;&#26041;&#24046;&#20943;&#23567;&#65289;&#36817;&#31471;&#26799;&#24230;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Stochastic (Variance-Reduced) Proximal Gradient Method for Regularized Expected Reward Optimization. (arXiv:2401.12508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#27491;&#21017;&#21270;&#39044;&#26399;&#22870;&#21169;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26041;&#24046;&#20943;&#23567;&#30340;&#25216;&#26415;&#20197;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#36798;&#21040;$O(\epsilon^{-3})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#38750;&#26126;&#26174;&#35774;&#32622;&#20013;&#30340;&#27491;&#21017;&#21270;&#39044;&#26399;&#22870;&#21169;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#28085;&#30422;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35768;&#22810;&#29616;&#26377;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#26679;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#24212;&#29992;&#21644;&#20998;&#26512;&#20102;&#32463;&#20856;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#24050;&#32463;&#35777;&#26126;&#22312;&#26631;&#20934;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#36798;&#21040;$O(\epsilon^{-4})$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26469;&#23547;&#25214;$\epsilon$-&#31283;&#23450;&#28857;&#12290;&#30001;&#20110;&#32463;&#20856;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#36890;&#24120;&#36739;&#22823;&#65292;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38543;&#26426;&#26041;&#24046;&#20943;&#23567;&#36817;&#31471;&#26799;&#24230;&#27861;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#22522;&#20110;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#27010;&#29575;&#26799;&#24230;&#20272;&#35745;&#22120; (PAGE)&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#24212;&#29992;&#20195;&#34920;&#20102;&#22312;&#35299;&#20915;&#19968;&#33324;&#30340;&#27491;&#21017;&#21270;&#22870;&#21169;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#39069;&#22806;&#26465;&#20214;&#19979;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#20174;$O(\epsilon^{-4})$&#25552;&#39640;&#21040;$O(\epsilon^{-3})$&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a regularized expected reward optimization problem in the non-oblivious setting that covers many existing problems in reinforcement learning (RL). In order to solve such an optimization problem, we apply and analyze the classical stochastic proximal gradient method. In particular, the method has shown to admit an $O(\epsilon^{-4})$ sample complexity to an $\epsilon$-stationary point, under standard conditions. Since the variance of the classical stochastic gradient estimator is typically large which slows down the convergence, we also apply an efficient stochastic variance-reduce proximal gradient method with an importance sampling based ProbAbilistic Gradient Estimator (PAGE). To the best of our knowledge, the application of this method represents a novel approach in addressing the general regularized reward optimization problem. Our analysis shows that the sample complexity can be improved from $O(\epsilon^{-4})$ to $O(\epsilon^{-3})$ under additional conditions. Our resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Causal Bisimulation Modeling (CBM)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#26469;&#26500;&#24314;&#26368;&#23567;&#21644;&#21487;&#37325;&#29992;&#30340;&#20219;&#21153;&#29305;&#23450;&#25277;&#35937;&#12290;&#23454;&#35777;&#39564;&#35777;&#34920;&#26126;&#65292;CBM&#23398;&#20064;&#21040;&#30340;&#38544;&#24335;&#21160;&#24577;&#27169;&#22411;&#27604;&#26174;&#24335;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#24213;&#23618;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#29366;&#24577;&#25277;&#35937;&#12290;</title><link>http://arxiv.org/abs/2401.12497</link><description>&lt;p&gt;
&#26500;&#24314;&#26368;&#23567;&#21644;&#21487;&#37325;&#29992;&#30340;&#22240;&#26524;&#29366;&#24577;&#25277;&#35937;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Building Minimal and Reusable Causal State Abstractions for Reinforcement Learning. (arXiv:2401.12497v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Causal Bisimulation Modeling (CBM)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#26469;&#26500;&#24314;&#26368;&#23567;&#21644;&#21487;&#37325;&#29992;&#30340;&#20219;&#21153;&#29305;&#23450;&#25277;&#35937;&#12290;&#23454;&#35777;&#39564;&#35777;&#34920;&#26126;&#65292;CBM&#23398;&#20064;&#21040;&#30340;&#38544;&#24335;&#21160;&#24577;&#27169;&#22411;&#27604;&#26174;&#24335;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#24213;&#23618;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#29366;&#24577;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20004;&#20010;&#26399;&#26395;&#26159;&#33021;&#22815;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#24182;&#23398;&#20064;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#38382;&#39064;&#35268;&#33539;&#30340;&#31574;&#30053;&#12290;&#22312;&#22240;&#32032;&#21270;&#29366;&#24577;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#23398;&#20064;&#29366;&#24577;&#25277;&#35937;&#65292;&#21482;&#20445;&#30041;&#23398;&#20064;&#20219;&#21153;&#25152;&#38656;&#30340;&#21464;&#37327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Causal Bisimulation Modeling (CBM)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#30340;&#21160;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#33719;&#24471;&#19968;&#20010;&#26368;&#23567;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#25277;&#35937;&#12290;CBM&#21033;&#29992;&#21644;&#25913;&#36827;&#20102;&#38544;&#24335;&#24314;&#27169;&#25216;&#26415;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#39640;&#20445;&#30495;&#24230;&#30340;&#22240;&#26524;&#21160;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21516;&#19968;&#29615;&#22659;&#20013;&#20026;&#25152;&#26377;&#20219;&#21153;&#37325;&#22797;&#20351;&#29992;&#12290;&#22312;&#25805;&#20316;&#29615;&#22659;&#21644;Deepmind Control Suite&#19978;&#30340;&#23454;&#35777;&#39564;&#35777;&#34920;&#26126;&#65292;CBM&#23398;&#20064;&#21040;&#30340;&#38544;&#24335;&#21160;&#24577;&#27169;&#22411;&#27604;&#26174;&#24335;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#24213;&#23618;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#29366;&#24577;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two desiderata of reinforcement learning (RL) algorithms are the ability to learn from relatively little experience and the ability to learn policies that generalize to a range of problem specifications. In factored state spaces, one approach towards achieving both goals is to learn state abstractions, which only keep the necessary variables for learning the tasks at hand. This paper introduces Causal Bisimulation Modeling (CBM), a method that learns the causal relationships in the dynamics and reward functions for each task to derive a minimal, task-specific abstraction. CBM leverages and improves implicit modeling to train a high-fidelity causal dynamics model that can be reused for all tasks in the same environment. Empirical validation on manipulation environments and Deepmind Control Suite reveals that CBM's learned implicit dynamics models identify the underlying causal relationships and state abstractions more accurately than explicit ones. Furthermore, the derived state abstrac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35302;&#35273;&#28789;&#24039;&#24615;&#23547;&#25214;&#21644;&#25805;&#20316;&#29289;&#20307;&#30340;&#22810;&#25351;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#36890;&#36807;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#29289;&#20307;&#25628;&#32034;&#21644;&#25805;&#20316;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#27809;&#26377;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#20154;&#20063;&#33021;&#22815;&#20855;&#22791;&#31867;&#20284;&#20154;&#31867;&#30340;&#35302;&#35273;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.12496</link><description>&lt;p&gt;
DexTouch&#65306;&#23398;&#20064;&#20351;&#29992;&#35302;&#35273;&#28789;&#24039;&#24615;&#23547;&#25214;&#21644;&#25805;&#20316;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
DexTouch: Learning to Seek and Manipulate Objects with Tactile Dexterity. (arXiv:2401.12496v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35302;&#35273;&#28789;&#24039;&#24615;&#23547;&#25214;&#21644;&#25805;&#20316;&#29289;&#20307;&#30340;&#22810;&#25351;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#36890;&#36807;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#29289;&#20307;&#25628;&#32034;&#21644;&#25805;&#20316;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#27809;&#26377;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#20154;&#20063;&#33021;&#22815;&#20855;&#22791;&#31867;&#20284;&#20154;&#31867;&#30340;&#35302;&#35273;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35302;&#35273;&#33021;&#21147;&#23545;&#20110;&#29087;&#32451;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25628;&#32034;&#21644;&#25805;&#20316;&#29289;&#20307;&#12290;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#23558;&#20154;&#31867;&#30340;&#35302;&#35273;&#33021;&#21147;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#25351;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#26088;&#22312;&#21033;&#29992;&#35302;&#35273;&#24863;&#21463;&#22120;&#25628;&#32034;&#21644;&#25805;&#20316;&#29289;&#20307;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#12290;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#26469;&#25628;&#32034;&#38543;&#26426;&#25918;&#32622;&#30340;&#30446;&#26631;&#29289;&#20307;&#65292;&#24182;&#36827;&#34892;&#27169;&#25311;&#26085;&#24120;&#20219;&#21153;&#30340;&#29289;&#20307;&#25805;&#20316;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36171;&#20104;&#26426;&#22120;&#20154;&#31867;&#20284;&#20154;&#31867;&#30340;&#35302;&#35273;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#25163;&#30340;&#19968;&#20391;&#23454;&#29616;&#20102;&#20108;&#20540;&#35302;&#35273;&#20256;&#24863;&#22120;&#65292;&#20197;&#23613;&#37327;&#20943;&#23569;&#27169;&#25311;&#19982;&#30495;&#23454;&#29615;&#22659;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#22312;&#20223;&#30495;&#20013;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#23558;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#36716;&#31227;&#21040;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#29289;&#20307;&#25628;&#32034;&#21644;&#25805;&#20316;&#26159;&#21487;&#34892;&#30340;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sense of touch is an essential ability for skillfully performing a variety of tasks, providing the capacity to search and manipulate objects without relying on visual information. Extensive research has been conducted over time to apply these human tactile abilities to robots. In this paper, we introduce a multi-finger robot system designed to search for and manipulate objects using the sense of touch without relying on visual information. Randomly located target objects are searched using tactile sensors, and the objects are manipulated for tasks that mimic daily-life. The objective of the study is to endow robots with human-like tactile capabilities. To achieve this, binary tactile sensors are implemented on one side of the robot hand to minimize the Sim2Real gap. Training the policy through reinforcement learning in simulation and transferring the trained policy to the real environment, we demonstrate that object search and manipulation using tactile sensors is possible even in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12492</link><description>&lt;p&gt;
&#27604;&#36739;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35821;&#35328;&#24314;&#27169;&#65306;&#27169;&#25311;&#32676;&#20307;&#12289;&#20010;&#20307;&#29305;&#28857;&#36824;&#26159;&#20004;&#32773;&#20860;&#39038;&#65311;
&lt;/p&gt;
&lt;p&gt;
Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?. (arXiv:2401.12492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#23558;&#20154;&#30340;&#19978;&#19979;&#25991;&#32435;&#20837;&#20854;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20351;&#29992;&#32676;&#20307;&#23646;&#24615;&#65288;&#22914;45&#23681;&#20197;&#19978;&#30340;&#20154;&#32676;&#65289;&#36824;&#26159;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#26356;&#26377;&#25928;&#30340;&#38382;&#39064;&#23578;&#26410;&#30830;&#23450;&#12290;&#32676;&#20307;&#23646;&#24615;&#22312;&#25216;&#26415;&#19978;&#26356;&#23481;&#26131;&#23454;&#29616;&#65292;&#20294;&#26159;&#36807;&#20110;&#31895;&#31961;&#65306;&#24182;&#38750;&#25152;&#26377;45&#23681;&#20197;&#19978;&#30340;&#20154;&#37117;&#20197;&#30456;&#21516;&#30340;&#26041;&#24335;&#20070;&#20889;&#12290;&#30456;&#21453;&#65292;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#33021;&#22815;&#25429;&#25417;&#27599;&#20010;&#20154;&#36523;&#20221;&#30340;&#22797;&#26434;&#24615;&#65292;&#20801;&#35768;&#26356;&#20010;&#24615;&#21270;&#30340;&#34920;&#31034;&#65292;&#20294;&#25105;&#20204;&#21487;&#33021;&#38656;&#35201;&#27169;&#25311;&#26080;&#38480;&#25968;&#37327;&#30340;&#29992;&#25143;&#24182;&#19988;&#38656;&#35201;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36890;&#36807;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#23558;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#30340;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#65288;&#22914;&#24180;&#40836;&#20272;&#35745;&#25110;&#20010;&#24615;&#35780;&#20272;&#65289;&#30340;&#24615;&#33021;&#12290;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#65288;&#22914;&#31435;&#22330;&#21644;&#20027;&#39064;&#26816;&#27979;&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing has made progress in incorporating human context into its models, but whether it is more effective to use group-wise attributes (e.g., over-45-year-olds) or model individuals remains open. Group attributes are technically easier but coarse: not all 45-year-olds write the same way. In contrast, modeling individuals captures the complexity of each person's identity. It allows for a more personalized representation, but we may have to model an infinite number of users and require data that may be impossible to get. We compare modeling human context via group attributes, individual users, and combined approaches. Combining group and individual features significantly benefits user-level regression tasks like age estimation or personality assessment from a user's documents. Modeling individual users significantly improves the performance of single document-level classification tasks like stance and topic detection. We also find that individual-user modeling does w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26377;&#38480;&#24046;&#20998;&#27531;&#24046;&#32422;&#26463;&#30340;&#27874;&#21160;&#26041;&#31243;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#12289;&#20302;&#25104;&#26412;&#21644;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#20256;&#32479;&#30340;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.12489</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#24046;&#20998;&#27531;&#24046;&#32422;&#26463;&#25439;&#22833;&#30340;&#27874;&#21160;&#26041;&#31243;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning Method for the Wave Equation Based on Finite Difference Residual Constraints Loss. (arXiv:2401.12489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26377;&#38480;&#24046;&#20998;&#27531;&#24046;&#32422;&#26463;&#30340;&#27874;&#21160;&#26041;&#31243;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#12289;&#20302;&#25104;&#26412;&#21644;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#20256;&#32479;&#30340;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27874;&#21160;&#26041;&#31243;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#29289;&#29702;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#22312;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#21152;&#36895;&#25110;&#26367;&#20195;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#20013;&#35299;&#20915;&#27874;&#21160;&#26041;&#31243;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#30528;&#39640;&#25968;&#25454;&#33719;&#21462;&#25104;&#26412;&#12289;&#20302;&#35757;&#32451;&#25928;&#29575;&#20197;&#21450;&#23545;&#36793;&#30028;&#26465;&#20214;&#30340;&#19981;&#20805;&#20998;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#24046;&#20998;&#27531;&#24046;&#32422;&#26463;&#30340;&#27874;&#21160;&#26041;&#31243;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;&#32593;&#26684;&#21644;&#26377;&#38480;&#24046;&#20998;&#26041;&#27861;&#30340;&#26032;&#22411;&#26377;&#38480;&#24046;&#20998;&#27531;&#24046;&#32422;&#26463;&#65292;&#20197;&#21450;&#19968;&#31181;&#26080;&#30417;&#30563;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#26080;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#24182;&#39044;&#27979;&#27874;&#30340;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26377;&#38480;&#24046;&#20998;&#27531;&#24046;&#32422;&#26463;&#30456;&#23545;&#20110;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#32422;&#26463;&#30340;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#26356;&#20302;&#30340;&#25311;&#21512;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The wave equation is an important physical partial differential equation, and in recent years, deep learning has shown promise in accelerating or replacing traditional numerical methods for solving it. However, existing deep learning methods suffer from high data acquisition costs, low training efficiency, and insufficient generalization capability for boundary conditions. To address these issues, this paper proposes an unsupervised learning method for the wave equation based on finite difference residual constraints. We construct a novel finite difference residual constraint based on structured grids and finite difference methods, as well as an unsupervised training strategy, enabling convolutional neural networks to train without data and predict the forward propagation process of waves. Experimental results show that finite difference residual constraints have advantages over physics-informed neural networks (PINNs) type physical information constraints, such as easier fitting, lowe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#32477;&#28909;&#37327;&#23376;&#26041;&#27861;&#65292;&#19982;&#32463;&#20856;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26102;&#38388;&#22797;&#26434;&#24230;&#19978;&#21462;&#24471;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19982;&#32463;&#20856;&#26041;&#27861;&#30456;&#24403;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12485</link><description>&lt;p&gt;
&#32477;&#28909;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Adiabatic Quantum Support Vector Machines. (arXiv:2401.12485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#32477;&#28909;&#37327;&#23376;&#26041;&#27861;&#65292;&#19982;&#32463;&#20856;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26102;&#38388;&#22797;&#26434;&#24230;&#19978;&#21462;&#24471;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19982;&#32463;&#20856;&#26041;&#27861;&#30456;&#24403;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#26426;&#21487;&#20197;&#35299;&#20915;&#22256;&#38590;&#30340;&#20248;&#21270;&#38382;&#39064;&#65288;&#20363;&#22914;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;&#38382;&#39064;&#65289;&#65292;&#24182;&#19988;&#23427;&#20204;&#20284;&#20046;&#38750;&#24120;&#36866;&#21512;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#32477;&#28909;&#37327;&#23376;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#37327;&#23376;&#26041;&#27861;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#27604;&#32463;&#20856;&#26041;&#27861;&#22909;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;Iris&#65292;Wisconsin&#20083;&#33146;&#30284;&#65288;WBC&#65289;&#65292;Wine&#65292;Digits&#21644;Lambeq&#65289;&#19978;&#23558;&#25105;&#20204;&#30340;&#37327;&#23376;&#26041;&#27861;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#19982;&#20351;&#29992;Python&#20013;&#30340;Scikit-learn&#24211;&#30340;&#32463;&#20856;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#37327;&#23376;&#26041;&#27861;&#33719;&#24471;&#20102;&#19982;&#32463;&#20856;&#26041;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;&#65292;&#20854;&#20013;&#25105;&#20204;&#35745;&#31639;&#20102;&#37327;&#23376;&#26041;&#27861;&#21644;&#32463;&#20856;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#29305;&#24449;&#25968;&#37327;&#21644;&#25968;&#25454;&#28857;&#25968;&#37327;&#22686;&#21152;&#26102;&#30340;&#24635;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#21487;&#25193;&#23637;&#24615;&#32467;&#26524;&#26174;&#31034;&#65292;&#37327;&#23376;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adiabatic quantum computers can solve difficult optimization problems (e.g., the quadratic unconstrained binary optimization problem), and they seem well suited to train machine learning models. In this paper, we describe an adiabatic quantum approach for training support vector machines. We show that the time complexity of our quantum approach is an order of magnitude better than the classical approach. Next, we compare the test accuracy of our quantum approach against a classical approach that uses the Scikit-learn library in Python across five benchmark datasets (Iris, Wisconsin Breast Cancer (WBC), Wine, Digits, and Lambeq). We show that our quantum approach obtains accuracies on par with the classical approach. Finally, we perform a scalability study in which we compute the total training times of the quantum approach and the classical approach with increasing number of features and number of data points in the training dataset. Our scalability results show that the quantum approa
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23567;&#25209;&#37327;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#26368;&#22823;&#21270;&#38750;&#36127;&#21333;&#35843;&#21487;&#20998;&#35299;&#30340;&#23376;&#27169;&#20989;&#25968;F&#65292;&#35813;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#27604;&#22522;&#20110;&#31232;&#30095;&#21270;&#26041;&#27861;&#30340;&#20570;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.12478</link><description>&lt;p&gt;
&#23567;&#25209;&#37327;&#23376;&#27169;&#26368;&#22823;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mini-batch Submodular Maximization. (arXiv:2401.12478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12478
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23567;&#25209;&#37327;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#26368;&#22823;&#21270;&#38750;&#36127;&#21333;&#35843;&#21487;&#20998;&#35299;&#30340;&#23376;&#27169;&#20989;&#25968;F&#65292;&#35813;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#27604;&#22522;&#20110;&#31232;&#30095;&#21270;&#26041;&#27861;&#30340;&#20570;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#22312;&#19968;&#32452;&#32422;&#26463;&#26465;&#20214;&#19979;&#26368;&#22823;&#21270;&#19968;&#20010;&#38750;&#36127;&#21333;&#35843;&#21487;&#20998;&#35299;&#30340;&#23376;&#27169;&#20989;&#25968;F&#30340;&#23567;&#25209;&#37327;&#31639;&#27861;&#65292;&#20854;&#20013;F&#31561;&#20110;$f^i$&#30340;&#21644;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#37117;&#36229;&#36234;&#20102;&#22522;&#20110;&#31232;&#30095;&#21270;&#26041;&#27861;&#30340;&#20570;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#29983;&#25104;&#30340;&#35299;&#27604;&#22522;&#20110;&#31232;&#30095;&#21270;&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#35201;&#22909;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the first mini-batch algorithm for maximizing a non-negative monotone decomposable submodular function, $F=\sum_{i=1}^N f^i$, under a set of constraints. We improve over the sparsifier based approach both in theory and in practice. We experimentally observe that our algorithm generates solutions that are far superior to those generated by the sparsifier based approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#32467;&#26500;&#20445;&#25345;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32479;&#35745;&#30456;&#20851;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#19988;&#36890;&#36807;&#23558;&#32467;&#26500;&#20445;&#25345;&#26041;&#27861;&#32435;&#20837;&#26694;&#26550;&#20013;&#65292;&#25552;&#20379;&#20102;&#23545;&#39640;&#32500;&#31995;&#32479;&#30340;&#39640;&#25928;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2401.12476</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#38477;&#38454;&#24314;&#27169;&#36827;&#34892;&#36125;&#21494;&#26031;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#35782;&#21035;&#21644;&#22810;&#39033;&#24335;&#22122;&#22768; (arXiv:2401.12476v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
Bayesian identification of nonseparable Hamiltonians with multiplicative noise using deep learning and reduced-order modeling. (arXiv:2401.12476v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#32467;&#26500;&#20445;&#25345;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32479;&#35745;&#30456;&#20851;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#19988;&#36890;&#36807;&#23558;&#32467;&#26500;&#20445;&#25345;&#26041;&#27861;&#32435;&#20837;&#26694;&#26550;&#20013;&#65292;&#25552;&#20379;&#20102;&#23545;&#39640;&#32500;&#31995;&#32479;&#30340;&#39640;&#25928;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#25345;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20351;&#29992;&#38543;&#26426;&#21160;&#21147;&#27169;&#22411;&#30340;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20801;&#35768;&#32479;&#35745;&#30456;&#20851;&#30340;&#65292;&#30690;&#37327;&#20540;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#27979;&#37327;&#22122;&#22768;&#12290;&#35813;&#26041;&#27861;&#30001;&#19977;&#20010;&#20027;&#35201;&#26041;&#38754;&#32452;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#36125;&#21494;&#26031;&#21518;&#39564;&#20013;&#30340;&#20284;&#28982;&#20989;&#25968;&#25152;&#38656;&#30340;&#32479;&#35745;&#30456;&#20851;&#30340;&#65292;&#30690;&#37327;&#20540;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#27169;&#22411;&#30340;&#39640;&#26031;&#28388;&#27874;&#22120;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#39640;&#32500;&#31995;&#32479;&#36827;&#34892;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#31995;&#32479;&#35782;&#21035;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#32467;&#26500;&#20445;&#25345;&#26041;&#27861;&#32435;&#20837;&#25152;&#25552;&#35758;&#30340;&#26694;&#26550;&#20013;&#65292;&#20351;&#29992;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#31995;&#32479;&#20316;&#20026;&#19968;&#20010;&#20030;&#20363;&#30340;&#31995;&#32479;&#31867;&#21035;&#12290;&#25105;&#20204;&#23558;&#36125;&#21494;&#26031;&#26041;&#27861;&#19982;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#19968;&#20010;&#20856;&#22411;&#30340;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#27169;&#22411;&#21644;&#24102;&#26377;&#23567;&#22411;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#28151;&#27788;&#21452;&#25670;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
This paper presents a structure-preserving Bayesian approach for learning nonseparable Hamiltonian systems using stochastic dynamic models allowing for statistically-dependent, vector-valued additive and multiplicative measurement noise. The approach is comprised of three main facets. First, we derive a Gaussian filter for a statistically-dependent, vector-valued, additive and multiplicative noise model that is needed to evaluate the likelihood within the Bayesian posterior. Second, we develop a novel algorithm for cost-effective application of Bayesian system identification to high-dimensional systems. Third, we demonstrate how structure-preserving methods can be incorporated into the proposed framework, using nonseparable Hamiltonians as an illustrative system class. We compare the Bayesian method to a state-of-the-art machine learning method on a canonical nonseparable Hamiltonian model and a chaotic double pendulum model with small, noisy training datasets. The results show that us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Ditto&#30340;&#35282;&#33394;&#25198;&#28436;&#33258;&#25105;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35282;&#33394;&#30693;&#35782;&#30340;&#21033;&#29992;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Ditto&#22312;&#35282;&#33394;&#25198;&#28436;&#22522;&#20934;&#21644;MT-Bench&#30340;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.12474</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#25152;&#26377;&#23383;&#31526;&#30340;&#21472;&#21152;&#65306;&#36890;&#36807;&#33258;&#25105;&#23545;&#40784;&#23454;&#29616;&#20219;&#24847;&#35282;&#33394;&#25198;&#28436;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment. (arXiv:2401.12474v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Ditto&#30340;&#35282;&#33394;&#25198;&#28436;&#33258;&#25105;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35282;&#33394;&#30693;&#35782;&#30340;&#21033;&#29992;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Ditto&#22312;&#35282;&#33394;&#25198;&#28436;&#22522;&#20934;&#21644;MT-Bench&#30340;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#24037;&#20316;&#24050;&#32463;&#25237;&#20837;&#21040;&#36890;&#36807;&#27169;&#25311;&#19987;&#26377;&#23545;&#25163;&#26469;&#22686;&#24378;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;LLMs&#26412;&#36136;&#19978;&#20855;&#26377;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#24191;&#27867;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#34164;&#21547;&#20102;&#23545;&#35282;&#33394;&#21644;&#28508;&#22312;&#23545;&#35805;&#30340;&#24191;&#27867;&#20102;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Ditto&#65292;&#19968;&#31181;&#29992;&#20110;&#35282;&#33394;&#25198;&#28436;&#30340;&#33258;&#25105;&#23545;&#40784;&#26041;&#27861;&#12290;Ditto&#21033;&#29992;&#35282;&#33394;&#30693;&#35782;&#65292;&#40723;&#21169;LLM&#25353;&#29031;&#25351;&#20196;&#27169;&#25311;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#65292;&#20316;&#20026;&#38405;&#35835;&#29702;&#35299;&#30340;&#19968;&#31181;&#21464;&#20307;&#12290;&#35813;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#20010;&#23383;&#31526;&#30340;&#35282;&#33394;&#25198;&#28436;&#35757;&#32451;&#38598;&#65292;&#30456;&#23545;&#20110;&#30446;&#21069;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#35282;&#33394;&#25968;&#37327;&#22686;&#21152;&#20102;&#21313;&#20493;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#12290;&#36890;&#36807;&#35780;&#20272;&#25105;&#20204;&#31934;&#24515;&#26500;&#24314;&#19988;&#21487;&#22797;&#21046;&#30340;&#35282;&#33394;&#25198;&#28436;&#22522;&#20934;&#21644;MT-Bench&#30340;&#35282;&#33394;&#25198;&#28436;&#23376;&#38598;&#65292;Ditto&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Considerable efforts have been invested in augmenting the role-playing proficiency of open-source large language models (LLMs) by emulating proprietary counterparts. Nevertheless, we posit that LLMs inherently harbor role-play capabilities, owing to the extensive knowledge of characters and potential dialogues ingrained in their vast training corpora. Thus, in this study, we introduce Ditto, a self-alignment method for role-play. Ditto capitalizes on character knowledge, encouraging an instruction-following LLM to simulate role-play dialogues as a variant of reading comprehension. This method creates a role-play training set comprising 4,000 characters, surpassing the scale of currently available datasets by tenfold regarding the number of roles. Subsequently, we fine-tune the LLM using this self-generated dataset to augment its role-playing capabilities. Upon evaluating our meticulously constructed and reproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in var
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#23492;&#23384;&#22120;&#20998;&#37197;&#38382;&#39064;&#36716;&#21270;&#20026;&#22270;&#30528;&#33394;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;Proximal Policy Optimization&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#35299;&#20915;&#22270;&#30528;&#33394;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22270;&#30340;&#26631;&#35760;&#23545;&#27169;&#22411;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#20855;&#26377;&#26631;&#31614;&#37325;&#26032;&#25490;&#24207;&#19981;&#21464;&#24615;&#30340;&#22270;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.12470</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30528;&#33394;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#29702;&#35299;&#38750;&#26631;&#31614;&#19981;&#21464;&#34920;&#31034;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Graph Coloring: Understanding the Power and Limits of Non-Label Invariant Representations. (arXiv:2401.12470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#23492;&#23384;&#22120;&#20998;&#37197;&#38382;&#39064;&#36716;&#21270;&#20026;&#22270;&#30528;&#33394;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;Proximal Policy Optimization&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#35299;&#20915;&#22270;&#30528;&#33394;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22270;&#30340;&#26631;&#35760;&#23545;&#27169;&#22411;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#20855;&#26377;&#26631;&#31614;&#37325;&#26032;&#25490;&#24207;&#19981;&#21464;&#24615;&#30340;&#22270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23492;&#23384;&#22120;&#20998;&#37197;&#26159;&#29616;&#20195;&#32534;&#35793;&#22120;&#20013;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#22312;&#25317;&#26377;&#20960;&#20046;&#26080;&#38480;&#25968;&#37327;&#30340;&#29992;&#25143;&#21464;&#37327;&#21644;&#23569;&#37327;CPU&#23492;&#23384;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#21464;&#37327;&#20998;&#37197;&#32473;&#23492;&#23384;&#22120;&#20197;&#36991;&#20813;&#20914;&#31361;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#23558;&#23492;&#23384;&#22120;&#20998;&#37197;&#38382;&#39064;&#36716;&#21270;&#20026;&#22270;&#30528;&#33394;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;PyTorch&#21644;OpenAI Gymnasium Environments&#31561;&#25216;&#26415;&#23637;&#31034;&#20102;Proximal Policy Optimization&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#35299;&#20915;&#22270;&#30528;&#33394;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22270;&#30340;&#26631;&#35760;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#33719;&#21462;&#22270;&#30340;&#30697;&#38453;&#34920;&#31034;&#24182;&#23545;&#20854;&#36827;&#34892;&#25490;&#21015;&#26469;&#36827;&#34892;&#27979;&#35797;&#12290;&#28982;&#21518;&#27979;&#35797;&#27169;&#22411;&#22312;&#27599;&#20010;&#25490;&#21015;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#24403;&#32473;&#20986;&#21516;&#19968;&#22270;&#30340;&#37325;&#26032;&#26631;&#35760;&#26102;&#20854;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#34920;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#20855;&#26377;&#26631;&#31614;&#37325;&#26032;&#25490;&#24207;&#19981;&#21464;&#24615;&#30340;&#22270;&#34920;&#31034;&#20197;&#23454;&#29616;&#19968;&#33268;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Register allocation is one of the most important problems for modern compilers. With a practically unlimited number of user variables and a small number of CPU registers, assigning variables to registers without conflicts is a complex task. This work demonstrates the use of casting the register allocation problem as a graph coloring problem. Using technologies such as PyTorch and OpenAI Gymnasium Environments we will show that a Proximal Policy Optimization model can learn to solve the graph coloring problem. We will also show that the labeling of a graph is critical to the performance of the model by taking the matrix representation of a graph and permuting it. We then test the model's effectiveness on each of these permutations and show that it is not effective when given a relabeling of the same graph. Our main contribution lies in showing the need for label reordering invariant representations of graphs for machine learning models to achieve consistent performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#20013;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#30340;&#22810;Agent&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#31649;&#29702;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#31995;&#32479;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#24230;&#31354;&#38388;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#32422;&#26463;&#26465;&#20214;&#26102;&#33021;&#22815;&#38477;&#20302;&#38271;&#26399;&#39118;&#38505;&#21644;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.12455</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#20013;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#30340;&#22810;Agent&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-agent deep reinforcement learning with centralized training and decentralized execution for transportation infrastructure management. (arXiv:2401.12455v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12455
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#20013;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#30340;&#22810;Agent&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#31649;&#29702;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#31995;&#32479;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#24230;&#31354;&#38388;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#32422;&#26463;&#26465;&#20214;&#26102;&#33021;&#22815;&#38477;&#20302;&#38271;&#26399;&#39118;&#38505;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;Agent&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20869;&#36827;&#34892;&#31649;&#29702;&#12290;&#36825;&#31181;&#24037;&#31243;&#31995;&#32479;&#30340;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#26159;&#19968;&#20010;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#36866;&#24403;&#30340;&#39034;&#24207;&#26816;&#26597;&#21644;&#32500;&#25252;&#20915;&#31574;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#32422;&#26463;&#26465;&#20214;&#26102;&#38477;&#20302;&#38271;&#26399;&#39118;&#38505;&#21644;&#25104;&#26412;&#65292;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#21644;&#32422;&#26463;&#26465;&#20214;&#23384;&#22312;&#20110;&#39640;&#32500;&#31354;&#38388;&#20013;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#38745;&#24577;&#30340;&#22522;&#20110;&#24180;&#40836;&#25110;&#26465;&#20214;&#30340;&#32500;&#25252;&#26041;&#27861;&#21644;&#22522;&#20110;&#39118;&#38505;&#25110;&#23450;&#26399;&#26816;&#26597;&#35745;&#21010;&#20027;&#35201;&#35299;&#20915;&#20102;&#36825;&#31867;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#26041;&#27861;&#19979;&#65292;&#20248;&#21270;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#38480;&#21046;&#32463;&#24120;&#26174;&#29616;&#20986;&#26469;&#12290;&#26412;&#24037;&#20316;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#20197;&#32422;&#26463;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(POMDPs)&#26694;&#26550;&#20026;&#22522;&#30784;&#65292;&#20026;&#20855;&#26377;&#35266;&#23519;&#19981;&#30830;&#23450;&#24615;&#12289;&#39118;&#38505;&#32771;&#34385;&#21644;&#38543;&#26426;&#39034;&#24207;&#20915;&#31574;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#32508;&#21512;&#30340;&#25968;&#23398;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a multi-agent Deep Reinforcement Learning (DRL) framework for managing large transportation infrastructure systems over their life-cycle. Life-cycle management of such engineering systems is a computationally intensive task, requiring appropriate sequential inspection and maintenance decisions able to reduce long-term risks and costs, while dealing with different uncertainties and constraints that lie in high-dimensional spaces. To date, static age- or condition-based maintenance methods and risk-based or periodic inspection plans have mostly addressed this class of optimization problems. However, optimality, scalability, and uncertainty limitations are often manifested under such approaches. The optimization problem in this work is cast in the framework of constrained Partially Observable Markov Decision Processes (POMDPs), which provides a comprehensive mathematical basis for stochastic sequential decision settings with observation uncertainties, risk considerations, and l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#23884;&#20837;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#27880;&#20876;&#21644;&#36816;&#34892;&#26102;&#35828;&#35805;&#20154;&#35782;&#21035;&#27169;&#22411;&#32806;&#21512;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20849;&#20139;&#35828;&#35805;&#20154;&#23884;&#20837;&#31354;&#38388;&#20013;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#35745;&#31639;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12440</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#32806;&#27880;&#20876;&#21644;&#36816;&#34892;&#26102;&#35828;&#35805;&#20154;&#35782;&#21035;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#23884;&#20837;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Post-Training Embedding Alignment for Decoupling Enrollment and Runtime Speaker Recognition Models. (arXiv:2401.12440v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#23884;&#20837;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#27880;&#20876;&#21644;&#36816;&#34892;&#26102;&#35828;&#35805;&#20154;&#35782;&#21035;&#27169;&#22411;&#32806;&#21512;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20849;&#20139;&#35828;&#35805;&#20154;&#23884;&#20837;&#31354;&#38388;&#20013;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#35745;&#31639;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35828;&#35805;&#20154;&#35782;&#21035;&#26159;&#20010;&#20154;&#21270;&#24191;&#27867;&#35821;&#38899;&#26381;&#21153;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20256;&#32479;&#30340;&#35828;&#35805;&#20154;&#35782;&#21035;&#31995;&#32479;&#20351;&#29992;&#23545;&#31216;&#30340;&#27880;&#20876;-&#39564;&#35777;&#26694;&#26550;&#65292;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#20174;&#27880;&#20876;&#35821;&#38899;&#20013;&#31163;&#32447;&#25552;&#21462;&#23884;&#20837;&#65292;&#21516;&#26102;&#20174;&#36816;&#34892;&#26102;&#35821;&#38899;&#20013;&#22312;&#32447;&#25552;&#21462;&#23884;&#20837;&#12290;&#30001;&#20110;&#27880;&#20876;&#21644;&#36816;&#34892;&#26102;&#30340;&#19981;&#21516;&#24773;&#20917;&#65292;&#22914;&#35745;&#31639;&#21644;&#24310;&#36831;&#32422;&#26463;&#30340;&#24046;&#24322;&#65292;&#19968;&#20123;&#24212;&#29992;&#31243;&#24207;&#20250;&#20174;&#37319;&#29992;&#19981;&#23545;&#31216;&#30340;&#27880;&#20876;-&#39564;&#35777;&#26694;&#26550;&#20013;&#33719;&#30410;&#65292;&#36825;&#31181;&#26694;&#26550;&#20351;&#29992;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#27880;&#20876;&#21644;&#36816;&#34892;&#26102;&#30340;&#23884;&#20837;&#29983;&#25104;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#31181;&#19981;&#23545;&#31216;&#30340;&#35828;&#35805;&#20154;&#35782;&#21035;&#65292;&#20854;&#20013;&#30340;&#20004;&#20010;&#27169;&#22411;&#21487;&#20197;&#29420;&#31435;&#26356;&#26032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#23558;&#20004;&#20010;&#29420;&#31435;&#27169;&#22411;&#30340;&#23884;&#20837;&#26144;&#23556;&#21040;&#20849;&#20139;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20849;&#20139;&#35828;&#35805;&#20154;&#36923;&#36753;&#31354;&#38388;&#20013;&#26174;&#33879;&#20248;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#35745;&#31639;&#65292;&#36825;&#20123;&#35745;&#31639;&#26159;&#20351;&#29992;&#21453;&#21521;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated speaker identification (SID) is a crucial step for the personalization of a wide range of speech-enabled services. Typical SID systems use a symmetric enrollment-verification framework with a single model to derive embeddings both offline for voice profiles extracted from enrollment utterances, and online from runtime utterances. Due to the distinct circumstances of enrollment and runtime, such as different computation and latency constraints, several applications would benefit from an asymmetric enrollment-verification framework that uses different models for enrollment and runtime embedding generation. To support this asymmetric SID where each of the two models can be updated independently, we propose using a lightweight neural network to map the embeddings from the two independent models to a shared speaker embedding space. Our results show that this approach significantly outperforms cosine scoring in a shared speaker logit space for models that were trained with a contra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#23433;&#20840;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#35786;&#26029;COVID-19&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#35774;&#22791;&#19978;&#36827;&#34892;&#26412;&#22320;&#25968;&#25454;&#26679;&#26412;&#30340;&#31639;&#27861;&#35757;&#32451;&#65292;&#26080;&#38656;&#25968;&#25454;&#20849;&#20139;&#12290;&#35813;&#27169;&#22411;&#22312;&#33016;&#37096;X&#20809;&#35786;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#24182;&#35299;&#20915;&#20102;HIPAA&#21512;&#35268;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.12438</link><description>&lt;p&gt;
&#20351;&#29992;&#23433;&#20840;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#35786;&#26029;COVID-19
&lt;/p&gt;
&lt;p&gt;
Secure Federated Learning Approaches to Diagnosing COVID-19. (arXiv:2401.12438v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#23433;&#20840;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#35786;&#26029;COVID-19&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#35774;&#22791;&#19978;&#36827;&#34892;&#26412;&#22320;&#25968;&#25454;&#26679;&#26412;&#30340;&#31639;&#27861;&#35757;&#32451;&#65292;&#26080;&#38656;&#25968;&#25454;&#20849;&#20139;&#12290;&#35813;&#27169;&#22411;&#22312;&#33016;&#37096;X&#20809;&#35786;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#24182;&#35299;&#20915;&#20102;HIPAA&#21512;&#35268;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#27969;&#34892;&#30149;&#20984;&#26174;&#20102;&#22312;&#21307;&#38498;&#29615;&#22659;&#20013;&#20934;&#30830;&#35786;&#26029;COVID-19&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#36825;&#26041;&#38754;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22522;&#20110;&#33016;&#37096;X&#20809;&#29255;&#23558;COVID-19&#19982;&#20854;&#20182;&#21628;&#21560;&#36947;&#30142;&#30149;&#21306;&#20998;&#24320;&#26469;&#65292;&#36825;&#36824;&#21463;HIPAA&#21512;&#35268;&#38480;&#21046;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#38480;&#21046;&#38480;&#21046;&#20102;&#23545;&#24739;&#32773;X&#20809;&#29255;&#30340;&#27604;&#36739;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31526;&#21512;HIPAA&#21512;&#35268;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#21327;&#21161;&#35786;&#26029;COVID-19&#65292;&#21033;&#29992;&#20102;&#32852;&#37030;&#23398;&#20064;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#22810;&#20010;&#20998;&#25955;&#35774;&#22791;&#19978;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#31639;&#27861;&#35757;&#32451;&#65292;&#26080;&#38656;&#25968;&#25454;&#20849;&#20139;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#33016;&#37096;X&#20809;&#35786;&#26029;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#35813;&#39046;&#22495;&#30340;&#30693;&#21517;&#31454;&#36187;&#20013;&#30340;&#39046;&#20808;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#29305;&#23450;&#21307;&#38498;&#25968;&#25454;&#30340;&#33258;&#24049;&#30340;&#27169;&#22411;&#12290;&#32771;&#34385;&#21040;&#27169;&#22411;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#36816;&#20316;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20559;&#20506;&#25968;&#25454;&#26356;&#26032;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent pandemic has underscored the importance of accurately diagnosing COVID-19 in hospital settings. A major challenge in this regard is differentiating COVID-19 from other respiratory illnesses based on chest X-rays, compounded by the restrictions of HIPAA compliance which limit the comparison of patient X-rays. This paper introduces a HIPAA-compliant model to aid in the diagnosis of COVID-19, utilizing federated learning. Federated learning is a distributed machine learning approach that allows for algorithm training across multiple decentralized devices using local data samples, without the need for data sharing. Our model advances previous efforts in chest X-ray diagnostic models. We examined leading models from established competitions in this domain and developed our own models tailored to be effective with specific hospital data. Considering the model's operation in a federated learning context, we explored the potential impact of biased data updates on the model's perform
&lt;/p&gt;</description></item><item><title>Wasserstein Differential Privacy&#26159;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#30340;&#26367;&#20195;DP&#26694;&#26550;&#65292;&#28385;&#36275;&#23545;&#31216;&#24615;&#21644;&#19977;&#35282;&#19981;&#31561;&#24335;&#24615;&#36136;&#65292;&#24182;&#20855;&#26377;13&#20010;&#20248;&#31168;&#24615;&#36136;&#12290;Wasserstein accountant&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#38544;&#31169;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#31283;&#23450;&#22320;&#33719;&#24471;&#38544;&#31169;&#39044;&#31639;&#12290;</title><link>http://arxiv.org/abs/2401.12436</link><description>&lt;p&gt;
Wasserstein&#24046;&#20998;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Differential Privacy. (arXiv:2401.12436v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12436
&lt;/p&gt;
&lt;p&gt;
Wasserstein Differential Privacy&#26159;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#30340;&#26367;&#20195;DP&#26694;&#26550;&#65292;&#28385;&#36275;&#23545;&#31216;&#24615;&#21644;&#19977;&#35282;&#19981;&#31561;&#24335;&#24615;&#36136;&#65292;&#24182;&#20855;&#26377;13&#20010;&#20248;&#31168;&#24615;&#36136;&#12290;Wasserstein accountant&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#38544;&#31169;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#31283;&#23450;&#22320;&#33719;&#24471;&#38544;&#31169;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#22312;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DP&#26694;&#26550;&#24182;&#19981;&#28385;&#36275;&#25104;&#20026;&#24230;&#37327;&#30340;&#25152;&#26377;&#26465;&#20214;&#65292;&#36825;&#23548;&#33268;&#23427;&#20204;&#26080;&#27861;&#25512;&#23548;&#20986;&#26356;&#22909;&#30340;&#22522;&#26412;&#31169;&#26377;&#24615;&#36136;&#65292;&#24182;&#23548;&#33268;&#36807;&#39640;&#30340;&#38544;&#31169;&#39044;&#31639;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Wasserstein&#24046;&#20998;&#38544;&#31169;&#65288;WDP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#30340;&#26367;&#20195;DP&#26694;&#26550;&#65292;&#28385;&#36275;&#23545;&#31216;&#24615;&#21644;&#19977;&#35282;&#19981;&#31561;&#24335;&#24615;&#36136;&#12290;&#25105;&#20204;&#23637;&#31034;&#24182;&#35777;&#26126;WDP&#20855;&#26377;13&#20010;&#20248;&#31168;&#24615;&#36136;&#65292;&#36825;&#20123;&#24615;&#36136;&#21487;&#20197;&#20026;WDP&#27604;&#20854;&#20182;DP&#26694;&#26550;&#34920;&#29616;&#26356;&#22909;&#25552;&#20379;&#29702;&#35770;&#25903;&#25345;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#31181;&#31216;&#20026;Wasserstein&#26426;&#21046;&#30340;&#36890;&#29992;&#38544;&#31169;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#24471;WDP&#21487;&#20197;&#24212;&#29992;&#20110;&#21253;&#21547;&#23376;&#37319;&#26679;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22330;&#26223;&#12290;&#22522;&#26412;&#26426;&#21046;&#12289;&#32452;&#21512;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30001;Wasserstein&#26426;&#21046;&#24471;&#21040;&#30340;&#38544;&#31169;&#39044;&#31639;&#30456;&#23545;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential privacy (DP) has achieved remarkable results in the field of privacy-preserving machine learning. However, existing DP frameworks do not satisfy all the conditions for becoming metrics, which prevents them from deriving better basic private properties and leads to exaggerated values on privacy budgets. We propose Wasserstein differential privacy (WDP), an alternative DP framework to measure the risk of privacy leakage, which satisfies the properties of symmetry and triangle inequality. We show and prove that WDP has 13 excellent properties, which can be theoretical supports for the better performance of WDP than other DP frameworks. In addition, we derive a general privacy accounting method called Wasserstein accountant, which enables WDP to be applied in stochastic gradient descent (SGD) scenarios containing sub-sampling. Experiments on basic mechanisms, compositions and deep learning show that the privacy budgets obtained by Wasserstein accountant are relatively stable a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23545;&#32454;&#32990;&#22806;&#38388;&#38553;&#20013;&#20998;&#23376;&#20256;&#36755;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23545;&#20998;&#23376;&#20256;&#36755;&#24418;&#24335;&#19981;&#28165;&#26970;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#35745;&#31639;&#25193;&#25955;&#31995;&#25968;&#21644;&#20998;&#23376;&#36895;&#24230;&#30340;&#20248;&#21270;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12435</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23545;&#32454;&#32990;&#22806;&#38388;&#38553;&#20013;&#30340;&#20998;&#23376;&#20256;&#36755;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Quantitative Analysis of Molecular Transport in the Extracellular Space Using Physics-Informed Neural Network. (arXiv:2401.12435v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23545;&#32454;&#32990;&#22806;&#38388;&#38553;&#20013;&#20998;&#23376;&#20256;&#36755;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23545;&#20998;&#23376;&#20256;&#36755;&#24418;&#24335;&#19981;&#28165;&#26970;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#35745;&#31639;&#25193;&#25955;&#31995;&#25968;&#21644;&#20998;&#23376;&#36895;&#24230;&#30340;&#20248;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#30340;&#32454;&#32990;&#22806;&#38388;&#38553; (ECS)&#26159;&#20301;&#20110;&#32454;&#32990;&#20043;&#38388;&#25110;&#32454;&#32990;&#19982;&#34880;&#31649;&#20043;&#38388;&#30340;&#19981;&#35268;&#21017;&#12289;&#26497;&#20854;&#36802;&#22238;&#30340;&#32435;&#31859;&#32423;&#31354;&#38388;&#65292;&#23545;&#31070;&#32463;&#32454;&#32990;&#30340;&#29983;&#23384;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#22312;&#35760;&#24518;&#12289;&#24773;&#32490;&#21644;&#24863;&#35273;&#31561;&#39640;&#32423;&#33041;&#21151;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;ECS&#20869;&#20998;&#23376;&#20256;&#36755;&#30340;&#20855;&#20307;&#24418;&#24335;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476; (PINN) &#35299;&#20915;&#20174;&#23545;&#27969;-&#25193;&#25955;&#26041;&#31243; (ADE) &#23548;&#20986;&#30340;&#19968;&#20010;&#36870;&#38382;&#39064;&#65292;&#23450;&#37327;&#20998;&#26512;ECS&#20869;&#30340;&#20998;&#23376;&#20256;&#36755;&#12290;PINN&#20026;ADE&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#25968;&#23398;&#20844;&#24335;&#25110;&#32593;&#26684;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;PINN&#30340;&#20248;&#21270;&#21151;&#33021;&#21487;&#33258;&#21160;&#35745;&#31639;&#20915;&#23450;&#38271;&#26399;&#20998;&#23376;&#20256;&#36755;&#30340;&#25193;&#25955;&#31995;&#25968;&#21644;&#30001;&#23545;&#27969;&#39537;&#21160;&#30340;&#20998;&#23376;&#36895;&#24230;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20801;&#35768;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The brain extracellular space (ECS), an irregular, extremely tortuous nanoscale space located between cells or between cells and blood vessels, is crucial for nerve cell survival. It plays a pivotal role in high-level brain functions such as memory, emotion, and sensation. However, the specific form of molecular transport within the ECS remain elusive. To address this challenge, this paper proposes a novel approach to quantitatively analyze the molecular transport within the ECS by solving an inverse problem derived from the advection-diffusion equation (ADE) using a physics-informed neural network (PINN). PINN provides a streamlined solution to the ADE without the need for intricate mathematical formulations or grid settings. Additionally, the optimization of PINN facilitates the automatic computation of the diffusion coefficient governing long-term molecule transport and the velocity of molecules driven by advection. Consequently, the proposed method allows for the quantitative analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27010;&#24565;&#30340;&#39057;&#29575;&#65292;&#24182;&#21457;&#29616;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2401.12425</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#24573;&#35270;&#30340;&#23614;&#37096;
&lt;/p&gt;
&lt;p&gt;
The Neglected Tails of Vision-Language Models. (arXiv:2401.12425v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27010;&#24565;&#30340;&#39057;&#29575;&#65292;&#24182;&#21457;&#29616;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#38646;&#26679;&#26412;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#27010;&#24565;&#19978;&#30340;&#34920;&#29616;&#26497;&#19981;&#22343;&#34913;&#12290;&#20363;&#22914;&#65292;&#23613;&#31649;CLIP&#22312;ImageNet&#19978;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24179;&#22343;&#38646;&#26679;&#26412;&#20934;&#30830;&#29575;&#65288;72.7&#65285;&#65289;&#65292;&#20294;&#22312;&#21313;&#20010;&#27010;&#24565;&#65288;&#22914;gyromitra&#21644;night snake&#65289;&#19978;&#30340;&#20934;&#30830;&#29575;&#19981;&#21040;10&#65285;&#65292;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#36825;&#20123;&#27010;&#24565;&#22312;VLM&#30340;&#38750;&#22343;&#34913;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#19981;&#36275;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36825;&#31181;&#19981;&#24179;&#34913;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;VLM&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#35745;&#31639;&#29305;&#23450;&#27010;&#24565;&#30340;&#39057;&#29575;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#27979;&#37327;&#27010;&#24565;&#39057;&#29575;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#24110;&#21161;&#35745;&#31639;&#21253;&#21547;&#32473;&#23450;&#27010;&#24565;&#30340;&#21516;&#20041;&#35789;&#30340;&#30456;&#20851;&#25991;&#26412;&#65292;&#24182;&#35299;&#20915;&#35821;&#35328;&#27495;&#20041;&#12290;&#25105;&#20204;&#30830;&#35748;&#20687;LAION&#36825;&#26679;&#30340;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#30830;&#23454;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#24182;&#19988;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#24403;&#20195;&#30340;&#22810;&#27169;&#24335;&#31995;&#32479;&#65292;&#22914;&#35270;&#35273;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#25991;&#26412;-&#35270;&#35273;&#25512;&#29702;&#27169;&#22411;&#65292;&#22312;&#36825;&#31181;&#38271;&#23614;&#20998;&#24067;&#19979;&#32463;&#24120;&#38590;&#20197;&#36798;&#21040;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) excel in zero-shot recognition but exhibit drastically imbalanced performance across visual concepts. For example, CLIP, despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields $&lt;$10% on ten concepts (e.g., gyromitra and night snake), presumably, because these concepts are under-represented in VLMs' imbalanced pretraining data. Yet, assessing this imbalance is challenging as it is non-trivial to calculate the frequency of specific concepts within VLMs' large-scale pretraining data. Our work makes the first attempt to measure the concept frequency by analyzing pretraining texts. We use off-the-shelf language models to help count relevant texts that contain synonyms of the given concepts and resolve linguistic ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit long-tailed concept distributions, which strongly correlate with per-class accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and text-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#31639;&#27861;DALex&#65292;&#23427;&#36890;&#36807;&#21152;&#26435;&#35757;&#32451;&#26696;&#20363;&#35823;&#24046;&#30340;&#21644;&#26469;&#36873;&#25321;&#26368;&#20339;&#20010;&#20307;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;&#35789;&#27861;&#36873;&#25321;&#26356;&#24555;&#36895;&#12290;</title><link>http://arxiv.org/abs/2401.12424</link><description>&lt;p&gt;
DALex: &#36890;&#36807;&#22810;&#26679;&#32858;&#21512;&#23454;&#29616;&#31867;&#20284;&#35789;&#27861;&#36873;&#25321;&#30340;&#36873;&#25321;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DALex: Lexicase-like Selection via Diverse Aggregation. (arXiv:2401.12424v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#31639;&#27861;DALex&#65292;&#23427;&#36890;&#36807;&#21152;&#26435;&#35757;&#32451;&#26696;&#20363;&#35823;&#24046;&#30340;&#21644;&#26469;&#36873;&#25321;&#26368;&#20339;&#20010;&#20307;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;&#35789;&#27861;&#36873;&#25321;&#26356;&#24555;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36827;&#21270;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#20010;&#39046;&#22495;&#20013;&#65292;&#35789;&#27861;&#36873;&#25321;&#34987;&#35777;&#26126;&#30456;&#27604;&#20854;&#20182;&#36873;&#25321;&#31639;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;&#35789;&#27861;&#36873;&#25321;&#22312;&#20854;&#26631;&#20934;&#24418;&#24335;&#19979;&#65292;&#26681;&#25454;&#38543;&#26426;&#39034;&#24207;&#30340;&#35757;&#32451;&#26696;&#20363;&#36827;&#34892;&#36880;&#19968;&#32771;&#34385;&#65292;&#24182;&#22522;&#20110;&#27492;&#36807;&#31243;&#23545;&#31181;&#32676;&#25110;&#20854;&#20182;&#38598;&#21512;&#36827;&#34892;&#31579;&#36873;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36880;&#27493;&#31579;&#36873;&#30340;&#36807;&#31243;&#21487;&#33021;&#20250;&#32791;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#22823;&#37327;&#35757;&#32451;&#26696;&#20363;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DALex&#65288;&#21363;&#22810;&#26679;&#32858;&#21512;&#35789;&#27861;&#36873;&#25321;&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#36873;&#25321;&#20010;&#20307;&#26041;&#38754;&#19982;&#35789;&#27861;&#36873;&#25321;&#20960;&#20046;&#31561;&#25928;&#65292;&#20294;&#36895;&#24230;&#26356;&#24555;&#12290;DALex&#26041;&#27861;&#26681;&#25454;&#21152;&#26435;&#35757;&#32451;&#26696;&#20363;&#35823;&#24046;&#30340;&#21644;&#36873;&#25321;&#26368;&#20339;&#20010;&#20307;&#65292;&#20854;&#20013;&#26435;&#37325;&#26159;&#38543;&#26426;&#37319;&#26679;&#30340;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#36873;&#25321;&#25152;&#38656;&#30340;&#26680;&#24515;&#35745;&#31639;&#24418;&#24335;&#21270;&#20026;&#30697;&#38453;&#20056;&#27861;&#65292;&#32780;&#19981;&#26159;&#36882;&#24402;&#24490;&#29615;&#27604;&#36739;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#20248;&#21270;&#21644;&#24182;&#34892;&#21270;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lexicase selection has been shown to provide advantages over other selection algorithms in several areas of evolutionary computation and machine learning. In its standard form, lexicase selection filters a population or other collection based on randomly ordered training cases that are considered one at a time. This iterated filtering process can be time-consuming, particularly in settings with large numbers of training cases. In this paper, we propose a new method that is nearly equivalent to lexicase selection in terms of the individuals that it selects, but which does so significantly more quickly. The new method, called DALex (for Diversely Aggregated Lexicase), selects the best individual with respect to a weighted sum of training case errors, where the weights are randomly sampled. This allows us to formulate the core computation required for selection as matrix multiplication instead of recursive loops of comparisons, which in turn allows us to take advantage of optimized and pa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#25913;&#36827;&#28145;&#24230;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#21644;&#19981;&#20934;&#30830;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#25552;&#20379;&#30340;&#21518;&#39564;&#36817;&#20284;&#21644;&#36793;&#32536;&#20284;&#28982;&#19979;&#30028;&#65292;&#21487;&#20197;&#20248;&#21270;&#36229;&#21442;&#25968;&#24182;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2401.12418</link><description>&lt;p&gt;
&#25913;&#36827;&#28145;&#24230;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Improved Variational Inference for Deep Bayesian Models. (arXiv:2401.12418v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#25913;&#36827;&#28145;&#24230;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#21644;&#19981;&#20934;&#30830;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#25552;&#20379;&#30340;&#21518;&#39564;&#36817;&#20284;&#21644;&#36793;&#32536;&#20284;&#28982;&#19979;&#30028;&#65292;&#21487;&#20197;&#20248;&#21270;&#36229;&#21442;&#25968;&#24182;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#65292;&#24182;&#19988;&#32473;&#20986;&#30340;&#39044;&#27979;&#19981;&#20934;&#30830;&#12290;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#35797;&#22270;&#36890;&#36807;&#32473;&#27169;&#22411;&#21442;&#25968;&#35774;&#32622;&#20808;&#39564;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#28982;&#21518;&#23558;&#20808;&#39564;&#19982;&#20284;&#28982;&#20989;&#25968;&#32467;&#21512;&#36827;&#34892;&#21518;&#39564;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#28145;&#24230;&#27169;&#22411;&#26469;&#35828;&#65292;&#30495;&#23454;&#30340;&#21518;&#39564;&#26159;&#26080;&#27861;&#35745;&#31639;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#20351;&#29992;&#36817;&#20284;&#26041;&#27861;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#20316;&#20026;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#26082;&#21487;&#20197;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#21448;&#21487;&#20197;&#25552;&#20379;&#36793;&#32536;&#20284;&#28982;&#30340;&#19979;&#30028;&#12290;&#22914;&#26524;&#19979;&#30028;&#36275;&#22815;&#32039;&#33268;&#65292;&#36825;&#20010;&#19979;&#30028;&#21487;&#20197;&#29992;&#26469;&#20248;&#21270;&#36229;&#21442;&#25968;&#21644;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#24456;&#23569;&#21463;&#21040;&#37325;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has revolutionized the last decade, being at the forefront of extraordinary advances in a wide range of tasks including computer vision, natural language processing, and reinforcement learning, to name but a few. However, it is well-known that deep models trained via maximum likelihood estimation tend to be overconfident and give poorly-calibrated predictions. Bayesian deep learning attempts to address this by placing priors on the model parameters, which are then combined with a likelihood to perform posterior inference. Unfortunately, for deep models, the true posterior is intractable, forcing the user to resort to approximations. In this thesis, we explore the use of variational inference (VI) as an approximation, as it is unique in simultaneously approximating the posterior and providing a lower bound to the marginal likelihood. If tight enough, this lower bound can be used to optimize hyperparameters and to facilitate model selection. However, this capacity has rarel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20498;&#32622;&#24402;&#19968;&#21270;&#21644;&#38543;&#26426;&#20223;&#23556;&#21464;&#25442;&#26469;&#25552;&#39640;&#20869;&#23384;&#35745;&#31639;&#26550;&#26500;&#20013;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#21644;&#25512;&#29702;&#31934;&#24230;&#30340;&#26041;&#27861;</title><link>http://arxiv.org/abs/2401.12416</link><description>&lt;p&gt;
&#22312;&#36793;&#32536;&#19978;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#65306;&#20498;&#32622;&#24402;&#19968;&#21270;&#19982;&#38543;&#26426;&#20223;&#23556;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Enhancing Reliability of Neural Networks at the Edge: Inverted Normalization with Stochastic Affine Transformations. (arXiv:2401.12416v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20498;&#32622;&#24402;&#19968;&#21270;&#21644;&#38543;&#26426;&#20223;&#23556;&#21464;&#25442;&#26469;&#25552;&#39640;&#20869;&#23384;&#35745;&#31639;&#26550;&#26500;&#20013;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#21644;&#25512;&#29702;&#31934;&#24230;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BayNNs&#65289;&#22312;&#39044;&#27979;&#20013;&#33258;&#28982;&#22320;&#25552;&#20379;&#20102;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#21512;&#36866;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#22522;&#20110;&#24518;&#38459;&#22120;&#30340;&#20869;&#23384;&#35745;&#31639;&#65288;IMC&#65289;&#26550;&#26500;&#23454;&#29616;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20351;&#20854;&#36866;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#24212;&#29992;&#12290;&#38500;&#20102;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#22806;&#65292;&#23545;&#35745;&#31639;&#20013;&#30340;&#22122;&#38899;&#20855;&#26377;&#22266;&#26377;&#30340;&#40065;&#26834;&#24615;&#20063;&#26159;&#30830;&#20445;&#21151;&#33021;&#23433;&#20840;&#30340;&#20851;&#38190;&#12290;&#29305;&#21035;&#26159;&#65292;&#22522;&#20110;&#24518;&#38459;&#22120;&#30340;IMC&#23545;&#21046;&#36896;&#21644;&#36816;&#34892;&#26102;&#30340;&#21464;&#21270;&#12289;&#28418;&#31227;&#21644;&#25925;&#38556;&#31561;&#21508;&#31181;&#38750;&#29702;&#24819;&#22240;&#32032;&#25935;&#24863;&#65292;&#36825;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;&#25512;&#29702;&#31934;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20197;&#22312;&#37096;&#32626;&#22312;IMC&#26550;&#26500;&#20013;&#30340;BayNNs&#19978;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#25512;&#29702;&#31934;&#24230;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24402;&#19968;&#21270;&#23618;&#19982;&#38543;&#26426;&#20223;&#23556;&#21464;&#25442;&#30456;&#32467;&#21512;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#20986;&#25512;&#29702;&#31934;&#24230;&#30340;&#36880;&#28176;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
Bayesian Neural Networks (BayNNs) naturally provide uncertainty in their predictions, making them a suitable choice in safety-critical applications. Additionally, their realization using memristor-based in-memory computing (IMC) architectures enables them for resource-constrained edge applications. In addition to predictive uncertainty, however, the ability to be inherently robust to noise in computation is also essential to ensure functional safety. In particular, memristor-based IMCs are susceptible to various sources of non-idealities such as manufacturing and runtime variations, drift, and failure, which can significantly reduce inference accuracy. In this paper, we propose a method to inherently enhance the robustness and inference accuracy of BayNNs deployed in IMC architectures. To achieve this, we introduce a novel normalization layer combined with stochastic affine transformations. Empirical results in various benchmark datasets show a graceful degradation in inference accurac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20165;&#26377;&#30340;&#23569;&#37327;&#24494;&#23567;&#22810;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#26469;&#22686;&#24378;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#65292;&#23454;&#29616;&#22823;&#24133;&#24230;&#30340;&#38750;&#33521;&#25991;&#25972;&#20307;&#25913;&#36827;&#65292;&#24182;&#20445;&#25345;&#33521;&#25991;&#26041;&#21521;&#19978;&#30340;&#24615;&#33021;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#25277;&#21462;&#30340;&#23569;&#37327;&#26041;&#21521;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#25972;&#20307;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.12413</link><description>&lt;p&gt;
100&#20010;&#26679;&#26412;&#21487;&#20197;&#36208;&#22810;&#36828;&#65311;&#36890;&#36807;&#24494;&#23567;&#30340;&#22810;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#35299;&#38145;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data. (arXiv:2401.12413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20165;&#26377;&#30340;&#23569;&#37327;&#24494;&#23567;&#22810;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#26469;&#22686;&#24378;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#65292;&#23454;&#29616;&#22823;&#24133;&#24230;&#30340;&#38750;&#33521;&#25991;&#25972;&#20307;&#25913;&#36827;&#65292;&#24182;&#20445;&#25345;&#33521;&#25991;&#26041;&#21521;&#19978;&#30340;&#24615;&#33021;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#25277;&#21462;&#30340;&#23569;&#37327;&#26041;&#21521;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#25972;&#20307;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#32763;&#35793;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#26088;&#22312;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#20013;&#32763;&#35793;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#12290;&#19968;&#31181;&#24120;&#35265;&#20294;&#36164;&#28304;&#28040;&#32791;&#36739;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23613;&#21487;&#33021;&#25366;&#25496;&#26356;&#22810;&#30340;&#32763;&#35793;&#26041;&#21521;&#24182;&#28155;&#21152;&#21040;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#20165;&#26377;&#30340;&#23569;&#37327;&#24494;&#23567;&#22810;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#26469;&#20248;&#21270;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#22312;EC30&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#20351;&#29992;100&#20010;&#22810;&#35821;&#35328;&#24179;&#34892;&#26679;&#26412;&#23601;&#33021;&#22815;&#23454;&#29616;+21.7 ChrF&#38750;&#33521;&#25991;&#25972;&#20307;&#25913;&#36827;&#65288;870&#20010;&#26041;&#21521;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#26041;&#21521;&#19978;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#24494;&#35843;&#25968;&#25454;&#30340;&#35268;&#27169;&#25928;&#24212;&#21644;&#20854;&#36716;&#31227;&#33021;&#21147;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#22312;&#19968;&#20010;&#23567;&#30340;&#12289;&#38543;&#26426;&#25277;&#21462;&#30340;&#26041;&#21521;&#38598;&#65288;10%&#65289;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#25972;&#20307;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25152;&#24471;&#21040;&#30340;&#38750;&#33521;&#25991;&#24615;&#33021;&#19982;&#33521;&#25991;&#24615;&#33021;&#38750;&#24120;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot translation is an open problem, aiming to translate between language pairs unseen during training in Multilingual Machine Translation (MMT). A common, albeit resource-consuming, solution is to mine as many translation directions as possible to add to the parallel corpus. In this paper, we show that the zero-shot capability of an English-centric model can be easily enhanced by fine-tuning with a very small amount of multi-parallel data. For example, on the EC30 dataset, we show that up to +21.7 ChrF non-English overall improvements (870 directions) can be achieved by using only 100 multi-parallel samples, meanwhile preserving capability in English-centric directions. We further study the size effect of fine-tuning data and its transfer capabilities. Surprisingly, our empirical analysis shows that comparable overall improvements can be achieved even through fine-tuning in a small, randomly sampled direction set (10\%). Also, the resulting non-English performance is quite close 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32447;&#24615;&#25506;&#27979;&#26657;&#20934;&#65288;LinC&#65289;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#22312;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#19978;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12406</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#24615;&#25506;&#27979;&#26657;&#20934;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing In-context Learning via Linear Probe Calibration. (arXiv:2401.12406v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32447;&#24615;&#25506;&#27979;&#26657;&#20934;&#65288;LinC&#65289;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#22312;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#19978;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33539;&#24335;&#65292;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31561;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#21253;&#21547;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#25552;&#31034;&#26469;&#20026;&#26032;&#30340;&#26597;&#35810;&#36755;&#20837;&#29983;&#25104;&#30456;&#24212;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#24212;&#29992;ICL&#26080;&#27861;&#38543;&#30528;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#65292;&#24182;&#19988;&#23545;&#19981;&#21516;&#30340;&#25552;&#31034;&#27169;&#26495;&#21644;&#28436;&#31034;&#25490;&#21015;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#20351;&#29992;ICL&#30340;GPT&#27169;&#22411;&#22522;&#20110;&#22522;&#20110;&#39321;&#20892;&#29109;&#30340;&#26032;&#24230;&#37327;&#32780;&#23548;&#33268;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32447;&#24615;&#25506;&#27979;&#26657;&#20934;&#65288;LinC&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26657;&#20934;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#20174;&#32780;&#24471;&#21040;&#21487;&#38752;&#30340;&#39044;&#27979;&#21644;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#19988;&#20165;&#38656;&#35201;&#26497;&#23569;&#37327;&#30340;&#39069;&#22806;&#26679;&#26412;&#65288;&#20165;&#38656;&#20116;&#20010;&#24050;&#26631;&#35760;&#30340;&#25968;&#25454;&#26679;&#26412;&#65289;&#12290;LinC&#26174;&#33879;&#25552;&#39640;&#20102;GPT&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;ICL&#27979;&#35797;&#24615;&#33021;&#65292;&#24179;&#22343;&#25913;&#21892;&#25928;&#26524;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) is a new paradigm for natural language processing that utilizes Generative Pre-trained Transformer (GPT)-like models. This approach uses prompts that include in-context demonstrations to generate the corresponding output for a new query input. However, applying ICL in real cases does not scale with the number of samples, and lacks robustness to different prompt templates and demonstration permutations. In this paper, we first show that GPT-like models using ICL result in unreliable predictions based on a new metric based on Shannon entropy. Then, to solve this problem, we propose a new technique called the Linear Probe Calibration (LinC), a method that calibrates the model's output probabilities, resulting in reliable predictions and improved performance, while requiring only minimal additional samples (as few as five labeled data samples). LinC significantly enhances the ICL test performance of GPT models on various benchmark datasets, with an average improve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#22235;&#25152;&#21152;&#25343;&#22823;&#20027;&#35201;&#22823;&#23398;&#30340;&#23398;&#29983;&#22312;Reddit&#19978;&#25776;&#20889;&#30340;&#24086;&#23376;&#36827;&#34892;&#32437;&#21521;&#24773;&#24863;&#20998;&#31867;&#12290;&#36890;&#36807;&#35843;&#25972;&#24773;&#24863;&#38408;&#20540;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#23558;&#24086;&#23376;&#24773;&#24863;&#20998;&#31867;&#20026;&#31215;&#26497;&#21644;&#28040;&#26497;&#31867;&#21035;&#65292;&#24182;&#19988;&#32467;&#26524;&#22312;&#19981;&#21516;&#22823;&#23398;&#25968;&#25454;&#38598;&#20013;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2401.12382</link><description>&lt;p&gt;
Reddit&#24086;&#23376;&#30340;&#32437;&#21521;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Longitudinal Sentiment Classification of Reddit Posts. (arXiv:2401.12382v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22235;&#25152;&#21152;&#25343;&#22823;&#20027;&#35201;&#22823;&#23398;&#30340;&#23398;&#29983;&#22312;Reddit&#19978;&#25776;&#20889;&#30340;&#24086;&#23376;&#36827;&#34892;&#32437;&#21521;&#24773;&#24863;&#20998;&#31867;&#12290;&#36890;&#36807;&#35843;&#25972;&#24773;&#24863;&#38408;&#20540;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#23558;&#24086;&#23376;&#24773;&#24863;&#20998;&#31867;&#20026;&#31215;&#26497;&#21644;&#28040;&#26497;&#31867;&#21035;&#65292;&#24182;&#19988;&#32467;&#26524;&#22312;&#19981;&#21516;&#22823;&#23398;&#25968;&#25454;&#38598;&#20013;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;&#23545;&#22235;&#25152;&#21152;&#25343;&#22823;&#20027;&#35201;&#22823;&#23398;&#30340;&#23398;&#29983;&#25776;&#20889;&#30340;Reddit&#24086;&#23376;&#36827;&#34892;&#32437;&#21521;&#24773;&#24863;&#20998;&#31867;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#24086;&#23376;&#30340;&#25991;&#26412;&#65292;&#37325;&#28857;&#20851;&#27880;2020&#24180;&#33267;2023&#24180;&#20043;&#38388;&#30340;&#26102;&#38388;&#12290;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#24773;&#24863;&#38408;&#20540;&#22312;[-0.075, 0.075]&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#33021;&#22815;&#23558;&#24086;&#23376;&#24773;&#24863;&#20998;&#31867;&#20026;&#31215;&#26497;&#21644;&#28040;&#26497;&#31867;&#21035;&#30340;&#20998;&#31867;&#22120;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#24773;&#24863;&#20998;&#31867;&#32467;&#26524;&#22312;&#22235;&#20010;&#22823;&#23398;&#25968;&#25454;&#38598;&#20013;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report results of a longitudinal sentiment classification of Reddit posts written by students of four major Canadian universities. We work with the texts of the posts, concentrating on the years 2020-2023. By finely tuning a sentiment threshold to a range of [-0.075,0.075], we successfully built classifiers proficient in categorizing post sentiments into positive and negative categories. Noticeably, our sentiment classification results are consistent across the four university data sets.
&lt;/p&gt;</description></item><item><title>SubgroupTE&#26159;&#19968;&#31181;&#26032;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#27169;&#22411;&#65292;&#36890;&#36807;&#23376;&#32676;&#35782;&#21035;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#23376;&#32676;&#20307;&#30340;&#27835;&#30103;&#21453;&#24212;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.12369</link><description>&lt;p&gt;
SubgroupTE: &#20511;&#21161;&#23376;&#32676;&#35782;&#21035;&#25512;&#36827;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
SubgroupTE: Advancing Treatment Effect Estimation with Subgroup Identification. (arXiv:2401.12369v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12369
&lt;/p&gt;
&lt;p&gt;
SubgroupTE&#26159;&#19968;&#31181;&#26032;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#27169;&#22411;&#65292;&#36890;&#36807;&#23376;&#32676;&#35782;&#21035;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#23376;&#32676;&#20307;&#30340;&#27835;&#30103;&#21453;&#24212;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#23545;&#20110;&#35780;&#20272;&#24178;&#39044;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#23398;&#20064;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20013;&#30340;&#21487;&#23545;&#31435;&#34920;&#31034;&#26041;&#38754;&#23637;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#20010;&#20027;&#35201;&#23616;&#38480;&#26159;&#23558;&#25972;&#20010;&#20154;&#32676;&#35270;&#20026;&#19968;&#20010;&#21516;&#36136;&#32676;&#20307;&#65292;&#24573;&#35270;&#20855;&#26377;&#19981;&#21516;&#27835;&#30103;&#25928;&#26524;&#30340;&#19981;&#21516;&#23376;&#32676;&#20307;&#30340;&#22810;&#26679;&#24615;&#12290;&#36825;&#20010;&#23616;&#38480;&#38480;&#21046;&#20102;&#20934;&#30830;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#21644;&#25552;&#20379;&#23376;&#32676;&#20307;&#29305;&#23450;&#27835;&#30103;&#24314;&#35758;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SubgroupTE&#30340;&#26032;&#39062;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20013;&#32467;&#21512;&#20102;&#23376;&#32676;&#35782;&#21035;&#12290;SubgroupTE&#36890;&#36807;&#32771;&#34385;&#23376;&#32676;&#29305;&#23450;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#35782;&#21035;&#20855;&#26377;&#19981;&#21516;&#27835;&#30103;&#21453;&#24212;&#30340;&#24322;&#36136;&#23376;&#32676;&#20307;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;SubgroupTE&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#23376;&#32676;&#20998;&#32452;&#21644;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#65292;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise estimation of treatment effects is crucial for evaluating intervention effectiveness. While deep learning models have exhibited promising performance in learning counterfactual representations for treatment effect estimation (TEE), a major limitation in most of these models is that they treat the entire population as a homogeneous group, overlooking the diversity of treatment effects across potential subgroups that have varying treatment effects. This limitation restricts the ability to precisely estimate treatment effects and provide subgroup-specific treatment recommendations. In this paper, we propose a novel treatment effect estimation model, named SubgroupTE, which incorporates subgroup identification in TEE. SubgroupTE identifies heterogeneous subgroups with different treatment responses and more precisely estimates treatment effects by considering subgroup-specific causal effects. In addition, SubgroupTE iteratively optimizes subgrouping and treatment effect estimation n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#20351;&#29992;&#19981;&#21516;&#24120;&#29992;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;sigmoid&#21644;&#21452;&#26354;&#27491;&#20999;&#65289;&#26102;&#30340;VC&#32500;&#24230;&#65292;&#37319;&#29992;&#20102;Pfaffian&#20989;&#25968;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#26550;&#26500;&#21442;&#25968;&#21644;&#21512;&#20316;&#25968;&#37327;&#25552;&#20379;&#20102;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.12362</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24102;&#26377;Pfaffian&#28608;&#27963;&#20989;&#25968;&#30340;VC&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
VC dimension of Graph Neural Networks with Pfaffian activation functions. (arXiv:2401.12362v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#20351;&#29992;&#19981;&#21516;&#24120;&#29992;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;sigmoid&#21644;&#21452;&#26354;&#27491;&#20999;&#65289;&#26102;&#30340;VC&#32500;&#24230;&#65292;&#37319;&#29992;&#20102;Pfaffian&#20989;&#25968;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#26550;&#26500;&#21442;&#25968;&#21644;&#21512;&#20316;&#25968;&#37327;&#25552;&#20379;&#20102;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36817;&#24180;&#26469;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#20986;&#29616;&#65292;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23398;&#20064;&#21508;&#31181;&#22270;&#39046;&#22495;&#30340;&#20219;&#21153;&#65307;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;GNN&#30001;&#20110;&#20854;&#19982;Weisfeiler-Lehman&#65288;WL&#65289;&#22270;&#21516;&#26500;&#27979;&#35797;&#23494;&#20999;&#30456;&#20851;&#30340;&#30452;&#35266;&#34920;&#36798;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#31561;&#20215;&#12290;&#20174;&#29702;&#35770;&#35282;&#24230;&#30475;&#65292;GNN&#34987;&#35777;&#26126;&#26159;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#24182;&#19988;&#26368;&#36817;&#23545;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65288;&#21363;&#65292;&#23545;Vapnik Cherovenikis&#65288;VC&#65289;&#32500;&#24230;&#30340;&#30028;&#38480;&#65289;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30446;&#26631;&#26159;&#23558;&#23545;GNN&#30340;VC&#32500;&#24230;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#20854;&#20182;&#24120;&#29992;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;sigmoid&#21644;&#21452;&#26354;&#27491;&#20999;&#65292;&#20351;&#29992;Pfaffian&#20989;&#25968;&#29702;&#35770;&#26694;&#26550;&#12290;&#25552;&#20379;&#20102;&#19982;&#26550;&#26500;&#21442;&#25968;&#65288;&#28145;&#24230;&#65292;&#31070;&#32463;&#20803;&#25968;&#37327;&#65292;&#36755;&#20837;&#23610;&#23544;&#65289;&#20197;&#21450;&#19982;&#21512;&#20316;&#25968;&#37327;&#26377;&#20851;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged in recent years as a powerful tool to learn tasks across a wide range of graph domains in a data-driven fashion; based on a message passing mechanism, GNNs have gained increasing popularity due to their intuitive formulation, closely linked with the Weisfeiler-Lehman (WL) test for graph isomorphism, to which they have proven equivalent. From a theoretical point of view, GNNs have been shown to be universal approximators, and their generalization capability (namely, bounds on the Vapnik Chervonekis (VC) dimension) has recently been investigated for GNNs with piecewise polynomial activation functions. The aim of our work is to extend this analysis on the VC dimension of GNNs to other commonly used activation functions, such as sigmoid and hyperbolic tangent, using the framework of Pfaffian function theory. Bounds are provided with respect to architecture parameters (depth, number of neurons, input size) as well as with respect to the number of co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35774;&#22791;&#27169;&#22411;&#26435;&#37325;&#20043;&#38388;&#30340;&#36317;&#31163;&#35780;&#20272;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;&#35774;&#22791;&#20043;&#38388;&#30340;&#32852;&#21512;&#24418;&#25104;&#65292;&#20197;&#21450;&#21033;&#29992;&#36136;&#24515;&#30340;&#27010;&#24565;&#23545;&#26469;&#33258;&#22810;&#20010;&#35774;&#22791;&#30340;&#26356;&#26032;&#36827;&#34892;&#32858;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#29289;&#32852;&#32593;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#21147;&#30340;&#32467;&#26500;&#21270;&#12289;&#20248;&#36234;&#24615;&#33021;&#21644;&#39640;&#25928;&#36890;&#20449;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.12356</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#36890;&#36807;&#22522;&#20110;&#26435;&#37325;&#30340;&#32852;&#21512;&#21160;&#21147;&#23398;&#23454;&#29616;&#39640;&#25928;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Efficient Collaborations through Weight-Driven Coalition Dynamics in Federated Learning Systems. (arXiv:2401.12356v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35774;&#22791;&#27169;&#22411;&#26435;&#37325;&#20043;&#38388;&#30340;&#36317;&#31163;&#35780;&#20272;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;&#35774;&#22791;&#20043;&#38388;&#30340;&#32852;&#21512;&#24418;&#25104;&#65292;&#20197;&#21450;&#21033;&#29992;&#36136;&#24515;&#30340;&#27010;&#24565;&#23545;&#26469;&#33258;&#22810;&#20010;&#35774;&#22791;&#30340;&#26356;&#26032;&#36827;&#34892;&#32858;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#29289;&#32852;&#32593;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#21147;&#30340;&#32467;&#26500;&#21270;&#12289;&#20248;&#36234;&#24615;&#33021;&#21644;&#39640;&#25928;&#36890;&#20449;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#26102;&#20195;&#65292;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#31361;&#20986;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#35774;&#22791;&#27169;&#22411;&#26435;&#37325;&#20043;&#38388;&#30340;&#27431;&#27663;&#36317;&#31163;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;&#36825;&#26159;&#25105;&#20204;&#31995;&#32479;&#30340;&#22522;&#30784;&#65292;&#29992;&#20110;&#26681;&#25454;&#27169;&#22411;&#26435;&#37325;&#30340;&#25509;&#36817;&#31243;&#24230;&#26469;&#25351;&#23548;&#35774;&#22791;&#20043;&#38388;&#30340;&#32852;&#21512;&#24418;&#25104;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24341;&#20837;&#36136;&#24515;&#30340;&#27010;&#24565;&#65292;&#34920;&#31034;&#27169;&#22411;&#26435;&#37325;&#30340;&#24179;&#22343;&#20540;&#65292;&#26377;&#21161;&#20110;&#23545;&#26469;&#33258;&#22810;&#20010;&#35774;&#22791;&#30340;&#26356;&#26032;&#36827;&#34892;&#32858;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;&#21516;&#36136;&#21644;&#24322;&#36136;&#25968;&#25454;&#20998;&#24067;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#24179;&#22343;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#20379;&#32467;&#26500;&#21270;&#12289;&#24615;&#33021;&#20248;&#36234;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;&#29289;&#32852;&#32593;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of the Internet of Things (IoT), decentralized paradigms for machine learning are gaining prominence. In this paper, we introduce a federated learning model that capitalizes on the Euclidean distance between device model weights to assess their similarity and disparity. This is foundational for our system, directing the formation of coalitions among devices based on the closeness of their model weights. Furthermore, the concept of a barycenter, representing the average of model weights, helps in the aggregation of updates from multiple devices. We evaluate our approach using homogeneous and heterogeneous data distribution, comparing it against traditional federated learning averaging algorithm. Numerical results demonstrate its potential in offering structured, outperformed and communication-efficient model for IoT-based machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36739;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#23454;&#29616;&#37327;&#21270;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;QA-NAS&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22359;&#29366;&#24418;&#24335;&#23454;&#29616;&#20102;&#22312;&#36793;&#32536;&#35745;&#31639;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#30456;&#23545;&#20110;DeepLabV3&#27169;&#22411;&#26356;&#23567;&#30340;FB-MP&#27169;&#22411;&#21644;&#26356;&#24555;&#30340;INT8&#27169;&#22411;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12350</link><description>&lt;p&gt;
&#22312;&#36793;&#32536;&#35745;&#31639;&#20013;&#25193;&#23637;&#37327;&#21270;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20197;&#36827;&#34892;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scaling Up Quantization-Aware Neural Architecture Search for Efficient Deep Learning on the Edge. (arXiv:2401.12350v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36739;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#23454;&#29616;&#37327;&#21270;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;QA-NAS&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22359;&#29366;&#24418;&#24335;&#23454;&#29616;&#20102;&#22312;&#36793;&#32536;&#35745;&#31639;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#30456;&#23545;&#20110;DeepLabV3&#27169;&#22411;&#26356;&#23567;&#30340;FB-MP&#27169;&#22411;&#21644;&#26356;&#24555;&#30340;INT8&#27169;&#22411;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#24050;&#25104;&#20026;&#36793;&#32536;&#35774;&#22791;&#35774;&#35745;&#20934;&#30830;&#39640;&#25928;&#32593;&#32476;&#30340;&#20107;&#23454;&#26631;&#20934;&#26041;&#27861;&#12290;&#30001;&#20110;&#36793;&#32536;&#37096;&#32626;&#36890;&#24120;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#37327;&#21270;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;QA-NAS&#65289;&#20197;&#25628;&#32034;&#39640;&#31934;&#24230;&#39640;&#25928;&#30340;&#37327;&#21270;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;QA-NAS&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23569;&#20301;&#28151;&#21512;&#31934;&#24230;&#65288;FB-MP&#65289;&#26041;&#27861;&#65292;&#22312;&#36739;&#22823;&#20219;&#21153;&#19978;&#26080;&#27861;&#25193;&#23637;&#12290;&#22240;&#27492;&#65292;QA-NAS&#22823;&#22810;&#21463;&#38480;&#20110;&#20302;&#35268;&#27169;&#20219;&#21153;&#21644;&#23567;&#22411;&#32593;&#32476;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;block-wise NAS&#24341;&#20837;&#30340;&#22359;&#29366;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#21551;&#29992;QA-NAS&#65288;INT8&#21644;FB-MP&#65289;&#12290;&#25105;&#20204;&#22312;Cityscapes&#25968;&#25454;&#38598;&#30340;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#25214;&#21040;&#20102;&#27604;DeepLabV3&#65288;INT8&#65289;&#23610;&#23544;&#23567;33%&#30340;FB-MP&#27169;&#22411;&#21644;&#36895;&#24230;&#24555;17.6%&#30340;INT8&#27169;&#22411;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) has become the de-facto approach for designing accurate and efficient networks for edge devices. Since models are typically quantized for edge deployment, recent work has investigated quantization-aware NAS (QA-NAS) to search for highly accurate and efficient quantized models. However, existing QA-NAS approaches, particularly few-bit mixed-precision (FB-MP) methods, do not scale to larger tasks. Consequently, QA-NAS has mostly been limited to low-scale tasks and tiny networks. In this work, we present an approach to enable QA-NAS (INT8 and FB-MP) on large-scale tasks by leveraging the block-wise formulation introduced by block-wise NAS. We demonstrate strong results for the semantic segmentation task on the Cityscapes dataset, finding FB-MP models 33% smaller and INT8 models 17.6% faster than DeepLabV3 (INT8) without compromising task performance.
&lt;/p&gt;</description></item><item><title>OCT-SelfNet&#26159;&#19968;&#31181;&#29992;&#20110;&#30524;&#31185;&#30142;&#30149;&#26816;&#27979;&#30340;&#33258;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#24191;&#20041;&#21644;&#40065;&#26834;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.12344</link><description>&lt;p&gt;
OCT-SelfNet:&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#24191;&#20041;&#21644;&#40065;&#26834;&#30340;&#35270;&#32593;&#33180;&#30142;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
OCT-SelfNet: A Self-Supervised Framework with Multi-Modal Datasets for Generalized and Robust Retinal Disease Detection. (arXiv:2401.12344v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12344
&lt;/p&gt;
&lt;p&gt;
OCT-SelfNet&#26159;&#19968;&#31181;&#29992;&#20110;&#30524;&#31185;&#30142;&#30149;&#26816;&#27979;&#30340;&#33258;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#24191;&#20041;&#21644;&#40065;&#26834;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;AI&#30340;&#38761;&#21629;&#24615;&#24433;&#21709;&#21644;&#26412;&#22320;&#35757;&#32451;&#31639;&#27861;&#30340;&#21457;&#23637;&#65292;&#20294;&#22312;&#21307;&#23398;AI&#20013;&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#23454;&#29616;&#24191;&#20041;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#24046;&#36317;&#38480;&#21046;&#20102;&#21487;&#25193;&#23637;&#21307;&#23398;AI&#35299;&#20915;&#26041;&#26696;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;OCT-SelfNet&#65292;&#29992;&#20110;&#20351;&#29992;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#22270;&#20687;&#26816;&#27979;&#30524;&#31185;&#30142;&#30149;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#32508;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#26426;&#26500;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#26356;&#20840;&#38754;&#30340;&#34920;&#24449;&#33539;&#22260;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;SwinV2&#39592;&#26550;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#30340;&#21452;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#20020;&#24202;&#23454;&#38469;&#37096;&#32626;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#19981;&#21516;&#32534;&#30721;&#22120;&#39592;&#26550;&#12289;&#20302;&#25968;&#25454;&#35774;&#32622;&#12289;&#26410;&#35265;&#25968;&#25454;&#35774;&#32622;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the revolutionary impact of AI and the development of locally trained algorithms, achieving widespread generalized learning from multi-modal data in medical AI remains a significant challenge. This gap hinders the practical deployment of scalable medical AI solutions. Addressing this challenge, our research contributes a self-supervised robust machine learning framework, OCT-SelfNet, for detecting eye diseases using optical coherence tomography (OCT) images. In this work, various data sets from various institutions are combined enabling a more comprehensive range of representation. Our method addresses the issue using a two-phase training approach that combines self-supervised pretraining and supervised fine-tuning with a mask autoencoder based on the SwinV2 backbone by providing a solution for real-world clinical deployment. Extensive experiments on three datasets with different encoder backbones, low data settings, unseen data settings, and the effect of augmentation show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#28151;&#21512;&#38750;&#37197;&#23545;&#22495;&#36716;&#25442;&#32593;&#32476;&#65288;H-CUT&#65289;&#26469;&#35299;&#20915;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#65288;ATR&#65289;&#20013;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#36328;&#39046;&#22495;&#36716;&#23548;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20302;&#30340;FID&#20998;&#25968;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#21644;&#29109;&#26469;&#24378;&#35843;&#39046;&#22495;&#29305;&#23450;&#21306;&#22495;&#65292;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2401.12340</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#36328;&#39046;&#22495;&#36716;&#23548;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#30446;&#26631;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning and Cycle Consistency-based Transductive Transfer Learning for Target Annotation. (arXiv:2401.12340v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#28151;&#21512;&#38750;&#37197;&#23545;&#22495;&#36716;&#25442;&#32593;&#32476;&#65288;H-CUT&#65289;&#26469;&#35299;&#20915;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#65288;ATR&#65289;&#20013;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#36328;&#39046;&#22495;&#36716;&#23548;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20302;&#30340;FID&#20998;&#25968;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#21644;&#29109;&#26469;&#24378;&#35843;&#39046;&#22495;&#29305;&#23450;&#21306;&#22495;&#65292;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#65288;ATR&#65289;&#30340;&#27880;&#37322;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#30001;&#20110;&#30446;&#26631;&#22495;&#20013;&#26631;&#35760;&#25968;&#25454;&#30340;&#32570;&#20047;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#21033;&#29992;&#28304;&#22495;&#22270;&#20687;&#30340;&#26631;&#35760;&#20449;&#24687;&#26469;&#26500;&#24314;&#26368;&#20339;&#30446;&#26631;&#22495;&#20998;&#31867;&#22120;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20808;&#21069;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#22522;&#20110;CycleGAN&#30340;&#38750;&#37197;&#23545;&#22495;&#36716;&#25442;&#32593;&#32476;&#30340;&#36328;&#39046;&#22495;&#36716;&#23548;&#36801;&#31227;&#23398;&#20064;&#65288;TTL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;ATR&#26631;&#27880;&#12290;&#23613;&#31649;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;ATR&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#23427;&#20005;&#37325;&#21463;&#21040;&#27880;&#37322;&#24615;&#33021;&#36739;&#20302;&#12289;&#26356;&#39640;&#30340;Fr\'echet Inception Distance&#65288;FID&#65289;&#20998;&#25968;&#20197;&#21450;&#21512;&#25104;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#35270;&#35273;&#20266;&#24433;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#28151;&#21512;&#38750;&#37197;&#23545;&#22495;&#36716;&#25442;&#65288;H-CUT&#65289;&#32593;&#32476;&#65292;&#23427;&#23454;&#29616;&#20102;&#26174;&#33879;&#36739;&#20302;&#30340;FID&#20998;&#25968;&#12290;&#23427;&#32467;&#21512;&#20102;&#27880;&#24847;&#21147;&#21644;&#29109;&#26469;&#24378;&#35843;&#39046;&#22495;&#29305;&#23450;&#30340;&#21306;&#22495;&#65292;&#22122;&#22768;&#29305;&#24449;&#28151;&#21512;&#27169;&#22359;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotating automatic target recognition (ATR) is a highly challenging task, primarily due to the unavailability of labeled data in the target domain. Hence, it is essential to construct an optimal target domain classifier by utilizing the labeled information of the source domain images. The transductive transfer learning (TTL) method that incorporates a CycleGAN-based unpaired domain translation network has been previously proposed in the literature for effective ATR annotation. Although this method demonstrates great potential for ATR, it severely suffers from lower annotation performance, higher Fr\'echet Inception Distance (FID) score, and the presence of visual artifacts in the synthetic images. To address these issues, we propose a hybrid contrastive learning base unpaired domain translation (H-CUT) network that achieves a significantly lower FID score. It incorporates both attention and entropy to emphasize the domain-specific region, a noisy feature mixup module to generate high
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31934;&#30830;&#21051;&#30011;&#20102;&#21033;&#29992;&#25439;&#22833;&#38754;&#20960;&#20309;&#20998;&#26512; SGD &#31283;&#23450;&#24615;&#30340;&#20851;&#38190;&#26465;&#20214;&#65292;&#20026;&#29702;&#35299;&#20854;&#23454;&#38469;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12332</link><description>&lt;p&gt;
&#21033;&#29992;&#25439;&#22833;&#38754;&#20960;&#20309;&#36827;&#34892; SGD &#31283;&#23450;&#24615;&#30340;&#31934;&#30830;&#21051;&#30011;
&lt;/p&gt;
&lt;p&gt;
A Precise Characterization of SGD Stability Using Loss Surface Geometry. (arXiv:2401.12332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31934;&#30830;&#21051;&#30011;&#20102;&#21033;&#29992;&#25439;&#22833;&#38754;&#20960;&#20309;&#20998;&#26512; SGD &#31283;&#23450;&#24615;&#30340;&#20851;&#38190;&#26465;&#20214;&#65292;&#20026;&#29702;&#35299;&#20854;&#23454;&#38469;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#32463;&#39564;&#23454;&#35777;&#25104;&#21151;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20294;&#20854;&#29702;&#35770;&#29702;&#35299;&#30456;&#23545;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;SGD&#23454;&#38469;&#26377;&#25928;&#24615;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#23427;&#24341;&#21457;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#12290;&#19968;&#20123;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#31283;&#23450;&#28857;&#38468;&#36817;&#30340;SGD&#30340;&#32447;&#24615;&#31283;&#23450;&#24615;&#23646;&#24615;&#65292;&#20316;&#20026;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23574;&#38160;&#24230;&#21644;&#27867;&#21270;&#35823;&#24046;&#30340;&#39044;&#27979;&#20195;&#29702;&#65288;Wu&#31561;&#20154;&#65292;2022&#65307;Jastrzebski&#31561;&#20154;&#65292;2019&#65307;Cohen&#31561;&#20154;&#65292;2021&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#32447;&#24615;&#31283;&#23450;&#24615;&#19982;&#23574;&#38160;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35814;&#32454;&#21010;&#23450;&#20102;&#32447;&#24615;&#31283;&#23450;&#24615;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#36825;&#21462;&#20915;&#20110;SGD&#30340;&#36229;&#21442;&#25968;&#21644;&#26368;&#20248;&#35299;&#22788;&#30340;&#23574;&#38160;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25439;&#22833;Hessian&#30340;&#26032;&#22411;&#19968;&#33268;&#24615;&#24230;&#37327;&#65292;&#23427;&#21253;&#21547;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#30456;&#20851;&#20960;&#20309;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent (SGD) stands as a cornerstone optimization algorithm with proven real-world empirical successes but relatively limited theoretical understanding. Recent research has illuminated a key factor contributing to its practical efficacy: the implicit regularization it instigates. Several studies have investigated the linear stability property of SGD in the vicinity of a stationary point as a predictive proxy for sharpness and generalization error in overparameterized neural networks (Wu et al., 2022; Jastrzebski et al., 2019; Cohen et al., 2021). In this paper, we delve deeper into the relationship between linear stability and sharpness. More specifically, we meticulously delineate the necessary and sufficient conditions for linear stability, contingent on hyperparameters of SGD and the sharpness at the optimum. Towards this end, we introduce a novel coherence measure of the loss Hessian that encapsulates pertinent geometric properties of the loss function that are
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;Agent&#21160;&#24577;&#20851;&#31995;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#25512;&#26029;&#20851;&#31995;&#32467;&#26500;&#30340;&#28436;&#21270;&#65292;&#26469;&#23454;&#29616;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#26041;&#27861;&#21253;&#25324;&#25512;&#26029;&#36229;&#36793;&#32536;&#20197;&#23454;&#29616;&#32676;&#20307;&#25512;&#29702;&#21644;&#36712;&#36857;&#39044;&#27979;&#22120;&#29983;&#25104;&#26410;&#26469;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2401.12275</link><description>&lt;p&gt;
&#22810;Agent&#21160;&#24577;&#20851;&#31995;&#25512;&#29702;&#29992;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Dynamic Relational Reasoning for Social Robot Navigation. (arXiv:2401.12275v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;Agent&#21160;&#24577;&#20851;&#31995;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#25512;&#26029;&#20851;&#31995;&#32467;&#26500;&#30340;&#28436;&#21270;&#65292;&#26469;&#23454;&#29616;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#26041;&#27861;&#21253;&#25324;&#25512;&#26029;&#36229;&#36793;&#32536;&#20197;&#23454;&#29616;&#32676;&#20307;&#25512;&#29702;&#21644;&#36712;&#36857;&#39044;&#27979;&#22120;&#29983;&#25104;&#26410;&#26469;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#22312;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#31181;&#24773;&#26223;&#19979;&#21487;&#20197;&#25552;&#20379;&#24110;&#21161;&#65292;&#20294;&#38656;&#35201;&#23433;&#20840;&#30340;&#20154;&#26426;&#20132;&#20114;&#21644;&#39640;&#25928;&#30340;&#36712;&#36857;&#35268;&#21010;&#12290;&#22312;&#22810;Agent&#20132;&#20114;&#31995;&#32479;&#20013;&#65292;&#24314;&#27169;&#25104;&#23545;&#30340;&#20851;&#31995;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#25429;&#25417;&#26356;&#22823;&#35268;&#27169;&#30340;&#32676;&#20307;&#27963;&#21160;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#20851;&#31995;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#25512;&#26029;&#27491;&#22312;&#28436;&#21464;&#30340;&#20851;&#31995;&#32467;&#26500;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;Agent&#36712;&#36857;&#39044;&#27979;&#21644;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#38500;&#20102;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#36793;&#32536;&#65288;&#21363;Agent&#65289;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#25512;&#26029;&#36229;&#36793;&#32536;&#30340;&#26041;&#27861;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#36830;&#25509;&#22810;&#20010;&#33410;&#28857;&#65292;&#20197;&#20415;&#36827;&#34892;&#32676;&#20307;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#26029;&#21160;&#24577;&#28436;&#21270;&#30340;&#20851;&#31995;&#22270;&#21644;&#36229;&#22270;&#65292;&#20197;&#25429;&#25417;&#20851;&#31995;&#30340;&#28436;&#21270;&#65292;&#36712;&#36857;&#39044;&#27979;&#22120;&#21033;&#29992;&#36825;&#20123;&#22270;&#26469;&#29983;&#25104;&#26410;&#26469;&#29366;&#24577;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#38160;&#24230;&#21644;&#36923;&#36753;&#31232;&#30095;&#24615;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social robot navigation can be helpful in various contexts of daily life but requires safe human-robot interactions and efficient trajectory planning. While modeling pairwise relations has been widely studied in multi-agent interacting systems, the ability to capture larger-scale group-wise activities is limited. In this paper, we propose a systematic relational reasoning approach with explicit inference of the underlying dynamically evolving relational structures, and we demonstrate its effectiveness for multi-agent trajectory prediction and social robot navigation. In addition to the edges between pairs of nodes (i.e., agents), we propose to infer hyperedges that adaptively connect multiple nodes to enable group-wise reasoning in an unsupervised manner. Our approach infers dynamically evolving relation graphs and hypergraphs to capture the evolution of relations, which the trajectory predictor employs to generate future states. Meanwhile, we propose to regularize the sharpness and sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#36801;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32622;&#20449;&#38408;&#20540;&#20272;&#35745;&#22120;&#26469;&#23454;&#29616;&#28176;&#36817;&#26368;&#23567;&#39118;&#38505;&#65292;&#24182;&#21457;&#29616;&#20102;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#29420;&#29305;&#29616;&#35937;&#65306;&#33258;&#21160;&#24179;&#28369;&#21644;&#36229;&#21152;&#36895;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#24191;&#27867;&#30340;&#21442;&#25968;&#31354;&#38388;&#65292;&#24182;&#22312;&#20223;&#30495;&#30740;&#31350;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.12272</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#38750;&#28176;&#36817;&#26497;&#23567;&#21270;&#20998;&#26512;&#21644;&#33258;&#36866;&#24212;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for Nonparametric Regression: Non-asymptotic Minimax Analysis and Adaptive Procedure. (arXiv:2401.12272v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#36801;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32622;&#20449;&#38408;&#20540;&#20272;&#35745;&#22120;&#26469;&#23454;&#29616;&#28176;&#36817;&#26368;&#23567;&#39118;&#38505;&#65292;&#24182;&#21457;&#29616;&#20102;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#29420;&#29305;&#29616;&#35937;&#65306;&#33258;&#21160;&#24179;&#28369;&#21644;&#36229;&#21152;&#36895;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#24191;&#27867;&#30340;&#21442;&#25968;&#31354;&#38388;&#65292;&#24182;&#22312;&#20223;&#30495;&#30740;&#31350;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#36801;&#31227;&#23398;&#20064;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35813;&#38382;&#39064;&#30340;&#38750;&#28176;&#36817;&#26497;&#23567;&#39118;&#38505;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#31216;&#20026;&#32622;&#20449;&#38408;&#20540;&#20272;&#35745;&#22120;&#65292;&#35777;&#26126;&#35813;&#20272;&#35745;&#22120;&#22312;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#30340;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#28176;&#36817;&#26497;&#23567;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#29420;&#29305;&#29616;&#35937;&#65306;&#33258;&#21160;&#24179;&#28369;&#21644;&#36229;&#21152;&#36895;&#65292;&#36825;&#20351;&#20854;&#19982;&#20256;&#32479;&#35774;&#32622;&#20013;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#26377;&#25152;&#21306;&#21035;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#22312;&#24191;&#27867;&#30340;&#21442;&#25968;&#31354;&#38388;&#20013;&#23454;&#29616;&#20102;&#23545;&#25968;&#22240;&#23376;&#30340;&#28176;&#36817;&#26368;&#23567;&#39118;&#38505;&#12290;&#36890;&#36807;&#20223;&#30495;&#30740;&#31350;&#35780;&#20272;&#20102;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#20540;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#26469;&#23637;&#31034;&#35813;&#26041;&#27861;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning for nonparametric regression is considered. We first study the non-asymptotic minimax risk for this problem and develop a novel estimator called the confidence thresholding estimator, which is shown to achieve the minimax optimal risk up to a logarithmic factor. Our results demonstrate two unique phenomena in transfer learning: auto-smoothing and super-acceleration, which differentiate it from nonparametric regression in a traditional setting. We then propose a data-driven algorithm that adaptively achieves the minimax risk up to a logarithmic factor across a wide range of parameter spaces. Simulation studies are conducted to evaluate the numerical performance of the adaptive transfer learning algorithm, and a real-world example is provided to demonstrate the benefits of the proposed method.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#36807;&#37319;&#26679;&#26041;&#27861;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#32467;&#21512;&#22534;&#21472;&#29305;&#24449;&#23884;&#20837;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12262</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#38024;&#23545;&#22823;&#35268;&#27169;&#21644;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#24212;&#29992;&#65292;&#20351;&#29992;&#36807;&#37319;&#26679;&#12289;&#22534;&#21472;&#29305;&#24449;&#23884;&#20837;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based network intrusion detection for big and imbalanced data using oversampling, stacking feature embedding and feature extraction. (arXiv:2401.12262v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12262
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#36807;&#37319;&#26679;&#26041;&#27861;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#32467;&#21512;&#22534;&#21472;&#29305;&#24449;&#23884;&#20837;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23433;&#20840;&#24050;&#25104;&#20026;&#20840;&#29699;&#20851;&#27880;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#36890;&#36807;&#26816;&#27979;&#24694;&#24847;&#34892;&#20026;&#21644;&#27963;&#21160;&#22312;&#20445;&#25252;&#20114;&#32852;&#32593;&#32476;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#34892;&#20026;&#20998;&#26512;&#22312;IDS&#20013;&#20855;&#26377;&#21457;&#29616;&#21160;&#24577;&#32593;&#32476;&#23041;&#32961;&#12289;&#35782;&#21035;&#24322;&#24120;&#21644;&#24694;&#24847;&#34892;&#20026;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#24403;&#35757;&#32451;ML&#27169;&#22411;&#26102;&#65292;&#38477;&#32500;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#38543;&#26426;&#36807;&#37319;&#26679;&#65288;RO&#65289;&#26469;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#32467;&#21512;&#32858;&#31867;&#32467;&#26524;&#30340;&#22534;&#21472;&#29305;&#24449;&#23884;&#20837;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#36827;&#34892;&#38477;&#32500;&#65292;&#19987;&#38376;&#38024;&#23545;&#22823;&#35268;&#27169;&#21644;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#21069;&#27839;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;UNSW-NB15&#12289;CIC-IDS-2017&#21644;CIC-IDS&#65289;&#20180;&#32454;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cybersecurity has emerged as a critical global concern. Intrusion Detection Systems (IDS) play a critical role in protecting interconnected networks by detecting malicious actors and activities. Machine Learning (ML)-based behavior analysis within the IDS has considerable potential for detecting dynamic cyber threats, identifying abnormalities, and identifying malicious conduct within the network. However, as the number of data grows, dimension reduction becomes an increasingly difficult task when training ML models. Addressing this, our paper introduces a novel ML-based network intrusion detection model that uses Random Oversampling (RO) to address data imbalance and Stacking Feature Embedding based on clustering results, as well as Principal Component Analysis (PCA) for dimension reduction and is specifically designed for large and imbalanced datasets. This model's performance is carefully evaluated using three cutting-edge benchmark datasets: UNSW-NB15, CIC-IDS-2017, and CIC-IDS-201
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;</title><link>http://arxiv.org/abs/2401.12258</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#30340;&#26032;&#20852;&#25903;&#37197;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Emergent Dominance Hierarchies in Reinforcement Learning Agents. (arXiv:2401.12258v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33021;&#22815;&#32988;&#36807;&#20154;&#31867;&#12290;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#35774;&#32622;&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#25104;&#21151;&#30340;&#28151;&#21512;&#21160;&#26426;&#20195;&#29702;&#21327;&#20316;&#21462;&#20915;&#20110;&#20010;&#20307;&#21644;&#32676;&#20307;&#30446;&#26631;&#20043;&#38388;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;&#31038;&#20250;&#20064;&#24815;&#21644;&#35268;&#33539;&#65292;&#24448;&#24448;&#21463;&#21040;&#20154;&#31867;&#26426;&#26500;&#30340;&#21551;&#21457;&#65292;&#34987;&#29992;&#20316;&#23454;&#29616;&#36825;&#31181;&#24179;&#34913;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#26412;&#19988;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#31038;&#20250;&#20064;&#24815;&#65292;&#21363;&#25903;&#37197;&#31561;&#32423;&#65292;&#23427;&#22312;&#21160;&#29289;&#21644;&#20154;&#31867;&#31038;&#20250;&#20013;&#37117;&#23384;&#22312;&#12290;&#25105;&#20204;&#23558;&#25903;&#37197;&#31561;&#32423;&#30340;&#34892;&#20026;&#29702;&#35770;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#24182;&#23613;&#21487;&#33021;&#23569;&#22320;&#20462;&#25913;&#29616;&#26377;&#30340;&#26415;&#35821;&#21644;&#23450;&#20041;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#25110;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#32676;&#20307;&#33021;&#22815;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;&#25152;&#20135;&#29983;&#30340;&#25903;&#37197;&#31561;&#32423;&#26377;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges, and successful cooperation in mixed-motive groups of agents depends on a delicate balancing act between individual and group objectives. Social conventions and norms, often inspired by human institutions, are used as tools for striking this balance.  In this paper, we examine a fundamental, well-studied social convention that underlies cooperation in both animal and human societies: Dominance hierarchies.  We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing the established terminology and definitions with as few amendments as possible. We demonstrate that populations of RL agents, operating without explicit programming or intrinsic rewards, can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge have a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#22686;&#24378;&#30340;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#27169;&#22411;&#30340;&#32435;&#31859;&#20809;&#23376;&#32467;&#26500;&#36870;&#21521;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#39044;&#27979;&#22810;&#20010;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.12254</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#30340;&#32435;&#31859;&#20809;&#23376;&#23398;&#36870;&#21521;&#24314;&#27169;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
Transfer learning-assisted inverse modeling in nanophotonics based on mixture density networks. (arXiv:2401.12254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#22686;&#24378;&#30340;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#27169;&#22411;&#30340;&#32435;&#31859;&#20809;&#23376;&#32467;&#26500;&#36870;&#21521;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#39044;&#27979;&#22810;&#20010;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32435;&#31859;&#20809;&#23376;&#32467;&#26500;&#30340;&#27169;&#25311;&#20381;&#36182;&#20110;&#30005;&#30913;&#27714;&#35299;&#22120;&#65292;&#22312;&#29702;&#35299;&#20854;&#34892;&#20026;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27714;&#35299;&#22120;&#36890;&#24120;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#20248;&#21270;&#31561;&#35774;&#35745;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#24050;&#32463;&#25506;&#32034;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#31934;&#30830;&#21644;&#39640;&#25928;&#22320;&#24314;&#27169;&#21644;&#35774;&#35745;&#20809;&#23376;&#22120;&#20214;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20010;&#39046;&#22495;&#29305;&#21035;&#21463;&#21040;&#20851;&#27880;&#12290;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#21019;&#24314;&#21069;&#21521;&#27169;&#22411;&#21644;&#36870;&#21521;&#27169;&#22411;&#12290;&#36870;&#21521;&#24314;&#27169;&#26041;&#27861;&#36991;&#20813;&#20102;&#23558;&#21069;&#21521;&#27169;&#22411;&#19982;&#20248;&#21270;&#22120;&#32806;&#21512;&#30340;&#38656;&#27714;&#65292;&#24182;&#30452;&#25509;&#25191;&#34892;&#26368;&#20339;&#35774;&#35745;&#21442;&#25968;&#20540;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The simulation of nanophotonic structures relies on electromagnetic solvers, which play a crucial role in understanding their behavior. However, these solvers often come with a significant computational cost, making their application in design tasks, such as optimization, impractical. To address this challenge, machine learning techniques have been explored for accurate and efficient modeling and design of photonic devices. Deep neural networks, in particular, have gained considerable attention in this field. They can be used to create both forward and inverse models. An inverse modeling approach avoids the need for coupling a forward model with an optimizer and directly performs the prediction of the optimal design parameters values.  In this paper, we propose an inverse modeling method for nanophotonic structures, based on a mixture density network model enhanced by transfer learning. Mixture density networks can predict multiple possible solutions at a time including their respectiv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;Sinkhorn&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25552;&#21069;&#20572;&#27490;&#21644;&#29275;&#39039;&#36845;&#20195;&#23376;&#31243;&#24207;&#65292;&#23454;&#29616;&#20102;&#21487;&#33021;&#30340;&#36229;&#25351;&#25968;&#25910;&#25947;&#12290;&#20182;&#20204;&#21033;&#29992;&#20102;Sinkhorn&#31639;&#27861;&#26368;&#22823;&#21270;&#20985;&#24615;&#26446;&#38597;&#26222;&#35834;&#22827;&#21183;&#30340;&#29305;&#24615;&#65292;&#21457;&#29616;&#20102;&#21183;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#36817;&#20284;&#31232;&#30095;&#65292;&#20174;&#32780;&#23558;&#27599;&#27425;&#36845;&#20195;&#30340;&#22797;&#26434;&#24615;&#38477;&#20302;&#21040;&#20102;$O(n^2)$&#12290;</title><link>http://arxiv.org/abs/2401.12253</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#29275;&#39039;&#36845;&#20195;&#21152;&#36895;Sinkhorn&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accelerating Sinkhorn Algorithm with Sparse Newton Iterations. (arXiv:2401.12253v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12253
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;Sinkhorn&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25552;&#21069;&#20572;&#27490;&#21644;&#29275;&#39039;&#36845;&#20195;&#23376;&#31243;&#24207;&#65292;&#23454;&#29616;&#20102;&#21487;&#33021;&#30340;&#36229;&#25351;&#25968;&#25910;&#25947;&#12290;&#20182;&#20204;&#21033;&#29992;&#20102;Sinkhorn&#31639;&#27861;&#26368;&#22823;&#21270;&#20985;&#24615;&#26446;&#38597;&#26222;&#35834;&#22827;&#21183;&#30340;&#29305;&#24615;&#65292;&#21457;&#29616;&#20102;&#21183;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#36817;&#20284;&#31232;&#30095;&#65292;&#20174;&#32780;&#23558;&#27599;&#27425;&#36845;&#20195;&#30340;&#22797;&#26434;&#24615;&#38477;&#20302;&#21040;&#20102;$O(n^2)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#35745;&#31639;&#32479;&#35745;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#31361;&#30772;&#24615;&#36827;&#23637;&#26159;&#29109;&#27491;&#21017;&#21270;&#21644;Sinkhorn&#31639;&#27861;&#65292;&#23427;&#21482;&#20351;&#29992;&#30697;&#38453;&#32553;&#25918;&#24182;&#20445;&#35777;&#36817;&#20284;&#35299;&#30340;&#32447;&#24615;&#36816;&#34892;&#26102;&#38388;&#12290;&#23613;&#31649;Sinkhorn&#31639;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#36845;&#20195;&#26469;&#36798;&#21040;&#25910;&#25947;&#65292;&#23427;&#30340;&#36816;&#34892;&#26102;&#38388;&#20173;&#21487;&#33021;&#36739;&#24930;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#33021;&#30340;&#36229;&#25351;&#25968;&#25910;&#25947;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sinkhorn-Newton-Sparse&#65288;SNS&#65289;&#65292;&#36825;&#26159;Sinkhorn&#31639;&#27861;&#30340;&#19968;&#20010;&#25193;&#23637;&#65292;&#36890;&#36807;&#24341;&#20837;&#30697;&#38453;&#32553;&#25918;&#27493;&#39588;&#30340;&#25552;&#21069;&#20572;&#27490;&#21644;&#19968;&#20010;&#29305;&#24449;&#29275;&#39039;&#23376;&#31243;&#24207;&#30340;&#31532;&#20108;&#38454;&#27573;&#26469;&#23454;&#29616;&#12290;&#37319;&#29992;Sinkhorn&#31639;&#27861;&#26368;&#22823;&#21270;&#20985;&#24615;&#26446;&#38597;&#26222;&#35834;&#22827;&#21183;&#30340;&#21464;&#20998;&#35270;&#35282;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#21183;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#36817;&#20284;&#31232;&#30095;&#12290;&#31232;&#30095;&#21270;Hessian&#30697;&#38453;&#23548;&#33268;&#27599;&#27425;&#36845;&#20195;&#30340;&#22797;&#26434;&#24615;&#20026;&#24555;&#36895;&#30340;$O(n^2)$&#65292;&#19982;&#20256;&#32479;Sinkhorn&#31639;&#27861;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computing the optimal transport distance between statistical distributions is a fundamental task in machine learning. One remarkable recent advancement is entropic regularization and the Sinkhorn algorithm, which utilizes only matrix scaling and guarantees an approximated solution with near-linear runtime. Despite the success of the Sinkhorn algorithm, its runtime may still be slow due to the potentially large number of iterations needed for convergence. To achieve possibly super-exponential convergence, we present Sinkhorn-Newton-Sparse (SNS), an extension to the Sinkhorn algorithm, by introducing early stopping for the matrix scaling steps and a second stage featuring a Newton-type subroutine. Adopting the variational viewpoint that the Sinkhorn algorithm maximizes a concave Lyapunov potential, we offer the insight that the Hessian matrix of the potential function is approximately sparse. Sparsification of the Hessian results in a fast $O(n^2)$ per-iteration complexity, the same as t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#25193;&#25955;&#26144;&#23556;&#24418;&#24335;&#65292;&#29992;&#20110;&#22788;&#29702;&#30001;&#38750;&#23545;&#31216;&#26680;&#24341;&#23548;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20351;&#29992;&#20808;&#39564;&#22352;&#26631;&#31995;&#21644;&#22522;&#20110;Fourier&#22522;&#30340;&#22352;&#26631;&#31995;&#65292;&#21487;&#20197;&#20943;&#23569;&#25968;&#25454;&#38598;&#30340;&#32500;&#25968;&#65292;&#24182;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12251</link><description>&lt;p&gt;
&#38750;&#23545;&#31216;&#26680;&#30340;&#25193;&#25955;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Diffusion Representation for Asymmetric Kernels. (arXiv:2401.12251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#25193;&#25955;&#26144;&#23556;&#24418;&#24335;&#65292;&#29992;&#20110;&#22788;&#29702;&#30001;&#38750;&#23545;&#31216;&#26680;&#24341;&#23548;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20351;&#29992;&#20808;&#39564;&#22352;&#26631;&#31995;&#21644;&#22522;&#20110;Fourier&#22522;&#30340;&#22352;&#26631;&#31995;&#65292;&#21487;&#20197;&#20943;&#23569;&#25968;&#25454;&#38598;&#30340;&#32500;&#25968;&#65292;&#24182;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#25193;&#25955;&#26144;&#23556;&#24418;&#24335;&#25193;&#23637;&#21040;&#30001;&#38750;&#23545;&#31216;&#26680;&#24341;&#23548;&#30340;&#25968;&#25454;&#38598;&#12290;&#35777;&#26126;&#20102;&#32467;&#26524;&#23637;&#24320;&#30340;&#20998;&#26512;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25191;&#34892;&#32500;&#25968;&#20943;&#23569;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#32467;&#26500;&#26159;&#30001;&#38750;&#23545;&#31216;&#26680;&#24341;&#23548;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#20808;&#39564;&#22352;&#26631;&#31995;&#26469;&#34920;&#31034;&#36825;&#20010;&#20960;&#20309;&#32467;&#26500;&#65292;&#20174;&#32780;&#33021;&#22815;&#25552;&#39640;&#25968;&#25454;&#38598;&#20943;&#23569;&#32500;&#25968;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#37319;&#29992;&#19982;Fourier&#22522;&#30340;&#24352;&#37327;&#31215;&#30456;&#20851;&#32852;&#30340;&#22352;&#26631;&#31995;&#26469;&#34920;&#31034;&#25193;&#25955;&#26144;&#23556;&#24471;&#21040;&#30340;&#24213;&#23618;&#20960;&#20309;&#32467;&#26500;&#65292;&#20174;&#32780;&#20943;&#23569;&#25968;&#25454;&#38598;&#30340;&#32500;&#25968;&#65292;&#24182;&#21033;&#29992;&#20108;&#32500;&#24555;&#36895;Fourier&#21464;&#25442;&#31639;&#27861;&#65288;2-D FFT&#65289;&#25552;&#20379;&#30340;&#21152;&#36895;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#20854;&#20182;&#29305;&#24449;&#20540;&#23637;&#24320;&#24471;&#21040;&#30340;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#21644;&#21253;&#25324;&#32858;&#31867;&#31561;&#23454;&#38469;&#24212;&#29992;&#30340;&#30495;&#23454;&#25968;&#25454;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend the diffusion-map formalism to data sets that are induced by asymmetric kernels. Analytical convergence results of the resulting expansion are proved, and an algorithm is proposed to perform the dimensional reduction. In this work we study data sets in which its geometry structure is induced by an asymmetric kernel. We use a priori coordinate system to represent this geometry and, thus, be able to improve the computational complexity of reducing the dimensionality of data sets. A coordinate system connected to the tensor product of Fourier basis is used to represent the underlying geometric structure obtained by the diffusion-map, thus reducing the dimensionality of the data set and making use of the speedup provided by the two-dimensional Fast Fourier Transform algorithm (2-D FFT). We compare our results with those obtained by other eigenvalue expansions, and verify the efficiency of the algorithms with synthetic data, as well as with real data from applications including cl
&lt;/p&gt;</description></item><item><title>Orion-14B&#26159;&#19968;&#20010;&#20855;&#26377;140&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#25968;&#25454;&#35843;&#24230;&#26041;&#27861;&#23545;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#22810;&#31181;&#35821;&#35328;&#30340;2.5&#19975;&#20159;&#20010;&#26631;&#35760;&#30340;&#22810;&#26679;&#21270;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#36824;&#23545;&#23545;&#35805;&#24212;&#29992;&#21644;&#20854;&#20182;&#29305;&#23450;&#29992;&#20363;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;Orion-14B&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;Orion-14B&#27169;&#22411;&#31995;&#21015;&#21450;&#20854;&#30456;&#20851;&#20195;&#30721;&#20844;&#24320;&#65292;&#20197;&#40723;&#21169;&#26410;&#26469;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.12246</link><description>&lt;p&gt;
Orion-14B: &#24320;&#28304;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Orion-14B: Open-source Multilingual Large Language Models. (arXiv:2401.12246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12246
&lt;/p&gt;
&lt;p&gt;
Orion-14B&#26159;&#19968;&#20010;&#20855;&#26377;140&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#25968;&#25454;&#35843;&#24230;&#26041;&#27861;&#23545;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#22810;&#31181;&#35821;&#35328;&#30340;2.5&#19975;&#20159;&#20010;&#26631;&#35760;&#30340;&#22810;&#26679;&#21270;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#36824;&#23545;&#23545;&#35805;&#24212;&#29992;&#21644;&#20854;&#20182;&#29305;&#23450;&#29992;&#20363;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;Orion-14B&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;Orion-14B&#27169;&#22411;&#31995;&#21015;&#21450;&#20854;&#30456;&#20851;&#20195;&#30721;&#20844;&#24320;&#65292;&#20197;&#40723;&#21169;&#26410;&#26469;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Orion-14B&#65292;&#19968;&#20010;&#20855;&#26377;140&#20159;&#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#12290;&#25105;&#20204;&#37319;&#29992;&#25968;&#25454;&#35843;&#24230;&#26041;&#27861;&#65292;&#22312;&#21253;&#25324;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#26085;&#35821;&#12289;&#38889;&#35821;&#21644;&#20854;&#20182;&#35821;&#35328;&#30340;&#25991;&#26412;&#20013;&#65292;&#23545;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;2.5&#19975;&#20159;&#20010;&#26631;&#35760;&#30340;&#22810;&#26679;&#21270;&#35821;&#26009;&#24211;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#38024;&#23545;&#23545;&#35805;&#24212;&#29992;&#21644;&#20854;&#20182;&#29305;&#23450;&#29992;&#20363;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Orion-14B&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;Orion-14B&#27169;&#22411;&#31995;&#21015;&#21450;&#20854;&#30456;&#20851;&#20195;&#30721;&#20844;&#24320;&#21487;&#35775;&#38382;&#65292;&#26088;&#22312;&#28608;&#21457;&#26410;&#26469;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce Orion-14B, a collection of multilingual large language models with 14 billion parameters. We utilize a data scheduling approach to train a foundational model on a diverse corpus of 2.5 trillion tokens, sourced from texts in English, Chinese, Japanese, Korean, and other languages. Additionally, we fine-tuned a series of models tailored for conversational applications and other specific use cases. Our evaluation results demonstrate that Orion-14B achieves state-of-the-art performance across a broad spectrum of tasks. We make the Orion-14B model family and its associated code publicly accessible https://github.com/OrionStarAI/Orion, aiming to inspire future research and practical applications in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#29983;&#25104;&#26356;&#21463;&#20154;&#31867;&#21916;&#27426;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.12244</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large-scale Reinforcement Learning for Diffusion Models. (arXiv:2401.12244v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#29983;&#25104;&#26356;&#21463;&#20154;&#31867;&#21916;&#27426;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#32593;&#39029;&#35268;&#27169;&#30340;&#25991;&#26412;-&#22270;&#20687;&#35757;&#32451;&#23545;&#30340;&#38544;&#24335;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#22320;&#24314;&#27169;&#25105;&#20204;&#20851;&#24515;&#30340;&#22270;&#20687;&#26041;&#38754;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#30340;&#26679;&#26412;&#12289;&#27169;&#22411;&#20559;&#35265;&#21644;&#19982;&#20154;&#31867;&#36947;&#24503;&#21644;&#21916;&#22909;&#19981;&#31526;&#30340;&#22270;&#20687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#25913;&#36827;&#25193;&#25955;&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#25968;&#30334;&#19975;&#20010;&#22270;&#20687;&#30340;&#20154;&#31867;&#20559;&#22909;&#12289;&#32452;&#21512;&#24615;&#21644;&#20844;&#24179;&#24615;&#31561;&#22810;&#26679;&#30340;&#22238;&#25253;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#22823;&#22823;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#20351;&#25193;&#25955;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;&#36825;&#22914;&#20309;&#22823;&#22823;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#30340;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#27169;&#22411;&#65292;&#25152;&#29983;&#25104;&#30340;&#26679;&#26412;&#22312;80.3%&#30340;&#26102;&#38388;&#20869;&#20248;&#20110;&#22522;&#26412;SD&#27169;&#22411;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models are a class of deep generative models that have demonstrated an impressive capacity for high-quality image generation. However, these models are susceptible to implicit biases that arise from web-scale text-image training pairs and may inaccurately model aspects of images we care about. This can result in suboptimal samples, model bias, and images that do not align with human ethics and preferences. In this paper, we present an effective scalable algorithm to improve diffusion models using Reinforcement Learning (RL) across a diverse set of reward functions, such as human preference, compositionality, and fairness over millions of images. We illustrate how our approach substantially outperforms existing methods for aligning diffusion models with human preferences. We further illustrate how this substantially improves pretrained Stable Diffusion (SD) models, generating samples that are preferred by humans 80.3% of the time over those from the base SD model
&lt;/p&gt;</description></item><item><title>Constraint-Generation Policy Optimization (CGPO)&#26159;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;MDPs&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#30028;&#30340;&#31574;&#30053;&#35823;&#24046;&#20445;&#35777;&#65292;&#25512;&#23548;&#20986;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#29366;&#24577;&#36712;&#36857;&#26469;&#35786;&#26029;&#31574;&#30053;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2401.12243</link><description>&lt;p&gt;
Constraint-Generation Policy Optimization (CGPO): &#38024;&#23545;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;MDPs&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#30340;&#38750;&#32447;&#24615;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Constraint-Generation Policy Optimization (CGPO): Nonlinear Programming for Policy Optimization in Mixed Discrete-Continuous MDPs. (arXiv:2401.12243v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12243
&lt;/p&gt;
&lt;p&gt;
Constraint-Generation Policy Optimization (CGPO)&#26159;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;MDPs&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#30028;&#30340;&#31574;&#30053;&#35823;&#24046;&#20445;&#35777;&#65292;&#25512;&#23548;&#20986;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#29366;&#24577;&#36712;&#36857;&#26469;&#35786;&#26029;&#31574;&#30053;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Constraint-Generation Policy Optimization (CGPO)&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;Markov Decision Processes (DC-MDPs)&#20013;&#20248;&#21270;&#31574;&#30053;&#21442;&#25968;&#12290;CGPO&#19981;&#20165;&#33021;&#22815;&#25552;&#20379;&#26377;&#30028;&#30340;&#31574;&#30053;&#35823;&#24046;&#20445;&#35777;&#65292;&#35206;&#30422;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#26080;&#25968;&#21021;&#22987;&#29366;&#24577;&#33539;&#22260;&#30340;DC-MDPs&#65292;&#32780;&#19988;&#22312;&#32467;&#26463;&#26102;&#21487;&#20197;&#26126;&#30830;&#22320;&#25512;&#23548;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;CGPO&#36824;&#33021;&#22815;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#29366;&#24577;&#36712;&#36857;&#26469;&#35786;&#26029;&#31574;&#30053;&#32570;&#38519;&#65292;&#24182;&#25552;&#20379;&#26368;&#20248;&#34892;&#21160;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#32467;&#26524;&#65292;CGPO&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#30340;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23450;&#20041;&#30340;&#34920;&#36798;&#33021;&#21147;&#31867;&#21035;&#65288;&#21363;&#20998;&#27573;(&#38750;)&#32447;&#24615;&#65289;&#20869;&#20248;&#21270;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#30340;&#32422;&#26463;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#29366;&#24577;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#20511;&#21161;&#29616;&#20195;&#38750;&#32447;&#24615;&#20248;&#21270;&#22120;&#65292;CGPO&#21487;&#20197;&#33719;&#24471;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Constraint-Generation Policy Optimization (CGPO) for optimizing policy parameters within compact and interpretable policy classes for mixed discrete-continuous Markov Decision Processes (DC-MDPs). CGPO is not only able to provide bounded policy error guarantees over an infinite range of initial states for many DC-MDPs with expressive nonlinear dynamics, but it can also provably derive optimal policies in cases where it terminates with zero error. Furthermore, CGPO can generate worst-case state trajectories to diagnose policy deficiencies and provide counterfactual explanations of optimal actions. To achieve such results, CGPO proposes a bi-level mixed-integer nonlinear optimization framework for optimizing policies within defined expressivity classes (i.e. piecewise (non)-linear) and reduces it to an optimal constraint generation methodology that adversarially generates worst-case state trajectories. Furthermore, leveraging modern nonlinear optimizers, CGPO can obtain soluti
&lt;/p&gt;</description></item><item><title>BadChain&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#37319;&#29992;&#20174;&#24605;&#32500;&#38142;&#25552;&#31034;(COT)&#30340;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#12290;&#23427;&#19981;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#38598;&#25110;&#27169;&#22411;&#21442;&#25968;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2401.12242</link><description>&lt;p&gt;
BadChain: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21518;&#38376;&#24605;&#32500;&#38142;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models. (arXiv:2401.12242v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12242
&lt;/p&gt;
&lt;p&gt;
BadChain&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#37319;&#29992;&#20174;&#24605;&#32500;&#38142;&#25552;&#31034;(COT)&#30340;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#12290;&#23427;&#19981;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#38598;&#25110;&#27169;&#22411;&#21442;&#25968;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22788;&#29702;&#38656;&#35201;&#31995;&#32479;&#25512;&#29702;&#36807;&#31243;&#30340;&#20219;&#21153;&#26102;&#65292;&#34920;&#29616;&#20986;&#20174;&#24605;&#32500;&#38142;&#25552;&#31034;(COT)&#20013;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;COT&#25552;&#31034;&#20063;&#22312;&#25512;&#29702;&#20013;&#20986;&#29616;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#24418;&#24335;&#65292;&#21363;&#22312;&#29305;&#23450;&#30340;&#21518;&#38376;&#35302;&#21457;&#26465;&#20214;&#19979;&#65292;&#27169;&#22411;&#23558;&#36755;&#20986;&#24847;&#22806;&#30340;&#24694;&#24847;&#20869;&#23481;&#12290;&#20256;&#32479;&#30340;&#21457;&#21160;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#21253;&#25324;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#38598;&#25110;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#30452;&#25509;&#25805;&#32437;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#36890;&#24120;&#36890;&#36807;API&#35775;&#38382;&#30340;&#21830;&#19994;LLM&#26469;&#35828;&#24182;&#19981;&#23454;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BadChain&#65292;&#23427;&#26159;&#38024;&#23545;&#37319;&#29992;COT&#25552;&#31034;&#30340;LLM&#30340;&#31532;&#19968;&#20010;&#21518;&#38376;&#25915;&#20987;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#38598;&#25110;&#27169;&#22411;&#21442;&#25968;&#65292;&#24182;&#19988;&#35745;&#31639;&#24320;&#38144;&#36739;&#20302;&#12290;BadChain&#21033;&#29992;LLM&#30340;&#20869;&#22312;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25554;&#20837;&#19968;&#20010;&#21518;&#38376;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are shown to benefit from chain-of-thought (COT) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, COT prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output unintended malicious content under specific backdoor-triggered conditions during inference. Traditional methods for launching backdoor attacks involve either contaminating the training dataset with backdoored instances or directly manipulating the model parameters during deployment. However, these approaches are not practical for commercial LLMs that typically operate via API access. In this paper, we propose BadChain, the first backdoor attack against LLMs employing COT prompting, which does not require access to the training dataset or model parameters and imposes low computational overhead. BadChain leverages the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning step int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#21151;&#32791;&#27773;&#36710;&#32593;&#32476;&#20013;&#20351;&#29992;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#20316;&#20026;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;&#24310;&#36831;&#21644;&#25512;&#29702;&#33021;&#32791;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#20284;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12240</link><description>&lt;p&gt;
&#20302;&#21151;&#32791;&#27773;&#36710;&#32593;&#32476;&#20013;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantised Neural Network Accelerators for Low-Power IDS in Automotive Networks. (arXiv:2401.12240v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#21151;&#32791;&#27773;&#36710;&#32593;&#32476;&#20013;&#20351;&#29992;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#20316;&#20026;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;&#24310;&#36831;&#21644;&#25512;&#29702;&#33021;&#32791;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#20284;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20316;&#20026;&#27773;&#36710;&#25511;&#21046;&#21306;&#22495;&#32593;&#32476;&#65288;CAN&#65289;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#20351;&#29992;&#20302;&#21151;&#32791;&#23450;&#21046;&#37327;&#21270;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;AMD / Xilinx&#30340;FINN&#26694;&#26550;&#23545;&#25105;&#20204;&#30340;MLP&#36827;&#34892;&#37327;&#21270;&#65292;&#35757;&#32451;&#21644;&#29983;&#25104;&#30828;&#20214;IP&#65292;&#20197;&#26816;&#27979;CAN&#32593;&#32476;&#19978;&#30340;&#25298;&#32477;&#26381;&#21153;&#65288;DoS&#65289;&#21644;&#27169;&#31946;&#25915;&#20987;&#65292;&#20351;&#29992;ZCU104&#65288;XCZU7EV&#65289;FPGA&#20316;&#20026;&#25105;&#20204;&#30340;&#30446;&#26631;ECU&#26550;&#26500;&#65292;&#38598;&#25104;IDS&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24310;&#36831;&#65288;&#27599;&#26465;&#28040;&#24687;&#22788;&#29702;&#24310;&#36831;0.12&#27627;&#31186;&#65289;&#21644;&#25512;&#29702;&#33021;&#32791;&#65288;&#27599;&#27425;&#25512;&#29702;0.25&#27627;&#28966;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#22312;&#25991;&#29486;&#20013;&#36798;&#21040;&#20102;&#31867;&#20284;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore low-power custom quantised Multi-Layer Perceptrons (MLPs) as an Intrusion Detection System (IDS) for automotive controller area network (CAN). We utilise the FINN framework from AMD/Xilinx to quantise, train and generate hardware IP of our MLP to detect denial of service (DoS) and fuzzying attacks on CAN network, using ZCU104 (XCZU7EV) FPGA as our target ECU architecture with integrated IDS capabilities. Our approach achieves significant improvements in latency (0.12 ms per-message processing latency) and inference energy consumption (0.25 mJ per inference) while achieving similar classification performance as state-of-the-art approaches in the literature.
&lt;/p&gt;</description></item><item><title>Spatial Scaper&#26159;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#21644;&#22686;&#24378;&#22768;&#22330;&#30340;&#24211;&#65292;&#21487;&#29992;&#20110;&#22768;&#38899;&#20107;&#20214;&#23450;&#20301;&#21644;&#26816;&#27979;&#12290;&#23427;&#36890;&#36807;&#27169;&#25311;&#34394;&#25311;&#25151;&#38388;&#21644;&#24212;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#26356;&#20855;&#22810;&#26679;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22768;&#23398;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12238</link><description>&lt;p&gt;
Spatial Scaper&#65306;&#19968;&#20010;&#29992;&#20110;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#27169;&#25311;&#21644;&#22686;&#24378;&#22768;&#22330;&#20197;&#36827;&#34892;&#22768;&#38899;&#20107;&#20214;&#23450;&#20301;&#21644;&#26816;&#27979;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
Spatial Scaper: A Library to Simulate and Augment Soundscapes for Sound Event Localization and Detection in Realistic Rooms. (arXiv:2401.12238v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12238
&lt;/p&gt;
&lt;p&gt;
Spatial Scaper&#26159;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#21644;&#22686;&#24378;&#22768;&#22330;&#30340;&#24211;&#65292;&#21487;&#29992;&#20110;&#22768;&#38899;&#20107;&#20214;&#23450;&#20301;&#21644;&#26816;&#27979;&#12290;&#23427;&#36890;&#36807;&#27169;&#25311;&#34394;&#25311;&#25151;&#38388;&#21644;&#24212;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#26356;&#20855;&#22810;&#26679;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22768;&#23398;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#20107;&#20214;&#23450;&#20301;&#21644;&#26816;&#27979;(SELD)&#26159;&#26426;&#22120;&#21548;&#35273;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#12290;&#20027;&#35201;&#30340;&#36827;&#23637;&#20381;&#36182;&#20110;&#22312;&#29305;&#23450;&#25151;&#38388;&#20013;&#20855;&#26377;&#22768;&#38899;&#20107;&#20214;&#21644;&#24378;&#26102;&#31354;&#26631;&#31614;&#30340;&#27169;&#25311;&#25968;&#25454;&#12290;SELD&#25968;&#25454;&#36890;&#36807;&#23558;&#31354;&#38388;&#23450;&#20301;&#30340;&#25151;&#38388;&#20914;&#28608;&#21709;&#24212;&#65288;RIRs&#65289;&#19982;&#22768;&#27874;&#24418;&#36827;&#34892;&#21367;&#31215;&#26469;&#27169;&#25311;&#65292;&#20197;&#23558;&#22768;&#38899;&#20107;&#20214;&#25918;&#32622;&#22312;&#22768;&#26223;&#20013;&#12290;&#28982;&#32780;&#65292;RIRs&#38656;&#35201;&#22312;&#29305;&#23450;&#25151;&#38388;&#20013;&#25163;&#21160;&#25910;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SpatialScaper&#65292;&#19968;&#20010;&#29992;&#20110;SELD&#25968;&#25454;&#27169;&#25311;&#21644;&#22686;&#24378;&#30340;&#24211;&#12290;&#19982;&#29616;&#26377;&#24037;&#20855;&#30456;&#27604;&#65292;SpatialScaper&#36890;&#36807;&#21442;&#25968;&#65288;&#22914;&#22823;&#23567;&#21644;&#22681;&#38754;&#21560;&#25910;&#65289;&#27169;&#25311;&#34394;&#25311;&#25151;&#38388;&#12290;&#36825;&#20801;&#35768;&#23545;&#21069;&#26223;&#21644;&#32972;&#26223;&#22768;&#28304;&#36827;&#34892;&#21442;&#25968;&#21270;&#25918;&#32622;&#65288;&#21253;&#25324;&#31227;&#21160;&#65289;&#12290;SpatialScaper&#36824;&#21253;&#25324;&#21487;&#24212;&#29992;&#20110;&#29616;&#26377;SELD&#25968;&#25454;&#30340;&#25968;&#25454;&#22686;&#24378;&#27969;&#27700;&#32447;&#12290;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#20351;&#29992;SpatialScaper&#21521;DCASE SELD&#25968;&#25454;&#20013;&#28155;&#21152;&#20102;&#25151;&#38388;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#23548;&#33268;&#20102;&#38543;&#30528;&#22768;&#23398;&#22810;&#26679;&#24615;&#30340;&#30452;&#25509;&#25552;&#39640;&#30340;&#28176;&#36827;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sound event localization and detection (SELD) is an important task in machine listening. Major advancements rely on simulated data with sound events in specific rooms and strong spatio-temporal labels. SELD data is simulated by convolving spatialy-localized room impulse responses (RIRs) with sound waveforms to place sound events in a soundscape. However, RIRs require manual collection in specific rooms. We present SpatialScaper, a library for SELD data simulation and augmentation. Compared to existing tools, SpatialScaper emulates virtual rooms via parameters such as size and wall absorption. This allows for parameterized placement (including movement) of foreground and background sound sources. SpatialScaper also includes data augmentation pipelines that can be applied to existing SELD data. As a case study, we use SpatialScaper to add rooms to the DCASE SELD data. Training a model with our data led to progressive performance improves as a direct function of acoustic diversity. These 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;D-Mapper&#30340;&#20998;&#24067;&#24341;&#23548;Mapper&#31639;&#27861;&#65292;&#20351;&#29992;&#27010;&#29575;&#27169;&#22411;&#21644;&#25968;&#25454;&#22266;&#26377;&#29305;&#24449;&#29983;&#25104;&#23494;&#24230;&#24341;&#23548;&#30340;&#35206;&#30422;&#65292;&#24182;&#25552;&#20379;&#22686;&#24378;&#30340;&#25299;&#25169;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.12237</link><description>&lt;p&gt;
&#19968;&#31181;&#20998;&#24067;&#24341;&#23548;&#30340;Mapper&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A distribution-guided Mapper algorithm. (arXiv:2401.12237v1 [math.AT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12237
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;D-Mapper&#30340;&#20998;&#24067;&#24341;&#23548;Mapper&#31639;&#27861;&#65292;&#20351;&#29992;&#27010;&#29575;&#27169;&#22411;&#21644;&#25968;&#25454;&#22266;&#26377;&#29305;&#24449;&#29983;&#25104;&#23494;&#24230;&#24341;&#23548;&#30340;&#35206;&#30422;&#65292;&#24182;&#25552;&#20379;&#22686;&#24378;&#30340;&#25299;&#25169;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#65306;Mapper&#31639;&#27861;&#26159;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#25506;&#32034;&#25968;&#25454;&#24418;&#29366;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#20351;&#29992;&#25968;&#25454;&#38598;&#20316;&#20026;&#36755;&#20837;&#65292;Mapper&#31639;&#27861;&#36755;&#20986;&#20195;&#34920;&#25972;&#20010;&#25968;&#25454;&#38598;&#25299;&#25169;&#29305;&#24449;&#30340;&#22270;&#24418;&#12290;&#36825;&#20010;&#22270;&#24418;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#25968;&#25454;&#30340;&#19968;&#20010;Reeb&#22270;&#30340;&#36817;&#20284;&#12290;&#32463;&#20856;&#30340;Mapper&#31639;&#27861;&#20351;&#29992;&#22266;&#23450;&#30340;&#21306;&#38388;&#38271;&#24230;&#21644;&#37325;&#21472;&#27604;&#29575;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25581;&#31034;&#25968;&#25454;&#30340;&#24494;&#22937;&#29305;&#24449;&#65292;&#23588;&#20854;&#26159;&#24403;&#24213;&#23618;&#32467;&#26500;&#22797;&#26434;&#26102;&#12290;&#32467;&#26524;&#65306;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;D-Mapper&#30340;&#20998;&#24067;&#24341;&#23548;Mapper&#31639;&#27861;&#65292;&#21033;&#29992;&#27010;&#29575;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#25968;&#25454;&#22266;&#26377;&#29305;&#24449;&#29983;&#25104;&#23494;&#24230;&#24341;&#23548;&#30340;&#35206;&#30422;&#65292;&#24182;&#25552;&#20379;&#22686;&#24378;&#30340;&#25299;&#25169;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#38750;&#27010;&#29575;&#24615;&#26041;&#27861;&#30340;&#26367;&#20195;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24230;&#37327;&#26469;&#32771;&#34385;&#37325;&#21472;&#32858;&#31867;&#30340;&#36136;&#37327;&#21644;&#25193;&#23637;&#25345;&#32493;&#21516;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: The Mapper algorithm is an essential tool to explore shape of data in topology data analysis. With a dataset as an input, the Mapper algorithm outputs a graph representing the topological features of the whole dataset. This graph is often regarded as an approximation of a reeb graph of data. The classic Mapper algorithm uses fixed interval lengths and overlapping ratios, which might fail to reveal subtle features of data, especially when the underlying structure is complex.  Results: In this work, we introduce a distribution guided Mapper algorithm named D-Mapper, that utilizes the property of the probability model and data intrinsic characteristics to generate density guided covers and provides enhanced topological features. Our proposed algorithm is a probabilistic model-based approach, which could serve as an alternative to non-prababilistic ones. Moreover, we introduce a metric accounting for both the quality of overlap clustering and extended persistence homology to me
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#21363;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#22122;&#22768;&#25968;&#25454;&#25311;&#21512;&#24471;&#24456;&#22909;&#65292;&#23545;&#25932;&#23545;&#31034;&#20363;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#24403;&#38754;&#20020;&#25932;&#23545;&#25805;&#32437;&#30340;&#25968;&#25454;&#26102;&#65292;&#36807;&#24230;&#25311;&#21512;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#32473;&#31995;&#32479;&#24102;&#26469;&#24847;&#22806;&#30340;&#21361;&#23475;&#12290;</title><link>http://arxiv.org/abs/2401.12236</link><description>&lt;p&gt;
&#26080;&#23475;&#36807;&#24230;&#25311;&#21512;&#23545;&#25932;&#23545;&#40065;&#26834;&#24615;&#30340;&#24847;&#22806;&#21361;&#23475;
&lt;/p&gt;
&lt;p&gt;
The Surprising Harmfulness of Benign Overfitting for Adversarial Robustness. (arXiv:2401.12236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#21363;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#22122;&#22768;&#25968;&#25454;&#25311;&#21512;&#24471;&#24456;&#22909;&#65292;&#23545;&#25932;&#23545;&#31034;&#20363;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#24403;&#38754;&#20020;&#25932;&#23545;&#25805;&#32437;&#30340;&#25968;&#25454;&#26102;&#65292;&#36807;&#24230;&#25311;&#21512;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#32473;&#31995;&#32479;&#24102;&#26469;&#24847;&#22806;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#35757;&#32451;&#22122;&#22768;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65306;&#21363;&#20351;&#30495;&#27491;&#30340;&#25968;&#25454;&#26412;&#36523;&#23545;&#25932;&#23545;&#31034;&#20363;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#36807;&#24230;&#25311;&#21512;&#30340;&#27169;&#22411;&#22312;&#8220;&#26631;&#20934;&#8221;&#30340;&#26679;&#26412;&#22806;&#39118;&#38505;&#30446;&#26631;&#19978;&#26159;&#26080;&#23475;&#30340;&#65292;&#20294;&#22312;&#26679;&#26412;&#22806;&#25968;&#25454;&#21463;&#21040;&#25932;&#23545;&#25805;&#32437;&#26102;&#65292;&#36825;&#31181;&#26080;&#23475;&#30340;&#36807;&#24230;&#25311;&#21512;&#36807;&#31243;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#21253;&#21547;&#20004;&#20010;&#37096;&#20998;&#65306;&#65288;i&#65289;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#20013;&#65292;&#26368;&#23567;&#33539;&#25968;&#20272;&#35745;&#24635;&#26159;&#22312;&#8220;&#26080;&#23475;&#36807;&#24230;&#25311;&#21512;&#8221;&#35774;&#32622;&#20013;&#23548;&#33268;&#25932;&#23545;&#26131;&#21463;&#25915;&#20987;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#39564;&#35777;&#20102;&#27599;&#20010;&#23725;&#22238;&#24402;&#20272;&#35745;&#22120;&#30340;&#26631;&#20934;&#39118;&#38505;&#21644;&#8220;&#25932;&#23545;&#8221;&#39118;&#38505;&#20043;&#38388;&#30340;&#28176;&#36827;&#26435;&#34913;&#32467;&#26524;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#36866;&#24403;&#30340;&#26465;&#20214;&#19979;&#65292;&#36825;&#20004;&#20010;&#39033;&#30446;&#19981;&#33021;&#21516;&#26102;&#36890;&#36807;&#20219;&#20309;&#21333;&#20010;&#23725;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#36873;&#25321;&#26469;&#20445;&#25345;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent empirical and theoretical studies have established the generalization capabilities of large machine learning models that are trained to (approximately or exactly) fit noisy data. In this work, we prove a surprising result that even if the ground truth itself is robust to adversarial examples, and the benignly overfitted model is benign in terms of the ``standard'' out-of-sample risk objective, this benign overfitting process can be harmful when out-of-sample data are subject to adversarial manipulation. More specifically, our main results contain two parts: (i) the min-norm estimator in overparameterized linear model always leads to adversarial vulnerability in the ``benign overfitting'' setting; (ii) we verify an asymptotic trade-off result between the standard risk and the ``adversarial'' risk of every ridge regression estimator, implying that under suitable conditions these two items cannot both be small at the same time by any single choice of the ridge regularization parame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#20803;&#22270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20855;&#26377;&#39640;&#27867;&#21270;&#24615;&#21644;&#23569;&#26679;&#26412;&#36866;&#24212;&#24615;&#30340;&#38543;&#26426;&#21160;&#24577;&#30005;&#21147;&#20998;&#37197;&#12290;&#36890;&#36807;&#24341;&#20837;&#26356;&#19968;&#33324;&#21270;&#30340;&#19978;&#19979;&#25991;MDP&#21644;&#21487;&#25193;&#23637;&#30340;&#22270;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22810;&#21464;&#37327;&#19981;&#30830;&#23450;&#24615;&#30340;&#23454;&#26102;&#22810;&#38454;&#27573;&#38543;&#26426;&#30005;&#21147;&#20998;&#37197;&#38382;&#39064;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#27867;&#21270;&#24615;&#21644;&#23454;&#29992;&#24615;&#20302;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.12235</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#20803;&#22270;&#24378;&#21270;&#23398;&#20064;&#65292;&#23454;&#29616;&#20855;&#26377;&#39640;&#27867;&#21270;&#24615;&#21644;&#23569;&#26679;&#26412;&#36866;&#24212;&#24615;&#30340;&#38543;&#26426;&#21160;&#24577;&#30005;&#21147;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Stochastic Dynamic Power Dispatch with High Generalization and Few-Shot Adaption via Contextual Meta Graph Reinforcement Learning. (arXiv:2401.12235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#20803;&#22270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20855;&#26377;&#39640;&#27867;&#21270;&#24615;&#21644;&#23569;&#26679;&#26412;&#36866;&#24212;&#24615;&#30340;&#38543;&#26426;&#21160;&#24577;&#30005;&#21147;&#20998;&#37197;&#12290;&#36890;&#36807;&#24341;&#20837;&#26356;&#19968;&#33324;&#21270;&#30340;&#19978;&#19979;&#25991;MDP&#21644;&#21487;&#25193;&#23637;&#30340;&#22270;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22810;&#21464;&#37327;&#19981;&#30830;&#23450;&#24615;&#30340;&#23454;&#26102;&#22810;&#38454;&#27573;&#38543;&#26426;&#30005;&#21147;&#20998;&#37197;&#38382;&#39064;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#27867;&#21270;&#24615;&#21644;&#23454;&#29992;&#24615;&#20302;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#38454;&#27573;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30340;&#26032;&#20852;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32771;&#34385;&#22810;&#21464;&#37327;&#19981;&#30830;&#23450;&#24615;&#30340;&#23454;&#26102;&#22810;&#38454;&#27573;&#38543;&#26426;&#30005;&#21147;&#20998;&#37197;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#23384;&#22312;&#27867;&#21270;&#24615;&#21644;&#23454;&#29992;&#24615;&#19981;&#39640;&#30340;&#38382;&#39064;&#65292;&#21363;&#23398;&#20064;&#21040;&#30340;&#20998;&#37197;&#31574;&#30053;&#21482;&#33021;&#22788;&#29702;&#29305;&#23450;&#24773;&#26223;&#19979;&#30340;&#20998;&#37197;&#38382;&#39064;&#65292;&#22914;&#26524;&#23454;&#38469;&#26679;&#26412;&#21644;&#35757;&#32451;&#26679;&#26412;&#19981;&#19968;&#33268;&#65292;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#20803;&#22270;&#24378;&#21270;&#23398;&#20064;&#65288;Meta-GRL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#30340;&#22810;&#38454;&#27573;&#26368;&#20248;&#20998;&#37197;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24341;&#20837;&#20102;&#26356;&#19968;&#33324;&#21270;&#30340;&#19978;&#19979;&#25991;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#21644;&#21487;&#25193;&#23637;&#30340;&#22270;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#26356;&#19968;&#33324;&#21270;&#30340;&#22810;&#38454;&#27573;&#38543;&#26426;&#30005;&#21147;&#20998;&#37197;&#24314;&#27169;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#23618;&#20803;&#23398;&#20064;&#22120;&#65292;&#29992;&#20110;&#23545;&#19981;&#21516;&#30340;&#20998;&#37197;&#24773;&#26223;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#23398;&#20064;&#22914;&#20309;&#23454;&#29616;&#20998;&#37197;&#20219;&#21153;&#30340;&#35782;&#21035;&#65292;&#32780;&#19979;&#23618;&#31574;&#30053;&#23398;&#20064;&#22120;&#21017;&#23398;&#20064;&#20855;&#20307;&#30340;&#30005;&#21147;&#20998;&#37197;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is an emerging approaches to facilitate multi-stage sequential decision-making problems. This paper studies a real-time multi-stage stochastic power dispatch considering multivariate uncertainties. Current researches suffer from low generalization and practicality, that is, the learned dispatch policy can only handle a specific dispatch scenario, its performance degrades significantly if actual samples and training samples are inconsistent. To fill these gaps, a novel contextual meta graph reinforcement learning (Meta-GRL) for a highly generalized multi-stage optimal dispatch policy is proposed. Specifically, a more general contextual Markov decision process (MDP) and scalable graph representation are introduced to achieve a more generalized multi-stage stochastic power dispatch modeling. An upper meta-learner is proposed to encode context for different dispatch scenarios and learn how to achieve dispatch task identification while the lower policy learner learns 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423;FPGA&#30340;IDS-ECU&#26550;&#26500;&#65292;&#22312;&#20256;&#32479;ECU&#21151;&#33021;&#30340;&#22522;&#30784;&#19978;&#65292;&#29992;&#20110;&#27773;&#36710;CAN&#24635;&#32447;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(IDS)&#12290;&#36890;&#36807;&#20351;&#29992;&#29616;&#25104;&#30340;&#28145;&#24230;&#23398;&#20064;&#22788;&#29702;&#21333;&#20803;(DPU)IP&#22359;&#36827;&#34892;&#21152;&#36895;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#26816;&#27979;&#22810;&#31181;&#25915;&#20987;&#21521;&#37327;&#65292;&#24182;&#20855;&#26377;&#20960;&#20046;&#38646;&#30340;ECU&#21151;&#33021;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2401.12234</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#27773;&#36710;CAN&#24635;&#32447;&#30340;&#22522;&#20110;&#36731;&#37327;&#32423;FPGA&#30340;IDS-ECU&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Lightweight FPGA-based IDS-ECU Architecture for Automotive CAN. (arXiv:2401.12234v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423;FPGA&#30340;IDS-ECU&#26550;&#26500;&#65292;&#22312;&#20256;&#32479;ECU&#21151;&#33021;&#30340;&#22522;&#30784;&#19978;&#65292;&#29992;&#20110;&#27773;&#36710;CAN&#24635;&#32447;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(IDS)&#12290;&#36890;&#36807;&#20351;&#29992;&#29616;&#25104;&#30340;&#28145;&#24230;&#23398;&#20064;&#22788;&#29702;&#21333;&#20803;(DPU)IP&#22359;&#36827;&#34892;&#21152;&#36895;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#26816;&#27979;&#22810;&#31181;&#25915;&#20987;&#21521;&#37327;&#65292;&#24182;&#20855;&#26377;&#20960;&#20046;&#38646;&#30340;ECU&#21151;&#33021;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36710;&#36742;&#20013;&#22797;&#26434;&#30340;&#36719;&#20214;&#39537;&#21160;&#21151;&#33021;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#23548;&#33268;&#30005;&#23376;&#25511;&#21046;&#21333;&#20803;(ECU)&#12289;&#32593;&#32476;&#33021;&#21147;&#21644;&#25509;&#21475;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#12290;&#25193;&#23637;&#30340;&#21151;&#33021;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#28431;&#27934;&#23618;&#38754;&#65292;&#20351;&#20837;&#20405;&#26816;&#27979;&#21644;&#31649;&#29702;&#25104;&#20026;&#20851;&#38190;&#33021;&#21147;&#65307;&#28982;&#32780;&#65292;&#36825;&#24448;&#24448;&#20250;&#23548;&#33268;&#26356;&#22810;&#30340;ECU&#21644;&#32593;&#32476;&#20803;&#32032;&#65292;&#22240;&#20026;&#35745;&#31639;&#24320;&#38144;&#24456;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#30340;ECU&#26550;&#26500;&#65292;&#21253;&#25324;&#22522;&#20110;&#29616;&#25104;&#28151;&#21512;FPGA&#35774;&#22791;&#30340;&#27773;&#36710;&#25511;&#21046;&#22120;&#21306;&#22495;&#32593;&#32476;(CAN)&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(IDS)&#65292;&#20197;&#21450;&#20256;&#32479;ECU&#21151;&#33021;&#65292;&#23545;&#20110;ECU&#21151;&#33021;&#20960;&#20046;&#27809;&#26377;&#24320;&#38144;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#37327;&#21270;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;(QMLP)&#20316;&#20026;&#29420;&#31435;&#30340;IDS&#65292;&#29992;&#20110;&#26816;&#27979;&#19968;&#31995;&#21015;&#25915;&#20987;&#21521;&#37327;&#65292;&#21253;&#25324;&#25298;&#32477;&#26381;&#21153;&#12289;&#27169;&#31946;&#27979;&#35797;&#21644;&#27450;&#39575;&#25915;&#20987;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;Xilinx&#30340;&#29616;&#25104;&#28145;&#24230;&#23398;&#20064;&#22788;&#29702;&#21333;&#20803;(DPU)IP&#22359;&#36827;&#34892;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen an exponential rise in complex software-driven functionality in vehicles, leading to a rising number of electronic control units (ECUs), network capabilities, and interfaces. These expanded capabilities also bring-in new planes of vulnerabilities making intrusion detection and management a critical capability; however, this can often result in more ECUs and network elements due to the high computational overheads. In this paper, we present a consolidated ECU architecture incorporating an Intrusion Detection System (IDS) for Automotive Controller Area Network (CAN) along with traditional ECU functionality on an off-the-shelf hybrid FPGA device, with near-zero overhead for the ECU functionality. We propose two quantised multi-layer perceptrons (QMLP's) as isolated IDSs for detecting a range of attack vectors including Denial-of-Service, Fuzzing and Spoofing, which are accelerated using off-the-shelf deep-learning processing unit (DPU) IP block from Xilinx, operatin
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#21270;&#38382;&#39064;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SSLMem&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#20041;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#24378;&#25968;&#25454;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#65292;&#35760;&#24518;&#21270;&#20173;&#28982;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2401.12233</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#21270;&#25552;&#39640;&#20102;&#19979;&#28216;&#27010;&#25324;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Memorization in Self-Supervised Learning Improves Downstream Generalization. (arXiv:2401.12233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12233
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#21270;&#38382;&#39064;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SSLMem&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#20041;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#24378;&#25968;&#25454;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#65292;&#35760;&#24518;&#21270;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22240;&#20854;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#35757;&#32451;&#39640;&#24615;&#33021;&#32534;&#30721;&#22120;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#26469;&#28304;&#20110;&#20114;&#32852;&#32593;&#30340;&#25235;&#21462;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;SSL&#32534;&#30721;&#22120;&#20250;&#35760;&#24518;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#31169;&#20154;&#20449;&#24687;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#27844;&#38706;&#36825;&#20123;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#30417;&#30563;&#23398;&#20064;&#35760;&#24518;&#21270;&#30340;&#29702;&#35770;&#23450;&#20041;&#20381;&#36182;&#20110;&#26631;&#31614;&#65292;&#22240;&#27492;&#26080;&#27861;&#36866;&#29992;&#20110;SSL&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SSLMem&#65292;&#19968;&#20010;&#22312;SSL&#20869;&#23450;&#20041;&#35760;&#24518;&#21270;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#23450;&#20041;&#36890;&#36807;&#27604;&#36739;&#35757;&#32451;&#22312;&#36825;&#20123;&#25968;&#25454;&#28857;&#19978;&#30340;&#32534;&#30721;&#22120;&#21644;&#26410;&#34987;&#35757;&#32451;&#22312;&#36825;&#20123;&#25968;&#25454;&#28857;&#19978;&#30340;&#32534;&#30721;&#22120;&#36820;&#22238;&#30340;&#25968;&#25454;&#28857;&#21644;&#20182;&#20204;&#30340;&#22686;&#24378;&#35270;&#22270;&#30340;&#34920;&#31034;&#30340;&#23545;&#40784;&#24046;&#24322;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#30340;&#32508;&#21512;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#21363;&#20351;SSL&#20381;&#36182;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#24378;&#22823;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#36825;&#37117;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#20316;&#20026;&#27491;&#21017;&#21270;&#25163;&#27573;&#30340;&#24050;&#30693;&#25216;&#26415;&#65292;&#35760;&#24518;&#21270;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently received significant attention due to its ability to train high-performance encoders purely on unlabeled data-often scraped from the internet. This data can still be sensitive and empirical evidence suggests that SSL encoders memorize private information of their training data and can disclose them at inference time. Since existing theoretical definitions of memorization from supervised learning rely on labels, they do not transfer to SSL. To address this gap, we propose SSLMem, a framework for defining memorization within SSL. Our definition compares the difference in alignment of representations for data points and their augmented views returned by both encoders that were trained on these data points and encoders that were not. Through comprehensive empirical analysis on diverse encoder architectures and datasets we highlight that even though SSL relies on large datasets and strong augmentations-both known in supervised learning as regulari
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25581;&#31034;&#20102;SiRNA&#32467;&#26500;&#19982;&#25928;&#21147;&#20851;&#31995;&#65292;&#20026;&#25239;&#20987;Sars-Cov-2 Spike&#22522;&#22240;&#25552;&#20379;&#20102;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.12232</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24314;&#27169;SiRNA&#32467;&#26500;&#19982;&#25928;&#21147;&#20851;&#31995;&#21450;&#20854;&#22312;Sars-Cov-2 Spike&#22522;&#22240;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Modeling Of SiRNA Structure-Potency Relationship With Applications Against Sars-Cov-2 Spike Gene. (arXiv:2401.12232v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12232
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25581;&#31034;&#20102;SiRNA&#32467;&#26500;&#19982;&#25928;&#21147;&#20851;&#31995;&#65292;&#20026;&#25239;&#20987;Sars-Cov-2 Spike&#22522;&#22240;&#25552;&#20379;&#20102;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#33647;&#30740;&#21457;&#36807;&#31243;&#28459;&#38271;&#19988;&#26114;&#36149;&#65292;&#23558;&#36817;&#21313;&#24180;&#26102;&#38388;&#25165;&#33021;&#23558;&#19968;&#31181;&#26032;&#33647;&#24102;&#21040;&#24066;&#22330;&#19978;&#12290;&#28982;&#32780;&#65292;&#29983;&#29289;&#25216;&#26415;&#12289;&#35745;&#31639;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#36827;&#27493;&#20855;&#26377;&#25913;&#38761;&#33647;&#29289;&#21457;&#29616;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#21152;&#24555;&#36827;&#31243;&#24182;&#25913;&#21892;&#24739;&#32773;&#30340;&#32467;&#26524;&#12290;COVID-19&#30123;&#24773;&#36827;&#19968;&#27493;&#21152;&#36895;&#21644;&#28145;&#21270;&#20102;&#23545;&#36825;&#20123;&#25216;&#26415;&#30340;&#28508;&#21147;&#30340;&#35748;&#35782;&#65292;&#29305;&#21035;&#26159;&#22312;&#33647;&#29289;&#37325;&#29992;&#21644;&#30103;&#25928;&#39044;&#27979;&#26041;&#38754;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#38750;&#23567;&#20998;&#23376;&#27835;&#30103;&#27169;&#24335;&#22914;&#32454;&#32990;&#30103;&#27861;&#12289;&#21333;&#20811;&#38534;&#25239;&#20307;&#21644;RNA&#24178;&#25200;&#25216;&#26415;&#22240;&#20854;&#33021;&#22815;&#38024;&#23545;&#29305;&#23450;&#30142;&#30149;&#36884;&#24452;&#21644;/&#25110;&#24739;&#32773;&#20154;&#32676;&#32780;&#21464;&#24471;&#37325;&#35201;&#12290;&#22312;RNA&#24178;&#25200;&#39046;&#22495;&#65292;&#24050;&#36827;&#34892;&#20102;&#35768;&#22810;&#23454;&#39564;&#20197;&#35774;&#35745;&#21644;&#36873;&#25321;&#39640;&#25928;&#30340;siRNA&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#39640;&#25928;siRNA&#30340;&#24050;&#24314;&#31435;&#27169;&#24335;&#26377;&#26102;&#30683;&#30462;&#24182;&#19988;&#26080;&#27861;&#19968;&#33268;&#22320;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pharmaceutical Research and development (R&amp;D) process is lengthy and costly, taking nearly a decade to bring a new drug to the market. However, advancements in biotechnology, computational methods, and machine learning algorithms have the potential to revolutionize drug discovery, speeding up the process and improving patient outcomes. The COVID-19 pandemic has further accelerated and deepened the recognition of the potential of these techniques, especially in the areas of drug repurposing and efficacy predictions. Meanwhile, non-small molecule therapeutic modalities such as cell therapies, monoclonal antibodies, and RNA interference (RNAi) technology have gained importance due to their ability to target specific disease pathways and/or patient populations. In the field of RNAi, many experiments have been carried out to design and select highly efficient siRNAs. However, the established patterns for efficient siRNAs are sometimes contradictory and unable to consistently determine t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22823;&#35268;&#27169;&#22270;&#30340;&#35299;&#32544;&#32467;&#20957;&#32858;&#26041;&#27861;DisCo&#65292;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#30340;&#20957;&#32858;&#27169;&#22359;&#23454;&#29616;&#20102;&#23545;&#22823;&#35268;&#27169;&#22270;&#30340;&#39640;&#25928;&#32553;&#20957;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#21387;&#32553;&#22270;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.12231</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#30340;&#35299;&#32544;&#32467;&#20957;&#32858;
&lt;/p&gt;
&lt;p&gt;
Disentangled Condensation for Large-scale Graphs. (arXiv:2401.12231v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22823;&#35268;&#27169;&#22270;&#30340;&#35299;&#32544;&#32467;&#20957;&#32858;&#26041;&#27861;DisCo&#65292;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#30340;&#20957;&#32858;&#27169;&#22359;&#23454;&#29616;&#20102;&#23545;&#22823;&#35268;&#27169;&#22270;&#30340;&#39640;&#25928;&#32553;&#20957;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#21387;&#32553;&#22270;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#35299;&#32544;&#32467;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#36259;&#30340;&#25216;&#26415;&#65292;&#20026;&#22823;&#35268;&#27169;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#32039;&#20945;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#23567;&#22270;&#65292;&#20197;&#33410;&#30465;&#22823;&#35268;&#27169;&#22270;&#23398;&#20064;&#30340;&#26114;&#36149;&#25104;&#26412;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20294;&#20808;&#21069;&#30340;&#22270;&#35299;&#32544;&#32467;&#26041;&#27861;&#24120;&#24120;&#37319;&#29992;&#32416;&#32544;&#30340;&#32553;&#20957;&#31574;&#30053;&#65292;&#21516;&#26102;&#28041;&#21450;&#33410;&#28857;&#21644;&#36793;&#30340;&#32553;&#20957;&#65292;&#23548;&#33268;&#22823;&#37327;&#30340;GPU&#20869;&#23384;&#38656;&#27714;&#12290;&#36825;&#31181;&#32416;&#32544;&#30340;&#31574;&#30053;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#22270;&#35299;&#32544;&#32467;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#21066;&#24369;&#20102;&#23427;&#23545;&#26497;&#22823;&#35268;&#27169;&#22270;&#30340;&#32553;&#20957;&#21644;&#39640;&#20445;&#30495;&#24230;&#21387;&#32553;&#22270;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22823;&#35268;&#27169;&#22270;&#30340;&#35299;&#32544;&#32467;&#20957;&#32858;&#65292;&#31616;&#31216;&#20026;DisCo&#65292;&#20197;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#22270;&#35299;&#32544;&#32467;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#35268;&#27169;&#30340;&#22270;&#12290;DisCo&#30340;&#26680;&#24515;&#26159;&#20004;&#20010;&#20114;&#34917;&#30340;&#32452;&#20214;&#65292;&#21363;&#33410;&#28857;&#21644;&#36793;&#30340;&#20957;&#32858;&#27169;&#22359;&#65292;&#22312;&#35299;&#32544;&#30340;&#26041;&#24335;&#19979;&#23454;&#29616;&#33410;&#28857;&#21644;&#36793;&#30340;&#20957;&#32858;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph condensation has emerged as an intriguing technique to provide Graph Neural Networks for large-scale graphs with a more compact yet informative small graph to save the expensive costs of large-scale graph learning. Despite the promising results achieved, previous graph condensation methods often employ an entangled condensation strategy that involves condensing nodes and edges simultaneously, leading to substantial GPU memory demands. This entangled strategy has considerably impeded the scalability of graph condensation, impairing its capability to condense extremely large-scale graphs and produce condensed graphs with high fidelity. Therefore, this paper presents Disentangled Condensation for large-scale graphs, abbreviated as DisCo, to provide scalable graph condensation for graphs of varying sizes. At the heart of DisCo are two complementary components, namely node and edge condensation modules, that realize the condensation of nodes and edges in a disentangled manner. In the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;&#21644;&#20113;&#21407;&#29983;&#35745;&#31639;&#26550;&#26500;&#30340;&#20132;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20113;&#21407;&#29983;&#25216;&#26415;&#21644;&#39640;&#32423;&#26426;&#22120;&#23398;&#20064;&#36816;&#34892;&#26102;&#30340;AI&#21407;&#29983;&#35745;&#31639;&#33539;&#24335;&#65292;&#26088;&#22312;&#20248;&#21270;&#25104;&#26412;&#24182;&#25552;&#39640;&#36164;&#28304;&#21487;&#35775;&#38382;&#24615;&#65292;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.12230</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#26102;&#20195;&#30340;&#35745;&#31639;&#65306;&#20174;&#20113;&#21407;&#29983;&#21040;AI&#21407;&#29983;
&lt;/p&gt;
&lt;p&gt;
Computing in the Era of Large Generative Models: From Cloud-Native to AI-Native. (arXiv:2401.12230v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;&#21644;&#20113;&#21407;&#29983;&#35745;&#31639;&#26550;&#26500;&#30340;&#20132;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20113;&#21407;&#29983;&#25216;&#26415;&#21644;&#39640;&#32423;&#26426;&#22120;&#23398;&#20064;&#36816;&#34892;&#26102;&#30340;AI&#21407;&#29983;&#35745;&#31639;&#33539;&#24335;&#65292;&#26088;&#22312;&#20248;&#21270;&#25104;&#26412;&#24182;&#25552;&#39640;&#36164;&#28304;&#21487;&#35775;&#38382;&#24615;&#65292;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;&#21644;&#20113;&#21407;&#29983;&#35745;&#31639;&#26550;&#26500;&#30340;&#20132;&#38598;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#22312;&#21151;&#33021;&#19978;&#20855;&#26377;&#38761;&#21629;&#24615;&#65292;&#20294;&#38754;&#20020;&#30528;&#25104;&#26412;&#19978;&#28072;&#21644;&#23545;&#39640;&#31471;GPU&#38656;&#27714;&#22686;&#21152;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#22823;&#22411;&#27169;&#22411;&#20316;&#20026;&#26381;&#21153;&#65288;LMaaS&#65289;&#21644;&#20113;&#25968;&#25454;&#24211;&#20316;&#20026;&#26381;&#21153;&#65288;DBaaS&#65289;&#36827;&#34892;&#31867;&#27604;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;AI&#21407;&#29983;&#35745;&#31639;&#33539; Paradigm, &#20854;&#21033;&#29992;&#20102;&#20113;&#21407;&#29983;&#25216;&#26415;&#65288;&#22914;&#22810;&#31199;&#25143;&#21644;&#26080;&#26381;&#21153;&#22120;&#35745;&#31639;&#65289;&#21644;&#39640;&#32423;&#26426;&#22120;&#23398;&#20064;&#36816;&#34892;&#26102;&#65288;&#22914;&#25209;&#37327;LoRA&#25512;&#29702;&#65289;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#20849;&#21516;&#30340;&#21162;&#21147;&#26088;&#22312;&#20248;&#21270;&#25104;&#26412;&#21644;&#25552;&#39640;&#36164;&#28304;&#21487;&#35775;&#38382;&#24615;&#12290;&#23558;&#36825;&#20004;&#20010;&#39046;&#22495;&#34701;&#21512;&#30340;&#26053;&#31243;&#25165;&#21018;&#21018;&#24320;&#22987;&#65292;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#26410;&#26469;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the intersection of large generative AI models and cloud-native computing architectures. Recent large models such as ChatGPT, while revolutionary in their capabilities, face challenges like escalating costs and demand for high-end GPUs. Drawing analogies between large-model-as-a-service (LMaaS) and cloud database-as-a-service (DBaaS), we describe an AI-native computing paradigm that harnesses the power of both cloud-native technologies (e.g., multi-tenancy and serverless computing) and advanced machine learning runtime (e.g., batched LoRA inference). These joint efforts aim to optimize costs-of-goods-sold (COGS) and improve resource accessibility. The journey of merging these two domains is just at the beginning and we hope to stimulate future research and development in this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#30446;&#26631;&#26816;&#27979;&#21644;&#24369;&#30417;&#30563;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25972;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#22312;DataComp&#31454;&#36187;&#30340;&#36807;&#28388;&#22120;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;4%&#33267;4.2%&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.12225</link><description>&lt;p&gt;
&#36890;&#36807;&#30446;&#26631;&#26816;&#27979;&#21644;&#36807;&#28388;&#22120;&#38598;&#25104;&#36827;&#34892;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#29702;
&lt;/p&gt;
&lt;p&gt;
Multimodal Data Curation via Object Detection and Filter Ensembles. (arXiv:2401.12225v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#30446;&#26631;&#26816;&#27979;&#21644;&#24369;&#30417;&#30563;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25972;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#22312;DataComp&#31454;&#36187;&#30340;&#36807;&#28388;&#22120;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;4%&#33267;4.2%&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25972;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;2023&#24180;DataComp&#31454;&#36187;&#30340;&#36807;&#28388;&#22120;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#32467;&#21512;&#20102;&#30446;&#26631;&#26816;&#27979;&#21644;&#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#38598;&#25104;&#23398;&#20064;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24320;&#31665;&#21363;&#29992;&#30340;&#38646;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#25552;&#21462;&#32454;&#31890;&#24230;&#20449;&#24687;&#24182;&#29983;&#25104;&#21508;&#31181;&#36807;&#28388;&#22120;&#35774;&#35745;&#12290;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24369;&#30417;&#30563;&#26469;&#38598;&#25104;&#36807;&#28388;&#35268;&#21017;&#12290;&#36825;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20339;&#22522;&#20934;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;4&#65285;&#65292;&#22312;&#25776;&#20889;&#26412;&#25991;&#26102;&#22312;&#23567;&#35268;&#27169;&#36712;&#36947;&#19978;&#25490;&#21517;&#31532;&#19968;&#12290;&#27492;&#22806;&#65292;&#22312;&#20013;&#31561;&#35268;&#27169;&#36712;&#36947;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#22320;&#20351;&#29992;&#29616;&#26377;&#22522;&#32447;&#21644;&#24369;&#30417;&#30563;&#36827;&#34892;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#30456;&#23545;&#20110;&#22522;&#32447;&#30340;&#26174;&#30528;4.2&#65285;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach for curating multimodal data that we used for our entry in the 2023 DataComp competition filtering track. Our technique combines object detection and weak supervision-based ensembling. In the first of two steps in our approach, we employ an out-of-the-box zero-shot object detection model to extract granular information and produce a variety of filter designs. In the second step, we employ weak supervision to ensemble filtering rules. This approach results in a 4% performance improvement when compared to the best-performing baseline, producing the top-ranking position in the small scale track at the time of writing. Furthermore, in the medium scale track, we achieve a noteworthy 4.2% improvement over the baseline by simply ensembling existing baselines with weak supervision.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#22810;&#32454;&#32990;&#38598;&#21512;&#20307;&#30340;&#36816;&#21160;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#22810;&#32454;&#32990;&#29983;&#29289;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#22270;&#29305;&#24449;&#65292;&#24182;&#36229;&#36234;&#20256;&#32479;&#26426;&#26800;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#32773;&#24314;&#35758;&#36890;&#36807;&#21512;&#20316;&#21162;&#21147;&#26469;&#26500;&#24314;&#19968;&#20010;&#22810;&#32454;&#32990;&#25968;&#25454;&#24211;&#65292;&#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#22810;&#32454;&#32990;&#21160;&#21147;&#23398;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.12196</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#22810;&#32454;&#32990;&#22270;&#20013;&#23398;&#20064;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Learning Dynamics from Multicellular Graphs with Deep Neural Networks. (arXiv:2401.12196v1 [physics.bio-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#22810;&#32454;&#32990;&#38598;&#21512;&#20307;&#30340;&#36816;&#21160;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#22810;&#32454;&#32990;&#29983;&#29289;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#22270;&#29305;&#24449;&#65292;&#24182;&#36229;&#36234;&#20256;&#32479;&#26426;&#26800;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#32773;&#24314;&#35758;&#36890;&#36807;&#21512;&#20316;&#21162;&#21147;&#26469;&#26500;&#24314;&#19968;&#20010;&#22810;&#32454;&#32990;&#25968;&#25454;&#24211;&#65292;&#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#22810;&#32454;&#32990;&#21160;&#21147;&#23398;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32454;&#32990;&#33258;&#32452;&#35013;&#30340;&#25512;&#26029;&#26159;&#29702;&#35299;&#24418;&#24577;&#21457;&#29983;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#21253;&#25324;&#32986;&#32974;&#12289;&#22120;&#23448;&#32467;&#26500;&#12289;&#32959;&#30244;&#31561;&#12290;&#28982;&#32780;&#65292;&#24456;&#38590;&#25214;&#21040;&#33021;&#22815;&#25351;&#31034;&#22810;&#32454;&#32990;&#21160;&#21147;&#23398;&#30340;&#32467;&#26500;&#29305;&#24449;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#39044;&#27979;&#33021;&#21147;&#26469;&#21457;&#29616;&#21487;&#20197;&#39044;&#27979;&#21160;&#21147;&#23398;&#30340;&#37325;&#35201;&#22270;&#29305;&#24449;&#12290;&#20026;&#20102;&#35777;&#26126;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#20010;&#29289;&#29702;&#23398;&#21551;&#21457;&#30340; GNN&#65288;piGNN&#65289;&#26469;&#39044;&#27979;&#22810;&#32454;&#32990;&#38598;&#21512;&#20307;&#30340;&#36816;&#21160;&#33021;&#21147;&#65292;&#20174;&#23427;&#20204;&#22312;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;&#30340;&#20301;&#32622;&#24555;&#29031;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102; piGNN &#33021;&#22815;&#22312;&#22810;&#32454;&#32990;&#29983;&#29289;&#31995;&#32479;&#30340;&#22797;&#26434;&#22270;&#29305;&#24449;&#20013;&#23548;&#33322;&#65292;&#36825;&#26159;&#32463;&#20856;&#26426;&#26800;&#27169;&#22411;&#26080;&#27861;&#23454;&#29616;&#30340;&#12290;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#22810;&#32454;&#32990;&#25968;&#25454;&#30340;&#31215;&#32047;&#65292;&#25105;&#20204;&#25552;&#20986;&#21487;&#20197;&#36827;&#34892;&#21512;&#20316;&#21162;&#21147;&#65292;&#21019;&#24314;&#19968;&#20010;&#22810;&#32454;&#32990;&#25968;&#25454;&#24211;&#65288;MDB&#65289;&#65292;&#20174;&#20013;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#30340;&#22810;&#32454;&#32990;&#22270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inference of multicellular self-assembly is the central quest of understanding morphogenesis, including embryos, organoids, tumors, and many others. However, it has been tremendously difficult to identify structural features that can indicate multicellular dynamics. Here we propose to harness the predictive power of graph-based deep neural networks (GNN) to discover important graph features that can predict dynamics. To demonstrate, we apply a physically informed GNN (piGNN) to predict the motility of multicellular collectives from a snapshot of their positions both in experiments and simulations. We demonstrate that piGNN is capable of navigating through complex graph features of multicellular living systems, which otherwise can not be achieved by classical mechanistic models. With increasing amounts of multicellular data, we propose that collaborative efforts can be made to create a multicellular data bank (MDB) from which it is possible to construct a large multicellular graph m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20302;&#23433;&#20840;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#20197;&#20934;&#30830;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.11792</link><description>&lt;p&gt;
&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20302;&#23433;&#20840;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#20197;&#20934;&#30830;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26234;&#33021;&#39550;&#39542;&#31995;&#32479;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#24403;&#21069;&#29615;&#22659;&#21644;&#36710;&#36742;&#29366;&#24577;&#21160;&#24577;&#21046;&#23450;&#36866;&#24403;&#30340;&#39550;&#39542;&#31574;&#30053;&#65292;&#21516;&#26102;&#30830;&#20445;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#23433;&#20840;&#24615;&#20302;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#65292;&#32780;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#26159;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#30340;&#21069;&#25552;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#32780;&#22810;&#26679;&#22330;&#26223;&#19979;&#30340;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#12290;&#25105;&#20204;&#30340;SGADS&#19982;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#32467;&#21512;&#65292;&#20351;&#26234;&#33021;&#36710;&#36742;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#31034;&#33539;&#30456;&#32467;&#21512;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
An intelligent driving system should be capable of dynamically formulating appropriate driving strategies based on the current environment and vehicle status, while ensuring the security and reliability of the system. However, existing methods based on reinforcement learning and imitation learning suffer from low safety, poor generalization, and inefficient sampling. Additionally, they cannot accurately predict future driving trajectories, and the accurate prediction of future driving trajectories is a precondition for making optimal decisions. To solve these problems, in this paper, we introduce a Safe and Generalized end-to-end Autonomous Driving System (SGADS) for complex and various scenarios. Our SGADS incorporates variational inference with normalizing flows, enabling the intelligent vehicle to accurately predict future driving trajectories. Moreover, we propose the formulation of robust safety constraints. Furthermore, we combine reinforcement learning with demonstrations to aug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26041;&#27861;GI-PIP&#65292;&#19981;&#38656;&#35201;&#20381;&#36182;&#19981;&#20999;&#23454;&#38469;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20174;&#36739;&#23569;&#30340;&#25968;&#25454;&#20013;&#25429;&#33719;&#24213;&#23618;&#20998;&#24067;&#65292;&#24182;&#33021;&#22312;&#22270;&#20687;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#20063;&#26356;&#24378;&#22823;&#12290;</title><link>http://arxiv.org/abs/2401.11748</link><description>&lt;p&gt;
GI-PIP&#65306;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26159;&#21542;&#38656;&#35201;&#19981;&#20999;&#23454;&#38469;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient Inversion Attacks?. (arXiv:2401.11748v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26041;&#27861;GI-PIP&#65292;&#19981;&#38656;&#35201;&#20381;&#36182;&#19981;&#20999;&#23454;&#38469;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20174;&#36739;&#23569;&#30340;&#25968;&#25454;&#20013;&#25429;&#33719;&#24213;&#23618;&#20998;&#24067;&#65292;&#24182;&#33021;&#22312;&#22270;&#20687;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#20063;&#26356;&#24378;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#36890;&#36807;&#20934;&#30830;&#22320;&#24674;&#22797;&#20849;&#20139;&#26799;&#24230;&#20013;&#30340;&#38544;&#31169;&#25968;&#25454;&#65292;&#23545;&#32852;&#37030;&#23398;&#20064;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#22312;&#35775;&#38382;&#36807;&#22810;&#30340;&#36741;&#21161;&#25968;&#25454;&#26041;&#38754;&#20381;&#36182;&#20110;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#36825;&#36829;&#21453;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#22522;&#26412;&#25968;&#25454;&#20998;&#21306;&#21407;&#21017;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#23454;&#29992;&#22270;&#20687;&#20808;&#39564;&#30340;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#65288;GI-PIP&#65289;&#65292;&#22312;&#32463;&#36807;&#20462;&#35746;&#30340;&#23041;&#32961;&#27169;&#22411;&#19979;&#12290;GI-PIP&#21033;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20174;&#26356;&#23569;&#30340;&#25968;&#25454;&#20013;&#25429;&#33719;&#24213;&#23618;&#20998;&#24067;&#65292;&#32780;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#38656;&#35201;&#28040;&#32791;&#26356;&#22810;&#30340;&#25968;&#25454;&#26469;&#21512;&#25104;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#25552;&#21462;&#20986;&#30340;&#20998;&#24067;&#26469;&#35843;&#33410;&#25915;&#20987;&#36807;&#31243;&#20316;&#20026;&#24322;&#24120;&#24471;&#20998;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GI-PIP&#21482;&#20351;&#29992;&#20102;ImageNet&#25968;&#25454;&#30340;3.8%&#21363;&#21487;&#23454;&#29616;16.12 dB&#30340;PSNR&#24674;&#22797;&#65292;&#32780;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#21017;&#38656;&#35201;&#36229;&#36807;70%&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;GI-PIP&#22312;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep gradient inversion attacks expose a serious threat to Federated Learning (FL) by accurately recovering private data from shared gradients. However, the state-of-the-art heavily relies on impractical assumptions to access excessive auxiliary data, which violates the basic data partitioning principle of FL. In this paper, a novel method, Gradient Inversion Attack using Practical Image Prior (GI-PIP), is proposed under a revised threat model. GI-PIP exploits anomaly detection models to capture the underlying distribution from fewer data, while GAN-based methods consume significant more data to synthesize images. The extracted distribution is then leveraged to regulate the attack process as Anomaly Score loss. Experimental results show that GI-PIP achieves a 16.12 dB PSNR recovery using only 3.8% data of ImageNet, while GAN-based methods necessitate over 70%. Moreover, GI-PIP exhibits superior capability on distribution generalization compared to GAN-based methods. Our approach signif
&lt;/p&gt;</description></item><item><title>TIM &#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26102;&#38388;&#20132;&#20114;&#27169;&#22359;&#65292;&#29992;&#20110;&#22686;&#24378;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476; (SNNs) &#30340;&#26102;&#38388;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11687</link><description>&lt;p&gt;
TIM: &#19968;&#31181;&#39640;&#25928;&#30340;&#26102;&#38388;&#20132;&#20114;&#27169;&#22359;&#29992;&#20110;&#33033;&#20914;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
TIM: An Efficient Temporal Interaction Module for Spiking Transformer. (arXiv:2401.11687v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11687
&lt;/p&gt;
&lt;p&gt;
TIM &#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26102;&#38388;&#20132;&#20114;&#27169;&#22359;&#65292;&#29992;&#20110;&#22686;&#24378;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476; (SNNs) &#30340;&#26102;&#38388;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476; (SNNs) &#20316;&#20026;&#31532;&#19977;&#20195;&#31070;&#32463;&#32593;&#32476;&#65292;&#22240;&#20854;&#29983;&#29289;&#21512;&#29702;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#32780;&#22791;&#21463;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#26041;&#38754;&#12290;&#21463;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25972;&#21512;&#23548;&#33268;&#20102;&#33033;&#20914;&#21464;&#21387;&#22120;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#21464;&#21387;&#22120;&#22312;&#22686;&#24378; SNNs &#30340;&#33021;&#21147;&#26041;&#38754;&#26174;&#31034;&#20102;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#38745;&#24577;&#25968;&#25454;&#38598;&#21644;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#38598;&#30340;&#39046;&#22495;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#31995;&#32479;&#20013;&#23384;&#22312;&#30528;&#26126;&#26174;&#30340;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#33033;&#20914;&#33258;&#27880;&#24847; (SSA) &#26426;&#21046;&#22312;&#21033;&#29992; SNNs &#30340;&#26102;&#38388;&#22788;&#29702;&#28508;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026; Temporal Interaction Module (TIM) &#30340;&#26032;&#22411;&#21367;&#31215;&#22686;&#24378;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378; SNN &#26550;&#26500;&#20013;&#30340;&#26102;&#38388;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#12290;TIM &#30340;&#25972;&#21512;&#19982;&#29616;&#26377;&#30340; SNN &#26694;&#26550;&#26080;&#32541;&#34900;&#25509;&#65292;&#39640;&#25928;&#65292;&#21482;&#38656;&#35201;&#39069;&#22806;&#30340;&#26368;&#23569;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs), as the third generation of neural networks, have gained prominence for their biological plausibility and computational efficiency, especially in processing diverse datasets. The integration of attention mechanisms, inspired by advancements in neural network architectures, has led to the development of Spiking Transformers. These have shown promise in enhancing SNNs' capabilities, particularly in the realms of both static and neuromorphic datasets. Despite their progress, a discernible gap exists in these systems, specifically in the Spiking Self Attention (SSA) mechanism's effectiveness in leveraging the temporal processing potential of SNNs. To address this, we introduce the Temporal Interaction Module (TIM), a novel, convolution-based enhancement designed to augment the temporal data processing abilities within SNN architectures. TIM's integration into existing SNN frameworks is seamless and efficient, requiring minimal additional parameters while sign
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;HARDCORE&#26041;&#27861;&#65292;&#20351;&#29992;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#29289;&#29702;&#20449;&#24687;&#25193;&#23637;&#26469;&#39640;&#25928;&#20272;&#35745;&#38081;&#33455;&#20013;&#20219;&#24847;&#27874;&#24418;&#30340;H&#22330;&#21644;&#21151;&#29575;&#25439;&#32791;&#12290;&#20851;&#38190;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#37325;&#24314;bh&#26354;&#32447;&#24182;&#26681;&#25454;&#26354;&#32447;&#30340;&#38754;&#31215;&#20272;&#35745;&#21151;&#29575;&#25439;&#32791;&#65292;&#24182;&#37319;&#29992;&#19987;&#23478;&#29305;&#24449;&#24037;&#31243;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#36755;&#20837;&#26469;&#23454;&#29616;&#31616;&#26126;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.11488</link><description>&lt;p&gt;
HARDCORE&#65306;&#22522;&#20110;&#27531;&#24046;&#12289;&#25193;&#24352;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#38081;&#33455;&#20013;&#36827;&#34892;&#20219;&#24847;&#27874;&#24418;&#30340;H&#22330;&#21644;&#21151;&#29575;&#25439;&#32791;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
HARDCORE: H-field and power loss estimation for arbitrary waveforms with residual, dilated convolutional neural networks in ferrite cores. (arXiv:2401.11488v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;HARDCORE&#26041;&#27861;&#65292;&#20351;&#29992;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#29289;&#29702;&#20449;&#24687;&#25193;&#23637;&#26469;&#39640;&#25928;&#20272;&#35745;&#38081;&#33455;&#20013;&#20219;&#24847;&#27874;&#24418;&#30340;H&#22330;&#21644;&#21151;&#29575;&#25439;&#32791;&#12290;&#20851;&#38190;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#37325;&#24314;bh&#26354;&#32447;&#24182;&#26681;&#25454;&#26354;&#32447;&#30340;&#38754;&#31215;&#20272;&#35745;&#21151;&#29575;&#25439;&#32791;&#65292;&#24182;&#37319;&#29992;&#19987;&#23478;&#29305;&#24449;&#24037;&#31243;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#36755;&#20837;&#26469;&#23454;&#29616;&#31616;&#26126;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MagNet Challenge 2023&#21628;&#21505;&#21442;&#36187;&#32773;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#29615;&#24418;&#38081;&#33455;&#20013;&#30340;&#26448;&#26009;&#29305;&#23450;&#12289;&#27874;&#24418;&#26080;&#20851;&#30340;&#31283;&#24577;&#21151;&#29575;&#25439;&#32791;&#36827;&#34892;&#20272;&#35745;&#12290;&#20197;&#19979;&#20171;&#32461;&#30340;HARDCORE&#26041;&#27861;&#34920;&#26126;&#65292;&#39044;&#20808;&#22312;&#35266;&#27979;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20855;&#26377;&#29289;&#29702;&#20449;&#24687;&#25193;&#23637;&#30340;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#39640;&#25928;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#35299;&#20915;&#26041;&#26696;&#20803;&#32032;&#26159;&#19968;&#20010;&#20013;&#38388;&#27169;&#22411;&#23618;&#65292;&#39318;&#20808;&#37325;&#24314;bh&#26354;&#32447;&#65292;&#28982;&#21518;&#26681;&#25454;&#26354;&#32447;&#30340;&#38754;&#31215;&#20272;&#35745;&#21151;&#29575;&#25439;&#32791;&#65292;&#20174;&#32780;&#20351;&#25152;&#25552;&#20986;&#30340;&#25299;&#25169;&#32467;&#26500;&#22312;&#29289;&#29702;&#19978;&#21487;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#30528;&#37325;&#20110;&#22522;&#20110;&#19987;&#23478;&#30340;&#29305;&#24449;&#24037;&#31243;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#36755;&#20837;&#65292;&#20197;&#23454;&#29616;&#31616;&#26126;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#20026;&#27599;&#31181;&#26448;&#26009;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65292;&#32780;&#25299;&#25169;&#32467;&#26500;&#20445;&#25345;&#19981;&#21464;&#12290;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#20102;&#24085;&#32047;&#25176;&#24335;&#30340;&#24615;&#33021;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The MagNet Challenge 2023 calls upon competitors to develop data-driven models for the material-specific, waveform-agnostic estimation of steady-state power losses in toroidal ferrite cores. The following HARDCORE (H-field and power loss estimation for Arbitrary waveforms with Residual, Dilated convolutional neural networks in ferrite COREs) approach shows that a residual convolutional neural network with physics-informed extensions can serve this task efficiently when trained on observational data beforehand. One key solution element is an intermediate model layer which first reconstructs the bh curve and then estimates the power losses based on the curve's area rendering the proposed topology physically interpretable. In addition, emphasis was placed on expert-based feature engineering and information-rich inputs in order to enable a lean model architecture. A model is trained from scratch for each material, while the topology remains the same. A Pareto-style trade-off between model 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#38750;&#20381;&#20174;&#39118;&#38505;&#21644;&#30456;&#20851;&#30340;&#31995;&#32479;&#30151;&#29366;&#35780;&#20998;&#65292;&#20026;&#38271;&#26399;&#36807;&#25935;&#24615;&#40763;&#28814;&#20122;&#21345;&#28608;&#32032;&#30382;&#19979;&#20813;&#30123;&#27835;&#30103;&#30340;&#31649;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.11447</link><description>&lt;p&gt;
&#39044;&#27979;&#36807;&#25935;&#24615;&#40763;&#28814;&#20122;&#21345;&#28608;&#32032;&#30382;&#19979;&#20813;&#30123;&#27835;&#30103;&#20013;&#24739;&#32773;&#20381;&#20174;&#24615;&#30340;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sequential Model for Predicting Patient Adherence in Subcutaneous Immunotherapy for Allergic Rhinitis. (arXiv:2401.11447v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#38750;&#20381;&#20174;&#39118;&#38505;&#21644;&#30456;&#20851;&#30340;&#31995;&#32479;&#30151;&#29366;&#35780;&#20998;&#65292;&#20026;&#38271;&#26399;&#36807;&#25935;&#24615;&#40763;&#28814;&#20122;&#21345;&#28608;&#32032;&#30382;&#19979;&#20813;&#30123;&#27835;&#30103;&#30340;&#31649;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#30382;&#19979;&#20813;&#30123;&#27835;&#30103;(SCIT)&#26159;&#36807;&#25935;&#24615;&#40763;&#28814;&#30340;&#38271;&#25928;&#22240;&#26524;&#27835;&#30103;&#12290;&#22914;&#20309;&#25552;&#39640;&#24739;&#32773;&#23545;&#21464;&#24212;&#21407;&#20813;&#30123;&#27835;&#30103;(AIT)&#30340;&#20381;&#20174;&#24615;&#20197;&#26368;&#22823;&#21270;&#27835;&#30103;&#25928;&#26524;&#65292;&#22312;AIT&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#38750;&#20381;&#20174;&#39118;&#38505;&#21644;&#30456;&#20851;&#30340;&#31995;&#32479;&#30151;&#29366;&#35780;&#20998;&#65292;&#20026;&#38271;&#26399;AIT&#30340;&#31649;&#29702;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#26412;&#30740;&#31350;&#24320;&#21457;&#21644;&#20998;&#26512;&#20102;&#20004;&#31181;&#27169;&#22411;&#65292;&#24207;&#21015;&#28508;&#22312;&#34892;&#20026;&#32773;-&#35780;&#35770;&#23478;&#27169;&#22411;(SLAC)&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#27169;&#22411;(LSTM)&#65292;&#24182;&#22522;&#20110;&#35780;&#20998;&#21644;&#20381;&#20174;&#24615;&#39044;&#27979;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#65306;&#22312;&#25490;&#38500;&#31532;&#19968;&#26102;&#38388;&#27493;&#30340;&#20559;&#20506;&#26679;&#26412;&#21518;&#65292;SLAC&#27169;&#22411;&#30340;&#39044;&#27979;&#20381;&#20174;&#20934;&#30830;&#29575;&#20026;60%-72%&#65292;&#32780;LSTM&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20026;66%-84%&#65292;&#26681;&#25454;&#26102;&#38388;&#27493;&#38271;&#30340;&#19981;&#21516;&#32780;&#21464;&#21270;&#12290;SLAC&#27169;&#22411;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;(RMSE)&#33539;&#22260;&#22312;0.93&#21040;2.22&#20043;&#38388;&#65292;&#32780;LSTM&#27169;&#22411;&#30340;RMSE&#33539;&#22260;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Objective: Subcutaneous Immunotherapy (SCIT) is the long-lasting causal treatment of allergic rhinitis. How to enhance the adherence of patients to maximize the benefit of allergen immunotherapy (AIT) plays a crucial role in the management of AIT. This study aims to leverage novel machine learning models to precisely predict the risk of non-adherence of patients and related systematic symptom scores, to provide a novel approach in the management of long-term AIT.  Methods: The research develops and analyzes two models, Sequential Latent Actor-Critic (SLAC) and Long Short-Term Memory (LSTM), evaluating them based on scoring and adherence prediction capabilities.  Results: Excluding the biased samples at the first time step, the predictive adherence accuracy of the SLAC models is from $60\,\%$ to $72\%$, and for LSTM models, it is $66\,\%$ to $84\,\%$, varying according to the time steps. The range of Root Mean Square Error (RMSE) for SLAC models is between $0.93$ and $2.22$, while for L
&lt;/p&gt;</description></item><item><title>PartIR&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#21306;&#31995;&#32479;&#65292;&#20855;&#22791;&#34920;&#36798;&#21147;&#24378;&#21644;&#21487;&#39044;&#27979;&#24615;&#24378;&#30340;&#29305;&#28857;&#12290;&#23427;&#36890;&#36807;&#39640;&#32423;&#31243;&#24207;&#21592;&#21457;&#20986;&#30340;&#20998;&#21306;&#31574;&#30053;&#39537;&#21160;&#65292;&#24182;&#37319;&#29992;&#22686;&#37327;&#37325;&#20889;&#26041;&#27861;&#65292;&#33021;&#22815;&#32452;&#21512;&#19981;&#21516;&#30340;&#20998;&#29255;&#31574;&#30053;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#39044;&#27979;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#36798;&#21040;&#23792;&#20540;&#24615;&#33021;&#33021;&#21147;&#24378;&#12290;</title><link>http://arxiv.org/abs/2401.11202</link><description>&lt;p&gt;
PartIR: &#20026;&#26426;&#22120;&#23398;&#20064;&#32452;&#21512;SPMD&#20998;&#21306;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
PartIR: Composing SPMD Partitioning Strategies for Machine Learning. (arXiv:2401.11202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11202
&lt;/p&gt;
&lt;p&gt;
PartIR&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#21306;&#31995;&#32479;&#65292;&#20855;&#22791;&#34920;&#36798;&#21147;&#24378;&#21644;&#21487;&#39044;&#27979;&#24615;&#24378;&#30340;&#29305;&#28857;&#12290;&#23427;&#36890;&#36807;&#39640;&#32423;&#31243;&#24207;&#21592;&#21457;&#20986;&#30340;&#20998;&#21306;&#31574;&#30053;&#39537;&#21160;&#65292;&#24182;&#37319;&#29992;&#22686;&#37327;&#37325;&#20889;&#26041;&#27861;&#65292;&#33021;&#22815;&#32452;&#21512;&#19981;&#21516;&#30340;&#20998;&#29255;&#31574;&#30053;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#39044;&#27979;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#36798;&#21040;&#23792;&#20540;&#24615;&#33021;&#33021;&#21147;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#35757;&#32451;&#38656;&#35201;&#32467;&#21512;&#25968;&#25454;&#12289;&#27169;&#22411;&#25110;&#20248;&#21270;&#22120;&#20998;&#29255;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;&#24403;&#31574;&#30053;&#21464;&#24471;&#22797;&#26434;&#26102;&#65292;&#20998;&#21306;&#24037;&#20855;&#38656;&#35201;&#20855;&#22791;&#20197;&#19979;&#29305;&#28857;&#65306;1&#65289;&#34920;&#36798;&#21147;&#24378;&#65292;&#20801;&#35768;&#32452;&#21512;&#31616;&#21333;&#31574;&#30053;&#65307;2&#65289;&#21487;&#39044;&#27979;&#24615;&#24378;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#20272;&#31639;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PartIR&#65292;&#19968;&#31181;&#29992;&#20110;NN&#20998;&#21306;&#30340;&#35774;&#35745;&#12290;PartIR&#37319;&#29992;&#22686;&#37327;&#37325;&#20889;&#26041;&#27861;&#65292;&#19982;&#30828;&#20214;&#21644;&#36816;&#34892;&#26102;&#26080;&#20851;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;API&#29992;&#20110;&#32452;&#21512;&#20998;&#29255;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#22120;&#36827;&#34892;&#39564;&#35777;&#12290;&#25972;&#20010;&#36807;&#31243;&#30001;&#39640;&#32423;&#31243;&#24207;&#21592;&#21457;&#20986;&#30340;&#20998;&#21306;&#31574;&#30053;&#39537;&#21160;&#65292;&#26082;&#21487;&#20197;&#25163;&#21160;&#20063;&#21487;&#20197;&#33258;&#21160;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#31574;&#30053;&#19982;&#27169;&#22411;&#20195;&#30721;&#20998;&#24320;&#25351;&#23450;&#65292;&#26131;&#20110;&#26356;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#31181;&#19981;&#21516;&#27169;&#22411;&#30340;&#35780;&#20272;&#26469;&#23637;&#31034;PartIR&#30340;&#21487;&#39044;&#27979;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#36798;&#21040;&#23792;&#20540;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training of modern large neural networks (NN) requires a combination of parallelization strategies encompassing data, model, or optimizer sharding. When strategies increase in complexity, it becomes necessary for partitioning tools to be 1) expressive, allowing the composition of simpler strategies, and 2) predictable to estimate performance analytically. We present PartIR, our design for a NN partitioning system. PartIR is focused on an incremental approach to rewriting and is hardware-and-runtime agnostic. We present a simple but powerful API for composing sharding strategies and a simulator to validate them. The process is driven by high-level programmer-issued partitioning tactics, which can be both manual and automatic. Importantly, the tactics are specified separately from the model code, making them easy to change. We evaluate PartIR on several different models to demonstrate its predictability, expressibility, and ability to reach peak performance..
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23545;&#20132;&#36890;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#21457;&#29616;&#26102;&#38388;&#24207;&#21015;&#39034;&#24207;&#21644;&#25513;&#30721;&#30340;&#22686;&#24378;&#22312;&#20132;&#36890;&#20998;&#31867;&#20013;&#26356;&#36866;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#28508;&#22312;&#31354;&#38388;&#20998;&#26512;&#21487;&#20197;&#35299;&#37322;&#22686;&#24378;&#25928;&#26524;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.10754</link><description>&lt;p&gt;
&#20132;&#36890;&#20998;&#31867;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Traffic Classification. (arXiv:2401.10754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10754
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23545;&#20132;&#36890;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#21457;&#29616;&#26102;&#38388;&#24207;&#21015;&#39034;&#24207;&#21644;&#25513;&#30721;&#30340;&#22686;&#24378;&#22312;&#20132;&#36890;&#20998;&#31867;&#20013;&#26356;&#36866;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#28508;&#22312;&#31354;&#38388;&#20998;&#26512;&#21487;&#20197;&#35299;&#37322;&#22686;&#24378;&#25928;&#26524;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#26679;&#26412;&#26469;&#20016;&#23500;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#20132;&#36890;&#20998;&#31867;&#65288;TC&#65289;&#20219;&#21153;&#20013;&#65292;DA&#24456;&#38590;&#33719;&#24471;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;18&#31181;&#22686;&#24378;&#20989;&#25968;&#24212;&#29992;&#20110;3&#20010;TC&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#21253;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#34920;&#31034;&#65292;&#24182;&#32771;&#34385;&#21508;&#31181;&#35757;&#32451;&#26465;&#20214;&#65292;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#65288;i&#65289;DA&#21487;&#20197;&#33719;&#24471;&#20197;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#22909;&#22788;&#65292;&#65288;ii&#65289;&#20316;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#39034;&#24207;&#21644;&#25513;&#30721;&#30340;&#22686;&#24378;&#22312;TC&#20013;&#26356;&#21512;&#36866;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#31616;&#21333;&#30340;&#28508;&#22312;&#31354;&#38388;&#20998;&#26512;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#20026;&#20160;&#20040;&#22686;&#24378;&#20250;&#20135;&#29983;&#31215;&#26497;&#25110;&#28040;&#26497;&#24433;&#21709;&#30340;&#19968;&#20123;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation (DA) -- enriching training data by adding synthetic samples -- is a technique widely adopted in Computer Vision (CV) and Natural Language Processing (NLP) tasks to improve models performance. Yet, DA has struggled to gain traction in networking contexts, particularly in Traffic Classification (TC) tasks. In this work, we fulfill this gap by benchmarking 18 augmentation functions applied to 3 TC datasets using packet time series as input representation and considering a variety of training conditions. Our results show that (i) DA can reap benefits previously unexplored with (ii) augmentations acting on time series sequence order and masking being a better suit for TC and (iii) simple latent space analysis can provide hints about why augmentations have positive or negative effects.
&lt;/p&gt;</description></item><item><title>ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.10225</link><description>&lt;p&gt;
ChatQA: &#26500;&#24314;GPT-4&#32423;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10225
&lt;/p&gt;
&lt;p&gt;
ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatQA&#65292;&#19968;&#31995;&#21015;&#20855;&#26377;GPT-4&#32423;&#21035;&#20934;&#30830;&#24615;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22788;&#29702;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22810;&#36718;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#24494;&#35843;&#65292;&#36825;&#26679;&#21487;&#20197;&#25552;&#20379;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ChatQA-70B&#21487;&#20197;&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#20998;&#19978;&#36229;&#36807;GPT-4&#65288;54.14 vs. 53.90&#65289;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;OpenAI GPT&#27169;&#22411;&#30340;&#20219;&#20309;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.10134</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#29305;&#23450;&#20301;&#32622;&#30340;&#26410;&#26469;&#20132;&#36890;&#24773;&#20917;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#24378;&#35843;&#24320;&#21457;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20294;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#24182;&#26410;&#30456;&#24212;&#25552;&#39640;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;LLMs&#20027;&#35201;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#26469;&#36827;&#27493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22522;&#26412;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ST-LLM&#23558;&#27599;&#20010;&#20301;&#32622;&#30340;&#26102;&#38388;&#27493;&#38271;&#23450;&#20041;&#20026;&#26631;&#35760;&#65292;&#24182;&#32467;&#21512;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#26469;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#34920;&#31034;&#34987;&#34701;&#21512;&#20197;&#20026;&#27599;&#20010;&#26631;&#35760;&#25552;&#20379;&#32479;&#19968;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24130;&#28388;&#27874;&#22120;&#31070;&#32463;&#32593;&#32476; (GPFN)&#65292;&#36890;&#36807;&#20351;&#29992;&#24130;&#32423;&#25968;&#22270;&#28388;&#27874;&#22120;&#26469;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#12290;GPFN&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#25947;&#24130;&#32423;&#25968;&#30340;&#20855;&#26377;&#26080;&#38480;&#25509;&#25910;&#22495;&#30340;&#22270;&#28388;&#27874;&#22120;&#26500;&#24314;&#26041;&#27861;&#65292;&#24182;&#33021;&#38598;&#25104;&#20219;&#20309;&#24130;&#32423;&#25968;&#24182;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.09943</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#22495;&#22270;&#28388;&#27874;&#22120;&#65306;&#21033;&#29992;&#24130;&#32423;&#25968;&#22686;&#24378;&#31232;&#30095;&#20449;&#24687;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Infinite-Horizon Graph Filters: Leveraging Power Series to Enhance Sparse Information Aggregation. (arXiv:2401.09943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09943
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24130;&#28388;&#27874;&#22120;&#31070;&#32463;&#32593;&#32476; (GPFN)&#65292;&#36890;&#36807;&#20351;&#29992;&#24130;&#32423;&#25968;&#22270;&#28388;&#27874;&#22120;&#26469;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#12290;GPFN&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#25947;&#24130;&#32423;&#25968;&#30340;&#20855;&#26377;&#26080;&#38480;&#25509;&#25910;&#22495;&#30340;&#22270;&#28388;&#27874;&#22120;&#26500;&#24314;&#26041;&#27861;&#65292;&#24182;&#33021;&#38598;&#25104;&#20219;&#20309;&#24130;&#32423;&#25968;&#24182;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24120;&#24120;&#21463;&#21040;&#26377;&#38480;&#25509;&#25910;&#22495;&#30340;&#38480;&#21046;&#65292;&#22312;&#31232;&#30095;&#22270;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#20005;&#23803;&#12290;&#37492;&#20110;&#20855;&#26377;&#26080;&#38480;&#25193;&#23637;&#33021;&#21147;&#30340;&#24130;&#32423;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24130;&#28388;&#27874;&#22120;&#31070;&#32463;&#32593;&#32476; (GPFN)&#65292;&#36890;&#36807;&#20351;&#29992;&#24130;&#32423;&#25968;&#22270;&#28388;&#27874;&#22120;&#26469;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;GPFN&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#25947;&#24130;&#32423;&#25968;&#30340;&#20855;&#26377;&#26080;&#38480;&#25509;&#25910;&#22495;&#30340;&#22270;&#28388;&#27874;&#22120;&#26500;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#39057;&#35889;&#21644;&#31354;&#38388;&#22495;&#20013;&#36827;&#34892;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;GPFN&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#38598;&#25104;&#20219;&#20309;&#24130;&#32423;&#25968;&#24182;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;GPFN&#22312;&#31232;&#30095;&#22270;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have shown considerable effectiveness in a variety of graph learning tasks, particularly those based on the message-passing approach in recent years. However, their performance is often constrained by a limited receptive field, a challenge that becomes more acute in the presence of sparse graphs. In light of the power series, which possesses infinite expansion capabilities, we propose a novel \underline{G}raph \underline{P}ower \underline{F}ilter \underline{N}eural Network (GPFN) that enhances node classification by employing a power series graph filter to augment the receptive field. Concretely, our GPFN designs a new way to build a graph filter with an infinite receptive field based on the convergence power series, which can be analyzed in the spectral and spatial domains. Besides, we theoretically prove that our GPFN is a general framework that can integrate any power series and capture long-range dependencies. Finally, experimental results on three data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25193;&#20805;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#26089;&#34701;&#21512;&#21644;&#26202;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25351;&#26631;&#65292;&#23454;&#29616;&#39118;&#38505;&#24863;&#30693;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2401.09479</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep Learning. (arXiv:2401.09479v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25193;&#20805;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#26089;&#34701;&#21512;&#21644;&#26202;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25351;&#26631;&#65292;&#23454;&#29616;&#39118;&#38505;&#24863;&#30693;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#20449;&#20219;&#30340;&#26080;&#21378;&#26080;&#21360;&#36896;&#21046;&#36896;&#26102;&#20195;&#65292;&#30828;&#20214;&#29305;&#27931;&#20234;&#22312;&#33455;&#29255;&#29983;&#20135;&#30340;&#21508;&#20010;&#38454;&#27573;&#34987;&#25554;&#20837;&#30340;&#39118;&#38505;&#22686;&#21152;&#20102;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#26816;&#27979;&#30828;&#20214;&#29305;&#27931;&#20234;&#12290;&#23613;&#31649;&#22823;&#37096;&#20998;&#20851;&#27880;&#28857;&#37117;&#38598;&#20013;&#22312;&#32479;&#35745;&#23398;&#25110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19978;&#65292;&#20294;&#21463;&#21040;&#29305;&#27931;&#20234;&#24863;&#26579;&#22522;&#20934;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#30340;&#24433;&#21709;&#65292;&#26816;&#27979;&#20934;&#30830;&#24615;&#21463;&#38480;&#65292;&#26080;&#27861;&#26816;&#27979;&#21040;&#38646;&#26085;&#29305;&#27931;&#20234;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#25193;&#20805;&#25968;&#25454;&#65292;&#20197;&#20004;&#31181;&#26367;&#20195;&#34920;&#31034;&#27169;&#24577;&#65292;&#22270;&#24418;&#21644;&#34920;&#26684;&#65292;&#30830;&#20445;&#25968;&#25454;&#38598;&#20197;&#20195;&#34920;&#24615;&#30340;&#26041;&#24335;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#27979;&#30828;&#20214;&#29305;&#27931;&#20234;&#65292;&#24182;&#35780;&#20272;&#20102;&#26089;&#34701;&#21512;&#21644;&#26202;&#34701;&#21512;&#31574;&#30053;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#20272;&#35745;&#20102;&#27599;&#20010;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25351;&#26631;&#65292;&#29992;&#20110;&#39118;&#38505;&#24863;&#30693;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#32467;&#26524;&#19981;&#20165;&#30830;&#35748;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#19988;&#34920;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The risk of hardware Trojans being inserted at various stages of chip production has increased in a zero-trust fabless era. To counter this, various machine learning solutions have been developed for the detection of hardware Trojans. While most of the focus has been on either a statistical or deep learning approach, the limited number of Trojan-infected benchmarks affects the detection accuracy and restricts the possibility of detecting zero-day Trojans. To close the gap, we first employ generative adversarial networks to amplify our data in two alternative representation modalities, a graph and a tabular, ensuring that the dataset is distributed in a representative manner. Further, we propose a multimodal deep learning approach to detect hardware Trojans and evaluate the results from both early fusion and late fusion strategies. We also estimate the uncertainty quantification metrics of each prediction for risk-aware decision-making. The outcomes not only confirms the efficacy of our
&lt;/p&gt;</description></item><item><title>&#37096;&#20998;&#38899;&#26631;&#21270;&#26159;&#36873;&#25321;&#26631;&#35760;&#37096;&#20998;&#23383;&#31526;&#26469;&#25552;&#39640;&#38405;&#35835;&#21487;&#35835;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#19978;&#19979;&#25991;&#23545;&#27604;&#30340;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;CCPD&#65289;&#38598;&#25104;&#20102;&#29616;&#26377;&#30340;&#38463;&#25289;&#20271;&#38899;&#26631;&#21270;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#34913;&#37327;&#37096;&#20998;&#38899;&#26631;&#21270;&#30340;&#26032;&#25351;&#26631;&#26469;&#21028;&#26029;&#38656;&#35201;&#26631;&#35760;&#21738;&#20123;&#23383;&#31526;&#12290;</title><link>http://arxiv.org/abs/2401.08919</link><description>&lt;p&gt;
&#37096;&#20998;&#38899;&#26631;&#21270;&#65306;&#19968;&#31181;&#19978;&#19979;&#25991;&#23545;&#27604;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Partial Diacritization: A Context-Contrastive Inference Approach. (arXiv:2401.08919v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08919
&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#38899;&#26631;&#21270;&#26159;&#36873;&#25321;&#26631;&#35760;&#37096;&#20998;&#23383;&#31526;&#26469;&#25552;&#39640;&#38405;&#35835;&#21487;&#35835;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#19978;&#19979;&#25991;&#23545;&#27604;&#30340;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;CCPD&#65289;&#38598;&#25104;&#20102;&#29616;&#26377;&#30340;&#38463;&#25289;&#20271;&#38899;&#26631;&#21270;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#34913;&#37327;&#37096;&#20998;&#38899;&#26631;&#21270;&#30340;&#26032;&#25351;&#26631;&#26469;&#21028;&#26029;&#38656;&#35201;&#26631;&#35760;&#21738;&#20123;&#23383;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#26631;&#21270;&#22312;&#25552;&#39640;&#38463;&#25289;&#20271;&#25991;&#26412;&#21487;&#35835;&#24615;&#21644;&#28040;&#38500;&#27495;&#20041;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30446;&#21069;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#26631;&#35760;&#27599;&#20010;&#31526;&#21512;&#26465;&#20214;&#30340;&#23383;&#31526;&#65288;&#20840;&#38899;&#26631;&#21270;&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;PD&#65289;&#26159;&#36873;&#25321;&#26631;&#35760;&#23376;&#38598;&#20197;&#22312;&#24517;&#35201;&#26102;&#25552;&#20379;&#24110;&#21161;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36807;&#22810;&#30340;&#38899;&#26631;&#31526;&#21495;&#20250;&#22952;&#30861;&#29087;&#32451;&#35835;&#32773;&#65292;&#38477;&#20302;&#38405;&#35835;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#34892;&#20026;&#23454;&#39564;&#65292;&#24182;&#26174;&#31034;&#20986;&#37096;&#20998;&#26631;&#35760;&#30340;&#25991;&#26412;&#36890;&#24120;&#27604;&#23436;&#20840;&#26631;&#35760;&#30340;&#25991;&#26412;&#26356;&#23481;&#26131;&#38405;&#35835;&#65292;&#26377;&#26102;&#29978;&#33267;&#27604;&#32431;&#25991;&#26412;&#26356;&#23481;&#26131;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19978;&#19979;&#25991;&#23545;&#27604;&#30340;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;CCPD&#65289;-&#19968;&#31181;&#19982;&#29616;&#26377;&#38463;&#25289;&#20271;&#38899;&#26631;&#21270;&#31995;&#32479;&#26080;&#32541;&#38598;&#25104;&#30340;&#26032;&#26041;&#27861;&#12290;CCPD&#23545;&#27599;&#20010;&#21333;&#35789;&#36827;&#34892;&#20004;&#27425;&#22788;&#29702;&#65292;&#19968;&#27425;&#26377;&#19978;&#19979;&#25991;&#65292;&#19968;&#27425;&#27809;&#26377;&#65292;&#24182;&#19988;&#21482;&#23545;&#20004;&#27425;&#25512;&#29702;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#30340;&#23383;&#31526;&#36827;&#34892;&#38899;&#26631;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#34913;&#37327;&#37096;&#20998;&#38899;&#26631;&#21270;&#30340;&#26032;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diacritization plays a pivotal role in improving readability and disambiguating the meaning of Arabic texts. Efforts have so far focused on marking every eligible character (Full Diacritization). Comparatively overlooked, Partial Diacritzation (PD) is the selection of a subset of characters to be marked to aid comprehension where needed. Research has indicated that excessive diacritic marks can hinder skilled readers--reducing reading speed and accuracy. We conduct a behavioral experiment and show that partially marked text is often easier to read than fully marked text, and sometimes easier than plain text. In this light, we introduce Context-Contrastive Partial Diacritization (CCPD)--a novel approach to PD which integrates seamlessly with existing Arabic diacritization systems. CCPD processes each word twice, once with context and once without, and diacritizes only the characters with disparities between the two inferences. Further, we introduce novel indicators for measuring partial
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35889;&#25193;&#25955;&#26694;&#26550;SpecSTG&#65292;&#29992;&#20110;&#27010;&#29575;&#26102;&#31354;&#20132;&#36890;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#35889;&#22495;&#20013;&#29983;&#25104;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#30340;&#20613;&#37324;&#21494;&#34920;&#31034;&#65292;&#21033;&#29992;&#31354;&#38388;&#20449;&#24687;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#31995;&#32479;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.08119</link><description>&lt;p&gt;
SpecSTG:&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;&#26102;&#31354;&#20132;&#36890;&#39044;&#27979;&#30340;&#24555;&#36895;&#35889;&#25193;&#25955;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SpecSTG: A Fast Spectral Diffusion Framework for Probabilistic Spatio-Temporal Traffic Forecasting. (arXiv:2401.08119v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08119
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35889;&#25193;&#25955;&#26694;&#26550;SpecSTG&#65292;&#29992;&#20110;&#27010;&#29575;&#26102;&#31354;&#20132;&#36890;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#35889;&#22495;&#20013;&#29983;&#25104;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#30340;&#20613;&#37324;&#21494;&#34920;&#31034;&#65292;&#21033;&#29992;&#31354;&#38388;&#20449;&#24687;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#31995;&#32479;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#65292;&#20256;&#32479;&#19978;&#20381;&#36182;&#30830;&#23450;&#24615;&#27169;&#22411;&#36827;&#34892;&#20934;&#30830;&#30340;&#28857;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#35782;&#21035;&#26410;&#26469;&#35266;&#27979;&#20013;&#24847;&#22806;&#27874;&#21160;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27010;&#29575;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#30340;&#21464;&#31181;&#65292;&#24050;&#25104;&#20026;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#26041;&#27861;&#36890;&#24120;&#21482;&#20851;&#27880;&#20026;&#20132;&#36890;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#20256;&#24863;&#22120;&#29983;&#25104;&#21333;&#29420;&#30340;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#65292;&#23548;&#33268;&#31354;&#38388;&#32593;&#32476;&#29305;&#24449;&#22312;&#27010;&#29575;&#23398;&#20064;&#36807;&#31243;&#20013;&#21442;&#19982;&#19981;&#36275;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#20132;&#36890;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#31995;&#32479;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35889;&#25193;&#25955;&#26694;&#26550;&#8212;&#8212;SpecSTG&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#30340;&#20613;&#37324;&#21494;&#34920;&#31034;&#65292;&#23558;&#23398;&#20064;&#36807;&#31243;&#36716;&#21270;&#20026;&#20805;&#28385;&#31354;&#38388;&#20449;&#24687;&#30340;&#35889;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#32467;&#21512;&#20102;&#31354;&#38388;&#32593;&#32476;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting, a crucial application of spatio-temporal graph (STG) learning, has traditionally relied on deterministic models for accurate point estimations. Yet, these models fall short of identifying latent risks of unexpected volatility in future observations. To address this gap, probabilistic methods, especially variants of diffusion models, have emerged as uncertainty-aware solutions. However, existing diffusion methods typically focus on generating separate future time series for individual sensors in the traffic network, resulting in insufficient involvement of spatial network characteristics in the probabilistic learning process. To better leverage spatial dependencies and systematic patterns inherent in traffic data, we propose SpecSTG, a novel spectral diffusion framework. Our method generates the Fourier representation of future time series, transforming the learning process into the spectral domain enriched with spatial information. Additionally, our approach incorp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65292;&#36890;&#36807;&#32852;&#21512;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#35757;&#32451;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06980</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#36827;&#34892;&#32852;&#21512;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#35757;&#32451;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Joint Unsupervised and Supervised Training for Automatic Speech Recognition via Bilevel Optimization. (arXiv:2401.06980v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65292;&#36890;&#36807;&#32852;&#21512;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#35757;&#32451;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20219;&#21153;&#20013;&#30340;&#22768;&#23398;&#27169;&#22411;&#35757;&#32451;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#21452;&#23618;&#32852;&#21512;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#35757;&#32451;&#65288;BL-JUST&#65289;&#8221;&#12290;BL-JUST&#37319;&#29992;&#19979;&#23618;&#21644;&#19978;&#23618;&#20248;&#21270;&#65292;&#20998;&#21035;&#20351;&#29992;&#26080;&#30417;&#30563;&#25439;&#22833;&#21644;&#30417;&#30563;&#25439;&#22833;&#65292;&#21033;&#29992;&#26368;&#36817;&#22312;&#24809;&#32602;&#22411;&#21452;&#23618;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#26469;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#21487;&#25215;&#21463;&#22797;&#26434;&#24230;&#21644;&#20005;&#26684;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#25361;&#25112;&#24615;ASR&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel bilevel optimization-based training approach to training acoustic models for automatic speech recognition (ASR) tasks that we term {bi-level joint unsupervised and supervised training (BL-JUST)}. {BL-JUST employs a lower and upper level optimization with an unsupervised loss and a supervised loss respectively, leveraging recent advances in penalty-based bilevel optimization to solve this challenging ASR problem with affordable complexity and rigorous convergence guarantees.} To evaluate BL-JUST, extensive experiments on the LibriSpeech and TED-LIUM v2 datasets have been conducted. BL-JUST achieves superior performance over the commonly used pre-training followed by fine-tuning strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#30149;&#29702;&#23398;&#23478;&#20026;&#30149;&#29702;&#23398;&#23478;&#26500;&#24314;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#25968;&#25454;&#25972;&#29702;&#21644;&#34701;&#20837;&#30149;&#29702;&#23398;&#39046;&#22495;&#30693;&#35782;&#65292;&#25193;&#23637;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20840;&#29627;&#29255;&#22270;&#20687;&#22522;&#30784;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#32597;&#35265;&#30142;&#30149;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.04079</link><description>&lt;p&gt;
RudolfV&#65306;&#19968;&#31181;&#30001;&#30149;&#29702;&#23398;&#23478;&#20026;&#30149;&#29702;&#23398;&#23478;&#26500;&#24314;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RudolfV: A Foundation Model by Pathologists for Pathologists. (arXiv:2401.04079v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#30149;&#29702;&#23398;&#23478;&#20026;&#30149;&#29702;&#23398;&#23478;&#26500;&#24314;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#25968;&#25454;&#25972;&#29702;&#21644;&#34701;&#20837;&#30149;&#29702;&#23398;&#39046;&#22495;&#30693;&#35782;&#65292;&#25193;&#23637;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20840;&#29627;&#29255;&#22270;&#20687;&#22522;&#30784;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#32597;&#35265;&#30142;&#30149;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#22312;&#20020;&#24202;&#21307;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#34429;&#28982;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#30149;&#29702;&#23398;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#27867;&#21270;&#21644;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#32597;&#35265;&#30142;&#30149;&#26041;&#38754;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#23398;&#20064;&#26469;&#33258;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#20043;&#21069;&#65292;&#20174;&#26080;&#26631;&#35760;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21322;&#33258;&#21160;&#25968;&#25454;&#25972;&#29702;&#21644;&#34701;&#20837;&#30149;&#29702;&#23398;&#39046;&#22495;&#30693;&#35782;&#65292;&#25193;&#23637;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20840;&#29627;&#29255;&#22270;&#20687;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32467;&#21512;&#35745;&#31639;&#21644;&#30149;&#29702;&#23398;&#39046;&#22495;&#30693;&#35782;(1)&#25972;&#29702;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;10.3&#19975;&#20010;&#29627;&#29255;&#22270;&#20687;&#23545;&#24212;&#30340;7.5&#20159;&#20010;&#22270;&#20687;&#22359;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27431;&#32654;&#19981;&#21516;&#20462;&#22797;&#12289;&#26579;&#33394;&#21644;&#25195;&#25551;&#21327;&#35758;&#20197;&#21450;&#19981;&#21516;&#25351;&#31034;&#21644;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#65292;(2)&#29992;&#20110;&#23545;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#29627;&#29255;&#21644;&#32452;&#32455;&#22359;&#36827;&#34892;&#20998;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Histopathology plays a central role in clinical medicine and biomedical research. While artificial intelligence shows promising results on many pathological tasks, generalization and dealing with rare diseases, where training data is scarce, remains a challenge. Distilling knowledge from unlabeled data into a foundation model before learning from, potentially limited, labeled data provides a viable path to address these challenges. In this work, we extend the state of the art of foundation models for digital pathology whole slide images by semi-automated data curation and incorporating pathologist domain knowledge. Specifically, we combine computational and pathologist domain knowledge (1) to curate a diverse dataset of 103k slides corresponding to 750 million image patches covering data from different fixation, staining, and scanning protocols as well as data from different indications and labs across the EU and US, (2) for grouping semantically similar slides and tissue patches, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;S$^{2}$-DMs&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;$L_{skip}$&#37325;&#26032;&#25972;&#21512;&#36873;&#25321;&#24615;&#37319;&#26679;&#38454;&#27573;&#20013;&#30465;&#30053;&#30340;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#19988;&#23454;&#29616;&#31616;&#21333;&#65292;&#23545;&#20195;&#30721;&#20462;&#25913;&#35201;&#27714;&#23569;&#65292;&#19982;&#21508;&#31181;&#37319;&#26679;&#31639;&#27861;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2401.01520</link><description>&lt;p&gt;
S$^{2}$-DMs&#65306;&#36339;&#36807;&#27493;&#39588;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
S$^{2}$-DMs:Skip-Step Diffusion Models. (arXiv:2401.01520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;S$^{2}$-DMs&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;$L_{skip}$&#37325;&#26032;&#25972;&#21512;&#36873;&#25321;&#24615;&#37319;&#26679;&#38454;&#27573;&#20013;&#30465;&#30053;&#30340;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#19988;&#23454;&#29616;&#31616;&#21333;&#65292;&#23545;&#20195;&#30721;&#20462;&#25913;&#35201;&#27714;&#23569;&#65292;&#19982;&#21508;&#31181;&#37319;&#26679;&#31639;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#24378;&#22823;&#30340;&#29983;&#25104;&#24037;&#20855;&#65292;&#26679;&#26412;&#36136;&#37327;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30456;&#24403;&#65292;&#24182;&#19988;&#21453;&#26144;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20284;&#28982;&#20998;&#25968;&#12290;&#20854;&#20013;&#19968;&#37096;&#20998;&#27169;&#22411;&#65292;&#22914;DDIMs&#65292;&#23637;&#31034;&#20102;&#22266;&#26377;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;$T$&#20010;&#27493;&#39588;&#65292;&#20294;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#21482;&#20174;&#20854;&#20013;&#30340;&#23376;&#38598;&#36827;&#34892;&#37319;&#26679;&#12290;&#36825;&#31181;&#36873;&#25321;&#24615;&#37319;&#26679;&#26041;&#27861;&#34429;&#28982;&#20248;&#21270;&#20102;&#36895;&#24230;&#65292;&#20294;&#26080;&#24847;&#20013;&#38169;&#36807;&#20102;&#26410;&#37319;&#26679;&#27493;&#39588;&#20013;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#23548;&#33268;&#26679;&#26412;&#36136;&#37327;&#21487;&#33021;&#20986;&#29616;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;S$^{2}$-DMs&#65292;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#21019;&#26032;&#30340;$L_{skip}$&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#37325;&#26032;&#25972;&#21512;&#22312;&#36873;&#25321;&#24615;&#37319;&#26679;&#38454;&#27573;&#20013;&#30465;&#30053;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22909;&#22788;&#24456;&#22810;&#65306;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#36136;&#37327;&#65292;&#23454;&#29616;&#36215;&#26469;&#38750;&#24120;&#31616;&#21333;&#65292;&#38656;&#35201;&#26368;&#23569;&#30340;&#20195;&#30721;&#20462;&#25913;&#65292;&#24182;&#19988;&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#20197;&#19982;&#21508;&#31181;&#37319;&#26679;&#31639;&#27861;&#20860;&#23481;&#12290;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#27169;&#22411;&#35757;&#32451;&#30340;&#32467;&#26524;...
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as powerful generative tools, rivaling GANs in sample quality and mirroring the likelihood scores of autoregressive models. A subset of these models, exemplified by DDIMs, exhibit an inherent asymmetry: they are trained over $T$ steps but only sample from a subset of $T$ during generation. This selective sampling approach, though optimized for speed, inadvertently misses out on vital information from the unsampled steps, leading to potential compromises in sample quality. To address this issue, we present the S$^{2}$-DMs, which is a new training method by using an innovative $L_{skip}$, meticulously designed to reintegrate the information omitted during the selective sampling phase. The benefits of this approach are manifold: it notably enhances sample quality, is exceptionally simple to implement, requires minimal code modifications, and is flexible enough to be compatible with various sampling algorithms. On the CIFAR10 dataset, models trained using our 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#26469;&#25552;&#39640;&#26893;&#29289;&#21494;&#29255;&#30142;&#30149;&#20998;&#31867;&#27169;&#22411;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#33719;&#24471;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#21516;&#26102;&#36890;&#36807;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#40065;&#26834;&#24615;&#21487;&#33021;&#20197;&#20998;&#31867;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#65292;&#32780;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#24615;&#33021;&#25439;&#22833;&#33976;&#39311;&#22797;&#26434;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.00334</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#21487;&#35299;&#37322;&#24615;&#23548;&#21521;&#21494;&#29255;&#30142;&#30149;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Explainability-Driven Leaf Disease Classification using Adversarial Training and Knowledge Distillation. (arXiv:2401.00334v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#26469;&#25552;&#39640;&#26893;&#29289;&#21494;&#29255;&#30142;&#30149;&#20998;&#31867;&#27169;&#22411;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#33719;&#24471;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#21516;&#26102;&#36890;&#36807;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#40065;&#26834;&#24615;&#21487;&#33021;&#20197;&#20998;&#31867;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#65292;&#32780;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#24615;&#33021;&#25439;&#22833;&#33976;&#39311;&#22797;&#26434;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#26893;&#29289;&#21494;&#29255;&#30142;&#30149;&#20998;&#31867;&#65292;&#24182;&#25506;&#32034;&#20102;&#23545;&#25239;&#35757;&#32451;&#12289;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21644;&#27169;&#22411;&#21387;&#32553;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#22686;&#24378;&#27169;&#22411;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#23041;&#32961;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30830;&#20445;&#20934;&#30830;&#20998;&#31867;&#12290;&#20511;&#21161;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#20197;&#20248;&#21270;&#35745;&#31639;&#25928;&#29575;&#21516;&#26102;&#20445;&#25345;&#20998;&#31867;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#40065;&#26834;&#24615;&#21487;&#33021;&#20197;&#20998;&#31867;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#65292;&#23545;&#24120;&#35268;&#27979;&#35797;&#30340;&#24615;&#33021;&#25439;&#22833;&#20026;3%-20%&#65292;&#23545;&#23545;&#25239;&#25915;&#20987;&#27979;&#35797;&#30340;&#24615;&#33021;&#25552;&#39640;&#20026;50%-70%&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#22312;&#24615;&#33021;&#31245;&#26377;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#27604;&#22797;&#26434;&#27169;&#22411;&#39640;15-25&#20493;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33976;&#39311;&#20102;&#26356;&#22797;&#26434;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work focuses on plant leaf disease classification and explores three crucial aspects: adversarial training, model explainability, and model compression. The models' robustness against adversarial attacks is enhanced through adversarial training, ensuring accurate classification even in the presence of threats. Leveraging explainability techniques, we gain insights into the model's decision-making process, improving trust and transparency. Additionally, we explore model compression techniques to optimize computational efficiency while maintaining classification performance. Through our experiments, we determine that on a benchmark dataset, the robustness can be the price of the classification accuracy with performance reductions of 3%-20% for regular tests and gains of 50%-70% for adversarial attack tests. We also demonstrate that a student model can be 15-25 times more computationally efficient for a slight performance reduction, distilling the knowledge of more complex models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#21892;&#24694;&#21155;&#26465;&#20214;&#19979;&#20010;&#24615;&#21270;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#22312;&#24178;&#20928;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#33021;&#22815;&#24471;&#21040;&#26356;&#40065;&#26834;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2312.16613</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#29992;&#20110;&#24694;&#21155;&#26465;&#20214;&#19979;&#30340;&#20010;&#24615;&#21270;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Pretraining for Robust Personalized Voice Activity Detection in Adverse Conditions. (arXiv:2312.16613v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#21892;&#24694;&#21155;&#26465;&#20214;&#19979;&#20010;&#24615;&#21270;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#22312;&#24178;&#20928;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#33021;&#22815;&#24471;&#21040;&#26356;&#40065;&#26834;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#24694;&#21155;&#26465;&#20214;&#19979;&#20010;&#24615;&#21270;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#22238;&#24402;&#39044;&#27979;&#32534;&#30721;&#26694;&#26550;&#39044;&#35757;&#32451;&#20102;&#19968;&#20010;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)-&#32534;&#30721;&#22120;&#65292;&#24182;&#23545;&#20010;&#24615;&#21270;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#22122;&#21464;&#20307;&#30340;&#33258;&#22238;&#24402;&#39044;&#27979;&#32534;&#30721;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#20010;&#24615;&#21270;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23545;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21253;&#25324;&#22312;&#24178;&#20928;&#35821;&#38899;&#21644;&#19981;&#21516;&#20449;&#22122;&#27604;&#27700;&#24179;&#19979;&#21463;&#21508;&#31181;&#22122;&#22768;&#27745;&#26579;&#30340;&#35821;&#38899;&#19978;&#30340;&#35780;&#20272;&#65292;&#24182;&#19982;&#32431;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#19981;&#20165;&#33021;&#22312;&#24178;&#20928;&#26465;&#20214;&#19979;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19988;&#30456;&#27604;&#20110;&#32431;&#30417;&#30563;&#23398;&#20064;&#65292;&#36824;&#33021;&#20135;&#29983;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the use of self-supervised pretraining on a large unlabelled data set to improve the performance of a personalized voice activity detection (VAD) model in adverse conditions. We pretrain a long short-term memory (LSTM)-encoder using the autoregressive predictive coding (APC) framework and fine-tune it for personalized VAD. We also propose a denoising variant of APC, with the goal of improving the robustness of personalized VAD. The trained models are systematically evaluated on both clean speech and speech contaminated by various types of noise at different SNR-levels and compared to a purely supervised model. Our experiments show that self-supervised pretraining not only improves performance in clean conditions, but also yields models which are more robust to adverse conditions compared to purely supervised learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23450;&#20215;&#29615;&#22659;&#19979;&#36827;&#34892;&#38656;&#27714;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22240;&#26524;&#25512;&#26029;&#30340;&#21452;&#37325;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#39044;&#27979;&#27169;&#22411;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23436;&#20840;&#25511;&#21046;&#30340;&#24773;&#20917;&#19979;&#26356;&#22909;&#22320;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#31163;&#32447;&#25919;&#31574;&#35774;&#32622;&#20013;&#20248;&#20110;&#20854;&#20182;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.15282</link><description>&lt;p&gt;
&#23450;&#20215;&#30340;&#22240;&#26524;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal Forecasting for Pricing. (arXiv:2312.15282v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23450;&#20215;&#29615;&#22659;&#19979;&#36827;&#34892;&#38656;&#27714;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22240;&#26524;&#25512;&#26029;&#30340;&#21452;&#37325;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#39044;&#27979;&#27169;&#22411;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23436;&#20840;&#25511;&#21046;&#30340;&#24773;&#20917;&#19979;&#26356;&#22909;&#22320;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#31163;&#32447;&#25919;&#31574;&#35774;&#32622;&#20013;&#20248;&#20110;&#20854;&#20182;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23450;&#20215;&#29615;&#22659;&#19979;&#36827;&#34892;&#38656;&#27714;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24314;&#27169;&#20215;&#26684;&#20316;&#20026;&#38656;&#27714;&#30340;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#38646;&#21806;&#21830;&#30340;&#30446;&#26631;&#26159;&#20197;&#65288;&#21033;&#28070;&#65289;&#26368;&#20339;&#26041;&#24335;&#35774;&#23450;&#20215;&#26684;&#65292;&#20197;&#35299;&#20915;&#19979;&#28216;&#20915;&#31574;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22240;&#26524;&#25512;&#26029;&#30340;&#21452;&#37325;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#39044;&#27979;&#27169;&#22411;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#19968;&#26041;&#38754;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23436;&#20840;&#25511;&#21046;&#30340;&#24773;&#20917;&#19979;&#23545;&#21512;&#25104;&#30340;&#12289;&#20294;&#29616;&#23454;&#30340;&#25968;&#25454;&#26356;&#22909;&#22320;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#25968;&#25454;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31163;&#32447;&#25919;&#31574;&#35774;&#32622;&#65288;&#21363;&#23450;&#20215;&#25919;&#31574;&#21457;&#29983;&#21464;&#21270;&#26102;&#65289;&#20013;&#20248;&#20110;&#20854;&#20182;&#39044;&#27979;&#26041;&#27861;&#65292;&#32780;&#22312;&#22312;&#32447;&#25919;&#31574;&#35774;&#32622;&#20013;&#30053;&#26377;&#33853;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel method for demand forecasting in a pricing context. Here, modeling the causal relationship between price as an input variable to demand is crucial because retailers aim to set prices in a (profit) optimal manner in a downstream decision making problem. Our methods bring together the Double Machine Learning methodology for causal inference and state-of-the-art transformer-based forecasting models. In extensive empirical experiments, we show on the one hand that our method estimates the causal effect better in a fully controlled setting via synthetic, yet realistic data. On the other hand, we demonstrate on real-world data that our method outperforms forecasting methods in off-policy settings (i.e., when there's a change in the pricing policy) while only slightly trailing in the on-policy setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#20915;&#31574;&#26641;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#26377;&#19977;&#20010;&#36129;&#29486;&#65306;&#25552;&#20379;&#20102;&#23545;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#40065;&#26834;&#24615;&#30340;&#26032;&#27934;&#23519;&#65292;&#24341;&#20837;&#20102;&#20998;&#24067;&#25439;&#22833;&#20989;&#25968;&#30340;&#26694;&#26550;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36138;&#23146;&#20943;&#23569;&#19981;&#32431;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.12937</link><description>&lt;p&gt;
&#35757;&#32451;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#20915;&#31574;&#26641;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Robust Loss Functions for Training Decision Trees with Noisy Labels. (arXiv:2312.12937v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#20915;&#31574;&#26641;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#26377;&#19977;&#20010;&#36129;&#29486;&#65306;&#25552;&#20379;&#20102;&#23545;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#40065;&#26834;&#24615;&#30340;&#26032;&#27934;&#23519;&#65292;&#24341;&#20837;&#20102;&#20998;&#24067;&#25439;&#22833;&#20989;&#25968;&#30340;&#26694;&#26550;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36138;&#23146;&#20943;&#23569;&#19981;&#32431;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#35757;&#32451;&#20915;&#31574;&#26641;&#65292;&#37325;&#28857;&#30740;&#31350;&#21487;&#20197;&#23548;&#33268;&#40065;&#26834;&#23398;&#20064;&#31639;&#27861;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#20915;&#31574;&#26641;&#23398;&#20064;&#32972;&#26223;&#19979;&#35768;&#22810;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#30340;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#29702;&#35770;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#25439;&#22833;&#23646;&#20110;&#25105;&#20204;&#25152;&#31216;&#30340;&#20445;&#23432;&#25439;&#22833;&#31867;&#21035;&#65292;&#24182;&#19988;&#20445;&#23432;&#25439;&#22833;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20250;&#20986;&#29616;&#25552;&#21069;&#20572;&#27490;&#34892;&#20026;&#65292;&#32780;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#20855;&#26377;&#23481;&#24525;&#22122;&#22768;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26500;&#24314;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#20998;&#24067;&#25439;&#22833;&#12290;&#36825;&#20123;&#25439;&#22833;&#22522;&#20110;&#20551;&#35774;&#30340;&#36793;&#32536;&#20998;&#24067;&#24212;&#29992;&#22522;&#20110;&#30334;&#20998;&#20301;&#30340;&#24809;&#32602;&#65292;&#23427;&#20204;&#36890;&#36807;&#40065;&#26834;&#24615;&#21442;&#25968;&#33258;&#28982;&#22320;&#20801;&#35768;&#36866;&#24212;&#19981;&#21516;&#30340;&#22122;&#22768;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#36127;&#25351;&#25968;&#25439;&#22833;&#30340;&#26032;&#25439;&#22833;&#65292;&#23427;&#21487;&#20197;&#23548;&#33268;&#39640;&#25928;&#30340;&#36138;&#23146;&#20943;&#23569;&#19981;&#32431;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#22122;&#22768;&#26465;&#20214;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider training decision trees using noisily labeled data, focusing on loss functions that can lead to robust learning algorithms. Our contributions are threefold. First, we offer novel theoretical insights on the robustness of many existing loss functions in the context of decision tree learning. We show that some of the losses belong to a class of what we call conservative losses, and the conservative losses lead to an early stopping behavior during training and noise-tolerant predictions during testing. Second, we introduce a framework for constructing robust loss functions, called distribution losses. These losses apply percentile-based penalties based on an assumed margin distribution, and they naturally allow adapting to different noise rates via a robustness parameter. In particular, we introduce a new loss called the negative exponential loss, which leads to an efficient greedy impurity-reduction learning algorithm. Lastly, our experiments on multiple datasets and noise se
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36861;&#36394;&#20219;&#20309;&#29289;&#20307;&#30340;&#38750;&#29616;&#24577;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#24494;&#35843;&#29616;&#24577;&#36319;&#36394;&#22120;&#65292;&#21487;&#20197;&#25552;&#39640;&#36861;&#36394;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.12433</link><description>&lt;p&gt;
&#36861;&#36394;&#20219;&#20309;&#29289;&#20307;&#30340;&#38750;&#29616;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tracking Any Object Amodally. (arXiv:2312.12433v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36861;&#36394;&#20219;&#20309;&#29289;&#20307;&#30340;&#38750;&#29616;&#24577;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#24494;&#35843;&#29616;&#24577;&#36319;&#36394;&#22120;&#65292;&#21487;&#20197;&#25552;&#39640;&#36861;&#36394;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#29616;&#24577;&#24863;&#30693;&#26159;&#19968;&#31181;&#20174;&#37096;&#20998;&#21487;&#35265;&#24615;&#20013;&#29702;&#35299;&#23436;&#25972;&#29289;&#20307;&#32467;&#26500;&#30340;&#22522;&#26412;&#25216;&#33021;&#65292;&#23427;&#23545;&#20110;&#23156;&#20799;&#29978;&#33267;&#26159;&#25104;&#20154;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#23427;&#30340;&#37325;&#35201;&#24615;&#24310;&#20280;&#21040;&#20102;&#33258;&#21160;&#39550;&#39542;&#31561;&#24212;&#29992;&#39046;&#22495;&#65292;&#23545;&#20110;&#29702;&#35299;&#37325;&#21472;&#29289;&#20307;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#31639;&#27861;&#36890;&#24120;&#24573;&#35270;&#20102;&#36825;&#19968;&#20851;&#38190;&#33021;&#21147;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#20351;&#29992;&#30340;&#26159;&#29616;&#24577;&#26631;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#29616;&#24577;&#25968;&#25454;&#30340;&#21294;&#20047;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TAO-Amodal&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#21315;&#20010;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;880&#20010;&#22810;&#26679;&#21270;&#30340;&#29289;&#20307;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#21487;&#35265;&#21644;&#36974;&#25377;&#23545;&#35937;&#30340;&#38750;&#29616;&#24577;&#21644;&#29616;&#24577;&#36793;&#30028;&#26694;&#65292;&#21253;&#25324;&#37096;&#20998;&#36229;&#20986;&#30011;&#38754;&#33539;&#22260;&#30340;&#29289;&#20307;&#12290;&#20026;&#20102;&#22686;&#24378;&#38750;&#29616;&#24577;&#36861;&#36394;&#30340;&#30446;&#26631;&#27704;&#20037;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25554;&#20214;&#27169;&#22359;&#65292;&#21363;&#38750;&#29616;&#24577;&#25193;&#23637;&#22120;&#65292;&#36890;&#36807;&#23545;&#20960;&#30334;&#20010;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#24494;&#35843;&#65292;&#23558;&#26631;&#20934;&#30340;&#29616;&#24577;&#36319;&#36394;&#22120;&#36716;&#21270;&#20026;&#38750;&#29616;&#24577;&#36319;&#36394;&#22120;&#12290;&#25105;&#20204;&#21462;&#24471;&#20102;3.3&#65285;&#21644;1.6&#65285;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amodal perception, the ability to comprehend complete object structures from partial visibility, is a fundamental skill, even for infants. Its significance extends to applications like autonomous driving, where a clear understanding of heavily occluded objects is essential. However, modern detection and tracking algorithms often overlook this critical capability, perhaps due to the prevalence of modal annotations in most datasets. To address the scarcity of amodal data, we introduce the TAO-Amodal benchmark, featuring 880 diverse categories in thousands of video sequences. Our dataset includes amodal and modal bounding boxes for visible and occluded objects, including objects that are partially out-of-frame. To enhance amodal tracking with object permanence, we leverage a lightweight plug-in module, the amodal expander, to transform standard, modal trackers into amodal ones through fine-tuning on a few hundred video sequences with data augmentation. We achieve a 3.3\% and 1.6\% improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20559;&#22909;&#21644;&#24182;&#21457;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#21644;&#20851;&#27880;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#25429;&#25417;&#29992;&#25143;&#19982;&#29289;&#21697;&#30340;&#39640;&#38454;&#20449;&#24687;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.11486</link><description>&lt;p&gt;
&#20559;&#22909;&#21644;&#24182;&#21457;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Preference and Concurrence Aware Bayesian Graph Neural Networks for Recommender Systems. (arXiv:2312.11486v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20559;&#22909;&#21644;&#24182;&#21457;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#21644;&#20851;&#27880;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#25429;&#25417;&#29992;&#25143;&#19982;&#29289;&#21697;&#30340;&#39640;&#38454;&#20449;&#24687;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#23545;&#20110;&#25512;&#33616;&#31995;&#32479;&#20855;&#26377;&#31361;&#20986;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#25429;&#25417;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#39640;&#38454;&#20449;&#24687;&#65292;&#20854;&#20013;&#22270;&#26159;&#30001;&#35266;&#23519;&#21040;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#26500;&#24314;&#30340;&#65292;&#36825;&#20123;&#20132;&#20114;&#21487;&#33021;&#20250;&#20002;&#22833;&#38142;&#25509;&#25110;&#21253;&#21547;&#34394;&#20551;&#30340;&#27491;&#20132;&#20114;&#12290;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20851;&#38190;&#38382;&#39064;&#26159;&#35774;&#35745;&#19968;&#26063;&#36866;&#21512;&#25512;&#33616;&#31995;&#32479;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21516;&#26102;&#32771;&#34385;&#20102;&#29992;&#25143;&#30340;&#20559;&#22909;&#12289;&#29289;&#21697;&#30340;&#24182;&#21457;&#20197;&#21450;&#19968;&#20123;&#37325;&#35201;&#30340;&#22270;&#32467;&#26500;&#20449;&#24687;&#12290;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based collaborative filtering methods have prevailing performance for recommender systems since they can capture high-order information between users and items, in which the graphs are constructed from the observed user-item interactions that might miss links or contain spurious positive interactions in industrial scenarios. The Bayesian Graph Neural Network framework approaches this issue with generative models for the interaction graphs. The critical problem is to devise a proper family of graph generative models tailored to recommender systems. We propose an efficient generative model that jointly considers the preferences of users, the concurrence of items and some important graph structure information. Experiments on four popular benchmark datasets demonstrate the effectiveness of our proposed graph generative methods for recommender systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#26500;&#24314;&#21487;&#20449;&#30340;AI&#36719;&#20214;&#24320;&#21457;&#21161;&#25163;&#12290;&#36890;&#36807;&#32508;&#21512;&#26550;&#26500;&#12289;&#35757;&#32451;&#22522;&#30784;LLM&#21644;&#20351;&#29992;&#22270;&#24418;&#20195;&#30721;&#34920;&#31034;&#65292;&#20197;&#21450;&#25972;&#21512;&#30693;&#35782;&#22270;&#31561;&#25216;&#26415;&#25163;&#27573;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#30456;&#20851;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2312.09126</link><description>&lt;p&gt;
&#26500;&#24314;&#21487;&#20449;&#30340;AI&#36719;&#20214;&#24320;&#21457;&#21161;&#25163;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy AI Software Development Assistance. (arXiv:2312.09126v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#26500;&#24314;&#21487;&#20449;&#30340;AI&#36719;&#20214;&#24320;&#21457;&#21161;&#25163;&#12290;&#36890;&#36807;&#32508;&#21512;&#26550;&#26500;&#12289;&#35757;&#32451;&#22522;&#30784;LLM&#21644;&#20351;&#29992;&#22270;&#24418;&#20195;&#30721;&#34920;&#31034;&#65292;&#20197;&#21450;&#25972;&#21512;&#30693;&#35782;&#22270;&#31561;&#25216;&#26415;&#25163;&#27573;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#30456;&#20851;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35745;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#65292;AI&#36719;&#20214;&#24320;&#21457;&#21161;&#25163;&#23558;&#22312;&#36719;&#20214;&#34892;&#19994;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#36719;&#20214;&#24320;&#21457;&#21161;&#25163;&#24448;&#24448;&#19981;&#21487;&#38752;&#65292;&#32463;&#24120;&#29983;&#25104;&#38169;&#35823;&#12289;&#19981;&#23433;&#20840;&#25110;&#20302;&#36136;&#37327;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32508;&#21512;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#29992;&#20110;&#26500;&#24314;&#12289;&#35757;&#32451;&#21644;&#20351;&#29992;&#21487;&#20449;&#30340;AI&#36719;&#20214;&#24320;&#21457;&#21161;&#25163;&#12290;&#26550;&#26500;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22312;&#20195;&#34920;&#30495;&#23454;&#19990;&#30028;&#32534;&#30721;&#22330;&#26223;&#21644;&#22797;&#26434;&#36719;&#20214;&#26550;&#26500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22522;&#30784;LLM&#65292;&#24182;&#22312;&#20195;&#30721;&#36136;&#37327;&#26631;&#20934;&#19978;&#36827;&#34892;&#20102;&#31934;&#35843;&#12290;LLM&#23558;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#20195;&#30721;&#34920;&#31034;&#36827;&#34892;&#39640;&#32423;&#35821;&#20041;&#29702;&#35299;&#12290;&#25105;&#20204;&#35774;&#24819;&#23558;&#30693;&#35782;&#22270;&#38598;&#25104;&#21040;&#31995;&#32479;&#20013;&#65292;&#25552;&#20379;&#26368;&#26032;&#30340;&#32972;&#26223;&#30693;&#35782;&#65292;&#24182;&#20351;&#21161;&#25163;&#33021;&#22815;&#25552;&#20379;&#36866;&#24403;&#30340;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#19968;&#20010;&#29992;&#20110;&#21463;&#38480;&#35299;&#30721;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#23558;&#30830;&#20445;&#26576;&#20123;&#20445;&#35777;&#65288;&#20363;&#22914;&#65292;&#23545;&#27491;&#30830;&#24615;&#30340;&#20445;&#35777;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is expected that in the near future, AI software development assistants will play an important role in the software industry. However, current software development assistants tend to be unreliable, often producing incorrect, unsafe, or low-quality code. We seek to resolve these issues by introducing a holistic architecture for constructing, training, and using trustworthy AI software development assistants. In the center of the architecture, there is a foundational LLM trained on datasets representative of real-world coding scenarios and complex software architectures, and fine-tuned on code quality criteria beyond correctness. The LLM will make use of graph-based code representations for advanced semantic comprehension. We envision a knowledge graph integrated into the system to provide up-to-date background knowledge and to enable the assistant to provide appropriate explanations. Finally, a modular framework for constrained decoding will ensure that certain guarantees (e.g., for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#26680;&#26426;&#22120;&#39044;&#22788;&#29702;&#20013;&#20351;&#29992;Nystrom&#36924;&#36817;&#30340;&#26435;&#34913;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#23545;&#25968;&#22823;&#23567;&#30340;&#26679;&#26412;&#33021;&#22815;&#35753;Nystrom&#36924;&#36817;&#30340;&#39044;&#22788;&#29702;&#22120;&#20960;&#20046;&#19982;&#26799;&#24230;&#19979;&#38477;&#21516;&#26679;&#26377;&#25928;&#22320;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2312.03311</link><description>&lt;p&gt;
&#23545;&#20110;&#26680;&#26426;&#22120;&#22312;&#39044;&#22788;&#29702;&#20013;&#30340;Nystrom&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
On the Nystrom Approximation for Preconditioning in Kernel Machines. (arXiv:2312.03311v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#26680;&#26426;&#22120;&#39044;&#22788;&#29702;&#20013;&#20351;&#29992;Nystrom&#36924;&#36817;&#30340;&#26435;&#34913;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#23545;&#25968;&#22823;&#23567;&#30340;&#26679;&#26412;&#33021;&#22815;&#35753;Nystrom&#36924;&#36817;&#30340;&#39044;&#22788;&#29702;&#22120;&#20960;&#20046;&#19982;&#26799;&#24230;&#19979;&#38477;&#21516;&#26679;&#26377;&#25928;&#22320;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#26041;&#27861;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#31867;&#27969;&#34892;&#30340;&#38750;&#32447;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;&#23398;&#20064;&#26680;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#38656;&#35201;&#20855;&#26377;&#36845;&#20195;&#24615;&#36136;&#65292;&#20294;&#30001;&#20110;&#31967;&#31957;&#30340;&#26465;&#20214;&#65292;&#25910;&#25947;&#21487;&#33021;&#24456;&#24930;&#12290;&#35889;&#39044;&#22788;&#29702;&#26159;&#21152;&#24555;&#35757;&#32451;&#26680;&#27169;&#22411;&#36845;&#20195;&#31639;&#27861;&#25910;&#25947;&#36895;&#24230;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#21644;&#23384;&#20648;&#35889;&#39044;&#22788;&#29702;&#22120;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#65292;&#20250;&#23548;&#33268;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#65292;&#38480;&#21046;&#20102;&#26680;&#26041;&#27861;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;Nystrom&#36924;&#36817;&#30340;&#35889;&#39044;&#22788;&#29702;&#22120;&#36890;&#24120;&#26356;&#20415;&#23452;&#21644;&#26356;&#23481;&#26131;&#35745;&#31639;&#21644;&#23384;&#20648;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#20351;&#29992;&#36825;&#31181;&#36924;&#36817;&#39044;&#22788;&#29702;&#22120;&#30340;&#26435;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#34920;&#26126;&#19982;&#25968;&#25454;&#38598;&#22823;&#23567;&#30456;&#20851;&#30340;&#23545;&#25968;&#26679;&#26412;&#25968;&#37327;&#33021;&#22815;&#35753;&#22522;&#20110;Nystrom&#36924;&#36817;&#30340;&#39044;&#22788;&#29702;&#22120;&#20960;&#20046;&#19982;&#26799;&#24230;&#19979;&#38477;&#21516;&#26679;&#26377;&#25928;&#22320;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel methods are a popular class of nonlinear predictive models in machine learning. Scalable algorithms for learning kernel models need to be iterative in nature, but convergence can be slow due to poor conditioning. Spectral preconditioning is an important tool to speed-up the convergence of such iterative algorithms for training kernel models. However computing and storing a spectral preconditioner can be expensive which can lead to large computational and storage overheads, precluding the application of kernel methods to problems with large datasets. A Nystrom approximation of the spectral preconditioner is often cheaper to compute and store, and has demonstrated success in practical applications. In this paper we analyze the trade-offs of using such an approximated preconditioner. Specifically, we show that a sample of logarithmic size (as a function of the size of the dataset) enables the Nystrom-based approximated preconditioner to accelerate gradient descent nearly as well as
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#35843;&#24230;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.02246</link><description>&lt;p&gt;
&#26465;&#20214;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conditional Variational Diffusion Models. (arXiv:2312.02246v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#35843;&#24230;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#38382;&#39064;&#26088;&#22312;&#20174;&#35266;&#27979;&#20013;&#30830;&#23450;&#21442;&#25968;&#65292;&#36825;&#26159;&#24037;&#31243;&#21644;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#65292;&#22240;&#20854;&#33021;&#22815;&#20135;&#29983;&#36924;&#30495;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#33391;&#22909;&#30340;&#25968;&#23398;&#29305;&#24615;&#32780;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#32570;&#28857;&#26159;&#23545;&#26041;&#24046;&#35843;&#24230;&#30340;&#36873;&#25321;&#25935;&#24863;&#65292;&#35813;&#35843;&#24230;&#25511;&#21046;&#30528;&#25193;&#25955;&#36807;&#31243;&#30340;&#21160;&#24577;&#12290;&#20026;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#24494;&#35843;&#36825;&#20010;&#35843;&#24230;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#26102;&#38388;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#19981;&#33021;&#20445;&#35777;&#26368;&#20248;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;&#35843;&#24230;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25903;&#25345;&#23545;&#25968;&#25454;&#30340;&#27010;&#29575;&#26465;&#20214;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#24320;&#38144;&#19979;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#19981;&#30456;&#20851;&#30340;&#36870;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;&#36229;&#20998;&#36776;&#29575;&#26174;&#24494;&#38236;&#21644;&#23450;&#37327;&#30456;&#20301;&#25104;&#20687;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#36739;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems aim to determine parameters from observations, a crucial task in engineering and science. Lately, generative models, especially diffusion models, have gained popularity in this area for their ability to produce realistic solutions and their good mathematical properties. Despite their success, an important drawback of diffusion models is their sensitivity to the choice of variance schedule, which controls the dynamics of the diffusion process. Fine-tuning this schedule for specific applications is crucial but time-costly and does not guarantee an optimal result. We propose a novel approach for learning the schedule as part of the training process. Our method supports probabilistic conditioning on data, provides high-quality solutions, and is flexible, proving able to adapt to different applications with minimum overhead. This approach is tested in two unrelated inverse problems: super-resolution microscopy and quantitative phase imaging, yielding comparable or superior 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;</title><link>http://arxiv.org/abs/2312.01185</link><description>&lt;p&gt;
&#26102;&#38388;&#20013;&#30340;&#28063;&#28458;&#65306;&#32654;&#22269;&#21382;&#21490;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Kaggle&#30340;&#22269;&#24773;&#21672;&#25991;&#25968;&#25454;&#38598;&#23545;&#32654;&#22269;&#21382;&#21490;&#30340;&#24635;&#20307;&#26102;&#38388;&#32447;&#21450;&#21672;&#25991;&#26412;&#36523;&#30340;&#29305;&#28857;&#21644;&#24615;&#36136;&#36827;&#34892;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#65288;&#20063;&#26377;&#20123;&#19981;&#37027;&#20040;&#20196;&#20154;&#24778;&#35766;&#65289;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#65292;&#22914;BERT&#65288;DistilBERT&#65289;&#21644;GPT-2&#12290;&#34429;&#28982;&#24191;&#27867;&#35748;&#20026;BERT&#65288;&#21450;&#20854;&#21464;&#20307;&#65289;&#26368;&#36866;&#21512;NLP&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;GPT-2&#32467;&#21512;UMAP&#31561;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#26356;&#24378;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;&#36825;&#20351;&#24471;GPT-2 + UMAP&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#23601;&#36275;&#22815;&#22909;&#29992;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#26469;&#26816;&#27979;&#21738;&#20301;&#24635;&#32479;&#21457;&#34920;&#20102;&#21738;&#31687;&#28436;&#35762;&#65292;&#24182;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#65288;&#20934;&#30830;&#29575;&#20026;93\% - 95\%&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#36816;&#34892;&#24773;&#20917;&#65289;&#12290;&#20026;&#20102;&#30830;&#23450;&#20889;&#20316;&#24180;&#20221;&#65292;&#25105;&#20204;&#36824;&#25191;&#34892;&#20102;&#19968;&#20010;&#31867;&#20284;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20174;&#21333;&#20010;&#19977;&#32500;&#32467;&#26500;MRI&#24555;&#29031;&#20013;&#20272;&#35745;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#65288;GBM&#65289;&#29983;&#38271;&#27169;&#22411;&#30340;&#24739;&#32773;&#29305;&#24322;&#24615;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#29702;&#35770;&#21644;&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#20026;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#30340;&#27835;&#30103;&#35774;&#35745;&#25552;&#20379;&#20102;&#20851;&#38190;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2311.16536</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#39044;&#27979;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#28024;&#28070;:&#25968;&#23398;&#27169;&#22411;&#12289;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#27169;&#24577;&#25195;&#25551;
&lt;/p&gt;
&lt;p&gt;
Personalized Predictions of Glioblastoma Infiltration: Mathematical Models, Physics-Informed Neural Networks and Multimodal Scans. (arXiv:2311.16536v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20174;&#21333;&#20010;&#19977;&#32500;&#32467;&#26500;MRI&#24555;&#29031;&#20013;&#20272;&#35745;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#65288;GBM&#65289;&#29983;&#38271;&#27169;&#22411;&#30340;&#24739;&#32773;&#29305;&#24322;&#24615;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#29702;&#35770;&#21644;&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#20026;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#30340;&#27835;&#30103;&#35774;&#35745;&#25552;&#20379;&#20102;&#20851;&#38190;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#65288;GBM&#65289;&#20174;&#21307;&#23398;MRI&#25195;&#25551;&#20013;&#30340;&#28024;&#28070;&#23545;&#20110;&#29702;&#35299;&#32959;&#30244;&#29983;&#38271;&#21160;&#21147;&#23398;&#21644;&#35774;&#35745;&#20010;&#20307;&#21270;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;GBM&#29983;&#38271;&#30340;&#25968;&#23398;&#27169;&#22411;&#21487;&#20197;&#22312;&#39044;&#27979;&#32959;&#30244;&#32454;&#32990;&#30340;&#31354;&#38388;&#20998;&#24067;&#20013;&#34917;&#20805;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#20174;&#20020;&#24202;&#25968;&#25454;&#20013;&#20272;&#35745;&#27169;&#22411;&#30340;&#24739;&#32773;&#29305;&#24322;&#24615;&#21442;&#25968;&#65292;&#30001;&#20110;&#26102;&#38388;&#25968;&#25454;&#26377;&#38480;&#19988;&#25104;&#20687;&#21644;&#35786;&#26029;&#20043;&#38388;&#30340;&#26102;&#38388;&#26377;&#38480;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21453;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20174;&#21333;&#20010;&#19977;&#32500;&#32467;&#26500;MRI&#24555;&#29031;&#20013;&#20272;&#35745;GBM&#29983;&#38271;&#21453;&#24212;&#25193;&#25955;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#30340;&#24739;&#32773;&#29305;&#24322;&#24615;&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;PINNs&#23558;&#25968;&#25454;&#21644;PDE&#23884;&#20837;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#20174;&#32780;&#25972;&#21512;&#20102;&#29702;&#35770;&#21644;&#25968;&#25454;&#12290;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#29305;&#24449;&#26080;&#37327;&#32434;&#21442;&#25968;&#30340;&#35782;&#21035;&#21644;&#20272;&#35745;&#65292;&#21033;&#29992;&#26080;&#37327;&#32434;&#21442;&#25968;&#30340;&#39044;&#35757;&#32451;&#27493;&#39588;&#20197;&#21450;&#24494;&#35843;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the infiltration of Glioblastoma (GBM) from medical MRI scans is crucial for understanding tumor growth dynamics and designing personalized radiotherapy treatment plans.Mathematical models of GBM growth can complement the data in the prediction of spatial distributions of tumor cells. However, this requires estimating patient-specific parameters of the model from clinical data, which is a challenging inverse problem due to limited temporal data and the limited time between imaging and diagnosis. This work proposes a method that uses Physics-Informed Neural Networks (PINNs) to estimate patient-specific parameters of a reaction-diffusion PDE model of GBM growth from a single 3D structural MRI snapshot. PINNs embed both the data and the PDE into a loss function, thus integrating theory and data. Key innovations include the identification and estimation of characteristic non-dimensional parameters, a pre-training step that utilizes the non-dimensional parameters and a fine-tunin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24819;&#35937;&#21147;&#22686;&#24378;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064; (IAHRL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25972;&#21512;&#24819;&#35937;&#21147;&#21040;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#23548;&#33322;&#20219;&#21153;&#20013;&#23398;&#20064;&#23433;&#20840;&#21644;&#20132;&#20114;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2311.10309</link><description>&lt;p&gt;
&#22522;&#20110;&#24819;&#35937;&#21147;&#22686;&#24378;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22478;&#24066;&#29615;&#22659;&#20013;&#23433;&#20840;&#20132;&#20114;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Imagination-Augmented Hierarchical Reinforcement Learning for Safe and Interactive Autonomous Driving in Urban Environments. (arXiv:2311.10309v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.10309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24819;&#35937;&#21147;&#22686;&#24378;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064; (IAHRL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25972;&#21512;&#24819;&#35937;&#21147;&#21040;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#23548;&#33322;&#20219;&#21153;&#20013;&#23398;&#20064;&#23433;&#20840;&#21644;&#20132;&#20114;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#26174;&#24335;&#22320;&#21033;&#29992;&#20998;&#23618;&#32467;&#26500;&#23558;&#26102;&#38388;&#25277;&#35937;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#20013;&#12290;&#29616;&#20195;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#35774;&#35745;&#19968;&#20010;&#30001;&#39640;&#23618;&#31574;&#30053;&#21644;&#20302;&#23618;&#31574;&#30053;&#32452;&#25104;&#30340;&#20998;&#23618;&#20195;&#29702;&#12290;&#39640;&#23618;&#31574;&#30053;&#36873;&#25321;&#36739;&#20302;&#39057;&#29575;&#28608;&#27963;&#30340;&#20302;&#23618;&#31574;&#30053;&#65292;&#32780;&#28608;&#27963;&#30340;&#20302;&#23618;&#31574;&#30053;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#36873;&#25321;&#19968;&#20010;&#21160;&#20316;&#12290;&#26368;&#36817;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21512;&#25104;&#23548;&#33322;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#26631;&#20934;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#26080;&#27861;&#23558;&#36825;&#20123;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#23548;&#33322;&#20219;&#21153;&#12290;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#29616;&#23454;&#19990;&#30028;&#30340;&#23548;&#33322;&#20219;&#21153;&#38656;&#35201;&#26234;&#33021;&#20307;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#25191;&#34892;&#23433;&#20840;&#21644;&#20132;&#20114;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24819;&#35937;&#21147;&#22686;&#24378;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;(IAHRL)&#65292;&#23427;&#26377;&#25928;&#22320;&#23558;&#24819;&#35937;&#21147;&#25972;&#21512;&#21040;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#23548;&#33322;&#20219;&#21153;&#20013;&#23398;&#20064;&#23433;&#20840;&#21644;&#20132;&#20114;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical reinforcement learning (HRL) incorporates temporal abstraction into reinforcement learning (RL) by explicitly taking advantage of hierarchical structure. Modern HRL typically designs a hierarchical agent composed of a high-level policy and low-level policies. The high-level policy selects which low-level policy to activate at a lower frequency and the activated low-level policy selects an action at each time step. Recent HRL algorithms have achieved performance gains over standard RL algorithms in synthetic navigation tasks. However, we cannot apply these HRL algorithms to real-world navigation tasks. One of the main challenges is that real-world navigation tasks require an agent to perform safe and interactive behaviors in dynamic environments. In this paper, we propose imagination-augmented HRL (IAHRL) that efficiently integrates imagination into HRL to enable an agent to learn safe and interactive behaviors in real-world navigation tasks. Imagination is to predict the c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27700;&#24211;&#35745;&#31639;&#32593;&#32476;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#30005;&#29983;&#29702;&#27979;&#37327;&#25968;&#25454;&#20013;&#35299;&#30721;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#26102;&#31354;&#20449;&#24687;&#65292;&#24182;&#22312;&#23439;&#35266;&#39046;&#22495;&#20869;&#37325;&#24314;&#32593;&#32476;&#32467;&#26500;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#27604;&#20854;&#20182;&#24120;&#29992;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#32593;&#32476;&#30340;&#36830;&#25509;&#22270;&#65292;&#24182;&#19988;&#33021;&#22815;&#39044;&#27979;&#32593;&#32476;&#23545;&#29305;&#23450;&#36755;&#20837;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.03131</link><description>&lt;p&gt;
&#20174;&#30005;&#29983;&#29702;&#25968;&#25454;&#20013;&#26144;&#23556;&#21644;&#39044;&#27979;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#27700;&#24211;&#35745;&#31639;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Reservoir-Computing Model for Mapping and Forecasting Neuronal Interactions from Electrophysiological Data. (arXiv:2311.03131v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03131
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27700;&#24211;&#35745;&#31639;&#32593;&#32476;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#30005;&#29983;&#29702;&#27979;&#37327;&#25968;&#25454;&#20013;&#35299;&#30721;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#26102;&#31354;&#20449;&#24687;&#65292;&#24182;&#22312;&#23439;&#35266;&#39046;&#22495;&#20869;&#37325;&#24314;&#32593;&#32476;&#32467;&#26500;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#27604;&#20854;&#20182;&#24120;&#29992;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#32593;&#32476;&#30340;&#36830;&#25509;&#22270;&#65292;&#24182;&#19988;&#33021;&#22815;&#39044;&#27979;&#32593;&#32476;&#23545;&#29305;&#23450;&#36755;&#20837;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#30005;&#29983;&#29702;&#29305;&#24615;&#33021;&#22815;&#22312;&#38750;&#24120;&#30701;&#30340;&#26102;&#38388;&#23610;&#24230;&#20869;&#25581;&#31034;&#19981;&#21516;&#32454;&#32990;&#21333;&#20803;&#20043;&#38388;&#30340;&#21508;&#31181;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#20998;&#26512;&#36825;&#20123;&#20449;&#21495;&#30340;&#36807;&#31243;&#20013;&#65292;&#19968;&#20010;&#25361;&#25112;&#23601;&#26159;&#25214;&#20986;&#32473;&#23450;&#32593;&#32476;&#30340;&#24418;&#24577;&#21644;&#21151;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#27700;&#24211;&#35745;&#31639;&#32593;&#32476;&#65288;RCN&#65289;&#26550;&#26500;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#35299;&#30721;&#31070;&#32463;&#20803;&#22521;&#20859;&#29289;&#30340;&#30005;&#29983;&#29702;&#27979;&#37327;&#25968;&#25454;&#30340;&#26102;&#31354;&#20449;&#24687;&#65292;&#24182;&#22312;&#23439;&#35266;&#39046;&#22495;&#20869;&#37325;&#24314;&#34920;&#31034;&#31070;&#32463;&#20803;&#21333;&#20803;&#20043;&#38388;&#36830;&#25509;&#24615;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#35813;&#27169;&#22411;&#21487;&#20197;&#27604;&#20132;&#21449;&#30456;&#20851;&#21644;&#20256;&#36882;&#29109;&#31561;&#24120;&#29992;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#32593;&#32476;&#30340;&#36830;&#25509;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#39044;&#27979;&#32593;&#32476;&#23545;&#29305;&#23450;&#36755;&#20837;&#65288;&#22914;&#23616;&#37096;&#21050;&#28608;&#65289;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrophysiological nature of neuronal networks allows to reveal various interactions between different cell units at a very short time-scales. One of the many challenges in analyzing these signals is to retrieve the morphology and functionality of a given network. In this work we developed a computational model, based on Reservoir Computing Network (RCN) architecture, which decodes the spatio-temporal data from electro-physiological measurements of neuronal cultures and reconstructs the network structure on a macroscopic domain, representing the connectivity between neuronal units. We demonstrate that the model can predict the connectivity map of the network with higher accuracy than the common methods such as Cross-Correlation and Transfer-Entropy. In addition, we experimentally demonstrate the ability of the model to predict a network response to a specific input, such as localized stimulus.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#29289;&#32852;&#32593;&#65288;GIoT&#65289;&#30340;&#27010;&#24565;&#21644;&#28508;&#22312;&#21069;&#26223;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#38598;&#25104;&#21040;&#29616;&#20195;&#29289;&#32852;&#32593;&#20013;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#21644;&#26234;&#33021;&#30340;&#29289;&#32852;&#32593;&#24212;&#29992;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;GAI&#30340;&#23433;&#20840;&#28608;&#21169;&#26426;&#21046;&#26694;&#26550;&#65292;&#37319;&#29992;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;GDM&#65289;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#26469;&#35299;&#20915;GIoT&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#21478;&#22806;&#65292;&#23545;&#29616;&#20195;&#36710;&#36742;&#20132;&#36890;&#30417;&#25511;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.18382</link><description>&lt;p&gt;
&#20174;&#29983;&#25104;AI&#21040;&#29983;&#25104;&#29289;&#32852;&#32593;&#65306;&#22522;&#30784;&#30693;&#35782;&#12289;&#26694;&#26550;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
From Generative AI to Generative Internet of Things: Fundamentals, Framework, and Outlooks. (arXiv:2310.18382v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#29289;&#32852;&#32593;&#65288;GIoT&#65289;&#30340;&#27010;&#24565;&#21644;&#28508;&#22312;&#21069;&#26223;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#38598;&#25104;&#21040;&#29616;&#20195;&#29289;&#32852;&#32593;&#20013;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#21644;&#26234;&#33021;&#30340;&#29289;&#32852;&#32593;&#24212;&#29992;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;GAI&#30340;&#23433;&#20840;&#28608;&#21169;&#26426;&#21046;&#26694;&#26550;&#65292;&#37319;&#29992;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;GDM&#65289;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#26469;&#35299;&#20915;GIoT&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#21478;&#22806;&#65292;&#23545;&#29616;&#20195;&#36710;&#36742;&#20132;&#36890;&#30417;&#25511;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20855;&#26377;&#29983;&#25104;&#36924;&#30495;&#25968;&#25454;&#21644;&#20419;&#36827;&#39640;&#32423;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;GAI&#38598;&#25104;&#21040;&#29616;&#20195;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#20013;&#65292;&#29983;&#25104;&#29289;&#32852;&#32593;&#65288;GIoT&#65289;&#27491;&#22312;&#20852;&#36215;&#65292;&#20855;&#26377;&#38761;&#21629;&#21270;&#31038;&#20250;&#21508;&#20010;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#23454;&#29616;&#26356;&#39640;&#25928;&#21644;&#26234;&#33021;&#30340;&#29289;&#32852;&#32593;&#24212;&#29992;&#65292;&#22914;&#26234;&#33021;&#30417;&#25511;&#21644;&#35821;&#38899;&#21161;&#25163;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GIoT&#30340;&#27010;&#24565;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#28508;&#22312;&#21069;&#26223;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#22235;&#31181;GAI&#25216;&#26415;&#65292;&#24182;&#30740;&#31350;&#20102;&#26377;&#21069;&#26223;&#30340;GIoT&#24212;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#23454;&#29616;GIoT&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;GAI&#30340;&#23433;&#20840;&#28608;&#21169;&#26426;&#21046;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;GDM&#65289;&#36827;&#34892;&#28608;&#21169;&#26426;&#21046;&#35774;&#35745;&#65292;&#24182;&#24212;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#36827;&#34892;&#23433;&#20840;&#30340;GIoT&#31649;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#29616;&#20195;&#36710;&#36742;&#20132;&#36890;&#30417;&#25511;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (GAI) possesses the capabilities of generating realistic data and facilitating advanced decision-making. By integrating GAI into modern Internet of Things (IoT), Generative Internet of Things (GIoT) is emerging and holds immense potential to revolutionize various aspects of society, enabling more efficient and intelligent IoT applications, such as smart surveillance and voice assistants. In this article, we present the concept of GIoT and conduct an exploration of its potential prospects. Specifically, we first overview four GAI techniques and investigate promising GIoT applications. Then, we elaborate on the main challenges in enabling GIoT and propose a general GAI-based secure incentive mechanism framework to address them, in which we adopt Generative Diffusion Models (GDMs) for incentive mechanism designs and apply blockchain technologies for secure GIoT management. Moreover, we conduct a case study on modern Internet of Vehicle traffic monitoring
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#36873;&#25321;&#22238;&#28335;&#31383;&#21475;&#26469;&#26368;&#22823;&#21270;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#65292;&#24182;&#20445;&#25345;&#32047;&#31215;&#20559;&#24046;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#20869;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#23545;&#26410;&#30693;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#65292;&#36951;&#25022;&#30028;&#22312;&#24378;&#20984;&#25110;&#28385;&#36275;Lipschitz&#26465;&#20214;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#20989;&#25968;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.18304</link><description>&lt;p&gt;
&#23398;&#20064;&#38750;&#31283;&#24577;&#26465;&#20214;&#19979;&#30340;&#31283;&#23450;&#24615;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Stability Principle for Learning under Non-Stationarity. (arXiv:2310.18304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#36873;&#25321;&#22238;&#28335;&#31383;&#21475;&#26469;&#26368;&#22823;&#21270;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#65292;&#24182;&#20445;&#25345;&#32047;&#31215;&#20559;&#24046;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#20869;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#23545;&#26410;&#30693;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#65292;&#36951;&#25022;&#30028;&#22312;&#24378;&#20984;&#25110;&#28385;&#36275;Lipschitz&#26465;&#20214;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#20989;&#25968;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#26469;&#36873;&#25321;&#19968;&#20010;&#22238;&#28335;&#31383;&#21475;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#65292;&#21516;&#26102;&#23558;&#32047;&#31215;&#20559;&#24046;&#20445;&#25345;&#22312;&#19982;&#38543;&#26426;&#35823;&#24046;&#30456;&#23545;&#21487;&#25509;&#21463;&#30340;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#26410;&#30693;&#38750;&#31283;&#23450;&#24615;&#30340;&#36866;&#24212;&#24615;&#12290;&#24403;&#20154;&#21475;&#25439;&#22833;&#20989;&#25968;&#24378;&#20984;&#25110;&#20165;&#28385;&#36275;Lipschitz&#26465;&#20214;&#26102;&#65292;&#36951;&#25022;&#30028;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#65292;&#20165;&#21463;&#23545;&#25968;&#22240;&#23376;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26680;&#24515;&#26159;&#20004;&#20010;&#26032;&#39062;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#20989;&#25968;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#23558;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#20026;&#20934;&#31283;&#24577;&#29255;&#27573;&#30340;&#20998;&#21106;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a versatile framework for statistical learning in non-stationary environments. In each time period, our approach applies a stability principle to select a look-back window that maximizes the utilization of historical data while keeping the cumulative bias within an acceptable range relative to the stochastic error. Our theory showcases the adaptability of this approach to unknown non-stationarity. The regret bound is minimax optimal up to logarithmic factors when the population losses are strongly convex, or Lipschitz only. At the heart of our analysis lie two novel components: a measure of similarity between functions and a segmentation technique for dividing the non-stationary data sequence into quasi-stationary pieces.
&lt;/p&gt;</description></item><item><title>HetGPT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15318</link><description>&lt;p&gt;
HetGPT: &#21033;&#29992;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25552;&#31034;&#35843;&#25972;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks. (arXiv:2310.15318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15318
&lt;/p&gt;
&lt;p&gt;
HetGPT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#29616;&#20026;&#34920;&#31034;&#21644;&#20998;&#26512;Web&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20016;&#23500;&#20449;&#24687;&#30340;&#33258;&#28982;&#36873;&#25321;&#65292;&#20351;&#24471;&#22312;&#32447;&#39029;&#38754;&#20998;&#31867;&#21644;&#31038;&#20132;&#25512;&#33616;&#31561;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#8220;&#39044;&#35757;&#32451;&#65292;&#24494;&#35843;&#8221;&#33539;&#24335;&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#26631;&#35760;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#24448;&#24448;&#23384;&#22312;&#39044;&#35757;&#32451;&#30446;&#26631;&#20219;&#21153;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36825;&#31181;&#24046;&#36317;&#21487;&#33021;&#23548;&#33268;&#8220;&#36127;&#36716;&#31227;&#8221;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#30340;&#20852;&#36215;&#34920;&#26126;&#20102;&#23558;&#8220;&#39044;&#35757;&#32451;&#65292;&#25552;&#31034;&#8221;&#33539;&#24335;&#24212;&#29992;&#20110;&#22270;&#24418;&#30340;&#28508;&#21147;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#24418;&#25552;&#31034;&#25216;&#26415;&#38024;&#23545;&#30340;&#26159;&#21516;&#36136;&#22270;&#65292;&#24573;&#35270;&#20102;Web&#22270;&#30340;&#20869;&#22312;&#24322;&#26500;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HetGPT&#65292;
&lt;/p&gt;
&lt;p&gt;
Graphs have emerged as a natural choice to represent and analyze the intricate patterns and rich information of the Web, enabling applications such as online page classification and social recommendation. The prevailing "pre-train, fine-tune" paradigm has been widely adopted in graph machine learning tasks, particularly in scenarios with limited labeled nodes. However, this approach often exhibits a misalignment between the training objectives of pretext tasks and those of downstream tasks. This gap can result in the "negative transfer" problem, wherein the knowledge gained from pre-training adversely affects performance in the downstream tasks. The surge in prompt-based learning within Natural Language Processing (NLP) suggests the potential of adapting a "pre-train, prompt" paradigm to graphs as an alternative. However, existing graph prompting techniques are tailored to homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To bridge this gap, we propose HetGPT, a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.05207</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12289;&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#26469;&#25552;&#39640;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#30340;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction. (arXiv:2310.05207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914;&#20309;&#23558;&#22823;&#37327;&#30340;&#22312;&#37326;&#38750;&#26631;&#35760;&#38754;&#37096;&#22270;&#20687;&#24341;&#20837;&#30417;&#30563;&#24335;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#20013;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AU&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21516;&#26500;&#38754;&#37096;&#25552;&#21462;&#27169;&#22359;&#30340;&#21442;&#25968;&#65292;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#21516;&#26102;&#23398;&#20064;AU&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#20197;&#21450;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#29305;&#24449;&#23545;&#40784;&#26041;&#26696;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#22120;&#21644;&#25913;&#36827;&#30340;&#23545;&#27604;&#25439;&#22833;&#28155;&#21152;&#20102;&#22235;&#20010;&#39069;&#22806;&#30340;&#20013;&#38388;&#30417;&#30563;&#22120;&#26469;&#20419;&#36827;&#29305;&#24449;&#37325;&#24314;&#30340;&#36807;&#31243;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently how to introduce large amounts of unlabeled facial images in the wild into supervised Facial Action Unit (AU) detection frameworks has become a challenging problem. In this paper, we propose a new AU detection framework where multi-task learning is introduced to jointly learn AU domain separation and reconstruction and facial landmark detection by sharing the parameters of homostructural facial extraction modules. In addition, we propose a new feature alignment scheme based on contrastive learning by simple projectors and an improved contrastive loss, which adds four additional intermediate supervisors to promote the feature reconstruction process. Experimental results on two benchmarks demonstrate our superiority against the state-of-the-art methods for AU detection in the wild.
&lt;/p&gt;</description></item><item><title>"&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#26816;&#32034;&#22686;&#24378;&#21644;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21457;&#29616;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;LLM&#20351;&#29992;4K&#19978;&#19979;&#25991;&#31383;&#21475;&#21487;&#20197;&#21462;&#24471;&#19982;&#36890;&#36807;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#24494;&#35843;&#30340;LLM&#20351;&#29992;16K&#19978;&#19979;&#25991;&#31383;&#21475;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#37327;&#35201;&#23569;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#22914;&#20309;&#65292;&#26816;&#32034;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;"</title><link>http://arxiv.org/abs/2310.03025</link><description>&lt;p&gt;
"&#26816;&#32034;&#36935;&#19978;&#38271;&#31687;&#22823;&#35821;&#35328;&#27169;&#22411;"
&lt;/p&gt;
&lt;p&gt;
Retrieval meets Long Context Large Language Models. (arXiv:2310.03025v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03025
&lt;/p&gt;
&lt;p&gt;
"&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#26816;&#32034;&#22686;&#24378;&#21644;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21457;&#29616;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;LLM&#20351;&#29992;4K&#19978;&#19979;&#25991;&#31383;&#21475;&#21487;&#20197;&#21462;&#24471;&#19982;&#36890;&#36807;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#24494;&#35843;&#30340;LLM&#20351;&#29992;16K&#19978;&#19979;&#25991;&#31383;&#21475;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#37327;&#35201;&#23569;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#22914;&#20309;&#65292;&#26816;&#32034;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#26368;&#36817;&#65292;&#25193;&#23637;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#32780;&#23558;&#26816;&#32034;&#19982;LLM&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#23384;&#22312;&#22810;&#24180;&#12290;&#33258;&#28982;&#32780;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;&#26816;&#32034;&#22686;&#24378;&#19982;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#21738;&#20010;&#23545;&#19979;&#28216;&#20219;&#21153;&#26356;&#22909;&#65311;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#32467;&#21512;&#36215;&#26469;&#20197;&#20860;&#39038;&#21033;&#24330;&#21527;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;LLM&#65288;&#21363;&#19968;&#20010;&#31169;&#26377;&#30340;43B GPT&#21644;Llama2-70B&#65289;&#26469;&#30740;&#31350;&#36825;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#20063;&#35768;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#65292;LLM&#20351;&#29992;4K&#19978;&#19979;&#25991;&#31383;&#21475;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#26816;&#32034;&#22686;&#24378;&#22312;&#29983;&#25104;&#26102;&#21487;&#20197;&#36798;&#21040;&#19982;&#36890;&#36807;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#36827;&#34892;&#20301;&#32622;&#25554;&#20540;&#30340;&#24494;&#35843;LLM&#20351;&#29992;16K&#19978;&#19979;&#25991;&#31383;&#21475;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#37327;&#35201;&#23569;&#24471;&#22810;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#35770;&#20854;&#25193;&#23637;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#22914;&#20309;&#65292;&#26816;&#32034;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#26159;&#20855;&#26377;32K&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#26816;&#32034;&#22686;&#24378;Llama2-70B&#12290;"
&lt;/p&gt;
&lt;p&gt;
Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#21033;&#29992;&#20960;&#20309;&#32467;&#26500;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#20960;&#20309;&#65292;&#23558;&#32479;&#35745;&#20381;&#36182;&#21644;&#29305;&#24449;&#32479;&#19968;&#21040;&#21516;&#19968;&#31354;&#38388;&#20013;&#65292;&#24182;&#20351;&#29992;&#23884;&#22871;&#25216;&#26415;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#21464;&#37327;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.10140</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#30340;&#20960;&#20309;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Geometric Framework for Neural Feature Learning. (arXiv:2309.10140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#21033;&#29992;&#20960;&#20309;&#32467;&#26500;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#20960;&#20309;&#65292;&#23558;&#32479;&#35745;&#20381;&#36182;&#21644;&#29305;&#24449;&#32479;&#19968;&#21040;&#21516;&#19968;&#31354;&#38388;&#20013;&#65292;&#24182;&#20351;&#29992;&#23884;&#22871;&#25216;&#26415;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#21464;&#37327;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#23398;&#20064;&#31995;&#32479;&#35774;&#35745;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#24449;&#20960;&#20309;&#65292;&#23427;&#23558;&#32479;&#35745;&#20381;&#36182;&#21644;&#29305;&#24449;&#32479;&#19968;&#21040;&#21516;&#19968;&#20010;&#20855;&#26377;&#20960;&#20309;&#32467;&#26500;&#30340;&#20989;&#25968;&#31354;&#38388;&#20013;&#12290;&#36890;&#36807;&#24212;&#29992;&#29305;&#24449;&#20960;&#20309;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#23398;&#20064;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#35299;&#20915;&#30001;&#23398;&#20064;&#35774;&#32622;&#25351;&#23450;&#30340;&#20381;&#36182;&#32452;&#20214;&#30340;&#26368;&#20339;&#29305;&#24449;&#36817;&#20284;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#22871;&#25216;&#26415;&#26469;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#26368;&#20339;&#29305;&#24449;&#65292;&#36825;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#20248;&#21270;&#22120;&#12290;&#20026;&#20102;&#23637;&#31034;&#23884;&#22871;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#22810;&#21464;&#37327;&#23398;&#20064;&#38382;&#39064;&#65292;&#21253;&#25324;&#26465;&#20214;&#25512;&#29702;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#20339;&#29305;&#24449;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#19982;&#32463;&#20856;&#26041;&#27861;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel framework for learning system design based on neural feature extractors by exploiting geometric structures in feature spaces. First, we introduce the feature geometry, which unifies statistical dependence and features in the same functional space with geometric structures. By applying the feature geometry, we formulate each learning problem as solving the optimal feature approximation of the dependence component specified by the learning setting. We propose a nesting technique for designing learning algorithms to learn the optimal features from data samples, which can be applied to off-the-shelf network architectures and optimizers. To demonstrate the application of the nesting technique, we further discuss multivariate learning problems, including conditioned inference and multimodal learning, where we present the optimal features and reveal their connections to classical approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3&#22312;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#33647;&#29289;&#30340;SMILES&#34920;&#31034;&#21644;&#32454;&#32990;&#31995;&#30340;&#22522;&#22240;&#32452;&#31361;&#21464;&#29305;&#24449;&#23545;&#33647;&#29289;&#21453;&#24212;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#22312;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.10016</link><description>&lt;p&gt;
GPT-3&#29992;&#20110;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction. (arXiv:2309.10016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3&#22312;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#33647;&#29289;&#30340;SMILES&#34920;&#31034;&#21644;&#32454;&#32990;&#31995;&#30340;&#22522;&#22240;&#32452;&#31361;&#21464;&#29305;&#24449;&#23545;&#33647;&#29289;&#21453;&#24212;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#22312;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32467;&#26500;&#21270;&#30340;&#33647;&#29289;&#22522;&#22240;&#32452;&#25968;&#25454;&#65292;&#22312;&#20116;&#31181;&#32452;&#32455;&#31867;&#22411;&#20013;&#25506;&#31350;&#20102;GPT-3&#22312;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20998;&#21035;&#37319;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;&#24494;&#35843;&#33539;&#24335;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#33647;&#29289;&#30340;SMILES&#34920;&#31034;&#21644;&#32454;&#32990;&#31995;&#30340;&#22522;&#22240;&#32452;&#31361;&#21464;&#29305;&#24449;&#23545;&#33647;&#29289;&#21453;&#24212;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#26377;&#26395;&#20026;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigated the potential of GPT-3 for the anti-cancer drug sensitivity prediction task using structured pharmacogenomics data across five tissue types and evaluated its performance with zero-shot prompting and fine-tuning paradigms. The drug's smile representation and cell line's genomic mutation features were predictive of the drug response. The results from this study have the potential to pave the way for designing more efficient treatment protocols in precision oncology.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;PET&#22270;&#20687;&#37325;&#24314;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#23545;PET&#22270;&#20687;&#37325;&#24314;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#39640;&#26041;&#24046;&#30340;&#27850;&#26494;&#22122;&#22768;&#21644;&#24191;&#27867;&#30340;&#21160;&#24577;&#33539;&#22260;&#65292;&#23637;&#31034;&#20102;&#25913;&#36827;PET&#37325;&#24314;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.14190</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;PET&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Score-Based Generative Models for PET Image Reconstruction. (arXiv:2308.14190v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;PET&#22270;&#20687;&#37325;&#24314;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#23545;PET&#22270;&#20687;&#37325;&#24314;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#39640;&#26041;&#24046;&#30340;&#27850;&#26494;&#22122;&#22768;&#21644;&#24191;&#27867;&#30340;&#21160;&#24577;&#33539;&#22260;&#65292;&#23637;&#31034;&#20102;&#25913;&#36827;PET&#37325;&#24314;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#25110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#31561;&#21307;&#23398;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#39640;&#24230;&#26377;&#21069;&#26223;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;&#65288;PET&#65289;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#30456;&#23545;&#26410;&#30693;&#12290;PET&#22270;&#20687;&#37325;&#24314;&#28041;&#21450;&#22810;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#39640;&#26041;&#24046;&#30340;&#27850;&#26494;&#22122;&#22768;&#21644;&#24191;&#27867;&#30340;&#21160;&#24577;&#33539;&#22260;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#36866;&#29992;&#20110;PET&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#29305;&#23450;&#36866;&#24212;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;2D&#21644;3D PET&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#20351;&#29992;&#30913;&#20849;&#25391;&#22270;&#20687;&#36827;&#34892;&#24341;&#23548;&#37325;&#24314;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#27809;&#26377;&#30149;&#21464;&#30340;&#24739;&#32773;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;2D&#21644;3D&#30340;$\textit{in-silico}$&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27809;&#26377;&#30149;&#21464;&#30340;&#25968;&#25454;&#20197;&#21450;&#24102;&#26377;&#30149;&#21464;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;PET&#37325;&#24314;&#25913;&#36827;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models have demonstrated highly promising results for medical image reconstruction tasks in magnetic resonance imaging or computed tomography. However, their application to Positron Emission Tomography (PET) is still largely unexplored. PET image reconstruction involves a variety of challenges, including Poisson noise with high variance and a wide dynamic range. To address these challenges, we propose several PET-specific adaptations of score-based generative models. The proposed framework is developed for both 2D and 3D PET. In addition, we provide an extension to guided reconstruction using magnetic resonance images. We validate the approach through extensive 2D and 3D $\textit{in-silico}$ experiments with a model trained on patient-realistic data without lesions, and evaluate on data without lesions as well as out-of-distribution data with lesions. This demonstrates the proposed method's robustness and significant potential for improved PET reconstruction.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#34920;&#31034;&#25216;&#26415;&#23545;TCAD&#22120;&#20214;&#27169;&#25311;&#20013;&#30340;&#21322;&#23548;&#20307;&#22120;&#20214;&#36827;&#34892;&#32534;&#30721;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#36890;&#29992;&#32534;&#30721;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22312;&#35774;&#22791;&#32423;&#19978;&#24212;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11624</link><description>&lt;p&gt;
&#29992;&#36890;&#29992;&#35774;&#22791;&#32534;&#30721;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#38761;&#26032;TCAD&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks. (arXiv:2308.11624v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#34920;&#31034;&#25216;&#26415;&#23545;TCAD&#22120;&#20214;&#27169;&#25311;&#20013;&#30340;&#21322;&#23548;&#20307;&#22120;&#20214;&#36827;&#34892;&#32534;&#30721;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#36890;&#29992;&#32534;&#30721;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22312;&#35774;&#22791;&#32423;&#19978;&#24212;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#34920;&#31034;&#25216;&#26415;&#26469;&#23545;TCAD&#22120;&#20214;&#27169;&#25311;&#20013;&#30340;&#21322;&#23548;&#20307;&#22120;&#20214;&#36827;&#34892;&#32534;&#30721;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#36890;&#29992;&#32534;&#30721;&#26041;&#26696;&#65292;&#19981;&#20165;&#32771;&#34385;&#20102;&#26448;&#26009;&#32423;&#21644;&#22120;&#20214;&#32423;&#23884;&#20837;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31354;&#38388;&#20851;&#31995;&#30340;&#23884;&#20837;&#65292;&#21463;&#26377;&#38480;&#20803;&#32593;&#26684;&#20013;&#24120;&#29992;&#30340;&#25554;&#20540;&#25805;&#20316;&#21551;&#21457;&#32780;&#26469;&#12290;&#21033;&#29992;&#22120;&#20214;&#27169;&#25311;&#30340;&#36890;&#29992;&#29289;&#29702;&#23450;&#24459;&#36827;&#34892;&#20840;&#38754;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65292;&#21253;&#25324;&#22522;&#20110;&#27850;&#26494;&#20223;&#30495;&#30340;&#26367;&#20195;&#21644;&#22522;&#20110;&#28418;&#31227;&#25193;&#25955;&#27169;&#22411;&#30340;&#30005;&#27969;-&#30005;&#21387;&#65288;IV&#65289;&#39044;&#27979;&#12290;&#36825;&#20004;&#32773;&#37117;&#26159;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;&#31216;&#20026;RelGAT&#65289;&#23454;&#29616;&#30340;&#12290;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;Sentaurus TCAD&#22120;&#20214;&#27169;&#25311;&#22120;&#30340;&#35814;&#32454;&#25216;&#26415;&#32454;&#33410;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#22312;&#35774;&#22791;&#32423;&#19978;&#37319;&#29992;&#25552;&#20986;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
An innovative methodology that leverages artificial intelligence (AI) and graph representation for semiconductor device encoding in TCAD device simulation is proposed. A graph-based universal encoding scheme is presented that not only considers material-level and device-level embeddings, but also introduces a novel spatial relationship embedding inspired by interpolation operations typically used in finite element meshing. Universal physical laws from device simulations are leveraged for comprehensive data-driven modeling, which encompasses surrogate Poisson emulation and current-voltage (IV) prediction based on drift-diffusion model. Both are achieved using a novel graph attention network, referred to as RelGAT. Comprehensive technical details based on the device simulator Sentaurus TCAD are presented, empowering researchers to adopt the proposed AI-driven Electronic Design Automation (EDA) solution at the device level.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25913;&#36827;&#20102;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#20998;&#31867;&#26641;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#20943;&#23569;&#23376;&#38382;&#39064;&#25968;&#37327;&#12289;&#20351;&#29992;&#25968;&#25454;&#20381;&#36182;&#32422;&#26463;&#20316;&#20026;&#21106;&#24179;&#38754;&#20197;&#21450;&#29983;&#25104;&#36829;&#21453;&#32422;&#26463;&#30340;&#25968;&#25454;&#28857;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#21487;&#20280;&#32553;&#24615;&#24182;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.11477</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#20998;&#31867;&#26641;
&lt;/p&gt;
&lt;p&gt;
Revisiting column-generation-based matheuristic for learning classification trees. (arXiv:2308.11477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25913;&#36827;&#20102;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#20998;&#31867;&#26641;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#20943;&#23569;&#23376;&#38382;&#39064;&#25968;&#37327;&#12289;&#20351;&#29992;&#25968;&#25454;&#20381;&#36182;&#32422;&#26463;&#20316;&#20026;&#21106;&#24179;&#38754;&#20197;&#21450;&#29983;&#25104;&#36829;&#21453;&#32422;&#26463;&#30340;&#25968;&#25454;&#28857;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#21487;&#20280;&#32553;&#24615;&#24182;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#35299;&#20915;&#20998;&#31867;&#38382;&#39064;&#30340;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#20915;&#31574;&#26641;&#24555;&#36895;&#20294;&#29983;&#25104;&#30340;&#26641;&#22312;&#20934;&#30830;&#24615;&#19978;&#19981;&#22815;&#20248;&#21270;&#12290;&#25991;&#29486;&#20013;&#20854;&#20182;&#31163;&#25955;&#20248;&#21270;&#27169;&#22411;&#35299;&#20915;&#20102;&#26368;&#20248;&#24615;&#38382;&#39064;&#20294;&#21482;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;firat2020column&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#23398;&#20064;&#20915;&#31574;&#26641;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#21487;&#20280;&#32553;&#24615;&#65292;&#24182;&#21487;&#20197;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#23545;&#35813;&#21015;&#29983;&#25104;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#23376;&#38382;&#39064;&#27169;&#22411;&#20197;&#26174;&#33879;&#20943;&#23569;&#22810;&#31867;&#20998;&#31867;&#23454;&#20363;&#20013;&#30340;&#23376;&#38382;&#39064;&#25968;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20027;&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#20381;&#36182;&#32422;&#26463;&#26159;&#34164;&#21547;&#30340;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#21106;&#24179;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20998;&#31163;&#27169;&#22411;&#26469;&#29983;&#25104;&#32447;&#24615;&#35268;&#21010;&#26494;&#24347;&#35299;&#36829;&#21453;&#20854;&#23545;&#24212;&#30340;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are highly interpretable models for solving classification problems in machine learning (ML). The standard ML algorithms for training decision trees are fast but generate suboptimal trees in terms of accuracy. Other discrete optimization models in the literature address the optimality problem but only work well on relatively small datasets. \cite{firat2020column} proposed a column-generation-based heuristic approach for learning decision trees. This approach improves scalability and can work with large datasets. In this paper, we describe improvements to this column generation approach. First, we modify the subproblem model to significantly reduce the number of subproblems in multiclass classification instances. Next, we show that the data-dependent constraints in the master problem are implied, and use them as cutting planes. Furthermore, we describe a separation model to generate data points for which the linear programming relaxation solution violates their correspond
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#28151;&#21512;&#31995;&#32479;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21644;&#31526;&#21495;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26816;&#26597;&#30693;&#35782;&#24211;&#26469;&#30830;&#23450;&#23427;&#20204;&#22312;&#20419;&#36827;&#25104;&#21151;&#23398;&#20064;&#26041;&#38754;&#30340;&#25928;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#35768;&#22810;&#30693;&#35782;&#24211;&#28385;&#36275;&#21028;&#25454;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#26080;&#27861;&#28385;&#36275;&#30340;&#30693;&#35782;&#24211;&#12290;</title><link>http://arxiv.org/abs/2308.10487</link><description>&lt;p&gt;
&#29992;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#35299;&#23494;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#20013;&#30340;&#21407;&#22987;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Deciphering Raw Data in Neuro-Symbolic Learning with Provable Guarantees. (arXiv:2308.10487v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#28151;&#21512;&#31995;&#32479;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21644;&#31526;&#21495;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26816;&#26597;&#30693;&#35782;&#24211;&#26469;&#30830;&#23450;&#23427;&#20204;&#22312;&#20419;&#36827;&#25104;&#21151;&#23398;&#20064;&#26041;&#38754;&#30340;&#25928;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#35768;&#22810;&#30693;&#35782;&#24211;&#28385;&#36275;&#21028;&#25454;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#26080;&#27861;&#28385;&#36275;&#30340;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#28151;&#21512;&#31995;&#32479;&#22312;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#31526;&#21495;&#25512;&#29702;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#65292;&#20854;&#20013;&#24863;&#30693;&#27169;&#22411;&#36890;&#36807;&#36923;&#36753;&#25512;&#29702;&#20174;&#31526;&#21495;&#30693;&#35782;&#24211;&#20013;&#25512;&#26029;&#20986;&#20449;&#24687;&#12290;&#23613;&#31649;&#26377;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#28151;&#21512;&#31995;&#32479;&#33021;&#22815;&#23398;&#20064;&#20934;&#30830;&#30340;&#24863;&#30693;&#27169;&#22411;&#65292;&#20294;&#23545;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20026;&#20160;&#20040;&#28151;&#21512;&#31995;&#32479;&#33021;&#22815;&#25104;&#21151;&#23436;&#25104;&#29305;&#23450;&#20219;&#21153;&#20197;&#21450;&#22312;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#19979;&#21487;&#33021;&#22833;&#36133;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#65292;&#29992;&#20110;&#25551;&#36848;&#26469;&#33258;&#30693;&#35782;&#24211;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21028;&#25454;&#26469;&#30830;&#23450;&#30693;&#35782;&#22312;&#20419;&#36827;&#25104;&#21151;&#23398;&#20064;&#26041;&#38754;&#30340;&#25928;&#21147;&#12290;&#36825;&#26159;&#39318;&#27425;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#27491;&#22312;&#30740;&#31350;&#30340;&#30693;&#35782;&#24211;&#26469;&#22238;&#31572;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#35768;&#22810;&#30693;&#35782;&#24211;&#28385;&#36275;&#21028;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#23398;&#20064;&#65292;&#32780;&#26377;&#20123;&#21017;&#26080;&#27861;&#28385;&#36275;&#65292;&#34920;&#26126;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic hybrid systems are promising for integrating machine learning and symbolic reasoning, where perception models are facilitated with information inferred from a symbolic knowledge base through logical reasoning. Despite empirical evidence showing the ability of hybrid systems to learn accurate perception models, the theoretical understanding of learnability is still lacking. Hence, it remains unclear why a hybrid system succeeds for a specific task and when it may fail given a different knowledge base. In this paper, we introduce a novel way of characterising supervision signals from a knowledge base, and establish a criterion for determining the knowledge's efficacy in facilitating successful learning. This, for the first time, allows us to address the two questions above by inspecting the knowledge base under investigation. Our analysis suggests that many knowledge bases satisfy the criterion, thus enabling effective learning, while some fail to satisfy it, indicating po
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03212</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#22478;&#24066;&#21306;&#22495;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings. (arXiv:2307.03212v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03212
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#21306;&#22495;&#23884;&#20837;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#39640;&#24230;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#22478;&#24066;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#24615;&#36136;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#65292;&#20197;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#32780;&#19981;&#21463;&#21018;&#24615;&#37051;&#22495;&#26465;&#20214;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19987;&#27880;&#20110;&#20174;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#34920;&#31034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#31227;&#21160;&#27969;&#27169;&#24335;&#12289;POI&#35821;&#20041;&#21644;&#31614;&#21040;&#21160;&#24577;&#20013;&#25429;&#25417;&#22810;&#35270;&#35282;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#20840;&#23616;&#22270;&#27880;&#24847;&#32593;&#32476;&#26469;&#23398;&#20064;&#22270;&#20013;&#20219;&#24847;&#20004;&#20010;&#39030;&#28857;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#20840;&#38754;&#32771;&#34385;&#21644;&#20849;&#20139;&#22810;&#20010;&#35270;&#35282;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#34701;&#21512;&#27169;&#22359;&#65292;&#21033;&#29992;&#22806;&#37096;&#27880;&#24847;&#21147;&#23398;&#20064;&#26435;&#37325;&#26469;&#34701;&#21512;&#22810;&#35270;&#35282;&#23884;&#20837;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban region embedding is an important and yet highly challenging issue due to the complexity and constantly changing nature of urban data. To address the challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER) to capture multi-view dependencies and learn expressive representations of urban regions without the constraints of rigid neighbourhood region conditions. Our model focus on learn urban region representation from multi-source urban data. First, we capture the multi-view correlations from mobility flow patterns, POI semantics and check-in dynamics. Then, we adopt global graph attention networks to learn similarity of any two vertices in graphs. To comprehensively consider and share features of multiple views, a two-stage fusion module is further proposed to learn weights with external attention to fuse multi-view embeddings. Extensive experiments for two downstream tasks on real-world datasets demonstrate that our model outperforms state-of-the-art methods
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#20309;&#26102;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#32423;&#32852;&#24310;&#36831;&#21487;&#33021;&#22833;&#36133;&#65292;&#20197;&#21450;&#20309;&#26102;&#22791;&#36873;&#30340;&#24310;&#36831;&#31574;&#30053;&#21487;&#33021;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20107;&#21518;&#24310;&#36831;&#26426;&#21046;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02764</link><description>&lt;p&gt;
&#20309;&#26102;&#20351;&#29992;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#32423;&#32852;&#24310;&#36831;&#36275;&#22815;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Does Confidence-Based Cascade Deferral Suffice?. (arXiv:2307.02764v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#20309;&#26102;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#32423;&#32852;&#24310;&#36831;&#21487;&#33021;&#22833;&#36133;&#65292;&#20197;&#21450;&#20309;&#26102;&#22791;&#36873;&#30340;&#24310;&#36831;&#31574;&#30053;&#21487;&#33021;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20107;&#21518;&#24310;&#36831;&#26426;&#21046;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32423;&#32852;&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#23454;&#29616;&#36866;&#24212;&#24615;&#22320;&#22312;&#26679;&#26412;&#20043;&#38388;&#21464;&#21270;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;&#20854;&#20013;&#25353;&#39034;&#24207;&#35843;&#29992;&#19968;&#31995;&#21015;&#20998;&#31867;&#22120;&#12290;&#24310;&#36831;&#35268;&#21017;&#30830;&#23450;&#26159;&#21542;&#35843;&#29992;&#24207;&#21015;&#20013;&#30340;&#19979;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#25110;&#32773;&#32456;&#27490;&#39044;&#27979;&#12290;&#19968;&#31181;&#31616;&#21333;&#30340;&#24310;&#36831;&#35268;&#21017;&#21033;&#29992;&#24403;&#21069;&#20998;&#31867;&#22120;&#30340;&#32622;&#20449;&#24230;&#65292;&#20363;&#22914;&#22522;&#20110;&#26368;&#22823;&#39044;&#27979;&#30340;softmax&#27010;&#29575;&#12290;&#23613;&#31649;&#23545;&#32423;&#32852;&#32467;&#26500;&#19981;&#25935;&#24863;&#8212;&#8212;&#20363;&#22914;&#19981;&#24314;&#27169;&#19979;&#28216;&#27169;&#22411;&#30340;&#38169;&#35823;&#8212;&#8212;&#20294;&#36825;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#24310;&#36831;&#32463;&#24120;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#24310;&#36831;&#21487;&#33021;&#22833;&#36133;&#30340;&#26465;&#20214;&#65292;&#20197;&#21450;&#20309;&#26102;&#22791;&#36873;&#30340;&#24310;&#36831;&#31574;&#30053;&#21487;&#33021;&#26356;&#22909;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#26368;&#20248;&#24310;&#36831;&#35268;&#21017;&#36827;&#34892;&#20102;&#29702;&#35770;&#34920;&#24449;&#65292;&#31934;&#30830;&#22320;&#25551;&#36848;&#20102;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#24310;&#36831;&#21487;&#33021;&#21463;&#21040;&#24433;&#21709;&#30340;&#35774;&#32622;&#12290;&#28982;&#21518;&#25105;&#20204;&#30740;&#31350;&#20102;&#20107;&#21518;&#24310;&#36831;&#26426;&#21046;&#65292;&#24182;&#39564;&#35777;&#23427;&#20204;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cascades are a classical strategy to enable inference cost to vary adaptively across samples, wherein a sequence of classifiers are invoked in turn. A deferral rule determines whether to invoke the next classifier in the sequence, or to terminate prediction. One simple deferral rule employs the confidence of the current classifier, e.g., based on the maximum predicted softmax probability. Despite being oblivious to the structure of the cascade -- e.g., not modelling the errors of downstream models -- such confidence-based deferral often works remarkably well in practice. In this paper, we seek to better understand the conditions under which confidence-based deferral may fail, and when alternate deferral strategies can perform better. We first present a theoretical characterisation of the optimal deferral rule, which precisely characterises settings under which confidence-based deferral may suffer. We then study post-hoc deferral mechanisms, and demonstrate they can significantly improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32423;&#32852;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CasTGAN&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#36924;&#30495;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#36755;&#20986;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#32423;&#32852;&#26550;&#26500;&#65292;&#20854;&#20013;&#19987;&#38376;&#30340;&#29983;&#25104;&#22120;&#23545;&#27599;&#20010;&#29305;&#24449;&#36827;&#34892;&#37319;&#26679;&#65292;&#20351;&#24471;&#21512;&#25104;&#36755;&#20986;&#26356;&#33021;&#20195;&#34920;&#30495;&#23454;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CasTGAN&#33021;&#22815;&#20135;&#29983;&#26356;&#30495;&#23454;&#26377;&#25928;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.00384</link><description>&lt;p&gt;
CasTGAN: &#29992;&#20110;&#36924;&#30495;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#30340;&#32423;&#32852;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CasTGAN: Cascaded Generative Adversarial Network for Realistic Tabular Data Synthesis. (arXiv:2307.00384v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32423;&#32852;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CasTGAN&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#36924;&#30495;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#36755;&#20986;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#32423;&#32852;&#26550;&#26500;&#65292;&#20854;&#20013;&#19987;&#38376;&#30340;&#29983;&#25104;&#22120;&#23545;&#27599;&#20010;&#29305;&#24449;&#36827;&#34892;&#37319;&#26679;&#65292;&#20351;&#24471;&#21512;&#25104;&#36755;&#20986;&#26356;&#33021;&#20195;&#34920;&#30495;&#23454;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CasTGAN&#33021;&#22815;&#20135;&#29983;&#26356;&#30495;&#23454;&#26377;&#25928;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22240;&#20854;&#22312;&#29983;&#25104;&#21487;&#29992;&#20110;&#22810;&#31181;&#30446;&#30340;&#30340;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;GAN&#24050;&#32463;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#22797;&#21046;&#21407;&#22987;&#25968;&#25454;&#38598;&#21160;&#24577;&#30340;&#21512;&#25104;&#25968;&#25454;&#26679;&#26412;&#65292;&#20294;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#22312;&#30340;&#38544;&#31169;&#38382;&#39064;&#20173;&#28982;&#26159;&#19981;&#23481;&#24573;&#35270;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32423;&#32852;&#34920;&#26684;GAN&#26694;&#26550;&#65288;CasTGAN&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#36924;&#30495;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#36755;&#20986;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#20010;&#19978;&#19979;&#25991;&#20013;&#65292;&#26377;&#25928;&#24615;&#26159;&#25351;&#22312;&#30495;&#23454;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36890;&#24120;&#34987;&#20256;&#32479;&#30340;&#29983;&#25104;&#27169;&#22411;&#25152;&#35823;&#35299;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#37319;&#29992;&#32423;&#32852;&#26550;&#26500;&#65292;&#20854;&#20013;&#19987;&#38376;&#30340;&#29983;&#25104;&#22120;&#23545;&#27599;&#20010;&#29305;&#24449;&#36827;&#34892;&#37319;&#26679;&#65292;&#20351;&#24471;&#21512;&#25104;&#36755;&#20986;&#26356;&#33021;&#20195;&#34920;&#30495;&#23454;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CasTGAN&#33021;&#22815;&#20135;&#29983;&#26356;&#30495;&#23454;&#26377;&#25928;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) have drawn considerable attention in recent years for their proven capability in generating synthetic data which can be utilised for multiple purposes. While GANs have demonstrated tremendous successes in producing synthetic data samples that replicate the dynamics of the original datasets, the validity of the synthetic data and the underlying privacy concerns represent major challenges which are not sufficiently addressed. In this work, we design a cascaded tabular GAN framework (CasTGAN) for generating realistic tabular data with a specific focus on the validity of the output. In this context, validity refers to the the dependency between features that can be found in the real data, but is typically misrepresented by traditional generative models. Our key idea entails that employing a cascaded architecture in which a dedicated generator samples each feature, the synthetic output becomes more representative of the real data. Our experimental resu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23398;&#20064;Koopman&#31639;&#23376;&#12290; FlowDMD&#31639;&#27861;&#21033;&#29992;&#32806;&#21512;&#27969;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#65292;&#23398;&#20064;Koopman&#31639;&#23376;&#30340;&#19981;&#21464;&#23376;&#31354;&#38388;&#65292;&#24182;&#20934;&#30830;&#37325;&#26500;&#29366;&#24577;&#21464;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17396</link><description>&lt;p&gt;
&#29992;&#20110;Koopman&#31639;&#23376;&#23398;&#20064;&#30340;&#29289;&#29702;&#20449;&#24687;&#21453;&#36716;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Physics-informed invertible neural network for the Koopman operator learning. (arXiv:2306.17396v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23398;&#20064;Koopman&#31639;&#23376;&#12290; FlowDMD&#31639;&#27861;&#21033;&#29992;&#32806;&#21512;&#27969;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#65292;&#23398;&#20064;Koopman&#31639;&#23376;&#30340;&#19981;&#21464;&#23376;&#31354;&#38388;&#65292;&#24182;&#20934;&#30830;&#37325;&#26500;&#29366;&#24577;&#21464;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Koopman&#31639;&#23376;&#29702;&#35770;&#20013;&#65292;&#36890;&#36807;&#19968;&#32452;&#21487;&#35266;&#27979;&#20989;&#25968;&#65292;&#23558;&#19968;&#20010;&#26377;&#38480;&#32500;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#36716;&#21270;&#20026;&#19968;&#20010;&#26080;&#31351;&#20294;&#32447;&#24615;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#20808;&#21069;&#30693;&#35782;&#25163;&#21160;&#36873;&#25321;&#33021;&#22815;&#35206;&#30422;Koopman&#31639;&#23376;&#19981;&#21464;&#23376;&#31354;&#38388;&#30340;&#21487;&#35266;&#27979;&#20989;&#25968;&#26159;&#20302;&#25928;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#24213;&#23618;&#31995;&#32479;&#20960;&#20046;&#27809;&#26377;&#20449;&#24687;&#25110;&#27809;&#26377;&#20219;&#20309;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#21487;&#35266;&#27979;&#20989;&#25968;&#21487;&#36870;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#23548;&#33268;&#32467;&#26524;&#19981;&#20934;&#30830;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25152;&#35859;&#30340;FlowDMD&#65292;&#21363;&#22522;&#20110;&#27969;&#30340;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65292;&#21033;&#29992;&#32806;&#21512;&#27969;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;CF-INN&#65289;&#26694;&#26550;&#12290;FlowDMD&#21033;&#29992;CF-INN&#30340;&#20869;&#22312;&#21487;&#36870;&#29305;&#24615;&#65292;&#23398;&#20064;Koopman&#31639;&#23376;&#30340;&#19981;&#21464;&#23376;&#31354;&#38388;&#65292;&#24182;&#20934;&#30830;&#37325;&#26500;&#29366;&#24577;&#21464;&#37327;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30456;&#27604;&#24403;&#21069;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Koopman operator theory, a finite-dimensional nonlinear system is transformed into an infinite but linear system using a set of observable functions. However, manually selecting observable functions that span the invariant subspace of the Koopman operator based on prior knowledge is inefficient and challenging, particularly when little or no information is available about the underlying systems. Furthermore, current methodologies tend to disregard the importance of the invertibility of observable functions, which leads to inaccurate results. To address these challenges, we propose the so-called FlowDMD, a Flow-based Dynamic Mode Decomposition that utilizes the Coupling Flow Invertible Neural Network (CF-INN) framework. FlowDMD leverages the intrinsically invertible characteristics of the CF-INN to learn the invariant subspaces of the Koopman operator and accurately reconstruct state variables. Numerical experiments demonstrate the superior performance of our algorithm compared to st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#20445;&#38505;&#19994;&#30456;&#20851;&#27010;&#24565;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#32852;&#31995;&#36215;&#26469;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#31361;&#20986;&#20102;&#34987;&#24573;&#35270;&#30340;&#36131;&#20219;&#21644;&#38598;&#21512;&#19982;&#20010;&#20307;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.14624</link><description>&lt;p&gt;
&#20174;&#20445;&#38505;&#19994;&#30340;&#32463;&#39564;&#23545;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#30340;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
Insights From Insurance for Fair Machine Learning. (arXiv:2306.14624v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#20445;&#38505;&#19994;&#30456;&#20851;&#27010;&#24565;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#32852;&#31995;&#36215;&#26469;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#31361;&#20986;&#20102;&#34987;&#24573;&#35270;&#30340;&#36131;&#20219;&#21644;&#38598;&#21512;&#19982;&#20010;&#20307;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#20445;&#38505;&#19994;&#21487;&#20197;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#31038;&#20250;&#29615;&#22659;&#30340;&#31867;&#27604;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#23398;&#20064;&#23398;&#32773;&#33021;&#22815;&#20174;&#20016;&#23500;&#32780;&#36328;&#23398;&#31185;&#30340;&#20445;&#38505;&#25991;&#29486;&#20013;&#33719;&#24471;&#21551;&#31034;&#12290;&#36890;&#36807;&#36861;&#36394;&#20445;&#38505;&#20013;&#19981;&#30830;&#23450;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#23558;&#20445;&#38505;&#20844;&#24179;&#35266;&#24565;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#30456;&#20851;&#35266;&#24565;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#26725;&#26753;&#26469;&#38382;&#39064;&#21270;&#26657;&#20934;&#20844;&#24179;&#24615;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#20004;&#20010;&#20027;&#39064;&#65306;&#36131;&#20219;&#21644;&#38598;&#21512;&#19982;&#20010;&#20307;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We argue that insurance can act as an analogon for the social situatedness of machine learning systems, hence allowing machine learning scholars to take insights from the rich and interdisciplinary insurance literature. Tracing the interaction of uncertainty, fairness and responsibility in insurance provides a fresh perspective on fairness in machine learning. We link insurance fairness conceptions to their machine learning relatives, and use this bridge to problematize fairness as calibration. In this process, we bring to the forefront two themes that have been largely overlooked in the machine learning literature: responsibility and aggregate-individual tensions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;QH9&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#20998;&#23376;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#22522;&#20934;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2306.09549</link><description>&lt;p&gt;
QH9&#65306;QM9&#20998;&#23376;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules. (arXiv:2306.09549v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;QH9&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#20998;&#23376;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#22522;&#20934;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#21152;&#36895;&#30005;&#23376;&#32467;&#26500;&#39044;&#27979;&#65292;&#20316;&#20026;&#31532;&#19968;&#24615;&#21407;&#29702;&#35745;&#31639;&#26041;&#27861;&#65288;&#22914;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#65289;&#30340;&#26367;&#20195;&#21697;&#12290;&#34429;&#28982;&#35768;&#22810;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#21270;&#23398;&#24615;&#36136;&#21644;&#21407;&#23376;&#21147;&#65292;&#20294;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#39044;&#27979;&#21704;&#23494;&#39039;&#30697;&#38453;&#30340;&#33021;&#21147;&#26159;&#38750;&#24120;&#37325;&#35201;&#21644;&#22522;&#26412;&#30340;&#29289;&#29702;&#37327;&#65292;&#23427;&#30830;&#23450;&#20102;&#29289;&#29702;&#31995;&#32479;&#21644;&#21270;&#23398;&#24615;&#36136;&#30340;&#37327;&#23376;&#29366;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;QH9&#65292;&#22522;&#20110;QM9&#25968;&#25454;&#38598;&#20026;2,399&#20010;&#20998;&#23376;&#21160;&#21147;&#23398;&#36712;&#36857;&#21644;130,831&#20010;&#31283;&#23450;&#20998;&#23376;&#20960;&#20309;&#24418;&#24577;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#21508;&#31181;&#20998;&#23376;&#30340;&#22522;&#20934;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;QH9&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27169;&#22411;&#37117;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised machine learning approaches have been increasingly used in accelerating electronic structure prediction as surrogates of first-principle computational methods, such as density functional theory (DFT). While numerous quantum chemistry datasets focus on chemical properties and atomic forces, the ability to achieve accurate and efficient prediction of the Hamiltonian matrix is highly desired, as it is the most important and fundamental physical quantity that determines the quantum states of physical systems and chemical properties. In this work, we generate a new Quantum Hamiltonian dataset, named as QH9, to provide precise Hamiltonian matrices for 2,399 molecular dynamics trajectories and 130,831 stable molecular geometries, based on the QM9 dataset. By designing benchmark tasks with various molecules, we show that current machine learning models have the capacity to predict Hamiltonian matrices for arbitrary molecules. Both the QH9 dataset and the baseline models are provided
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27700;&#19979;&#29615;&#22659;&#19979;&#65292;&#20351;&#29992;EfficientDet&#31561;&#27169;&#22411;&#25552;&#39640;&#27700;&#19979;&#29289;&#20307;&#26816;&#27979;&#30340;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#22810;&#20010;&#27169;&#22411;&#30340;&#31934;&#24230;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21457;&#29616;&#25928;&#29575;&#26356;&#39640;&#30340;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#24212;&#23545;&#27700;&#19979;&#29289;&#20307;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.06075</link><description>&lt;p&gt;
DeepSeaNet: &#20351;&#29992;EfficientDet&#25552;&#39640;&#27700;&#19979;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
DeepSeaNet: Improving Underwater Object Detection using EfficientDet. (arXiv:2306.06075v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27700;&#19979;&#29615;&#22659;&#19979;&#65292;&#20351;&#29992;EfficientDet&#31561;&#27169;&#22411;&#25552;&#39640;&#27700;&#19979;&#29289;&#20307;&#26816;&#27979;&#30340;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#22810;&#20010;&#27169;&#22411;&#30340;&#31934;&#24230;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21457;&#29616;&#25928;&#29575;&#26356;&#39640;&#30340;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#24212;&#23545;&#27700;&#19979;&#29289;&#20307;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#21160;&#29289;&#21644;&#28145;&#28023;&#29289;&#20307;&#22312;&#20445;&#25252;&#27700;&#29983;&#29983;&#29289;&#30340;&#23433;&#20840;&#26102;&#24456;&#38590;&#35782;&#21035;&#21644;&#30417;&#27979;&#12290;&#24403;&#27700;&#20013;&#21547;&#26377;&#30416;&#21644;&#39063;&#31890;&#29366;&#26434;&#36136;&#26102;&#65292;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#20005;&#23803;&#12290;&#22312;&#36825;&#31181;&#22825;&#28982;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#65292;&#20256;&#32479;&#30340;CNN&#26041;&#27861;&#24320;&#22987;&#22833;&#36133;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#26412;&#39033;&#30446;&#21253;&#25324;&#22312;&#29616;&#26377;&#30340;&#24102;&#27880;&#37322;&#30340;&#27700;&#19979;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#8220;Brackish-Dataset&#8221;&#65289;&#19978;&#23454;&#26045;&#21644;&#35780;&#20272;&#21508;&#31181;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#21253;&#25324;EfficientDet&#12289;YOLOv5&#12289;YOLOv8&#21644;Detectron2&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#22312;&#35270;&#37326;&#26377;&#38480;&#30340;&#19979;&#21033;&#22982;&#24343;&#32422;&#23572;&#30331;&#27700;&#20013;&#25429;&#25417;&#21040;&#30340;&#40060;&#31867;&#12289;&#34809;&#12289;&#28023;&#26143;&#21644;&#20854;&#20182;&#27700;&#29983;&#21160;&#29289;&#30340;&#26631;&#27880;&#22270;&#20687;&#24207;&#21015;&#12290;&#26412;&#30740;&#31350;&#39033;&#30446;&#30340;&#30446;&#30340;&#26159;&#30740;&#31350;&#36739;&#26032;&#27169;&#22411;&#22312;&#21516;&#26679;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#31934;&#24230;&#21644;&#25512;&#29702;&#26102;&#38388;&#23558;&#23427;&#20204;&#19982;&#20808;&#21069;&#32467;&#26524;&#36827;&#34892;&#23545;&#27604;&#12290;&#39318;&#20808;&#65292;&#25105;&#27604;&#36739;&#20102;YOLOv3&#65288;31.10%&#24179;&#22343;&#31934;&#24230;&#22343;&#20540;&#65288;mAP&#65289;&#65289;&#12289;YOLOv4&#65288;83.72% mAP&#65289;&#12289;YOLOv5&#65288;97.6%&#65289;&#12289;YOLOv8&#65288;98.20%&#65289;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Marine animals and deep underwater objects are difficult to recognize and monitor for safety of aquatic life. There is an increasing challenge when the water is saline with granular particles and impurities. In such natural adversarial environment, traditional approaches like CNN start to fail and are expensive to compute. This project involves implementing and evaluating various object detection models, including EfficientDet, YOLOv5, YOLOv8, and Detectron2, on an existing annotated underwater dataset, called the Brackish-Dataset. The dataset comprises annotated image sequences of fish, crabs, starfish, and other aquatic animals captured in Limfjorden water with limited visibility. The aim of this research project is to study the efficiency of newer models on the same dataset and contrast them with the previous results based on accuracy and inference time. Firstly, I compare the results of YOLOv3 (31.10% mean Average Precision (mAP)), YOLOv4 (83.72% mAP), YOLOv5 (97.6%), YOLOv8 (98.20
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#26641;&#24418;&#31995;&#32479;&#25506;&#32034;&#21644;&#25512;&#26029;&#65292;&#29992;&#20110;&#26377;&#26681;&#21644;&#26080;&#26681;&#26641;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20339;&#26041;&#27861;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#25928;&#26524;&#65292;&#21487;&#29992;&#20110;&#21152;&#36895;&#29983;&#21629;&#31185;&#23398;&#30340;&#26032;&#36827;&#21270;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05739</link><description>&lt;p&gt;
&#36291;&#36801;&#20110;&#26641;&#31354;&#38388;&#65306;&#36830;&#32493;&#30340;&#26641;&#24418;&#31995;&#32479;&#25512;&#26029;&#26041;&#27861;&#29992;&#20110;&#26377;&#26681;&#21644;&#26080;&#26681;&#26641;
&lt;/p&gt;
&lt;p&gt;
Leaping through tree space: continuous phylogenetic inference for rooted and unrooted trees. (arXiv:2306.05739v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#26641;&#24418;&#31995;&#32479;&#25506;&#32034;&#21644;&#25512;&#26029;&#65292;&#29992;&#20110;&#26377;&#26681;&#21644;&#26080;&#26681;&#26641;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20339;&#26041;&#27861;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#25928;&#26524;&#65292;&#21487;&#29992;&#20110;&#21152;&#36895;&#29983;&#21629;&#31185;&#23398;&#30340;&#26032;&#36827;&#21270;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#36827;&#21270;&#31995;&#32479;&#23398;&#29616;&#22312;&#26159;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#30784;&#65292;&#21487;&#20197;&#38416;&#26126;&#29983;&#21629;&#26089;&#26399;&#25903;&#31995;&#21644;&#20256;&#26579;&#30149;&#30340;&#36215;&#28304;&#21644;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#20174;&#21487;&#33021;&#30340;&#26641;&#30340;&#24191;&#38420;&#31354;&#38388;&#20013;&#25214;&#21040;&#21512;&#36866;&#30340;&#31995;&#32479;&#26641;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#20102;&#26641;&#24418;&#31995;&#32479;&#25506;&#32034;&#21644;&#25512;&#26029;&#65292;&#20351;&#26799;&#24230;&#35745;&#31639;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#36830;&#32493;&#30340;&#25918;&#26494;&#26041;&#24335;&#20801;&#35768;&#22312;&#26377;&#26681;&#21644;&#26080;&#26681;&#26641;&#20013;&#36328;&#36234;&#26641;&#31354;&#38388;&#65292;&#19988;&#19981;&#26131;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20339;&#30340;&#26080;&#26681;&#26641;&#25512;&#26029;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#20013;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#26641;&#21644;&#26641;&#26681;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#20013;&#20063;&#24456;&#26377;&#25928;&#65292;&#25105;&#20204;&#22312;&#39052;&#21475;&#21160;&#29289;&#30340;&#31995;&#32479;&#21457;&#32946;&#20013;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#20107;&#23454;&#19978;&#65292;&#20165;&#20855;&#26377;&#36229;&#25351;&#25968;&#20449;&#21495;&#30340;&#23569;&#25968;&#22522;&#22240;&#36890;&#24120;&#36275;&#20197;&#20998;&#36776;&#33034;&#26894;&#21160;&#29289;&#30340;&#20027;&#35201;&#35889;&#31995;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24076;&#26395;&#21152;&#36895;&#21457;&#29616;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#26032;&#36827;&#21270;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phylogenetics is now fundamental in life sciences, providing insights into the earliest branches of life and the origins and spread of epidemics. However, finding suitable phylogenies from the vast space of possible trees remains challenging. To address this problem, for the first time, we perform both tree exploration and inference in a continuous space where the computation of gradients is possible. This continuous relaxation allows for major leaps across tree space in both rooted and unrooted trees, and is less susceptible to convergence to local minima. Our approach outperforms the current best methods for inference on unrooted trees and, in simulation, accurately infers the tree and root in ultrametric cases. The approach is effective in cases of empirical data with negligible amounts of data, which we demonstrate on the phylogeny of jawed vertebrates. Indeed, only a few genes with an ultrametric signal were generally sufficient for resolving the major lineages of vertebrate. With
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#35843;&#25972;&#32852;&#37030;&#24179;&#22343;&#20013;&#30340;&#32858;&#21512;&#26435;&#37325;&#65292;&#36890;&#36807;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#30340;&#21442;&#19982;&#21382;&#21490;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21442;&#19982;&#29575;&#30340;&#23458;&#25143;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#26410;&#30693;&#21442;&#19982;&#27010;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03401</link><description>&lt;p&gt;
&#22788;&#29702;&#32852;&#37030;&#24179;&#22343;&#20013;&#26410;&#30693;&#21442;&#19982;&#27010;&#29575;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Lightweight Method for Tackling Unknown Participation Probabilities in Federated Averaging. (arXiv:2306.03401v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#35843;&#25972;&#32852;&#37030;&#24179;&#22343;&#20013;&#30340;&#32858;&#21512;&#26435;&#37325;&#65292;&#36890;&#36807;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#30340;&#21442;&#19982;&#21382;&#21490;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21442;&#19982;&#29575;&#30340;&#23458;&#25143;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#26410;&#30693;&#21442;&#19982;&#27010;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#23458;&#25143;&#31471;&#36890;&#24120;&#20855;&#26377;&#20808;&#39564;&#26410;&#30693;&#30340;&#19981;&#21516;&#21442;&#19982;&#29575;&#65292;&#22914;&#26524;&#19981;&#36866;&#24403;&#22788;&#29702;&#65292;&#21017;&#21487;&#33021;&#20250;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#36896;&#25104;&#37325;&#22823;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#20840;&#23616;&#26041;&#24046;&#32553;&#20943;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#39069;&#22806;&#30340;&#20869;&#23384;&#65292;&#20854;&#20056;&#27861;&#22240;&#23376;&#31561;&#20110;&#23458;&#25143;&#24635;&#25968;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#26159;&#25214;&#21040;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#22791;&#19981;&#21516;&#21442;&#19982;&#29575;&#23458;&#25143;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#30340;&#21442;&#19982;&#21382;&#21490;&#26469;&#35843;&#25972;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#20013;&#30340;&#32858;&#21512;&#26435;&#37325;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#20855;&#26377;&#24322;&#26500;&#21442;&#19982;&#27010;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#38750;&#26368;&#20248;&#32858;&#21512;&#26435;&#37325;&#30340;FedAvg&#21487;&#33021;&#20250;&#20174;&#21407;&#22987;FL&#30446;&#26631;&#30340;&#26368;&#20248;&#35299;&#20559;&#31163;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#25214;&#21040;&#26368;&#20248;&#32858;&#21512;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#24403;&#21442;&#19982;&#27010;&#29575;&#19981;&#21487;&#30693;&#26102;&#35745;&#31639;&#26368;&#20248;&#26435;&#37325;&#38750;&#24120;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning (FL), clients usually have diverse participation probabilities that are unknown a priori, which can significantly harm the performance of FL if not handled properly. Existing works aiming at addressing this problem are usually based on global variance reduction, which requires a substantial amount of additional memory in a multiplicative factor equal to the total number of clients. An important open problem is to find a lightweight method for FL in the presence of clients with unknown participation rates. In this paper, we address this problem by adapting the aggregation weights in federated averaging (FedAvg) based on the participation history of each client. We first show that, with heterogeneous participation probabilities, FedAvg with non-optimal aggregation weights can diverge from the optimal solution of the original FL objective, indicating the need of finding optimal aggregation weights. However, it is difficult to compute the optimal weights when the part
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#22312;&#20855;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20445;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#23454;&#38469;&#36951;&#25022;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#23454;&#38469;&#20013;&#21462;&#24471;&#20102;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.02869</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#36951;&#25022;&#24179;&#34913;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Regret Balancing for Online Model Selection in Bandits. (arXiv:2306.02869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02869
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#22312;&#20855;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20445;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#23454;&#38469;&#36951;&#25022;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#23454;&#38469;&#20013;&#21462;&#24471;&#20102;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#20855;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#27169;&#22411;&#36873;&#25321;&#65292;&#20854;&#20013;&#20803;&#23398;&#20064;&#22120;&#21487;&#20197;&#20351;&#29992;&#19968;&#32452;&#22522;&#26412;&#23398;&#20064;&#22120;&#65292;&#24182;&#26681;&#25454;&#27599;&#20010;&#22522;&#26412;&#23398;&#20064;&#22120;&#25512;&#33616;&#30340;&#31574;&#30053;&#21160;&#24577;&#20915;&#31574;&#12290;&#25105;&#20204;&#36890;&#36807;&#36951;&#25022;&#24179;&#34913;&#26469;&#25191;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#20294;&#19982;&#27492;&#30456;&#20851;&#30340;&#26368;&#36817;&#25991;&#29486;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#27809;&#26377;&#20551;&#35774;&#20219;&#20309;&#20851;&#20110;&#22522;&#26412;&#23398;&#20064;&#22120;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#22914;&#20505;&#36873;&#36951;&#25022;&#20445;&#35777;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#25581;&#31034;&#36825;&#20123;&#25968;&#37327;&#12290;&#22240;&#27492;&#65292;&#20803;&#23398;&#20064;&#22120;&#33021;&#22815;&#21033;&#29992;&#27599;&#20010;&#22522;&#26412;&#23398;&#20064;&#22120;&#22312;&#32473;&#23450;&#30340;&#23398;&#20064;&#29615;&#22659;&#20013;&#20135;&#29983;&#30340;&#23454;&#38469;&#36951;&#25022;&#65288;&#32780;&#19981;&#26159;&#26399;&#26395;&#36951;&#25022;&#65289;&#65292;&#24182;&#25361;&#36873;&#20986;&#26368;&#20339;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#25805;&#20316;&#26356;&#20026;&#38596;&#24515;&#21187;&#21187;&#30340;&#36951;&#25022;&#27010;&#24565;&#65292;&#24182;&#19988;&#38500;&#20102;&#36890;&#36807;&#36951;&#25022;&#24179;&#34913;&#35777;&#26126;&#27169;&#22411;&#36873;&#25321;&#20445;&#35777;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#22788;&#29702;&#23454;&#38469;&#36951;&#25022;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#23454;&#38469;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider model selection for sequential decision making in stochastic environments with bandit feedback, where a meta-learner has at its disposal a pool of base learners, and decides on the fly which action to take based on the policies recommended by each base learner. Model selection is performed by regret balancing but, unlike the recent literature on this subject, we do not assume any prior knowledge about the base learners like candidate regret guarantees; instead, we uncover these quantities in a data-driven manner. The meta-learner is therefore able to leverage the realized regret incurred by each base learner for the learning environment at hand (as opposed to the expected regret), and single out the best such regret. We design two model selection algorithms operating with this more ambitious notion of regret and, besides proving model selection guarantees via regret balancing, we experimentally demonstrate the compelling practical benefits of dealing with actual regrets ins
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20855;&#26377;&#38750;&#30697;&#24418;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#24378;&#20581;MDP&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#25237;&#23556;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#21644;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#12290;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19004</link><description>&lt;p&gt;
&#38750;&#30697;&#24418;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#24378;&#20581;MDP&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Algorithms for Robust MDPs with Non-Rectangular Uncertainty Sets. (arXiv:2305.19004v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20855;&#26377;&#38750;&#30697;&#24418;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#24378;&#20581;MDP&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#25237;&#23556;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#21644;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#12290;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#20855;&#26377;&#38750;&#30697;&#24418;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#24378;&#20581;&#26080;&#38480;&#26102;&#22495;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24378;&#20581;MDP&#25991;&#29486;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#30830;&#23454;&#65292;&#26174;&#31034;&#32479;&#35745;&#26368;&#20248;&#24615;&#36136;&#24182;&#20805;&#20998;&#21033;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#24448;&#24448;&#19981;&#26159;&#30697;&#24418;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#24212;&#30340;&#24378;&#20581;MDPs&#19981;&#33021;&#29992;&#21160;&#24577;&#35268;&#21010;&#25216;&#26415;&#35299;&#20915;&#65292;&#24182;&#19988;&#23454;&#38469;&#19978;&#26159;&#21487;&#35777;&#26126;&#30340;&#19981;&#21487;&#35299;&#20915;&#30340;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#19968;&#20010;&#38024;&#23545;&#24378;&#20581;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#37327;&#36523;&#23450;&#21046;&#30340;&#25237;&#23556;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#25552;&#20379;&#20840;&#23616;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36817;&#20284;&#35299;&#20915;&#20102;&#24378;&#20581;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#36817;&#20284;&#35823;&#24046;&#19982;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#38750;&#30697;&#24418;&#24230;&#37327;&#25104;&#27604;&#20363;&#12290;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25237;&#24433;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#21487;&#20197;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#65292;&#32780;&#31639;&#27861;&#26159;&#37327;&#36523;&#23450;&#21046;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a policy gradient algorithm for robust infinite-horizon Markov Decision Processes (MDPs) with non-rectangular uncertainty sets, thereby addressing an open challenge in the robust MDP literature. Indeed, uncertainty sets that display statistical optimality properties and make optimal use of limited data often fail to be rectangular. Unfortunately, the corresponding robust MDPs cannot be solved with dynamic programming techniques and are in fact provably intractable. This prompts us to develop a projected Langevin dynamics algorithm tailored to the robust policy evaluation problem, which offers global optimality guarantees. We also propose a deterministic policy gradient method that solves the robust policy evaluation problem approximately, and we prove that the approximation error scales with a new measure of non-rectangularity of the uncertainty set. Numerical experiments showcase that our projected Langevin dynamics algorithm can escape local optima, while algorithms tailor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;&#28857;&#36807;&#31243;&#27880;&#24847;&#21147;&#21644;&#32593;&#26684;&#32534;&#30721;&#30340;&#31639;&#27861;&#65292;&#22312;&#20998;&#24067;&#22806;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#20026;&#29702;&#35299;&#22823;&#33041;&#24378;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.18417</link><description>&lt;p&gt;
&#28857;&#36807;&#31243;&#27880;&#24847;&#21147;&#25903;&#25345;&#32593;&#26684;&#32534;&#30721;&#20197;&#23454;&#29616;&#20998;&#24067;&#22806;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Determinantal Point Process Attention Over Grid Codes Supports Out of Distribution Generalization. (arXiv:2305.18417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;&#28857;&#36807;&#31243;&#27880;&#24847;&#21147;&#21644;&#32593;&#26684;&#32534;&#30721;&#30340;&#31639;&#27861;&#65292;&#22312;&#20998;&#24067;&#22806;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#20026;&#29702;&#35299;&#22823;&#33041;&#24378;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#20223;&#31867;&#20154;&#26234;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#26469;&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#35299;&#20915;&#22797;&#26434;&#30340;&#35745;&#31639;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#19981;&#33021;&#25552;&#20379;&#20851;&#20110;&#22823;&#33041;&#22914;&#20309;&#25903;&#25345;&#20154;&#31867;&#33021;&#22815;&#23454;&#29616;&#30340;&#24378;&#24418;&#24335;&#27867;&#21270;&#30340;&#35265;&#35299;&#12290;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#26159;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#27867;&#21270;&#8212;&#8212;&#22312;&#35757;&#32451;&#38598;&#20998;&#24067;&#20043;&#22806;&#30340;&#27979;&#35797;&#26679;&#20363;&#19978;&#25104;&#21151;&#25191;&#34892;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#22823;&#33041;&#22788;&#29702;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#33021;&#26377;&#21161;&#20110;&#23454;&#29616;&#36825;&#31181;&#33021;&#21147;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20004;&#37096;&#20998;&#31639;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#35745;&#31639;&#30340;&#29305;&#23450;&#29305;&#24449;&#23454;&#29616;OOD&#27867;&#21270;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35748;&#30693;&#20219;&#21153;&#30340;&#34920;&#29616;&#26469;&#25552;&#20379;&#27010;&#24565;&#39564;&#35777;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#21754;&#20083;&#21160;&#29289;&#22823;&#33041;&#20351;&#29992;&#31867;&#20284;&#32593;&#26684;&#30340;&#34920;&#31034;&#65288;&#20363;&#22914;&#22312;&#20869;&#21957;&#30382;&#23618;&#20013;&#65289;&#26469;&#34920;&#31034;&#24230;&#37327;&#31354;&#38388;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have made tremendous gains in emulating human-like intelligence, and have been used increasingly as ways of understanding how the brain may solve the complex computational problems on which this relies. However, these still fall short of, and therefore fail to provide insight into how the brain supports strong forms of generalization of which humans are capable. One such case is out-of-distribution (OOD) generalization -successful performance on test examples that lie outside the distribution of the training set. Here, we identify properties of processing in the brain that may contribute to this ability. We describe a two-part algorithm that draws on specific features of neural computation to achieve OOD generalization, and provide a proof of concept by evaluating performance on two challenging cognitive tasks. First we draw on the fact that the mammalian brain represents metric spaces using grid-like representations (e.g., in entorhinal cortex): abstract represe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14259</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25991;&#29486;&#30340;&#35821;&#22659;&#21270;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#65288;LBD&#65289;&#26088;&#22312;&#36890;&#36807;&#25366;&#25496;&#35770;&#25991;&#24182;&#29983;&#25104;&#20551;&#35774;&#26469;&#21457;&#29616;&#26032;&#30340;&#31185;&#23398;&#30693;&#35782;&#12290;&#26631;&#20934;&#30340;LBD&#20165;&#38480;&#20110;&#39044;&#27979;&#31163;&#25955;&#27010;&#24565;&#20043;&#38388;&#30340;&#20004;&#20004;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#21644;&#30142;&#30149;&#30340;&#20851;&#32852;&#65289;&#12290;LBD&#36824;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23454;&#39564;&#35774;&#32622;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#35780;&#20272;&#30340;&#29305;&#23450;&#24739;&#32773;&#32676;&#20307;&#65289;&#21644;&#20154;&#31867;&#31185;&#23398;&#23478;&#32771;&#34385;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#21160;&#26426;&#65288;&#20363;&#22914;&#65292;&#25214;&#21040;&#27809;&#26377;&#29305;&#23450;&#21103;&#20316;&#29992;&#30340;&#33647;&#29289;&#20505;&#36873;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#21270;LBD&#65288;C-LBD&#65289;&#34920;&#36848;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65306;&#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31185;&#23398;&#20551;&#35774;&#65292;&#21516;&#26102;&#23558;&#23427;&#20204;&#32852;&#31995;&#21040;&#25511;&#21046;&#20551;&#35774;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#65292;&#20351;&#29992;&#33719;&#24471;&#30340;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#24322;&#26500;&#32593;&#32476;&#20013;&#30340;&#8220;&#28789;&#24863;&#8221;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;&#35770;&#25991;&#20013;&#27966;&#29983;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
&lt;/p&gt;</description></item><item><title>SpecInfer&#26159;&#19968;&#31181;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#26469;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.09781</link><description>&lt;p&gt;
SpecInfer&#65306;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09781
&lt;/p&gt;
&lt;p&gt;
SpecInfer&#26159;&#19968;&#31181;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#26469;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#22240;&#27492;&#24555;&#36895;&#21644;&#24265;&#20215;&#22320;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;SpecInfer&#65292;&#19968;&#20010;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#21152;&#36895;&#29983;&#25104;&#24335;LLM&#25512;&#26029;&#12290;SpecInfer&#32972;&#21518;&#30340;&#20851;&#38190;&#26159;&#23558;&#21508;&#31181;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#20307;&#25552;&#21319;&#35843;&#25972;&#65292;&#20849;&#21516;&#39044;&#27979;LLM&#30340;&#36755;&#20986;&#65307; &#39044;&#27979;&#32467;&#26524;&#32452;&#32455;&#25104;&#19968;&#20010;&#20196;&#29260;&#26641;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#37117;&#34920;&#31034;&#20505;&#36873;&#20196;&#29260;&#24207;&#21015;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26641;&#30340;&#24182;&#34892;&#35299;&#30721;&#26426;&#21046;&#65292;&#20197;LMM&#20316;&#20026;&#20196;&#29260;&#26641;&#39564;&#35777;&#22120;&#26469;&#39564;&#35777;&#20196;&#29260;&#26641;&#25152;&#20195;&#34920;&#30340;&#25152;&#26377;&#20505;&#36873;&#20196;&#29260;&#24207;&#21015;&#30340;&#27491;&#30830;&#24615;&#12290;SpecInfer&#20351;&#29992;LLM&#20316;&#20026;&#20196;&#29260;&#26641;&#39564;&#35777;&#22120;&#65292;&#32780;&#19981;&#26159;&#22686;&#37327;&#35299;&#30721;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#29983;&#25104;&#24335;LLM&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#21487;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree verification. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21512;&#24182;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#8220;ZipIt&#65281;&#8221;&#12290;</title><link>http://arxiv.org/abs/2305.03053</link><description>&lt;p&gt;
ZipIt&#65281;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#21512;&#24182;&#19981;&#21516;&#20219;&#21153;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ZipIt! Merging Models from Different Tasks without Training. (arXiv:2305.03053v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21512;&#24182;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#8220;ZipIt&#65281;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30340;&#28145;&#24230;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#33021;&#22815;&#25191;&#34892;&#23427;&#20204;&#32463;&#36807;&#35757;&#32451;&#30340;&#21333;&#19968;&#20219;&#21153;&#12290;&#26412;&#25991;&#35299;&#20915;&#23558;&#23436;&#20840;&#19981;&#21516;&#30340;&#12289;&#27599;&#20010;&#35299;&#20915;&#19968;&#20010;&#29420;&#31435;&#20219;&#21153;&#30340;&#27169;&#22411;&#21512;&#24182;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#26497;&#20854;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#20197;&#21069;&#30340;&#27169;&#22411;&#21512;&#24182;&#24037;&#20316;&#23558;&#19968;&#20010;&#27169;&#22411;&#32622;&#25442;&#21040;&#21478;&#19968;&#20010;&#27169;&#22411;&#30340;&#31354;&#38388;&#20013;&#65292;&#20877;&#23558;&#23427;&#20204;&#30456;&#21152;&#12290;&#34429;&#28982;&#36825;&#23545;&#20110;&#22312;&#21516;&#19968;&#20010;&#20219;&#21153;&#19978;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#36215;&#20316;&#29992;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#26410;&#33021;&#32771;&#34385;&#21040;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;ZipIt&#65281;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#24182;&#30456;&#21516;&#32467;&#26500;&#30340;&#20004;&#20010;&#20219;&#24847;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#32771;&#34385;&#21040;&#22312;&#27169;&#22411;&#20043;&#38388;&#27809;&#26377;&#20849;&#20139;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#21512;&#24182;&#38382;&#39064;&#25193;&#23637;&#21040;&#36824;&#20801;&#35768;&#21512;&#24182;&#27599;&#20010;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#65292;&#23450;&#20041;&#19968;&#20010;&#36890;&#29992;&#30340;&#8220;zip&#8221;&#25805;&#20316;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28155;&#21152;&#25903;&#25345;&#37096;&#20998;&#21387;&#32553;&#27169;&#22411;&#30340;&#21151;&#33021;&#65292;&#30452;&#21040;&#29305;&#23450;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typical deep visual recognition models are capable of performing the one task they were trained on. In this paper, we tackle the extremely difficult problem of combining completely distinct models with different initializations, each solving a separate task, into one multi-task model without any additional training. Prior work in model merging permutes one model to the space of the other then adds them together. While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce "ZipIt!", a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren't shared between models, we expand the model merging problem to additionally allow for merging features within each model by defining a general "zip" operation. Second, we add support for partially zipping the models up until a specified layer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.14391</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#22330;&#26223;&#37325;&#25490;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#37322;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30456;&#23545;&#23545;&#35937;&#25490;&#21015;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#34920;&#31034;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#12290;&#35821;&#35328;&#35299;&#26512;&#22120;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#32780;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#21442;&#25968;&#22522;&#20110;&#22330;&#26223;&#20013;&#30340;&#30456;&#20851;&#23545;&#35937;&#36827;&#34892;&#20462;&#27491;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#27714;&#35299;&#33021;&#37327;&#20989;&#25968;&#30340;&#24635;&#21644;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26412;&#22320;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#31574;&#30053;&#23558;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#21040;&#25512;&#26029;&#30340;&#30446;&#26631;&#20301;&#32622;&#65292;&#21363;&#21487;&#29983;&#25104;&#30446;&#26631;&#22330;&#26223;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#32489;&#25928;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-RNN&#28151;&#21512;&#29305;&#24449;&#34701;&#21512;&#24314;&#27169;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#22478;&#24066;&#27946;&#27700;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#38745;&#24577;&#21644;&#21160;&#24577;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#22810;&#20010;CNN&#21644;RNN&#27169;&#22411;&#65292;&#22312;&#31934;&#24230;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.09994</link><description>&lt;p&gt;
&#22522;&#20110;LSTM-DeepLabv3+&#21644;&#26102;&#31354;&#29305;&#24449;&#34701;&#21512;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#22478;&#24066;&#27946;&#27700;&#39044;&#27979;&#27169;&#22411;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian Optimization with Spatiotemporal feature fusion. (arXiv:2304.09994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-RNN&#28151;&#21512;&#29305;&#24449;&#34701;&#21512;&#24314;&#27169;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#22478;&#24066;&#27946;&#27700;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#38745;&#24577;&#21644;&#21160;&#24577;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#22810;&#20010;CNN&#21644;RNN&#27169;&#22411;&#65292;&#22312;&#31934;&#24230;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22240;&#20854;&#30456;&#23545;&#20256;&#32479;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#36880;&#28176;&#25104;&#20026;&#27969;&#34892;&#30340;&#27946;&#27700;&#39044;&#27979;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21333;&#29420;&#30340;&#31354;&#38388;&#25110;&#26102;&#38388;&#29305;&#24449;&#20998;&#26512;&#65292;&#24182;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#32500;&#24230;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CNN-RNN&#30340;&#28151;&#21512;&#29305;&#24449;&#34701;&#21512;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22478;&#24066;&#27946;&#27700;&#39044;&#27979;&#65292;&#23558;CNN&#22312;&#22788;&#29702;&#31354;&#38388;&#29305;&#24449;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;RNN&#22312;&#20998;&#26512;&#19981;&#21516;&#32500;&#24230;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#30340;&#20248;&#21183;&#25972;&#21512;&#36215;&#26469;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#36827;&#34892;&#38745;&#24577;&#21644;&#21160;&#24577;&#30340;&#27946;&#27700;&#39044;&#27979;&#12290;&#24212;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#30830;&#23450;&#19971;&#20010;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#27946;&#27700;&#39537;&#21160;&#22240;&#32032;&#65292;&#24182;&#30830;&#23450;&#26368;&#20339;&#32452;&#21512;&#31574;&#30053;&#12290;&#36890;&#36807;&#32467;&#21512;&#22235;&#20010;CNN&#65288;FCN&#65292;UNet&#65292;SegNet&#65292;DeepLabv3+&#65289;&#21644;&#19977;&#20010;RNN&#65288;LSTM&#65292;BiLSTM&#65292;GRU&#65289;&#65292;&#26368;&#20248;&#28151;&#21512;&#27169;&#22411;&#34987;&#30830;&#23450;&#20026;LSTM-DeepLabv3+&#12290;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65288;MAE&#12289;RMSE&#12289;NSE&#21644;KGE&#29575;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have become increasingly popular for flood prediction due to their superior accuracy and efficiency compared to traditional methods. However, current machine learning methods often rely on separate spatial or temporal feature analysis and have limitations on the types, number, and dimensions of input data. This study presented a CNN-RNN hybrid feature fusion modelling approach for urban flood prediction, which integrated the strengths of CNNs in processing spatial features and RNNs in analyzing different dimensions of time sequences. This approach allowed for both static and dynamic flood predictions. Bayesian optimization was applied to identify the seven most influential flood-driven factors and determine the best combination strategy. By combining four CNNs (FCN, UNet, SegNet, DeepLabv3+) and three RNNs (LSTM, BiLSTM, GRU), the optimal hybrid model was identified as LSTM-DeepLabv3+. This model achieved the highest prediction accuracy (MAE, RMSE, NSE, and KGE wer
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#26469;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#30340;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#23398;&#20064;&#19981;&#21463;&#25197;&#26354;&#24433;&#21709;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#20197;&#24314;&#31435;&#38750;&#22270;&#20687;&#25511;&#21046;&#20219;&#21153;&#30340;&#39044;&#27979;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.07846</link><description>&lt;p&gt;
&#39640;&#25928;&#29575;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-efficient Adversarial Imitation Learning. (arXiv:2303.07846v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#26469;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#30340;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#23398;&#20064;&#19981;&#21463;&#25197;&#26354;&#24433;&#21709;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#20197;&#24314;&#31435;&#38750;&#22270;&#20687;&#25511;&#21046;&#20219;&#21153;&#30340;&#39044;&#27979;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#21363;&#36890;&#36807;&#28436;&#31034;&#36827;&#34892;&#23398;&#20064;&#65292;&#24050;&#32463;&#34987;&#30740;&#31350;&#24182;&#24212;&#29992;&#20110;&#24207;&#36143;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#65292;&#22870;&#21169;&#20989;&#25968;&#24182;&#19981;&#26159;&#39044;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#20173;&#38656;&#35201;&#22823;&#37327;&#30340;&#19987;&#23478;&#28436;&#31034;&#26679;&#26412;&#25165;&#33021;&#25104;&#21151;&#27169;&#20223;&#19987;&#23478;&#30340;&#34892;&#20026;&#12290;&#20026;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#25968;&#25454;&#29983;&#25104;&#22823;&#37327;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#19981;&#21463;&#21508;&#31181;&#25197;&#26354;&#24433;&#21709;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#65292;&#24182;&#24314;&#31435;&#38750;&#22270;&#20687;&#25511;&#21046;&#20219;&#21153;&#30340;&#39044;&#27979;&#34920;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#19982;&#29616;&#26377;&#30340;&#34920;&#26684;&#25968;&#25454;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#30340;&#19981;&#21516;&#25439;&#22351;&#26041;&#27861;&#65292;&#20197;&#20351;&#20854;&#33021;&#22815;&#25269;&#25239;&#21508;&#31181;&#25197;&#26354;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#35266;&#23519;&#34920;&#26126;&#65292;&#20351;&#19968;&#20010;&#20449;&#24687;&#37327;&#22823;&#30340;&#29305;&#24449;&#27969;&#24418;&#19982;&#19968;&#20010;&#31616;&#21333;&#30340;&#29983;&#25104;&#22120;&#19982;&#19968;&#20010;&#22797;&#26434;&#30340;&#20998;&#31867;&#22120;&#21327;&#21516;&#24037;&#20316;&#33021;&#22815;&#25552;&#39640;&#29366;&#24577;&#34920;&#24449;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning, in which learning is performed by demonstration, has been studied and advanced for sequential decision-making tasks in which a reward function is not predefined. However, imitation learning methods still require numerous expert demonstration samples to successfully imitate an expert's behavior. To improve sample efficiency, we utilize self-supervised representation learning, which can generate vast training signals from the given data. In this study, we propose a self-supervised representation-based adversarial imitation learning method to learn state and action representations that are robust to diverse distortions and temporally predictive, on non-image control tasks. In particular, in comparison with existing self-supervised learning methods for tabular data, we propose a different corruption method for state and action representations that is robust to diverse distortions. We theoretically and empirically observe that making an informative feature manifold with 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Sparse Gaussian Process attention (SGPA)&#26469;&#26657;&#20934;Transformer&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;SGPA-based Transformers&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#20869;&#20998;&#24067;&#26657;&#20934;&#21644;&#22806;&#20998;&#24067;&#30340;&#40065;&#26834;&#24615;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.02444</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#26657;&#20934;Transformer
&lt;/p&gt;
&lt;p&gt;
Calibrating Transformers via Sparse Gaussian Processes. (arXiv:2303.02444v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02444
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Sparse Gaussian Process attention (SGPA)&#26469;&#26657;&#20934;Transformer&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;SGPA-based Transformers&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#20869;&#20998;&#24067;&#26657;&#20934;&#21644;&#22806;&#20998;&#24067;&#30340;&#40065;&#26834;&#24615;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#24191;&#27867;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23558;Transformer&#30340;&#25104;&#21151;&#25193;&#23637;&#21040;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#38656;&#35201;&#20934;&#30830;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#27880;&#24847;&#21147;&#65288;SGPA&#65289;&#65292;&#23427;&#30452;&#25509;&#22312;Transformer&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#65288;MHA&#65289;&#30340;&#36755;&#20986;&#31354;&#38388;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20197;&#26657;&#20934;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#23427;&#29992;&#19968;&#20010;&#26377;&#25928;&#30340;&#23545;&#31216;&#26680;&#26367;&#20195;&#20102;&#32553;&#25918;&#28857;&#31215;&#25805;&#20316;&#65292;&#24182;&#20351;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#65288;SGP&#65289;&#25216;&#26415;&#26469;&#36817;&#20284;MHA&#36755;&#20986;&#30340;&#21518;&#39564;&#36807;&#31243;&#12290;&#32463;&#39564;&#19978;&#65292;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#30340;&#19968;&#31995;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;SGPA&#30340;Transformer&#27169;&#22411;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#26174;&#33879;&#25913;&#21892;&#20102;&#20869;&#20998;&#24067;&#26657;&#20934;&#21644;&#22806;&#20998;&#24067;&#30340;&#40065;&#26834;&#24615;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have achieved profound success in prediction tasks in a wide range of applications in natural language processing, speech recognition and computer vision. Extending Transformer's success to safety-critical domains requires calibrated uncertainty estimation which remains under-explored. To address this, we propose Sparse Gaussian Process attention (SGPA), which performs Bayesian inference directly in the output space of multi-head attention blocks (MHAs) in transformer to calibrate its uncertainty. It replaces the scaled dot-product operation with a valid symmetric kernel and uses sparse Gaussian processes (SGP) techniques to approximate the posterior processes of MHA outputs. Empirically, on a suite of prediction tasks on text, images and graphs, SGPA-based Transformers achieve competitive predictive accuracy, while noticeably improving both in-distribution calibration and out-of-distribution robustness and detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#21487;&#20132;&#25442;&#25968;&#25454;&#36827;&#34892;&#25439;&#22833;&#25511;&#21046;&#39044;&#27979;&#30340;&#26657;&#20934;&#39044;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#20445;&#25345;&#20132;&#25442;&#24615;&#36136;&#30340;&#21464;&#25442;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#33719;&#24471;&#27979;&#35797;&#26631;&#31614;&#26102;&#26377;&#26377;&#38480;&#26679;&#26412;&#25511;&#21046;&#20445;&#35777;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#36817;&#20284;&#26041;&#27861;&#26469;&#26500;&#24314;&#39044;&#27979;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#31526;&#21512;&#25439;&#22833;&#25511;&#21046;&#39044;&#27979;&#30340;&#33258;&#28982;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2301.04378</link><description>&lt;p&gt;
&#39044;&#27979;&#27169;&#22411;&#30340;&#25439;&#22833;&#25511;&#21046;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Loss-Controlling Calibration for Predictive Models. (arXiv:2301.04378v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#21487;&#20132;&#25442;&#25968;&#25454;&#36827;&#34892;&#25439;&#22833;&#25511;&#21046;&#39044;&#27979;&#30340;&#26657;&#20934;&#39044;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#20445;&#25345;&#20132;&#25442;&#24615;&#36136;&#30340;&#21464;&#25442;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#33719;&#24471;&#27979;&#35797;&#26631;&#31614;&#26102;&#26377;&#26377;&#38480;&#26679;&#26412;&#25511;&#21046;&#20445;&#35777;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#36817;&#20284;&#26041;&#27861;&#26469;&#26500;&#24314;&#39044;&#27979;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#31526;&#21512;&#25439;&#22833;&#25511;&#21046;&#39044;&#27979;&#30340;&#33258;&#28982;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#21487;&#20132;&#25442;&#25968;&#25454;&#36827;&#34892;&#25439;&#22833;&#25511;&#21046;&#39044;&#27979;&#30340;&#26657;&#20934;&#39044;&#27979;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#30340;&#36866;&#29992;&#20110;&#26356;&#19968;&#33324;&#24773;&#20917;&#30340;&#31526;&#21512;&#25439;&#22833;&#25511;&#21046;&#39044;&#27979;&#12290;&#19982;&#20043;&#30456;&#27604;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#25439;&#22833;&#25511;&#21046;&#26041;&#27861;&#26500;&#24314;&#30340;&#39044;&#27979;&#22120;&#19981;&#38480;&#20110;&#38598;&#21512;&#39044;&#27979;&#22120;&#65292;&#24182;&#19988;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#26159;&#20219;&#20309;&#21487;&#27979;&#20989;&#25968;&#65292;&#32780;&#19981;&#38656;&#35201;&#21333;&#35843;&#24615;&#20551;&#35774;&#12290;&#20026;&#20102;&#20197;&#26377;&#25928;&#30340;&#26041;&#24335;&#25511;&#21046;&#25439;&#22833;&#20540;&#65292;&#25105;&#20204;&#24341;&#20837;&#20445;&#25345;&#20132;&#25442;&#24615;&#36136;&#30340;&#21464;&#25442;&#65292;&#20197;&#35777;&#26126;&#22312;&#33719;&#24471;&#27979;&#35797;&#26631;&#31614;&#26102;&#26377;&#26377;&#38480;&#26679;&#26412;&#25511;&#21046;&#20445;&#35777;&#65292;&#24182;&#28982;&#21518;&#21457;&#23637;&#20986;&#19968;&#31181;&#26500;&#24314;&#39044;&#27979;&#22120;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;&#36825;&#20123;&#21464;&#25442;&#21487;&#20197;&#24314;&#31435;&#22312;&#20219;&#20309;&#39044;&#23450;&#20041;&#30340;&#20989;&#25968;&#19978;&#65292;&#21253;&#25324;&#20351;&#29992;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#21442;&#25968;&#25628;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#31526;&#21512;&#25439;&#22833;&#25511;&#21046;&#39044;&#27979;&#30340;&#33258;&#28982;&#25193;&#23637;&#65292;&#22240;&#20026;&#24403;&#38598;&#21512;&#39044;&#27979;&#22120;&#20855;&#26377;&#23884;&#22871;&#23646;&#24615;&#19988;&#25439;&#22833;&#20989;&#25968;&#20026;&#21333;&#35843;&#20989;&#25968;&#26102;&#65292;&#23427;&#21487;&#20197;&#21270;&#31616;&#20026;&#21518;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a learning framework for calibrating predictive models to make loss-controlling prediction for exchangeable data, which extends our recently proposed conformal loss-controlling prediction for more general cases. By comparison, the predictors built by the proposed loss-controlling approach are not limited to set predictors, and the loss function can be any measurable function without the monotone assumption. To control the loss values in an efficient way, we introduce transformations preserving exchangeability to prove finite-sample controlling guarantee when the test label is obtained, and then develop an approximation approach to construct predictors. The transformations can be built on any predefined function, which include using optimization algorithms for parameter searching. This approach is a natural extension of conformal loss-controlling prediction, since it can be reduced to the latter when the set predictors have the nesting property and the loss functions are mono
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25805;&#32437;&#25439;&#22833;&#25511;&#21046;&#30340;&#39044;&#27979;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#19968;&#33268;&#39044;&#27979;&#30340;&#33539;&#22260;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#38656;&#35201;&#25511;&#21046;&#25439;&#22833;&#20989;&#25968;&#20540;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#25511;&#21046;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.02424</link><description>&lt;p&gt;
&#25805;&#32437;&#25439;&#22833;&#25511;&#21046;&#30340;&#19968;&#33268;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal Loss-Controlling Prediction. (arXiv:2301.02424v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25805;&#32437;&#25439;&#22833;&#25511;&#21046;&#30340;&#39044;&#27979;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#19968;&#33268;&#39044;&#27979;&#30340;&#33539;&#22260;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#38656;&#35201;&#25511;&#21046;&#25439;&#22833;&#20989;&#25968;&#20540;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#25511;&#21046;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#39044;&#27979;&#26159;&#19968;&#20010;&#25511;&#21046;&#39044;&#27979;&#38598;&#30340;&#39044;&#27979;&#35206;&#30422;&#29575;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#24314;&#31435;&#22312;&#20219;&#20309;&#29992;&#20110;&#28857;&#39044;&#27979;&#30340;&#23398;&#20064;&#31639;&#27861;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#25805;&#32437;&#25439;&#22833;&#25511;&#21046;&#30340;&#39044;&#27979;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#23558;&#19968;&#33268;&#39044;&#27979;&#25193;&#23637;&#21040;&#38656;&#35201;&#25511;&#21046;&#25439;&#22833;&#20989;&#25968;&#20540;&#30340;&#24773;&#20917;&#12290;&#19982;&#29616;&#26377;&#30340;&#20851;&#20110;&#25511;&#21046;&#39044;&#27979;&#38598;&#39118;&#38505;&#21644;&#19968;&#33268;&#39118;&#38505;&#25511;&#21046;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#35813;&#35770;&#25991;&#20013;&#25552;&#20986;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#23545;&#20219;&#20309;&#27979;&#35797;&#23545;&#35937;&#30340;&#25439;&#22833;&#65292;&#36825;&#26159;&#20174;&#38169;&#35823;&#35206;&#30422;&#25439;&#22833;&#21040;&#19968;&#20123;&#36890;&#29992;&#25439;&#22833;&#30340;&#19968;&#33268;&#39044;&#27979;&#30340;&#25193;&#23637;&#12290;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20551;&#35774;&#25968;&#25454;&#30340;&#21487;&#20114;&#25442;&#24615;&#35777;&#26126;&#20102;&#25511;&#21046;&#20445;&#35777;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#31867;&#20855;&#26377;&#21464;&#21270;&#25439;&#22833;&#30340;&#23454;&#39564;&#21644;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#24212;&#29992;&#30340;&#32479;&#35745;&#21518;&#22788;&#29702;&#36827;&#34892;&#20102;&#23454;&#35777;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction is a learning framework controlling prediction coverage of prediction sets, which can be built on any learning algorithm for point prediction. This work proposes a learning framework named conformal loss-controlling prediction, which extends conformal prediction to the situation where the value of a loss function needs to be controlled. Different from existing works about risk-controlling prediction sets and conformal risk control with the purpose of controlling the expected values of loss functions, the proposed approach in this paper focuses on the loss for any test object, which is an extension of conformal prediction from miscoverage loss to some general loss. The controlling guarantee is proved under the assumption of exchangeability of data in finite-sample cases and the framework is tested empirically for classification with a class-varying loss and statistical postprocessing of numerical weather forecasting applications, which are introduced as point-wise c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#31934;&#30830;&#22320;&#34920;&#24449;&#20102;&#31616;&#21333;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#32972;&#26223;&#38543;&#26426;&#22359;&#27169;&#22411;&#19978;&#30340;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#21516;&#36136;&#24615;&#22312;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#27867;&#21270;&#20013;&#30340;&#35843;&#21046;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.13069</link><description>&lt;p&gt;
&#21516;&#36136;&#24615;&#22312;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#21452;&#19979;&#38477;&#27867;&#21270;&#20013;&#30340;&#35843;&#21046;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Homophily modulates double descent generalization in graph convolution networks. (arXiv:2212.13069v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#31934;&#30830;&#22320;&#34920;&#24449;&#20102;&#31616;&#21333;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#32972;&#26223;&#38543;&#26426;&#22359;&#27169;&#22411;&#19978;&#30340;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#21516;&#36136;&#24615;&#22312;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#27867;&#21270;&#20013;&#30340;&#35843;&#21046;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#20851;&#31995;&#25968;&#25454;&#38598;&#65288;&#22914;&#20195;&#35874;&#12289;&#20132;&#36890;&#21644;&#31038;&#20132;&#32593;&#32476;&#65289;&#30340;&#26368;&#25104;&#21151;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#25968;&#25454;&#20013;&#32534;&#30721;&#30340;&#21508;&#31181;&#20132;&#20114;&#30340;&#24378;&#22823;&#27867;&#21270;&#30340;&#20915;&#23450;&#22240;&#32032;&#24182;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26469;&#33258;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#30340;&#26041;&#27861;&#26080;&#27861;&#35299;&#37322;&#20986;&#29616;&#30340;&#29616;&#35937;&#65292;&#22914;&#21452;&#19979;&#38477;&#25110;&#39118;&#38505;&#21462;&#20915;&#20110;&#20132;&#20114;&#24615;&#36136;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#20998;&#26512;&#24037;&#20855;&#26469;&#31934;&#30830;&#22320;&#34920;&#24449;&#31616;&#21333;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#32972;&#26223;&#38543;&#26426;&#22359;&#27169;&#22411;&#19978;&#30340;&#27867;&#21270;&#12290;&#23548;&#20986;&#30340;&#26354;&#32447;&#29616;&#35937;&#23398;&#19978;&#21313;&#20998;&#20016;&#23500;&#65306;&#23427;&#20204;&#35299;&#37322;&#20102;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#23398;&#20064;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#24182;&#39044;&#27979;&#20102;&#26368;&#36817;&#20316;&#21697;&#25152;&#36136;&#30097;&#30340;GNN&#20013;&#21452;&#19979;&#38477;&#29616;&#35937;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39118;&#38505;&#22914;&#20309;&#21462;&#20915;&#20110;&#22270;&#20013;&#30340;&#22122;&#22768;&#12289;&#29305;&#24449;&#20013;&#30340;&#22122;&#22768;&#21644;&#29992;&#20110;&#35757;&#32451;&#30340;&#33410;&#28857;&#27604;&#20363;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#29702;&#35299;&#21516;&#36136;&#24615;&#22914;&#20309;&#35843;&#21046;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#25552;&#20379;&#20102;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are among the most successful machine learning models for relational datasets like metabolic, transportation, and social networks. Yet the determinants of their strong generalization for diverse interactions encoded in the data are not well understood. Methods from statistical learning theory do not explain emergent phenomena such as double descent or the dependence of risk on the nature of interactions. We use analytical tools from statistical physics and random matrix theory to precisely characterize generalization in simple graph convolution networks on the contextual stochastic block model. The derived curves are phenomenologically rich: they explain the distinction between learning on homophilic and heterophilic and they predict double descent whose existence in GNNs has been questioned by recent work. We show how risk depends on the interplay between the noise in the graph, noise in the features, and the proportion of nodes used for training. Our analysis pr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36793;&#32536;&#39044;&#27979;&#33539;&#24335;&#65288;EMPIRE&#65289;&#65292;&#36890;&#36807;&#32454;&#21270;&#36793;&#32536;&#20351;&#29992;&#26041;&#27861;&#35299;&#20915;&#20102;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#21644;&#36793;&#32536;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#36793;&#32536;&#25286;&#20998;&#25216;&#26415;&#21644;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36793;&#32536;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2212.12970</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36793;&#32536;&#39044;&#27979;&#20013;&#32454;&#21270;&#36793;&#32536;&#20351;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Refined Edge Usage of Graph Neural Networks for Edge Prediction. (arXiv:2212.12970v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12970
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36793;&#32536;&#39044;&#27979;&#33539;&#24335;&#65288;EMPIRE&#65289;&#65292;&#36890;&#36807;&#32454;&#21270;&#36793;&#32536;&#20351;&#29992;&#26041;&#27861;&#35299;&#20915;&#20102;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#21644;&#36793;&#32536;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#36793;&#32536;&#25286;&#20998;&#25216;&#26415;&#21644;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36793;&#32536;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26368;&#21021;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#65292;&#20063;&#28608;&#21457;&#20102;&#35768;&#22810;&#20851;&#20110;&#36793;&#32536;&#39044;&#27979;&#65288;&#21363;&#38142;&#36335;&#39044;&#27979;&#65289;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#20851;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#21306;&#21035;&#26041;&#38754;&#32570;&#20047;&#31934;&#32454;&#30340;&#35774;&#35745;&#65292;&#36825;&#19968;&#28857;&#24120;&#24120;&#34987;&#24573;&#35270;&#65306;&#65288;i&#65289;&#23545;&#20110;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#32780;&#35328;&#65292;&#36793;&#20165;&#26500;&#25104;&#25299;&#25169;&#32467;&#26500;&#65292;&#20294;&#22312;&#36793;&#32536;&#39044;&#27979;&#20219;&#21153;&#20013;&#26082;&#21487;&#20197;&#20316;&#20026;&#25299;&#25169;&#32467;&#26500;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65288;&#21363;&#26631;&#31614;&#65289;&#65307;&#65288;ii&#65289;&#33410;&#28857;&#20998;&#31867;&#26159;&#23545;&#27599;&#20010;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#36793;&#32536;&#39044;&#27979;&#21017;&#30001;&#27599;&#23545;&#33410;&#28857;&#20915;&#23450;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36793;&#32536;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;EMPIRE&#65289;&#30340;&#26032;&#22411;&#36793;&#32536;&#39044;&#27979;&#33539;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#19968;&#31181;&#36793;&#32536;&#25286;&#20998;&#25216;&#26415;&#26469;&#25351;&#23450;&#27599;&#20010;&#36793;&#30340;&#20351;&#29992;&#26041;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#36793;&#20165;&#29992;&#20316;&#25299;&#25169;&#32467;&#26500;&#25110;&#30417;&#30563;&#20449;&#21495;&#65288;&#20998;&#21035;&#31216;&#20026;&#25299;&#25169;&#36793;&#25110;&#30417;&#30563;&#36793;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#29983;&#25104;&#28040;&#24687;&#30340;&#20256;&#36882;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs), originally proposed for node classification, have also motivated many recent works on edge prediction (a.k.a., link prediction). However, existing methods lack elaborate design regarding the distinctions between two tasks that have been frequently overlooked: (i) edges only constitute the topology in the node classification task but can be used as both the topology and the supervisions (i.e., labels) in the edge prediction task; (ii) the node classification makes prediction over each individual node, while the edge prediction is determinated by each pair of nodes. To this end, we propose a novel edge prediction paradigm named Edge-aware Message PassIng neuRal nEtworks (EMPIRE). Concretely, we first introduce an edge splitting technique to specify use of each edge where each edge is solely used as either the topology or the supervision (named as topology edge or supervision edge). We then develop a new message passing mechanism that generates the messages t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#38543;&#26426;&#36807;&#31243;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20132;&#26367;&#21327;&#26041;&#24046;&#20272;&#35745;&#21644;&#35268;&#33539;&#21270;&#20132;&#21449;&#23494;&#24230;&#26469;&#34913;&#37327;&#22810;&#21464;&#37327;&#32479;&#35745;&#20381;&#36182;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20013;&#12290;</title><link>http://arxiv.org/abs/2212.04631</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#20132;&#21449;&#23494;&#24230;&#20989;&#25968;&#65306;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#38543;&#26426;&#36807;&#31243;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Normalized Cross Density Functional: A Framework to Quantify Statistical Dependence for Random Processes. (arXiv:2212.04631v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#38543;&#26426;&#36807;&#31243;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20132;&#26367;&#21327;&#26041;&#24046;&#20272;&#35745;&#21644;&#35268;&#33539;&#21270;&#20132;&#21449;&#23494;&#24230;&#26469;&#34913;&#37327;&#22810;&#21464;&#37327;&#32479;&#35745;&#20381;&#36182;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38463;&#23572;&#24343;&#38647;&#24503;&#183;&#38647;&#23612;&#65288;Alfr\'ed R\'enyi&#65289;&#30340;&#21151;&#33021;&#26041;&#27861;&#35770;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#36830;&#32493;&#38543;&#26426;&#36807;&#31243;&#65288;r.p.&#65289;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#26032;&#39062;&#30340;&#22810;&#21464;&#37327;&#23450;&#20041;&#12290;&#23558;&#38543;&#26426;&#36807;&#31243;&#26679;&#26412;&#23545;&#30340;&#20114;&#20449;&#24687;&#30340;&#23545;&#25968;&#35770;&#35777;&#21629;&#21517;&#20026;&#35268;&#33539;&#21270;&#20132;&#21449;&#23494;&#24230;&#65288;NCD&#65289;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#23545;&#31216;&#21644;&#33258;&#20276;&#30340;&#27491;&#23450;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#26368;&#22823;&#21270;&#20132;&#26367;&#21327;&#26041;&#24046;&#20272;&#35745;&#65288;ACE&#65289;&#36882;&#24402;&#24212;&#29992;&#20110;&#36755;&#20837;&#26679;&#26412;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#65292;&#31526;&#21512;&#38647;&#23612;&#30340;&#26368;&#22823;&#30456;&#20851;&#24615;&#30340;&#25152;&#26377;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NCD&#30340;&#29305;&#24449;&#35889;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#21464;&#37327;&#24230;&#37327;&#65292;&#29992;&#20110;&#34913;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;r.p.&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#12290;&#21033;&#29992;r.p.&#30340;&#23454;&#29616;&#65292;&#20063;&#21487;&#20197;&#30452;&#25509;&#20272;&#35745;&#22810;&#21464;&#37327;&#32479;&#35745;&#20381;&#36182;&#24615;&#12290;&#25552;&#20986;&#30340;&#21151;&#33021;&#26368;&#22823;&#30456;&#20851;&#31639;&#27861;&#65288;FMCA&#65289;&#24212;&#29992;&#20110;&#30001;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#19978;&#65292;&#36890;&#36807;&#36924;&#36817;&#32852;&#21512;&#35757;&#32451;&#26469;&#21516;&#26102;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel multivariate definition of statistical dependence between two continuous random processes (r.p.) using a functional methodology inspired by Alfr\'ed R\'enyi. The argument of the logarithm of mutual information between pairs of samples of a r.p., named here the normalized cross density (NCD), defines a symmetric and self-adjoint positive definite function. We show that maximizing the alternating covariance estimation (ACE) recursion, applied to each of the joint probability density of input sample pairs, obeys all the properties of Renyi's maximal correlation. We propose the NCD's eigenspectrum as a novel multivariate measure of the statistical dependence between the input and output r.p.  The multivariate statistical dependence can also be estimated directly from r.p. realizations. The proposed functional maximum correlation algorithm (FMCA) is applied to a machine learning architecture built from two neural networks that learn concurrently by approximating 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Copula &#32852;&#21512;&#39044;&#27979;&#31639;&#27861; CopulaCPTS&#65292;&#29992;&#20110;&#22810;&#20803;&#12289;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#20854;&#32622;&#20449;&#21306;&#38388;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#31934;&#20934;&#21644;&#26356;&#38160;&#21033;&#12290;</title><link>http://arxiv.org/abs/2212.03281</link><description>&lt;p&gt;
Copula&#32852;&#21512;&#39044;&#27979;&#29992;&#20110;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Copula Conformal Prediction for Multi-step Time Series Forecasting. (arXiv:2212.03281v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Copula &#32852;&#21512;&#39044;&#27979;&#31639;&#27861; CopulaCPTS&#65292;&#29992;&#20110;&#22810;&#20803;&#12289;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#20854;&#32622;&#20449;&#21306;&#38388;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#31934;&#20934;&#21644;&#26356;&#38160;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26159;&#26500;&#24314;&#24378;&#22823;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#25311;&#21512;&#39044;&#27979;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26080;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#31639;&#27861;&#65292;&#22240;&#20854;&#26131;&#20110;&#23454;&#29616;&#12289;&#32479;&#35745;&#35206;&#30422;&#20445;&#35777;&#21644;&#23545;&#24213;&#23618;&#39044;&#27979;&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#32622;&#20449;&#39044;&#27979;&#31639;&#27861;&#20165;&#38480;&#20110;&#21333;&#27493;&#39044;&#27979;&#65292;&#26410;&#32771;&#34385;&#26102;&#24207;&#20381;&#36182;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181; Copula &#32852;&#21512;&#39044;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#20803;&#12289;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979; CopulaCPTS&#12290;&#25105;&#20204;&#35777;&#26126;&#20102; CopulaCPTS &#20855;&#26377;&#26377;&#38480;&#30340;&#26679;&#26412;*&#26377;&#25928;&#24615;&#20445;&#35777;&#12290;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; CopulaCPTS &#30340;&#22810;&#27493;&#39044;&#27979;&#21487;&#20135;&#29983;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#31934;&#20934;&#21644;&#26356;&#38160;&#21033;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate uncertainty measurement is a key step to building robust and reliable machine learning systems. Conformal prediction is a distribution-free uncertainty quantification algorithm popular for its ease of implementation, statistical coverage guarantees, and versatility for underlying forecasters. However, existing conformal prediction algorithms for time series are limited to single-step prediction without considering the temporal dependency. In this paper we propose a Copula Conformal Prediction algorithm for multivariate, multi-step Time Series forecasting, CopulaCPTS. We prove that CopulaCPTS has finite sample validity guarantee. On several synthetic and real-world multivariate time series datasets, we show that CopulaCPTS produces more calibrated and sharp confidence intervals for multi-step prediction tasks than existing techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#24773;&#20917;&#19979;&#30340;&#20114;&#34917;&#22797;&#21512;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26399;&#26395;&#36229;&#39069;&#39118;&#38505;&#30028;&#38480;&#21644;&#39640;&#27010;&#29575;&#36229;&#39069;&#39118;&#38505;&#30028;&#38480;&#30340;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31867;&#20855;&#26377;&#37325;&#35201;&#29702;&#35770;&#21644;&#23454;&#38469;&#24847;&#20041;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.01758</link><description>&lt;p&gt;
&#38543;&#26426;&#20114;&#34917;&#22797;&#21512;&#26368;&#23567;&#21270;&#30340;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Algorithms for Stochastic Complementary Composite Minimization. (arXiv:2211.01758v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#24773;&#20917;&#19979;&#30340;&#20114;&#34917;&#22797;&#21512;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26399;&#26395;&#36229;&#39069;&#39118;&#38505;&#30028;&#38480;&#21644;&#39640;&#27010;&#29575;&#36229;&#39069;&#39118;&#38505;&#30028;&#38480;&#30340;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31867;&#20855;&#26377;&#37325;&#35201;&#29702;&#35770;&#21644;&#23454;&#38469;&#24847;&#20041;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#24773;&#20917;&#19979;&#30340;&#20114;&#34917;&#22797;&#21512;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#23545;&#24212;&#20110;&#26368;&#23567;&#21270;&#19968;&#20010;&#65288;&#24369;&#65289;&#24179;&#28369;&#20989;&#25968;&#19982;&#19968;&#20010;&#20855;&#26377;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#30340;&#32467;&#26500;&#21270;&#22343;&#21248;&#20984;&#65288;&#21487;&#33021;&#26159;&#38750;&#24179;&#28369;&#21644;&#38750;Lipschitz&#65289;&#27491;&#21017;&#21270;&#39033;&#20043;&#21644;&#12290;&#23613;&#31649;&#24050;&#32463;&#23545;&#30456;&#20851;&#30340;&#35774;&#32622;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#24037;&#20316;&#65292;&#20294;&#22312;&#25105;&#20204;&#24037;&#20316;&#20043;&#21069;&#65292;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#30028;&#38480;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#26032;&#30340;&#26399;&#26395;&#36229;&#39069;&#39118;&#38505;&#30028;&#38480;&#21644;&#39640;&#27010;&#29575;&#36229;&#39069;&#39118;&#38505;&#30028;&#38480;&#26469;&#22635;&#34917;&#20102;&#36825;&#20010;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#65292;&#36825;&#19968;&#28857;&#25105;&#20204;&#36890;&#36807;&#23545;&#36825;&#31867;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#36739;&#20302;&#30028;&#38480;&#30340;&#26032;&#35777;&#26126;&#26469;&#35777;&#26126;&#12290;&#25105;&#20204;&#26368;&#21518;&#36890;&#36807;&#25552;&#20379;&#25968;&#20540;&#32467;&#26524;&#26469;&#27604;&#36739;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by regularization techniques in statistics and machine learning, we study complementary composite minimization in the stochastic setting. This problem corresponds to the minimization of the sum of a (weakly) smooth function endowed with a stochastic first-order oracle, and a structured uniformly convex (possibly nonsmooth and non-Lipschitz) regularization term. Despite intensive work on closely related settings, prior to our work no complexity bounds for this problem were known. We close this gap by providing novel excess risk bounds, both in expectation and with high probability. Our algorithms are nearly optimal, which we prove via novel lower complexity bounds for this class of problems. We conclude by providing numerical results comparing our methods to the state of the art.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#35757;&#32451;&#26041;&#27861;&#65292;&#22522;&#20110;&#21516;&#27493;&#21644;&#21516;&#20262;&#20248;&#21270;&#65292;&#21487;&#20197;&#29992;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#21160;&#21147;&#23398;&#35268;&#24459;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#20462;&#25913;&#12290;</title><link>http://arxiv.org/abs/2210.01407</link><description>&lt;p&gt;
&#22522;&#20110;&#21516;&#27493;&#21644;&#21516;&#20262;&#20248;&#21270;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#31934;&#30830;&#21160;&#21147;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Homotopy-based training of NeuralODEs for accurate dynamics discovery. (arXiv:2210.01407v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#35757;&#32451;&#26041;&#27861;&#65292;&#22522;&#20110;&#21516;&#27493;&#21644;&#21516;&#20262;&#20248;&#21270;&#65292;&#21487;&#20197;&#29992;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#21160;&#21147;&#23398;&#35268;&#24459;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NeuralODEs&#65289;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#21644;&#29289;&#29702;&#31185;&#23398;&#22522;&#20110;&#24494;&#20998;&#26041;&#31243;&#30340;&#24314;&#27169;&#33539;&#24335;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#26159;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#21160;&#21147;&#23398;&#35268;&#24459;&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#20986;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#21644;&#27425;&#20248;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#38271;&#26102;&#38388;&#27573;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21516;&#27493;&#21644;&#21516;&#20262;&#20248;&#21270;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#35757;&#32451;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#25913;&#21464;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#27169;&#22411;&#21160;&#21147;&#23398;&#21644;&#35757;&#32451;&#25968;&#25454;&#21516;&#27493;&#21487;&#20197;&#39535;&#26381;&#21407;&#26412;&#19981;&#35268;&#21017;&#30340;&#25439;&#22833;&#21644;&#30340;&#26223;&#35937;&#65292;&#21516;&#20262;&#20248;&#21270;&#21487;&#20197;&#21033;&#29992;&#36825;&#19968;&#28857;&#26469;&#22686;&#24378;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Ordinary Differential Equations (NeuralODEs) present an attractive way to extract dynamical laws from time series data, as they bridge neural networks with the differential equation-based modeling paradigm of the physical sciences. However, these models often display long training times and suboptimal results, especially for longer duration data. While a common strategy in the literature imposes strong constraints to the NeuralODE architecture to inherently promote stable model dynamics, such methods are ill-suited for dynamics discovery as the unknown governing equation is not guaranteed to satisfy the assumed constraints. In this paper, we develop a new training method for NeuralODEs, based on synchronization and homotopy optimization, that does not require changes to the model architecture. We show that synchronizing the model dynamics and the training data tames the originally irregular loss landscape, which homotopy optimization can then leverage to enhance training. Throug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38024;&#23545;COVID-19&#24739;&#32773;&#30340;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#65306;Outcome-specific length-of-stay prediction &#21644; Early mortality prediction&#65292;&#26088;&#22312;&#22635;&#34917;&#20020;&#24202;&#24212;&#29992;&#21644;&#20256;&#32479;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#27604;&#36739;&#21508;&#31181;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2209.07805</link><description>&lt;p&gt;
&#19968;&#39033;&#20351;&#29992;&#30005;&#23376;&#30149;&#21382;&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#36827;&#34892; COVID-19 &#39044;&#27979;&#24314;&#27169;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Benchmark for COVID-19 Predictive Modeling Using Electronic Health Records in Intensive Care. (arXiv:2209.07805v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38024;&#23545;COVID-19&#24739;&#32773;&#30340;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#65306;Outcome-specific length-of-stay prediction &#21644; Early mortality prediction&#65292;&#26088;&#22312;&#22635;&#34917;&#20020;&#24202;&#24212;&#29992;&#21644;&#20256;&#32479;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#27604;&#36739;&#21508;&#31181;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19 &#30123;&#24773;&#32473;&#20840;&#29699;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#24102;&#26469;&#20102;&#27785;&#37325;&#30340;&#36127;&#25285;&#65292;&#36896;&#25104;&#24040;&#22823;&#30340;&#31038;&#20250;&#30772;&#22351;&#21644;&#32463;&#27982;&#25439;&#22833;&#12290;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#34987;&#25552;&#20986;&#26469;&#20351;&#29992;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#36827;&#34892;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#65292;&#20363;&#22914; COVID-19 &#24739;&#32773;&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#30340;&#27515;&#20129;&#39118;&#38505;&#39044;&#27979;&#12290;&#23613;&#31649;&#22312;&#26576;&#20123;&#20020;&#24202;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#21021;&#27493;&#30340;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#21487;&#20844;&#24179;&#27604;&#36739;&#21508;&#31181;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#65292;&#20197;&#20415;&#20026;&#23454;&#38469;&#20020;&#24202;&#20351;&#29992;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#39044;&#27979;&#20219;&#21153;&#30340;&#21046;&#23450;&#19982;&#37325;&#30151;&#30417;&#25252;&#23460;&#30340;&#23454;&#38469;&#20020;&#24202;&#23454;&#36341;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#65306;&#38024;&#23545; COVID-19 &#37325;&#30151;&#30417;&#25252;&#23460;&#24739;&#32773;&#30340;&#29305;&#23450;&#32467;&#26524;&#39044;&#27979;&#21644;&#26089;&#26399;&#27515;&#20129;&#39044;&#27979;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#26159;&#26681;&#25454;&#22825;&#30495;&#30340;&#20303;&#38498;&#26102;&#38388;&#21644;&#27515;&#20129;&#29575;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#35843;&#25972;&#30340;&#65292;&#20197;&#36866;&#24212; COVID-19 &#30340;&#20020;&#24202;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has posed a heavy burden to the healthcare system worldwide and caused huge social disruption and economic loss. Many deep learning models have been proposed to conduct clinical predictive tasks such as mortality prediction for COVID-19 patients in intensive care units using Electronic Health Record (EHR) data. Despite their initial success in certain clinical applications, there is currently a lack of benchmarking results to achieve a fair comparison so that we can select the optimal model for clinical use. Furthermore, there is a discrepancy between the formulation of traditional prediction tasks and real-world clinical practice in intensive care. To fill these gaps, we propose two clinical prediction tasks, Outcome-specific length-of-stay prediction and Early mortality prediction for COVID-19 patients in intensive care units. The two tasks are adapted from the naive length-of-stay and mortality prediction tasks to accommodate the clinical practice for COVID-19 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20250;&#21512;&#65292;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#22320;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#26143;&#38469;&#29289;&#20307;&#12290;&#23427;&#36890;&#36807;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#21644;&#35889;&#24402;&#19968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24341;&#23548;&#31574;&#30053;&#26469;&#25552;&#20379;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2208.04883</link><description>&lt;p&gt;
&#31070;&#32463;&#20250;&#21512;&#65306;&#38754;&#21521;&#26143;&#38469;&#29289;&#20307;&#30340;&#21487;&#38752;&#23548;&#33322;&#21644;&#25511;&#21046;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Neural-Rendezvous: Provably Robust Guidance and Control to Encounter Interstellar Objects. (arXiv:2208.04883v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20250;&#21512;&#65292;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#22320;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#26143;&#38469;&#29289;&#20307;&#12290;&#23427;&#36890;&#36807;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#21644;&#35889;&#24402;&#19968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24341;&#23548;&#31574;&#30053;&#26469;&#25552;&#20379;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26143;&#38469;&#29289;&#20307;&#65288;ISOs&#65289;&#24456;&#21487;&#33021;&#26159;&#19981;&#21487;&#26367;&#20195;&#30340;&#21407;&#22987;&#26448;&#26009;&#65292;&#22312;&#29702;&#35299;&#31995;&#22806;&#34892;&#26143;&#26143;&#31995;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36816;&#34892;&#36712;&#36947;&#38590;&#20197;&#32422;&#26463;&#65292;&#36890;&#24120;&#20855;&#26377;&#36739;&#39640;&#30340;&#20542;&#35282;&#21644;&#30456;&#23545;&#36895;&#24230;&#65292;&#20351;&#29992;&#20256;&#32479;&#30340;&#20154;&#22312;&#29615;&#36335;&#26041;&#27861;&#25506;&#32034;ISOs&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#20250;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23454;&#26102;&#20013;&#20197;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#30340;&#26041;&#24335;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#29289;&#20307;&#65292;&#21253;&#25324;ISOs&#12290;&#23427;&#22312;&#22522;&#20110;&#35889;&#24402;&#19968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24341;&#23548;&#31574;&#30053;&#20043;&#19978;&#20351;&#29992;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#65292;&#20854;&#20013;&#21442;&#25968;&#36890;&#36807;&#30452;&#25509;&#24809;&#32602;MPC&#29366;&#24577;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35843;&#20248;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#20250;&#21512;&#22312;&#39044;&#26399;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#19978;&#25552;&#20379;&#20102;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#65292;&#20854;&#35777;&#26126;&#21033;&#29992;&#20102;&#38543;&#26426;&#36882;&#22686;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interstellar objects (ISOs) are likely representatives of primitive materials invaluable in understanding exoplanetary star systems. Due to their poorly constrained orbits with generally high inclinations and relative velocities, however, exploring ISOs with conventional human-in-the-loop approaches is significantly challenging. This paper presents Neural-Rendezvous, a deep learning-based guidance and control framework for encountering fast-moving objects, including ISOs, robustly, accurately, and autonomously in real time. It uses pointwise minimum norm tracking control on top of a guidance policy modeled by a spectrally-normalized deep neural network, where its hyperparameters are tuned with a loss function directly penalizing the MPC state trajectory tracking error. We show that Neural-Rendezvous provides a high probability exponential bound on the expected spacecraft delivery error, the proof of which leverages stochastic incremental stability analysis. In particular, it is used to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32447;&#21452;&#23618;&#20248;&#21270;&#35774;&#32622;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#21452;&#23618;&#36951;&#25022;&#23450;&#20041;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#20809;&#28369;&#24615;&#30340;&#22312;&#32447;&#20132;&#26367;&#26102;&#38388;&#24179;&#22343;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#20851;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2207.02829</link><description>&lt;p&gt;
&#22312;&#32447;&#21452;&#23618;&#20248;&#21270;&#65306;&#22312;&#32447;&#20132;&#26367;&#26799;&#24230;&#26041;&#27861;&#30340;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Online Bilevel Optimization: Regret Analysis of Online Alternating Gradient Methods. (arXiv:2207.02829v5 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32447;&#21452;&#23618;&#20248;&#21270;&#35774;&#32622;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#21452;&#23618;&#36951;&#25022;&#23450;&#20041;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#20809;&#28369;&#24615;&#30340;&#22312;&#32447;&#20132;&#26367;&#26102;&#38388;&#24179;&#22343;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#20851;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32447;&#21452;&#23618;&#20248;&#21270;&#35774;&#32622;&#65292;&#20854;&#20013;&#20381;&#27425;&#36879;&#38706;&#19968;&#31995;&#21015;&#30340;&#26102;&#21464;&#21452;&#23618;&#38382;&#39064;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#24050;&#30693;&#30340;&#21333;&#23618;&#22312;&#32447;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#38480;&#21040;&#21452;&#23618;&#35774;&#32622;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;"&#21452;&#23618;&#36951;&#25022;"&#23450;&#20041;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#20809;&#28369;&#24615;&#30340;&#22312;&#32447;&#20132;&#26367;&#26102;&#38388;&#24179;&#22343;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20851;&#20110;&#20869;&#37096;&#21644;&#22806;&#37096;&#26368;&#23567;&#21270;&#24207;&#21015;&#30340;&#36335;&#24452;&#38271;&#24230;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an \textit{online bilevel optimization} setting in which a sequence of time-varying bilevel problems are revealed one after the other. We extend the known regret bounds for single-level online algorithms to the bilevel setting. Specifically, we provide new notions of \textit{bilevel regret}, develop an online alternating time-averaged gradient method that is capable of leveraging smoothness, and give regret bounds in terms of the path-length of the inner and outer minimizer sequences.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26631;&#20934;&#21270;&#27969;&#26469;&#35299;&#20915;&#36880;&#28176;&#39046;&#22495;&#36866;&#24212;&#20013;&#20013;&#38388;&#22495;&#26377;&#38480;&#19988;&#36317;&#31163;&#36739;&#22823;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20174;&#28304;&#22495;&#21040;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#23398;&#20064;&#30446;&#26631;&#22495;&#30340;&#20998;&#24067;&#21464;&#25442;&#12290;</title><link>http://arxiv.org/abs/2206.11492</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#20934;&#21270;&#27969;&#36827;&#34892;&#36880;&#28176;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Gradual Domain Adaptation via Normalizing Flows. (arXiv:2206.11492v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26631;&#20934;&#21270;&#27969;&#26469;&#35299;&#20915;&#36880;&#28176;&#39046;&#22495;&#36866;&#24212;&#20013;&#20013;&#38388;&#22495;&#26377;&#38480;&#19988;&#36317;&#31163;&#36739;&#22823;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20174;&#28304;&#22495;&#21040;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#23398;&#20064;&#30446;&#26631;&#22495;&#30340;&#20998;&#24067;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#26102;&#65292;&#20256;&#32479;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#25928;&#26524;&#19981;&#20339;&#12290;&#36880;&#28176;&#39046;&#22495;&#36866;&#24212;&#26159;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#23427;&#28041;&#21450;&#21033;&#29992;&#36880;&#28176;&#20174;&#28304;&#22495;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#30340;&#20013;&#38388;&#22495;&#12290;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#20551;&#35774;&#20013;&#38388;&#22495;&#30340;&#25968;&#37327;&#36739;&#22823;&#19988;&#30456;&#37051;&#22495;&#20043;&#38388;&#30340;&#36317;&#31163;&#36739;&#23567;&#65292;&#22240;&#27492;&#65292;&#28041;&#21450;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#30340;&#36880;&#28176;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#26159;&#21487;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36880;&#28176;&#33258;&#25105;&#35757;&#32451;&#23558;&#22833;&#36133;&#65292;&#22240;&#20026;&#20013;&#38388;&#22495;&#30340;&#25968;&#37327;&#26377;&#38480;&#19988;&#30456;&#37051;&#22495;&#20043;&#38388;&#30340;&#36317;&#31163;&#36739;&#22823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26631;&#20934;&#21270;&#27969;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20174;&#28304;&#22495;&#21040;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#23398;&#20064;&#30446;&#26631;&#22495;&#30340;&#20998;&#24067;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard domain adaptation methods do not work well when a large gap exists between the source and target domains. Gradual domain adaptation is one of the approaches used to address the problem. It involves leveraging the intermediate domain, which gradually shifts from the source domain to the target domain. In previous work, it is assumed that the number of intermediate domains is large and the distance between adjacent domains is small; hence, the gradual domain adaptation algorithm, involving self-training with unlabeled datasets, is applicable. In practice, however, gradual self-training will fail because the number of intermediate domains is limited and the distance between adjacent domains is large. We propose the use of normalizing flows to deal with this problem while maintaining the framework of unsupervised domain adaptation. The proposed method learns a transformation from the distribution of the target domain to the Gaussian mixture distribution via the source domain. We e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#24863;&#30693;&#30340;Weisfeiler-Lehman&#31639;&#27861;&#65292;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;NC-GNN&#26694;&#26550;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.02059</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#36793;&#30340;Weisfeiler-Lehman&#31639;&#27861;&#22686;&#24378;GNN
&lt;/p&gt;
&lt;p&gt;
Empowering GNNs via Edge-Aware Weisfeiler-Lehman Algorithm. (arXiv:2206.02059v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#24863;&#30693;&#30340;Weisfeiler-Lehman&#31639;&#27861;&#65292;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;NC-GNN&#26694;&#26550;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#34920;&#36798;&#33021;&#21147;&#34987;&#24050;&#30693;&#30340;&#19968;&#32500;Weisfeiler-Lehman (1-WL)&#31639;&#27861;&#19978;&#30028;&#25152;&#38480;&#21046;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;GNN&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#35201;&#20040;&#38656;&#35201;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;&#35201;&#20040;&#28041;&#21450;&#39640;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#25805;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#21487;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#34920;&#36798;&#21147;&#30340;GNN&#26694;&#26550;&#65292;&#20445;&#25345;&#20102;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32771;&#34385;&#37051;&#23621;&#20043;&#38388;&#30340;&#36793;&#32536;&#26469;&#25480;&#26435;1-WL&#36827;&#34892;&#22270;&#21516;&#26500;&#27979;&#35797;&#65292;&#20174;&#32780;&#20135;&#29983;NC-1-WL&#12290; NC-1-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#22312;&#29702;&#35770;&#19978;&#34987;&#26174;&#31034;&#20026;&#20005;&#26684;&#39640;&#20110;1-WL&#19988;&#20302;&#20110;3-WL&#12290;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NC-GNN&#26694;&#26550;&#20316;&#20026;NC-1-WL&#30340;&#21487;&#21306;&#20998;&#31070;&#32463;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#31616;&#21333;NC-GNN&#23454;&#29616;&#21487;&#35777;&#26126;&#19982;NC-1-WL&#19968;&#26679;&#24378;&#22823;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;NC-GNN&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing graph neural networks (GNNs) are known to have their expressiveness upper-bounded by 1-dimensional Weisfeiler-Lehman (1-WL) algorithm. To achieve more powerful GNNs, existing attempts either require ad hoc features, or involve operations that incur high time and space complexities. In this work, we propose a general and provably powerful GNN framework that preserves the scalability of the message passing scheme. In particular, we first propose to empower 1-WL for graph isomorphism test by considering edges among neighbors, giving rise to NC-1-WL. The expressiveness of NC-1-WL is shown to be strictly above 1-WL and below 3-WL theoretically. Further, we propose the NC-GNN framework as a differentiable neural version of NC-1-WL. Our simple implementation of NC-GNN is provably as powerful as NC-1-WL. Experiments demonstrate that our NC-GNN performs effectively and efficiently on various benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#26412;&#22320;&#37051;&#22495;&#30340;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#31232;&#30095;&#37319;&#26679;&#25968;&#25454;&#20013;&#39640;&#25928;&#37325;&#24314;MR&#22270;&#20687;&#65292;&#35813;&#25216;&#26415;&#20855;&#26377;&#36739;&#24378;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.00775</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#26412;&#22320;&#37051;&#22495;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20174;&#31232;&#30095;&#37319;&#26679;&#25968;&#25454;&#20013;&#37325;&#24314;MR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Adaptive Local Neighborhood-based Neural Networks for MR Image Reconstruction from Undersampled Data. (arXiv:2206.00775v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#26412;&#22320;&#37051;&#22495;&#30340;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#31232;&#30095;&#37319;&#26679;&#25968;&#25454;&#20013;&#39640;&#25928;&#37325;&#24314;MR&#22270;&#20687;&#65292;&#35813;&#25216;&#26415;&#20855;&#26377;&#36739;&#24378;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21307;&#23398;&#22270;&#20687;&#37325;&#24314;&#25216;&#26415;&#33268;&#21147;&#20110;&#20197;&#23613;&#21487;&#33021;&#20302;&#30340;&#25104;&#26412;&#21644;&#23545;&#24739;&#32773;&#20135;&#29983;&#26368;&#23567;&#19981;&#33391;&#24433;&#21709;&#30340;&#26041;&#24335;&#29983;&#25104;&#36866;&#29992;&#20110;&#20020;&#24202;&#20351;&#29992;&#30340;&#39640;&#36136;&#37327;&#21307;&#23398;&#22270;&#20687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#20174;&#31232;&#30095;&#37319;&#26679;&#30340;k&#31354;&#38388;&#25968;&#25454;&#20013;&#37325;&#24314;MR&#22270;&#20687;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#37325;&#24314;&#26102;&#36890;&#36807;&#23545;&#35757;&#32451;&#38598;&#30340;&#23567;&#21306;&#22495;&#36827;&#34892;&#36866;&#24212;&#24615;&#20272;&#35745;&#26469;&#24555;&#36895;&#20272;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25628;&#32034;&#19982;&#27979;&#35797;&#37325;&#24314;&#30456;&#20284;&#30340;&#25968;&#25454;&#38598;&#37051;&#23621;&#21644;&#35757;&#32451;&#36825;&#20123;&#37051;&#23621;&#19978;&#30340;&#23616;&#37096;&#32593;&#32476;&#65292;&#28982;&#21518;&#26356;&#26032;&#27979;&#35797;&#37325;&#24314;&#20043;&#38388;&#36827;&#34892;&#20132;&#26367;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#37325;&#24314;&#27169;&#22411;&#26159;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#19982;&#27491;&#22312;&#37325;&#24314;&#30340;&#22270;&#20687;&#30456;&#20284;&#30340;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#32780;&#19981;&#26159;&#22312;&#22823;&#35268;&#27169;&#22810;&#26679;&#30340;&#35757;&#32451;&#38598;&#19978;&#25311;&#21512;&#30340;&#65292;&#22240;&#27492;&#23427;&#23545;&#26032;&#30340;&#25195;&#25551;&#26356;&#20855;&#36866;&#24212;&#24615;&#12290;&#23427;&#36824;&#21487;&#20197;&#22788;&#29702;&#35757;&#32451;&#38598;&#20013;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent medical image reconstruction techniques focus on generating high-quality medical images suitable for clinical use at the lowest possible cost and with the fewest possible adverse effects on patients. Recent works have shown significant promise for reconstructing MR images from sparsely sampled k-space data using deep learning. In this work, we propose a technique that rapidly estimates deep neural networks directly at reconstruction time by fitting them on small adaptively estimated neighborhoods of a training set. In brief, our algorithm alternates between searching for neighbors in a data set that are similar to the test reconstruction, and training a local network on these neighbors followed by updating the test reconstruction. Because our reconstruction model is learned on a dataset that is in some sense similar to the image being reconstructed rather than being fit on a large, diverse training set, it is more adaptive to new scans. It can also handle changes in training set
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;PEAR&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#20010;&#33021;&#22815;&#38024;&#23545;&#26368;&#32456;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#26469;&#35745;&#31639;&#30446;&#26631;&#29992;&#25143;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#65292;&#28982;&#21518;&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#31639;&#27861;&#24178;&#39044;&#30340;&#32463;&#27982;&#23454;&#29992;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13743</link><description>&lt;p&gt;
&#24102;&#26377;&#20559;&#22909;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#31639;&#27861;&#24178;&#39044;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Personalized Algorithmic Recourse with Preference Elicitation. (arXiv:2205.13743v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13743
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;PEAR&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#20010;&#33021;&#22815;&#38024;&#23545;&#26368;&#32456;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#26469;&#35745;&#31639;&#30446;&#26631;&#29992;&#25143;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#65292;&#28982;&#21518;&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#31639;&#27861;&#24178;&#39044;&#30340;&#32463;&#27982;&#23454;&#29992;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#24178;&#39044;&#65288;AR&#65289;&#30340;&#38382;&#39064;&#26159;&#35745;&#31639;&#29992;&#25143;&#25191;&#34892;&#19968;&#31995;&#21015;&#25805;&#20316;&#20197;&#39072;&#35206;&#19981;&#33391;&#26426;&#22120;&#20915;&#31574;&#30340;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#30340;&#25805;&#20316;&#24207;&#21015;&#19981;&#24212;&#35813;&#23545;&#29992;&#25143;&#30340;&#23454;&#26045;&#25552;&#20986;&#36807;&#39640;&#30340;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;AR&#26041;&#27861;&#37117;&#20551;&#35774;&#25152;&#26377;&#29992;&#25143;&#30340;&#25805;&#20316;&#25104;&#26412;&#30456;&#21516;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#21521;&#26576;&#20123;&#29992;&#25143;&#25512;&#33616;&#26114;&#36149;&#30340;&#34917;&#25937;&#35745;&#21010;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEAR&#65292;&#36825;&#26159;&#19968;&#31181;&#39318;&#20010;&#21487;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#65292;&#20197;&#28385;&#36275;&#20219;&#20309;&#26368;&#32456;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;PEAR&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#21521;&#30446;&#26631;&#29992;&#25143;&#21457;&#20986;&#36873;&#25321;&#38598;&#26597;&#35810;&#26469;&#36845;&#20195;&#22320;&#25913;&#21892;&#23545;&#25805;&#20316;&#25104;&#26412;&#30340;&#20272;&#35745;&#20540;&#12290;&#36825;&#20123;&#26597;&#35810;&#30340;&#35745;&#31639;&#26159;&#36890;&#36807;&#26368;&#22823;&#21270;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#26469;&#35745;&#31639;&#30340;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#32771;&#34385;&#25104;&#26412;&#20272;&#35745;&#21644;&#29992;&#25143;&#21709;&#24212;&#19981;&#30830;&#23450;&#24615;&#30340;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#12290;PEAR&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#23454;&#29616;AR&#20219;&#21153;&#25152;&#38656;&#36798;&#25104;&#30446;&#26631;&#30340;&#20559;&#22909;&#65292;&#20197;&#21450;&#25191;&#34892;&#27599;&#20010;&#25805;&#20316;&#25152;&#28041;&#21450;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;AR&#20219;&#21153;&#26469;&#35780;&#20272;PEAR&#65292;&#24182;&#26174;&#31034;&#20854;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#25214;&#21040;&#20102;&#26356;&#20026;&#32463;&#27982;&#23454;&#29992;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#34917;&#25937;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Recourse (AR) is the problem of computing a sequence of actions that -- once performed by a user -- overturns an undesirable machine decision. It is paramount that the sequence of actions does not require too much effort for users to implement. Yet, most approaches to AR assume that actions cost the same for all users, and thus may recommend unfairly expensive recourse plans to certain users. Prompted by this observation, we introduce PEAR, the first human-in-the-loop approach capable of providing personalized algorithmic recourse tailored to the needs of any end-user. PEAR builds on insights from Bayesian Preference Elicitation to iteratively refine an estimate of the costs of actions by asking choice set queries to the target user. The queries themselves are computed by maximizing the Expected Utility of Selection, a principled measure of information gain accounting for uncertainty on both the cost estimate and the user's responses. PEAR integrates elicitation into a Rein
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36873;&#25321;&#26631;&#31614;&#19978;&#30340;&#22825;&#30495;&#36873;&#25321;&#23548;&#33268;&#20102;&#20302;&#20559;&#24046;&#21442;&#25968;&#20272;&#35745;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20351;&#29992;&#26377;&#24847;&#19981;&#26159;&#30495;&#23454;&#26631;&#31614;&#30340;&#35757;&#32451;&#26631;&#31614;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#30417;&#30563;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#27604;&#30417;&#30563;&#26041;&#27861;&#26356;&#20302;&#30340;&#20559;&#24046;&#21442;&#25968;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2205.05587</link><description>&lt;p&gt;
&#35757;&#32451;&#26631;&#31614;&#30340;&#36873;&#25321;&#24456;&#37325;&#35201;&#65306;&#22914;&#20309;&#26368;&#22909;&#22320;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23450;&#37327;MRI&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Choice of training label matters: how to best use deep learning for quantitative MRI parameter estimation. (arXiv:2205.05587v3 [physics.med-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36873;&#25321;&#26631;&#31614;&#19978;&#30340;&#22825;&#30495;&#36873;&#25321;&#23548;&#33268;&#20102;&#20302;&#20559;&#24046;&#21442;&#25968;&#20272;&#35745;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20351;&#29992;&#26377;&#24847;&#19981;&#26159;&#30495;&#23454;&#26631;&#31614;&#30340;&#35757;&#32451;&#26631;&#31614;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#30417;&#30563;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#27604;&#30417;&#30563;&#26041;&#27861;&#26356;&#20302;&#30340;&#20559;&#24046;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#23450;&#37327;MRI&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#31454;&#20105;&#24615;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#33258;&#30417;&#30563;&#26041;&#27861;&#26377;&#26102;&#34987;&#31216;&#20026;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#23427;&#20204;&#36890;&#24120;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#12290;&#32780;&#36804;&#20170;&#20026;&#27490;&#30340;&#30417;&#30563;&#26041;&#27861;&#21017;&#26159;&#22522;&#20110;&#24050;&#30693;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20004;&#31181;&#23398;&#20064;&#33539;&#24335;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#19981;&#21516;&#30340;&#20248;&#21183;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#33258;&#30417;&#30563;&#26041;&#27861;&#25552;&#20379;&#20102;&#27604;&#30417;&#30563;&#26041;&#27861;&#26356;&#20302;&#20559;&#24046;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;&#36825;&#20010;&#32467;&#26524;&#19982;&#30452;&#35273;&#30456;&#21453; - &#22312;&#29702;&#35770;&#19978;&#65292;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#19982;&#30417;&#30563;&#26631;&#31614;&#32467;&#21512;&#24212;&#35813;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30417;&#30563;&#26041;&#27861;&#30340;&#36825;&#31181;&#26126;&#26174;&#38480;&#21046;&#28304;&#20110;&#23545;&#35757;&#32451;&#26631;&#31614;&#30340;&#22825;&#30495;&#36873;&#25321;&#12290;&#36890;&#36807;&#35757;&#32451;&#19981;&#26159;&#30495;&#23454;&#26631;&#31614;&#30340;&#26631;&#31614;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20043;&#21069;&#19982;&#33258;&#30417;&#30563;&#30456;&#20851;&#32852;&#30340;&#20302;&#20559;&#24046;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) is gaining popularity as a parameter estimation method for quantitative MRI. A range of competing implementations have been proposed, relying on either supervised or self-supervised learning. Self-supervised approaches, sometimes referred to as unsupervised, have been loosely based on auto-encoders, whereas supervised methods have, to date, been trained on groundtruth labels. These two learning paradigms have been shown to have distinct strengths. Notably, self-supervised approaches have offered lower-bias parameter estimates than their supervised alternatives. This result is counterintuitive - incorporating prior knowledge with supervised labels should, in theory, lead to improved accuracy. In this work, we show that this apparent limitation of supervised approaches stems from the naive choice of groundtruth training labels. By training on labels which are deliberately not groundtruth, we show that the low-bias parameter estimation previously associated with self-su
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#21457;&#23637;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#24403;&#21069;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#65292;&#21516;&#26102;&#27604;&#36739;&#20102;&#36825;&#20123;&#27169;&#22411;&#19982;&#20854;&#20182;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.05173</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65306;&#32508;&#36848;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Anomaly Detection: A Survey and Outlook. (arXiv:2205.05173v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05173
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#21457;&#23637;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#24403;&#21069;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#65292;&#21516;&#26102;&#27604;&#36739;&#20102;&#36825;&#20123;&#27169;&#22411;&#19982;&#20854;&#20182;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#32593;&#32476;&#23433;&#20840;&#12289;&#37329;&#34701;&#21644;&#21307;&#30103;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#35782;&#21035;&#20559;&#31163;&#27491;&#24120;&#34892;&#20026;&#30340;&#27169;&#24335;&#25110;&#20107;&#20214;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26174;&#33879;&#22686;&#38271;&#20351;&#24471;&#22312;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#21457;&#23637;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#32508;&#36848;&#24403;&#21069;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#24182;&#35752;&#35770;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#36825;&#20123;&#27169;&#22411;&#19982;&#20854;&#20182;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#23545;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#26410;&#26469;&#26041;&#21521;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#21253;&#25324;&#24320;&#21457;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#31561;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) plays a crucial role in various domains, including cybersecurity, finance, and healthcare, by identifying patterns or events that deviate from normal behaviour. In recent years, significant progress has been made in this field due to the remarkable growth of deep learning models. Notably, the advent of self-supervised learning has sparked the development of novel AD algorithms that outperform the existing state-of-the-art approaches by a considerable margin. This paper aims to provide a comprehensive review of the current methodologies in self-supervised anomaly detection. We present technical details of the standard methods and discuss their strengths and drawbacks. We also compare the performance of these models against each other and other state-of-the-art anomaly detection models. Finally, the paper concludes with a discussion of future directions for self-supervised anomaly detection, including the development of more effective and efficient algorithms and t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24555;&#36895;&#21487;&#38752;&#30340;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#39030;&#28857;&#31995;&#32479;&#36827;&#34892;&#31283;&#23450;&#21270;&#25511;&#21046;&#12290;&#31163;&#32447;&#30340;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#26041;&#27861;&#30830;&#20445;&#20102;&#31995;&#32479;&#22312;&#21487;&#35843;&#25972;&#22823;&#23567;&#21644;&#25910;&#25947;&#36895;&#24230;&#30340;&#38598;&#21512;&#20869;&#20445;&#25345;&#26377;&#30028;&#12290;</title><link>http://arxiv.org/abs/2204.13209</link><description>&lt;p&gt;
&#36890;&#36807;&#24555;&#36895;&#21487;&#38752;&#30340;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#23545;&#22810;&#39030;&#28857;&#31995;&#32479;&#36827;&#34892;&#22362;&#22266;&#31283;&#23450;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust stabilization of polytopic systems via fast and reliable neural network-based approximations. (arXiv:2204.13209v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24555;&#36895;&#21487;&#38752;&#30340;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#39030;&#28857;&#31995;&#32479;&#36827;&#34892;&#31283;&#23450;&#21270;&#25511;&#21046;&#12290;&#31163;&#32447;&#30340;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#26041;&#27861;&#30830;&#20445;&#20102;&#31995;&#32479;&#22312;&#21487;&#35843;&#25972;&#22823;&#23567;&#21644;&#25910;&#25947;&#36895;&#24230;&#30340;&#38598;&#21512;&#20869;&#20445;&#25345;&#26377;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#24555;&#36895;&#21487;&#38752;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#36817;&#20284;&#20256;&#32479;&#31283;&#23450;&#25511;&#21046;&#22120;&#30340;&#35774;&#35745;&#65292;&#29992;&#20110;&#20855;&#26377;&#22810;&#39030;&#28857;&#19981;&#30830;&#23450;&#24615;&#30340;&#32447;&#24615;&#31995;&#32479;&#65292;&#21253;&#25324;&#20855;&#26377;&#21487;&#21464;&#32467;&#26500;&#21644;&#22522;&#20110;&#36873;&#25321;&#31574;&#30053;&#30340;&#25511;&#21046;&#24459;&#12290;&#22312;&#26368;&#36817;&#30340;&#21487;&#38752;&#25511;&#21046;&#26367;&#20195;&#35774;&#35745;&#26041;&#27861;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#36807;&#31243;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#24471;&#21040;&#30340;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#36817;&#20284;&#26367;&#20195;&#20256;&#32479;&#25511;&#21046;&#22120;&#26102;&#65292;&#35777;&#26126;&#32447;&#24615;&#19981;&#30830;&#23450;&#31995;&#32479;&#30340;&#38381;&#29615;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20805;&#20998;&#26465;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;ReLU&#22522;&#20110;&#20256;&#32479;&#25511;&#21046;&#22120;&#30340;&#29366;&#24577;&#21040;&#36755;&#20837;&#26144;&#23556;&#30340;&#26368;&#22351;&#24773;&#20917;&#36817;&#20284;&#35823;&#24046;&#65292;&#30830;&#20445;&#31995;&#32479;&#22312;&#21487;&#35843;&#25972;&#22823;&#23567;&#21644;&#25910;&#25947;&#36895;&#24230;&#30340;&#38598;&#21512;&#20869;&#26368;&#32456;&#26377;&#30028;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31163;&#32447;&#30340;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#31934;&#30830;&#35745;&#31639;&#35813;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the design of fast and reliable neural network (NN)-based approximations of traditional stabilizing controllers for linear systems with polytopic uncertainty, including control laws with variable structure and those based on a (minimal) selection policy. Building upon recent approaches for the design of reliable control surrogates with guaranteed structural properties, we develop a systematic procedure to certify the closed-loop stability and performance of a linear uncertain system when a trained rectified linear unit (ReLU)-based approximation replaces such traditional controllers. First, we provide a sufficient condition, which involves the worst-case approximation error between ReLU-based and traditional controller-based state-to-input mappings, ensuring that the system is ultimately bounded within a set with adjustable size and convergence rate. Then, we develop an offline, mixed-integer optimization-based method that allows us to compute that quantity exactly.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#36716;&#21464;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#24615;&#36136;&#20063;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20986;&#33258;&#21160;&#26816;&#27979;&#36328;&#27169;&#24577;&#19981;&#21327;&#35843;&#30340;&#25216;&#26415;&#65292;&#20294;&#20173;&#38754;&#20020;&#25361;&#25112;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26426;&#20250;&#20063;&#22312;&#31561;&#24453;&#30528;&#25366;&#25496;&#12290;</title><link>http://arxiv.org/abs/2203.13883</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65306;&#26041;&#27861;&#12289;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13883
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#36716;&#21464;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#24615;&#36136;&#20063;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20986;&#33258;&#21160;&#26816;&#27979;&#36328;&#27169;&#24577;&#19981;&#21327;&#35843;&#30340;&#25216;&#26415;&#65292;&#20294;&#20173;&#38754;&#20020;&#25361;&#25112;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26426;&#20250;&#20063;&#22312;&#31561;&#24453;&#30528;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20174;&#25991;&#26412;&#20026;&#20027;&#30340;&#35770;&#22363;&#36716;&#21521;&#22810;&#27169;&#24335;&#29615;&#22659;&#65292;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#34394;&#20551;&#20449;&#24687;&#30340;&#24615;&#36136;&#20063;&#30456;&#24212;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#21033;&#29992;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#35270;&#35273;&#27169;&#24577;&#26356;&#21463;&#29992;&#25143;&#38738;&#30544;&#21644;&#21560;&#24341;&#21147;&#30340;&#20107;&#23454;&#65292;&#20197;&#21450;&#25991;&#26412;&#20869;&#23481;&#26377;&#26102;&#34987;&#31895;&#30053;&#27983;&#35272;&#30340;&#24773;&#20917;&#65292;&#34394;&#20551;&#20449;&#24687;&#20256;&#25773;&#32773;&#26368;&#36817;&#24320;&#22987;&#38024;&#23545;&#27169;&#24577;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#36830;&#25509;&#65292;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20986;&#33258;&#21160;&#26816;&#27979;&#32593;&#39029;&#20869;&#23481;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36328;&#27169;&#24577;&#19981;&#21327;&#35843;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#20998;&#26512;&#12289;&#20998;&#31867;&#21644;&#35782;&#21035;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#23427;&#20204;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#20197;&#25581;&#31034;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#39046;&#22495;&#30340;&#26032;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
As social media platforms are evolving from text-based forums into multi-modal environments, the nature of misinformation in social media is also transforming accordingly. Taking advantage of the fact that visual modalities such as images and videos are more favorable and attractive to the users and textual contents are sometimes skimmed carelessly, misinformation spreaders have recently targeted contextual connections between the modalities e.g., text and image. Hence many researchers have developed automatic techniques for detecting possible cross-modal discordance in web-based content. We analyze, categorize and identify existing approaches in addition to challenges and shortcomings they face in order to unearth new research opportunities in the field of multi-modal misinformation detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20877;&#29983;&#31890;&#23376;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;RPTS&#65289;&#65292;&#36890;&#36807;&#37325;&#26032;&#29983;&#25104;&#36866;&#24212;&#30340;&#31890;&#23376;&#26469;&#35299;&#20915;&#27748;&#26222;&#26862;&#25277;&#26679;&#20013;&#31890;&#23376;&#26435;&#37325;&#25910;&#25947;&#20110;&#38646;&#30340;&#38382;&#39064;&#12290;RPTS&#22312;&#20195;&#34920;&#24615;&#36172;&#21338;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#28789;&#27963;&#24615;&#21644;&#25928;&#26524;&#30340;&#25552;&#21319;&#65292;&#21253;&#25324;&#23545;5G&#32593;&#32476;&#20999;&#29255;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2203.08082</link><description>&lt;p&gt;
&#20877;&#29983;&#31890;&#23376;&#27748;&#26222;&#26862;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Regenerative Particle Thompson Sampling. (arXiv:2203.08082v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.08082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20877;&#29983;&#31890;&#23376;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;RPTS&#65289;&#65292;&#36890;&#36807;&#37325;&#26032;&#29983;&#25104;&#36866;&#24212;&#30340;&#31890;&#23376;&#26469;&#35299;&#20915;&#27748;&#26222;&#26862;&#25277;&#26679;&#20013;&#31890;&#23376;&#26435;&#37325;&#25910;&#25947;&#20110;&#38646;&#30340;&#38382;&#39064;&#12290;RPTS&#22312;&#20195;&#34920;&#24615;&#36172;&#21338;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#28789;&#27963;&#24615;&#21644;&#25928;&#26524;&#30340;&#25552;&#21319;&#65292;&#21253;&#25324;&#23545;5G&#32593;&#32476;&#20999;&#29255;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20877;&#29983;&#31890;&#23376;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;RPTS&#65289;&#65292;&#36825;&#26159;&#27748;&#26222;&#26862;&#25277;&#26679;&#30340;&#19968;&#31181;&#28789;&#27963;&#21464;&#20307;&#12290;&#27748;&#26222;&#26862;&#25277;&#26679;&#26412;&#36523;&#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#32500;&#25252;&#36830;&#32493;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#65292;&#23427;&#24456;&#38590;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#12290;&#31890;&#23376;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;PTS&#65289;&#26159;&#27748;&#26222;&#26862;&#25277;&#26679;&#30340;&#19968;&#31181;&#36817;&#20284;&#26041;&#24335;&#65292;&#36890;&#36807;&#29992;&#31163;&#25955;&#20998;&#24067;&#26367;&#25442;&#22312;&#19968;&#32452;&#21152;&#26435;&#38745;&#24577;&#31890;&#23376;&#19978;&#25903;&#25345;&#30340;&#36830;&#32493;&#20998;&#24067;&#26469;&#33719;&#21462;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;PTS&#20013;&#65292;&#38500;&#20102;&#23569;&#25968;&#36866;&#24212;&#30340;&#31890;&#23376;&#20043;&#22806;&#65292;&#25152;&#26377;&#20854;&#20182;&#31890;&#23376;&#30340;&#26435;&#37325;&#37117;&#25910;&#25947;&#20110;&#38646;&#12290;RPTS&#22522;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65306;&#21024;&#38500;&#34928;&#20943;&#30340;&#19981;&#36866;&#24212;&#31890;&#23376;&#65292;&#24182;&#22312;&#36866;&#24212;&#30340;&#24184;&#23384;&#31890;&#23376;&#38468;&#36817;&#20877;&#29983;&#26032;&#31890;&#23376;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;PTS&#21040;RPTS&#30340;&#26222;&#36941;&#25913;&#36827;&#21644;RPTS&#22312;&#19968;&#32452;&#20195;&#34920;&#24615;&#36172;&#21338;&#38382;&#39064;&#20013;&#30340;&#28789;&#27963;&#24615;&#21644;&#21151;&#25928;&#65292;&#21253;&#25324;&#23545;5G&#32593;&#32476;&#20999;&#29255;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes regenerative particle Thompson sampling (RPTS), a flexible variation of Thompson sampling. Thompson sampling itself is a Bayesian heuristic for solving stochastic bandit problems, but it is hard to implement in practice due to the intractability of maintaining a continuous posterior distribution. Particle Thompson sampling (PTS) is an approximation of Thompson sampling obtained by simply replacing the continuous distribution by a discrete distribution supported at a set of weighted static particles. We observe that in PTS, the weights of all but a few fit particles converge to zero. RPTS is based on the heuristic: delete the decaying unfit particles and regenerate new particles in the vicinity of fit surviving particles. Empirical evidence shows uniform improvement from PTS to RPTS and flexibility and efficacy of RPTS across a set of representative bandit problems, including an application to 5G network slicing.
&lt;/p&gt;</description></item><item><title>SkipNode&#25552;&#20986;&#20102;&#19968;&#20010;&#25554;&#25300;&#24335;&#27169;&#22359;&#26469;&#32531;&#35299;&#28145;&#24230;&#22270;&#21367;&#31215;&#32593;&#32476;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#36890;&#36807;&#36339;&#36807;&#37096;&#20998;&#21367;&#31215;&#25805;&#20316;&#25928;&#26524;&#26174;&#33879;&#65292;&#26377;&#25928;&#25233;&#21046;&#36807;&#24230;&#24179;&#28369;&#21644;&#26799;&#24230;&#28040;&#22833;&#12290;</title><link>http://arxiv.org/abs/2112.11628</link><description>&lt;p&gt;
SkipNode: &#32531;&#35299;&#28145;&#24230;&#22270;&#21367;&#31215;&#32593;&#32476;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
SkipNode: On Alleviating Performance Degradation for Deep Graph Convolutional Networks. (arXiv:2112.11628v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11628
&lt;/p&gt;
&lt;p&gt;
SkipNode&#25552;&#20986;&#20102;&#19968;&#20010;&#25554;&#25300;&#24335;&#27169;&#22359;&#26469;&#32531;&#35299;&#28145;&#24230;&#22270;&#21367;&#31215;&#32593;&#32476;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#36890;&#36807;&#36339;&#36807;&#37096;&#20998;&#21367;&#31215;&#25805;&#20316;&#25928;&#26524;&#26174;&#33879;&#65292;&#26377;&#25928;&#25233;&#21046;&#36807;&#24230;&#24179;&#28369;&#21644;&#26799;&#24230;&#28040;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#22312;&#27169;&#22411;&#21152;&#28145;&#26102;&#23481;&#26131;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#20165;&#23558;&#24615;&#33021;&#36864;&#21270;&#24402;&#22240;&#20026;&#36807;&#24230;&#24179;&#28369;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#28145;&#24230;GCNs&#24615;&#33021;&#19979;&#38477;&#30340;&#26681;&#26412;&#21407;&#22240;&#65306;&#36807;&#24230;&#24179;&#28369;&#21644;&#26799;&#24230;&#28040;&#22833;&#26377;&#30528;&#30456;&#20114;&#21152;&#24378;&#30340;&#20316;&#29992;&#65292;&#22312;&#28145;&#24230;GCNs&#20013;&#23548;&#33268;&#24615;&#33021;&#36805;&#36895;&#24694;&#21270;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#21453;&#36807;&#24230;&#24179;&#28369;&#26041;&#27861;&#37117;&#22312;&#27169;&#22411;&#28145;&#24230;&#19978;&#25191;&#34892;&#23436;&#25972;&#21367;&#31215;&#25805;&#20316;&#12290;&#30001;&#20110;&#27169;&#22411;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#26080;&#27861;&#26377;&#25928;&#25269;&#25239;&#36807;&#24230;&#24179;&#28369;&#30340;&#25351;&#25968;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25554;&#25300;&#24335;&#27169;&#22359;Skipnode&#65292;&#26469;&#20811;&#26381;&#28145;&#24230;GCNs&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#22312;&#27599;&#20010;&#21367;&#31215;&#23618;&#20013;&#23545;&#22270;&#33410;&#28857;&#36827;&#34892;&#37319;&#26679;&#65292;&#36339;&#36807;&#21367;&#31215;&#25805;&#20316;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#26377;&#25928;&#25233;&#21046;&#36807;&#24230;&#24179;&#28369;&#21644;&#26799;&#24230;&#28040;&#22833;&#65292;&#22240;&#20026;&#65288;1&#65289;&#19981;&#23545;&#25152;&#26377;&#33410;&#28857;&#25191;&#34892;&#21367;&#31215;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Networks (GCNs) suffer from performance degradation when models go deeper. However, earlier works only attributed the performance degeneration to over-smoothing. In this paper, we conduct theoretical and experimental analysis to explore the fundamental causes of performance degradation in deep GCNs: over-smoothing and gradient vanishing have a mutually reinforcing effect that causes the performance to deteriorate more quickly in deep GCNs. On the other hand, existing anti-over-smoothing methods all perform full convolutions up to the model depth. They could not well resist the exponential convergence of over-smoothing due to model depth increasing. In this work, we propose a simple yet effective plug-and-play module, Skipnode, to overcome the performance degradation of deep GCNs. It samples graph nodes in each convolutional layer to skip the convolution operation. In this way, both over-smoothing and gradient vanishing can be effectively suppressed since (1) not all
&lt;/p&gt;</description></item><item><title>&#24191;&#20041;&#30340;&#31163;&#32676;&#26816;&#27979;&#30340;&#35843;&#26597;&#30740;&#31350;&#25506;&#35752;&#20102;&#31163;&#32676;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#19982;&#24322;&#24120;&#26816;&#27979;&#12289;&#26032;&#39062;&#24615;&#26816;&#27979;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#31561;&#38382;&#39064;&#30340;&#32852;&#31995;&#65292;&#23545;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#20855;&#26377;&#20851;&#38190;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2110.11334</link><description>&lt;p&gt;
&#24191;&#20041;&#30340;&#31163;&#32676;&#26816;&#27979;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalized Out-of-Distribution Detection: A Survey. (arXiv:2110.11334v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.11334
&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#30340;&#31163;&#32676;&#26816;&#27979;&#30340;&#35843;&#26597;&#30740;&#31350;&#25506;&#35752;&#20102;&#31163;&#32676;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#19982;&#24322;&#24120;&#26816;&#27979;&#12289;&#26032;&#39062;&#24615;&#26816;&#27979;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#31561;&#38382;&#39064;&#30340;&#32852;&#31995;&#65292;&#23545;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#20855;&#26377;&#20851;&#38190;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32676;&#26816;&#27979;&#23545;&#20110;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#24403;&#31995;&#32479;&#26816;&#27979;&#21040;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#19988;&#26080;&#27861;&#20316;&#20986;&#23433;&#20840;&#20915;&#31574;&#30340;&#24322;&#24120;&#22330;&#26223;&#25110;&#23545;&#35937;&#26102;&#65292;&#25105;&#20204;&#24076;&#26395;&#39550;&#39542;&#31995;&#32479;&#33021;&#21457;&#20986;&#35686;&#25253;&#24182;&#23558;&#25511;&#21046;&#20132;&#32473;&#20154;&#31867;&#12290;&#31163;&#32676;&#26816;&#27979;&#19968;&#35789;&#39318;&#27425;&#20986;&#29616;&#22312;2017&#24180;&#65292;&#33258;&#37027;&#20197;&#21518;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20174;&#22522;&#20110;&#20998;&#31867;&#30340;&#26041;&#27861;&#21040;&#22522;&#20110;&#23494;&#24230;&#21644;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#26041;&#27861;&#20116;&#33457;&#20843;&#38376;&#12290;&#21516;&#26102;&#65292;&#31163;&#32676;&#26816;&#27979;&#19982;&#24322;&#24120;&#26816;&#27979;&#12289;&#26032;&#39062;&#24615;&#26816;&#27979;&#12289;&#24320;&#25918;&#38598;&#35782;&#21035;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#20854;&#20182;&#38382;&#39064;&#23494;&#20999;&#30456;&#20851;&#12290;&#23613;&#31649;&#30446;&#26631;&#30456;&#21516;&#65292;&#20294;&#36825;&#20123;&#20027;&#39064;&#22312;&#23450;&#20041;&#21644;&#38382;&#39064;&#35774;&#32622;&#19978;&#26377;&#24494;&#22937;&#30340;&#24046;&#24322;&#65292;&#32463;&#24120;&#20351;&#35835;&#32773;&#21644;&#20174;&#19994;&#32773;&#24863;&#21040;&#22256;&#24785;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen during training time and cannot make a safe decision. The term, OOD detection, first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD), are closely related to OOD detection in terms of motivation and methodology. Despite common goals, these topics develop in isolation, and their subtle differences in definition and problem setting often confuse readers and practitioners. In this su
&lt;/p&gt;</description></item><item><title>DPGNN&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#27493;&#28040;&#24687;&#28304;&#12289;&#33410;&#28857;&#29305;&#23450;&#30340;&#28040;&#24687;&#36755;&#20986;&#21644;&#22810;&#31354;&#38388;&#28040;&#24687;&#20132;&#20114;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2110.07869</link><description>&lt;p&gt;
DPGNN: &#21452;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DPGNN: Dual-Perception Graph Neural Network for Representation Learning. (arXiv:2110.07869v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.07869
&lt;/p&gt;
&lt;p&gt;
DPGNN&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#27493;&#28040;&#24687;&#28304;&#12289;&#33410;&#28857;&#29305;&#23450;&#30340;&#28040;&#24687;&#36755;&#20986;&#21644;&#22810;&#31354;&#38388;&#28040;&#24687;&#20132;&#20114;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#22522;&#20110;&#22270;&#30340;&#20219;&#21153;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#20854;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#22270;&#19978;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#37117;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#22312;&#21333;&#19968;&#25299;&#25169;&#31354;&#38388;&#20013;&#36845;&#20195;&#22320;&#32858;&#21512;&#37051;&#23621;&#20449;&#24687;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21463;&#21040;&#19968;&#20123;&#32570;&#28857;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#28040;&#24687;&#28304;&#25193;&#23637;&#30340;&#19981;&#28789;&#27963;&#24615;&#12289;&#24573;&#35270;&#33410;&#28857;&#32423;&#28040;&#24687;&#36755;&#20986;&#30340;&#24046;&#24322;&#20197;&#21450;&#21333;&#19968;&#28040;&#24687;&#31354;&#38388;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#22522;&#20110;&#22810;&#27493;&#28040;&#24687;&#28304;&#30340;&#23646;&#24615;&#12289;&#33410;&#28857;&#29305;&#23450;&#30340;&#28040;&#24687;&#36755;&#20986;&#21644;&#22810;&#31354;&#38388;&#28040;&#24687;&#20132;&#20114;&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#23454;&#20363;&#21270;&#20026;&#19968;&#20010;&#21517;&#20026;DPGNN&#30340;&#21452;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#24212;&#29992;&#20102;&#19968;&#31181;&#33410;&#28857;&#23545;&#27493;&#39588;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#33410;&#28857;&#29305;&#23450;&#30340;&#22810;&#27493;&#37051;&#22495;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have drawn increasing attention in recent years and achieved remarkable performance in many graph-based tasks, especially in semi-supervised learning on graphs. However, most existing GNNs are based on the message-passing paradigm to iteratively aggregate neighborhood information in a single topology space. Despite their success, the expressive power of GNNs is limited by some drawbacks, such as inflexibility of message source expansion, negligence of node-level message output discrepancy, and restriction of single message space. To address these drawbacks, we present a novel message-passing paradigm, based on the properties of multi-step message source, node-specific message output, and multi-space message interaction. To verify its validity, we instantiate the new message-passing paradigm as a Dual-Perception Graph Neural Network (DPGNN), which applies a node-to-step attention mechanism to aggregate node-specific multi-step neighborhood information adapti
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#21160;&#24577;&#21830;&#21697;&#36873;&#25321;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#65292;&#21487;&#22312;&#26410;&#30693;&#38656;&#27714;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#24635;&#20307;&#39044;&#26399;&#25910;&#20837;&#12290;&#22312;&#22823;&#24211;&#23384;&#29615;&#22659;&#19979;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2106.01135</link><description>&lt;p&gt;
MNL-&#24102;&#26377;&#32972;&#21253;&#30340;&#21095;&#38598;&#25361;&#36873;&#38382;&#39064;&#65306;&#19968;&#31181;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
MNL-Bandit with Knapsacks: a near-optimal algorithm. (arXiv:2106.01135v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01135
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#21160;&#24577;&#21830;&#21697;&#36873;&#25321;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#65292;&#21487;&#22312;&#26410;&#30693;&#38656;&#27714;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#24635;&#20307;&#39044;&#26399;&#25910;&#20837;&#12290;&#22312;&#22823;&#24211;&#23384;&#29615;&#22659;&#19979;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#21160;&#24577;&#30340;&#21830;&#21697;&#36873;&#25321;&#38382;&#39064;&#65292;&#20854;&#20013;&#21334;&#26041;&#25317;&#26377;&#22266;&#23450;&#24211;&#23384;&#30340;N&#31181;&#21487;&#26367;&#20195;&#20135;&#21697;&#65292;&#24182;&#38754;&#20020;&#22312;T&#20010;&#26102;&#26399;&#20869;&#39034;&#24207;&#21040;&#36798;&#30340;&#26410;&#30693;&#38656;&#27714;&#12290;&#22312;&#27599;&#20010;&#26102;&#26399;&#65292;&#21334;&#26041;&#38656;&#35201;&#20915;&#23450;&#35201;&#21521;&#23458;&#25143;&#25552;&#20379;&#30340;&#20135;&#21697;&#32452;&#21512;&#65288;&#22522;&#25968;&#26368;&#22810;&#20026;K&#65289;&#12290;&#39038;&#23458;&#30340;&#21453;&#24212;&#36981;&#24490;&#20855;&#26377;&#21442;&#25968;v&#30340;&#26410;&#30693;&#22810;&#39033;&#24335;&#23545;&#25968;&#27169;&#22411;&#65288;MNL&#65289;&#12290;&#21334;&#26041;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#22266;&#23450;&#21021;&#22987;&#24211;&#23384;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#24635;&#20307;&#39044;&#26399;&#25910;&#20837;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#36798;&#21040;&#20102;$\tilde O\Big(K \sqrt{KN T}\Big(\sqrt{v_{\text{max}}} + \frac{1}{q_{\text{min}}}\text{OPT}\Big)\Big)$&#30340;&#36951;&#25022;&#20540;&#65292;&#22312;&#27169;&#22411;&#21442;&#25968;&#30340;&#19968;&#31181;&#28201;&#21644;&#20551;&#35774;&#19979;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#22312;&#39640;&#24211;&#23384;&#29615;&#22659;&#19979;&#36798;&#21040;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;$\tilde O(\sqrt{T})$&#36951;&#25022;&#20540;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#22522;&#20110;&#22522;&#20110;UCB&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a dynamic assortment selection problem where a seller has a fixed inventory of $N$ substitutable products and faces an unknown demand that arrives sequentially over $T$ periods. In each period, the seller needs to decide on the assortment of products (of cardinality at most $K$) to offer to the customers. The customer's response follows an unknown multinomial logit model (MNL) with parameters $v$. The goal of the seller is to maximize the total expected revenue given the fixed initial inventory of $N$ products. We give a policy that achieves a regret of $\tilde O\Big(K \sqrt{KN T}\Big(\sqrt{v_{\text{max}}} + \frac{1}{q_{\text{min}}}\text{OPT}\Big)\Big)$, where $v_{\text{max}}\leq 1$ is the maximum utility for any product and $q_{\text{min}}$ the minimum inventory level, under a mild assumption on the model parameters. In particular, our policy achieves a near-optimal $\tilde O(\sqrt{T})$ regret in a large-inventory setting.  Our policy builds upon the UCB-based approach for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#22312;&#32447;&#26031;&#22374;&#21464;&#20998;&#25512;&#29702;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#21644;&#25511;&#21046;&#36755;&#20837;&#30340;&#20998;&#24067;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2103.12890</link><description>&lt;p&gt;
&#25511;&#21046;&#19982;&#21160;&#21147;&#23398;&#38382;&#39064;&#30340;&#21452;&#22312;&#32447;&#26031;&#22374;&#21464;&#20998;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Dual Online Stein Variational Inference for Control and Dynamics. (arXiv:2103.12890v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.12890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#22312;&#32447;&#26031;&#22374;&#21464;&#20998;&#25512;&#29702;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#21644;&#25511;&#21046;&#36755;&#20837;&#30340;&#20998;&#24067;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26041;&#26696;&#22312;&#35768;&#22810;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31215;&#26497;&#32780;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#33021;&#22815;&#24212;&#23545;&#38750;&#32447;&#24615;&#31995;&#32479;&#21160;&#21147;&#23398;&#12289;&#32422;&#26463;&#21644;&#35266;&#27979;&#22122;&#22768;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#25511;&#21046;&#20998;&#24067;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#39640;&#24230;&#19981;&#30830;&#23450;&#21644;&#22797;&#26434;&#30340;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;MPC&#26694;&#26550;&#24517;&#39035;&#33021;&#22815;&#26681;&#25454;&#26368;&#26032;&#30340;&#27979;&#37327;&#32467;&#26524;&#36866;&#24212;&#31995;&#32479;&#21442;&#25968;&#30340;&#21464;&#21270;&#20998;&#24067;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#38544;&#24335;&#21464;&#20998;&#25512;&#29702;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#21644;&#25511;&#21046;&#36755;&#20837;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#26469;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#24418;&#24335;&#36827;&#34892;&#26356;&#26032;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#36817;&#20284;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#21518;&#39564;&#20998;&#24067;&#65292;&#36890;&#24120;&#20986;&#29616;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#29616;&#23454;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model predictive control (MPC) schemes have a proven track record for delivering aggressive and robust performance in many challenging control tasks, coping with nonlinear system dynamics, constraints, and observational noise. Despite their success, these methods often rely on simple control distributions, which can limit their performance in highly uncertain and complex environments. MPC frameworks must be able to accommodate changing distributions over system parameters, based on the most recent measurements. In this paper, we devise an implicit variational inference algorithm able to estimate distributions over model parameters and control inputs on-the-fly. The method incorporates Stein Variational gradient descent to approximate the target distributions as a collection of particles, and performs updates based on a Bayesian formulation. This enables the approximation of complex multi-modal posterior distributions, typically occurring in challenging and realistic robot navigation ta
&lt;/p&gt;</description></item></channel></rss>