<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>FlashDecoding++&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12289;&#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#21644;&#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2311.01282</link><description>&lt;p&gt;
FlashDecoding++: &#22312;GPU&#19978;&#21152;&#36895;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26356;&#24555;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01282
&lt;/p&gt;
&lt;p&gt;
FlashDecoding++&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12289;&#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#21644;&#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#26410;&#35299;&#20915;&#65306;(1) &#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12290;softmax&#25805;&#20316;&#38656;&#35201;&#21516;&#27493;&#26356;&#26032;&#27599;&#20010;&#37096;&#20998;softmax&#32467;&#26524;&#65292;&#23548;&#33268;LLM&#20013;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#24320;&#38144;&#22686;&#21152;&#32422;20%&#12290;(2) &#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#12290;&#22312;LLM&#25512;&#29702;&#20013;&#25191;&#34892;GEMM&#30340;&#30697;&#38453;&#24418;&#29366;&#26159;&#25153;&#24179;&#30340;&#65292;&#23548;&#33268;&#22312;&#20808;&#21069;&#30340;&#35774;&#35745;&#20013;&#22635;&#20805;&#38646;&#21518;&#35745;&#31639;&#26410;&#20805;&#20998;&#21033;&#29992;&#65292;&#24615;&#33021;&#25439;&#22833;&#36229;&#36807;50%&#12290;(3) &#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;LLM&#20013;&#30340;&#20869;&#26680;&#24615;&#33021;&#21462;&#20915;&#20110;&#19981;&#21516;&#30340;&#36755;&#20837;&#25968;&#25454;&#29305;&#24449;&#12289;&#30828;&#20214;&#37197;&#32622;&#31561;&#12290;&#21333;&#19968;&#21644;&#38745;&#24577;&#30340;&#25968;&#25454;&#27969;&#21487;&#33021;&#23548;&#33268;LLM&#25512;&#29702;&#20013;&#19981;&#21516;&#24418;&#29366;&#30340;GEMM&#30340;&#24615;&#33021;&#25439;&#22833;&#36798;&#21040;50.25%&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FlashDecoding++&#65292;&#19968;&#31181;&#24555;&#36895;&#25903;&#25345;&#20027;&#27969;LLM&#21644;&#30828;&#20214;&#21518;&#31471;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;FlashDecoding++&#23454;&#29616;&#20102;&#20197;&#19979;&#30446;&#26631;&#65306;
&lt;/p&gt;
&lt;p&gt;
As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and &gt;50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.  We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24471;&#20998;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25193;&#25955;&#25351;&#25968;&#31215;&#20998;&#22120;&#37319;&#26679;&#22120;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#20943;&#23567;&#31215;&#20998;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2311.00157</link><description>&lt;p&gt;
&#19968;&#20010;&#26356;&#24555;&#30340;&#25193;&#25955;&#25351;&#25968;&#31215;&#20998;&#22120;&#37319;&#26679;&#22120;&#30340;&#24471;&#20998;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score Normalization for a Faster Diffusion Exponential Integrator Sampler. (arXiv:2311.00157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00157
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24471;&#20998;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25193;&#25955;&#25351;&#25968;&#31215;&#20998;&#22120;&#37319;&#26679;&#22120;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#20943;&#23567;&#31215;&#20998;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24352;&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#24555;&#36895;&#29983;&#25104;&#26679;&#26412;&#30340;&#25193;&#25955;&#25351;&#25968;&#31215;&#20998;&#22120;&#37319;&#26679;&#22120;&#65288;DEIS&#65289;&#12290;&#23427;&#21033;&#29992;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#21322;&#32447;&#24615;&#29305;&#24615;&#26469;&#22823;&#22823;&#20943;&#23567;&#31215;&#20998;&#35823;&#24046;&#65292;&#24182;&#22312;&#20302;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#65288;NFEs&#65289;&#26102;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#24471;&#20998;&#20989;&#25968;&#37325;&#21442;&#25968;&#21270;&#65292;&#23427;&#36890;&#36807;&#20943;&#23569;&#22312;&#27599;&#20010;&#31215;&#20998;&#27493;&#39588;&#20013;&#20351;&#29992;&#22266;&#23450;&#24471;&#20998;&#20989;&#25968;&#20272;&#35745;&#32780;&#24341;&#36215;&#30340;&#31215;&#20998;&#35823;&#24046;&#12290;&#21407;&#22987;&#20316;&#32773;&#20351;&#29992;&#20102;&#29992;&#20110;&#22122;&#22768;&#39044;&#27979;&#35757;&#32451;&#30340;&#27169;&#22411;&#40664;&#35748;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21363;&#23558;&#24471;&#20998;&#20056;&#20197;&#26465;&#20214;&#27491;&#21521;&#22122;&#22768;&#20998;&#24067;&#30340;&#26631;&#20934;&#24046;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#36825;&#31181;&#24471;&#20998;&#21442;&#25968;&#21270;&#30340;&#32477;&#23545;&#24179;&#22343;&#20540;&#22312;&#22823;&#37096;&#20998;&#21453;&#21521;&#37319;&#26679;&#36807;&#31243;&#20013;&#25509;&#36817;&#24120;&#25968;&#65292;&#20294;&#22312;&#37319;&#26679;&#32467;&#26463;&#26102;&#23427;&#20250;&#36805;&#36895;&#21464;&#21270;&#12290;&#20026;&#20102;&#31616;&#21333;&#20462;&#22797;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23545;&#24471;&#20998;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65288;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Recently, zhang et al have proposed the Diffusion Exponential Integrator Sampler (DEIS) for fast generation of samples from Diffusion Models. It leverages the semi-linear nature of the probability flow ordinary differential equation (ODE) in order to greatly reduce integration error and improve generation quality at low numbers of function evaluations (NFEs). Key to this approach is the score function reparameterisation, which reduces the integration error incurred from using a fixed score function estimate over each integration step. The original authors use the default parameterisation used by models trained for noise prediction -- multiply the score by the standard deviation of the conditional forward noising distribution. We find that although the mean absolute value of this score parameterisation is close to constant for a large portion of the reverse sampling process, it changes rapidly at the end of sampling. As a simple fix, we propose to instead reparameterise the score (at in
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SERA&#30340;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#20869;&#22312;&#22870;&#21169;&#26469;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.19805</link><description>&lt;p&gt;
SERA&#65306;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#22870;&#21169;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
SERA:Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning. (arXiv:2310.19805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19805
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SERA&#30340;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#20869;&#22312;&#22870;&#21169;&#26469;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#28508;&#22312;&#24212;&#29992;&#26159;&#20351;&#29992;&#29616;&#26377;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#26469;&#21021;&#22987;&#21270;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#28982;&#21518;&#36827;&#34892;&#21518;&#32493;&#22312;&#32447;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23545;&#31163;&#32447;&#39044;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#24494;&#35843;&#24448;&#24448;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#31163;&#32447;&#20445;&#23432;&#26041;&#27861;&#38477;&#20302;&#20102;agent&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#22312;&#32447;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#22312;&#32447;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#22312;&#32447;&#24494;&#35843;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26679;&#26412;&#39640;&#25928;&#22870;&#21169;&#22686;&#24378;&#65288;SERA&#65289;&#30340;&#36890;&#29992;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#12290;SERA&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#25913;&#21892;&#22312;&#32447;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#38544;&#24335;&#22320;&#23454;&#29616;&#20102;&#29366;&#24577;&#36793;&#32536;&#21305;&#37197;&#65288;SMM&#65289;&#24182;&#24809;&#32602;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#29366;&#24577;&#34892;&#21160;&#65292;&#20174;&#32780;&#40723;&#21169;agent&#35206;&#30422;&#30446;&#26631;&#29366;&#24577;&#23494;&#24230;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prospective application of offline reinforcement learning (RL) involves initializing a pre-trained policy using existing static datasets for subsequent online fine-tuning. However, direct fine-tuning of the offline pre-trained policy often results in sub-optimal performance. A primary reason is that offline conservative methods diminish the agent's capability of exploration, thereby impacting online fine-tuning performance. To enhance exploration during online fine-tuning and thus enhance the overall online fine-tuning performance, we introduce a generalized reward augmentation framework called Sample Efficient Reward Augmentation (SERA). SERA aims to improve the performance of online fine-tuning by designing intrinsic rewards that encourage the agent to explore. Specifically, it implicitly implements State Marginal Matching (SMM) and penalizes out-of-distribution (OOD) state actions, thus encouraging agents to cover the target state density, and achieving better online fine-tuning r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#25239;&#24615;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;&#30340;&#38754;&#21521;Oracle&#39640;&#25928;&#30340;&#25918;&#26494;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#29992;&#31163;&#32447;&#20248;&#21270;Oracle&#26469;&#38477;&#20302;&#36951;&#25022;&#30028;&#38480;&#65292;&#24182;&#19988;&#22312;&#30028;&#38480;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36798;&#21040;&#20102;&#20808;&#21069;&#26368;&#20339;&#30028;&#38480;&#65292;&#24182;&#19982;&#21407;&#22987;&#30028;&#38480;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2310.19025</link><description>&lt;p&gt;
&#12298;&#19968;&#31181;&#25913;&#36827;&#30340;&#38754;&#21521;Oracle&#39640;&#25928;&#30340;&#23545;&#25239;&#24615;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;&#30340;&#25918;&#26494;&#26041;&#27861;&#12299;
&lt;/p&gt;
&lt;p&gt;
An Improved Relaxation for Oracle-Efficient Adversarial Contextual Bandits. (arXiv:2310.19025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19025
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#25239;&#24615;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;&#30340;&#38754;&#21521;Oracle&#39640;&#25928;&#30340;&#25918;&#26494;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#29992;&#31163;&#32447;&#20248;&#21270;Oracle&#26469;&#38477;&#20302;&#36951;&#25022;&#30028;&#38480;&#65292;&#24182;&#19988;&#22312;&#30028;&#38480;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36798;&#21040;&#20102;&#20808;&#21069;&#26368;&#20339;&#30028;&#38480;&#65292;&#24182;&#19982;&#21407;&#22987;&#30028;&#38480;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;Oracle&#39640;&#25928;&#30340;&#25918;&#26494;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#23545;&#25239;&#24615;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#26159;&#20174;&#24050;&#30693;&#20998;&#24067;&#20013;&#39034;&#24207;&#29420;&#31435;&#25277;&#21462;&#30340;&#65292;&#32780;&#25104;&#26412;&#24207;&#21015;&#21017;&#30001;&#22312;&#32447;&#23545;&#25163;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#19968;&#20010;$O(T^{\frac{2}{3}}(K\log(|\Pi|))^{\frac{1}{3}})$&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#24182;&#19988;&#27599;&#36718;&#26368;&#22810;&#35843;&#29992;$O(K)$&#27425;&#31163;&#32447;&#20248;&#21270;Oracle&#65292;&#20854;&#20013;$K$&#34920;&#31034;&#21160;&#20316;&#30340;&#25968;&#37327;&#65292;$T$&#34920;&#31034;&#36718;&#25968;&#65292;$\Pi$&#34920;&#31034;&#31574;&#30053;&#38598;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#25913;&#36827;Syrgkanis&#31561;&#20154;&#22312;NeurIPS 2016&#20013;&#33719;&#24471;&#30340;$O((TK)^{\frac{2}{3}}(\log(|\Pi|))^{\frac{1}{3}})$&#30028;&#38480;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20063;&#26159;&#19982;Langford&#21644;Zhang&#22312;NeurIPS 2007&#20013;&#20026;&#38543;&#26426;&#24773;&#20917;&#25552;&#20986;&#30340;&#21407;&#22987;&#30028;&#38480;&#30456;&#21305;&#37197;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an oracle-efficient relaxation for the adversarial contextual bandits problem, where the contexts are sequentially drawn i.i.d from a known distribution and the cost sequence is chosen by an online adversary. Our algorithm has a regret bound of $O(T^{\frac{2}{3}}(K\log(|\Pi|))^{\frac{1}{3}})$ and makes at most $O(K)$ calls per round to an offline optimization oracle, where $K$ denotes the number of actions, $T$ denotes the number of rounds and $\Pi$ denotes the set of policies. This is the first result to improve the prior best bound of $O((TK)^{\frac{2}{3}}(\log(|\Pi|))^{\frac{1}{3}})$ as obtained by Syrgkanis et al. at NeurIPS 2016, and the first to match the original bound of Langford and Zhang at NeurIPS 2007 which was obtained for the stochastic case.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#25442;&#22120;&#21644;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#21892;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#35265;&#25104;&#20687;&#21464;&#24322;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15952</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#24341;&#23548;&#25193;&#25955;&#21644;&#23884;&#22871;&#38598;&#25104;&#25913;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles. (arXiv:2310.15952v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#25442;&#22120;&#21644;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#21892;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#35265;&#25104;&#20687;&#21464;&#24322;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#23427;&#20204;&#23545;&#25152;&#33719;&#21462;&#30340;&#22270;&#20687;&#30340;&#21464;&#24322;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#35768;&#22810;&#26041;&#27861;&#20250;&#23545;&#35757;&#32451;&#25968;&#25454;&#24212;&#29992;&#39044;&#23450;&#20041;&#30340;&#36716;&#25442;&#65292;&#20197;&#22686;&#24378;&#27979;&#35797;&#26102;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#36825;&#20123;&#36716;&#25442;&#21487;&#33021;&#26080;&#27861;&#30830;&#20445;&#27169;&#22411;&#23545;&#24739;&#32773;&#22270;&#20687;&#20013;&#30340;&#22810;&#26679;&#24615;&#21464;&#24322;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#21644;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#19977;&#38454;&#27573;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#23454;&#36341;&#20013;&#24120;&#35265;&#30340;&#25104;&#20687;&#21464;&#24322;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#30830;&#23450;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#22810;&#20010;&#22270;&#20687;&#32534;&#30721;&#22120;&#39318;&#20808;&#23398;&#20064;&#20998;&#23618;&#29305;&#24449;&#34920;&#31034;&#26469;&#26500;&#24314;&#36776;&#21035;&#28508;&#22312;&#31354;&#38388;&#12290;&#25509;&#19979;&#26469;&#65292;&#19968;&#20010;&#30001;&#28508;&#22312;&#20195;&#30721;&#24341;&#23548;&#30340;&#36870;&#25193;&#25955;&#36807;&#31243;&#20316;&#29992;&#20110;&#26377;&#20449;&#24687;&#20808;&#39564;&#65292;&#24182;&#25552;&#20986;&#39044;&#27979;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning models have achieved remarkable success across a range of medical image analysis tasks, deployment of these models in real clinical contexts requires that they be robust to variability in the acquired images. While many methods apply predefined transformations to augment the training data to enhance test-time robustness, these transformations may not ensure the model's robustness to the diverse variability seen in patient images. In this paper, we introduce a novel three-stage approach based on transformers coupled with conditional diffusion models, with the goal of improving model robustness to the kinds of imaging variability commonly encountered in practice without the need for pre-determined data augmentation strategies. To this end, multiple image encoders first learn hierarchical feature representations to build discriminative latent spaces. Next, a reverse diffusion process, guided by the latent code, acts on an informative prior and proposes prediction candi
&lt;/p&gt;</description></item><item><title>ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13001</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65288;ConFIRM&#65289;
&lt;/p&gt;
&lt;p&gt;
Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13001
&lt;/p&gt;
&lt;p&gt;
ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#37329;&#34701;&#31561;&#19987;&#38376;&#39046;&#22495;&#30340;&#26032;&#20852;&#29305;&#24615;&#20855;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#31561;&#21463;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#29420;&#29305;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#38656;&#35201;&#20855;&#22791;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ConFIRM&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#21644;&#30693;&#35782;&#24211;&#26631;&#35760;&#12290;ConFIRM&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;2&#65289;&#35780;&#20272;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#22810;&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12290;ConFIRM&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#31526;&#21512;&#30417;&#31649;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;ConFIRM&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25552;&#21462;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#30340;&#31934;&#30830;&#26597;&#35810;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRIOR&#30340;&#20010;&#24615;&#21270;&#20808;&#39564;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#20840;&#23616;&#27169;&#22411;&#24573;&#35270;&#29305;&#23450;&#20449;&#24687;&#32780;&#23548;&#33268;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;&#38382;&#39064;&#12290;&#35813;&#26041;&#26696;&#20351;&#29992;&#24102;Bregman&#25955;&#24230;&#30340;PFL&#26694;&#26550;&#23558;&#20010;&#24615;&#21270;&#20808;&#39564;&#19982;&#26412;&#22320;&#30446;&#26631;&#20989;&#25968;&#35299;&#32806;&#65292;&#20197;&#25552;&#39640;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09183</link><description>&lt;p&gt;
PRIOR: &#20010;&#24615;&#21270;&#20808;&#39564;&#29992;&#20110;&#37325;&#26032;&#28608;&#27963;&#32852;&#37030;&#23398;&#20064;&#20013;&#34987;&#24573;&#35270;&#30340;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning. (arXiv:2310.09183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRIOR&#30340;&#20010;&#24615;&#21270;&#20808;&#39564;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#20840;&#23616;&#27169;&#22411;&#24573;&#35270;&#29305;&#23450;&#20449;&#24687;&#32780;&#23548;&#33268;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;&#38382;&#39064;&#12290;&#35813;&#26041;&#26696;&#20351;&#29992;&#24102;Bregman&#25955;&#24230;&#30340;PFL&#26694;&#26550;&#23558;&#20010;&#24615;&#21270;&#20808;&#39564;&#19982;&#26412;&#22320;&#30446;&#26631;&#20989;&#25968;&#35299;&#32806;&#65292;&#20197;&#25552;&#39640;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#24322;&#36136;&#25968;&#25454;&#29305;&#24615;&#38477;&#20302;&#20102;&#23616;&#37096;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#36890;&#36807;&#20174;&#20840;&#23616;&#27169;&#22411;&#20013;&#21512;&#25104;&#20010;&#24615;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20840;&#23616;&#27169;&#22411;&#21487;&#33021;&#24573;&#35270;&#20102;&#23458;&#25143;&#31471;&#34987;&#37319;&#26679;&#30340;&#29305;&#23450;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#20010;&#24615;&#21270;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20840;&#23616;&#27169;&#22411;&#20013;&#65292;&#35797;&#22270;&#20943;&#36731;PFL&#20013;&#24341;&#20837;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#21363;&#24102;Bregman&#25955;&#24230;&#65288;pFedBreD&#65289;&#30340;PFL&#65292;&#36890;&#36807;&#22312;&#20010;&#24615;&#21270;&#22330;&#26223;&#20013;&#20351;&#29992;Bregman&#25955;&#24230;&#27491;&#21017;&#21270;&#30340;&#26412;&#22320;&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#20010;&#24615;&#21270;&#20808;&#39564;&#19982;&#20043;&#35299;&#32806;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#36824;&#25918;&#26494;&#20102;&#38236;&#20687;&#19979;&#38477;&#65288;RMD&#65289;&#65292;&#20197;&#26174;&#24335;&#22320;&#25552;&#21462;&#20808;&#39564;&#30693;&#35782;&#65292;&#25552;&#20379;&#21487;&#36873;&#25321;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical federated learning (FL) enables training machine learning models without sharing data for privacy preservation, but heterogeneous data characteristic degrades the performance of the localized model. Personalized FL (PFL) addresses this by synthesizing personalized models from a global model via training on local data. Such a global model may overlook the specific information that the clients have been sampled. In this paper, we propose a novel scheme to inject personalized prior knowledge into the global model in each client, which attempts to mitigate the introduced incomplete information problem in PFL. At the heart of our proposed approach is a framework, the PFL with Bregman Divergence (pFedBreD), decoupling the personalized prior from the local objective function regularized by Bregman divergence for greater adaptability in personalized scenarios. We also relax the mirror descent (RMD) to extract the prior explicitly to provide optional strategies. Additionally, our pFed
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25511;&#21046;&#21464;&#37327;&#30340;&#26041;&#27861;&#31283;&#23450;Shapley&#20540;&#30340;&#20272;&#35745;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.07672</link><description>&lt;p&gt;
&#29992;&#25511;&#21046;&#21464;&#37327;&#31283;&#23450;Shapley&#20540;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Estimates of Shapley Values with Control Variates. (arXiv:2310.07672v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07672
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25511;&#21046;&#21464;&#37327;&#30340;&#26041;&#27861;&#31283;&#23450;Shapley&#20540;&#30340;&#20272;&#35745;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley&#20540;&#26159;&#35299;&#37322;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26368;&#27969;&#34892;&#30340;&#24037;&#20855;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#27492;&#37319;&#29992;&#25277;&#26679;&#36817;&#20284;&#26469;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#31283;&#23450;&#36825;&#20123;&#27169;&#22411;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#21464;&#37327;&#30340;&#33945;&#29305;&#21345;&#27931;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;ControlSHAP&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25110;&#24314;&#27169;&#24037;&#20316;&#12290;&#22312;&#22810;&#20010;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;Shapley&#20272;&#35745;&#30340;&#33945;&#29305;&#21345;&#27931;&#21464;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shapley values are among the most popular tools for explaining predictions of blackbox machine learning models. However, their high computational cost motivates the use of sampling approximations, inducing a considerable degree of uncertainty. To stabilize these model explanations, we propose ControlSHAP, an approach based on the Monte Carlo technique of control variates. Our methodology is applicable to any machine learning model and requires virtually no extra computation or modeling effort. On several high-dimensional datasets, we find it can produce dramatic reductions in the Monte Carlo variability of Shapley estimates.
&lt;/p&gt;</description></item><item><title>&#22312;gelu-4l&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#20869;&#23384;&#31649;&#29702;&#23545;&#20110;transformer&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35828;&#26126;&#20102;Direct Logit Attribution&#25216;&#26415;&#30340;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.07325</link><description>&lt;p&gt;
&#30452;&#25509;&#36923;&#36753;&#23646;&#24615;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65306;gelu-4l&#20013;&#30340;&#20869;&#23384;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l. (arXiv:2310.07325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07325
&lt;/p&gt;
&lt;p&gt;
&#22312;gelu-4l&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#20869;&#23384;&#31649;&#29702;&#23545;&#20110;transformer&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35828;&#26126;&#20102;Direct Logit Attribution&#25216;&#26415;&#30340;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;4&#23618;transformer&#20013;&#20869;&#23384;&#31649;&#29702;&#30340;&#20855;&#20307;&#35777;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#32452;&#20214;&#19968;&#33268;&#22320;&#31227;&#38500;&#21069;&#38754;&#32452;&#20214;&#30340;&#36755;&#20986;&#65292;&#36825;&#26159;&#19968;&#31181;&#28165;&#29702;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#37322;&#24615;&#25216;&#26415;Direct Logit Attribution&#25552;&#20379;&#20102;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26126;&#30830;&#30340;&#20363;&#23376;&#65292;&#35777;&#26126;&#36825;&#31181;&#25216;&#26415;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32771;&#34385;&#21040;&#28165;&#29702;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide concrete evidence for memory management in a 4-layer transformer. Specifically, we identify clean-up behavior, in which model components consistently remove the output of preceeding components during a forward pass. Our findings suggest that the interpretability technique Direct Logit Attribution provides misleading results. We show explicit examples where this technique is inaccurate, as it does not account for clean-up behavior.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;PAC-Bayesian&#29702;&#35770;&#20026;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#32479;&#35745;&#20445;&#35777;&#65292;&#21253;&#25324;&#23545;&#21518;&#39564;&#20998;&#24067;&#12289;&#37325;&#26500;&#25439;&#22833;&#21644;&#36755;&#20837;&#19982;&#29983;&#25104;&#20998;&#24067;&#20043;&#38388;&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.04935</link><description>&lt;p&gt;
&#20351;&#29992;PAC-Bayesian&#29702;&#35770;&#32473;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Statistical Guarantees for Variational Autoencoders using PAC-Bayesian Theory. (arXiv:2310.04935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04935
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;PAC-Bayesian&#29702;&#35770;&#20026;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#32479;&#35745;&#20445;&#35777;&#65292;&#21253;&#25324;&#23545;&#21518;&#39564;&#20998;&#24067;&#12289;&#37325;&#26500;&#25439;&#22833;&#21644;&#36755;&#20837;&#19982;&#29983;&#25104;&#20998;&#24067;&#20043;&#38388;&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#23427;&#20204;&#30340;&#38382;&#19990;&#20197;&#26469;&#65292;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#23427;&#20204;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20851;&#20110;&#23427;&#20204;&#30340;&#29702;&#35770;&#24615;&#36136;&#20173;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#12290;&#26412;&#25991;&#21033;&#29992;PAC-Bayesian&#29702;&#35770;&#20026;VAEs&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22522;&#20110;&#29420;&#31435;&#26679;&#26412;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#39318;&#20010;PAC-Bayesian&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#20026;VAE&#30340;&#37325;&#26500;&#25439;&#22833;&#25552;&#20379;&#20102;&#27867;&#21270;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#36755;&#20837;&#20998;&#24067;&#19982;VAE&#29983;&#25104;&#27169;&#22411;&#23450;&#20041;&#30340;&#20998;&#24067;&#20043;&#38388;&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36755;&#20837;&#20998;&#24067;&#19982;VAE&#29983;&#25104;&#27169;&#22411;&#23450;&#20041;&#30340;&#20998;&#24067;&#20043;&#38388;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their inception, Variational Autoencoders (VAEs) have become central in machine learning. Despite their widespread use, numerous questions regarding their theoretical properties remain open. Using PAC-Bayesian theory, this work develops statistical guarantees for VAEs. First, we derive the first PAC-Bayesian bound for posterior distributions conditioned on individual samples from the data-generating distribution. Then, we utilize this result to develop generalization guarantees for the VAE's reconstruction loss, as well as upper bounds on the distance between the input and the regenerated distributions. More importantly, we provide upper bounds on the Wasserstein distance between the input distribution and the distribution defined by the VAE's generative model.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#21152;&#36895;&#22522;&#22240;&#32534;&#36753;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#32534;&#36753;&#32467;&#26524;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02919</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#30897;&#22522;&#32534;&#36753;&#32467;&#26524;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Attention-based Multi-task Learning for Base Editor Outcome Prediction. (arXiv:2310.02919v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02919
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#21152;&#36895;&#22522;&#22240;&#32534;&#36753;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#32534;&#36753;&#32467;&#26524;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36951;&#20256;&#30142;&#30149;&#36890;&#24120;&#30001;&#28857;&#31361;&#21464;&#24341;&#36215;&#65292;&#36825;&#20984;&#26174;&#20102;&#23545;&#31934;&#30830;&#22522;&#22240;&#32452;&#32534;&#36753;&#25216;&#26415;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#20854;&#20013;&#65292;&#30897;&#22522;&#32534;&#36753;&#20197;&#20854;&#33021;&#22815;&#22312;&#21333;&#20010;&#26680;&#33527;&#37240;&#27700;&#24179;&#19978;&#36827;&#34892;&#23450;&#21521;&#25913;&#21464;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#20854;&#20020;&#24202;&#24212;&#29992;&#21463;&#21040;&#32534;&#36753;&#25928;&#29575;&#20302;&#21644;&#38750;&#39044;&#26399;&#31361;&#21464;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#22312;&#23454;&#39564;&#23460;&#36827;&#34892;&#22823;&#37327;&#30340;&#35797;&#38169;&#23454;&#39564;&#12290;&#20026;&#20102;&#21152;&#36895;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20004;&#38454;&#27573;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#39044;&#27979;&#32473;&#23450;&#22522;&#22240;&#32452;&#30446;&#26631;&#24207;&#21015;&#30340;&#25152;&#26377;&#21487;&#33021;&#32534;&#36753;&#32467;&#26524;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#24335;&#65292;&#21516;&#26102;&#23398;&#20064;&#22810;&#31181;&#30897;&#22522;&#32534;&#36753;&#22120;&#65288;&#21363;&#21464;&#20307;&#65289;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#30897;&#22522;&#32534;&#36753;&#22120;&#21464;&#20307;&#19978;&#22987;&#32456;&#34920;&#29616;&#20986;&#19982;&#23454;&#38469;&#23454;&#39564;&#32467;&#26524;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#27169;&#22411;&#25913;&#36827;&#21644;&#21152;&#36895;&#22522;&#22240;&#32534;&#36753;&#35774;&#35745;&#36807;&#31243;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human genetic diseases often arise from point mutations, emphasizing the critical need for precise genome editing techniques. Among these, base editing stands out as it allows targeted alterations at the single nucleotide level. However, its clinical application is hindered by low editing efficiency and unintended mutations, necessitating extensive trial-and-error experimentation in the laboratory. To speed up this process, we present an attention-based two-stage machine learning model that learns to predict the likelihood of all possible editing outcomes for a given genomic target sequence. We further propose a multi-task learning schema to jointly learn multiple base editors (i.e. variants) at once. Our model's predictions consistently demonstrated a strong correlation with the actual experimental results on multiple datasets and base editor variants. These results provide further validation for the models' capacity to enhance and accelerate the process of refining base editing desig
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#20984;&#25903;&#37197;&#65288;PCM&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25674;&#38144;&#20248;&#21270;&#20013;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#36924;&#36817;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20984;&#20248;&#21270;&#33719;&#24471;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.02519</link><description>&lt;p&gt;
&#21442;&#21442;&#25968;&#21270;&#20984;&#25903;&#37197;&#27861;&#29992;&#20110;&#25674;&#38144;&#20248;&#21270;&#20013;&#30446;&#26631;&#20989;&#25968;&#30340;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Parameterized Convex Minorant for Objective Function Approximation in Amortized Optimization. (arXiv:2310.02519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02519
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#20984;&#25903;&#37197;&#65288;PCM&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25674;&#38144;&#20248;&#21270;&#20013;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#36924;&#36817;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20984;&#20248;&#21270;&#33719;&#24471;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#20984;&#25903;&#37197;&#65288;PCM&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25674;&#38144;&#20248;&#21270;&#20013;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#22120;&#30001;PCM&#21644;&#38750;&#36127;&#38388;&#38553;&#20989;&#25968;&#20043;&#21644;&#34920;&#31034;&#65292;&#20854;&#20013;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#22120;&#22312;&#20248;&#21270;&#21464;&#37327;&#19978;&#30001;PCM&#20984;&#20989;&#25968;&#19979;&#30028;&#32422;&#26463;&#12290;&#25152;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#22120;&#26159;&#36830;&#32493;&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;PCM&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#36798;&#21040;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20984;&#20248;&#21270;&#33719;&#21462;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#20316;&#20026;&#25152;&#25552;&#26041;&#27861;&#30340;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;&#30340;&#21442;&#25968;&#21270;&#23545;&#25968;&#21644;&#25351;&#25968;&#32593;&#32476;&#65292;&#21033;&#29992;&#21442;&#25968;&#21270;&#30340;&#23545;&#25968;&#21644;&#25351;&#25968;&#32593;&#32476;&#20316;&#20026;PCM&#12290;&#23545;&#20110;&#38750;&#21442;&#25968;&#21270;&#20984;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#36827;&#34892;&#20102;&#25968;&#20540;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameterized convex minorant (PCM) method is proposed for the approximation of the objective function in amortized optimization. In the proposed method, the objective function approximator is expressed by the sum of a PCM and a nonnegative gap function, where the objective function approximator is bounded from below by the PCM convex in the optimization variable. The proposed objective function approximator is a universal approximator for continuous functions, and the global minimizer of the PCM attains the global minimum of the objective function approximator. Therefore, the global minimizer of the objective function approximator can be obtained by a single convex optimization. As a realization of the proposed method, extended parameterized log-sum-exp network is proposed by utilizing a parameterized log-sum-exp network as the PCM. Numerical simulation is performed for non-parameterized-convex objective function approximation and for learning-based nonlinear model predictive control 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20805;&#20998;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#30340;&#25216;&#24039;&#65292;&#21253;&#25324;&#23567;&#25209;&#37327;&#24402;&#19968;&#21270;&#12289;&#27969;&#24179;&#34913;&#12289;&#21487;&#38752;&#26679;&#26412;&#36873;&#25321;&#21644;&#32593;&#32476;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#25216;&#26415;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#33021;&#21147;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02416</link><description>&lt;p&gt;
&#20805;&#20998;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
Bag of Tricks for Fully Test-Time Adaptation. (arXiv:2310.02416v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20805;&#20998;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#30340;&#25216;&#24039;&#65292;&#21253;&#25324;&#23567;&#25209;&#37327;&#24402;&#19968;&#21270;&#12289;&#27969;&#24179;&#34913;&#12289;&#21487;&#38752;&#26679;&#26412;&#36873;&#25321;&#21644;&#32593;&#32476;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#25216;&#26415;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#33021;&#21147;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20805;&#20998;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26088;&#22312;&#36866;&#24212;&#25968;&#25454;&#28418;&#31227;&#65292;&#26368;&#36817;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25216;&#24039;&#21644;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#22312;&#20219;&#24847;&#26410;&#26631;&#35760;&#25968;&#25454;&#27969;&#19978;&#36827;&#34892;&#31283;&#20581;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#27599;&#20010;&#20010;&#20307;&#25216;&#26415;&#30340;&#30495;&#27491;&#24433;&#21709;&#24182;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24110;&#21161;&#24041;&#22266;&#31038;&#21306;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25152;&#36873;&#27491;&#20132;TTA&#25216;&#26415;&#30340;&#20998;&#31867;&#65292;&#21253;&#25324;&#23567;&#25209;&#37327;&#24402;&#19968;&#21270;&#12289;&#27969;&#24179;&#34913;&#12289;&#21487;&#38752;&#26679;&#26412;&#36873;&#25321;&#21644;&#32593;&#32476;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;&#27599;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#24863;&#20852;&#36259;&#30340;&#22330;&#26223;&#19979;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#20934;&#30830;&#24615;&#12289;&#25152;&#38656;&#35745;&#31639;&#33021;&#21147;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#20043;&#38388;&#20135;&#29983;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#32467;&#21512;&#25216;&#26415;&#26102;&#20135;&#29983;&#30340;&#21327;&#21516;&#25928;&#24212;&#65292;&#24182;&#33021;&#22815;&#24314;&#31435;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully Test-Time Adaptation (TTA), which aims at adapting models to data drifts, has recently attracted wide interest. Numerous tricks and techniques have been proposed to ensure robust learning on arbitrary streams of unlabeled data. However, assessing the true impact of each individual technique and obtaining a fair comparison still constitutes a significant challenge. To help consolidate the community's knowledge, we present a categorization of selected orthogonal TTA techniques, including small batch normalization, stream rebalancing, reliable sample selection, and network confidence calibration. We meticulously dissect the effect of each approach on different scenarios of interest. Through our analysis, we shed light on trade-offs induced by those techniques between accuracy, the computational power required, and model complexity. We also uncover the synergy that arises when combining techniques and are able to establish new state-of-the-art results.
&lt;/p&gt;</description></item><item><title>causalimages R &#21253;&#25552;&#20379;&#20102;&#20351;&#29992;&#22270;&#20687;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#24037;&#20855;&#65292;&#24182;&#33021;&#22815;&#25972;&#21512;&#21355;&#26143;&#21644;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#31561;&#26032;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#20998;&#35299;&#27835;&#30103;&#25928;&#26524;&#30340;&#24322;&#36136;&#24615;&#21644;&#25511;&#21046;&#28151;&#28102;&#21464;&#37327;&#26469;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.00233</link><description>&lt;p&gt;
CausalImages: &#19968;&#20010;&#29992;&#20110;&#22320;&#29699;&#35266;&#27979;&#12289;&#29983;&#29289;&#21307;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#22270;&#20687;&#30340;&#22240;&#26524;&#25512;&#26029;&#30340; R &#21253;
&lt;/p&gt;
&lt;p&gt;
CausalImages: An R Package for Causal Inference with Earth Observation, Bio-medical, and Social Science Images. (arXiv:2310.00233v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00233
&lt;/p&gt;
&lt;p&gt;
causalimages R &#21253;&#25552;&#20379;&#20102;&#20351;&#29992;&#22270;&#20687;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#24037;&#20855;&#65292;&#24182;&#33021;&#22815;&#25972;&#21512;&#21355;&#26143;&#21644;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#31561;&#26032;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#20998;&#35299;&#27835;&#30103;&#25928;&#26524;&#30340;&#24322;&#36136;&#24615;&#21644;&#25511;&#21046;&#28151;&#28102;&#21464;&#37327;&#26469;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
causalimages R &#21253;&#33021;&#22815;&#36890;&#36807;&#22270;&#20687;&#21644;&#22270;&#20687;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#65292;&#20026;&#23558;&#21355;&#26143;&#21644;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#31561;&#26032;&#25968;&#25454;&#28304;&#25972;&#21512;&#21040;&#22240;&#26524;&#20851;&#31995;&#30740;&#31350;&#20013;&#25552;&#20379;&#26032;&#24037;&#20855;&#12290;&#19968;&#32452;&#20989;&#25968;&#21487;&#29992;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#22240;&#26524;&#25512;&#26029;&#20998;&#26512;&#12290;&#20363;&#22914;&#65292;&#19968;&#31181;&#20851;&#38190;&#20989;&#25968;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#36890;&#36807;&#22270;&#20687;&#26469;&#20998;&#35299;&#27835;&#30103;&#25928;&#26524;&#30340;&#24322;&#36136;&#24615;&#12290;&#36825;&#26679;&#21487;&#20197;&#30830;&#23450;&#21738;&#31181;&#31867;&#22411;&#30340;&#22270;&#20687;&#25110;&#22270;&#20687;&#24207;&#21015;&#23545;&#24178;&#39044;&#26368;&#25935;&#24863;&#12290;&#31532;&#20108;&#20010;&#24314;&#27169;&#20989;&#25968;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#22270;&#20687;&#26469;&#25511;&#21046;&#28151;&#28102;&#21464;&#37327;&#12290;&#35813;&#21253;&#36824;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#29983;&#25104;&#20316;&#20026;&#22270;&#20687;&#25110;&#35270;&#39057;&#20869;&#23481;&#21521;&#37327;&#25688;&#35201;&#30340;&#23884;&#20837;&#12290;&#26368;&#21518;&#65292;&#36824;&#25552;&#20379;&#20102;&#22522;&#30784;&#32467;&#26500;&#20989;&#25968;&#65292;&#20363;&#22914;&#29992;&#20110;&#23558;&#22823;&#35268;&#27169;&#22270;&#20687;&#21644;&#22270;&#20687;&#24207;&#21015;&#25968;&#25454;&#20889;&#20837;&#24207;&#21015;&#21270;&#23383;&#33410;&#23383;&#31526;&#20018;&#20197;&#36827;&#34892;&#26356;&#24555;&#36895;&#30340;&#22270;&#20687;&#20998;&#26512;&#30340;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;causalimages &#22312; R &#20013;&#24320;&#21551;&#20102;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#33021;&#21147;&#65292;&#35753;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The causalimages R package enables causal inference with image and image sequence data, providing new tools for integrating novel data sources like satellite and bio-medical imagery into the study of cause and effect. One set of functions enables image-based causal inference analyses. For example, one key function decomposes treatment effect heterogeneity by images using an interpretable Bayesian framework. This allows for determining which types of images or image sequences are most responsive to interventions. A second modeling function allows researchers to control for confounding using images. The package also allows investigators to produce embeddings that serve as vector summaries of the image or video content. Finally, infrastructural functions are also provided, such as tools for writing large-scale image and image sequence data as sequentialized byte strings for more rapid image analysis. causalimages therefore opens new capabilities for causal inference in R, letting research
&lt;/p&gt;</description></item><item><title>Outage-Watch&#20351;&#29992;&#26497;&#31471;&#20107;&#20214;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#25552;&#26089;&#39044;&#27979;&#20113;&#26381;&#21153;&#20013;&#26029;&#65292;&#36890;&#36807;&#25429;&#33719;&#36136;&#37327;&#25351;&#26631;&#30340;&#24694;&#21270;&#24773;&#20917;&#65292;&#25913;&#21892;&#28789;&#27963;&#24615;&#24182;&#25552;&#39640;&#23614;&#37096;&#20998;&#24067;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.17340</link><description>&lt;p&gt;
Outage-Watch: &#20351;&#29992;&#26497;&#31471;&#20107;&#20214;&#27491;&#21017;&#21270;&#25552;&#26089;&#39044;&#27979;&#26381;&#21153;&#20013;&#26029;
&lt;/p&gt;
&lt;p&gt;
Outage-Watch: Early Prediction of Outages using Extreme Event Regularizer. (arXiv:2309.17340v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17340
&lt;/p&gt;
&lt;p&gt;
Outage-Watch&#20351;&#29992;&#26497;&#31471;&#20107;&#20214;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#25552;&#26089;&#39044;&#27979;&#20113;&#26381;&#21153;&#20013;&#26029;&#65292;&#36890;&#36807;&#25429;&#33719;&#36136;&#37327;&#25351;&#26631;&#30340;&#24694;&#21270;&#24773;&#20917;&#65292;&#25913;&#21892;&#28789;&#27963;&#24615;&#24182;&#25552;&#39640;&#23614;&#37096;&#20998;&#24067;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#26381;&#21153;&#26080;&#22788;&#19981;&#22312;&#65292;&#20851;&#38190;&#20113;&#26381;&#21153;&#30340;&#25925;&#38556;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#20107;&#23454;&#12290;&#20026;&#20102;&#20445;&#30041;&#23458;&#25143;&#24182;&#38450;&#27490;&#25910;&#20837;&#25439;&#22833;&#65292;&#25552;&#20379;&#39640;&#21487;&#38752;&#24615;&#20445;&#35777;&#23545;&#20110;&#36825;&#20123;&#26381;&#21153;&#26469;&#35828;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#39044;&#27979;&#26381;&#21153;&#20013;&#26029;&#26159;&#19968;&#31181;&#20943;&#36731;&#20005;&#37325;&#31243;&#24230;&#21644;&#24674;&#22797;&#26102;&#38388;&#30340;&#26041;&#27861;&#12290;&#30001;&#20110;&#36825;&#20123;&#20107;&#20214;&#30340;&#32597;&#35265;&#24615;&#65292;&#39044;&#27979;&#20851;&#38190;&#24615;&#25925;&#38556;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#20851;&#38190;&#25925;&#38556;&#22312;&#21487;&#35266;&#27979;&#25968;&#25454;&#26041;&#38754;&#23450;&#20041;&#27169;&#31946;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;Outage-Watch&#23558;&#20851;&#38190;&#30340;&#26381;&#21153;&#20013;&#26029;&#23450;&#20041;&#20026;&#19968;&#32452;&#25351;&#26631;&#25152;&#25429;&#33719;&#30340;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#30340;&#24694;&#21270;&#24773;&#20917;&#12290;Outage-Watch&#36890;&#36807;&#20351;&#29992;&#24403;&#21069;&#31995;&#32479;&#29366;&#24577;&#26469;&#39044;&#27979;QoS&#25351;&#26631;&#26159;&#21542;&#20250;&#36229;&#36807;&#38408;&#20540;&#24182;&#24341;&#21457;&#26497;&#31471;&#20107;&#20214;&#65292;&#25552;&#21069;&#26816;&#27979;&#27492;&#31867;&#20013;&#26029;&#12290;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26469;&#27169;&#25311;QoS&#25351;&#26631;&#30340;&#20998;&#24067;&#20197;&#25552;&#39640;&#28789;&#27963;&#24615;&#65292;&#32780;&#26497;&#31471;&#20107;&#20214;&#27491;&#21017;&#21270;&#26377;&#21161;&#20110;&#25552;&#39640;&#23614;&#37096;&#20998;&#24067;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud services are omnipresent and critical cloud service failure is a fact of life. In order to retain customers and prevent revenue loss, it is important to provide high reliability guarantees for these services. One way to do this is by predicting outages in advance, which can help in reducing the severity as well as time to recovery. It is difficult to forecast critical failures due to the rarity of these events. Moreover, critical failures are ill-defined in terms of observable data. Our proposed method, Outage-Watch, defines critical service outages as deteriorations in the Quality of Service (QoS) captured by a set of metrics. Outage-Watch detects such outages in advance by using current system state to predict whether the QoS metrics will cross a threshold and initiate an extreme event. A mixture of Gaussian is used to model the distribution of the QoS metrics for flexibility and an extreme event regularizer helps in improving learning in tail of the distribution. An outage is 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#65292;&#23454;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2309.15877</link><description>&lt;p&gt;
&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neuro-Inspired Hierarchical Multimodal Learning. (arXiv:2309.15877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15877
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#65292;&#23454;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#21644;&#22788;&#29702;&#26469;&#33258;&#22810;&#31181;&#20449;&#24687;&#28304;&#25110;&#27169;&#24577;&#23545;&#20110;&#33719;&#24471;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20449;&#24687;&#35770;&#20998;&#23618;&#24863;&#30693;(ITHP)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#20449;&#24687;&#29942;&#39048;&#30340;&#27010;&#24565;&#12290;&#19982;&#22823;&#22810;&#25968;&#26088;&#22312;&#23558;&#25152;&#26377;&#27169;&#24577;&#32435;&#20837;&#36755;&#20837;&#30340;&#20256;&#32479;&#34701;&#21512;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#20027;&#35201;&#27169;&#24577;&#25351;&#23450;&#20026;&#36755;&#20837;&#65292;&#32780;&#20854;&#20313;&#27169;&#24577;&#21017;&#20316;&#20026;&#20449;&#24687;&#36335;&#24452;&#20013;&#30340;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24863;&#30693;&#27169;&#22411;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#22312;&#28508;&#22312;&#29366;&#24577;&#21644;&#36755;&#20837;&#27169;&#24577;&#29366;&#24577;&#20043;&#38388;&#26368;&#23567;&#21270;&#30456;&#20114;&#20449;&#24687;&#24182;&#22312;&#28508;&#22312;&#29366;&#24577;&#21644;&#20854;&#20313;&#27169;&#24577;&#20043;&#38388;&#26368;&#22823;&#21270;&#30456;&#20114;&#20449;&#24687;&#30340;&#24179;&#34913;&#65292;&#26500;&#24314;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#24182;&#26368;&#23567;&#21270;&#20887;&#20313;&#30340;&#32039;&#20945;&#28508;&#22312;&#29366;&#24577;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating and processing information from various sources or modalities are critical for obtaining a comprehensive and accurate perception of the real world. Drawing inspiration from neuroscience, we develop the Information-Theoretic Hierarchical Perception (ITHP) model, which utilizes the concept of information bottleneck. Distinct from most traditional fusion models that aim to incorporate all modalities as input, our model designates the prime modality as input, while the remaining modalities act as detectors in the information pathway. Our proposed perception model focuses on constructing an effective and compact information flow by achieving a balance between the minimization of mutual information between the latent state and the input modal state, and the maximization of mutual information between the latent states and the remaining modal states. This approach leads to compact latent state representations that retain relevant information while minimizing redundancy, thereby sub
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33647;&#21160;&#23398;&#20808;&#39564;&#39044;&#27979;&#27835;&#30103;&#21453;&#24212;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#25552;&#20379;&#33647;&#29289;&#30340;&#33647;&#21160;&#23398;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36924;&#30495;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#65292;&#35813;&#26041;&#27861;&#27604;&#22522;&#20934;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#32422;11%&#21644;8%&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#22810;&#31181;&#26377;&#30410;&#24212;&#29992;&#65292;&#22914;&#21457;&#20986;&#26089;&#26399;&#35686;&#21578;&#21644;&#23450;&#37327;&#29305;&#23450;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13135</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33647;&#21160;&#23398;&#20808;&#39564;&#39044;&#27979;&#27835;&#30103;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Forecasting Response to Treatment with Deep Learning and Pharmacokinetic Priors. (arXiv:2309.13135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13135
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33647;&#21160;&#23398;&#20808;&#39564;&#39044;&#27979;&#27835;&#30103;&#21453;&#24212;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#25552;&#20379;&#33647;&#29289;&#30340;&#33647;&#21160;&#23398;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36924;&#30495;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#65292;&#35813;&#26041;&#27861;&#27604;&#22522;&#20934;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#32422;11%&#21644;8%&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#22810;&#31181;&#26377;&#30410;&#24212;&#29992;&#65292;&#22914;&#21457;&#20986;&#26089;&#26399;&#35686;&#21578;&#21644;&#23450;&#37327;&#29305;&#23450;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#23545;&#20110;&#26089;&#26399;&#26816;&#27979;&#19981;&#33391;&#32467;&#26524;&#21644;&#24739;&#32773;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#22024;&#26434;&#21644;&#38388;&#27463;&#24615;&#65292;&#23454;&#38469;&#20013;&#39044;&#27979;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#36825;&#20123;&#25361;&#25112;&#36890;&#24120;&#36890;&#36807;&#22806;&#37096;&#22240;&#32032;&#35825;&#23548;&#30340;&#21464;&#21270;&#28857;&#65288;&#22914;&#33647;&#29289;&#20351;&#29992;&#65289;&#32780;&#21152;&#21095;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#21521;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#33647;&#29289;&#30340;&#33647;&#21160;&#23398;&#25928;&#24212;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#21463;&#27835;&#30103;&#24433;&#21709;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#20351;&#29992;&#36924;&#30495;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#39044;&#27979;&#34880;&#31958;&#30340;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#33647;&#21160;&#23398;&#32534;&#30721;&#22120;&#20351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#36229;&#36807;&#22522;&#20934;&#32422;11&#65285;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36229;&#36807;8&#65285;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#22810;&#31181;&#26377;&#30410;&#24212;&#29992;&#65292;&#20363;&#22914;&#21457;&#20986;&#20851;&#20110;&#24847;&#22806;&#27835;&#30103;&#21453;&#24212;&#30340;&#26089;&#26399;&#35686;&#21578;&#65292;&#25110;&#24110;&#21161;&#34920;&#24449;&#29305;&#23450;&#20110;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting healthcare time series is crucial for early detection of adverse outcomes and for patient monitoring. Forecasting, however, can be difficult in practice due to noisy and intermittent data. The challenges are often exacerbated by change points induced via extrinsic factors, such as the administration of medication. We propose a novel encoder that informs deep learning models of the pharmacokinetic effects of drugs to allow for accurate forecasting of time series affected by treatment. We showcase the effectiveness of our approach in a task to forecast blood glucose using both realistically simulated and real-world data. Our pharmacokinetic encoder helps deep learning models surpass baselines by approximately 11% on simulated data and 8% on real-world data. The proposed approach can have multiple beneficial applications in clinical practice, such as issuing early warnings about unexpected treatment responses, or helping to characterize patient-specific treatment effects in te
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#38750;AI&#19987;&#23478;&#23545;AI&#20915;&#31574;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#23450;&#20041;&#39034;&#24207;&#20915;&#31574;&#35774;&#32622;&#20013;&#30340;&#8220;&#27010;&#24565;&#8221;&#20197;&#21450;&#25506;&#32034;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#23545;RL agent&#23398;&#20064;&#25928;&#26524;&#21644;&#26368;&#32456;&#29992;&#25143;&#23545;agent&#20915;&#31574;&#29702;&#35299;&#30340;&#21452;&#37325;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.12482</link><description>&lt;p&gt;
State2Explanation:&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#65306;&#26377;&#21033;&#20110;Agent&#23398;&#20064;&#21644;&#29992;&#25143;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding. (arXiv:2309.12482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#38750;AI&#19987;&#23478;&#23545;AI&#20915;&#31574;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#23450;&#20041;&#39034;&#24207;&#20915;&#31574;&#35774;&#32622;&#20013;&#30340;&#8220;&#27010;&#24565;&#8221;&#20197;&#21450;&#25506;&#32034;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#23545;RL agent&#23398;&#20064;&#25928;&#26524;&#21644;&#26368;&#32456;&#29992;&#25143;&#23545;agent&#20915;&#31574;&#29702;&#35299;&#30340;&#21452;&#37325;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38750;AI&#19987;&#23478;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;AI&#31995;&#32479;&#26469;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#21162;&#21147;&#24320;&#21457;&#33021;&#22815;&#20026;&#38750;AI&#19987;&#23478;&#29702;&#35299;&#30340;AI&#20915;&#31574;&#25552;&#20379;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#21033;&#29992;&#39640;&#32423;&#27010;&#24565;&#24182;&#29983;&#25104;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#37117;&#26159;&#20026;&#20998;&#31867;&#25216;&#26415;&#32780;&#24320;&#21457;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#20851;&#20110;&#39034;&#24207;&#20915;&#31574;&#30340;&#26041;&#27861;&#36824;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#22312;&#39034;&#24207;&#20915;&#31574;&#35774;&#32622;&#20013;&#23450;&#20041;&#8220;&#27010;&#24565;&#8221;&#30340;&#24895;&#26395;&#12290;&#21463;&#21040;&#8220;Protege&#25928;&#24212;&#8221;&#30340;&#21551;&#21457;&#65292;&#35813;&#25928;&#24212;&#35828;&#26126;&#35299;&#37322;&#30693;&#35782;&#36890;&#24120;&#20250;&#22686;&#24378;&#20010;&#20307;&#30340;&#33258;&#20027;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#23545;RL agent&#30340;&#23398;&#20064;&#25928;&#26524;&#21644;&#26368;&#32456;&#29992;&#25143;&#23545;agent&#20915;&#31574;&#29702;&#35299;&#30340;&#21452;&#37325;&#22909;&#22788;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;St
&lt;/p&gt;
&lt;p&gt;
With more complex AI systems used by non-AI experts to complete daily tasks, there is an increasing effort to develop methods that produce explanations of AI decision making understandable by non-AI experts. Towards this effort, leveraging higher-level concepts and producing concept-based explanations have become a popular method. Most concept-based explanations have been developed for classification techniques, and we posit that the few existing methods for sequential decision making are limited in scope. In this work, we first contribute a desiderata for defining "concepts" in sequential decision making settings. Additionally, inspired by the Protege Effect which states explaining knowledge often reinforces one's self-learning, we explore the utility of concept-based explanations providing a dual benefit to the RL agent by improving agent learning rate, and to the end-user by improving end-user understanding of agent decision making. To this end, we contribute a unified framework, St
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#21160;&#24577;&#27169;&#24335;&#20998;&#35299;(DMD)&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;Koopman&#26694;&#26550;&#20013;&#24341;&#20837;&#26041;&#24046;&#26469;&#35299;&#20915;&#25361;&#25112;&#65292;&#21253;&#25324;&#34394;&#20551;&#27169;&#24335;&#21644;&#26412;&#24449;&#35889;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38656;&#35201;&#36229;&#36234;&#39044;&#26399;&#26469;&#26377;&#25928;&#22788;&#29702;&#38543;&#26426;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.10697</link><description>&lt;p&gt;
&#36229;&#36234;&#39044;&#26399;&#65306;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#30340;&#21097;&#20313;&#21160;&#24577;&#27169;&#24335;&#20998;&#35299;&#21644;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
Beyond expectations: Residual Dynamic Mode Decomposition and Variance for Stochastic Dynamical Systems. (arXiv:2308.10697v2 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#21160;&#24577;&#27169;&#24335;&#20998;&#35299;(DMD)&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;Koopman&#26694;&#26550;&#20013;&#24341;&#20837;&#26041;&#24046;&#26469;&#35299;&#20915;&#25361;&#25112;&#65292;&#21253;&#25324;&#34394;&#20551;&#27169;&#24335;&#21644;&#26412;&#24449;&#35889;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38656;&#35201;&#36229;&#36234;&#39044;&#26399;&#26469;&#26377;&#25928;&#22788;&#29702;&#38543;&#26426;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#31526;&#23558;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#32447;&#24615;&#21270;&#65292;&#20351;&#24471;&#20854;&#35889;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#31639;&#27861;&#26469;&#36817;&#20284;&#36825;&#20123;&#35889;&#29305;&#24615;&#65292;&#20854;&#20013;&#21160;&#24577;&#27169;&#24335;&#20998;&#35299;(DMD)&#26159;&#22522;&#20110;&#25237;&#24433;&#26041;&#27861;&#30340;&#20856;&#22411;&#20195;&#34920;&#12290;&#23613;&#31649;Koopman&#31639;&#31526;&#26412;&#36523;&#26159;&#32447;&#24615;&#30340;&#65292;&#20294;&#23427;&#22312;&#26080;&#31351;&#32500;&#30340;&#21487;&#35266;&#27979;&#31354;&#38388;&#20013;&#36215;&#20316;&#29992;&#65292;&#36825;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#21253;&#25324;&#34394;&#20551;&#27169;&#24335;&#65292;&#26412;&#24449;&#35889;&#21644;Koopman&#27169;&#24335;&#20998;&#35299;&#30340;&#39564;&#35777;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#30830;&#23450;&#24615;&#31995;&#32479;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#23545;&#20110;&#38543;&#26426;&#31995;&#32479;&#30340;&#24050;&#39564;&#35777;&#30340;DMD&#26041;&#27861;&#20173;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#65292;&#20854;&#20013;Koopman&#31639;&#31526;&#27979;&#37327;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#38656;&#35201;&#36229;&#36234;&#39044;&#26399;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#26041;&#24046;&#32435;&#20837;Koopman&#26694;&#26550;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#39069;&#22806;&#30340;&#31867;&#20284;DMD&#30340;&#30697;&#38453;&#65292;&#25105;&#20204;&#36817;&#20284;&#19968;&#20010;&#27531;&#24046;&#24179;&#26041;&#21644;&#30340;&#21644;&#20197;&#21450;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Koopman operators linearize nonlinear dynamical systems, making their spectral information of crucial interest. Numerous algorithms have been developed to approximate these spectral properties, and Dynamic Mode Decomposition (DMD) stands out as the poster child of projection-based methods. Although the Koopman operator itself is linear, the fact that it acts in an infinite-dimensional space of observables poses challenges. These include spurious modes, essential spectra, and the verification of Koopman mode decompositions. While recent work has addressed these challenges for deterministic systems, there remains a notable gap in verified DMD methods for stochastic systems, where the Koopman operator measures the expectation of observables. We show that it is necessary to go beyond expectations to address these issues. By incorporating variance into the Koopman framework, we address these challenges. Through an additional DMD-type matrix, we approximate the sum of a squared residual and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;paNNG&#30340;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;kNN&#21644;&#22522;&#20110;&#20998;&#24067;&#30340;&#22270;&#26500;&#24314;&#12290;&#36890;&#36807;&#21253;&#21547;&#20998;&#24067;&#20449;&#24687;&#65292;paNNG&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#27169;&#31946;&#26679;&#26412;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02442</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#20248;&#20808;&#32423;&#38468;&#21152;kNN&#22270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Preferential Attached kNN Graph With Distribution-Awareness. (arXiv:2308.02442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;paNNG&#30340;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;kNN&#21644;&#22522;&#20110;&#20998;&#24067;&#30340;&#22270;&#26500;&#24314;&#12290;&#36890;&#36807;&#21253;&#21547;&#20998;&#24067;&#20449;&#24687;&#65292;paNNG&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#27169;&#31946;&#26679;&#26412;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;kNN&#31639;&#27861;&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#26377;&#25928;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24191;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;kNN&#22270;&#23545;&#20110;k&#20540;&#30340;&#22266;&#23450;&#20381;&#36182;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#20998;&#31867;&#27169;&#22411;&#31867;&#20284;&#65292;&#20915;&#31574;&#36793;&#30028;&#19978;&#23384;&#22312;&#30340;&#27169;&#31946;&#26679;&#26412;&#24120;&#24120;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#23481;&#26131;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20248;&#20808;&#32423;&#38468;&#21152;k-&#26368;&#36817;&#37051;&#22270;&#65288;paNNG&#65289;&#65292;&#23427;&#23558;&#33258;&#36866;&#24212;&#30340;kNN&#19982;&#22522;&#20110;&#20998;&#24067;&#30340;&#22270;&#26500;&#24314;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#20998;&#24067;&#20449;&#24687;&#65292;paNNG&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#31946;&#26679;&#26412;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#8220;&#25289;&#8221;&#23427;&#20204;&#22238;&#21040;&#21407;&#22987;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#25913;&#36827;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;paNNG&#30340;&#24615;&#33021;&#36229;&#36234;&#20102;&#29616;&#26377;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based kNN algorithms have garnered widespread popularity for machine learning tasks, due to their simplicity and effectiveness. However, the conventional kNN graph's reliance on a fixed value of k can hinder its performance, especially in scenarios involving complex data distributions. Moreover, like other classification models, the presence of ambiguous samples along decision boundaries often presents a challenge, as they are more prone to incorrect classification. To address these issues, we propose the Preferential Attached k-Nearest Neighbors Graph (paNNG), which combines adaptive kNN with distribution-based graph construction. By incorporating distribution information, paNNG can significantly improve performance for ambiguous samples by "pulling" them towards their original classes and hence enable enhanced overall accuracy and generalization capability. Through rigorous evaluations on diverse benchmark datasets, paNNG outperforms state-of-the-art algorithms, showcasing its 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#38382;&#39064;&#20013;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#26159;&#21542;&#33021;&#32988;&#36807;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;SimCLR-Rank&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.00177</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#20013;&#32988;&#36807;GBDTs
&lt;/p&gt;
&lt;p&gt;
Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity. (arXiv:2308.00177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#38382;&#39064;&#20013;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#26159;&#21542;&#33021;&#32988;&#36807;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;SimCLR-Rank&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#39046;&#22495;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#20294;&#23427;&#20204;&#22312;&#34920;&#26684;&#24418;&#24335;&#30340;Learning-To-Rank&#38382;&#39064;&#19978;&#23578;&#26410;&#19968;&#33268;&#22320;&#32988;&#36807;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;(GBDTs)&#12290;&#36817;&#26399;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20219;&#21153;&#19978;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#27604;&#26377;&#26631;&#31614;&#25968;&#25454;&#22810;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#36824;&#26410;&#24212;&#29992;&#20110;Learning-To-Rank&#38382;&#39064;&#65292;&#32780;&#35813;&#38382;&#39064;&#36890;&#24120;&#20135;&#29983;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26159;&#21542;&#33021;&#25552;&#39640;LTR&#24615;&#33021;&#65292;&#19982;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#35774;&#35745;&#36873;&#25321;(&#21253;&#25324;SimCLR-Rank&#65292;&#36825;&#26159;&#25105;&#20204;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#20462;&#25913;&#30340;SimCLR&#26041;&#27861;)&#65292;&#25105;&#20204;&#20135;&#29983;&#20102;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#26377;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#19988;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#20248;&#20110;GBDTs(&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;)&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. To the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. In this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. Using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#26469;&#25913;&#36827;&#27169;&#22411;&#22312;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.15870</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Semi-Supervised Federated Learning for Heterogeneous Participants. (arXiv:2307.15870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#26469;&#25913;&#36827;&#27169;&#22411;&#22312;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#21327;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#22411;&#27169;&#22411;&#29992;&#20110;&#24191;&#27867;&#24212;&#29992;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20998;&#31163;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#36890;&#36807;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36127;&#25285;&#25552;&#20379;&#20102;&#20248;&#31168;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;SFL&#36890;&#24120;&#20551;&#35774;&#23458;&#25143;&#31471;&#20855;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#65292;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#24182;&#38750;&#24635;&#26159;&#22914;&#27492;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#21322;&#30417;&#30563;&#25216;&#26415;&#26469;&#21033;&#29992;FL&#20013;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#25968;&#25454;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24615;&#25552;&#20986;&#20102;&#30830;&#20445;&#35757;&#32451;&#25928;&#29575;&#30340;&#21478;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;Pseudo-Clustering Semi-SFL&#65292;&#29992;&#20110;&#22312;&#26631;&#35760;&#25968;&#25454;&#20301;&#20110;&#26381;&#21153;&#22120;&#19978;&#30340;&#24773;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged to allow multiple clients to collaboratively train machine learning models on their private data. However, training and deploying large models for broader applications is challenging in resource-constrained environments. Fortunately, Split Federated Learning (SFL) offers an excellent solution by alleviating the computation and communication burden on the clients SFL often assumes labeled data for local training on clients, however, it is not the case in practice.Prior works have adopted semi-supervised techniques for leveraging unlabeled data in FL, but data non-IIDness poses another challenge to ensure training efficiency. Herein, we propose Pseudo-Clustering Semi-SFL, a novel system for training models in scenarios where labeled data reside on the server. By introducing Clustering Regularization, model performance under data non-IIDness can be improved. Besides, our theoretical and experimental investigations into model convergence reveal that the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20256;&#32479;&#30340;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#25193;&#23637;&#21040;&#21512;&#24182;&#26641;&#30340;Wasserstein&#24230;&#37327;&#31354;&#38388;&#12290;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;</title><link>http://arxiv.org/abs/2307.02509</link><description>&lt;p&gt;
Wasserstein&#33258;&#32534;&#30721;&#21512;&#24182;&#26641;&#65288;&#21644;&#25345;&#32493;&#22270;&#65289;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams). (arXiv:2307.02509v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20256;&#32479;&#30340;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#25193;&#23637;&#21040;&#21512;&#24182;&#26641;&#30340;Wasserstein&#24230;&#37327;&#31354;&#38388;&#12290;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#24182;&#26641;&#30340;Wasserstein&#33258;&#32534;&#30721;(MT-WAE)&#65292;&#36825;&#26159;&#23558;&#20256;&#32479;&#30340;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25193;&#23637;&#21040;&#21512;&#24182;&#26641;&#30340;Wasserstein&#24230;&#37327;&#31354;&#38388;&#30340;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#12290;&#19982;&#25805;&#20316;&#21521;&#37327;&#21270;&#25968;&#25454;&#30340;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20844;&#24335;&#22312;&#32593;&#32476;&#30340;&#27599;&#19968;&#23618;&#26126;&#30830;&#22320;&#25805;&#20316;&#21512;&#24182;&#26641;&#30340;&#20851;&#32852;&#24230;&#37327;&#31354;&#38388;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#20026;&#21069;&#26399;&#32447;&#24615;&#23581;&#35797;[65]&#30340;&#38750;&#32447;&#24615;&#25512;&#24191;&#12290;&#23427;&#20063;&#24456;&#23481;&#26131;&#25193;&#23637;&#21040;&#25345;&#32493;&#22270;&#12290;&#23545;&#20844;&#20849;&#36830;&#38145;&#21453;&#24212;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;MT-WAE&#30340;&#35745;&#31639;&#24179;&#22343;&#26102;&#38388;&#20165;&#20026;&#20960;&#20998;&#38047;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#36129;&#29486;&#30340;&#23454;&#29992;&#24615;&#36890;&#36807;&#20004;&#20010;&#24212;&#29992;&#26469;&#23637;&#31034;&#65292;&#36825;&#20123;&#24212;&#29992;&#26159;&#22522;&#20110;&#20197;&#21069;&#20851;&#20110;&#21512;&#24182;&#26641;&#32534;&#30721;&#30340;&#24037;&#20316;[65]&#36827;&#34892;&#35843;&#25972;&#24471;&#21040;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;MT-WAE&#24212;&#29992;&#20110;&#25968;&#25454;&#21387;&#32553;&#65292;&#24182;&#21487;&#38752;&#22320;&#21387;&#32553;&#21512;&#24182;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a computational framework for the Wasserstein auto-encoding of merge trees (MT-WAE), a novel extension of the classical auto-encoder neural network architecture to the Wasserstein metric space of merge trees. In contrast to traditional auto-encoders which operate on vectorized data, our formulation explicitly manipulates merge trees on their associated metric space at each layer of the network, resulting in superior accuracy and interpretability. Our novel neural network approach can be interpreted as a non-linear generalization of previous linear attempts [65] at merge tree encoding. It also trivially extends to persistence diagrams. Extensive experiments on public ensembles demonstrate the efficiency of our algorithms, with MT-WAE computations in the orders of minutes on average. We show the utility of our contributions in two applications adapted from previous work on merge tree encoding [65]. First, we apply MT-WAE to data reduction and reliably compress merge t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#20215;&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#65292;&#36824;&#33021;&#22815;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#65292;&#21363;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;KGF&#21644;KRR&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;KRR&#27867;&#21270;&#65292;&#20351;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.16838</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Kernel Ridge Regression with Gradient-Based Optimization Methods. (arXiv:2306.16838v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#20215;&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#65292;&#36824;&#33021;&#22815;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#65292;&#21363;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;KGF&#21644;KRR&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;KRR&#27867;&#21270;&#65292;&#20351;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#26159;&#32447;&#24615;&#23725;&#22238;&#24402;&#30340;&#38750;&#32447;&#24615;&#25512;&#24191;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KRR&#30446;&#26631;&#20989;&#25968;&#30340;&#31561;&#20215;&#24418;&#24335;&#65292;&#20026;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#21644;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#25171;&#24320;&#20102;&#21487;&#33021;&#12290;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#8212;&#8212;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#35753;&#25105;&#20204;&#33021;&#22815;&#22312;KGF&#21644;KRR&#20043;&#38388;&#29702;&#35770;&#19978;&#30028;&#23450;&#24046;&#24322;&#12290;&#25105;&#20204;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#23558;KRR&#27867;&#21270;&#65292;&#24182;&#21033;&#29992;&#31867;&#20284;KGF&#21644;KRR&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#29992;&#36825;&#20123;&#24809;&#32602;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#20351;&#29992;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#65288;&#20063;&#31216;&#20026;&#22352;&#26631;&#19979;&#38477;&#65289;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel ridge regression, KRR, is a non-linear generalization of linear ridge regression. Here, we introduce an equivalent formulation of the objective function of KRR, opening up both for using other penalties than the ridge penalty and for studying kernel ridge regression from the perspective of gradient descent. Using a continuous-time perspective, we derive a closed-form solution, kernel gradient flow, KGF, with regularization through early stopping, which allows us to theoretically bound the differences between KGF and KRR. We generalize KRR by replacing the ridge penalty with the $\ell_1$ and $\ell_\infty$ penalties and utilize the fact that analogously to the similarities between KGF and KRR, the solutions obtained when using these penalties are very similar to those obtained from forward stagewise regression (also known as coordinate descent) and sign gradient descent in combination with early stopping. Thus the need for computationally heavy proximal gradient descent algorithms
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;SE(3)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#26230;&#20307;&#32467;&#26500;&#24377;&#24615;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#20135;&#29983;&#20934;&#30830;&#30340;&#24377;&#24615;&#27169;&#37327;&#65292;&#24182;&#19988;&#33021;&#22815;&#39044;&#27979;&#24212;&#21464;&#33021;&#23494;&#24230;&#21644;&#30456;&#20851;&#30340;&#24377;&#24615;&#24120;&#25968;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#25299;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12818</link><description>&lt;p&gt;
StrainNet: &#20351;&#29992;SE(3)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#26230;&#20307;&#32467;&#26500;&#24377;&#24615;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
StrainNet: Predicting crystal structure elastic properties using SE(3)-equivariant graph neural networks. (arXiv:2306.12818v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;SE(3)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#26230;&#20307;&#32467;&#26500;&#24377;&#24615;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#20135;&#29983;&#20934;&#30830;&#30340;&#24377;&#24615;&#27169;&#37327;&#65292;&#24182;&#19988;&#33021;&#22815;&#39044;&#27979;&#24212;&#21464;&#33021;&#23494;&#24230;&#21644;&#30456;&#20851;&#30340;&#24377;&#24615;&#24120;&#25968;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#25299;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26230;&#24577;&#22266;&#20307;&#30340;&#24377;&#24615;&#24615;&#36136;&#23545;&#20110;&#35745;&#31639;&#26448;&#26009;&#31185;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20174;&#22836;&#31639;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#30740;&#31350;&#20855;&#26377;&#22823;&#37327;&#21407;&#23376;&#30340;&#22797;&#26434;&#26448;&#26009;&#26102;&#22914;&#27492;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20351;&#29992;SE(3)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#39640;&#25928;&#39044;&#27979;&#26230;&#20307;&#32467;&#26500;&#30340;&#24377;&#24615;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#19982;&#26368;&#36817;&#30340;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#30456;&#24403;&#20934;&#30830;&#30340;&#37325;&#35201;&#26631;&#37327;&#24377;&#24615;&#27169;&#37327;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23545;&#31216;&#24863;&#30693;GNN&#27169;&#22411;&#36824;&#33021;&#22815;&#39044;&#27979;&#24212;&#21464;&#33021;&#23494;&#24230;(SED)&#21644;&#30456;&#20851;&#24377;&#24615;&#24120;&#25968;&#65292;&#36825;&#20123;&#26159;&#21463;&#26230;&#20307;&#23398;&#32676;&#24433;&#21709;&#26174;&#33879;&#30340;&#22522;&#26412;&#24352;&#37327;&#37327;&#12290;&#35813;&#27169;&#22411;&#19968;&#33268;&#21306;&#20998;SED&#24352;&#37327;&#30340;&#29420;&#31435;&#20803;&#32032;&#65292;&#31526;&#21512;&#26230;&#20307;&#32467;&#26500;&#30340;&#23545;&#31216;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#25299;&#23637;&#24615;&#65292;&#21487;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#26230;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting the elastic properties of crystalline solids is vital for computational materials science. However, traditional atomistic scale ab initio approaches are computationally intensive, especially for studying complex materials with a large number of atoms in a unit cell. We introduce a novel data-driven approach to efficiently predict the elastic properties of crystal structures using SE(3)-equivariant graph neural networks (GNNs). This approach yields important scalar elastic moduli with the accuracy comparable to recent data-driven studies. Importantly, our symmetry-aware GNNs model also enables the prediction of the strain energy density (SED) and the associated elastic constants, the fundamental tensorial quantities that are significantly influenced by a material's crystallographic group. The model consistently distinguishes independent elements of SED tensors, in accordance with the symmetry of the crystal structures. Finally, our deep learning model possesses mea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#20197;&#20943;&#23569;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#31243;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21482;&#38656;&#35201;&#20351;&#29992;&#19968;&#31181;&#36741;&#21161;&#32593;&#32476;&#23601;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05674</link><description>&lt;p&gt;
&#38754;&#21521;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#20943;&#23569;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Uncertainty Quantification and Reduction for Over-Parameterized Neural Networks. (arXiv:2306.05674v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#20197;&#20943;&#23569;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#31243;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21482;&#38656;&#35201;&#20351;&#29992;&#19968;&#31181;&#36741;&#21161;&#32593;&#32476;&#23601;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#35780;&#20272;&#21644;&#25913;&#36827;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#19981;&#20165;&#26469;&#33258;&#25968;&#25454;&#65292;&#36824;&#26469;&#33258;&#35757;&#32451;&#36807;&#31243;&#20013;&#27880;&#20837;&#30340;&#22823;&#37327;&#22122;&#22768;&#21644;&#20559;&#24046;&#12290;&#36825;&#20123;&#22122;&#22768;&#21644;&#20559;&#24046;&#22952;&#30861;&#20102;&#32479;&#35745;&#20445;&#35777;&#30340;&#23454;&#29616;&#65292;&#24182;&#19988;&#30001;&#20110;&#38656;&#35201;&#37325;&#22797;&#30340;&#32593;&#32476;&#37325;&#26032;&#35757;&#32451;&#65292;&#23545;UQ&#25552;&#20986;&#20102;&#35745;&#31639;&#25361;&#25112;&#12290;&#22522;&#20110;&#26368;&#36817;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;&#29702;&#35770;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#26041;&#26696;&#65292;&#20197;&#36890;&#36807;&#38750;&#24120;&#20302;&#30340;&#35745;&#31639;&#37327;&#37327;&#21270;&#21644;&#20943;&#23569;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36807;&#31243;&#19981;&#30830;&#23450;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25105;&#20204;&#31216;&#20026;&#36807;&#31243;&#22122;&#22768;&#26657;&#27491;&#65288;PNC&#65289;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#21482;&#20351;&#29992;&#19968;&#31181;&#36866;&#24403;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#36741;&#21161;&#32593;&#32476;&#26469;&#28040;&#38500;&#36807;&#31243;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#28145;&#23618;&#38598;&#25104;&#20013;&#30340;&#35768;&#22810;&#37325;&#26032;&#35757;&#32451;&#30340;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;PNC&#39044;&#27979;&#22120;&#19982;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#32593;&#32476;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification (UQ) is important for reliability assessment and enhancement of machine learning models. In deep learning, uncertainties arise not only from data, but also from the training procedure that often injects substantial noises and biases. These hinder the attainment of statistical guarantees and, moreover, impose computational challenges on UQ due to the need for repeated network retraining. Building upon the recent neural tangent kernel theory, we create statistically guaranteed schemes to principally \emph{quantify}, and \emph{remove}, the procedural uncertainty of over-parameterized neural networks with very low computation effort. In particular, our approach, based on what we call a procedural-noise-correcting (PNC) predictor, removes the procedural uncertainty by using only \emph{one} auxiliary network that is trained on a suitably labeled data set, instead of many retrained networks employed in deep ensembles. Moreover, by combining our PNC predictor with su
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#32447;&#24615;transformers&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;transformers&#33021;&#22815;&#23454;&#29616;&#19968;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#65292;&#32780;&#23545;&#20110;&#19968;&#20010;$k$&#20010;&#27880;&#24847;&#21147;&#23618;&#30340;transformer&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#22810;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2306.00297</link><description>&lt;p&gt;
Transformers&#23398;&#20064;&#23454;&#29616;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transformers learn to implement preconditioned gradient descent for in-context learning. (arXiv:2306.00297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#32447;&#24615;transformers&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;transformers&#33021;&#22815;&#23454;&#29616;&#19968;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#65292;&#32780;&#23545;&#20110;&#19968;&#20010;$k$&#20010;&#27880;&#24847;&#21147;&#23618;&#30340;transformer&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#22810;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#30340;&#39537;&#21160;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;transformers&#21487;&#20197;&#23454;&#29616;&#20687;&#26799;&#24230;&#19979;&#38477;&#36825;&#26679;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#31934;&#24515;&#30340;&#26435;&#37325;&#26500;&#36896;&#65292;&#36825;&#20123;&#30740;&#31350;&#34920;&#26126;&#22810;&#23618;transformers&#20855;&#26377;&#36275;&#22815;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#27169;&#25311;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#12290;&#36229;&#36234;&#34920;&#36798;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#38382;&#65306;transformers&#33021;&#21542;&#36890;&#36807;&#22312;&#38543;&#26426;&#38382;&#39064;&#23454;&#20363;&#19978;&#35757;&#32451;&#26469;&#23398;&#20064;&#23454;&#29616;&#36825;&#26679;&#30340;&#31639;&#27861;&#65311;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36890;&#36807;&#23545;&#32447;&#24615;&#22238;&#24402;&#30340;&#38543;&#26426;&#23454;&#20363;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#31532;&#19968;&#27425;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#29702;&#35770;&#36827;&#23637;&#65292;&#36890;&#36807;&#23545;&#32447;&#24615;transformers&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20998;&#26512;&#12290;&#23545;&#20110;&#19968;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35757;&#32451;&#30446;&#26631;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#23454;&#29616;&#20102;&#19968;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#39044;&#22788;&#29702;&#30697;&#38453;&#19981;&#20165;&#36866;&#24212;&#36755;&#20837;&#20998;&#24067;&#65292;&#32780;&#19988;&#36824;&#36866;&#24212;&#20110;&#25968;&#25454;&#19981;&#20805;&#20998;&#24341;&#36215;&#30340;&#26041;&#24046;&#12290;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;$k$&#20010;&#27880;&#24847;&#21147;&#23618;&#30340;transformer&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#23450;&#26465;&#20214;&#19979;&#23427;&#21487;&#20197;&#23454;&#29616;&#22810;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the striking ability of transformers for in-context learning, several works demonstrate that transformers can implement algorithms like gradient descent. By a careful construction of weights, these works show that multiple layers of transformers are expressive enough to simulate gradient descent iterations. Going beyond the question of expressivity, we ask: Can transformers learn to implement such algorithms by training over random problem instances? To our knowledge, we make the first theoretical progress toward this question via analysis of the loss landscape for linear transformers trained over random instances of linear regression. For a single attention layer, we prove the global minimum of the training objective implements a single iteration of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the input distribution but also to the variance induced by data inadequacy. For a transformer with $k$ attention layers, we prove certain 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Channel-aware dynamic sparse (Chase)&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#20102;GPU&#21451;&#22909;&#30340;&#36890;&#36947;&#32423;&#21035;&#31232;&#30095;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#27530;&#25805;&#20316;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#22312;&#36890;&#29992;&#30828;&#20214;&#19978;&#21152;&#36895;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19454</link><description>&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#26159;&#36890;&#36947;&#32423;&#31232;&#30095;&#30340;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparsity Is Channel-Level Sparsity Learner. (arXiv:2305.19454v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Channel-aware dynamic sparse (Chase)&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#20102;GPU&#21451;&#22909;&#30340;&#36890;&#36947;&#32423;&#21035;&#31232;&#30095;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#27530;&#25805;&#20316;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#22312;&#36890;&#29992;&#30828;&#20214;&#19978;&#21152;&#36895;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#35757;&#32451;&#30001;&#20110;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#21644;&#25512;&#29702;&#20013;&#20855;&#26377;&#35825;&#20154;&#30340;&#33410;&#30465;&#33021;&#21147;&#32780;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;(DST)&#20316;&#20026;&#19968;&#31181;&#39046;&#20808;&#30340;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#36798;&#21040;&#19982;&#23494;&#38598;&#23545;&#24212;&#29289;&#24615;&#33021;&#30456;&#21305;&#37197;&#30340;&#39640;&#31232;&#30095;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;DST&#20043;&#21069;&#30340;&#30740;&#31350;&#37117;&#34920;&#26126;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#26159;&#22312;&#39640;&#24230;&#19981;&#35268;&#21017;&#30340;&#31232;&#30095;&#27169;&#24335;&#19979;&#30340;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#19978;&#65292;&#36825;&#22312;&#24120;&#35265;&#30828;&#20214;&#19978;&#24471;&#21040;&#20102;&#26377;&#38480;&#30340;&#25903;&#25345;&#12290;&#36825;&#31181;&#38480;&#21046;&#38459;&#30861;&#20102;DST&#22312;&#23454;&#36341;&#20013;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36890;&#36947;&#24863;&#30693;&#21160;&#24577;&#31232;&#30095;&#65288;Chase&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#38750;&#32467;&#26500;&#21270;&#21160;&#24577;&#31232;&#30095;&#30340;&#24615;&#33021;&#36716;&#25442;&#20026;&#36866;&#21512;GPU&#21451;&#22909;&#30340;&#36890;&#36947;&#32423;&#21035;&#31232;&#30095;&#65292;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#29616;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#27530;&#30340;&#25805;&#20316;&#12290;&#25152;&#24471;&#21040;&#30340;&#23567;&#22411;&#31232;&#30095;&#32593;&#32476;&#21487;&#20197;&#30452;&#25509;&#36890;&#36807;&#36890;&#29992;&#30828;&#20214;&#21152;&#36895;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#19987;&#29992;&#30340;&#31232;&#30095;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Chase&#21487;&#20197;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#23454;&#29616;&#39640;&#36890;&#36947;&#32423;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#65292;&#24182;&#26174;&#30528;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse training has received an upsurging interest in machine learning due to its tantalizing saving potential for the entire training process as well as inference. Dynamic sparse training (DST), as a leading sparse training approach, can train deep neural networks at high sparsity from scratch to match the performance of their dense counterparts. However, most if not all DST prior arts demonstrate their effectiveness on unstructured sparsity with highly irregular sparse patterns, which receives limited support in common hardware. This limitation hinders the usage of DST in practice. In this paper, we propose Channel-aware dynamic sparse (Chase), which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity (not fine-grained N:M or group sparsity) during one end-to-end training process, without any ad-hoc operations. The resulting small sparse networks can be directly accelerated by commodity hardware, without using a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32570;&#22833;&#20540;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#30340;&#20998;&#31867;&#12290;&#20256;&#32479;&#26041;&#27861;&#20250;&#21152;&#21095;&#27495;&#35270;&#12290;&#26412;&#25991;&#35777;&#26126;&#20174;&#25554;&#34917;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#20250;&#24694;&#21270;&#32452;&#20844;&#24179;&#24615;&#21644;&#24179;&#22343;&#20934;&#30830;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#20844;&#24179;&#24178;&#39044;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#22788;&#29702;&#25152;&#26377;&#21487;&#33021;&#30340;&#32570;&#22833;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.19429</link><description>&lt;p&gt;
&#32570;&#22833;&#20540;&#19979;&#30340;&#20844;&#24179;&#24615;&#24178;&#39044;&#25514;&#26045;&#30340;&#36866;&#24212;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adapting Fairness Interventions to Missing Values. (arXiv:2305.19429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32570;&#22833;&#20540;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#30340;&#20998;&#31867;&#12290;&#20256;&#32479;&#26041;&#27861;&#20250;&#21152;&#21095;&#27495;&#35270;&#12290;&#26412;&#25991;&#35777;&#26126;&#20174;&#25554;&#34917;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#20250;&#24694;&#21270;&#32452;&#20844;&#24179;&#24615;&#21644;&#24179;&#22343;&#20934;&#30830;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#20844;&#24179;&#24178;&#39044;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#22788;&#29702;&#25152;&#26377;&#21487;&#33021;&#30340;&#32570;&#22833;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#20013;&#25968;&#25454;&#30340;&#32570;&#22833;&#20540;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#25552;&#20986;&#20102;&#26174;&#33879;&#32780;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#19981;&#21516;&#30340;&#26063;&#32676;&#21487;&#33021;&#19981;&#20250;&#21516;&#31561;&#22320;&#21463;&#21040;&#32570;&#22833;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#32780;&#22788;&#29702;&#32570;&#22833;&#20540;&#30340;&#26631;&#20934;&#31243;&#24207;&#65292;&#21363;&#20808;&#23545;&#25968;&#25454;&#36827;&#34892;&#25554;&#34917;&#65292;&#28982;&#21518;&#20351;&#29992;&#25554;&#34917;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#36825;&#20010;&#36807;&#31243;&#34987;&#31216;&#20026;&#8220;&#25554;&#34917;&#20877;&#20998;&#31867;&#8221;&#65292;&#20250;&#21152;&#21095;&#27495;&#35270;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#32570;&#22833;&#20540;&#22914;&#20309;&#24433;&#21709;&#31639;&#27861;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#20174;&#25554;&#34917;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#20250;&#26174;&#33879;&#24694;&#21270;&#21487;&#20197;&#23454;&#29616;&#30340;&#32452;&#20844;&#24179;&#24615;&#21644;&#24179;&#22343;&#20934;&#30830;&#24615;&#30340;&#20540;&#12290;&#36825;&#26159;&#22240;&#20026;&#25554;&#34917;&#25968;&#25454;&#20250;&#23548;&#33268;&#25968;&#25454;&#32570;&#22833;&#27169;&#24335;&#30340;&#20002;&#22833;&#65292;&#25968;&#25454;&#32570;&#22833;&#27169;&#24335;&#36890;&#24120;&#20250;&#20256;&#36798;&#26377;&#20851;&#39044;&#27979;&#26631;&#31614;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#32570;&#22833;&#20540;&#30340;&#20844;&#24179;&#20998;&#31867;&#12290;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#20844;&#24179;&#24178;&#39044;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#22788;&#29702;&#25152;&#26377;&#21487;&#33021;&#30340;&#32570;&#22833;&#27169;&#24335;&#65292;&#24182;&#20445;&#30041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing values in real-world data pose a significant and unique challenge to algorithmic fairness. Different demographic groups may be unequally affected by missing data, and the standard procedure for handling missing values where first data is imputed, then the imputed data is used for classification -- a procedure referred to as "impute-then-classify" -- can exacerbate discrimination. In this paper, we analyze how missing values affect algorithmic fairness. We first prove that training a classifier from imputed data can significantly worsen the achievable values of group fairness and average accuracy. This is because imputing data results in the loss of the missing pattern of the data, which often conveys information about the predictive label. We present scalable and adaptive algorithms for fair classification with missing values. These algorithms can be combined with any preexisting fairness-intervention algorithm to handle all possible missing patterns while preserving informatio
&lt;/p&gt;</description></item><item><title>&#12298;Zero-TPrune&#12299;&#26159;&#19968;&#20010;&#32771;&#34385;&#21040;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21644;&#30456;&#20284;&#24615;&#30340;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#36827;&#34892;&#20196;&#29260;&#21098;&#26525;&#65292;&#20197;&#27714;&#35299;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;Transformer&#27169;&#22411;&#21363;&#25554;&#21363;&#29992;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17328</link><description>&lt;p&gt;
&#12298;Zero-TPrune: &#22522;&#20110;&#39044;&#35757;&#32451;Transformers&#20851;&#27880;&#22270;&#30340;&#38646;&#23556;&#20987;&#20196;&#29260;&#21098;&#26525;&#26041;&#27861;&#12299;
&lt;/p&gt;
&lt;p&gt;
Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers. (arXiv:2305.17328v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17328
&lt;/p&gt;
&lt;p&gt;
&#12298;Zero-TPrune&#12299;&#26159;&#19968;&#20010;&#32771;&#34385;&#21040;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21644;&#30456;&#20284;&#24615;&#30340;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#36827;&#34892;&#20196;&#29260;&#21098;&#26525;&#65292;&#20197;&#27714;&#35299;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;Transformer&#27169;&#22411;&#21363;&#25554;&#21363;&#29992;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;Transformer&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#27169;&#22411;&#30340;&#20307;&#31215;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#32780;&#25512;&#29702;&#25104;&#26412;&#21017;&#38543;&#36755;&#20837;&#24207;&#21015;&#20013;&#20196;&#29260;&#25968;&#37327;&#30340;&#24179;&#26041;&#25552;&#39640;&#12290;&#20196;&#29260;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26032;&#20852;&#35299;&#20915;&#26041;&#27861;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#26131;&#20110;&#22312;&#21508;&#31181;Transformer&#25903;&#25345;&#30340;&#27169;&#22411;&#19978;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20196;&#29260;&#21098;&#26525;&#26041;&#27861;&#38656;&#35201;&#22312;&#21098;&#26525;&#21518;&#25110;&#26399;&#38388;&#36827;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#36825;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#25506;&#35752;&#20102;&#27809;&#26377;&#24494;&#35843;&#30340;&#21363;&#25554;&#21363;&#29992;&#30340;&#39044;&#35757;&#32451;Transformer&#30340;&#21098;&#26525;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#21482;&#32771;&#34385;&#20102;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Zero-TPrune&#65292;&#36825;&#26159;&#19968;&#31181;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#26082;&#32771;&#34385;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21448;&#32771;&#34385;&#30456;&#20284;&#24615;&#26469;&#25191;&#34892;&#20196;&#29260;&#21098;&#26525;&#12290;Zero-TPrune&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#20026;&#20196;&#29260;&#29983;&#25104;&#19968;&#20010;&#37325;&#35201;&#24615;&#25490;&#21517;&#24182;&#31227;&#38500;&#20449;&#24687;&#36739;&#23569;&#30340;&#20196;&#29260;&#12290;&#27880;&#24847;&#30697;&#38453;&#21487;&#29992;&#20110;&#25512;&#26029;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployment of Transformer models on the edge is increasingly challenging due to the exponentially growing model size and inference cost that scales quadratically with the number of tokens in the input sequence. Token pruning is an emerging solution to address this challenge due to its ease of deployment on various Transformer backbones. However, most token pruning methods require a computationally-expensive fine-tuning process after or during pruning, which is not desirable in many cases. Some recent works explore pruning of off-the-shelf pre-trained Transformers without fine-tuning. However, they only take the importance of tokens into consideration. In this work, we propose Zero-TPrune, the first zero-shot method that considers both the importance and similarity of tokens in performing token pruning. Zero-TPrune leverages the attention graph of pre-trained Transformer models to produce an importance rank for tokens and removes the less informative tokens. The attention matrix can be 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22522;&#20110;Haar&#38543;&#26426;&#37193;&#25110;&#27491;&#20132;&#28145;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26576;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#20250;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39640;&#26031;&#36807;&#31243;&#19981;&#33021;&#29992;&#20110;&#36890;&#36807;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#26469;&#26377;&#25928;&#39044;&#27979;QNN&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.09957</link><description>&lt;p&gt;
&#28145;&#24230;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#23545;&#24212;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep quantum neural networks form Gaussian processes. (arXiv:2305.09957v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22522;&#20110;Haar&#38543;&#26426;&#37193;&#25110;&#27491;&#20132;&#28145;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26576;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#20250;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39640;&#26031;&#36807;&#31243;&#19981;&#33021;&#29992;&#20110;&#36890;&#36807;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#26469;&#26377;&#25928;&#39044;&#27979;QNN&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#20174;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20808;&#39564;&#26465;&#20214;&#24320;&#22987;&#21021;&#22987;&#21270;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#38544;&#34255;&#23618;&#31070;&#32463;&#20803;&#25968;&#30446;&#36275;&#22815;&#22823;&#30340;&#26497;&#38480;&#19979;&#25910;&#25947;&#21040;&#39640;&#26031;&#36807;&#31243;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#20063;&#23384;&#22312;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;Haar&#38543;&#26426;&#37193;&#25110;&#27491;&#20132;&#28145;QNNs&#30340;&#26576;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#22312;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#32500;&#24230;$d$&#36275;&#22815;&#22823;&#26102;&#20250;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#12290;&#30001;&#20110;&#36755;&#20837;&#29366;&#24577;&#12289;&#27979;&#37327;&#30340;&#21487;&#35266;&#27979;&#37327;&#20197;&#21450;&#37193;&#30697;&#38453;&#30340;&#20803;&#32032;&#19981;&#29420;&#31435;&#31561;&#22240;&#32032;&#30340;&#20316;&#29992;&#65292;&#26412;&#25991;&#23545;&#36825;&#19968;&#32467;&#26524;&#30340;&#25512;&#23548;&#27604;&#32463;&#20856;&#24773;&#24418;&#26356;&#21152;&#24494;&#22937;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#19968;&#20010;&#37325;&#35201;&#21518;&#26524;&#26159;&#65292;&#36825;&#20010;&#32467;&#26524;&#24471;&#21040;&#30340;&#39640;&#26031;&#36807;&#31243;&#19981;&#33021;&#36890;&#36807;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#26469;&#26377;&#25928;&#22320;&#39044;&#27979;QNN&#30340;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;Haar&#38543;&#26426;QNNs&#20013;&#30340;&#27979;&#37327;&#29616;&#35937;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#35201;&#26356;&#20005;&#37325;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28436;&#21592;&#30340;&#38598;&#20013;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well known that artificial neural networks initialized from independent and identically distributed priors converge to Gaussian processes in the limit of large number of neurons per hidden layer. In this work we prove an analogous result for Quantum Neural Networks (QNNs). Namely, we show that the outputs of certain models based on Haar random unitary or orthogonal deep QNNs converge to Gaussian processes in the limit of large Hilbert space dimension $d$. The derivation of this result is more nuanced than in the classical case due the role played by the input states, the measurement observable, and the fact that the entries of unitary matrices are not independent. An important consequence of our analysis is that the ensuing Gaussian processes cannot be used to efficiently predict the outputs of the QNN via Bayesian statistics. Furthermore, our theorems imply that the concentration of measure phenomenon in Haar random QNNs is much worse than previously thought, as we prove that ex
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#27973;&#23618;&#25110;&#32447;&#24615;&#35299;&#30721;&#36716;&#25442;&#26469;&#32553;&#23567;&#35299;&#30721;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#21033;&#29992;&#36890;&#24120;&#19981;&#23545;&#31216;&#30340;&#35745;&#31639;&#39044;&#31639;&#12289;&#26356;&#24378;&#30340;&#32534;&#30721;&#32593;&#32476;&#21644;&#36845;&#20195;&#32534;&#30721;&#26469;&#20445;&#25345;&#21387;&#32553;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#31454;&#20105;&#65292;&#35299;&#30721;&#22797;&#26434;&#24230;&#38477;&#20302;80%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2304.06244</link><description>&lt;p&gt;
&#20855;&#26377;&#27973;&#23618;&#35299;&#30721;&#22120;&#30340;&#19981;&#23545;&#31216;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Asymmetrically-powered Neural Image Compression with Shallow Decoders. (arXiv:2304.06244v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06244
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#27973;&#23618;&#25110;&#32447;&#24615;&#35299;&#30721;&#36716;&#25442;&#26469;&#32553;&#23567;&#35299;&#30721;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#21033;&#29992;&#36890;&#24120;&#19981;&#23545;&#31216;&#30340;&#35745;&#31639;&#39044;&#31639;&#12289;&#26356;&#24378;&#30340;&#32534;&#30721;&#32593;&#32476;&#21644;&#36845;&#20195;&#32534;&#30721;&#26469;&#20445;&#25345;&#21387;&#32553;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#31454;&#20105;&#65292;&#35299;&#30721;&#22797;&#26434;&#24230;&#38477;&#20302;80%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#34920;&#29616;&#36234;&#26469;&#36234;&#24378;&#12290;&#20294;&#19982;&#20256;&#32479;&#32534;&#35299;&#30721;&#22120;&#30456;&#27604;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#20986;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#36825;&#25104;&#20026;&#23454;&#38469;&#37096;&#32626;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#27973;&#23618;&#29978;&#33267;&#32447;&#24615;&#35299;&#30721;&#36716;&#25442;&#26469;&#32553;&#23567;&#35299;&#30721;&#22797;&#26434;&#24230;&#30340;&#24046;&#36317;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#20026;&#20102;&#24357;&#34917;&#30001;&#27492;&#23548;&#33268;&#30340;&#21387;&#32553;&#24615;&#33021;&#19979;&#38477;&#65292;&#20316;&#32773;&#21033;&#29992;&#32534;&#30721;&#21644;&#35299;&#30721;&#20043;&#38388;&#36890;&#24120;&#19981;&#23545;&#31216;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#37319;&#29992;&#26356;&#24378;&#22823;&#30340;&#32534;&#30721;&#32593;&#32476;&#21644;&#36845;&#20195;&#32534;&#30721;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36825;&#20010;&#24819;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#22312;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#30340;&#36895;&#29575;&#22833;&#30495;&#21644;&#35299;&#30721;&#22797;&#26434;&#24230;&#30340;&#26435;&#34913;&#26041;&#38754;&#24320;&#21551;&#20102;&#26032;&#30340;&#21069;&#27839;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;Minnen&#31561;&#20154;&#65288;2018&#65289;&#30340;&#24179;&#22343;&#27604;&#20363;&#36229;&#20808;&#39564;&#20307;&#31995;&#32467;&#26500;&#30456;&#31454;&#20105;&#30340;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23567;&#20102;&#24635;&#20307;&#35299;&#30721;&#22797;&#26434;&#24230;80&#65285;&#65292;&#25110;&#32773;&#36229;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural image compression methods have seen increasingly strong performance in recent years. However, they suffer orders of magnitude higher computational complexity compared to traditional codecs, which stands in the way of real-world deployment. This paper takes a step forward in closing this gap in decoding complexity by adopting shallow or even linear decoding transforms. To compensate for the resulting drop in compression performance, we exploit the often asymmetrical computation budget between encoding and decoding, by adopting more powerful encoder networks and iterative encoding. We theoretically formalize the intuition behind, and our experimental results establish a new frontier in the trade-off between rate-distortion and decoding complexity for neural image compression. Specifically, we achieve rate-distortion performance competitive with the established mean-scale hyperprior architecture of Minnen et al. (2018), while reducing the overall decoding complexity by 80 %, or ove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05727</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#39044;&#38450;&#24615;&#20462;&#21098;Clever Hans&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks. (arXiv:2304.05727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#24050;&#25104;&#20026;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#31574;&#30053;&#19982;&#29992;&#25143;&#30340;&#39046;&#22495;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65288;&#20363;&#22914;Clever Hans&#25928;&#24212;&#65289;&#20063;&#34987;&#35748;&#20026;&#26159;&#25913;&#36827;&#38169;&#35823;&#27169;&#22411;&#30340;&#36215;&#28857;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#35299;&#37322;&#36798;&#25104;&#19968;&#33268;&#26102;&#65292;&#35201;&#24590;&#20040;&#20570;&#23601;&#19981;&#37027;&#20040;&#28165;&#26970;&#20102;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#29992;&#25143;&#25509;&#21463;&#35299;&#37322;&#24182;&#19981;&#20445;&#35777;ML&#27169;&#22411;&#30340;&#33391;&#22909;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#19968;&#20123;&#38544;&#34255;&#30340;Clever Hans&#25928;&#24212;&#21487;&#33021;&#20173;&#28982;&#26410;&#34987;&#21457;&#29616;&#65292;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#36129;&#29486;&#19968;&#20010;&#26032;&#26041;&#27861;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#12290;&#33258;&#28982;&#30011;&#20687;&#25968;&#25454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#27169;&#22411;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;&#30340;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#22240;&#27492;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI has become a popular tool for validating machine learning models. Mismatches between the explained model's decision strategy and the user's domain knowledge (e.g. Clever Hans effects) have also been recognized as a starting point for improving faulty models. However, it is less clear what to do when the user and the explanation agree. In this paper, we demonstrate that acceptance of explanations by the user is not a guarantee for a ML model to function well, in particular, some Clever Hans effects may remain undetected. Such hidden flaws of the model can nevertheless be mitigated, and we demonstrate this by contributing a new method, Explanation-Guided Exposure Minimization (EGEM), that premptively prunes variations in the ML model that have not been the subject of positive explanation feedback. Experiments on natural image data demonstrate that our approach leads to models that strongly reduce their reliance on hidden Clever Hans strategies, and consequently achieve hig
&lt;/p&gt;</description></item><item><title>PriorCVAE &#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564; MCMC &#21442;&#25968;&#25512;&#26029;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#36807;&#23558; VAE &#24314;&#27169;&#26465;&#20214;&#21270;&#20110;&#38543;&#26426;&#36807;&#31243;&#36229;&#21442;&#25968;&#22788;&#29702;&#36229;&#21442;&#25968;&#25512;&#26029;&#19982;&#23398;&#20064;&#20808;&#39564;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#26029;&#35010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04307</link><description>&lt;p&gt;
PriorCVAE: &#22522;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637; MCMC &#21442;&#25968;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
PriorCVAE: scalable MCMC parameter inference with Bayesian deep generative modelling. (arXiv:2304.04307v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04307
&lt;/p&gt;
&lt;p&gt;
PriorCVAE &#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564; MCMC &#21442;&#25968;&#25512;&#26029;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#36807;&#23558; VAE &#24314;&#27169;&#26465;&#20214;&#21270;&#20110;&#38543;&#26426;&#36807;&#31243;&#36229;&#21442;&#25968;&#22788;&#29702;&#36229;&#21442;&#25968;&#25512;&#26029;&#19982;&#23398;&#20064;&#20808;&#39564;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#26029;&#35010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#25512;&#29702;&#36895;&#24230;&#21644;&#27169;&#22411;&#28789;&#27963;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#20855;&#26377;&#38543;&#26426;&#36807;&#31243;&#20808;&#39564;&#30340;&#27169;&#22411;&#20013;&#65288;&#22914;&#39640;&#26031;&#36807;&#31243;&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#31561;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#32534;&#30721;&#30001; GP &#20808;&#39564;&#25110;&#20854;&#26377;&#38480;&#23454;&#29616;&#24341;&#36215;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#24182;&#19988;&#25152;&#23398;&#29983;&#25104;&#22120;&#21487;&#20197;&#20195;&#26367; MCMC &#25512;&#26029;&#20013;&#30340;&#21407;&#22987;&#20808;&#39564;&#12290;&#34429;&#28982;&#27492;&#26041;&#27861;&#23454;&#29616;&#20102;&#24555;&#36895;&#32780;&#39640;&#25928;&#30340;&#25512;&#29702;&#65292;&#20294;&#23427;&#20002;&#22833;&#20102;&#20851;&#20110;&#38543;&#26426;&#36807;&#31243;&#36229;&#21442;&#25968;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#36229;&#21442;&#25968;&#25512;&#26029;&#19981;&#21487;&#33021;&#21644;&#23398;&#21040;&#30340;&#20808;&#39564;&#27169;&#31946;&#19981;&#28165;&#12290;&#25105;&#20204;&#24314;&#35758;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558; VAE &#24314;&#27169;&#26465;&#20214;&#21270;&#20110;&#38543;&#26426;&#36807;&#31243;&#36229;&#21442;&#25968;&#65292;&#20197;&#20415;&#36229;&#21442;&#25968;&#19982; GP &#23454;&#29616;&#19968;&#36215;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In applied fields where the speed of inference and model flexibility are crucial, the use of Bayesian inference for models with a stochastic process as their prior, e.g. Gaussian processes (GPs) is ubiquitous. Recent literature has demonstrated that the computational bottleneck caused by GP priors or their finite realizations can be encoded using deep generative models such as variational autoencoders (VAEs), and the learned generators can then be used instead of the original priors during Markov chain Monte Carlo (MCMC) inference in a drop-in manner. While this approach enables fast and highly efficient inference, it loses information about the stochastic process hyperparameters, and, as a consequence, makes inference over hyperparameters impossible and the learned priors indistinct. We propose to resolve the aforementioned issue and disentangle the learned priors by conditioning the VAE on stochastic process hyperparameters. This way, the hyperparameters are encoded alongside GP real
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#32463;&#20856;&#35748;&#30693;&#24515;&#29702;&#23398;&#25216;&#26415;&#26469;&#20272;&#31639;&#20154;&#31867;&#21644;GPT-3&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#35821;&#20041;&#32467;&#26500;&#65292;&#32467;&#26524;&#34920;&#26126;&#20154;&#31867;&#30340;&#27010;&#24565;&#32467;&#26500;&#31283;&#20581;&#40065;&#26834;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#20272;&#31639;&#32467;&#26500;&#26356;&#22810;&#21462;&#20915;&#20110;&#20855;&#20307;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.02754</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#32467;&#26500;&#34920;&#29616;&#30340;&#24046;&#24322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Behavioral estimates of conceptual structure are robust across tasks in humans but not large language models. (arXiv:2304.02754v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#32463;&#20856;&#35748;&#30693;&#24515;&#29702;&#23398;&#25216;&#26415;&#26469;&#20272;&#31639;&#20154;&#31867;&#21644;GPT-3&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#35821;&#20041;&#32467;&#26500;&#65292;&#32467;&#26524;&#34920;&#26126;&#20154;&#31867;&#30340;&#27010;&#24565;&#32467;&#26500;&#31283;&#20581;&#40065;&#26834;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#20272;&#31639;&#32467;&#26500;&#26356;&#22810;&#21462;&#20915;&#20110;&#20855;&#20307;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#20197;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#19968;&#30452;&#34987;&#29992;&#20316;&#30740;&#31350;&#24515;&#29702;&#21644;&#33041;&#37096;&#27010;&#24565;&#34920;&#24449;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#24403;&#20195;&#35821;&#35328;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#20960;&#20046;&#30456;&#21516;&#30340;&#26041;&#27861;&#26469;&#25506;&#35752;&#27010;&#24565;&#34920;&#24449;&#30340;&#28508;&#22312;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#32463;&#20856;&#30340;&#35748;&#30693;&#24515;&#29702;&#23398;&#25216;&#26415;&#26469;&#20272;&#31639;&#21644;&#27604;&#36739;&#20154;&#31867;&#21644;&#19968;&#20010;&#33879;&#21517;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-3&#30340;DaVinci&#21464;&#20307;&#65289;&#30340;&#35789;&#27719;&#35821;&#20041;&#32467;&#26500;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#30340;&#27010;&#24565;&#32467;&#26500;&#24378;&#22823;&#19988;&#40065;&#26834;&#65292;&#19981;&#21463;&#25991;&#21270;&#12289;&#35821;&#35328;&#21644;&#20272;&#31639;&#26041;&#27861;&#30340;&#24046;&#24322;&#24433;&#21709;&#65307;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#34892;&#20026;&#20272;&#31639;&#32467;&#26524;&#30456;&#23545;&#31283;&#23450;&#65292;&#20294;&#20855;&#20307;&#21462;&#20915;&#20110;&#20219;&#21153;&#26412;&#36523;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#20272;&#31639;&#32467;&#26524;&#21487;&#38752;&#65292;&#20294;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#31867;&#35748;&#30693;&#22788;&#29702;&#30456;&#20851;&#25512;&#26029;&#26102;&#65292;&#38656;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network models of language have long been used as a tool for developing hypotheses about conceptual representation in the mind and brain. For many years, such use involved extracting vector-space representations of words and using distances among these to predict or understand human behavior in various semantic tasks. In contemporary language AIs, however, it is possible to interrogate the latent structure of conceptual representations using methods nearly identical to those commonly used with human participants. The current work uses two common techniques borrowed from cognitive psychology to estimate and compare lexical-semantic structure in both humans and a well-known AI, the DaVinci variant of GPT-3. In humans, we show that conceptual structure is robust to differences in culture, language, and method of estimation. Structures estimated from AI behavior, while individually fairly consistent with those estimated from human behavior, depend much more upon the particular task 
&lt;/p&gt;</description></item><item><title>&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HALO&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#65292;&#19988;&#26080;&#38656;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02169</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#32423;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26497;&#39640;&#32500;&#38271;&#26399;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Synthesize Extremely High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model. (arXiv:2304.02169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02169
&lt;/p&gt;
&lt;p&gt;
&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HALO&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#65292;&#19988;&#26080;&#38656;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHRs)&#33021;&#22815;&#22312;&#26426;&#22120;&#23398;&#20064;(ML)&#21644;&#32479;&#35745;&#20998;&#26512;&#20013;&#20316;&#20026;&#30495;&#23454;EHRs&#30340;&#26367;&#20195;&#21697;&#65292;&#26082;&#30495;&#23454;&#21448;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#65292;&#20197;&#20854;&#21407;&#22987;&#39640;&#24230;&#32500;&#24418;&#24335;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#25968;&#25454;&#23545;&#29616;&#26377;&#26041;&#27861;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Hierarchical Autoregressive Language mOdel (HALO)&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#32437;&#21521;&#39640;&#32500;EHR&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#30495;&#23454;EHR&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#32780;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;HALO&#26041;&#27861;&#34987;&#35774;&#35745;&#20026;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29983;&#25104;&#19968;&#32452;&#38024;&#23545;&#21307;&#23398;&#20195;&#30721;&#12289;&#20020;&#24202;&#23601;&#35786;&#21644;&#30149;&#20154;&#35760;&#24405;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#20854;&#21407;&#22987;&#26410;&#32858;&#21512;&#24418;&#24335;&#19979;&#29983;&#25104;&#30495;&#23454;&#30340;EHR&#25968;&#25454;&#65292;&#26080;&#38656;&#36827;&#34892;&#21464;&#37327;&#36873;&#25321;&#25110;&#32858;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#20135;&#29983;&#22823;&#37327;&#30340;&#38543;&#26426;&#26679;&#26412;&#65292;&#20197;&#25552;&#20379;&#22797;&#26434;&#24230;&#36739;&#20302;&#20294;&#20173;&#26377;&#24847;&#20041;&#30340;EHR&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic electronic health records (EHRs) that are both realistic and preserve privacy can serve as an alternative to real EHRs for machine learning (ML) modeling and statistical analysis. However, generating high-fidelity and granular electronic health record (EHR) data in its original, highly-dimensional form poses challenges for existing methods due to the complexities inherent in high-dimensional data. In this paper, we propose Hierarchical Autoregressive Language mOdel (HALO) for generating longitudinal high-dimensional EHR, which preserve the statistical properties of real EHR and can be used to train accurate ML models without privacy concerns. Our HALO method, designed as a hierarchical autoregressive model, generates a probability density function of medical codes, clinical visits, and patient records, allowing for the generation of realistic EHR data in its original, unaggregated form without the need for variable selection or aggregation. Additionally, our model also produc
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#34987;&#24212;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#21160;&#24577;&#27169;&#25311;&#65292;&#21487;&#20197;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;10&#33267;1000&#20493;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#21644;&#25968;&#20540;&#31283;&#23450;&#24615;&#20063;&#36275;&#22815;&#28385;&#36275;&#38656;&#27714;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35268;&#33539;&#21270;NN&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.08994</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#22495;&#27169;&#25311;: &#31934;&#24230;&#65292;&#35745;&#31639;&#25104;&#26412;&#21644;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks for Time-Domain Simulations: Accuracy, Computational Cost, and Flexibility. (arXiv:2303.08994v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08994
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#34987;&#24212;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#21160;&#24577;&#27169;&#25311;&#65292;&#21487;&#20197;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;10&#33267;1000&#20493;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#21644;&#25968;&#20540;&#31283;&#23450;&#24615;&#20063;&#36275;&#22815;&#28385;&#36275;&#38656;&#27714;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35268;&#33539;&#21270;NN&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#31995;&#32479;&#21160;&#24577;&#27169;&#25311;&#26159;&#19968;&#20010;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#20219;&#21153;&#12290;&#32771;&#34385;&#21040;&#21457;&#30005;&#21644;&#38656;&#27714;&#27169;&#24335;&#19981;&#30830;&#23450;&#24615;&#30340;&#22686;&#21152;&#65292;&#38656;&#35201;&#19981;&#26029;&#35780;&#20272;&#25968;&#21315;&#20010;&#22330;&#26223;&#20197;&#30830;&#20445;&#30005;&#21147;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;&#21152;&#36895;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#35745;&#31639;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#30005;&#21147;&#31995;&#32479;&#21160;&#21147;&#23398;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#23545;&#36127;&#36733;&#25200;&#21160;&#30340;&#21160;&#24577;&#21709;&#24212;&#12290;&#36890;&#36807;&#23558; PINNs &#39044;&#27979;&#19982;&#20256;&#32479;&#27714;&#35299;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616; PINNs &#21487;&#20197;&#27604;&#20256;&#32479;&#27714;&#35299;&#22120;&#24555; 10 &#21040; 1000 &#20493;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#21363;&#20351;&#23545;&#20110;&#22823;&#26102;&#38388;&#27493;&#38271;&#20063;&#36275;&#22815;&#20934;&#30830;&#21644;&#25968;&#20540;&#31283;&#23450;&#12290;&#20026;&#20102;&#20419;&#36827;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#26412;&#25991;&#36824;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#24341;&#20837;&#22522;&#20110;&#26799;&#24230;&#30340;&#39033;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#35757;&#32451;&#30340;&#26032;&#35268;&#33539;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The simulation of power system dynamics poses a computationally expensive task. Considering the growing uncertainty of generation and demand patterns, thousands of scenarios need to be continuously assessed to ensure the safety of power systems. Physics-Informed Neural Networks (PINNs) have recently emerged as a promising solution for drastically accelerating computations of non-linear dynamical systems. This work investigates the applicability of these methods for power system dynamics, focusing on the dynamic response to load disturbances. Comparing the prediction of PINNs to the solution of conventional solvers, we find that PINNs can be 10 to 1000 times faster than conventional solvers. At the same time, we find them to be sufficiently accurate and numerically stable even for large time steps. To facilitate a deeper understanding, this paper also presents a new regularisation of Neural Network (NN) training by introducing a gradient-based term in the loss function. The resulting NN
&lt;/p&gt;</description></item><item><title>LExecutor&#26159;&#19968;&#20010;&#23398;&#20064;&#24341;&#23548;&#30340;&#25191;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#31070;&#32463;&#27169;&#22411;&#39044;&#27979;&#32570;&#22833;&#20540;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#21040;&#25191;&#34892;&#20013;&#65292;&#21487;&#20197;&#20197;&#33258;&#30001;&#32422;&#26463;&#30340;&#26041;&#24335;&#25191;&#34892;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#12290;&#35813;&#26041;&#27861;&#22312;Python&#20195;&#30721;&#21644;&#20174;Stack Overflow&#25552;&#21462;&#30340;&#20195;&#30721;&#29255;&#27573;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.02343</link><description>&lt;p&gt;
LExecutor: &#23398;&#20064;&#24341;&#23548;&#30340;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
LExecutor: Learning-Guided Execution. (arXiv:2302.02343v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02343
&lt;/p&gt;
&lt;p&gt;
LExecutor&#26159;&#19968;&#20010;&#23398;&#20064;&#24341;&#23548;&#30340;&#25191;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#31070;&#32463;&#27169;&#22411;&#39044;&#27979;&#32570;&#22833;&#20540;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#21040;&#25191;&#34892;&#20013;&#65292;&#21487;&#20197;&#20197;&#33258;&#30001;&#32422;&#26463;&#30340;&#26041;&#24335;&#25191;&#34892;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#12290;&#35813;&#26041;&#27861;&#22312;Python&#20195;&#30721;&#21644;&#20174;Stack Overflow&#25552;&#21462;&#30340;&#20195;&#30721;&#29255;&#27573;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25191;&#34892;&#20195;&#30721;&#23545;&#20110;&#21508;&#31181;&#31243;&#24207;&#20998;&#26512;&#20219;&#21153;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#20363;&#22914;&#36890;&#36807;&#24322;&#24120;&#26816;&#27979;&#38169;&#35823;&#25110;&#33719;&#21462;&#25191;&#34892;&#36319;&#36394;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#21160;&#24577;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#25191;&#34892;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#36890;&#24120;&#24456;&#22256;&#38590;&#65292;&#20363;&#22914;&#30001;&#20110;&#32570;&#23569;&#21464;&#37327;&#23450;&#20041;&#12289;&#32570;&#23569;&#29992;&#25143;&#36755;&#20837;&#21644;&#32570;&#23569;&#31532;&#19977;&#26041;&#20381;&#36182;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LExecutor&#65292;&#19968;&#31181;&#23398;&#20064;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#33258;&#30001;&#32422;&#26463;&#30340;&#26041;&#24335;&#25191;&#34892;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#35753;&#31070;&#32463;&#27169;&#22411;&#39044;&#27979;&#21542;&#21017;&#20250;&#23548;&#33268;&#31243;&#24207;&#20572;&#28382;&#30340;&#32570;&#22833;&#20540;&#65292;&#24182;&#23558;&#36825;&#20123;&#20540;&#27880;&#20837;&#21040;&#25191;&#34892;&#20013;&#12290;&#20363;&#22914;&#65292;LExecutor&#20026;&#26410;&#23450;&#20041;&#30340;&#21464;&#37327;&#27880;&#20837;&#21487;&#33021;&#30340;&#20540;&#65292;&#24182;&#20026;&#32570;&#22833;&#30340;&#20989;&#25968;&#35843;&#29992;&#36820;&#22238;&#39044;&#27979;&#21487;&#33021;&#30340;&#36820;&#22238;&#20540;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#27969;&#34892;&#24320;&#28304;&#39033;&#30446;&#30340;Python&#20195;&#30721;&#21644;&#20174;Stack Overflow&#20013;&#25552;&#21462;&#30340;&#20195;&#30721;&#29255;&#27573;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#12290;&#31070;&#32463;&#27169;&#22411;&#20197;79.5%&#21040;98.2%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;&#30495;&#23454;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Executing code is essential for various program analysis tasks, e.g., to detect bugs that manifest through exceptions or to obtain execution traces for further dynamic analysis. However, executing an arbitrary piece of code is often difficult in practice, e.g., because of missing variable definitions, missing user inputs, and missing third-party dependencies. This paper presents LExecutor, a learning-guided approach for executing arbitrary code snippets in an underconstrained way. The key idea is to let a neural model predict missing values that otherwise would cause the program to get stuck, and to inject these values into the execution. For example, LExecutor injects likely values for otherwise undefined variables and likely return values of calls to otherwise missing functions. We evaluate the approach on Python code from popular open-source projects and on code snippets extracted from Stack Overflow. The neural model predicts realistic values with an accuracy between 79.5% and 98.2
&lt;/p&gt;</description></item><item><title>NESTER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.04370</link><description>&lt;p&gt;
NESTER&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
NESTER: An Adaptive Neurosymbolic Method for Treatment Effect Estimation. (arXiv:2211.04370v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04370
&lt;/p&gt;
&lt;p&gt;
NESTER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#24402;&#32435;&#20559;&#32622;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#27599;&#31181;&#29616;&#26377;&#30340;&#25216;&#26415;&#37117;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#22120;&#26469;&#35299;&#20915;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#20363;&#22914;&#25511;&#21046;&#20542;&#21521;&#24471;&#20998;&#12289;&#24378;&#21046;&#38543;&#26426;&#21270;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#31216;&#20026;&#31070;&#32463;&#31526;&#21495;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#22120;&#65288;NESTER&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;NESTER&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;DSL&#65289;&#12290;&#25105;&#20204;&#36824;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;NESTER&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;NESTER&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment effect estimation from observational data is a central problem in causal inference. Methods based on potential outcomes framework solve this problem by exploiting inductive biases and heuristics from causal inference. Each existing technique addresses a specific aspect of treatment effect estimation, such as controlling propensity score, enforcing randomization, etc., by designing neural network architectures and regularizers. In this paper, we propose an adaptive method called Neurosymbolic Treatment Effect Estimator (NESTER), a generalized method for treatment effect estimation. NESTER brings together all the desiderata for treatment effect estimation into one framework. For this purpose, we design a Domain Specific Language (DSL) for the treatment effect estimation based on inductive biases used in literature. We also theoretically study NESTER's capability for the treatment effect estimation task. Our comprehensive empirical results show that NESTER performs better on ben
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#65292;&#24182;&#35780;&#20272;&#25104;&#26412;&#25935;&#24863;&#30340;PEGASOS SVM&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#26680;&#20989;&#25968;&#32435;&#20837;SVM&#20013;&#25193;&#23637;Ding&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2206.09311</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22987;&#20272;&#35745;&#20122;&#26799;&#24230;&#27714;&#35299;&#22120;&#30340;&#19981;&#24179;&#34913;&#20998;&#31867;SVM
&lt;/p&gt;
&lt;p&gt;
Primal Estimated Subgradient Solver for SVM for Imbalanced Classification. (arXiv:2206.09311v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#65292;&#24182;&#35780;&#20272;&#25104;&#26412;&#25935;&#24863;&#30340;PEGASOS SVM&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#26680;&#20989;&#25968;&#32435;&#20837;SVM&#20013;&#25193;&#23637;Ding&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#25104;&#26412;&#25935;&#24863;PEGASOS SVM&#22312;&#20027;&#22810;&#27425;&#35201;&#27604;&#20174;8.6&#65306;1&#21040;130&#65306;1&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#21253;&#25324;&#25130;&#36317;&#65288;&#20559;&#35265;&#65289;&#12289;&#27491;&#21017;&#21270;&#21644;&#21442;&#25968;&#26159;&#21542;&#20250;&#24433;&#21709;&#25105;&#20204;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#35768;&#22810;&#20154;&#37319;&#29992;SMOTE&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#26088;&#22312;&#37319;&#29992;&#19968;&#31181;&#35745;&#31639;&#37327;&#36739;&#23567;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26816;&#26597;&#23398;&#20064;&#26354;&#32447;&#26469;&#35780;&#20272;&#24615;&#33021;&#65292;&#36825;&#20123;&#26354;&#32447;&#21487;&#20197;&#35786;&#26029;&#25105;&#20204;&#26159;&#36807;&#24230;&#25311;&#21512;&#36824;&#26159;&#27424;&#25311;&#21512;&#65292;&#25110;&#32773;&#25105;&#20204;&#36873;&#25321;&#20102;&#36807;&#24230;&#20195;&#34920;&#24615;&#25110;&#27424;&#20195;&#34920;&#24615;&#30340;&#35757;&#32451;/&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#23558;&#22312;&#39564;&#35777;&#26354;&#32447;&#20013;&#26597;&#30475;&#36229;&#21442;&#25968;&#30340;&#32972;&#26223;&#19982;&#27979;&#35797;&#21644;&#35757;&#32451;&#35823;&#24046;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#22522;&#20934;&#21270;&#25105;&#20204;&#30340;PEGASOS&#25104;&#26412;&#25935;&#24863;SVM&#19982;Ding&#30340;LINEAR SVM DECIDL&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#20182;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#20102;0.5&#30340;ROC-AUC&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#36890;&#36807;&#23558;&#26680;&#20989;&#25968;&#32435;&#20837;SVM&#26469;&#25193;&#23637;Ding&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;Python&#32780;&#19981;&#26159;MATLAB&#65292;&#22240;&#20026;Python&#20855;&#26377;&#26356;&#26377;&#25928;&#22320;&#23384;&#20648;&#25105;&#30340;&#25968;&#25454;&#38598;&#30340;&#23383;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim to demonstrate in experiments that our cost sensitive PEGASOS SVM achieves good performance on imbalanced data sets with a Majority to Minority Ratio ranging from 8.6:1 to 130:1 and to ascertain whether the including intercept (bias), regularization and parameters affects performance on our selection of datasets. Although many resort to SMOTE methods, we aim for a less computationally intensive method. We evaluate the performance by examining the learning curves. These curves diagnose whether we overfit or underfit or we choose over representative or under representative training/test data. We will also see the background of the hyperparameters versus the test and train error in validation curves. We benchmark our PEGASOS Cost-Sensitive SVM's results of Ding's LINEAR SVM DECIDL method. He obtained an ROC-AUC of .5 in one dataset. Our work will extend the work of Ding by incorporating kernels into SVM. We will use Python rather than MATLAB as python has dictionaries for storing m
&lt;/p&gt;</description></item><item><title>&#29420;&#31435;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#22312;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#26377;&#25928;&#65292;&#36890;&#36807;&#26356;&#26032;Q&#20989;&#25968;&#21487;&#20197;&#24341;&#23548;&#31574;&#30053;&#25910;&#25947;&#21040;&#31283;&#23450;&#30340;&#32435;&#20160;&#24179;&#34913;&#28857;&#12290;</title><link>http://arxiv.org/abs/2205.14590</link><description>&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#30340;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Independent and Decentralized Learning in Markov Potential Games. (arXiv:2205.14590v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14590
&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#22312;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#26377;&#25928;&#65292;&#36890;&#36807;&#26356;&#26032;Q&#20989;&#25968;&#21487;&#20197;&#24341;&#23548;&#31574;&#30053;&#25910;&#25947;&#21040;&#31283;&#23450;&#30340;&#32435;&#20160;&#24179;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#30340;&#35774;&#32622;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#29609;&#23478;&#19981;&#20102;&#35299;&#28216;&#25103;&#27169;&#22411;&#65292;&#20063;&#19981;&#33021;&#36827;&#34892;&#21327;&#35843;&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#29609;&#23478;&#36890;&#36807;&#24322;&#27493;&#26041;&#24335;&#26356;&#26032;&#20182;&#20204;&#30340;&#25171;&#25200;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#20540;&#65292;&#35813;&#20989;&#25968;&#26681;&#25454;&#23454;&#29616;&#30340;&#19968;&#38454;&#27573;&#22870;&#21169;&#35780;&#20272;&#20182;&#20204;&#30340;&#24635;&#20307;&#26465;&#20214;&#20184;&#27454;&#12290;&#28982;&#21518;&#65292;&#29609;&#23478;&#36890;&#36807;&#23558;&#22522;&#20110;&#20272;&#35745;Q&#20989;&#25968;&#30340;&#24179;&#28369;&#26368;&#20248;&#19968;&#38454;&#27573;&#20559;&#24046;&#31574;&#30053;&#32435;&#20837;&#20854;&#31574;&#30053;&#20013;&#26469;&#29420;&#31435;&#22320;&#26356;&#26032;&#20854;&#31574;&#30053;&#12290;&#23398;&#20064;&#21160;&#24577;&#30340;&#20851;&#38190;&#29305;&#24449;&#26159;Q&#20989;&#25968;&#20272;&#35745;&#26159;&#20197;&#27604;&#31574;&#30053;&#26356;&#24555;&#30340;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#26356;&#26032;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#21160;&#24577;&#24341;&#23548;&#30340;&#31574;&#30053;&#22312;&#27010;&#29575;1&#30340;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#30340;&#31283;&#23450;&#32435;&#20160;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#31616;&#21333;&#23398;&#20064;&#21160;&#24577;&#22312;&#36798;&#21040;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#30340;&#31283;&#23450;&#32435;&#20160;&#24179;&#34913;&#26041;&#38754;&#30340;&#21151;&#25928;&#65292;&#21363;&#20351;&#26159;&#22312;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#20195;&#29702;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a multi-agent reinforcement learning dynamics, and analyze its convergence in infinite-horizon discounted Markov potential games. We focus on the independent and decentralized setting, where players do not have knowledge of the game model and cannot coordinate. In each stage, players update their estimate of a perturbed Q-function that evaluates their total contingent payoff based on the realized one-stage reward in an asynchronous manner. Then, players independently update their policies by incorporating a smoothed optimal one-stage deviation strategy based on the estimated Q-function. A key feature of the learning dynamics is that the Q-function estimates are updated at a faster timescale than the policies. We prove that the policies induced by our learning dynamics converge to a stationary Nash equilibrium in Markov potential games with probability 1. Our results highlight the efficacy of simple learning dynamics in reaching a stationary Nash equilibrium even in environme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#38544;&#31169;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#24182;&#26410;&#25552;&#20379;&#27604;&#32852;&#37030;&#23398;&#20064;&#26356;&#22909;&#30340;&#23433;&#20840;&#20248;&#21183;&#65292;&#21453;&#32780;&#22686;&#21152;&#20102;&#25915;&#20987;&#38754;&#12290;&#32780;&#19988;&#65292;&#38544;&#31169;&#20445;&#25252;&#37197;&#32622;&#38656;&#35201;&#20840;&#36830;&#25509;&#32593;&#32476;&#65292;&#22833;&#21435;&#20102;&#23454;&#38469;&#20248;&#21183;&#65292;&#23436;&#20840;&#25171;&#36133;&#20102;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2205.08443</link><description>&lt;p&gt;
&#20851;&#20110;&#28857;&#23545;&#28857;&#21435;&#20013;&#24515;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#65288;&#19981;&#65289;&#23433;&#20840;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the (In)security of Peer-to-Peer Decentralized Machine Learning. (arXiv:2205.08443v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#38544;&#31169;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#24182;&#26410;&#25552;&#20379;&#27604;&#32852;&#37030;&#23398;&#20064;&#26356;&#22909;&#30340;&#23433;&#20840;&#20248;&#21183;&#65292;&#21453;&#32780;&#22686;&#21152;&#20102;&#25915;&#20987;&#38754;&#12290;&#32780;&#19988;&#65292;&#38544;&#31169;&#20445;&#25252;&#37197;&#32622;&#38656;&#35201;&#20840;&#36830;&#25509;&#32593;&#32476;&#65292;&#22833;&#21435;&#20102;&#23454;&#38469;&#20248;&#21183;&#65292;&#23436;&#20840;&#25171;&#36133;&#20102;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#38544;&#31169;&#20998;&#26512;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#31181;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#30340;&#20027;&#35201;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21253;&#25324;&#34987;&#21160;&#21644;&#20027;&#21160;&#30340;&#21435;&#20013;&#24515;&#21270;&#25932;&#25163;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#25552;&#20986;&#32773;&#25152;&#22768;&#31216;&#30340;&#30456;&#21453;&#65292;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#24182;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#20851;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#20248;&#21183;&#12290;&#30456;&#21453;&#65292;&#23427;&#22686;&#21152;&#20102;&#25915;&#20987;&#38754;&#65292;&#20351;&#24471;&#31995;&#32479;&#20013;&#30340;&#20219;&#20309;&#29992;&#25143;&#37117;&#33021;&#22815;&#36827;&#34892;&#38544;&#31169;&#25915;&#20987;&#65292;&#20363;&#22914;&#26799;&#24230;&#36870;&#25512;&#65292;&#29978;&#33267;&#33719;&#24471;&#23545;&#35802;&#23454;&#29992;&#25143;&#30340;&#26412;&#22320;&#27169;&#22411;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#37492;&#20110;&#24403;&#21069;&#38450;&#25252;&#25216;&#26415;&#65292;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#37197;&#32622;&#38656;&#35201;&#20840;&#36830;&#25509;&#32593;&#32476;&#65292;&#22833;&#21435;&#20102;&#19982;&#32852;&#37030;&#35774;&#32622;&#30340;&#20219;&#20309;&#23454;&#38469;&#20248;&#21183;&#65292;&#22240;&#27492;&#23436;&#20840;&#25171;&#36133;&#20102;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we carry out the first, in-depth, privacy analysis of Decentralized Learning -- a collaborative machine learning framework aimed at addressing the main limitations of federated learning. We introduce a suite of novel attacks for both passive and active decentralized adversaries. We demonstrate that, contrary to what is claimed by decentralized learning proposers, decentralized learning does not offer any security advantage over federated learning. Rather, it increases the attack surface enabling any user in the system to perform privacy attacks such as gradient inversion, and even gain full control over honest users' local model. We also show that, given the state of the art in protections, privacy-preserving configurations of decentralized learning require fully connected networks, losing any practical advantage over the federated setup and therefore completely defeating the objective of the decentralized approach.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26080;&#27880;&#24847;&#21147;&#30340;&#24207;&#21015;&#27169;&#22411;SANSformer&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#39044;&#27979;&#65292;&#20805;&#20998;&#25366;&#25496;&#20102;Transformer&#22312;EHR&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#12290;&#20027;&#35201;&#24212;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#30340;&#21307;&#30103;&#36164;&#28304;&#21033;&#29992;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#19981;&#21516;&#24739;&#32773;&#23376;&#32452;&#65292;&#22914;&#32597;&#35265;&#30142;&#30149;&#24739;&#32773;&#12290;</title><link>http://arxiv.org/abs/2108.13672</link><description>&lt;p&gt;
SANSformers: &#26080;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SANSformers: Self-Supervised Forecasting in Electronic Health Records with Attention-Free Models. (arXiv:2108.13672v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.13672
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26080;&#27880;&#24847;&#21147;&#30340;&#24207;&#21015;&#27169;&#22411;SANSformer&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#39044;&#27979;&#65292;&#20805;&#20998;&#25366;&#25496;&#20102;Transformer&#22312;EHR&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#12290;&#20027;&#35201;&#24212;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#30340;&#21307;&#30103;&#36164;&#28304;&#21033;&#29992;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#19981;&#21516;&#24739;&#32773;&#23376;&#32452;&#65292;&#22914;&#32597;&#35265;&#30142;&#30149;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#31070;&#32463;&#32593;&#32476;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#24212;&#29992;&#38754;&#20020;&#25361;&#25112;&#65292;&#30001;&#20110;EHR&#25968;&#25454;&#20855;&#26377;&#29420;&#29305;&#30340;&#22810;&#32500;&#39034;&#24207;&#32467;&#26500;&#65292;&#25152;&#20197;&#19982;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#30456;&#27604;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;Transformer&#30340;&#20248;&#21183;&#65292;&#22914;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#21644;&#25913;&#36827;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;EHR&#24212;&#29992;&#20013;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SANSformer&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#27880;&#24847;&#21147;&#24207;&#21015;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#20197;&#36866;&#24212;EHR&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24212;&#29992;&#39046;&#22495;&#26159;&#39044;&#27979;&#26410;&#26469;&#30340;&#21307;&#30103;&#36164;&#28304;&#21033;&#29992;&#65292;&#36825;&#26159;&#26377;&#25928;&#20998;&#37197;&#21307;&#30103;&#36164;&#28304;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#24403;&#22788;&#29702;&#19981;&#21516;&#30340;&#24739;&#32773;&#23376;&#32452;&#26102;&#65292;&#36825;&#20010;&#20219;&#21153;&#21464;&#24471;&#29305;&#21035;&#22256;&#38590;&#12290;&#36825;&#20123;&#34987;&#21807;&#19968;&#30340;&#20581;&#24247;&#36712;&#36857;&#25152;&#29305;&#24449;&#21270;&#30340;&#23376;&#32452;&#65292;&#24448;&#24448;&#35268;&#27169;&#36739;&#23567;&#65292;&#22914;&#32597;&#35265;&#30142;&#30149;&#24739;&#32773;&#65292;&#38656;&#35201;&#29305;&#27530;&#30340;&#24314;&#27169;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#20010;&#23376;&#32452;&#30340;&#39044;&#27979;&#20219;&#21153;&#36716;&#21270;&#20026;&#19968;&#20010;&#25972;&#20307;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Transformer neural networks to Electronic Health Records (EHR) is challenging due to the distinct, multidimensional sequential structure of EHR data, often leading to underperformance when compared to simpler linear models. Thus, the advantages of Transformers, such as efficient transfer learning and improved scalability are not fully exploited in EHR applications. To overcome these challenges, we introduce SANSformer, a novel attention-free sequential model designed specifically with inductive biases to cater for the unique characteristics of EHR data.  Our main application area is predicting future healthcare utilization, a crucial task for effectively allocating healthcare resources. This task becomes particularly difficult when dealing with divergent patient subgroups. These subgroups, characterized by unique health trajectories and often small in size, such as patients with rare diseases, require specialized modeling approaches. To address this, we adopt a self-
&lt;/p&gt;</description></item><item><title>Chanakya&#26159;&#19968;&#31181;&#23398;&#20064;&#36817;&#20284;&#25191;&#34892;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#23454;&#26102;&#24863;&#30693;&#20013;&#24179;&#34913;&#31934;&#24230;&#21644;&#24310;&#36831;&#30340;&#35757;&#32451;&#65292;&#33258;&#21160;&#23398;&#20064;&#26435;&#34913;&#30340;&#20915;&#31574;&#65292;&#24182;&#22312;&#22810;&#20010;&#24863;&#30693;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.05665</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#26102;&#24863;&#30693;&#30340;&#36816;&#34892;&#26102;&#20915;&#31574;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Runtime Decisions for Adaptive Real-Time Perception. (arXiv:2106.05665v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.05665
&lt;/p&gt;
&lt;p&gt;
Chanakya&#26159;&#19968;&#31181;&#23398;&#20064;&#36817;&#20284;&#25191;&#34892;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#23454;&#26102;&#24863;&#30693;&#20013;&#24179;&#34913;&#31934;&#24230;&#21644;&#24310;&#36831;&#30340;&#35757;&#32451;&#65292;&#33258;&#21160;&#23398;&#20064;&#26435;&#34913;&#30340;&#20915;&#31574;&#65292;&#24182;&#22312;&#22810;&#20010;&#24863;&#30693;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#24863;&#30693;&#38656;&#35201;&#35745;&#21010;&#30340;&#36164;&#28304;&#21033;&#29992;&#12290;&#35745;&#31639;&#35268;&#21010;&#22312;&#23454;&#26102;&#24863;&#30693;&#20013;&#21463;&#31934;&#24230;&#21644;&#24310;&#36831;&#20004;&#20010;&#22240;&#32032;&#25511;&#21046;&#12290;&#23384;&#22312;&#21487;&#36816;&#34892;&#26102;&#20915;&#31574;&#65288;&#20363;&#22914;&#36873;&#25321;&#36755;&#20837;&#20998;&#36776;&#29575;&#65289;&#20250;&#24341;&#36215;&#26435;&#34913;&#65292;&#24433;&#21709;&#32473;&#23450;&#30828;&#20214;&#19978;&#30340;&#24615;&#33021;&#65292;&#26469;&#33258;&#22266;&#26377;&#65288;&#20869;&#23481;&#65292;&#20363;&#22914;&#22330;&#26223;&#28151;&#20081;&#65289;&#21644;&#22806;&#22312;&#65288;&#31995;&#32479;&#65292;&#20363;&#22914;&#36164;&#26009;&#20105;&#29992;&#65289;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Chanakya&#65292;&#19968;&#20010;&#23398;&#20064;&#36817;&#20284;&#25191;&#34892;&#26694;&#26550;&#65292;&#23427;&#33258;&#28982;&#22320;&#20174;&#27969;&#24335;&#24863;&#30693;&#33539;&#20363;&#20013;&#34893;&#29983;&#20986;&#26469;&#65292;&#20197;&#33258;&#21160;&#23398;&#20064;&#36825;&#20123;&#26435;&#34913;&#25152;&#24341;&#36215;&#30340;&#20915;&#31574;&#12290;Chanakya &#26159;&#36890;&#36807;&#26032;&#39062;&#30340;&#22870;&#21169;&#35757;&#32451;&#24179;&#34913;&#31934;&#24230;&#21644;&#24310;&#36831;&#26469;&#38544;&#24335;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#36817;&#20284;&#36825;&#20004;&#20010;&#30446;&#26631;&#12290;Chanakya &#21516;&#26102;&#32771;&#34385;&#20869;&#22312;&#21644;&#22806;&#22312;&#32972;&#26223;&#65292;&#20197;&#21450;&#26469;&#33258;&#22810;&#20010;&#35270;&#35282;&#30340;&#39044;&#27979;&#28145;&#24230;&#65292;&#20197;&#23398;&#20064;&#20174;&#22270;&#20687;&#24207;&#21015;&#21040;&#21160;&#24577;&#21487;&#35843;&#25972;&#30340;&#24863;&#30693;&#31639;&#27861;&#27969;&#27700;&#32447;&#30340;&#26144;&#23556;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#23454;&#26102;&#24863;&#30693;&#20013;&#30340;&#39640;&#25928;&#20915;&#31574;&#65292;&#24182;&#22312;&#22810;&#20010;&#24863;&#30693;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time perception requires planned resource utilization. Computational planning in real-time perception is governed by two considerations -- accuracy and latency. There exist run-time decisions (e.g. choice of input resolution) that induce tradeoffs affecting performance on a given hardware, arising from intrinsic (content, e.g. scene clutter) and extrinsic (system, e.g. resource contention) characteristics.  Earlier runtime execution frameworks employed rule-based decision algorithms and operated with a fixed algorithm latency budget to balance these concerns, which is sub-optimal and inflexible. We propose Chanakya, a learned approximate execution framework that naturally derives from the streaming perception paradigm, to automatically learn decisions induced by these tradeoffs instead. Chanakya is trained via novel rewards balancing accuracy and latency implicitly, without approximating either objectives. Chanakya simultaneously considers intrinsic and extrinsic context, and pred
&lt;/p&gt;</description></item></channel></rss>