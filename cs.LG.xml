<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#32508;&#21512;&#25351;&#21335;&#65292;&#21253;&#25324;&#29702;&#35770;&#22522;&#30784;&#12289;&#23454;&#38469;&#23454;&#29616;&#21644;&#27604;&#36739;&#32467;&#26524;&#65292;&#24182;&#23545;&#27491;&#21017;&#21270;&#30340;&#30410;&#22788;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.13662</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#26799;&#24230;&#30340;&#32456;&#26497;&#25351;&#21335;&#65306;&#29702;&#35770;&#12289;&#31639;&#27861;&#21644;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations. (arXiv:2401.13662v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#32508;&#21512;&#25351;&#21335;&#65292;&#21253;&#25324;&#29702;&#35770;&#22522;&#30784;&#12289;&#23454;&#38469;&#23454;&#29616;&#21644;&#27604;&#36739;&#32467;&#26524;&#65292;&#24182;&#23545;&#27491;&#21017;&#21270;&#30340;&#30410;&#22788;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#24378;&#22823;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#34429;&#28982;&#25152;&#26377;&#36825;&#20123;&#31639;&#27861;&#37117;&#24314;&#31435;&#22312;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#30340;&#22522;&#30784;&#19978;&#65292;&#20294;&#20855;&#20307;&#30340;&#35774;&#35745;&#36873;&#25321;&#22312;&#31639;&#27861;&#20043;&#38388;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25972;&#20307;&#30340;&#35270;&#35282;&#26469;&#27010;&#36848;&#22312;&#32447;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#20415;&#29702;&#35299;&#23427;&#20204;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#38469;&#23454;&#29616;&#12290;&#22312;&#36825;&#20010;&#27010;&#36848;&#20013;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#36830;&#32493;&#29256;&#26412;&#30340;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#30340;&#35814;&#32454;&#35777;&#26126;&#12289;&#25910;&#25947;&#32467;&#26524;&#21644;&#23545;&#23454;&#38469;&#31639;&#27861;&#30340;&#20840;&#38754;&#35752;&#35770;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#26368;&#37325;&#35201;&#30340;&#31639;&#27861;&#65292;&#24182;&#23545;&#27491;&#21017;&#21270;&#30340;&#30410;&#22788;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#35265;&#35299;&#12290;&#25152;&#26377;&#30340;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/Matt00n/PolicyGradientsJax&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, various powerful policy gradient algorithms have been proposed in deep reinforcement learning. While all these algorithms build on the Policy Gradient Theorem, the specific design choices differ significantly across algorithms. We provide a holistic overview of on-policy policy gradient algorithms to facilitate the understanding of both their theoretical foundations and their practical implementations. In this overview, we include a detailed proof of the continuous version of the Policy Gradient Theorem, convergence results and a comprehensive discussion of practical algorithms. We compare the most prominent algorithms on continuous control environments and provide insights on the benefits of regularization. All code is available at https://github.com/Matt00n/PolicyGradientsJax.
&lt;/p&gt;</description></item><item><title>MambaByte&#26159;&#19968;&#31181;&#26080;&#26631;&#35760;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#23383;&#33410;&#32423;&#21035;&#19978;&#36827;&#34892;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#33258;&#22238;&#24402;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#23637;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23376;&#35789;Transformer&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;MambaByte&#22312;&#26080;&#26631;&#35760;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13660</link><description>&lt;p&gt;
MambaByte: &#26080;&#26631;&#35760;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MambaByte: Token-free Selective State Space Model. (arXiv:2401.13660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13660
&lt;/p&gt;
&lt;p&gt;
MambaByte&#26159;&#19968;&#31181;&#26080;&#26631;&#35760;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#23383;&#33410;&#32423;&#21035;&#19978;&#36827;&#34892;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#33258;&#22238;&#24402;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#23637;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23376;&#35789;Transformer&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;MambaByte&#22312;&#26080;&#26631;&#35760;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26631;&#35760;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#20174;&#21407;&#22987;&#23383;&#33410;&#23398;&#20064;&#65292;&#28040;&#38500;&#20102;&#23376;&#35789;&#26631;&#35760;&#21270;&#30340;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#25805;&#20316;&#23383;&#33410;&#20250;&#23548;&#33268;&#24207;&#21015;&#38271;&#24230;&#26174;&#33879;&#22686;&#21152;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#33258;&#22238;&#24402;Transformer&#30340;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;MambaByte&#65292;&#23427;&#26159;&#22522;&#20110;&#23383;&#33410;&#24207;&#21015;&#33258;&#22238;&#24402;&#35757;&#32451;&#30340;&#26080;&#26631;&#35760;&#36866;&#24212;Mamba&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#23383;&#33410;&#32423;&#27169;&#22411;&#30456;&#27604;&#65292;MambaByte&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;MambaByte&#22312;&#24615;&#33021;&#19978;&#19982;&#29978;&#33267;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#23376;&#35789;Transformer&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#38271;&#24230;&#30340;&#32447;&#24615;&#25193;&#23637;&#65292;MambaByte&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#33719;&#24471;&#20102;&#24555;&#36895;&#24615;&#33021;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;Transformer&#21017;&#27809;&#26377;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#35777;&#23454;&#20102;MambaByte&#22312;&#23454;&#29616;&#26080;&#26631;&#35760;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. We experiment with MambaByte, a token-free adaptation of the Mamba state space model, trained autoregressively on byte sequences. Our experiments indicate the computational efficiency of MambaByte compared to other byte-level models. We also find MambaByte to be competitive with and even outperform state-of-the-art subword Transformers. Furthermore, owing to linear scaling in length, MambaByte benefits from fast inference compared to Transformers. Our findings establish the viability of MambaByte in enabling token-free language modeling.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#36716;&#31227;&#24773;&#20917;&#19979;&#36807;&#20110;&#33258;&#20449;&#12290;&#36825;&#24378;&#35843;&#20102;&#23545;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#21487;&#38752;&#20272;&#35745;&#21450;&#20854;&#21521;&#26368;&#32456;&#29992;&#25143;&#20256;&#36798;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13657</link><description>&lt;p&gt;
&#24120;&#35265;&#30340;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#22312;&#21487;&#38752;&#24615;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;
Inadequacy of common stochastic neural networks for reliable clinical decision support. (arXiv:2401.13657v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#36716;&#31227;&#24773;&#20917;&#19979;&#36807;&#20110;&#33258;&#20449;&#12290;&#36825;&#24378;&#35843;&#20102;&#23545;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#21487;&#38752;&#20272;&#35745;&#21450;&#20854;&#21521;&#26368;&#32456;&#29992;&#25143;&#20256;&#36798;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20262;&#29702;&#21644;&#23433;&#20840;&#30456;&#20851;&#30340;&#20851;&#20999;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#23398;&#20915;&#31574;&#20173;&#28982;&#21463;&#38459;&#12290;&#23545;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26469;&#35828;&#65292;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#36716;&#31227;&#19979;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#12290;&#36825;&#31181;&#22312;&#22522;&#20110;&#35777;&#25454;&#30340;&#22330;&#26223;&#20043;&#22806;&#19981;&#24688;&#24403;&#30340;&#25512;&#29702;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#12290;&#36825;&#20984;&#26174;&#20102;&#23545;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#21487;&#38752;&#20272;&#35745;&#21450;&#20854;&#21521;&#26368;&#32456;&#29992;&#25143;&#20256;&#36798;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#34987;&#35465;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20854;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#23454;&#38469;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#20197;&#20174;MIMIC3&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;EHR&#30340;ICU&#20303;&#38498;&#30149;&#27515;&#29575;&#39044;&#27979;&#20026;&#20363;&#26469;&#36827;&#34892;&#20998;&#26512;&#12290;&#23545;&#20110;EHR&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20165;&#32534;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#32435;&#20837;&#24120;&#35265;&#26041;&#27861;&#26469;&#23454;&#29616;&#27169;&#22411;&#20989;&#25968;&#30340;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Widespread adoption of AI for medical decision making is still hindered due to ethical and safety-related concerns. For AI-based decision support systems in healthcare settings it is paramount to be reliable and trustworthy. Common deep learning approaches, however, have the tendency towards overconfidence under data shift. Such inappropriate extrapolation beyond evidence-based scenarios may have dire consequences. This highlights the importance of reliable estimation of local uncertainty and its communication to the end user. While stochastic neural networks have been heralded as a potential solution to these issues, this study investigates their actual reliability in clinical applications. We centered our analysis on the exemplary use case of mortality prediction for ICU hospitalizations using EHR from MIMIC3 study. For predictions on the EHR time series, Encoder-Only Transformer models were employed. Stochasticity of model functions was achieved by incorporating common methods such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#19981;&#36830;&#32493;&#30028;&#38754;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#33021;&#21147;&#65292;&#22312;&#32500;&#24230;n = 2&#21644;n = 4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13652</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#32593;&#26684;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#30340;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors. (arXiv:2401.13652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#19981;&#36830;&#32493;&#30028;&#38754;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#33021;&#21147;&#65292;&#22312;&#32500;&#24230;n = 2&#21644;n = 4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#30340;&#19981;&#36830;&#32493;&#30028;&#38754;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;GINNs&#65289;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#35299;&#20915;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#12290;&#35757;&#32451;&#36807;&#30340;GINNs&#22312;&#31232;&#30095;&#32593;&#26684;&#19978;&#35782;&#21035;&#26377;&#38382;&#39064;&#30340;&#28857;&#65292;&#24182;&#21033;&#29992;&#26500;&#24314;&#22312;&#32593;&#26684;&#19978;&#30340;&#22270;&#32467;&#26500;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#36882;&#24402;&#31639;&#27861;&#29992;&#20110;&#19968;&#33324;&#30340;&#22522;&#20110;&#31232;&#30095;&#32593;&#26684;&#30340;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#26131;&#20110;&#24212;&#29992;&#24615;&#12290;&#22312;&#32500;&#24230;n=2&#21644;n=4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;GINNs&#22312;&#26816;&#27979;&#19981;&#36830;&#32493;&#30028;&#38754;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#21644;&#40065;&#26834;&#27867;&#21270;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;GINNs&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#21508;&#31181;&#31639;&#27861;&#20013;&#24182;&#20849;&#20139;&#32473;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function. This approach leverages Graph-Informed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances. We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability. Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization of GINNs in detecting discontinuity interfaces. Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users.
&lt;/p&gt;</description></item><item><title>VisualWebArena&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#27169;&#24577;Web&#20195;&#29702;&#24615;&#33021;&#30340;&#22522;&#20934;&#65292;&#22312;&#30495;&#23454;&#30340;&#8220;&#35270;&#35273;&#22522;&#30784;&#20219;&#21153;&#8221;&#19978;&#23545;&#20195;&#29702;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23427;&#35201;&#27714;&#20195;&#29702;&#20934;&#30830;&#22788;&#29702;&#22270;&#20687;-&#25991;&#26412;&#36755;&#20837;&#65292;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#21160;&#20316;&#26469;&#23436;&#25104;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.13649</link><description>&lt;p&gt;
VisualWebArena: &#22312;&#30495;&#23454;&#35270;&#35273;Web&#20219;&#21153;&#19978;&#35780;&#20272;&#22810;&#27169;&#24577;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks. (arXiv:2401.13649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13649
&lt;/p&gt;
&lt;p&gt;
VisualWebArena&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#27169;&#24577;Web&#20195;&#29702;&#24615;&#33021;&#30340;&#22522;&#20934;&#65292;&#22312;&#30495;&#23454;&#30340;&#8220;&#35270;&#35273;&#22522;&#30784;&#20219;&#21153;&#8221;&#19978;&#23545;&#20195;&#29702;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23427;&#35201;&#27714;&#20195;&#29702;&#20934;&#30830;&#22788;&#29702;&#22270;&#20687;-&#25991;&#26412;&#36755;&#20837;&#65292;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#21160;&#20316;&#26469;&#23436;&#25104;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#22312;&#32593;&#32476;&#19978;&#36827;&#34892;&#35745;&#21010;&#12289;&#25512;&#29702;&#21644;&#25191;&#34892;&#21160;&#20316;&#30340;&#33258;&#20027;&#20195;&#29702;&#20026;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;&#25991;&#26412;&#30340;&#20195;&#29702;&#65292;&#22312;&#25928;&#26524;&#19978;&#24573;&#35270;&#20102;&#35768;&#22810;&#38656;&#35201;&#35270;&#35273;&#20449;&#24687;&#25165;&#33021;&#26377;&#25928;&#35299;&#20915;&#30340;&#33258;&#28982;&#20219;&#21153;&#12290;&#37492;&#20110;&#22823;&#22810;&#25968;&#35745;&#31639;&#26426;&#30028;&#38754;&#26159;&#20026;&#20154;&#31867;&#24863;&#30693;&#32780;&#35774;&#35745;&#30340;&#65292;&#35270;&#35273;&#20449;&#24687;&#24448;&#24448;&#20197;&#25991;&#26412;&#25968;&#25454;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#30340;&#26041;&#24335;&#22686;&#24378;&#25991;&#26412;&#25968;&#25454;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VisualWebArena&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;Web&#20195;&#29702;&#22312;&#30495;&#23454;&#30340;&#8220;&#35270;&#35273;&#22522;&#30784;&#20219;&#21153;&#8221;&#19978;&#24615;&#33021;&#30340;&#22522;&#20934;&#12290;VisualWebArena&#21253;&#25324;&#19968;&#32452;&#22810;&#26679;&#19988;&#22797;&#26434;&#30340;&#22522;&#20110;Web&#30340;&#20219;&#21153;&#65292;&#35780;&#20272;&#33258;&#20027;&#22810;&#27169;&#24577;&#20195;&#29702;&#30340;&#21508;&#31181;&#33021;&#21147;&#12290;&#20026;&#20102;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#25191;&#34892;&#65292;&#20195;&#29702;&#38656;&#35201;&#20934;&#30830;&#22788;&#29702;&#22270;&#20687;-&#25991;&#26412;&#36755;&#20837;&#65292;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#21160;&#20316;&#20197;&#23436;&#25104;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an ext
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#32034;&#20102;&#22522;&#20110;GPT-4&#30340;ChatGPT&#22312;&#38754;&#37096;&#29983;&#29289;&#35782;&#21035;&#20013;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38754;&#37096;&#39564;&#35777;&#12289;&#36719;&#29983;&#29289;&#29305;&#24449;&#20272;&#35745;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ChatGPT&#30340;&#24212;&#29992;&#26377;&#26395;&#25552;&#39640;&#33258;&#21160;&#20915;&#31574;&#22312;&#20154;&#31867;&#22330;&#26223;&#20013;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.13641</link><description>&lt;p&gt;
ChatGPT&#22312;&#38754;&#37096;&#29983;&#29289;&#35782;&#21035;&#20013;&#30340;&#34920;&#29616;&#26377;&#22810;&#22909;&#65311;&#23545;&#35782;&#21035;&#12289;&#36719;&#29983;&#29289;&#29305;&#24449;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#21021;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability. (arXiv:2401.13641v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#32034;&#20102;&#22522;&#20110;GPT-4&#30340;ChatGPT&#22312;&#38754;&#37096;&#29983;&#29289;&#35782;&#21035;&#20013;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38754;&#37096;&#39564;&#35777;&#12289;&#36719;&#29983;&#29289;&#29305;&#24449;&#20272;&#35745;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ChatGPT&#30340;&#24212;&#29992;&#26377;&#26395;&#25552;&#39640;&#33258;&#21160;&#20915;&#31574;&#22312;&#20154;&#31867;&#22330;&#26223;&#20013;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;OpenAI&#24320;&#21457;&#30340;GPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#20026;&#25105;&#20204;&#30340;&#31038;&#20250;&#24341;&#20837;&#20102;&#24555;&#36895;&#21464;&#38761;&#12290;ChatGPT&#30340;&#21457;&#24067;&#36827;&#19968;&#27493;&#21152;&#24378;&#20102;&#36825;&#19968;&#24433;&#21709;&#65292;&#23427;&#20351;&#20219;&#20309;&#20154;&#37117;&#33021;&#20197;&#31616;&#21333;&#30340;&#23545;&#35805;&#26041;&#24335;&#19982;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#39046;&#22495;&#32463;&#39564;&#12290;&#22240;&#27492;&#65292;ChatGPT&#24050;&#34987;&#36805;&#36895;&#24212;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#22914;&#20195;&#30721;&#21644;&#27468;&#26354;&#21019;&#20316;&#12289;&#25945;&#32946;&#12289;&#34394;&#25311;&#21161;&#25163;&#31561;&#65292;&#23637;&#31034;&#20102;&#23545;&#20110;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#20219;&#21153;&#32780;&#35328;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65288;&#38646;&#26679;&#26412;&#23398;&#20064;&#65289;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#22522;&#20110;&#26368;&#26032;&#30340;GPT-4&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#30340;ChatGPT&#22312;&#38754;&#37096;&#29983;&#29289;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38754;&#37096;&#39564;&#35777;&#12289;&#36719;&#29983;&#29289;&#29305;&#24449;&#20272;&#35745;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ChatGPT&#23545;&#20110;&#36827;&#19968;&#27493;&#22686;&#21152;&#20154;&#31867;&#22330;&#26223;&#20013;&#33258;&#21160;&#20915;&#31574;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#23454;&#39564;&#34987;&#36827;&#34892;&#20197;&#35780;&#20272;ChatGPT&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as GPT developed by OpenAI, have already shown astonishing results, introducing quick changes in our society. This has been intensified by the release of ChatGPT which allows anyone to interact in a simple conversational way with LLMs, without any experience in the field needed. As a result, ChatGPT has been rapidly applied to many different tasks such as code- and song-writer, education, virtual assistants, etc., showing impressive results for tasks for which it was not trained (zero-shot learning).  The present study aims to explore the ability of ChatGPT, based on the recent GPT-4 multimodal LLM, for the task of face biometrics. In particular, we analyze the ability of ChatGPT to perform tasks such as face verification, soft-biometrics estimation, and explainability of the results. ChatGPT could be very valuable to further increase the explainability and transparency of the automatic decisions in human scenarios. Experiments are carried out in order
&lt;/p&gt;</description></item><item><title>&#36807;&#25311;&#21512;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#33021;&#22815;&#27867;&#21270;&#65292;&#32780;&#19988;&#21487;&#20197;&#36890;&#36807;&#21512;&#36866;&#30340;&#26465;&#20214;&#33719;&#24471;&#33391;&#22909;&#30340;&#40065;&#26834;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13624</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;&#33021;&#21542;&#27867;&#21270;&#65311;&#8212;&#8212;&#19968;&#20010;&#36817;&#20284;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Can overfitted deep neural networks in adversarial training generalize? -- An approximation viewpoint. (arXiv:2401.13624v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13624
&lt;/p&gt;
&lt;p&gt;
&#36807;&#25311;&#21512;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#33021;&#22815;&#27867;&#21270;&#65292;&#32780;&#19988;&#21487;&#20197;&#36890;&#36807;&#21512;&#36866;&#30340;&#26465;&#20214;&#33719;&#24471;&#33391;&#22909;&#30340;&#40065;&#26834;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23545;&#23545;&#25239;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#35266;&#23519;&#34920;&#26126;&#65292;&#22312;&#36807;&#21442;&#25968;&#21270;&#32593;&#32476;&#19978;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#24448;&#24448;&#20250;&#36973;&#21463;"&#40065;&#26834;&#24615;&#36807;&#25311;&#21512;"&#65306;&#23427;&#21487;&#20197;&#23454;&#29616;&#20960;&#20046;&#38646;&#30340;&#23545;&#25239;&#35757;&#32451;&#35823;&#24046;&#65292;&#32780;&#40065;&#26834;&#27867;&#21270;&#24615;&#33021;&#24182;&#19981;&#29702;&#24819;&#12290;&#26412;&#25991;&#20174;&#36817;&#20284;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#36807;&#25311;&#21512;&#30340;DNNs&#33021;&#21542;&#27867;&#21270;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#24635;&#32467;&#20026;&#19977;&#20010;&#26041;&#38754;&#65306;i) &#23545;&#20110;&#20998;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#30340;DNNs&#19978;&#21487;&#20197;&#26500;&#36896;&#20986;&#26080;&#38480;&#22810;&#20010;&#23545;&#25239;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#20854;&#33021;&#22815;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#19979;&#65288;&#28041;&#21450;&#25968;&#25454;&#36136;&#37327;&#65292;&#33391;&#22909;&#20998;&#31163;&#21644;&#25200;&#21160;&#31243;&#24230;&#65289;&#33719;&#24471;&#20219;&#24847;&#23567;&#30340;&#23545;&#25239;&#35757;&#32451;&#35823;&#24046;&#65288;&#36807;&#25311;&#21512;&#65289;&#65292;&#21516;&#26102;&#22312;&#40065;&#26834;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;ii) &#32447;&#24615;&#36229;&#36807;&#25311;&#21512;&#30340;DNNs&#20063;&#21487;&#20197;&#23454;&#29616;&#40065;&#26834;&#27867;&#21270;&#12290;iii) &#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is a widely used method to improve the robustness of deep neural networks (DNNs) over adversarial perturbations. However, it is empirically observed that adversarial training on over-parameterized networks often suffers from the \textit{robust overfitting}: it can achieve almost zero adversarial training error while the robust generalization performance is not promising. In this paper, we provide a theoretical understanding of the question of whether overfitted DNNs in adversarial training can generalize from an approximation viewpoint. Specifically, our main results are summarized into three folds: i) For classification, we prove by construction the existence of infinitely many adversarial training classifiers on over-parameterized DNNs that obtain arbitrarily small adversarial training error (overfitting), whereas achieving good robust generalization error under certain conditions concerning the data quality, well separated, and perturbation level. ii) Linear ove
&lt;/p&gt;</description></item><item><title>LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13586</link><description>&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#25552;&#31034;&#26435;&#37325;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13586
&lt;/p&gt;
&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23567;&#22411;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#22914;&#20309;&#24433;&#21709;&#22312;&#25351;&#20196;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;7B&#22823;&#23567;&#30340;LLaMA&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#37325;&#29616;&#20102;&#26031;&#22374;&#31119;&#22823;&#23398;&#30340;Alpaca&#23454;&#39564;&#65292;&#20854;&#20013;&#21253;&#25324;LLaMA 1&#21644;LLaMA 2&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19982;PLW&#20043;&#38388;&#23384;&#22312;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#22312;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19981;&#21463;PLW&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36793;&#32536;GPU&#19978;&#36827;&#34892;CNN&#26550;&#26500;&#25552;&#21462;&#30340;&#25915;&#20987;&#65292;&#36890;&#36807;&#20998;&#26512;GPU&#30340;&#30005;&#30913;&#36752;&#23556;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#20391;&#20449;&#36947;&#20998;&#26512;&#21487;&#36731;&#26494;&#21306;&#20998;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.13575</link><description>&lt;p&gt;
&#36793;&#32536;GPU&#19978;&#30340;CNN&#26550;&#26500;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
CNN architecture extraction on edge GPU. (arXiv:2401.13575v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36793;&#32536;GPU&#19978;&#36827;&#34892;CNN&#26550;&#26500;&#25552;&#21462;&#30340;&#25915;&#20987;&#65292;&#36890;&#36807;&#20998;&#26512;GPU&#30340;&#30005;&#30913;&#36752;&#23556;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#20391;&#20449;&#36947;&#20998;&#26512;&#21487;&#36731;&#26494;&#21306;&#20998;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22810;&#21151;&#33021;&#24615;&#21644;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#26368;&#26032;&#32467;&#26524;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35821;&#38899;&#35782;&#21035;&#12289;&#39044;&#27979;&#31561;&#65289;&#65292;&#31070;&#32463;&#32593;&#32476;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#24212;&#29992;&#20063;&#34987;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20391;&#20449;&#36947;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#23545;&#36870;&#21521;&#24037;&#31243;&#30340;&#25935;&#24863;&#24615;&#65292;&#20351;&#29992;NVIDIA Jetson Nano&#24494;&#22411;&#35745;&#31639;&#26426;&#36827;&#34892;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#25552;&#21462;&#25915;&#20987;&#12290;&#22312;&#25915;&#20987;&#20013;&#65292;&#23558;15&#31181;&#27969;&#34892;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;EfficientNets&#12289;MobileNets&#12289;NasNet&#31561;&#65289;&#22312;Jetson Nano&#30340;GPU&#19978;&#23454;&#29616;&#65292;&#24182;&#22312;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#25805;&#20316;&#26399;&#38388;&#20998;&#26512;GPU&#30340;&#30005;&#30913;&#36752;&#23556;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20391;&#20449;&#36947;&#20998;&#26512;&#21487;&#20197;&#36731;&#26494;&#21306;&#20998;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have become popular due to their versatility and state-of-the-art results in many applications, such as image classification, natural language processing, speech recognition, forecasting, etc. These applications are also used in resource-constrained environments such as embedded devices. In this work, the susceptibility of neural network implementations to reverse engineering is explored on the NVIDIA Jetson Nano microcomputer via side-channel analysis. To this end, an architecture extraction attack is presented. In the attack, 15 popular convolutional neural network architectures (EfficientNets, MobileNets, NasNet, etc.) are implemented on the GPU of Jetson Nano and the electromagnetic radiation of the GPU is analyzed during the inference operation of the neural networks. The results of the analysis show that neural network architectures are easily distinguishable using deep learning-based side-channel analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#21453;&#21521;&#35774;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26469;&#29983;&#25104;&#22522;&#20110;&#20307;&#32032;&#30340;&#26426;&#26800;&#36229;&#26448;&#26009;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#30701;&#30701;3&#31186;&#20869;&#29983;&#25104;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#30340;&#24494;&#32467;&#26500;&#65292;&#29992;&#20110;&#36924;&#36817;&#25351;&#23450;&#30340;&#22343;&#36136;&#21270;&#24352;&#37327;&#30697;&#38453;&#12290;&#36825;&#19968;&#24555;&#36895;&#30340;&#21453;&#21521;&#35774;&#35745;&#24037;&#20855;&#26377;&#21161;&#20110;&#25506;&#32034;&#26497;&#31471;&#36229;&#26448;&#26009;&#12289;&#36229;&#26448;&#26009;&#20013;&#30340;&#24207;&#21015;&#25554;&#20540;&#65292;&#20197;&#21450;&#29983;&#25104;&#22810;&#23610;&#24230;&#30340;&#22810;&#26679;&#24494;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.13570</link><description>&lt;p&gt;
&#23548;&#21521;&#25193;&#25955;&#29992;&#20110;&#24555;&#36895;&#21453;&#21521;&#35774;&#35745;&#22522;&#20110;&#23494;&#24230;&#30340;&#26426;&#26800;&#36229;&#26448;&#26009;
&lt;/p&gt;
&lt;p&gt;
Guided Diffusion for Fast Inverse Design of Density-based Mechanical Metamaterials. (arXiv:2401.13570v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#21453;&#21521;&#35774;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26469;&#29983;&#25104;&#22522;&#20110;&#20307;&#32032;&#30340;&#26426;&#26800;&#36229;&#26448;&#26009;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#30701;&#30701;3&#31186;&#20869;&#29983;&#25104;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#30340;&#24494;&#32467;&#26500;&#65292;&#29992;&#20110;&#36924;&#36817;&#25351;&#23450;&#30340;&#22343;&#36136;&#21270;&#24352;&#37327;&#30697;&#38453;&#12290;&#36825;&#19968;&#24555;&#36895;&#30340;&#21453;&#21521;&#35774;&#35745;&#24037;&#20855;&#26377;&#21161;&#20110;&#25506;&#32034;&#26497;&#31471;&#36229;&#26448;&#26009;&#12289;&#36229;&#26448;&#26009;&#20013;&#30340;&#24207;&#21015;&#25554;&#20540;&#65292;&#20197;&#21450;&#29983;&#25104;&#22810;&#23610;&#24230;&#30340;&#22810;&#26679;&#24494;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#26800;&#36229;&#26448;&#26009;&#26159;&#19968;&#31181;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#20854;&#20869;&#37096;&#32467;&#26500;&#65292;&#21487;&#20197;&#20855;&#26377;&#24322;&#24120;&#24377;&#24615;&#12289;&#21018;&#24230;&#21644;&#31283;&#23450;&#24615;&#31561;&#38750;&#20961;&#29289;&#29702;&#29305;&#24615;&#30340;&#21512;&#25104;&#26448;&#26009;&#12290;&#20026;&#20102;&#20351;&#36229;&#26448;&#26009;&#21253;&#21547;&#20855;&#26377;&#29420;&#29305;&#26426;&#26800;&#24615;&#33021;&#30340;&#31934;&#32454;&#23616;&#37096;&#32467;&#26500;&#65292;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#20307;&#32032;&#26469;&#34920;&#31034;&#23427;&#20204;&#26159;&#19968;&#31181;&#28508;&#22312;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#20026;&#27492;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#21453;&#21521;&#35774;&#35745;&#26041;&#27861;&#65292;&#20854;&#26680;&#24515;&#26159;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#22522;&#20110;&#20307;&#32032;&#30340;&#26426;&#26800;&#36229;&#26448;&#26009;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#20165;&#38656;3&#31186;&#30340;&#26102;&#38388;&#20869;&#29983;&#25104;&#20998;&#36776;&#29575;&#20026; $128^3$ &#30340;&#24494;&#32467;&#26500;&#65292;&#20197;&#36924;&#36817;&#25351;&#23450;&#30340;&#22343;&#36136;&#21270;&#24352;&#37327;&#30697;&#38453;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#24555;&#36895;&#30340;&#21453;&#21521;&#35774;&#35745;&#24037;&#20855;&#26377;&#21161;&#20110;&#25506;&#32034;&#26497;&#31471;&#36229;&#26448;&#26009;&#12289;&#36229;&#26448;&#26009;&#20013;&#30340;&#24207;&#21015;&#25554;&#20540;&#20197;&#21450;&#29983;&#25104;&#22810;&#23610;&#24230;&#30340;&#22810;&#26679;&#24494;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanical metamaterial is a synthetic material that can possess extraordinary physical characteristics, such as abnormal elasticity, stiffness, and stability, by carefully designing its internal structure. To make metamaterials contain delicate local structures with unique mechanical properties, it is a potential method to represent them through high-resolution voxels. However, it brings a substantial computational burden. To this end, this paper proposes a fast inverse design method, whose core is an advanced deep generative AI algorithm, to generate voxel-based mechanical metamaterials. Specifically, we use the self-conditioned diffusion model, capable of generating a microstructure with a resolution of $128^3$ to approach the specified homogenized tensor matrix in just 3 seconds. Accordingly, this rapid reverse design tool facilitates the exploration of extreme metamaterials, the sequence interpolation in metamaterials, and the generation of diverse microstructures for multi-scale 
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#24471;&#21040;&#30340;&#31070;&#32463;&#34920;&#31034;&#30340;&#20960;&#20309;&#32467;&#26500;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#24456;&#37325;&#35201;&#12290;&#30740;&#31350;&#21457;&#29616;&#28608;&#27963;&#20989;&#25968;&#23545;&#20960;&#20309;&#32467;&#26500;&#26377;&#37325;&#35201;&#24433;&#21709;&#65306;Tanh&#32593;&#32476;&#23398;&#21040;&#30340;&#34920;&#31034;&#21453;&#26144;&#20102;&#30446;&#26631;&#36755;&#20986;&#30340;&#32467;&#26500;&#65292;&#32780;ReLU&#32593;&#32476;&#20445;&#30041;&#20102;&#21407;&#22987;&#36755;&#20837;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.13558</link><description>&lt;p&gt;
&#20219;&#21153;&#32467;&#26500;&#21644;&#38750;&#32447;&#24615;&#20849;&#21516;&#20915;&#23450;&#20102;&#23398;&#20064;&#30340;&#34920;&#31034;&#20960;&#20309;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Task structure and nonlinearity jointly determine learned representational geometry. (arXiv:2401.13558v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13558
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#24471;&#21040;&#30340;&#31070;&#32463;&#34920;&#31034;&#30340;&#20960;&#20309;&#32467;&#26500;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#24456;&#37325;&#35201;&#12290;&#30740;&#31350;&#21457;&#29616;&#28608;&#27963;&#20989;&#25968;&#23545;&#20960;&#20309;&#32467;&#26500;&#26377;&#37325;&#35201;&#24433;&#21709;&#65306;Tanh&#32593;&#32476;&#23398;&#21040;&#30340;&#34920;&#31034;&#21453;&#26144;&#20102;&#30446;&#26631;&#36755;&#20986;&#30340;&#32467;&#26500;&#65292;&#32780;ReLU&#32593;&#32476;&#20445;&#30041;&#20102;&#21407;&#22987;&#36755;&#20837;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#23398;&#20064;&#20986;&#30340;&#31070;&#32463;&#34920;&#31034;&#30340;&#25928;&#29992;&#21462;&#20915;&#20110;&#23427;&#30340;&#20960;&#20309;&#32467;&#26500;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#25903;&#25345;&#31243;&#24230;&#12290;&#36825;&#20010;&#20960;&#20309;&#32467;&#26500;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#32467;&#26500;&#12289;&#30446;&#26631;&#36755;&#20986;&#30340;&#32467;&#26500;&#20197;&#21450;&#32593;&#32476;&#30340;&#32467;&#26500;&#12290;&#36890;&#36807;&#30740;&#31350;&#20855;&#26377;&#19968;&#20010;&#38544;&#34255;&#23618;&#30340;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#21457;&#29616;&#32593;&#32476;&#30340;&#28608;&#27963;&#20989;&#25968;&#23545;&#34920;&#31034;&#20960;&#20309;&#26377;&#19968;&#20010;&#24847;&#22806;&#30340;&#24378;&#28872;&#24433;&#21709;&#65306;Tanh&#32593;&#32476;&#20542;&#21521;&#20110;&#23398;&#20064;&#21453;&#26144;&#30446;&#26631;&#36755;&#20986;&#32467;&#26500;&#30340;&#34920;&#31034;&#65292;&#32780;ReLU&#32593;&#32476;&#20445;&#30041;&#20102;&#26356;&#22810;&#20851;&#20110;&#21407;&#22987;&#36755;&#20837;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;&#22312;&#19968;&#20010;&#21442;&#25968;&#21270;&#20219;&#21153;&#31867;&#20013;&#65292;&#25105;&#20204;&#35843;&#25972;&#20219;&#21153;&#36755;&#20837;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#20219;&#21153;&#26631;&#31614;&#30340;&#20960;&#20309;&#32467;&#26500;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#21457;&#29616;&#36825;&#31181;&#24046;&#24322;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#37117;&#26159;&#19968;&#33268;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#24182;&#23637;&#31034;&#20102;Tanh&#21644;ReLU&#38750;&#32447;&#24615;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#24322;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#28176;&#36827;&#34892;&#20026;&#30340;&#19981;&#23545;&#31216;&#24615;&#24341;&#36215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utility of a learned neural representation depends on how well its geometry supports performance in downstream tasks. This geometry depends on the structure of the inputs, the structure of the target outputs, and the architecture of the network. By studying the learning dynamics of networks with one hidden layer, we discovered that the network's activation function has an unexpectedly strong impact on the representational geometry: Tanh networks tend to learn representations that reflect the structure of the target outputs, while ReLU networks retain more information about the structure of the raw inputs. This difference is consistently observed across a broad class of parameterized tasks in which we modulated the degree of alignment between the geometry of the task inputs and that of the task labels. We analyzed the learning dynamics in weight space and show how the differences between the networks with Tanh and ReLU nonlinearities arise from the asymmetric asymptotic behavior of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#24212;&#29992;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#29616;&#20195;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#23545;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25581;&#31034;&#20102;&#19981;&#21516;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.13555</link><description>&lt;p&gt;
&#22270;&#20687;&#19978;&#37319;&#26679;&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Fairness of Image Upsampling Methods. (arXiv:2401.13555v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13555
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#24212;&#29992;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#29616;&#20195;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#23545;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25581;&#31034;&#20102;&#19981;&#21516;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#21019;&#24314;&#21512;&#25104;&#23186;&#20307;&#65288;&#22914;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#26085;&#24120;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#35825;&#20154;&#65292;&#20294;&#35780;&#20272;&#20854;&#20844;&#24179;&#24615;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#21463;&#30417;&#30563;&#20844;&#24179;&#24615;&#30340;&#28789;&#24863;&#26469;&#28304;&#8212;&#8212;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#36825;&#20010;&#29305;&#23450;&#24212;&#29992;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#21508;&#31181;&#29616;&#20195;&#19978;&#37319;&#26679;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20316;&#20026;&#22522;&#20934;&#27979;&#35797;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UnfairFace&#65292;&#36825;&#26159;FairFace&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#22797;&#21046;&#20102;&#24120;&#35265;&#22823;&#35268;&#27169;&#20154;&#33080;&#25968;&#25454;&#38598;&#30340;&#31181;&#26063;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#20984;&#26174;&#20102;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics$\unicode{x2013}$inspired by their supervised fairness counterparts$\unicode{x2013}$to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#40657;&#30418;&#27169;&#22411;&#21487;&#24178;&#39044;&#12290;&#36890;&#36807;&#22522;&#20110;&#27010;&#24565;&#30340;&#24178;&#39044;&#26469;&#24433;&#21709;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#23545;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#24178;&#39044;&#30340;&#25928;&#26524;&#65292;&#24182;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.13544</link><description>&lt;p&gt;
&#36229;&#36234;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65306;&#22914;&#20309;&#20351;&#40657;&#30418;&#27169;&#22411;&#21487;&#24178;&#39044;&#65311;
&lt;/p&gt;
&lt;p&gt;
Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?. (arXiv:2401.13544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#40657;&#30418;&#27169;&#22411;&#21487;&#24178;&#39044;&#12290;&#36890;&#36807;&#22522;&#20110;&#27010;&#24565;&#30340;&#24178;&#39044;&#26469;&#24433;&#21709;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#23545;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#24178;&#39044;&#30340;&#25928;&#26524;&#65292;&#24182;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#37325;&#26032;&#25506;&#32034;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#65292;&#21253;&#25324;&#20174;&#21407;&#22987;&#29305;&#24449;&#20013;&#36880;&#27493;&#39044;&#27979;&#39640;&#32423;&#27010;&#24565;&#21644;&#20174;&#39044;&#27979;&#30340;&#27010;&#24565;&#20013;&#39044;&#27979;&#30446;&#26631;&#21464;&#37327;&#12290;&#36825;&#20010;&#27169;&#22411;&#31867;&#21035;&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#20248;&#21183;&#26159;&#29992;&#25143;&#33021;&#22815;&#23545;&#39044;&#27979;&#30340;&#27010;&#24565;&#20540;&#36827;&#34892;&#24178;&#39044;&#65292;&#20174;&#32780;&#24433;&#21709;&#27169;&#22411;&#30340;&#19979;&#28216;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#24050;&#32463;&#35757;&#32451;&#22909;&#20294;&#26412;&#36136;&#19978;&#19981;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#22522;&#20110;&#27010;&#24565;&#30340;&#24178;&#39044;&#65292;&#32473;&#23450;&#19968;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#39564;&#35777;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#21487;&#24178;&#39044;&#24615;&#23450;&#20041;&#20026;&#22522;&#20110;&#27010;&#24565;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#30340;&#24230;&#37327;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#23450;&#20041;&#26469;&#23545;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#21644;&#33258;&#28982;&#22270;&#20687;&#22522;&#20934;&#19978;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#24178;&#39044;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24494;&#35843;&#25552;&#39640;&#20102;&#24178;&#39044;&#30340;&#25928;&#26524;&#65292;&#24182;&#32463;&#24120;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, interpretable machine learning has re-explored concept bottleneck models (CBM), comprising step-by-step prediction of the high-level concepts from the raw features and the target variable from the predicted concepts. A compelling advantage of this model class is the user's ability to intervene on the predicted concept values, affecting the model's downstream output. In this work, we introduce a method to perform such concept-based interventions on already-trained neural networks, which are not interpretable by design, given an annotated validation set. Furthermore, we formalise the model's intervenability as a measure of the effectiveness of concept-based interventions and leverage this definition to fine-tune black-box models. Empirically, we explore the intervenability of black-box classifiers on synthetic tabular and natural image benchmarks. We demonstrate that fine-tuning improves intervention effectiveness and often yields better-calibrated predictions. To showcase the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36974;&#34109;&#31890;&#23376;&#24314;&#27169;&#65288;MPM&#65289;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#33021;&#29289;&#29702;&#31185;&#23398;&#25968;&#25454;&#20013;&#26080;&#24207;&#36755;&#20837;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39044;&#35757;&#32451;&#23398;&#20064;&#32622;&#25442;&#19981;&#21464;&#30340;&#20989;&#25968;&#65292;&#22312;&#26500;&#24314;&#36866;&#29992;&#20110;&#22810;&#31181;&#20219;&#21153;&#30340;&#39640;&#33021;&#29289;&#29702;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.13537</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#21512;&#30340;&#36974;&#34109;&#31890;&#23376;&#24314;&#27169;&#65306;&#36208;&#21521;&#33258;&#30417;&#30563;&#39640;&#33021;&#29289;&#29702;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Masked Particle Modeling on Sets: Towards Self-Supervised High Energy Physics Foundation Models. (arXiv:2401.13537v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36974;&#34109;&#31890;&#23376;&#24314;&#27169;&#65288;MPM&#65289;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#33021;&#29289;&#29702;&#31185;&#23398;&#25968;&#25454;&#20013;&#26080;&#24207;&#36755;&#20837;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39044;&#35757;&#32451;&#23398;&#20064;&#32622;&#25442;&#19981;&#21464;&#30340;&#20989;&#25968;&#65292;&#22312;&#26500;&#24314;&#36866;&#29992;&#20110;&#22810;&#31181;&#20219;&#21153;&#30340;&#39640;&#33021;&#29289;&#29702;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;"&#36974;&#34109;&#31890;&#23376;&#24314;&#27169;"&#65288;MPM&#65289;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#33021;&#29289;&#29702;&#65288;HEP&#65289;&#31185;&#23398;&#25968;&#25454;&#20013;&#26080;&#24207;&#36755;&#20837;&#30340;&#36890;&#29992;&#12289;&#21487;&#36716;&#31227;&#21644;&#21487;&#37325;&#29992;&#34920;&#31034;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#22522;&#20110;&#36974;&#34109;&#24314;&#27169;&#30340;&#39044;&#35757;&#32451;&#26469;&#23398;&#20064;&#38598;&#21512;&#19978;&#30340;&#32622;&#25442;&#19981;&#21464;&#20989;&#25968;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#36825;&#39033;&#24037;&#20316;&#22312;&#26500;&#24314;&#21487;&#20197;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#36890;&#29992;&#39044;&#35757;&#32451;&#24182;&#31245;&#21518;&#31934;&#35843;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;HEP&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#22312;MPM&#20013;&#65292;&#38598;&#21512;&#20013;&#30340;&#31890;&#23376;&#34987;&#36974;&#34109;&#65292;&#35757;&#32451;&#30340;&#30446;&#26631;&#26159;&#24674;&#22797;&#23427;&#20204;&#30340;&#36523;&#20221;&#65292;&#36523;&#20221;&#30001;&#39044;&#35757;&#32451;&#30340;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#31163;&#25955;&#21270;&#26631;&#35760;&#34920;&#31034;&#23450;&#20041;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#23545;&#25758;&#26426;&#29289;&#29702;&#23454;&#39564;&#20013;&#39640;&#33021;&#21943;&#27880;&#26679;&#26412;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#31163;&#25955;&#21270;&#12289;&#32622;&#25442;&#19981;&#21464;&#24615;&#21644;&#25490;&#24207;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose \textit{masked particle modeling} (MPM) as a self-supervised method for learning generic, transferable, and reusable representations on unordered sets of inputs for use in high energy physics (HEP) scientific data. This work provides a novel scheme to perform masked modeling based pre-training to learn permutation invariant functions on sets. More generally, this work provides a step towards building large foundation models for HEP that can be generically pre-trained with self-supervised learning and later fine-tuned for a variety of down-stream tasks. In MPM, particles in a set are masked and the training objective is to recover their identity, as defined by a discretized token representation of a pre-trained vector quantized variational autoencoder. We study the efficacy of the method in samples of high energy jets at collider physics experiments, including studies on the impact of discretization, permutation invariance, and ordering. We also study the fine-tuning capabili
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20013;&#23637;&#31034;&#20102;&#22312;&#39640;&#33021;&#29289;&#29702;&#23398;&#20013;&#65292;&#36890;&#36807;&#36229;&#36234;&#39034;&#24207;&#20248;&#21270;&#25110;&#37325;&#24314;&#21644;&#20998;&#26512;&#32452;&#20214;&#30340;&#26631;&#20934;&#33539; paradigm&#65292;&#21487;&#20197;&#23454;&#29616;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#36890;&#36807;&#25628;&#32034;&#36890;&#36807;&#20013;&#38388; di-Higgs &#31995;&#32479;&#34928;&#21464;&#30340;&#37325;&#20849;&#25391;&#20307;&#20026;&#22235;&#20010; $b$-&#21943;&#27880;&#30340;&#31034;&#20363;&#29992;&#20363;&#65292;&#25105;&#20204;&#23558;&#39640;&#33021;&#29289;&#29702;&#23398;&#37325;&#24314;&#21644;&#20998;&#26512;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#36827;&#34892;&#20102;&#36830;&#25509;&#65292;&#24182;&#37327;&#21270;&#20102;&#20854;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2401.13536</link><description>&lt;p&gt;
&#20026;&#32852;&#21512;&#20998;&#26512;&#20248;&#21270;&#35843;&#25972;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Finetuning Foundation Models for Joint Analysis Optimization. (arXiv:2401.13536v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20013;&#23637;&#31034;&#20102;&#22312;&#39640;&#33021;&#29289;&#29702;&#23398;&#20013;&#65292;&#36890;&#36807;&#36229;&#36234;&#39034;&#24207;&#20248;&#21270;&#25110;&#37325;&#24314;&#21644;&#20998;&#26512;&#32452;&#20214;&#30340;&#26631;&#20934;&#33539; paradigm&#65292;&#21487;&#20197;&#23454;&#29616;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#36890;&#36807;&#25628;&#32034;&#36890;&#36807;&#20013;&#38388; di-Higgs &#31995;&#32479;&#34928;&#21464;&#30340;&#37325;&#20849;&#25391;&#20307;&#20026;&#22235;&#20010; $b$-&#21943;&#27880;&#30340;&#31034;&#20363;&#29992;&#20363;&#65292;&#25105;&#20204;&#23558;&#39640;&#33021;&#29289;&#29702;&#23398;&#37325;&#24314;&#21644;&#20998;&#26512;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#36827;&#34892;&#20102;&#36830;&#25509;&#65292;&#24182;&#37327;&#21270;&#20102;&#20854;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#33021;&#29289;&#29702;&#23398;&#20013;&#21487;&#20197;&#36890;&#36807;&#36229;&#36234;&#39034;&#24207;&#20248;&#21270;&#25110;&#37325;&#24314;&#21644;&#20998;&#26512;&#32452;&#20214;&#30340;&#26631;&#20934;&#33539; paradigm&#65292;&#23454;&#29616;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#25105;&#20204;&#23558;&#39640;&#33021;&#29289;&#29702;&#23398;&#37325;&#24314;&#21644;&#20998;&#26512;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#65288;&#22914;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#12289;&#39046;&#22495;&#36866;&#24212;&#21644;&#39640;&#32500;&#23884;&#20837;&#31354;&#38388;&#65289;&#36827;&#34892;&#20102;&#27010;&#24565;&#19978;&#30340;&#36830;&#25509;&#65292;&#24182;&#37327;&#21270;&#20102;&#36890;&#36807;&#25628;&#32034;&#36890;&#36807;&#20013;&#38388; di-Higgs &#31995;&#32479;&#34928;&#21464;&#30340;&#37325;&#20849;&#25391;&#20307;&#20026;&#22235;&#20010; $b$-&#21943;&#27880;&#30340;&#31034;&#20363;&#29992;&#20363;&#20013;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we demonstrate that significant gains in performance and data efficiency can be achieved in High Energy Physics (HEP) by moving beyond the standard paradigm of sequential optimization or reconstruction and analysis components. We conceptually connect HEP reconstruction and analysis to modern machine learning workflows such as pretraining, finetuning, domain adaptation and high-dimensional embedding spaces and quantify the gains in the example usecase of searches of heavy resonances decaying via an intermediate di-Higgs system to four $b$-jets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Wasserstein&#27010;&#29575;&#31354;&#38388;&#19978;&#30340;Riemannian SGD&#21644;SVRG&#27969;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#20016;&#23500;Wasserstein&#31354;&#38388;&#20013;&#30340;&#36830;&#32493;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13530</link><description>&lt;p&gt;
&#22312;Wasserstein&#27010;&#29575;&#31354;&#38388;&#19978;&#29702;&#35299;Riemannian SGD&#21644;SVRG&#27969;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Riemannian SGD and SVRG Flows on Wasserstein Probabilistic Space. (arXiv:2401.13530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Wasserstein&#27010;&#29575;&#31354;&#38388;&#19978;&#30340;Riemannian SGD&#21644;SVRG&#27969;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#20016;&#23500;Wasserstein&#31354;&#38388;&#20013;&#30340;&#36830;&#32493;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;Riemannian&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#30740;&#31350;&#20026;&#20248;&#21270;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#27010;&#29575;&#27979;&#24230;&#24230;&#37327;&#31354;&#38388;&#20316;&#20026;&#27969;&#24418;&#65292;&#37197;&#22791;&#31532;&#20108;&#38454;Wasserstein&#36317;&#31163;&#65292;&#23588;&#20854;&#24341;&#20154;&#20851;&#27880;&#65292;&#22240;&#20026;&#22312;&#20854;&#19978;&#30340;&#20248;&#21270;&#21487;&#20197;&#19982;&#23454;&#38469;&#30340;&#37319;&#26679;&#36807;&#31243;&#30456;&#20851;&#32852;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;Wasserstein&#31354;&#38388;&#19978;&#30340;&#26368;&#20248;&#21270;&#26041;&#27861;&#26159;Riemannian&#26799;&#24230;&#27969;&#65288;&#21363;&#65292;&#22312;&#26368;&#23567;&#21270;KL&#25955;&#24230;&#26102;&#30340;Langevin&#21160;&#21147;&#23398;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#23558;&#26799;&#24230;&#27969;&#24310;&#23637;&#21040;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#27969;&#21644;&#38543;&#26426;&#26041;&#24046;&#20943;&#23569;&#26799;&#24230;&#65288;SVRG&#65289;&#27969;&#65292;&#20016;&#23500;Wasserstein&#31354;&#38388;&#20013;&#30340;&#36830;&#32493;&#20248;&#21270;&#26041;&#27861;&#12290;Euclidean&#31354;&#38388;&#19978;&#30340;&#36825;&#20004;&#31181;&#27969;&#26159;&#26631;&#20934;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#32780;&#23427;&#20204;&#22312;Riemannian&#31354;&#38388;&#20013;&#30340;&#23545;&#24212;&#26041;&#27861;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#36890;&#36807;&#21033;&#29992;Wasserstein&#31354;&#38388;&#20013;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#26469;&#36817;&#20284;&#31163;&#25955;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, optimization on the Riemannian manifold has provided new insights to the optimization community. In this regard, the manifold taken as the probability measure metric space equipped with the second-order Wasserstein distance is of particular interest, since optimization on it can be linked to practical sampling processes. In general, the oracle (continuous) optimization method on Wasserstein space is Riemannian gradient flow (i.e., Langevin dynamics when minimizing KL divergence). In this paper, we aim to enrich the continuous optimization methods in the Wasserstein space by extending the gradient flow into the stochastic gradient descent (SGD) flow and stochastic variance reduction gradient (SVRG) flow. The two flows on Euclidean space are standard stochastic optimization methods, while their Riemannian counterparts are not explored yet. By leveraging the structures in Wasserstein space, we construct a stochastic differential equation (SDE) to approximate the discrete dynamic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32452;&#32455;&#21644;&#31508;&#26631;&#35760;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#21019;&#26032;&#21518;&#22788;&#29702;&#26041;&#27861;&#26469;&#20998;&#31163;&#32452;&#32455;&#27178;&#25130;&#38754;&#12290;</title><link>http://arxiv.org/abs/2401.13511</link><description>&lt;p&gt;
&#32452;&#32455;&#27178;&#25130;&#38754;&#21644;&#31508;&#26631;&#35760;&#22312;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#30340;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Tissue Cross-Section and Pen Marking Segmentation in Whole Slide Images. (arXiv:2401.13511v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32452;&#32455;&#21644;&#31508;&#26631;&#35760;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#21019;&#26032;&#21518;&#22788;&#29702;&#26041;&#27861;&#26469;&#20998;&#31163;&#32452;&#32455;&#27178;&#25130;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#20998;&#21106;&#26159;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#20998;&#26512;&#30340;&#24120;&#35268;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#36890;&#36807;&#25490;&#38500;&#32972;&#26223;&#21306;&#22495;&#26469;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#20256;&#32479;&#30340;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#36890;&#24120;&#29992;&#20110;&#32452;&#32455;&#20998;&#21106;&#65292;&#20294;&#24120;&#24120;&#38656;&#35201;&#25163;&#21160;&#35843;&#25972;&#21442;&#25968;&#20540;&#26469;&#22788;&#29702;&#38750;&#20856;&#22411;&#24773;&#20917;&#65292;&#26080;&#27861;&#23436;&#20840;&#25490;&#38500;&#24187;&#28783;&#29255;&#21644;&#25195;&#25551;&#24037;&#20214;&#23545;&#32972;&#26223;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#27861;&#20998;&#21106;&#33026;&#32938;&#32452;&#32455;&#12290;&#29305;&#21035;&#26159;&#31508;&#26631;&#35760;&#24037;&#20214;&#22914;&#26524;&#19981;&#31227;&#38500;&#21487;&#33021;&#25104;&#20026;&#21518;&#32493;&#20998;&#26512;&#30340;&#28508;&#22312;&#20559;&#24046;&#28304;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#24212;&#29992;&#38656;&#35201;&#20998;&#31163;&#21333;&#20010;&#27178;&#25130;&#38754;&#65292;&#20294;&#30001;&#20110;&#32452;&#32455;&#30772;&#30862;&#21644;&#30456;&#37051;&#23450;&#20301;&#30340;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tissue segmentation is a routine preprocessing step to reduce the computational cost of whole slide image (WSI) analysis by excluding background regions. Traditional image processing techniques are commonly used for tissue segmentation, but often require manual adjustments to parameter values for atypical cases, fail to exclude all slide and scanning artifacts from the background, and are unable to segment adipose tissue. Pen marking artifacts in particular can be a potential source of bias for subsequent analyses if not removed. In addition, several applications require the separation of individual cross-sections, which can be challenging due to tissue fragmentation and adjacent positioning. To address these problems, we develop a convolutional neural network for tissue and pen marking segmentation using a dataset of 200 H&amp;E stained WSIs. For separating tissue cross-sections, we propose a novel post-processing method based on clustering predicted centroid locations of the cross-sectio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20048;&#22120;&#29305;&#23450;&#36755;&#20837;&#34920;&#31034;&#21644;&#25193;&#25955;&#22806;&#25193;&#25216;&#26415;&#30340;&#34920;&#29616;&#21147;&#20016;&#23500;&#30340;&#22768;&#23398;&#21513;&#20182;&#22768;&#38899;&#21512;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#38899;&#39057;&#36136;&#37327;&#21644;&#26356;&#21152;&#36924;&#30495;&#30340;&#38899;&#33394;&#22768;&#38899;&#12290;</title><link>http://arxiv.org/abs/2401.13498</link><description>&lt;p&gt;
&#37319;&#29992;&#20048;&#22120;&#29305;&#23450;&#36755;&#20837;&#34920;&#31034;&#21644;&#25193;&#25955;&#22806;&#25193;&#25216;&#26415;&#30340;&#34920;&#29616;&#21147;&#20016;&#23500;&#30340;&#22768;&#23398;&#21513;&#20182;&#22768;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific Input Representation and Diffusion Outpainting. (arXiv:2401.13498v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20048;&#22120;&#29305;&#23450;&#36755;&#20837;&#34920;&#31034;&#21644;&#25193;&#25955;&#22806;&#25193;&#25216;&#26415;&#30340;&#34920;&#29616;&#21147;&#20016;&#23500;&#30340;&#22768;&#23398;&#21513;&#20182;&#22768;&#38899;&#21512;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#38899;&#39057;&#36136;&#37327;&#21644;&#26356;&#21152;&#36924;&#30495;&#30340;&#38899;&#33394;&#22768;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22797;&#38899;&#21644;&#34920;&#29616;&#30340;&#39640;&#24230;&#21464;&#24322;&#65292;&#21512;&#25104;&#28436;&#22863;&#21513;&#20182;&#22768;&#38899;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#20174;&#38899;&#20048;&#20048;&#35889;&#20013;&#21512;&#25104;&#34920;&#29616;&#21147;&#20016;&#23500;&#30340;&#22810;&#38899;&#20048;&#22120;&#22768;&#38899;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#36890;&#24120;&#20351;&#29992;&#36890;&#29992;&#30340;MIDI&#36755;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#23450;&#20041;&#20048;&#22120;&#36755;&#20837;&#34920;&#31034;&#30340;&#34920;&#29616;&#21147;&#20016;&#23500;&#30340;&#22768;&#23398;&#21513;&#20182;&#22768;&#38899;&#21512;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;guitarroll&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#22806;&#25193;&#25216;&#26415;&#23454;&#29616;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#38271;&#26399;&#19968;&#33268;&#24615;&#30340;&#38899;&#39057;&#12290;&#20026;&#20102;&#20811;&#26381;&#32570;&#20047;MIDI/&#38899;&#39057;&#37197;&#23545;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#19981;&#20165;&#20351;&#29992;&#20102;&#29616;&#26377;&#30340;&#21513;&#20182;&#25968;&#25454;&#38598;&#65292;&#36824;&#20174;&#39640;&#36136;&#37327;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#21513;&#20182;&#21512;&#25104;&#22120;&#20013;&#25910;&#38598;&#20102;&#25968;&#25454;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#27604;&#22522;&#20934;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#38899;&#39057;&#36136;&#37327;&#65292;&#24182;&#19988;&#27604;&#20808;&#21069;&#30340;&#39046;&#20808;&#24037;&#20316;&#29983;&#25104;&#26356;&#21152;&#36924;&#30495;&#30340;&#38899;&#33394;&#22768;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing performing guitar sound is a highly challenging task due to the polyphony and high variability in expression. Recently, deep generative models have shown promising results in synthesizing expressive polyphonic instrument sounds from music scores, often using a generic MIDI input. In this work, we propose an expressive acoustic guitar sound synthesis model with a customized input representation to the instrument, which we call guitarroll. We implement the proposed approach using diffusion-based outpainting which can generate audio with long-term consistency. To overcome the lack of MIDI/audio-paired datasets, we used not only an existing guitar dataset but also collected data from a high quality sample-based guitar synthesizer. Through quantitative and qualitative evaluations, we show that our proposed model has higher audio quality than the baseline model and generates more realistic timbre sounds than the previous leading work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#21644;&#28145;&#24230;&#33021;&#37327;&#26041;&#27861;&#65288;DEM&#65289;&#30340;&#24377;&#24615;&#38382;&#39064;&#27714;&#35299;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#31934;&#24230;&#19978;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#65292;&#24182;&#19988;&#22312;&#22797;&#26434;&#20960;&#20309;&#20307;&#19978;&#30340;&#32447;&#24615;&#24377;&#24615;&#29702;&#35770;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.13486</link><description>&lt;p&gt;
&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24377;&#24615;&#38382;&#39064;&#30340;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Separable Physics-Informed Neural Networks for the solution of elasticity problems. (arXiv:2401.13486v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#21644;&#28145;&#24230;&#33021;&#37327;&#26041;&#27861;&#65288;DEM&#65289;&#30340;&#24377;&#24615;&#38382;&#39064;&#27714;&#35299;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#31934;&#24230;&#19978;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#65292;&#24182;&#19988;&#22312;&#22797;&#26434;&#20960;&#20309;&#20307;&#19978;&#30340;&#32447;&#24615;&#24377;&#24615;&#29702;&#35770;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#21644;&#28145;&#24230;&#33021;&#37327;&#26041;&#27861;&#65288;DEM&#65289;&#30340;&#24377;&#24615;&#38382;&#39064;&#27714;&#35299;&#26041;&#27861;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#31934;&#24230;&#26174;&#33879;&#39640;&#20110;&#20256;&#32479;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20197;&#21450;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;SPINN&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;SPINN&#22312;DEM&#26041;&#27861;&#26694;&#26550;&#19979;&#65292;&#36824;&#21487;&#20197;&#35299;&#20915;&#22797;&#26434;&#20960;&#20309;&#20307;&#19978;&#30340;&#32447;&#24615;&#24377;&#24615;&#29702;&#35770;&#38382;&#39064;&#65292;&#32780;&#20256;&#32479;&#30340;PINN&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#19979;&#26080;&#27861;&#23454;&#29616;&#12290;&#25152;&#32771;&#34385;&#30340;&#38382;&#39064;&#22312;&#20960;&#20309;&#24418;&#29366;&#12289;&#21152;&#36733;&#21644;&#26448;&#26009;&#21442;&#25968;&#26041;&#38754;&#38750;&#24120;&#25509;&#36817;&#24037;&#19994;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A method for solving elasticity problems based on separable physics-informed neural networks (SPINN) in conjunction with the deep energy method (DEM) is presented. Numerical experiments have been carried out for a number of problems showing that this method has a significantly higher convergence rate and accuracy than the vanilla physics-informed neural networks (PINN) and even SPINN based on a system of partial differential equations (PDEs). In addition, using the SPINN in the framework of DEM approach it is possible to solve problems of the linear theory of elasticity on complex geometries, which is unachievable with the help of PINNs in frames of partial differential equations. Considered problems are very close to the industrial problems in terms of geometry, loading, and material parameters.
&lt;/p&gt;</description></item><item><title>MADRID&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#22330;&#26223;&#26469;&#25581;&#31034;&#39044;&#35757;&#32451;&#22810;Agent&#31574;&#30053;&#30340;&#25112;&#30053;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;&#36951;&#25022;&#20540;&#34913;&#37327;&#28431;&#27934;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.13460</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#24615;&#21551;&#31034;&#30340;&#22810;Agent&#35786;&#26029;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Diagnostics for Robustness via Illuminated Diversity. (arXiv:2401.13460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13460
&lt;/p&gt;
&lt;p&gt;
MADRID&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#22330;&#26223;&#26469;&#25581;&#31034;&#39044;&#35757;&#32451;&#22810;Agent&#31574;&#30053;&#30340;&#25112;&#30053;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;&#36951;&#25022;&#20540;&#34913;&#37327;&#28431;&#27934;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22810;Agent&#31995;&#32479;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#22312;&#38476;&#29983;&#21644;&#25932;&#23545;&#29615;&#22659;&#20013;&#30340;&#31283;&#20581;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36825;&#20123;&#31995;&#32479;&#22312;&#29087;&#24713;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#26032;&#24773;&#20917;&#19979;&#24448;&#24448;&#20250;&#22240;&#20026;&#35757;&#32451;&#38454;&#27573;&#30340;&#36807;&#25311;&#21512;&#32780;&#22833;&#36133;&#12290;&#22312;&#26082;&#21253;&#21547;&#21512;&#20316;&#21448;&#21253;&#21547;&#31454;&#20105;&#34892;&#20026;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#19968;&#38382;&#39064;&#23588;&#20026;&#31361;&#20986;&#65292;&#20307;&#29616;&#20102;&#36807;&#25311;&#21512;&#21644;&#27867;&#21270;&#25361;&#25112;&#30340;&#21452;&#37325;&#24615;&#36136;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22810;&#26679;&#24615;&#21551;&#31034;&#30340;&#22810;Agent&#31283;&#20581;&#24615;&#35786;&#26029;&#65288;MADRID&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#22810;Agent&#31574;&#30053;&#20013;&#26292;&#38706;&#25112;&#30053;&#28431;&#27934;&#30340;&#22810;&#26679;&#21270;&#23545;&#25239;&#22330;&#26223;&#30340;&#26032;&#26041;&#27861;&#12290;MADRID&#21033;&#29992;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#23548;&#33322;&#23545;&#25239;&#29615;&#22659;&#30340;&#24191;&#38420;&#31354;&#38388;&#65292;&#20351;&#29992;&#30446;&#26631;&#31574;&#30053;&#30340;&#36951;&#25022;&#20540;&#26469;&#34913;&#37327;&#36825;&#20123;&#29615;&#22659;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#22312;11vs11&#29256;&#30340;Google Research Football&#19978;&#35780;&#20272;&#20102;MADRID&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing field of multi-agent systems, ensuring robustness in unfamiliar and adversarial settings is crucial. Notwithstanding their outstanding performance in familiar environments, these systems often falter in new situations due to overfitting during the training phase. This is especially pronounced in settings where both cooperative and competitive behaviours are present, encapsulating a dual nature of overfitting and generalisation challenges. To address this issue, we present Multi-Agent Diagnostics for Robustness via Illuminated Diversity (MADRID), a novel approach for generating diverse adversarial scenarios that expose strategic vulnerabilities in pre-trained multi-agent policies. Leveraging the concepts from open-ended learning, MADRID navigates the vast space of adversarial settings, employing a target policy's regret to gauge the vulnerabilities of these settings. We evaluate the effectiveness of MADRID on the 11vs11 version of Google Research Football, one o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#21270;&#22320;&#25214;&#21040;&#22522;&#26412;&#21464;&#25442;&#35268;&#21017;&#21644;&#36880;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#26041;&#31243;&#30340;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.13447</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#27714;&#35299;&#31526;&#21495;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Symbolic Equation Solving via Reinforcement Learning. (arXiv:2401.13447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#21270;&#22320;&#25214;&#21040;&#22522;&#26412;&#21464;&#25442;&#35268;&#21017;&#21644;&#36880;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#26041;&#31243;&#30340;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36880;&#28176;&#22312;&#21508;&#31181;&#31038;&#20250;&#12289;&#32463;&#27982;&#21644;&#31185;&#23398;&#29615;&#22659;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#28982;&#32780;&#22312;&#31934;&#30830;&#25968;&#23398;&#19978;&#23427;&#20204;&#24120;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#35745;&#31639;&#26426;&#20195;&#25968;&#65292;&#21253;&#25324;&#31616;&#21270;&#25968;&#23398;&#26415;&#35821;&#12289;&#35745;&#31639;&#24418;&#24335;&#23548;&#25968;&#25110;&#25214;&#21040;&#20195;&#25968;&#26041;&#31243;&#30340;&#31934;&#30830;&#35299;&#31561;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#36719;&#20214;&#21253;&#36890;&#24120;&#22522;&#20110;&#19968;&#20010;&#24040;&#22823;&#30340;&#35268;&#21017;&#25968;&#25454;&#24211;&#65292;&#36825;&#20123;&#35268;&#21017;&#25551;&#36848;&#20102;&#19968;&#20010;&#29305;&#23450;&#25805;&#20316;&#65288;&#20363;&#22914;&#23548;&#25968;&#65289;&#22914;&#20309;&#23558;&#19968;&#20010;&#26415;&#35821;&#65288;&#20363;&#22914;&#27491;&#24358;&#20989;&#25968;&#65289;&#36716;&#21270;&#20026;&#21478;&#19968;&#20010;&#65288;&#20363;&#22914;&#20313;&#24358;&#20989;&#25968;&#65289;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20123;&#35268;&#21017;&#36890;&#24120;&#38656;&#35201;&#20154;&#31867;&#21457;&#29616;&#24182;&#32534;&#31243;&#12290;&#37325;&#28857;&#35752;&#35770;&#35299;&#20915;&#31526;&#21495;&#24418;&#24335;&#30340;&#32447;&#24615;&#26041;&#31243;&#30340;&#33539;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#21270;&#22320;&#25214;&#21040;&#22522;&#26412;&#21464;&#25442;&#35268;&#21017;&#21644;&#36880;&#27493;&#35299;&#20915;&#26041;&#26696;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learning methods are gradually being adopted in a great variety of social, economic, and scientific contexts, yet they are notorious for struggling with exact mathematics. A typical example is computer algebra, which includes tasks like simplifying mathematical terms, calculating formal derivatives, or finding exact solutions of algebraic equations. Traditional software packages for these purposes are commonly based on a huge database of rules for how a specific operation (e.g., differentiation) transforms a certain term (e.g., sine function) into another one (e.g., cosine function). Thus far, these rules have usually needed to be discovered and subsequently programmed by humans. Focusing on the paradigmatic example of solving linear equations in symbolic form, we demonstrate how the process of finding elementary transformation rules and step-by-step solutions can be automated using reinforcement learning with deep neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21028;&#26029;&#20004;&#20010;&#26631;&#20934;&#27491;&#24577;&#38543;&#26426;&#21521;&#37327;&#26159;&#21542;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#20284;&#28982;&#27604;&#30340;&#20108;&#38454;&#30697;&#65292;&#24182;&#21457;&#29616;&#20102;&#19982;&#25972;&#25968;&#20998;&#21106;&#20989;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.13429</link><description>&lt;p&gt;
&#30456;&#20851;&#38543;&#26426;&#21521;&#37327;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detection of Correlated Random Vectors. (arXiv:2401.13429v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21028;&#26029;&#20004;&#20010;&#26631;&#20934;&#27491;&#24577;&#38543;&#26426;&#21521;&#37327;&#26159;&#21542;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#20284;&#28982;&#27604;&#30340;&#20108;&#38454;&#30697;&#65292;&#24182;&#21457;&#29616;&#20102;&#19982;&#25972;&#25968;&#20998;&#21106;&#20989;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21028;&#26029;&#20004;&#20010;&#26631;&#20934;&#27491;&#24577;&#38543;&#26426;&#21521;&#37327;$\mathsf{X}\in\mathbb{R}^{n}$&#21644;$\mathsf{Y}\in\mathbb{R}^{n}$&#26159;&#21542;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#36825;&#34987;&#34920;&#36848;&#20026;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#22312;&#38646;&#20551;&#35774;&#19979;&#65292;&#36825;&#20123;&#21521;&#37327;&#26159;&#32479;&#35745;&#29420;&#31435;&#30340;&#65292;&#32780;&#22312;&#22791;&#25321;&#20551;&#35774;&#19979;&#65292;$\mathsf{X}$&#21644;&#38543;&#26426;&#22343;&#21248;&#32622;&#25442;&#30340;$\mathsf{Y}$&#26159;&#20855;&#26377;&#30456;&#20851;&#31995;&#25968;$\rho$&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20449;&#24687;&#35770;&#19978;&#19981;&#21487;&#33021;&#21644;&#21487;&#33021;&#30340;&#26368;&#20248;&#27979;&#35797;&#38408;&#20540;&#65292;&#20316;&#20026;$n$&#21644;$\rho$&#30340;&#20989;&#25968;&#12290;&#20026;&#20102;&#24471;&#20986;&#25105;&#20204;&#30340;&#20449;&#24687;&#35770;&#19979;&#30028;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#27491;&#20132;&#22810;&#39033;&#24335;&#23637;&#24320;&#26469;&#35780;&#20272;&#20284;&#28982;&#27604;&#30340;&#20108;&#38454;&#30697;&#30340;&#26032;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#25581;&#31034;&#20102;&#19982;&#25972;&#25968;&#20998;&#21106;&#20989;&#25968;&#20043;&#38388;&#30340;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19978;&#36848;&#35774;&#32622;&#30340;&#22810;&#32500;&#27867;&#21270;&#65292;&#20854;&#20013;&#25105;&#20204;&#35266;&#23519;&#21040;&#20004;&#20010;&#25968;&#25454;&#24211;/&#30697;&#38453;&#65292;&#32780;&#19981;&#26159;&#20004;&#20010;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the problem of deciding whether two standard normal random vectors $\mathsf{X}\in\mathbb{R}^{n}$ and $\mathsf{Y}\in\mathbb{R}^{n}$ are correlated or not. This is formulated as a hypothesis testing problem, where under the null hypothesis, these vectors are statistically independent, while under the alternative, $\mathsf{X}$ and a randomly and uniformly permuted version of $\mathsf{Y}$, are correlated with correlation $\rho$. We analyze the thresholds at which optimal testing is information-theoretically impossible and possible, as a function of $n$ and $\rho$. To derive our information-theoretic lower bounds, we develop a novel technique for evaluating the second moment of the likelihood ratio using an orthogonal polynomials expansion, which among other things, reveals a surprising connection to integer partition functions. We also study a multi-dimensional generalization of the above setting, where rather than two vectors we observe two databases/matrices
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20998;&#24067;&#24335;&#22266;&#23450;&#35774;&#35745;&#37327;&#23376;&#33455;&#29255;&#21644;&#37327;&#23376;&#20449;&#36947;&#30340;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#37327;&#23376;&#24577;&#30340;&#20256;&#36882;&#21644;&#32858;&#21512;&#26799;&#24230;&#26469;&#26356;&#26032;&#21442;&#25968;&#65292;&#25552;&#20379;&#26356;&#39640;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#25351;&#25968;&#32423;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13421</link><description>&lt;p&gt;
&#20855;&#26377;&#20998;&#24067;&#24335;&#22266;&#23450;&#35774;&#35745;&#37327;&#23376;&#33455;&#29255;&#21644;&#37327;&#23376;&#20449;&#36947;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated learning with distributed fixed design quantum chips and quantum channels. (arXiv:2401.13421v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20998;&#24067;&#24335;&#22266;&#23450;&#35774;&#35745;&#37327;&#23376;&#33455;&#29255;&#21644;&#37327;&#23376;&#20449;&#36947;&#30340;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#37327;&#23376;&#24577;&#30340;&#20256;&#36882;&#21644;&#32858;&#21512;&#26799;&#24230;&#26469;&#26356;&#26032;&#21442;&#25968;&#65292;&#25552;&#20379;&#26356;&#39640;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#25351;&#25968;&#32423;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#23458;&#25143;&#31471;&#30340;&#31934;&#24515;&#35774;&#35745;&#26597;&#35810;&#65292;&#32463;&#20856;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#21487;&#20197;&#34987;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#20013;&#30340;&#27979;&#37327;&#20250;&#23548;&#33268;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#37327;&#23376;&#36890;&#20449;&#20449;&#36947;&#34987;&#35748;&#20026;&#26356;&#21152;&#23433;&#20840;&#65292;&#22240;&#20026;&#21487;&#20197;&#26816;&#27979;&#21040;&#36825;&#31181;&#20449;&#24687;&#20002;&#22833;&#12290;&#22240;&#27492;&#65292;&#37327;&#23376;&#29256;&#26412;&#30340;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#37327;&#23376;&#20449;&#36947;&#21457;&#36865;N&#32500;&#25968;&#25454;&#21521;&#37327;&#38656;&#35201;&#21457;&#36865;log N&#20010;&#32416;&#32544;&#24577;&#37327;&#23376;&#27604;&#29305;&#65292;&#22914;&#26524;&#25968;&#25454;&#21521;&#37327;&#20316;&#20026;&#37327;&#23376;&#24577;&#33719;&#21462;&#65292;&#36825;&#21487;&#20197;&#25552;&#20379;&#25351;&#25968;&#32423;&#30340;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#22522;&#20110;&#30001;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#21457;&#36865;&#30340;&#37327;&#23376;&#24577;&#65292;&#25805;&#20316;&#22266;&#23450;&#35774;&#35745;&#30340;&#37327;&#23376;&#33455;&#29255;&#12290;&#22522;&#20110;&#25509;&#25910;&#21040;&#30340;&#21472;&#21152;&#24577;&#65292;&#23458;&#25143;&#31471;&#35745;&#31639;&#24182;&#23558;&#20854;&#26412;&#22320;&#26799;&#24230;&#20316;&#20026;&#37327;&#23376;&#24577;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#26381;&#21153;&#22120;&#23558;&#36825;&#20123;&#26799;&#24230;&#32858;&#21512;&#20197;&#26356;&#26032;&#21442;&#25968;&#12290;&#30001;&#20110;&#26381;&#21153;&#22120;&#19981;&#21457;&#36865;&#27169;&#22411;&#20449;&#24687;&#65292;
&lt;/p&gt;
&lt;p&gt;
The privacy in classical federated learning can be breached through the use of local gradient results by using engineered queries from the clients. However, quantum communication channels are considered more secure because the use of measurements in the data causes some loss of information, which can be detected. Therefore, the quantum version of federated learning can be used to provide more privacy. Additionally, sending an $N$ dimensional data vector through a quantum channel requires sending $\log N$ entangled qubits, which can provide exponential efficiency if the data vector is obtained as quantum states.  In this paper, we propose a quantum federated learning model where fixed design quantum chips are operated based on the quantum states sent by a centralized server. Based on the coming superposition states, the clients compute and then send their local gradients as quantum states to the server, where they are aggregated to update parameters. Since the server does not send model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32852;&#37030;&#22312;&#32447;&#23398;&#20064;&#25490;&#24207;&#20013;&#21024;&#38500;&#23458;&#25143;&#30340;&#36129;&#29486;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13410</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#32852;&#37030;&#22312;&#32447;&#23398;&#20064;&#25490;&#24207;&#20013;&#24536;&#35760;&#23458;&#25143;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Forget Clients in Federated Online Learning to Rank?. (arXiv:2401.13410v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32852;&#37030;&#22312;&#32447;&#23398;&#20064;&#25490;&#24207;&#20013;&#21024;&#38500;&#23458;&#25143;&#30340;&#36129;&#29486;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#22914;&#27431;&#30431;&#30340;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#65288;GDPR&#65289;&#24314;&#31435;&#20102;&#8220;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#8221;&#65306;&#29992;&#25143;&#65288;&#23458;&#25143;&#65289;&#21487;&#20197;&#35201;&#27714;&#23558;&#20351;&#29992;&#20182;&#20204;&#30340;&#25968;&#25454;&#36827;&#34892;&#30340;&#36129;&#29486;&#20174;&#23398;&#20064;&#30340;&#27169;&#22411;&#20013;&#21024;&#38500;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21024;&#38500;&#21442;&#19982;&#32852;&#37030;&#22312;&#32447;&#23398;&#20064;&#25490;&#24207;&#65288;FOLTR&#65289;&#31995;&#32479;&#20013;&#23458;&#25143;&#25152;&#20570;&#30340;&#36129;&#29486;&#12290;&#22312;FOLTR&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#32858;&#21512;&#23616;&#37096;&#26356;&#26032;&#21040;&#20840;&#23616;&#25490;&#24207;&#27169;&#22411;&#26469;&#23398;&#20064;&#25490;&#24207;&#22120;&#12290;&#23616;&#37096;&#26356;&#26032;&#26159;&#20197;&#22312;&#32447;&#26041;&#24335;&#22312;&#23458;&#25143;&#32423;&#21035;&#19978;&#20351;&#29992;&#26597;&#35810;&#21644;&#38544;&#24335;&#20132;&#20114;&#26469;&#23398;&#20064;&#30340;&#65292;&#36825;&#20123;&#26597;&#35810;&#21644;&#20132;&#20114;&#21457;&#29983;&#22312;&#29305;&#23450;&#23458;&#25143;&#20869;&#37096;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#27599;&#20010;&#23458;&#25143;&#30340;&#26412;&#22320;&#25968;&#25454;&#19981;&#20250;&#19982;&#20854;&#20182;&#23458;&#25143;&#25110;&#38598;&#20013;&#24335;&#25628;&#32034;&#26381;&#21153;&#20849;&#20139;&#65292;&#21516;&#26102;&#23458;&#25143;&#21487;&#20197;&#20174;&#32852;&#37030;&#20013;&#30340;&#27599;&#20010;&#23458;&#25143;&#30340;&#36129;&#29486;&#20013;&#21463;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#21024;&#38500;&#23458;&#25143;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data protection legislation like the European Union's General Data Protection Regulation (GDPR) establishes the \textit{right to be forgotten}: a user (client) can request contributions made using their data to be removed from learned models. In this paper, we study how to remove the contributions made by a client participating in a Federated Online Learning to Rank (FOLTR) system. In a FOLTR system, a ranker is learned by aggregating local updates to the global ranking model. Local updates are learned in an online manner at a client-level using queries and implicit interactions that have occurred within that specific client. By doing so, each client's local data is not shared with other clients or with a centralised search service, while at the same time clients can benefit from an effective global ranking model learned from contributions of each client in the federation.  In this paper, we study an effective and efficient unlearning method that can remove a client's contribution with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#20998;&#31867;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31616;&#21270;&#20572;&#29992;&#35789;&#25552;&#21462;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#28151;&#21512;&#32479;&#35745;&#21644;&#35821;&#35328;&#23398;&#26041;&#27861;&#21019;&#24314;&#20840;&#38754;&#30340;&#20572;&#29992;&#35789;&#21015;&#34920;&#65292;&#25552;&#39640;&#20102;&#38750;&#27954;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2401.13398</link><description>&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#21487;&#20197;&#22686;&#24378;&#39046;&#22495;&#26080;&#20851;&#30340;&#20572;&#29992;&#35789;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Text Categorization Can Enhance Domain-Agnostic Stopword Extraction. (arXiv:2401.13398v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#20998;&#31867;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31616;&#21270;&#20572;&#29992;&#35789;&#25552;&#21462;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#28151;&#21512;&#32479;&#35745;&#21644;&#35821;&#35328;&#23398;&#26041;&#27861;&#21019;&#24314;&#20840;&#38754;&#30340;&#20572;&#29992;&#35789;&#21015;&#34920;&#65292;&#25552;&#39640;&#20102;&#38750;&#27954;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#20998;&#31867;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31616;&#21270;&#20572;&#29992;&#35789;&#25552;&#21462;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#21253;&#25324;&#27861;&#35821;&#22312;&#20869;&#30340;9&#31181;&#38750;&#27954;&#35821;&#35328;&#12290;&#36890;&#36807;&#21033;&#29992;MasakhaNEWS&#12289;African Stopwords Project&#21644;MasakhaPOS&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#25991;&#26412;&#20998;&#31867;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#22823;&#22810;&#25968;&#32771;&#23519;&#35821;&#35328;&#20013;&#36229;&#36807;80&#65285;&#30340;&#39046;&#22495;&#26080;&#20851;&#30340;&#20572;&#29992;&#35789;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#21464;&#24322;&#23548;&#33268;&#26576;&#20123;&#35821;&#35328;&#30340;&#35782;&#21035;&#29575;&#36739;&#20302;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#34429;&#28982;&#36229;&#36807;40&#65285;&#30340;&#20572;&#29992;&#35789;&#22312;&#21508;&#31867;&#26032;&#38395;&#20013;&#37117;&#26159;&#20849;&#21516;&#30340;&#65292;&#20294;&#23569;&#20110;15&#65285;&#30340;&#20572;&#29992;&#35789;&#26159;&#26576;&#19968;&#31867;&#21035;&#29420;&#26377;&#30340;&#12290;&#19981;&#24120;&#35265;&#30340;&#20572;&#29992;&#35789;&#22686;&#21152;&#20102;&#25991;&#26412;&#30340;&#28145;&#24230;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#34987;&#20998;&#31867;&#20026;&#20572;&#29992;&#35789;&#21017;&#21462;&#20915;&#20110;&#19978;&#19979;&#25991;&#12290;&#22240;&#27492;&#65292;&#32467;&#21512;&#32479;&#35745;&#21644;&#35821;&#35328;&#23398;&#26041;&#27861;&#21487;&#20197;&#21019;&#24314;&#20840;&#38754;&#30340;&#20572;&#29992;&#35789;&#21015;&#34920;&#65292;&#20984;&#26174;&#20102;&#25105;&#20204;&#28151;&#21512;&#26041;&#27861;&#30340;&#20215;&#20540;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#39640;&#20102;&#38750;&#27954;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27700;&#24179;&#65292;&#24182;&#24378;&#35843;&#20102;&#25991;&#26412;&#20998;&#31867;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the role of text categorization in streamlining stopword extraction in natural language processing (NLP), specifically focusing on nine African languages alongside French. By leveraging the MasakhaNEWS, African Stopwords Project, and MasakhaPOS datasets, our findings emphasize that text categorization effectively identifies domain-agnostic stopwords with over 80% detection success rate for most examined languages. Nevertheless, linguistic variances result in lower detection rates for certain languages. Interestingly, we find that while over 40% of stopwords are common across news categories, less than 15% are unique to a single category. Uncommon stopwords add depth to text but their classification as stopwords depends on context. Therefore combining statistical and linguistic approaches creates comprehensive stopword lists, highlighting the value of our hybrid method. This research enhances NLP for African languages and underscores the importance of text catego
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#35780;&#20272;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#27969;&#34892;&#24230;&#25351;&#26631;&#25552;&#20986;&#36136;&#30097;&#65292;&#35748;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#32676;&#32452;&#20869;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#23548;&#33268;&#30340;&#39044;&#27979;&#26631;&#31614;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#29616;&#23454;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.13391</link><description>&lt;p&gt;
&#36229;&#36234;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65306;&#20572;&#27490;&#20165;&#26681;&#25454;&#32676;&#32452;&#38388;&#25351;&#26631;&#35780;&#20272;&#20559;&#35265;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond Accuracy-Fairness: Stop evaluating bias mitigation methods solely on between-group metrics. (arXiv:2401.13391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#35780;&#20272;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#27969;&#34892;&#24230;&#25351;&#26631;&#25552;&#20986;&#36136;&#30097;&#65292;&#35748;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#32676;&#32452;&#20869;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#23548;&#33268;&#30340;&#39044;&#27979;&#26631;&#31614;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#29616;&#23454;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#24341;&#21457;&#20102;&#23545;&#20854;&#20844;&#24179;&#24615;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#20844;&#24179;&#24615;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#26680;&#24515;&#20851;&#27880;&#28857;&#65292;&#20294;&#30446;&#21069;&#30340;&#35752;&#35770;&#24448;&#24448;&#24378;&#35843;&#22522;&#20110;&#32467;&#26524;&#30340;&#25351;&#26631;&#65292;&#32780;&#27809;&#26377;&#23545;&#23376;&#32676;&#20307;&#20013;&#30340;&#24046;&#24322;&#24433;&#21709;&#36827;&#34892;&#32454;&#33268;&#32771;&#34385;&#12290;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#19981;&#20165;&#24433;&#21709;&#25935;&#24863;&#32676;&#32452;&#20043;&#38388;&#30340;&#23454;&#20363;&#25490;&#24207;&#65292;&#32780;&#19988;&#36890;&#24120;&#36824;&#26174;&#33879;&#24433;&#21709;&#36825;&#20123;&#32676;&#32452;&#20869;&#23454;&#20363;&#30340;&#25490;&#24207;&#12290;&#36825;&#20123;&#21464;&#21270;&#38590;&#20197;&#35299;&#37322;&#65292;&#24182;&#24341;&#36215;&#20102;&#23545;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#30340;&#25285;&#24551;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#25928;&#24212;&#22312;&#36890;&#24120;&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;-&#20844;&#24179;&#24615;&#35780;&#20272;&#26694;&#26550;&#20013;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#23545;&#35780;&#20272;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#27969;&#34892;&#24230;&#25351;&#26631;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#35748;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#32676;&#32452;&#20869;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#23548;&#33268;&#30340;&#39044;&#27979;&#26631;&#31614;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#29616;&#23454;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) finds widespread applications across various domains, sparking concerns about fairness in its deployment. While fairness in AI remains a central concern, the prevailing discourse often emphasizes outcome-based metrics without a nuanced consideration of the differential impacts within subgroups. Bias mitigation techniques do not only affect the ranking of pairs of instances across sensitive groups, but often also significantly affect the ranking of instances within these groups. Such changes are hard to explain and raise concerns regarding the validity of the intervention. Unfortunately, these effects largely remain under the radar in the accuracy-fairness evaluation framework that is usually applied. This paper challenges the prevailing metrics for assessing bias mitigation techniques, arguing that they do not take into account the changes within-groups and that the resulting prediction labels fall short of reflecting real-world scenarios. We propose a para
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#20943;&#36731;&#31995;&#32479;&#20559;&#24046;&#30340;&#21160;&#24577;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#23458;&#25143;&#31471;&#30340;&#19978;&#20256;&#39057;&#29575;&#35780;&#20998;&#21644;&#35843;&#25972;&#27169;&#22411;&#26356;&#26032;&#30340;&#26435;&#37325;&#65292;&#20197;&#36866;&#24212;&#24322;&#26500;&#35774;&#22791;&#21644;&#38750;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#20840;&#23616;&#27169;&#22411;&#20934;&#30830;&#24615;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2401.13366</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#20943;&#36731;&#31995;&#32479;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Mitigating System Bias in Resource Constrained Asynchronous Federated Learning Systems. (arXiv:2401.13366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13366
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#20943;&#36731;&#31995;&#32479;&#20559;&#24046;&#30340;&#21160;&#24577;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#23458;&#25143;&#31471;&#30340;&#19978;&#20256;&#39057;&#29575;&#35780;&#20998;&#21644;&#35843;&#25972;&#27169;&#22411;&#26356;&#26032;&#30340;&#26435;&#37325;&#65292;&#20197;&#36866;&#24212;&#24322;&#26500;&#35774;&#22791;&#21644;&#38750;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#20840;&#23616;&#27169;&#22411;&#20934;&#30830;&#24615;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#22312;&#22788;&#29702;&#24322;&#26500;&#35774;&#22791;&#21644;&#23458;&#25143;&#31471;&#38750;&#21516;&#20998;&#24067;&#25968;&#25454;&#26102;&#38754;&#20020;&#24615;&#33021;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32858;&#21512;&#26041;&#27861;&#26681;&#25454;&#23458;&#25143;&#31471;&#30340;&#19978;&#20256;&#39057;&#29575;&#23545;&#20854;&#27169;&#22411;&#26356;&#26032;&#36827;&#34892;&#35780;&#20998;&#21644;&#35843;&#25972;&#26435;&#37325;&#65292;&#20197;&#36866;&#24212;&#35774;&#22791;&#33021;&#21147;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#23458;&#25143;&#31471;&#19978;&#20256;&#26412;&#22320;&#27169;&#22411;&#21518;&#31435;&#21363;&#21521;&#20854;&#25552;&#20379;&#26356;&#26032;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#20197;&#20943;&#23569;&#31354;&#38386;&#26102;&#38388;&#24182;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;10&#20010;&#27169;&#25311;&#23458;&#25143;&#31471;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#37096;&#32626;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#23458;&#25143;&#31471;&#20855;&#26377;&#24322;&#26500;&#35745;&#31639;&#32422;&#26463;&#21644;&#38750;IID&#25968;&#25454;&#12290;&#20351;&#29992;FashionMNIST&#25968;&#25454;&#38598;&#30340;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;PAPAYA&#21644;FedAsync&#30456;&#27604;&#65292;&#20840;&#23616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25913;&#21892;&#20102;10%&#21644;19%&#12290;&#25105;&#20204;&#30340;&#21160;&#24577;&#32858;&#21512;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#21487;&#38752;&#30340;&#20840;&#23616;&#27169;&#22411;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) systems face performance challenges in dealing with heterogeneous devices and non-identically distributed data across clients. We propose a dynamic global model aggregation method within Asynchronous Federated Learning (AFL) deployments to address these issues. Our aggregation method scores and adjusts the weighting of client model updates based on their upload frequency to accommodate differences in device capabilities. Additionally, we also immediately provide an updated global model to clients after they upload their local models to reduce idle time and improve training efficiency. We evaluate our approach within an AFL deployment consisting of 10 simulated clients with heterogeneous compute constraints and non-IID data. The simulation results, using the FashionMNIST dataset, demonstrate over 10% and 19% improvement in global model accuracy compared to state-of-the-art methods PAPAYA and FedAsync, respectively. Our dynamic aggregation method allows reliable g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#22122;&#22768;&#19987;&#23478;&#27169;&#22411;&#65288;ITEM&#65289;&#26469;&#35299;&#20915;&#26679;&#26412;&#36873;&#25321;&#20013;&#30340;&#35757;&#32451;&#20559;&#24046;&#21644;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#40065;&#26834;&#30340;&#32593;&#32476;&#26550;&#26500;&#26469;&#38598;&#25104;&#22810;&#20010;&#19987;&#23478;&#65292;&#21487;&#20197;&#20943;&#23569;&#36873;&#25321;&#38598;&#19981;&#24179;&#34913;&#21644;&#32047;&#31215;&#38169;&#35823;&#65292;&#24182;&#22312;&#20351;&#29992;&#26356;&#23569;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#36873;&#25321;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13360</link><description>&lt;p&gt;
&#23545;&#25239;&#22122;&#22768;&#26631;&#31614;&#30340;&#26080;&#20559;&#26679;&#26412;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Debiased Sample Selection for Combating Noisy Labels. (arXiv:2401.13360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#22122;&#22768;&#19987;&#23478;&#27169;&#22411;&#65288;ITEM&#65289;&#26469;&#35299;&#20915;&#26679;&#26412;&#36873;&#25321;&#20013;&#30340;&#35757;&#32451;&#20559;&#24046;&#21644;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#40065;&#26834;&#30340;&#32593;&#32476;&#26550;&#26500;&#26469;&#38598;&#25104;&#22810;&#20010;&#19987;&#23478;&#65292;&#21487;&#20197;&#20943;&#23569;&#36873;&#25321;&#38598;&#19981;&#24179;&#34913;&#21644;&#32047;&#31215;&#38169;&#35823;&#65292;&#24182;&#22312;&#20351;&#29992;&#26356;&#23569;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#36873;&#25321;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#22122;&#22768;&#26631;&#31614;&#26088;&#22312;&#30830;&#20445;&#27169;&#22411;&#22312;&#26631;&#31614;&#38169;&#35823;&#30340;&#35757;&#32451;&#38598;&#19978;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#36890;&#36807;&#36873;&#25321;&#21487;&#38752;&#30340;&#26631;&#31614;&#23376;&#38598;&#26469;&#23454;&#29616;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#23454;&#35777;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#23384;&#22312;&#25968;&#25454;&#21644;&#35757;&#32451;&#20559;&#24046;&#65292;&#20998;&#21035;&#34920;&#31034;&#20026;&#36873;&#25321;&#38598;&#19981;&#24179;&#34913;&#21644;&#32047;&#31215;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21482;&#22788;&#29702;&#20102;&#35757;&#32451;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#26679;&#26412;&#36873;&#25321;&#30340;&#26080;&#22122;&#22768;&#19987;&#23478;&#27169;&#22411;&#65288;ITEM&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#20943;&#36731;&#35757;&#32451;&#20559;&#24046;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#19982;&#22810;&#20010;&#19987;&#23478;&#38598;&#25104;&#12290;&#19982;&#30446;&#21069;&#30340;&#21452;&#20998;&#25903;&#32593;&#32476;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#35757;&#32451;&#26356;&#23569;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#38598;&#25104;&#36825;&#20123;&#19987;&#23478;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#36873;&#25321;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#20943;&#36731;&#25968;&#25454;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#37319;&#26679;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with noisy labels aims to ensure model generalization given a label-corrupted training set. The sample selection strategy achieves promising performance by selecting a label-reliable subset for model training. In this paper, we empirically reveal that existing sample selection methods suffer from both data and training bias that are represented as imbalanced selected sets and accumulation errors in practice, respectively. However, only the training bias was handled in previous studies. To address this limitation, we propose a noIse-Tolerant Expert Model (ITEM) for debiased learning in sample selection. Specifically, to mitigate the training bias, we design a robust network architecture that integrates with multiple experts. Compared with the prevailing double-branch network, our network exhibits better performance of selection and prediction by ensembling these experts while training with fewer parameters. Meanwhile, to mitigate the data bias, we propose a mixed sampling strat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#31526;&#21495;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#32463;&#39564;&#25945;&#35757;&#65292;&#21253;&#25324;&#22312;&#26426;&#22120;&#23398;&#20064;&#20043;&#21069;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#22312;&#26609;&#38754;&#20195;&#25968;&#20998;&#35299;&#20013;&#30340;&#21464;&#37327;&#25490;&#24207;&#36873;&#25321;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#22686;&#24378;&#25216;&#26415;&#26469;&#25913;&#21892;&#25968;&#25454;&#38598;&#30340;&#24179;&#34913;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13343</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#31526;&#21495;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#38598;&#21644;&#33539;&#24335;&#30340;&#25945;&#35757;&#65306;&#22522;&#20110;CAD&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Lessons on Datasets and Paradigms in Machine Learning for Symbolic Computation: A Case Study on CAD. (arXiv:2401.13343v1 [cs.SC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13343
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#31526;&#21495;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#32463;&#39564;&#25945;&#35757;&#65292;&#21253;&#25324;&#22312;&#26426;&#22120;&#23398;&#20064;&#20043;&#21069;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#22312;&#26609;&#38754;&#20195;&#25968;&#20998;&#35299;&#20013;&#30340;&#21464;&#37327;&#25490;&#24207;&#36873;&#25321;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#22686;&#24378;&#25216;&#26415;&#26469;&#25913;&#21892;&#25968;&#25454;&#38598;&#30340;&#24179;&#34913;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#35745;&#31639;&#31639;&#27861;&#21450;&#20854;&#22312;&#35745;&#31639;&#26426;&#20195;&#25968;&#31995;&#32479;&#20013;&#30340;&#23454;&#29616;&#36890;&#24120;&#21253;&#21547;&#19968;&#20123;&#36873;&#25321;&#65292;&#36825;&#20123;&#36873;&#25321;&#19981;&#20250;&#24433;&#21709;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#65292;&#20294;&#23545;&#25152;&#38656;&#36164;&#28304;&#26377;&#26174;&#33879;&#24433;&#21709;&#65306;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#38024;&#23545;&#27599;&#20010;&#38382;&#39064;&#21333;&#29420;&#20570;&#20986;&#36825;&#20123;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#25253;&#21578;&#20102;&#22312;&#31526;&#21495;&#35745;&#31639;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#32463;&#39564;&#25945;&#35757;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#20043;&#21069;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#21487;&#33021;&#20351;&#29992;&#30340;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12290;&#19979;&#38754;&#20197;&#26609;&#38754;&#20195;&#25968;&#20998;&#35299;&#20013;&#30340;&#21464;&#37327;&#25490;&#24207;&#36873;&#25321;&#20316;&#20026;&#19968;&#20010;&#20855;&#20307;&#26696;&#20363;&#30740;&#31350;&#30340;&#32467;&#26524;&#26469;&#23637;&#31034;&#65292;&#20294;&#35748;&#20026;&#25152;&#23398;&#21040;&#30340;&#25945;&#35757;&#36866;&#29992;&#20110;&#31526;&#21495;&#35745;&#31639;&#20013;&#30340;&#20854;&#20182;&#20915;&#31574;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#29616;&#26377;&#30340;&#20174;&#24212;&#29992;&#20013;&#23548;&#20986;&#30340;&#31034;&#20363;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#35813;&#25968;&#25454;&#38598;&#22312;&#21464;&#37327;&#25490;&#24207;&#20915;&#31574;&#26041;&#38754;&#23384;&#22312;&#19981;&#24179;&#34913;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#31995;&#32479;&#38382;&#39064;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#20351;&#24471;&#27599;&#20010;&#38382;&#39064;&#21487;&#20197;&#26377;&#22810;&#20010;&#31034;&#20363;&#20197;&#22686;&#24378;&#25968;&#25454;&#38598;&#30340;&#24179;&#34913;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic Computation algorithms and their implementation in computer algebra systems often contain choices which do not affect the correctness of the output but can significantly impact the resources required: such choices can benefit from having them made separately for each problem via a machine learning model. This study reports lessons on such use of machine learning in symbolic computation, in particular on the importance of analysing datasets prior to machine learning and on the different machine learning paradigms that may be utilised. We present results for a particular case study, the selection of variable ordering for cylindrical algebraic decomposition, but expect that the lessons learned are applicable to other decisions in symbolic computation.  We utilise an existing dataset of examples derived from applications which was found to be imbalanced with respect to the variable ordering decision. We introduce an augmentation technique for polynomial systems problems that allow
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26174;&#33879;&#24615;&#26816;&#39564;&#26041;&#27861;&#65288;nFBST&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25311;&#21512;&#38750;&#32447;&#24615;&#21644;&#22810;&#32500;&#20851;&#31995;&#65292;&#24182;&#35745;&#31639;&#35777;&#25454;&#20540;&#26469;&#26367;&#20195;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#29702;&#35770;&#25512;&#23548;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#27979;&#35797;&#20840;&#23616;&#12289;&#23616;&#37096;&#21644;&#23454;&#20363;&#32423;&#30340;&#26174;&#33879;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13335</link><description>&lt;p&gt;
&#20840;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26174;&#33879;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Full Bayesian Significance Testing for Neural Networks. (arXiv:2401.13335v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13335
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26174;&#33879;&#24615;&#26816;&#39564;&#26041;&#27861;&#65288;nFBST&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25311;&#21512;&#38750;&#32447;&#24615;&#21644;&#22810;&#32500;&#20851;&#31995;&#65292;&#24182;&#35745;&#31639;&#35777;&#25454;&#20540;&#26469;&#26367;&#20195;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#29702;&#35770;&#25512;&#23548;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#27979;&#35797;&#20840;&#23616;&#12289;&#23616;&#37096;&#21644;&#23454;&#20363;&#32423;&#30340;&#26174;&#33879;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#33879;&#24615;&#26816;&#39564;&#26088;&#22312;&#30830;&#23450;&#32473;&#23450;&#35266;&#27979;&#32467;&#26524;&#65292;&#20851;&#20110;&#24635;&#20307;&#20998;&#24067;&#30340;&#21629;&#39064;&#26159;&#21542;&#20026;&#30495;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26174;&#33879;&#24615;&#26816;&#39564;&#36890;&#24120;&#38656;&#35201;&#25512;&#23548;&#20986;&#26816;&#39564;&#32479;&#35745;&#37327;&#30340;&#20998;&#24067;&#65292;&#26080;&#27861;&#22788;&#29702;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#36125;&#21494;&#26031;&#26174;&#33879;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#31216;&#20026;nFBST&#65292;&#26088;&#22312;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#22312;&#20851;&#31995;&#34920;&#24449;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#21033;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25311;&#21512;&#38750;&#32447;&#24615;&#21644;&#22810;&#32500;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#35777;&#25454;&#20540;&#32780;&#19981;&#26159;&#36827;&#34892;&#32321;&#29712;&#30340;&#29702;&#35770;&#25512;&#23548;&#26469;&#36991;&#20813;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;nFBST&#36824;&#21487;&#20197;&#27979;&#35797;&#20840;&#23616;&#12289;&#23616;&#37096;&#21644;&#23454;&#20363;&#32423;&#30340;&#26174;&#33879;&#24615;&#65292;&#36825;&#26159;&#20043;&#21069;&#30340;&#26816;&#39564;&#26041;&#27861;&#25152;&#19981;&#20851;&#27880;&#30340;&#12290;&#27492;&#22806;&#65292;nFBST&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#25152;&#36873;&#30340;&#24230;&#37327;&#36827;&#34892;&#25193;&#23637;&#65292;&#22914;Grad-nFBST&#65292;LRP-nFBST&#65292;DeepLIFT-nFBST&#12290;
&lt;/p&gt;
&lt;p&gt;
Significance testing aims to determine whether a proposition about the population distribution is the truth or not given observations. However, traditional significance testing often needs to derive the distribution of the testing statistic, failing to deal with complex nonlinear relationships. In this paper, we propose to conduct Full Bayesian Significance Testing for neural networks, called \textit{n}FBST, to overcome the limitation in relationship characterization of traditional approaches. A Bayesian neural network is utilized to fit the nonlinear and multi-dimensional relationships with small errors and avoid hard theoretical derivation by computing the evidence value. Besides, \textit{n}FBST can test not only global significance but also local and instance-wise significance, which previous testing methods don't focus on. Moreover, \textit{n}FBST is a general framework that can be extended based on the measures selected, such as Grad-\textit{n}FBST, LRP-\textit{n}FBST, DeepLIFT-\t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;TNTRules&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#38388;&#38553;&#12290;</title><link>http://arxiv.org/abs/2401.13334</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Explainable Bayesian Optimization. (arXiv:2401.13334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;TNTRules&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#38388;&#38553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#39046;&#22495;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21442;&#25968;&#35843;&#20248;&#30340;&#25511;&#21046;&#31995;&#32479;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36817;&#20284;&#35823;&#24046;&#21644;&#31616;&#21270;&#30446;&#26631;&#65292;BO&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20559;&#31163;&#20154;&#31867;&#19987;&#23478;&#30340;&#30495;&#23454;&#30446;&#26631;&#65292;&#38656;&#35201;&#21518;&#32493;&#35843;&#25972;&#12290;BO&#30340;&#40657;&#30418;&#29305;&#24615;&#38480;&#21046;&#20102;&#21327;&#20316;&#35843;&#20248;&#36807;&#31243;&#65292;&#22240;&#20026;&#19987;&#23478;&#19981;&#20449;&#20219;BO&#30340;&#24314;&#35758;&#12290;&#30446;&#21069;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#27492;&#26080;&#27861;&#35299;&#20915;&#27492;&#38388;&#38553;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#38388;&#38553;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TNTRules&#65288;TUNE-NOTUNE&#35268;&#21017;&#65289;&#65292;&#19968;&#31181;&#20107;&#21518;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#20248;&#21270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#23545;&#22522;&#20934;&#20248;&#21270;&#38382;&#39064;&#21644;&#23454;&#38469;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;TNTRules&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#35299;&#37322;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;XAI&#26041;&#27861;&#12290;&#36825;&#39033;&#24037;&#20316;&#23545;BO&#21644;XAI&#30340;&#20132;&#21449;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry, Bayesian optimization (BO) is widely applied in the human-AI collaborative parameter tuning of cyber-physical systems. However, BO's solutions may deviate from human experts' actual goal due to approximation errors and simplified objectives, requiring subsequent tuning. The black-box nature of BO limits the collaborative tuning process because the expert does not trust the BO recommendations. Current explainable AI (XAI) methods are not tailored for optimization and thus fall short of addressing this gap. To bridge this gap, we propose TNTRules (TUNE-NOTUNE Rules), a post-hoc, rule-based explainability method that produces high quality explanations through multiobjective optimization. Our evaluation of benchmark optimization problems and real-world hyperparameter optimization tasks demonstrates TNTRules' superiority over state-of-the-art XAI methods in generating high quality explanations. This work contributes to the intersection of BO and XAI, providing interpretable opt
&lt;/p&gt;</description></item><item><title>NACHOS &#26159;&#19968;&#31181;&#38754;&#21521;&#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#35774;&#35745;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#24182;&#32771;&#34385;&#39592;&#24178;&#21644;&#26089;&#26399;&#36864;&#20986;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.13330</link><description>&lt;p&gt;
NACHOS: &#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks. (arXiv:2401.13330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13330
&lt;/p&gt;
&lt;p&gt;
NACHOS &#26159;&#19968;&#31181;&#38754;&#21521;&#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#35774;&#35745;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#24182;&#32771;&#34385;&#39592;&#24178;&#21644;&#26089;&#26399;&#36864;&#20986;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#65288;EENNs&#65289;&#20026;&#26631;&#20934;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#37197;&#22791;&#26089;&#26399;&#36864;&#20986;&#20998;&#31867;&#22120;&#65288;EECs&#65289;&#65292;&#22312;&#22788;&#29702;&#30340;&#20013;&#38388;&#28857;&#19978;&#25552;&#20379;&#36275;&#22815;&#30340;&#20998;&#31867;&#32622;&#20449;&#24230;&#26102;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#26041;&#38754;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;&#30446;&#21069;&#65292;EENNs&#30340;&#35774;&#35745;&#26159;&#30001;&#19987;&#23478;&#25163;&#21160;&#23436;&#25104;&#30340;&#65292;&#36825;&#26159;&#19968;&#39033;&#22797;&#26434;&#21644;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#32771;&#34385;&#35768;&#22810;&#26041;&#38754;&#65292;&#21253;&#25324;&#27491;&#30830;&#30340;&#25918;&#32622;&#12289;&#38408;&#20540;&#35774;&#32622;&#21644;EECs&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#27491;&#22312;&#25506;&#32034;&#20351;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#33258;&#21160;&#21270;&#35774;&#35745;EENNs&#12290;&#30446;&#21069;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#20010;&#23436;&#25972;&#30340;NAS&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;EENNs&#65292;&#24182;&#19988;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#32508;&#21512;&#35774;&#35745;&#31574;&#30053;&#65292;&#21516;&#26102;&#32771;&#34385;&#39592;&#24178;&#21644;EECs&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#21576;&#29616;&#20102;&#38754;&#21521;&#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NACHOS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN) with Early Exit Classifiers (EECs), to provide predictions at intermediate points of the processing when enough confidence in classification is achieved. This leads to many benefits in terms of effectiveness and efficiency. Currently, the design of EENNs is carried out manually by experts, a complex and time-consuming task that requires accounting for many aspects, including the correct placement, the thresholding, and the computational overhead of the EECs. For this reason, the research is exploring the use of Neural Architecture Search (NAS) to automatize the design of EENNs. Currently, few comprehensive NAS solutions for EENNs have been proposed in the literature, and a fully automated, joint design strategy taking into consideration both the backbone and the EECs remains an open problem. To this end, this work presents Neural Architecture Search for Hardware Constrained Early Exit Neural Networks (NACHOS),
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#21512;&#25104;&#20581;&#24247;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#38450;&#25252;&#65292;&#29983;&#25104;&#19982;&#21387;&#21147;&#26102;&#21051;&#30456;&#20851;&#30340;&#21512;&#25104;&#24207;&#21015;&#25968;&#25454;&#65292;&#30830;&#20445;&#24739;&#32773;&#20449;&#24687;&#30340;&#20445;&#25252;&#65292;&#24182;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12290;&#22312;&#21387;&#21147;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13327</link><description>&lt;p&gt;
&#20026;&#38544;&#31169;&#20445;&#25252;&#21487;&#31359;&#25140;&#21387;&#21147;&#26816;&#27979;&#29983;&#25104;&#21512;&#25104;&#20581;&#24247;&#20256;&#24863;&#22120;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable Stress Detection. (arXiv:2401.13327v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#21512;&#25104;&#20581;&#24247;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#38450;&#25252;&#65292;&#29983;&#25104;&#19982;&#21387;&#21147;&#26102;&#21051;&#30456;&#20851;&#30340;&#21512;&#25104;&#24207;&#21015;&#25968;&#25454;&#65292;&#30830;&#20445;&#24739;&#32773;&#20449;&#24687;&#30340;&#20445;&#25252;&#65292;&#24182;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12290;&#22312;&#21387;&#21147;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25163;&#34920;&#30340;&#20581;&#24247;&#20256;&#24863;&#22120;&#25968;&#25454;&#22312;&#26234;&#33021;&#20581;&#24247;&#24212;&#29992;&#21644;&#24739;&#32773;&#30417;&#27979;&#20013;&#36234;&#26469;&#36234;&#34987;&#20351;&#29992;&#65292;&#21253;&#25324;&#21387;&#21147;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#21307;&#30103;&#25968;&#25454;&#24448;&#24448;&#21253;&#21547;&#25935;&#24863;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#24182;&#19988;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#20197;&#36827;&#34892;&#30740;&#31350;&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#27880;&#38544;&#31169;&#30340;&#21512;&#25104;&#22810;&#20256;&#24863;&#22120;&#26234;&#33021;&#25163;&#34920;&#20581;&#24247;&#35835;&#25968;&#19982;&#21387;&#21147;&#26102;&#21051;&#30456;&#20851;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#21512;&#25104;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#26045;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#38450;&#25252;&#20197;&#20445;&#25252;&#24739;&#32773;&#20449;&#24687;&#12290;&#20026;&#20102;&#30830;&#20445;&#21512;&#25104;&#25968;&#25454;&#30340;&#23436;&#25972;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31995;&#21015;&#36136;&#37327;&#35780;&#20272;&#65292;&#24182;&#30417;&#27979;&#21512;&#25104;&#25968;&#25454;&#19982;&#21407;&#22987;&#25968;&#25454;&#20043;&#38388;&#30340;&#21512;&#29702;&#24615;&#12290;&#20026;&#20102;&#27979;&#35797;&#20854;&#26377;&#29992;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#24120;&#29992;&#20294;&#35268;&#27169;&#36739;&#23567;&#30340;&#21387;&#21147;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#21019;&#24314;&#20102;&#31169;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#22686;&#24378;&#29616;&#26377;&#25968;&#25454;&#22522;&#30784;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smartwatch health sensor data is increasingly utilized in smart health applications and patient monitoring, including stress detection. However, such medical data often comprises sensitive personal information and is resource-intensive to acquire for research purposes. In response to this challenge, we introduce the privacy-aware synthetization of multi-sensor smartwatch health readings related to moments of stress. Our method involves the generation of synthetic sequence data through Generative Adversarial Networks (GANs), coupled with the implementation of Differential Privacy (DP) safeguards for protecting patient information during model training. To ensure the integrity of our synthetic data, we employ a range of quality assessments and monitor the plausibility between synthetic and original data. To test the usefulness, we create private machine learning models on a commonly used, albeit small, stress detection dataset, exploring strategies for enhancing the existing data foundat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;ConTextual&#65292;&#29992;&#20110;&#35780;&#20272;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#65292;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.13311</link><description>&lt;p&gt;
ConTextual: &#22312;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#35780;&#20272;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models. (arXiv:2401.13311v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;ConTextual&#65292;&#29992;&#20110;&#35780;&#20272;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#65292;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#28041;&#21450;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#20363;&#22914;&#22312;&#20844;&#20849;&#22330;&#25152;&#23548;&#33322;&#22320;&#22270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ConTextual&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#19987;&#38376;&#35774;&#35745;&#30340;&#25351;&#20196;&#65292;&#29992;&#20110;&#35780;&#20272;LMMs&#22312;&#25191;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ConTextual&#24378;&#35843;&#20102;&#22810;&#26679;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#65288;&#20363;&#22914;&#26102;&#38388;&#38405;&#35835;&#12289;&#23548;&#33322;&#12289;&#36141;&#29289;&#31561;&#65289;&#65292;&#35201;&#27714;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#20339;&#34920;&#29616;&#30340;LMM&#65292;GPT-4V(ision)&#65292;&#19982;&#20154;&#31867;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;30.8%&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#20351;&#29992;&#20154;&#31867;&#35780;&#20272;&#25351;&#20986;&#22312;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#34429;&#28982;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#65288;&#22914;&#27169;&#22240;&#21644;&#24341;&#25991;&#35299;&#37322;&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#25972;&#20307;&#24615;&#33021;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in AI have led to the development of large multimodal models (LMMs) capable of processing complex tasks involving joint reasoning over text and visual content in the image (e.g., navigating maps in public places). This paper introduces ConTextual, a novel benchmark comprising instructions designed explicitly to evaluate LMMs' ability to perform context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse real-world scenarios (e.g., time-reading, navigation, shopping and more) demanding a deeper understanding of the interactions between textual and visual elements. Our findings reveal a significant performance gap of 30.8% between the best-performing LMM, GPT-4V(ision), and human capabilities using human evaluation indicating substantial room for improvement in context-sensitive text-rich visual reasoning. Notably, while GPT-4V excelled in abstract categories like meme and quote interpretation, its overall performance still lagged behind humans. In add
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#25918;&#23556;&#23398;&#23396;&#31435;&#32508;&#21512;&#24449;&#65288;RIS&#65289;&#21644;&#20020;&#24202;&#23396;&#31435;&#32508;&#21512;&#24449;&#65288;CIS&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;3T MRI&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#25552;&#39640;&#20102;&#23545;&#20122;&#20020;&#24202;&#24418;&#24335;&#30340;&#26816;&#27979;&#65292;&#26377;&#21161;&#20110;&#21306;&#20998;&#24739;&#32773;&#12290;</title><link>http://arxiv.org/abs/2401.13301</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#25918;&#23556;&#23398;&#23396;&#31435;&#32508;&#21512;&#24449;&#21644;&#20020;&#24202;&#23396;&#31435;&#32508;&#21512;&#24449;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Radiologically Isolated Syndrome and Clinically Isolated Syndrome with Machine-Learning Techniques. (arXiv:2401.13301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13301
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#25918;&#23556;&#23398;&#23396;&#31435;&#32508;&#21512;&#24449;&#65288;RIS&#65289;&#21644;&#20020;&#24202;&#23396;&#31435;&#32508;&#21512;&#24449;&#65288;CIS&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;3T MRI&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#25552;&#39640;&#20102;&#23545;&#20122;&#20020;&#24202;&#24418;&#24335;&#30340;&#26816;&#27979;&#65292;&#26377;&#21161;&#20110;&#21306;&#20998;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#21644;&#30446;&#30340;&#65306;&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#65292;&#26080;&#30151;&#29366;&#20027;&#39064;&#30340;&#22823;&#33041;&#30333;&#36136;&#30149;&#21464;&#34987;&#31216;&#20026;&#25918;&#23556;&#23398;&#23396;&#31435;&#32508;&#21512;&#24449;&#65288;RIS&#65289;&#12290;&#26089;&#26399;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#65288;MS&#65289;[&#21363;&#20020;&#24202;&#23396;&#31435;&#32508;&#21512;&#24449;&#65288;CIS&#65289;]&#19982;RIS&#20043;&#38388;&#30340;&#21306;&#21035;&#22312;&#20110;&#26159;&#21542;&#21457;&#29983;&#20020;&#24202;&#20107;&#20214;&#65292;&#22240;&#27492;&#65292;&#22312;&#19981;&#24178;&#25200;MRI&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#23545;&#20122;&#20020;&#24202;&#24418;&#24335;&#30340;&#26816;&#27979;&#26159;&#21512;&#29702;&#30340;&#65292;&#22240;&#20026;&#24050;&#26377;&#25918;&#23556;&#23398;&#35786;&#26029;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#35782;&#21035;&#24418;&#24577;&#27979;&#37327;&#25351;&#26631;&#65292;&#24110;&#21161;&#21306;&#20998;RIS&#24739;&#32773;&#21644;CIS&#24739;&#32773;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20351;&#29992;&#22810;&#27169;&#24577;3T MRI&#26041;&#27861;&#65292;&#32467;&#21512;MRI&#29983;&#29289;&#26631;&#24535;&#29289;&#65288;&#30382;&#23618;&#21402;&#24230;&#12289;&#30382;&#23618;&#21644;&#30382;&#36136;&#19979;&#28784;&#36136;&#20307;&#31215;&#20197;&#21450;&#30333;&#36136;&#23436;&#25972;&#24615;&#65289;&#65292;&#23545;17&#21517;RIS&#24739;&#32773;&#21644;17&#21517;CIS&#24739;&#32773;&#30340;&#21333;&#20010;&#24739;&#32773;&#36827;&#34892;&#20998;&#31867;&#12290;&#32467;&#26524;&#65306;&#39044;&#27979;dia
&lt;/p&gt;
&lt;p&gt;
Background and purpose: The unanticipated detection by magnetic resonance imaging (MRI) in the brain of asymptomatic subjects of white matter lesions suggestive of multiple sclerosis (MS) has been named radiologically isolated syndrome (RIS). As the difference between early MS [i.e. clinically isolated syndrome (CIS)] and RIS is the occurrence of a clinical event, it is logical to improve detection of the subclinical form without interfering with MRI as there are radiological diagnostic criteria for that. Our objective was to use machine-learning classification methods to identify morphometric measures that help to discriminate patients with RIS from those with CIS.  Methods: We used a multimodal 3-T MRI approach by combining MRI biomarkers (cortical thickness, cortical and subcortical grey matter volume, and white matter integrity) of a cohort of 17 patients with RIS and 17 patients with CIS for single-subject level classification.  Results: The best proposed models to predict the dia
&lt;/p&gt;</description></item><item><title>RefreshNet&#26159;&#19968;&#20010;&#22810;&#23610;&#24230;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#23618;&#21047;&#26032;&#26426;&#21046;&#26469;&#23398;&#20064;&#22797;&#26434;&#31995;&#32479;&#30340;&#21160;&#24577;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.13282</link><description>&lt;p&gt;
RefreshNet: &#36890;&#36807;&#20998;&#23618;&#21047;&#26032;&#23398;&#20064;&#22810;&#23610;&#24230;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
RefreshNet: Learning Multiscale Dynamics through Hierarchical Refreshing. (arXiv:2401.13282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13282
&lt;/p&gt;
&lt;p&gt;
RefreshNet&#26159;&#19968;&#20010;&#22810;&#23610;&#24230;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#23618;&#21047;&#26032;&#26426;&#21046;&#26469;&#23398;&#20064;&#22797;&#26434;&#31995;&#32479;&#30340;&#21160;&#24577;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#26399;&#39044;&#27979;&#65292;&#22987;&#32456;&#21463;&#21040;&#35823;&#24046;&#32047;&#31215;&#21644;&#35745;&#31639;&#36127;&#25285;&#30340;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RefreshNet&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#23610;&#24230;&#26694;&#26550;&#65292;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24179;&#34913;&#12290;RefreshNet&#32467;&#21512;&#20102;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#35782;&#21035;&#25429;&#25417;&#21160;&#24577;&#30340;&#22522;&#26412;&#29305;&#24449;&#30340;&#38477;&#38454;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20869;&#31574;&#30053;&#24615;&#22320;&#20351;&#29992;&#22810;&#20010;&#26102;&#38388;&#20998;&#36776;&#29575;&#19981;&#21516;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#22359;&#65292;&#20174;&#32780;&#20801;&#35768;&#22810;&#20010;&#26102;&#38388;&#23610;&#24230;&#19978;&#25429;&#25417;&#28508;&#22312;&#21160;&#24577;&#12290;RefreshNet&#20013;&#30340;&#29420;&#29305;&#30340;&#8220;&#21047;&#26032;&#8221;&#26426;&#21046;&#20801;&#35768;&#36739;&#31895;&#31961;&#30340;&#22359;&#37325;&#26032;&#35774;&#32622;&#36739;&#32454;&#22359;&#30340;&#36755;&#20837;&#65292;&#26377;&#25928;&#25511;&#21046;&#21644;&#20943;&#36731;&#35823;&#24046;&#32047;&#31215;&#12290;&#36825;&#31181;&#35774;&#35745;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#39044;&#27979;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting complex system dynamics, particularly for long-term predictions, is persistently hindered by error accumulation and computational burdens. This study presents RefreshNet, a multiscale framework developed to overcome these challenges, delivering an unprecedented balance between computational efficiency and predictive accuracy. RefreshNet incorporates convolutional autoencoders to identify a reduced order latent space capturing essential features of the dynamics, and strategically employs multiple recurrent neural network (RNN) blocks operating at varying temporal resolutions within the latent space, thus allowing the capture of latent dynamics at multiple temporal scales. The unique "refreshing" mechanism in RefreshNet allows coarser blocks to reset inputs of finer blocks, effectively controlling and alleviating error accumulation. This design demonstrates superiority over existing techniques regarding computational efficiency and predictive accuracy, especially in long-term
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#20247;&#21253;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#26032;&#39062;&#30340;&#32858;&#21512;&#26041;&#26696;&#65292;&#26681;&#25454;&#20247;&#21253;&#24037;&#20316;&#32773;&#23545;&#20808;&#21069;&#25968;&#37327;&#30340;&#20272;&#35745;&#35843;&#25972;&#26435;&#37325;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#38598;&#20307;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36866;&#24212;&#22797;&#26434;&#27169;&#22411;&#21644;&#20854;&#20182;&#23454;&#38469;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#35745;&#31639;&#30740;&#31350;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.13239</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20247;&#21253;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Crowdsourcing Via Self-Supervised Learning. (arXiv:2401.13239v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#20247;&#21253;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#26032;&#39062;&#30340;&#32858;&#21512;&#26041;&#26696;&#65292;&#26681;&#25454;&#20247;&#21253;&#24037;&#20316;&#32773;&#23545;&#20808;&#21069;&#25968;&#37327;&#30340;&#20272;&#35745;&#35843;&#25972;&#26435;&#37325;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#38598;&#20307;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36866;&#24212;&#22797;&#26434;&#27169;&#22411;&#21644;&#20854;&#20182;&#23454;&#38469;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#35745;&#31639;&#30740;&#31350;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#30340;&#20247;&#21253;&#31995;&#32479;&#36890;&#36807;&#23545;&#20247;&#22810;&#20247;&#21253;&#24037;&#20316;&#32773;&#25552;&#20379;&#30340;&#28508;&#22312;&#21033;&#30410;&#25968;&#37327;&#30340;&#20272;&#35745;&#36827;&#34892;&#24179;&#22343;&#24471;&#21040;&#38598;&#20307;&#20272;&#35745;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;--&#21482;&#39044;&#27979;&#20854;&#20182;&#20154;--&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#21512;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#26681;&#25454;&#20182;&#20204;&#23545;&#20808;&#21069;&#25968;&#37327;&#30340;&#20272;&#35745;&#32473;&#20104;&#20247;&#21253;&#24037;&#20316;&#32773;&#20998;&#37197;&#30340;&#26435;&#37325;&#26469;&#35843;&#25972;&#12290;&#24403;&#20247;&#21253;&#24037;&#20316;&#32773;&#30340;&#25216;&#33021;&#21464;&#21270;&#25110;&#20182;&#20204;&#30340;&#20272;&#35745;&#30456;&#20851;&#26102;&#65292;&#21152;&#26435;&#27714;&#21644;&#27604;&#24179;&#22343;&#27714;&#21644;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#38598;&#20307;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#31639;&#27861;&#65292;&#22914;&#26399;&#26395;&#26368;&#22823;&#21270;&#65292;&#33267;&#23569;&#22312;&#21407;&#21017;&#19978;&#21487;&#20197;&#20135;&#29983;&#31867;&#20284;&#20934;&#30830;&#30340;&#38598;&#20307;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#24403;&#38656;&#35201;&#20351;&#29992;&#35832;&#22914;&#31070;&#32463;&#32593;&#32476;&#20043;&#31867;&#30340;&#22797;&#26434;&#27169;&#22411;&#26469;&#34920;&#31034;&#20247;&#21253;&#24037;&#20316;&#32773;&#20043;&#38388;&#30340;&#20851;&#31995;&#26102;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#38656;&#27714;&#21464;&#24471;&#32321;&#37325;&#12290;&#21482;&#39044;&#27979;&#20854;&#20182;&#20154;&#36866;&#24212;&#20102;&#36825;&#31181;&#22797;&#26434;&#24615;&#20197;&#21450;&#35768;&#22810;&#20854;&#20182;&#23454;&#38469;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#35745;&#31639;&#30740;&#31350;&#20998;&#26512;&#20102;&#21482;&#39044;&#27979;&#20854;&#20182;&#20154;&#30340;&#21151;&#25928;&#12290;&#20854;&#20013;
&lt;/p&gt;
&lt;p&gt;
Common crowdsourcing systems average estimates of a latent quantity of interest provided by many crowdworkers to produce a group estimate. We develop a new approach -- just-predict-others -- that leverages self-supervised learning and a novel aggregation scheme. This approach adapts weights assigned to crowdworkers based on estimates they provided for previous quantities. When skills vary across crowdworkers or their estimates correlate, the weighted sum offers a more accurate group estimate than the average. Existing algorithms such as expectation maximization can, at least in principle, produce similarly accurate group estimates. However, their computational requirements become onerous when complex models, such as neural networks, are required to express relationships among crowdworkers. Just-predict-others accommodates such complexity as well as many other practical challenges. We analyze the efficacy of just-predict-others through theoretical and computational studies. Among other 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21512;&#20316;&#38382;&#39064;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21457;&#29616;&#21482;&#26377;&#19982;&#25317;&#26377;&#26356;&#22810;&#35757;&#32451;&#25968;&#25454;&#21644;&#30456;&#20284;&#25968;&#25454;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#21512;&#20316;&#65292;&#25165;&#33021;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26681;&#25454;&#36825;&#19968;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#23618;&#27425;&#32858;&#31867;&#30340;&#21512;&#20316;&#35757;&#32451;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.13236</link><description>&lt;p&gt;
&#22914;&#20309;&#21512;&#20316;&#65306;&#26397;&#30528;&#26368;&#22823;&#21270;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
How to Collaborate: Towards Maximizing the Generalization Performance in Cross-Silo Federated Learning. (arXiv:2401.13236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21512;&#20316;&#38382;&#39064;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21457;&#29616;&#21482;&#26377;&#19982;&#25317;&#26377;&#26356;&#22810;&#35757;&#32451;&#25968;&#25454;&#21644;&#30456;&#20284;&#25968;&#25454;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#21512;&#20316;&#65292;&#25165;&#33021;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26681;&#25454;&#36825;&#19968;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#23618;&#27425;&#32858;&#31867;&#30340;&#21512;&#20316;&#35757;&#32451;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#21560;&#24341;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20851;&#27880;&#20132;&#21449;&#25968;&#25454;&#28304;&#30340;FL&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#22312;&#35757;&#32451;&#21518;&#25104;&#20026;&#27169;&#22411;&#25152;&#26377;&#32773;&#65292;&#24182;&#19988;&#21482;&#20851;&#24515;&#27169;&#22411;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#30001;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#35201;&#27714;&#25152;&#26377;&#23458;&#25143;&#31471;&#21442;&#21152;&#21333;&#19968;&#30340;FL&#35757;&#32451;&#36807;&#31243;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35843;&#26597;&#21512;&#20316;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20102;&#27599;&#20010;&#23458;&#25143;&#31471;&#22312;&#19982;&#20854;&#20182;&#23458;&#25143;&#31471;&#21512;&#20316;&#25110;&#29420;&#31435;&#35757;&#32451;&#26102;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#36890;&#36807;&#19982;&#20855;&#26377;&#26356;&#22810;&#35757;&#32451;&#25968;&#25454;&#21644;&#30456;&#20284;&#25968;&#25454;&#20998;&#24067;&#30340;&#20854;&#20182;&#23458;&#25143;&#31471;&#21512;&#20316;&#65292;&#21487;&#20197;&#25913;&#21892;&#23458;&#25143;&#31471;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#23558;&#23458;&#25143;&#31471;&#20998;&#25104;&#22810;&#20010;&#21512;&#20316;&#32452;&#26469;&#21046;&#23450;&#23458;&#25143;&#31471;&#25928;&#29992;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#32858;&#31867;&#30340;&#21512;&#20316;&#35757;&#32451;&#65288;HCCT&#65289;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has attracted vivid attention as a privacy-preserving distributed learning framework. In this work, we focus on cross-silo FL, where clients become the model owners after training and are only concerned about the model's generalization performance on their local data. Due to the data heterogeneity issue, asking all the clients to join a single FL training process may result in model performance degradation. To investigate the effectiveness of collaboration, we first derive a generalization bound for each client when collaborating with others or when training independently. We show that the generalization performance of a client can be improved only by collaborating with other clients that have more training data and similar data distribution. Our analysis allows us to formulate a client utility maximization problem by partitioning clients into multiple collaborating groups. A hierarchical clustering-based collaborative training (HCCT) scheme is then proposed, wh
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20248;&#21270;&#20154;&#31867;&#27880;&#37322;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20256;&#32479;&#30340;&#38543;&#26426;&#36873;&#25321;&#25968;&#25454;&#26041;&#27861;&#24573;&#35270;&#20102;&#25968;&#25454;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#32780;&#35813;&#26041;&#27861;&#23558;&#32771;&#34385;&#36825;&#20123;&#22240;&#32032;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#36873;&#25321;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13229</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#21040;&#26377;&#20449;&#24687;&#36873;&#25321;&#25968;&#25454;&#65306;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#20248;&#21270;&#20154;&#31867;&#27880;&#37322;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning. (arXiv:2401.13229v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13229
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20248;&#21270;&#20154;&#31867;&#27880;&#37322;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20256;&#32479;&#30340;&#38543;&#26426;&#36873;&#25321;&#25968;&#25454;&#26041;&#27861;&#24573;&#35270;&#20102;&#25968;&#25454;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#32780;&#35813;&#26041;&#27861;&#23558;&#32771;&#34385;&#36825;&#20123;&#22240;&#32032;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#36873;&#25321;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#33719;&#21462;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#19968;&#31181;&#36873;&#25321;&#26159;&#20351;&#29992;&#20247;&#21253;&#24179;&#21488;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#20247;&#21253;&#24341;&#20837;&#20102;&#19982;&#27880;&#37322;&#32773;&#30340;&#32463;&#39564;&#12289;&#19968;&#33268;&#24615;&#21644;&#20559;&#35265;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#31181;&#36873;&#25321;&#26159;&#20351;&#29992;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#20294;&#19982;&#23569;&#26679;&#26412;&#25110;&#23436;&#20840;&#30417;&#30563;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#38646;&#26679;&#26412;&#26041;&#27861;&#26377;&#20854;&#23616;&#38480;&#24615;&#12290;&#26368;&#36817;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#21160;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#22312;&#25968;&#25454;&#20005;&#37325;&#21463;&#38480;&#30340;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#23427;&#20204;&#24448;&#24448;&#38590;&#20197;&#36866;&#24212;&#12290;&#22240;&#27492;&#65292;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20154;&#31867;&#38543;&#26426;&#27880;&#37322;&#19968;&#32452;&#25968;&#25454;&#28857;&#26469;&#26500;&#24314;&#21021;&#22987;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#25277;&#26679;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#23427;&#24573;&#35270;&#20102;&#25968;&#25454;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#29305;&#23450;&#38656;&#27714;&#12290;&#24403;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#26102;&#65292;&#24773;&#20917;&#26356;&#21152;&#31967;&#31957;&#65292;&#22240;&#20026;&#38543;&#26426;&#25277;&#26679;&#20542;&#21521;&#20110;&#20005;&#37325;&#20559;&#21521;&#22810;&#25968;&#31867;&#21035;&#65292;&#23548;&#33268;&#36807;&#22810;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in Natural Language Processing is obtaining annotated data for supervised learning. An option is the use of crowdsourcing platforms for data annotation. However, crowdsourcing introduces issues related to the annotator's experience, consistency, and biases. An alternative is to use zero-shot methods, which in turn have limitations compared to their few-shot or fully supervised counterparts. Recent advancements driven by large language models show potential, but struggle to adapt to specialized domains with severely limited data. The most common approaches therefore involve the human itself randomly annotating a set of datapoints to build initial datasets. But randomly sampling data to be annotated is often inefficient as it ignores the characteristics of the data and the specific needs of the model. The situation worsens when working with imbalanced datasets, as random sampling tends to heavily bias towards the majority classes, leading to excessive annotated data. To
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13227</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38142;&#25509;&#39044;&#27979;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22270;&#23398;&#20064;&#26159;&#19968;&#39033;&#26032;&#39062;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22270;&#20013;&#34164;&#21547;&#30340;&#22823;&#37327;&#20449;&#24687;&#32473;&#36825;&#19968;&#36807;&#31243;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;LPNL&#65288;Link Prediction via Natural Language&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#30340;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#22270;&#32454;&#33410;&#30340;&#21019;&#26032;&#25552;&#31034;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#37319;&#26679;&#27969;&#31243;&#65292;&#20174;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#26469;&#25511;&#21046;&#36755;&#20837;&#20196;&#29260;&#25968;&#37327;&#22312;&#39044;&#23450;&#38480;&#21046;&#20869;&#65292;&#35299;&#20915;&#20102;&#20449;&#24687;&#36807;&#36733;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;T5&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#22823;&#22411;&#20844;&#20849;&#24322;&#26500;&#22270;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;LPNL&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21508;&#31181;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring the application of large-scale language models to graph learning is a novel endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to this process. This paper focuses on the link prediction task and introduces LPNL (Link Prediction via Natural Language), a framework based on a large language model designed for scalable link prediction on large-scale heterogeneous graphs.We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from large-scale heterogeneous graphs, and a divide-and-conquer strategy to control the input token count within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for for link prediction. Extensive experiments on a large public heterogeneous graphs demonstrate that LPNL outperforms various advanced baselin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31232;&#32570;&#26631;&#35760;&#30340;&#38646;&#26679;&#26412;&#22522;&#22240;&#32452;&#20998;&#31867;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#31216;&#20026;TEPI&#12290;&#36890;&#36807;&#23558;&#22522;&#22240;&#32452;&#34920;&#31034;&#20026;&#20266;&#22270;&#20687;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#20998;&#31867;&#24863;&#30693;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#29289;&#31181;&#30340;&#32452;&#25104;&#21644;&#31995;&#32479;&#20998;&#31867;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.13219</link><description>&lt;p&gt;
TEPI: &#23545;&#31232;&#32570;&#26631;&#35760;&#30340;&#38646;&#26679;&#26412;&#22522;&#22240;&#32452;&#20998;&#31867;&#36827;&#34892;&#20998;&#31867;&#24863;&#30693;&#23884;&#20837;&#21644;&#20266;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
TEPI: Taxonomy-aware Embedding and Pseudo-Imaging for Scarcely-labeled Zero-shot Genome Classification. (arXiv:2401.13219v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31232;&#32570;&#26631;&#35760;&#30340;&#38646;&#26679;&#26412;&#22522;&#22240;&#32452;&#20998;&#31867;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#31216;&#20026;TEPI&#12290;&#36890;&#36807;&#23558;&#22522;&#22240;&#32452;&#34920;&#31034;&#20026;&#20266;&#22270;&#20687;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#20998;&#31867;&#24863;&#30693;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#29289;&#31181;&#30340;&#32452;&#25104;&#21644;&#31995;&#32479;&#20998;&#31867;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#31181;&#30340;&#22522;&#22240;&#32452;&#32534;&#30721;&#20102;&#26377;&#20215;&#20540;&#30340;&#36827;&#21270;&#12289;&#29983;&#29289;&#21644;&#31995;&#32479;&#20998;&#31867;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#29289;&#31181;&#35782;&#21035;&#12289;&#20998;&#31867;&#21644;&#29702;&#35299;&#22522;&#22240;&#26131;&#24863;&#24615;&#65292;&#22914;&#33647;&#29289;&#25239;&#24615;&#21644;&#27602;&#21147;&#12290;&#28982;&#32780;&#65292;&#24040;&#22823;&#30340;&#29289;&#31181;&#25968;&#37327;&#32473;&#24320;&#21457;&#36890;&#29992;&#30340;&#20840;&#22522;&#22240;&#32452;&#20998;&#31867;&#24037;&#20855;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#24037;&#20855;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#24517;&#39035;&#35299;&#20915;&#20855;&#26377;&#38271;&#23614;&#20998;&#24067;&#30340;&#22823;&#20998;&#31867;&#35789;&#27719;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;TEPI&#65292;&#21363;&#20998;&#31867;&#24863;&#30693;&#23884;&#20837;&#21644;&#20266;&#25104;&#20687;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#22522;&#22240;&#32452;&#34920;&#31034;&#20026;&#20266;&#22270;&#20687;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#20998;&#31867;&#24863;&#30693;&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#25512;&#29702;&#21644;&#20998;&#31867;&#12290;&#36825;&#20010;&#23884;&#20837;&#31354;&#38388;&#25429;&#25417;&#20102;&#29289;&#31181;&#30340;&#32452;&#25104;&#21644;&#31995;&#32479;&#20998;&#31867;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A species' genetic code or genome encodes valuable evolutionary, biological, and phylogenetic information that aids in species recognition, taxonomic classification, and understanding genetic predispositions like drug resistance and virulence. However, the vast number of potential species poses significant challenges in developing a general-purpose whole genome classification tool. Traditional bioinformatics tools have made notable progress but lack scalability and are computationally expensive. Machine learning-based frameworks show promise but must address the issue of large classification vocabularies with long-tail distributions. In this study, we propose addressing this problem through zero-shot learning using TEPI, Taxonomy-aware Embedding and Pseudo-Imaging. We represent each genome as pseudo-images and map them to a taxonomy-aware embedding space for reasoning and classification. This embedding space captures compositional and phylogenetic relationships of species, enabling pre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#20027;&#35201;&#21253;&#25324;&#23545;FedAvg&#31639;&#27861;&#30340;&#30028;&#38480;&#25506;&#32034;&#20197;&#21450;&#25552;&#20986;&#20102;&#32852;&#37030;&#21152;&#36895;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;FedAc&#65289;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13216</link><description>&lt;p&gt;
&#20851;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Principled Local Optimization Methods for Federated Learning. (arXiv:2401.13216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#20027;&#35201;&#21253;&#25324;&#23545;FedAvg&#31639;&#27861;&#30340;&#30028;&#38480;&#25506;&#32034;&#20197;&#21450;&#25552;&#20986;&#20102;&#32852;&#37030;&#21152;&#36895;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;FedAc&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#35774;&#22791;&#19978;&#21327;&#21516;&#36827;&#34892;&#23398;&#20064;&#65292;&#24050;&#32463;&#25104;&#20026;&#21435;&#20013;&#24515;&#21270;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#20687;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#36825;&#26679;&#30340;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#26159;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20013;&#26368;&#31361;&#20986;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#31616;&#21333;&#19988;&#21463;&#27426;&#36814;&#65292;&#20294;&#23545;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#19981;&#22815;&#28165;&#26224;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#25512;&#36827;&#23616;&#37096;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20027;&#35201;&#21253;&#25324;&#20197;&#19979;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20026;FedAvg&#24314;&#31435;&#20102;&#20005;&#26684;&#30340;&#30028;&#38480;&#65292;&#36825;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#26368;&#27969;&#34892;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FedAvg&#21487;&#33021;&#21463;&#21040;&#30340;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026;&#36845;&#20195;&#20559;&#35265;&#30340;&#27010;&#24565;&#65292;&#24182;&#19988;&#35828;&#26126;&#20102;&#39069;&#22806;&#30340;&#19977;&#38454;&#24179;&#28369;&#24615;&#20551;&#35774;&#22914;&#20309;&#20943;&#36731;&#36825;&#31181;&#24433;&#21709;&#24182;&#23548;&#33268;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#20174;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#21152;&#36895;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;FedAc&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26377;&#21407;&#21017;&#24615;&#19988;&#36895;&#24230;&#26356;&#24555;&#30340;&#32852;&#37030;&#23398;&#20064;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL), a distributed learning paradigm that scales on-device learning collaboratively, has emerged as a promising approach for decentralized AI applications. Local optimization methods such as Federated Averaging (FedAvg) are the most prominent methods for FL applications. Despite their simplicity and popularity, the theoretical understanding of local optimization methods is far from clear. This dissertation aims to advance the theoretical foundation of local methods in the following three directions.  First, we establish sharp bounds for FedAvg, the most popular algorithm in Federated Learning. We demonstrate how FedAvg may suffer from a notion we call iterate bias, and how an additional third-order smoothness assumption may mitigate this effect and lead to better convergence rates. We explain this phenomenon from a Stochastic Differential Equation (SDE) perspective.  Second, we propose Federated Accelerated Stochastic Gradient Descent (FedAc), the first principled a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#23618;&#27425;&#27880;&#24847;&#21147;&#32593;&#32476;(AMANet)&#65292;&#29992;&#20110;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;(SAR)&#22270;&#20687;&#20013;&#30340;&#33337;&#33334;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22810;&#23610;&#24230;&#29305;&#24449;&#21644;&#33258;&#36866;&#24212;&#32858;&#21512;&#26174;&#33879;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#28023;&#23736;&#29615;&#22659;&#20013;&#23567;&#22411;&#21644;&#27839;&#28023;&#33337;&#33334;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.13214</link><description>&lt;p&gt;
AMANet&#65306;&#21033;&#29992;&#33258;&#36866;&#24212;&#22810;&#23618;&#27425;&#27880;&#24847;&#21147;&#32593;&#32476;&#25552;&#21319;SAR&#33337;&#33334;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
AMANet: Advancing SAR Ship Detection with Adaptive Multi-Hierarchical Attention Network. (arXiv:2401.13214v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#23618;&#27425;&#27880;&#24847;&#21147;&#32593;&#32476;(AMANet)&#65292;&#29992;&#20110;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;(SAR)&#22270;&#20687;&#20013;&#30340;&#33337;&#33334;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22810;&#23610;&#24230;&#29305;&#24449;&#21644;&#33258;&#36866;&#24212;&#32858;&#21512;&#26174;&#33879;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#28023;&#23736;&#29615;&#22659;&#20013;&#23567;&#22411;&#21644;&#27839;&#28023;&#33337;&#33334;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;(SAR)&#22270;&#20687;&#30340;&#33337;&#33334;&#26816;&#27979;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#33337;&#33334;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#28023;&#23736;&#29615;&#22659;&#20013;&#29305;&#24449;&#26377;&#38480;&#21644;&#26434;&#20081;&#24178;&#25200;&#30340;&#21407;&#22240;&#65292;&#26816;&#27979;&#23567;&#22411;&#21644;&#27839;&#28023;&#33337;&#33334;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#22810;&#23618;&#27425;&#27880;&#24847;&#21147;&#27169;&#22359;(AMAM)&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#23610;&#24230;&#29305;&#24449;&#65292;&#24182;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#26174;&#33879;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#34701;&#21512;&#30456;&#37051;&#29305;&#24449;&#23618;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#23545;&#36739;&#23567;&#30446;&#26631;&#30340;&#26816;&#27979;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#23610;&#24230;&#29305;&#24449;&#22686;&#24378;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#28388;&#38500;&#22797;&#26434;&#32972;&#26223;&#30340;&#36127;&#38754;&#25928;&#26524;&#65292;&#25105;&#20204;&#22312;&#36890;&#36947;&#19978;&#35299;&#21078;&#20043;&#21069;&#34701;&#21512;&#30340;&#22810;&#32423;&#29305;&#24449;&#65292;&#21333;&#29420;&#25366;&#25496;&#26174;&#33879;&#21306;&#22495;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#27719;&#38598;&#26469;&#33258;&#19981;&#21516;&#36890;&#36947;&#30340;&#29305;&#24449;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#22810;&#23618;&#27425;&#27880;&#24847;&#21147;&#32593;&#32476;(AMANet)&#65292;&#29992;&#20110;&#32508;&#21512;&#24314;&#27169;&#21644;&#33337;&#33334;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, methods based on deep learning have been successfully applied to ship detection for synthetic aperture radar (SAR) images. Despite the development of numerous ship detection methodologies, detecting small and coastal ships remains a significant challenge due to the limited features and clutter in coastal environments. For that, a novel adaptive multi-hierarchical attention module (AMAM) is proposed to learn multi-scale features and adaptively aggregate salient features from various feature layers, even in complex environments. Specifically, we first fuse information from adjacent feature layers to enhance the detection of smaller targets, thereby achieving multi-scale feature enhancement. Then, to filter out the adverse effects of complex backgrounds, we dissect the previously fused multi-level features on the channel, individually excavate the salient regions, and adaptively amalgamate features originating from different channels. Thirdly, we present a novel adaptive multi-h
&lt;/p&gt;</description></item><item><title>AdCorDA&#26041;&#27861;&#36890;&#36807;&#23545;&#25239;&#20462;&#27491;&#21644;&#39046;&#22495;&#36866;&#24212;&#25913;&#36827;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#32593;&#32476;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#22312;&#26435;&#37325;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#20063;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.13212</link><description>&lt;p&gt;
AdCorDA: &#36890;&#36807;&#23545;&#25239;&#20462;&#27491;&#21644;&#39046;&#22495;&#36866;&#24212;&#36827;&#34892;&#20998;&#31867;&#22120;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
AdCorDA: Classifier Refinement via Adversarial Correction and Domain Adaptation. (arXiv:2401.13212v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13212
&lt;/p&gt;
&lt;p&gt;
AdCorDA&#26041;&#27861;&#36890;&#36807;&#23545;&#25239;&#20462;&#27491;&#21644;&#39046;&#22495;&#36866;&#24212;&#25913;&#36827;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#32593;&#32476;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#22312;&#26435;&#37325;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#20063;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#25913;&#36827;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#32593;&#32476;&#12290;&#25152;&#25552;&#20986;&#30340;AdCorDA&#26041;&#27861;&#22522;&#20110;&#20462;&#25913;&#35757;&#32451;&#38598;&#65292;&#24182;&#21033;&#29992;&#32593;&#32476;&#26435;&#37325;&#21644;&#23618;&#36755;&#20837;&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#36755;&#20837;&#31354;&#38388;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104; - &#23545;&#25239;&#20462;&#27491;&#65292;&#28982;&#21518;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#12290;&#23545;&#25239;&#20462;&#27491;&#20351;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#26469;&#20462;&#27491;&#38169;&#35823;&#30340;&#35757;&#32451;&#38598;&#20998;&#31867;&#12290;&#23558;&#35757;&#32451;&#38598;&#20013;&#38169;&#35823;&#20998;&#31867;&#30340;&#26679;&#26412;&#31227;&#38500;&#65292;&#24182;&#29992;&#23545;&#25239;&#20462;&#27491;&#30340;&#26679;&#26412;&#26367;&#25442;&#65292;&#24418;&#25104;&#26032;&#30340;&#35757;&#32451;&#38598;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#22238;&#21040;&#21407;&#22987;&#35757;&#32451;&#38598;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#65292;&#20934;&#30830;&#29575;&#25552;&#21319;&#36229;&#36807;5%&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#26435;&#37325;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#25913;&#36827;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#22522;&#32447;&#19978;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes a simple yet effective technique for refining a pretrained classifier network. The proposed AdCorDA method is based on modification of the training set and making use of the duality between network weights and layer inputs. We call this input space training. The method consists of two stages - adversarial correction followed by domain adaptation. Adversarial correction uses adversarial attacks to correct incorrect training-set classifications. The incorrectly classified samples of the training set are removed and replaced with the adversarially corrected samples to form a new training set, and then, in the second stage, domain adaptation is performed back to the original training set. Extensive experimental validations show significant accuracy boosts of over 5% on the CIFAR-100 dataset. The technique can be straightforwardly applied to refinement of weight-quantized neural networks, where experiments show substantial enhancement in performance over the baseline. T
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MITIGATE&#30340;&#22810;&#20219;&#21153;&#20027;&#21160;&#23398;&#20064;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#32806;&#21512;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#23427;&#33021;&#22815;&#26816;&#27979;&#21040;&#27809;&#26377;&#24050;&#30693;&#24322;&#24120;&#30340;&#31163;&#32676;&#33410;&#28857;&#65292;&#24182;&#21033;&#29992;&#22810;&#20010;&#20219;&#21153;&#30340;&#20849;&#20139;&#34920;&#31034;&#26469;&#25552;&#21319;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13210</link><description>&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#22810;&#20219;&#21153;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multitask Active Learning for Graph Anomaly Detection. (arXiv:2401.13210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13210
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MITIGATE&#30340;&#22810;&#20219;&#21153;&#20027;&#21160;&#23398;&#20064;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#32806;&#21512;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#23427;&#33021;&#22815;&#26816;&#27979;&#21040;&#27809;&#26377;&#24050;&#30693;&#24322;&#24120;&#30340;&#31163;&#32676;&#33410;&#28857;&#65292;&#24182;&#21033;&#29992;&#22810;&#20010;&#20219;&#21153;&#30340;&#20849;&#20139;&#34920;&#31034;&#26469;&#25552;&#21319;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#26102;&#20195;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#26080;&#22788;&#19981;&#22312;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#12290;&#20316;&#20026;&#25903;&#25345;&#32593;&#32476;&#23433;&#20840;&#21644;&#25552;&#39640;&#22522;&#20110;&#22270;&#30340;&#24212;&#29992;&#31243;&#24207;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#21152;&#12290;&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#25928;&#26524;&#65292;&#20294;&#20854;&#24615;&#33021;&#21462;&#20915;&#20110;&#36275;&#22815;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#35782;&#21035;&#22797;&#26434;&#22270;&#32467;&#26500;&#20013;&#30340;&#24322;&#24120;&#28857;&#26041;&#38754;&#65292;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#29305;&#24615;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#26469;&#33258;&#20854;&#20182;&#20219;&#21153;&#65288;&#20363;&#22914;&#33410;&#28857;&#20998;&#31867;&#65289;&#30340;&#38388;&#25509;&#30417;&#30563;&#20449;&#21495;&#30456;&#23545;&#20016;&#23500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#20027;&#21160;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#21517;&#20026;MITIGATE&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#32806;&#21512;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;MITIGATE&#33021;&#22815;&#26816;&#27979;&#21040;&#27809;&#26377;&#24050;&#30693;&#24322;&#24120;&#30340;&#31163;&#32676;&#33410;&#28857;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#22810;&#20010;&#20219;&#21153;&#30340;&#20849;&#20139;&#34920;&#31034;&#26469;&#25552;&#21319;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the web era, graph machine learning has been widely used on ubiquitous graph-structured data. As a pivotal component for bolstering web security and enhancing the robustness of graph-based applications, the significance of graph anomaly detection is continually increasing. While Graph Neural Networks (GNNs) have demonstrated efficacy in supervised and semi-supervised graph anomaly detection, their performance is contingent upon the availability of sufficient ground truth labels. The labor-intensive nature of identifying anomalies from complex graph structures poses a significant challenge in real-world applications. Despite that, the indirect supervision signals from other tasks (e.g., node classification) are relatively abundant. In this paper, we propose a novel MultItask acTIve Graph Anomaly deTEction framework, namely MITIGATE. Firstly, by coupling node classification tasks, MITIGATE obtains the capability to detect out-of-distribution nodes without known anomalies. Secondly, MI
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#33258;&#25105;&#25913;&#36827;&#24178;&#25200;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#35780;&#20272;&#20854;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25913;&#36827;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#24178;&#25200;&#31649;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13206</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#33258;&#25105;&#25913;&#36827;&#24178;&#25200;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Self-Improving Interference Management Based on Deep Learning With Uncertainty Quantification. (arXiv:2401.13206v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#33258;&#25105;&#25913;&#36827;&#24178;&#25200;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#35780;&#20272;&#20854;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25913;&#36827;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#24178;&#25200;&#31649;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#31361;&#30772;&#24615;&#33258;&#25105;&#25913;&#36827;&#24178;&#25200;&#31649;&#29702;&#26694;&#26550;&#65292;&#38024;&#23545;&#26080;&#32447;&#36890;&#20449;&#36827;&#34892;&#20102;&#23450;&#21046;&#21270;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#26368;&#20248;&#24178;&#25200;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#20248;&#21270;&#30340;&#31639;&#27861;&#25152;&#22266;&#26377;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#37325;&#22823;&#31361;&#30772;&#26159;&#25215;&#35748;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#27809;&#26377;&#20805;&#20998;&#20195;&#34920;&#30340;&#22330;&#26223;&#20013;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#20276;&#38543;&#30528;&#19968;&#20010;&#36164;&#26684;&#26631;&#20934;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;&#35813;&#26694;&#26550;&#26681;&#25454;&#37327;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#39044;&#27979;&#21487;&#20449;&#24230;&#65292;&#31574;&#30053;&#24615;&#22320;&#22312;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#20256;&#32479;&#31639;&#27861;&#20043;&#38388;&#36827;&#34892;&#20132;&#26367;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20013;&#24341;&#23548;&#19979;&#33021;&#22815;&#26377;&#25928;&#22320;&#25913;&#36827;&#24178;&#25200;&#31649;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a groundbreaking self-improving interference management framework tailored for wireless communications, integrating deep learning with uncertainty quantification to enhance overall system performance. Our approach addresses the computational challenges inherent in traditional optimization-based algorithms by harnessing deep learning models to predict optimal interference management solutions. A significant breakthrough of our framework is its acknowledgment of the limitations inherent in data-driven models, particularly in scenarios not adequately represented by the training dataset. To overcome these challenges, we propose a method for uncertainty quantification, accompanied by a qualifying criterion, to assess the trustworthiness of model predictions. This framework strategically alternates between model-generated solutions and traditional algorithms, guided by a criterion that assesses the prediction credibility based on quantified uncertainties. Experimental res
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#23884;&#20837;&#35760;&#24518;&#30340;&#23398;&#20064;&#25193;&#23637;&#22270;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#35299;&#20915;&#22312;&#19981;&#26029;&#25193;&#23637;&#30340;&#22270;&#19978;&#24212;&#29992;&#35760;&#24518;&#22238;&#25918;&#25216;&#26415;&#23548;&#33268;&#30340;&#20869;&#23384;&#29190;&#28856;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13200</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#23884;&#20837;&#35760;&#24518;&#30340;&#23398;&#20064;&#25193;&#23637;&#22270;
&lt;/p&gt;
&lt;p&gt;
Topology-aware Embedding Memory for Learning on Expanding Graphs. (arXiv:2401.13200v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13200
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#23884;&#20837;&#35760;&#24518;&#30340;&#23398;&#20064;&#25193;&#23637;&#22270;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#35299;&#20915;&#22312;&#19981;&#26029;&#25193;&#23637;&#30340;&#22270;&#19978;&#24212;&#29992;&#35760;&#24518;&#22238;&#25918;&#25216;&#26415;&#23548;&#33268;&#30340;&#20869;&#23384;&#29190;&#28856;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#22238;&#25918;&#30340;&#25216;&#26415;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#26159;&#30452;&#25509;&#24212;&#29992;&#20110;&#19981;&#26029;&#25193;&#23637;&#30340;&#22270;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#20869;&#23384;&#29190;&#28856;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#31995;&#32479;&#20998;&#26512;&#20102;&#20869;&#23384;&#29190;&#28856;&#38382;&#39064;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21363;Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM)&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#23558;&#20869;&#23384;&#31354;&#38388;&#22797;&#26434;&#24230;&#20174;$\mathcal{O}(nd^L)$&#38477;&#20302;&#21040;$\mathcal{O}(n)$&#65292;&#36824;&#20805;&#20998;&#21033;&#29992;&#20102;&#25299;&#25169;&#20449;&#24687;&#36827;&#34892;&#35760;&#24518;&#22238;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory replay based techniques have shown great success for continual learning with incrementally accumulated Euclidean data. Directly applying them to continually expanding graphs, however, leads to the potential memory explosion problem due to the need to buffer representative nodes and their associated topological neighborhood structures. To this end, we systematically analyze the key challenges in the memory explosion problem, and present a general framework, i.e., Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM), to tackle this issue. The proposed framework not only reduces the memory space complexity from $\mathcal{O}(nd^L)$ to $\mathcal{O}(n)$~\footnote{$n$: memory budget, $d$: average node degree, $L$: the radius of the GNN receptive field}, but also fully utilizes the topological information for memory replay. Specifically, PDGNNs decouple trainable parameters from the computation ego-subgraph via \textit{Topology-aware Embeddings} 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#36755;&#20837;&#32467;&#26500;&#21644;&#29983;&#25104;&#20840;&#26032;&#26448;&#26009;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.13192</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#34920;&#31034;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Generative Design of Crystal Structures by Point Cloud Representations and Diffusion Model. (arXiv:2401.13192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#36755;&#20837;&#32467;&#26500;&#21644;&#29983;&#25104;&#20840;&#26032;&#26448;&#26009;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#35774;&#35745;&#20013;&#65292;&#39640;&#25928;&#22320;&#29983;&#25104;&#33021;&#37327;&#31283;&#23450;&#30340;&#26230;&#20307;&#32467;&#26500;&#19968;&#30452;&#26159;&#20010;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#26230;&#26684;&#20013;&#21407;&#23376;&#30340;&#24040;&#22823;&#25490;&#21015;&#12290;&#20026;&#20102;&#20419;&#36827;&#31283;&#23450;&#26448;&#26009;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#21487;&#21512;&#25104;&#26448;&#26009;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28857;&#20113;&#34920;&#31034;&#26469;&#32534;&#30721;&#22797;&#26434;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#25903;&#26609;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#37325;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#36755;&#20837;&#32467;&#26500;&#65292;&#24182;&#20005;&#26684;&#39564;&#35777;&#20854;&#39640;&#37325;&#24314;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#20840;&#26032;&#30340;&#26448;&#26009;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#22522;&#20110;&#28857;&#20113;&#30340;&#26230;&#20307;&#25193;&#25955;(PCCD)&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#21512;&#25104;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#26448;&#26009;&#35774;&#35745;&#21644;&#21512;&#25104;&#30340;&#25512;&#36827;&#20013;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#29983;&#25104;&#35774;&#35745;&#26041;&#27861;&#65292;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently generating energetically stable crystal structures has long been a challenge in material design, primarily due to the immense arrangement of atoms in a crystal lattice. To facilitate the discovery of stable material, we present a framework for the generation of synthesizable materials, leveraging a point cloud representation to encode intricate structural information. At the heart of this framework lies the introduction of a diffusion model as its foundational pillar. To gauge the efficacy of our approach, we employ it to reconstruct input structures from our training datasets, rigorously validating its high reconstruction performance. Furthermore, we demonstrate the profound potential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely new materials, emphasizing their synthesizability. Our research stands as a noteworthy contribution to the advancement of materials design and synthesis through the cutting-edge avenue of generative design instead of the con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#39640;&#25928;&#35745;&#31639;&#35757;&#32451;&#38598;$\mathbf{X}^\mathbf{T}\mathbf{X}$&#21644;$\mathbf{X}^\mathbf{T}\mathbf{Y}$&#30340;&#31639;&#27861;&#65292;&#30456;&#27604;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#26174;&#33879;&#21152;&#36895;&#20132;&#21449;&#39564;&#35777;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35745;&#31639;&#30697;&#38453;&#20056;&#31215;&#25110;&#32479;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.13185</link><description>&lt;p&gt;
&#31616;&#21270;&#20132;&#21449;&#39564;&#35777;&#65306;&#39640;&#25928;&#22320;&#35745;&#31639;&#19981;&#38656;&#35201;&#20840;&#37327;&#37325;&#26032;&#35745;&#31639;&#30697;&#38453;&#20056;&#31215;&#25110;&#32479;&#35745;&#37327;&#30340;&#21015;&#21521;&#20013;&#24515;&#21270;&#21644;&#26631;&#20934;&#21270;&#35757;&#32451;&#38598;$\mathbf{X}^\mathbf{T}\mathbf{X}$&#21644;$\mathbf{X}^\mathbf{T}\mathbf{Y}$
&lt;/p&gt;
&lt;p&gt;
Shortcutting Cross-Validation: Efficiently Deriving Column-Wise Centered and Scaled Training Set $\mathbf{X}^\mathbf{T}\mathbf{X}$ and $\mathbf{X}^\mathbf{T}\mathbf{Y}$ Without Full Recomputation of Matrix Products or Statistical Moments. (arXiv:2401.13185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#39640;&#25928;&#35745;&#31639;&#35757;&#32451;&#38598;$\mathbf{X}^\mathbf{T}\mathbf{X}$&#21644;$\mathbf{X}^\mathbf{T}\mathbf{Y}$&#30340;&#31639;&#27861;&#65292;&#30456;&#27604;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#26174;&#33879;&#21152;&#36895;&#20132;&#21449;&#39564;&#35777;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35745;&#31639;&#30697;&#38453;&#20056;&#31215;&#25110;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#39564;&#35777;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#22312;&#26410;&#30693;&#25968;&#25454;&#19978;&#34920;&#29616;&#30340;&#25216;&#26415;&#12290;&#35768;&#22810;&#39044;&#27979;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;&#26680;&#30340;&#20559;&#26368;&#23567;&#20108;&#20056;&#65288;PLS&#65289;&#27169;&#22411;&#65292;&#38656;&#35201;&#20165;&#20351;&#29992;&#36755;&#20837;&#30697;&#38453;$\mathbf{X}$&#21644;&#36755;&#20986;&#30697;&#38453;$\mathbf{Y}$&#20013;&#30340;&#35757;&#32451;&#38598;&#26679;&#26412;&#26469;&#35745;&#31639;$\mathbf{X}^{\mathbf{T}}\mathbf{X}$&#21644;$\mathbf{X}^{\mathbf{T}}\mathbf{Y}$&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#39640;&#25928;&#35745;&#31639;&#36825;&#20123;&#30697;&#38453;&#30340;&#31639;&#27861;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;&#19981;&#38656;&#35201;&#21015;&#21521;&#39044;&#22788;&#29702;&#12290;&#31532;&#20108;&#31181;&#31639;&#27861;&#20801;&#35768;&#20197;&#35757;&#32451;&#38598;&#22343;&#20540;&#20026;&#20013;&#24515;&#21270;&#28857;&#36827;&#34892;&#21015;&#21521;&#20013;&#24515;&#21270;&#12290;&#31532;&#19977;&#31181;&#31639;&#27861;&#20801;&#35768;&#20197;&#35757;&#32451;&#38598;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#20026;&#20013;&#24515;&#21270;&#28857;&#21644;&#26631;&#20934;&#21270;&#28857;&#36827;&#34892;&#21015;&#21521;&#20013;&#24515;&#21270;&#21644;&#26631;&#20934;&#21270;&#12290;&#36890;&#36807;&#35777;&#26126;&#27491;&#30830;&#24615;&#21644;&#20248;&#36234;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23427;&#20204;&#30456;&#27604;&#20110;&#30452;&#25509;&#20132;&#21449;&#39564;&#35777;&#21644;&#20197;&#21069;&#30340;&#24555;&#36895;&#20132;&#21449;&#39564;&#35777;&#24037;&#20316;&#65292;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#20132;&#21449;&#39564;&#35777;&#21152;&#36895;&#65292;&#32780;&#26080;&#38656;&#25968;&#25454;&#27844;&#38706;&#12290;&#23427;&#20204;&#36866;&#21512;&#24182;&#34892;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-validation is a widely used technique for assessing the performance of predictive models on unseen data. Many predictive models, such as Kernel-Based Partial Least-Squares (PLS) models, require the computation of $\mathbf{X}^{\mathbf{T}}\mathbf{X}$ and $\mathbf{X}^{\mathbf{T}}\mathbf{Y}$ using only training set samples from the input and output matrices, $\mathbf{X}$ and $\mathbf{Y}$, respectively. In this work, we present three algorithms that efficiently compute these matrices. The first one allows no column-wise preprocessing. The second one allows column-wise centering around the training set means. The third one allows column-wise centering and column-wise scaling around the training set means and standard deviations. Demonstrating correctness and superior computational complexity, they offer significant cross-validation speedup compared with straight-forward cross-validation and previous work on fast cross-validation - all without data leakage. Their suitability for paralle
&lt;/p&gt;</description></item><item><title>AgentBoard&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#19987;&#20026;&#20998;&#26512;&#35780;&#20272;LLM&#26234;&#33021;&#20307;&#32780;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22312;&#22810;&#36718;&#20132;&#20114;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#23545;&#26234;&#33021;&#20307;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#36827;&#23637;&#29575;&#25351;&#26631;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#12290;</title><link>http://arxiv.org/abs/2401.13178</link><description>&lt;p&gt;
AgentBoard: &#19968;&#31181;&#22810;&#36718;LLM&#26234;&#33021;&#20307;&#30340;&#20998;&#26512;&#35780;&#20272;&#26495;
&lt;/p&gt;
&lt;p&gt;
AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents. (arXiv:2401.13178v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13178
&lt;/p&gt;
&lt;p&gt;
AgentBoard&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#19987;&#20026;&#20998;&#26512;&#35780;&#20272;LLM&#26234;&#33021;&#20307;&#32780;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22312;&#22810;&#36718;&#20132;&#20114;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#23545;&#26234;&#33021;&#20307;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#36827;&#23637;&#29575;&#25351;&#26631;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#36890;&#29992;&#26234;&#33021;&#20307;&#23545;&#20110;&#29702;&#35299;&#20854;&#33021;&#21147;&#24182;&#20419;&#36827;&#20854;&#34701;&#20837;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36807;&#31243;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#26159;&#22312;&#32479;&#19968;&#26694;&#26550;&#20869;&#23545;&#26234;&#33021;&#20307;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#29305;&#21035;&#26159;&#22312;&#32500;&#25252;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#21644;&#30830;&#20445;&#22810;&#36718;&#20132;&#20114;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26694;&#26550;&#20027;&#35201;&#20851;&#27880;&#26368;&#32456;&#25104;&#21151;&#29575;&#65292;&#36807;&#31243;&#20013;&#25552;&#20379;&#30340;&#35265;&#35299;&#24456;&#23569;&#65292;&#26080;&#27861;&#28145;&#20837;&#29702;&#35299;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AgentBoard&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#32508;&#21512;&#22522;&#20934;&#21644;&#20276;&#38543;&#30340;&#24320;&#28304;&#35780;&#20272;&#26694;&#26550;&#65292;&#19987;&#20026;LLM&#26234;&#33021;&#20307;&#30340;&#20998;&#26512;&#35780;&#20272;&#32780;&#35774;&#35745;&#12290;AgentBoard&#25552;&#20379;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#36827;&#23637;&#29575;&#25351;&#26631;&#65292;&#25429;&#25417;&#36880;&#27493;&#30340;&#36827;&#23637;&#65292;&#20197;&#21450;&#19968;&#20010;&#32508;&#21512;&#30340;&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#20855;&#26377;&#26131;&#20110;&#35780;&#20272;&#21644;&#20998;&#26512;&#27169;&#22411;&#33021;&#21147;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating large language models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assess
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;HuggingFace&#31038;&#21306;&#30340;&#35752;&#35770;&#35770;&#22363;&#21644;&#27169;&#22411;&#20013;&#24515;&#36827;&#34892;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#22312;&#31038;&#21306;&#20013;&#37325;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25361;&#25112;&#21644;&#30410;&#22788;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#27169;&#22411;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.13177</link><description>&lt;p&gt;
&#22312;HuggingFace&#31038;&#21306;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37325;&#29992;&#65306;&#25361;&#25112;&#12289;&#30410;&#22788;&#21644;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Model Reuse in the HuggingFace Community: Challenges, Benefit and Trends. (arXiv:2401.13177v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;HuggingFace&#31038;&#21306;&#30340;&#35752;&#35770;&#35770;&#22363;&#21644;&#27169;&#22411;&#20013;&#24515;&#36827;&#34892;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#22312;&#31038;&#21306;&#20013;&#37325;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25361;&#25112;&#21644;&#30410;&#22788;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#27169;&#22411;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTM&#65289;&#30340;&#26222;&#21450;&#26085;&#30410;&#22686;&#21152;&#65292;&#24341;&#21457;&#20102;&#23545;&#27169;&#22411;&#20013;&#24515;&#21644;&#19987;&#38376;&#29992;&#20110;&#25176;&#31649;PTM&#30340;&#24179;&#21488;&#30340;&#20852;&#36259;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#19968;&#36235;&#21183;&#65292;&#20294;&#20851;&#20110;&#29992;&#25143;&#36935;&#21040;&#30340;&#25361;&#25112;&#20197;&#21450;&#31038;&#21306;&#22914;&#20309;&#21033;&#29992;PTM&#30340;&#20840;&#38754;&#25506;&#32034;&#20173;&#28982;&#32570;&#20047;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#24191;&#27867;&#30340;&#28151;&#21512;&#26041;&#27861;&#23454;&#35777;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;HuggingFace&#30340;&#35752;&#35770;&#35770;&#22363;&#21644;&#27169;&#22411;&#20013;&#24515;&#65292;&#36825;&#26159;&#26368;&#22823;&#30340;&#20844;&#20849;&#27169;&#22411;&#20013;&#24515;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#35813;&#31038;&#21306;&#20013;&#37325;&#29992;PTM&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#21644;&#30410;&#22788;&#30340;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23450;&#37327;&#30740;&#31350;&#65292;&#36319;&#36394;&#27169;&#22411;&#31867;&#22411;&#30340;&#36235;&#21183;&#21644;&#27169;&#22411;&#25991;&#26723;&#30340;&#28436;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#19968;&#20123;&#26222;&#36941;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#32473;&#21021;&#23398;&#32773;&#25552;&#20379;&#26377;&#38480;&#30340;&#25351;&#23548;&#65292;&#27169;&#22411;&#22312;&#35757;&#32451;&#25110;&#25512;&#29702;&#20013;&#30340;&#36755;&#20986;&#29702;&#35299;&#22256;&#38590;&#20197;&#21450;&#32570;&#20047;&#27169;&#22411;&#29702;&#35299;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#27169;&#22411;&#36235;&#21183;&#65292;&#20854;&#20013;&#19968;&#20123;&#27169;&#22411;&#20445;&#25345;&#30528;&#39640;&#19978;&#20256;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ubiquity of large-scale Pre-Trained Models (PTMs) is on the rise, sparking interest in model hubs, and dedicated platforms for hosting PTMs. Despite this trend, a comprehensive exploration of the challenges that users encounter and how the community leverages PTMs remains lacking. To address this gap, we conducted an extensive mixed-methods empirical study by focusing on discussion forums and the model hub of HuggingFace, the largest public model hub. Based on our qualitative analysis, we present a taxonomy of the challenges and benefits associated with PTM reuse within this community. We then conduct a quantitative study to track model-type trends and model documentation evolution over time. Our findings highlight prevalent challenges such as limited guidance for beginner users, struggles with model output comprehensibility in training or inference, and a lack of model understanding. We also identified interesting trends among models where some models maintain high upload rates de
&lt;/p&gt;</description></item><item><title>&#36870;&#21521;&#35774;&#35745;&#36215;&#21040;&#20248;&#21270;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#20316;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#25429;&#33719;&#30340;&#23398;&#20064;&#33021;&#37327;&#20989;&#25968;&#65292;&#21487;&#20197;&#36991;&#20813;&#23545;&#25239;&#31034;&#20363;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#35774;&#35745;&#24615;&#33021;&#12290;&#36825;&#19968;&#35774;&#35745;&#31995;&#32479;&#26159;&#32452;&#21512;&#24615;&#30340;&#65292;&#20351;&#24471;&#21487;&#20197;&#35774;&#35745;&#20855;&#26377;&#27599;&#20010;&#25351;&#23450;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.13171</link><description>&lt;p&gt;
&#32452;&#21512;&#24335;&#29983;&#25104;&#36870;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Compositional Generative Inverse Design. (arXiv:2401.13171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13171
&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#35774;&#35745;&#36215;&#21040;&#20248;&#21270;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#20316;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#25429;&#33719;&#30340;&#23398;&#20064;&#33021;&#37327;&#20989;&#25968;&#65292;&#21487;&#20197;&#36991;&#20813;&#23545;&#25239;&#31034;&#20363;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#35774;&#35745;&#24615;&#33021;&#12290;&#36825;&#19968;&#35774;&#35745;&#31995;&#32479;&#26159;&#32452;&#21512;&#24615;&#30340;&#65292;&#20351;&#24471;&#21487;&#20197;&#35774;&#35745;&#20855;&#26377;&#27599;&#20010;&#25351;&#23450;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#35774;&#35745;&#26159;&#19968;&#31181;&#23547;&#27714;&#35774;&#35745;&#36755;&#20837;&#21464;&#37327;&#20197;&#20248;&#21270;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#22312;&#26426;&#26800;&#24037;&#31243;&#21040;&#33322;&#22825;&#24037;&#31243;&#31561;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#36870;&#35774;&#35745;&#36890;&#24120;&#34987;&#26500;&#24314;&#25104;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#24448;&#24448;&#20250;&#38519;&#20837;&#23545;&#25239;&#27169;&#24335;&#65292;&#38459;&#30861;&#26377;&#25928;&#30340;&#25277;&#26679;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#25429;&#33719;&#30340;&#23398;&#20064;&#33021;&#37327;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#36991;&#20813;&#36825;&#31181;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#35774;&#35745;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#26679;&#19968;&#20010;&#35774;&#35745;&#31995;&#32479;&#26159;&#32452;&#21512;&#24615;&#30340;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#32467;&#21512;&#22810;&#20010;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#20195;&#34920;&#25152;&#38656;&#31995;&#32479;&#30340;&#23376;&#32452;&#20214;&#65292;&#20174;&#32780;&#35774;&#35745;&#20855;&#26377;&#27599;&#20010;&#25351;&#23450;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;&#22312;&#19968;&#20010;N&#20307;&#30456;&#20114;&#20316;&#29992;&#20219;&#21153;&#21644;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20108;&#32500;&#22810;&#32764;&#22411;&#35774;&#35745;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#32452;&#21512;&#23398;&#20064;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#35774;&#35745;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse design, where we seek to design input variables in order to optimize an underlying objective function, is an important problem that arises across fields such as mechanical engineering to aerospace engineering. Inverse design is typically formulated as an optimization problem, with recent works leveraging optimization across learned dynamics models. However, as models are optimized they tend to fall into adversarial modes, preventing effective sampling. We illustrate that by instead optimizing over the learned energy function captured by the diffusion model, we can avoid such adversarial examples and significantly improve design performance. We further illustrate how such a design system is compositional, enabling us to combine multiple different diffusion models representing subcomponents of our desired system to design systems with every specified component. In an N-body interaction task and a challenging 2D multi-airfoil design task, we demonstrate that by composing the learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;SpacTor-T5&#65292;&#32467;&#21512;&#20102;&#36328;&#24230;&#30772;&#22351;&#21644;&#26367;&#25442;&#35789;&#27719;&#26816;&#27979;&#30340;&#28151;&#21512;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#37319;&#29992;&#20004;&#38454;&#27573;&#35838;&#31243;&#34920;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SpacTor-T5&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934;SC&#39044;&#35757;&#32451;&#30456;&#21516;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#39044;&#35757;&#32451;&#36845;&#20195;&#27425;&#25968;&#21644;&#24635;FLOP&#12290;</title><link>http://arxiv.org/abs/2401.13160</link><description>&lt;p&gt;
SpacTor-T5&#65306;&#20351;&#29992;&#36328;&#24230;&#30772;&#22351;&#21644;&#26367;&#25442;&#35789;&#27719;&#26816;&#27979;&#36827;&#34892;T5&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced Token Detection. (arXiv:2401.13160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;SpacTor-T5&#65292;&#32467;&#21512;&#20102;&#36328;&#24230;&#30772;&#22351;&#21644;&#26367;&#25442;&#35789;&#27719;&#26816;&#27979;&#30340;&#28151;&#21512;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#37319;&#29992;&#20004;&#38454;&#27573;&#35838;&#31243;&#34920;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SpacTor-T5&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934;SC&#39044;&#35757;&#32451;&#30456;&#21516;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#39044;&#35757;&#32451;&#36845;&#20195;&#27425;&#25968;&#21644;&#24635;FLOP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#26497;&#20854;&#32791;&#36153;&#36164;&#28304;&#19988;&#32463;&#24120;&#20302;&#25928;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#35757;&#32451;&#25991;&#26412;&#24207;&#21015;&#20013;&#25152;&#34164;&#21547;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SpacTor&#65292;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21253;&#25324;(1)&#23558;&#36328;&#24230;&#30772;&#22351;(SC)&#21644;&#26367;&#25442;&#35789;&#27719;&#26816;&#27979;(RTD)&#32467;&#21512;&#30340;&#28151;&#21512;&#30446;&#26631;&#20989;&#25968;&#65292;&#21644;(2)&#19968;&#20010;&#20004;&#38454;&#27573;&#35838;&#31243;&#34920;&#65292;&#39318;&#20808;&#22312;&#21021;&#22987;&#30340;$\tau$&#36845;&#20195;&#20013;&#20248;&#21270;&#28151;&#21512;&#30446;&#26631;&#20989;&#25968;&#65292;&#28982;&#21518;&#36807;&#28193;&#21040;&#26631;&#20934;&#30340;SC&#25439;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#28151;&#21512;&#30446;&#26631;&#20989;&#25968;&#30340;&#26377;&#25928;&#24615;&#19982;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#36827;&#24230;&#34920;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#23545;&#27492;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#23545;&#21508;&#31181;NLP&#20219;&#21153;&#36827;&#34892;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;(T5)&#30340;&#23454;&#39564;&#20013;&#65292;SpacTor-T5&#19982;&#26631;&#20934;&#30340;SC&#39044;&#35757;&#32451;&#20855;&#26377;&#30456;&#21516;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#39044;&#35757;&#32451;&#36845;&#20195;&#20943;&#23569;50%&#65292;&#24635;FLOP&#20943;&#23569;40%&#12290;&#21478;&#22806;&#65292;&#22312;&#32473;&#23450;&#30456;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;SpacTor&#20855;&#26377;&#27604;&#26631;&#20934;SC&#39044;&#35757;&#32451;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training large language models is known to be extremely resource intensive and often times inefficient, under-utilizing the information encapsulated in the training text sequences. In this paper, we present SpacTor, a new training procedure consisting of (1) a hybrid objective combining span corruption (SC) and token replacement detection (RTD), and (2) a two-stage curriculum that optimizes the hybrid objective over the initial $\tau$ iterations, then transitions to standard SC loss. We show empirically that the effectiveness of the hybrid objective is tied to the two-stage pre-training schedule, and provide extensive analysis on why this is the case. In our experiments with encoder-decoder architectures (T5) on a variety of NLP tasks, SpacTor-T5 yields the same downstream performance as standard SC pre-training, while enabling a 50% reduction in pre-training iterations and 40% reduction in total FLOPs. Alternatively, given the same amount of computing budget, we find that SpacTor 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24863;&#30693;&#30693;&#35782;&#34920;&#31034;&#26426;&#21046;&#65292;&#32858;&#28966;&#20110;&#22810;&#32500;&#25299;&#25169;&#20449;&#24687;&#21644;&#38544;&#21547;&#30340;&#26102;&#38388;&#30456;&#20851;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21333;&#21442;&#25968;&#25299;&#25169;&#25688;&#35201;&#29983;&#25104;&#25968;&#25454;&#30340;&#22810;&#32500;&#25299;&#25169;&#25351;&#32441;&#12290;</title><link>http://arxiv.org/abs/2401.13157</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#32500;&#25345;&#20037;&#24615;&#30340;&#21160;&#24577;&#23545;&#35937;&#30340;&#26102;&#38388;&#24863;&#30693;&#30693;&#35782;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Time-Aware Knowledge Representations of Dynamic Objects with Multidimensional Persistence. (arXiv:2401.13157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24863;&#30693;&#30693;&#35782;&#34920;&#31034;&#26426;&#21046;&#65292;&#32858;&#28966;&#20110;&#22810;&#32500;&#25299;&#25169;&#20449;&#24687;&#21644;&#38544;&#21547;&#30340;&#26102;&#38388;&#30456;&#20851;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21333;&#21442;&#25968;&#25299;&#25169;&#25688;&#35201;&#29983;&#25104;&#25968;&#25454;&#30340;&#22810;&#32500;&#25299;&#25169;&#25351;&#32441;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#21644;&#21160;&#24577;&#32593;&#32476;&#31561;&#26102;&#38388;&#21464;&#21270;&#30340;&#23545;&#35937;&#38656;&#35201;&#24320;&#21457;&#26032;&#30340;&#30693;&#35782;&#34920;&#31034;&#26426;&#21046;&#21644;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#38544;&#21547;&#30340;&#26102;&#38388;&#30456;&#20851;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#36890;&#24120;&#19981;&#30452;&#25509;&#35266;&#23519;&#21040;&#65292;&#20294;&#22312;&#23398;&#20064;&#20219;&#21153;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26102;&#38388;&#30456;&#20851;&#25968;&#25454;&#32780;&#35328;&#65292;&#30693;&#35782;&#32534;&#30721;&#26426;&#21046;&#20013;&#32570;&#23569;&#26102;&#38388;&#32500;&#24230;&#20250;&#23548;&#33268;&#39057;&#32321;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#23398;&#20064;&#24615;&#33021;&#24046;&#65292;&#20174;&#32780;&#24433;&#21709;&#20915;&#31574;&#30340;&#36136;&#37327;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24863;&#30693;&#30693;&#35782;&#34920;&#31034;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#22810;&#20010;&#20960;&#20309;&#32500;&#24230;&#19978;&#30340;&#38544;&#21547;&#26102;&#38388;&#30456;&#20851;&#25299;&#25169;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; \textit{Temporal MultiPersistence} (TMP) &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29616;&#26377;&#30340;&#21333;&#21442;&#25968;&#25299;&#25169;&#25688;&#35201;&#65292;&#29983;&#25104;&#25968;&#25454;&#30340;&#22810;&#32500;&#25299;&#25169;&#25351;&#32441;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning time-evolving objects such as multivariate time series and dynamic networks requires the development of novel knowledge representation mechanisms and neural network architectures, which allow for capturing implicit time-dependent information contained in the data. Such information is typically not directly observed but plays a key role in the learning task performance. In turn, lack of time dimension in knowledge encoding mechanisms for time-dependent data leads to frequent model updates, poor learning performance, and, as a result, subpar decision-making. Here we propose a new approach to a time-aware knowledge representation mechanism that notably focuses on implicit time-dependent topological information along multiple geometric dimensions. In particular, we propose a new approach, named \textit{Temporal MultiPersistence} (TMP), which produces multidimensional topological fingerprints of the data by using the existing single parameter topological summaries. The main idea be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;NLBAC&#26694;&#26550;&#65292;&#29992;&#20110;&#31283;&#23450;&#21644;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#26469;&#36817;&#20284;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#24182;&#23558;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBF&#65289;&#21644;&#25511;&#21046;Lyapunov&#20989;&#25968;&#65288;CLF&#65289;&#26694;&#26550;&#19982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#38598;&#25104;&#65292;&#20197;&#32500;&#25345;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13148</link><description>&lt;p&gt;
NLBAC: &#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#31283;&#23450;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
NLBAC: A Neural Ordinary Differential Equations-based Framework for Stable and Safe Reinforcement Learning. (arXiv:2401.13148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;NLBAC&#26694;&#26550;&#65292;&#29992;&#20110;&#31283;&#23450;&#21644;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#26469;&#36817;&#20284;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#24182;&#23558;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBF&#65289;&#21644;&#25511;&#21046;Lyapunov&#20989;&#25968;&#65288;CLF&#65289;&#26694;&#26550;&#19982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#38598;&#25104;&#65292;&#20197;&#32500;&#25345;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#35270;&#39057;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#26102;&#65292;&#30830;&#20445;&#23433;&#20840;&#21644;&#31283;&#23450;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#39318;&#20808;&#32473;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#31283;&#23450;&#24615;&#23450;&#20041;&#65292;&#28982;&#21518;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;Lyapunov&#23631;&#38556;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;NLBAC&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#26469;&#36817;&#20284;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#24182;&#23558;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBF&#65289;&#21644;&#25511;&#21046;Lyapunov&#20989;&#25968;&#65288;CLF&#65289;&#26694;&#26550;&#19982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#38598;&#25104;&#65292;&#20197;&#24110;&#21161;&#32500;&#25345;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#22686;&#24191;Lagrangian&#26041;&#27861;&#26469;&#26356;&#26032;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#22312;CBF&#32422;&#26463;&#29992;&#20110;&#23433;&#20840;&#24615;&#21644;CLF&#32422;&#26463;&#29992;&#20110;&#31283;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#22791;&#20221;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) excels in applications such as video games and robotics, but ensuring safety and stability remains challenging when using RL to control real-world systems where using model-free algorithms suffering from low sample efficiency might be prohibitive. This paper first provides safety and stability definitions for the RL system, and then introduces a Neural ordinary differential equations-based Lyapunov-Barrier Actor-Critic (NLBAC) framework that leverages Neural Ordinary Differential Equations (NODEs) to approximate system dynamics and integrates the Control Barrier Function (CBF) and Control Lyapunov Function (CLF) frameworks with the actor-critic method to assist in maintaining the safety and stability for the system. Within this framework, we employ the augmented Lagrangian method to update the RL-based controller parameters. Additionally, we introduce an extra backup controller in situations where CBF constraints for safety and the CLF constraint for stabili
&lt;/p&gt;</description></item><item><title>&#25910;&#32553;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;CDPMs&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#25910;&#32553;&#21518;&#21521;&#37319;&#26679;&#24182;&#20811;&#26381;&#20998;&#25968;&#21305;&#37197;&#35823;&#24046;&#21644;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#25910;&#32553;&#23376;&#26041;&#24046;&#20445;&#25345;&#65288;sub-VP&#65289;&#26159;&#34920;&#29616;&#26368;&#20339;&#30340;&#19968;&#31181;CDPMs&#12290;</title><link>http://arxiv.org/abs/2401.13115</link><description>&lt;p&gt;
&#25910;&#32553;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Contractive Diffusion Probabilistic Models. (arXiv:2401.13115v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13115
&lt;/p&gt;
&lt;p&gt;
&#25910;&#32553;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;CDPMs&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#25910;&#32553;&#21518;&#21521;&#37319;&#26679;&#24182;&#20811;&#26381;&#20998;&#25968;&#21305;&#37197;&#35823;&#24046;&#21644;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#25910;&#32553;&#23376;&#26041;&#24046;&#20445;&#25345;&#65288;sub-VP&#65289;&#26159;&#34920;&#29616;&#26368;&#20339;&#30340;&#19968;&#31181;CDPMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#32553;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#24314;&#27169;&#20013;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;DPMs&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#20004;&#20010;&#35201;&#32032;&#65306;&#39532;&#23572;&#31185;&#22827;&#25193;&#25955;&#36807;&#31243;&#30340;&#26102;&#38388;&#21453;&#28436;&#21644;&#20998;&#25968;&#21305;&#37197;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#38544;&#21547;&#22320;&#20551;&#35774;&#20998;&#25968;&#21305;&#37197;&#26159;&#25509;&#36817;&#23436;&#32654;&#30340;&#65292;&#32780;&#36825;&#20010;&#20551;&#35774;&#26159;&#20540;&#24471;&#24576;&#30097;&#30340;&#12290;&#37492;&#20110;&#21487;&#33021;&#26080;&#27861;&#20445;&#35777;&#30340;&#20998;&#25968;&#21305;&#37197;&#65292;&#25105;&#20204;&#22312;DPMs&#30340;&#35774;&#35745;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20934;&#21017;&#8212;&#8212;&#25910;&#32553;&#21518;&#21521;&#37319;&#26679;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#32553;DPMs&#65288;CDPMs&#65289;&#31867;&#65292;&#21253;&#25324;&#25910;&#32553;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#65288;OU&#65289;&#36807;&#31243;&#21644;&#25910;&#32553;&#23376;&#26041;&#24046;&#20445;&#25345;&#65288;sub-VP&#65289;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#12290;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#21518;&#21521;&#36807;&#31243;&#30340;&#25910;&#32553;&#33021;&#22815;&#32553;&#23567;&#20998;&#25968;&#21305;&#37197;&#35823;&#24046;&#21644;&#31163;&#25955;&#21270;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;CDPMs&#23545;&#20110;&#36825;&#20004;&#31181;&#35823;&#24046;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#24471;&#21040;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#25903;&#25345;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25910;&#32553;&#23376;&#26041;&#24046;&#20445;&#25345;&#22312;&#34920;&#29616;&#19978;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models (DPMs) have emerged as a promising technology in generative modeling. The success of DPMs relies on two ingredients: time reversal of Markov diffusion processes and score matching. Most existing work implicitly assumes that score matching is close to perfect, while this assumption is questionable. In view of possibly unguaranteed score matching, we propose a new criterion -- the contraction of backward sampling in the design of DPMs. This leads to a novel class of contractive DPMs (CDPMs), including contractive Ornstein-Uhlenbeck (OU) processes and contractive sub-variance preserving (sub-VP) stochastic differential equations (SDEs). The key insight is that the contraction in the backward process narrows score matching errors, as well as discretization error. Thus, the proposed CDPMs are robust to both sources of error. Our proposal is supported by theoretical results, and is corroborated by experiments. Notably, contractive sub-VP shows the best performa
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#31995;&#32479;&#21464;&#37327;&#21644;&#20989;&#25968;&#24211;&#19981;&#30830;&#23450;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;SINDy&#31639;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#31232;&#30095;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2401.13099</link><description>&lt;p&gt;
&#22312;&#24211;&#21644;&#31995;&#32479;&#19981;&#30830;&#23450;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#31232;&#30095;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Sparse identification of nonlinear dynamics in the presence of library and system uncertainty. (arXiv:2401.13099v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13099
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#31995;&#32479;&#21464;&#37327;&#21644;&#20989;&#25968;&#24211;&#19981;&#30830;&#23450;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;SINDy&#31639;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#31232;&#30095;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SINDy&#31639;&#27861;&#24050;&#25104;&#21151;&#22320;&#29992;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#35782;&#21035;&#21160;&#21147;&#31995;&#32479;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#28982;&#32780;&#65292;SINDy&#20551;&#35774;&#29992;&#25143;&#23545;&#31995;&#32479;&#20013;&#30340;&#21464;&#37327;&#21644;&#33021;&#22815;&#20316;&#20026;&#31995;&#32479;&#22522;&#30784;&#30340;&#20989;&#25968;&#24211;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#28436;&#31034;&#20102;&#22686;&#24378;SINDy&#31639;&#27861;&#22312;&#31995;&#32479;&#21464;&#37327;&#19981;&#30830;&#23450;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;SINDy&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#21516;&#26102;&#23384;&#22312;&#26102;&#65292;SINDy&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20197;&#23454;&#29616;&#31283;&#20581;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The SINDy algorithm has been successfully used to identify the governing equations of dynamical systems from time series data. However, SINDy assumes the user has prior knowledge of the variables in the system and of a function library that can act as a basis for the system. In this paper, we demonstrate on real world data how the Augmented SINDy algorithm outperforms SINDy in the presence of system variable uncertainty. We then show SINDy can be further augmented to perform robustly when both kinds of uncertainty are present.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#31561;&#22240;&#32032;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#27969;&#37327;&#65292;&#24182;&#29992;&#20110;&#25351;&#23548;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20013;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.13098</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#21147;&#20449;&#24687;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#39044;&#27979;&#38750;&#26412;&#22320;&#29289;&#31181;&#33337;&#33334;&#20132;&#36890;&#27969;&#37327;&#21644;&#20837;&#20405;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Gravity-Informed Deep Learning Framework for Predicting Ship Traffic Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge. (arXiv:2401.13098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13098
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#31561;&#22240;&#32032;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#27969;&#37327;&#65292;&#24182;&#29992;&#20110;&#25351;&#23548;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20013;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#20307;&#20013;&#30340;&#20837;&#20405;&#29289;&#31181;&#23545;&#20840;&#29699;&#29615;&#22659;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#30001;&#20110;&#20132;&#36890;&#21644;&#36152;&#26131;&#22686;&#21152;&#65292;&#38750;&#26412;&#22303;&#29289;&#31181;&#24050;&#32463;&#24341;&#20837;&#20102;&#26032;&#30340;&#29615;&#22659;&#65292;&#23548;&#33268;&#29983;&#24577;&#31995;&#32479;&#30772;&#22351;&#65292;&#24182;&#23548;&#33268;&#20892;&#19994;&#12289;&#26519;&#19994;&#21644;&#28180;&#19994;&#26041;&#38754;&#30340;&#32463;&#27982;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#25216;&#26415;&#20197;&#20943;&#36731;&#36825;&#20123;&#20837;&#20405;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#20132;&#36890;&#27969;&#37327;&#65292;&#24182;&#20197;&#27492;&#25351;&#23548;&#36890;&#36807;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20256;&#25773;&#30340;&#20837;&#20405;&#29289;&#31181;&#39118;&#38505;&#35780;&#20272;&#12290;&#21463;&#22269;&#38469;&#36152;&#26131;&#37325;&#21147;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#24433;&#21709;&#33337;&#33334;&#27963;&#21160;&#21487;&#33021;&#24615;&#21644;&#24433;&#21709;&#30340;&#21508;&#31181;&#22240;&#32032;&#65292;&#22914;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#20043;&#38388;&#30340;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20998;&#26512;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#32593;&#32476;&#65292;&#25105;&#20204;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#20837;&#20405;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invasive species in water bodies pose a major threat to the environment and biodiversity globally. Due to increased transportation and trade, non-native species have been introduced to new environments, causing damage to ecosystems and leading to economic losses in agriculture, forestry, and fisheries. Therefore, there is a pressing need for risk assessment and management techniques to mitigate the impact of these invasions. This study aims to develop a new physics-inspired model to forecast maritime shipping traffic and thus inform risk assessment of invasive species spread through global transportation networks. Inspired by the gravity model for international trades, our model considers various factors that influence the likelihood and impact of vessel activities, such as shipping flux density, distance between ports, trade flow, and centrality measures of transportation hubs. Additionally, by analyzing the risk network of invasive species, we provide a comprehensive framework for as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27010;&#29575;&#38656;&#27714;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#26377;&#30340;DeepAR&#27169;&#22411;&#20013;&#38598;&#25104;&#20102;GNN&#32534;&#30721;&#22120;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#25991;&#31456;&#23646;&#24615;&#30456;&#20284;&#24615;&#26500;&#24314;&#22270;&#30340;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13096</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27010;&#29575;&#38656;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Demand Forecasting with Graph Neural Networks. (arXiv:2401.13096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27010;&#29575;&#38656;&#27714;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#26377;&#30340;DeepAR&#27169;&#22411;&#20013;&#38598;&#25104;&#20102;GNN&#32534;&#30721;&#22120;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#25991;&#31456;&#23646;&#24615;&#30456;&#20284;&#24615;&#26500;&#24314;&#22270;&#30340;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#27714;&#39044;&#27979;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#21830;&#19994;&#24212;&#29992;&#26696;&#20363;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#38646;&#21806;&#21830;&#20248;&#21270;&#24211;&#23384;&#35268;&#21010;&#12289;&#29289;&#27969;&#21644;&#26680;&#24515;&#19994;&#21153;&#20915;&#31574;&#12290;&#38656;&#27714;&#39044;&#27979;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#32771;&#34385;&#25991;&#31456;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#20114;&#21160;&#12290;&#22823;&#22810;&#25968;&#29616;&#20195;&#39044;&#27979;&#26041;&#27861;&#25552;&#20379;&#29420;&#31435;&#30340;&#25991;&#31456;&#32423;&#39044;&#27979;&#65292;&#19981;&#32771;&#34385;&#30456;&#20851;&#25991;&#31456;&#30340;&#24433;&#21709;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#22312;&#20043;&#21069;&#30340;GNNs&#30740;&#31350;&#22522;&#30784;&#19978;&#20570;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;GNN&#32534;&#30721;&#22120;&#38598;&#25104;&#21040;&#26368;&#20808;&#36827;&#30340;DeepAR&#27169;&#22411;&#20013;&#12290;&#36825;&#20010;&#32452;&#21512;&#27169;&#22411;&#20135;&#29983;&#27010;&#29575;&#39044;&#27979;&#65292;&#36825;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25991;&#31456;&#23646;&#24615;&#30456;&#20284;&#24615;&#26500;&#24314;&#22270;&#65292;&#36991;&#20813;&#20381;&#36182;&#39044;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#12290;&#23545;&#19977;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demand forecasting is a prominent business use case that allows retailers to optimize inventory planning, logistics, and core business decisions. One of the key challenges in demand forecasting is accounting for relationships and interactions between articles. Most modern forecasting approaches provide independent article-level predictions that do not consider the impact of related articles. Recent research has attempted addressing this challenge using Graph Neural Networks (GNNs) and showed promising results. This paper builds on previous research on GNNs and makes two contributions. First, we integrate a GNN encoder into a state-of-the-art DeepAR model. The combined model produces probabilistic forecasts, which are crucial for decision-making under uncertainty. Second, we propose to build graphs using article attribute similarity, which avoids reliance on a pre-defined graph structure. Experiments on three real-world datasets show that the proposed approach consistently outperforms n
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#36136;&#37327;&#38382;&#39064;&#65292;&#21457;&#29616;&#26631;&#35760;&#21270;&#19981;&#21487;&#38752;&#12289;&#20559;&#35265;&#20197;&#21450;&#20449;&#24687;&#36136;&#37327;&#19979;&#38477;&#21487;&#33021;&#23548;&#33268;&#24187;&#35273;&#12289;&#25423;&#36896;&#20449;&#24687;&#65292;&#20174;&#32780;&#23545;&#20225;&#19994;&#20915;&#31574;&#20135;&#29983;&#38169;&#35823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13086</link><description>&lt;p&gt;
&#21521;&#21487;&#20449;&#36182;&#30340;&#35821;&#35328;&#27169;&#22411;&#36808;&#36827;&#65306;&#25506;&#31350;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Towards Trustable Language Models: Investigating Information Quality of Large Language Models. (arXiv:2401.13086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13086
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#36136;&#37327;&#38382;&#39064;&#65292;&#21457;&#29616;&#26631;&#35760;&#21270;&#19981;&#21487;&#38752;&#12289;&#20559;&#35265;&#20197;&#21450;&#20449;&#24687;&#36136;&#37327;&#19979;&#38477;&#21487;&#33021;&#23548;&#33268;&#24187;&#35273;&#12289;&#25423;&#36896;&#20449;&#24687;&#65292;&#20174;&#32780;&#23545;&#20225;&#19994;&#20915;&#31574;&#20135;&#29983;&#38169;&#35823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#36805;&#36895;&#29983;&#25104;&#22823;&#37327;&#20449;&#24687;&#65292;&#29992;&#25143;&#36234;&#26469;&#36234;&#20381;&#36182;&#21644;&#20449;&#20219;&#36825;&#20123;&#25968;&#25454;&#12290;&#23613;&#31649;LLM&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#20449;&#24687;&#36136;&#37327;&#30340;&#25361;&#25112;&#65292;LLM&#29983;&#25104;&#30340;&#20449;&#24687;&#24182;&#19981;&#23436;&#20840;&#21487;&#20449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLM&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26631;&#35760;&#21270;&#19981;&#21487;&#38752;&#12289;&#23384;&#22312;&#20559;&#35265;&#65292;&#23548;&#33268;&#20449;&#24687;&#36136;&#37327;&#30340;&#23436;&#25972;&#24615;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20449;&#24687;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;LLM&#21487;&#33021;&#20250;&#20135;&#29983;&#24187;&#35273;&#12289;&#25423;&#36896;&#20449;&#24687;&#12290;&#19981;&#21487;&#38752;&#30340;&#20449;&#24687;&#21487;&#33021;&#23548;&#33268;&#20225;&#19994;&#20570;&#20986;&#38169;&#35823;&#20915;&#31574;&#65292;&#24433;&#21709;&#32463;&#27982;&#27963;&#21160;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLM&#30340;&#26032;&#39062;&#25968;&#23398;&#20449;&#24687;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#20998;&#26512;&#21644;&#31361;&#20986;&#20102;&#20449;&#24687;&#36136;&#37327;&#25361;&#25112;&#65292;&#20197;&#31995;&#32479;&#22320;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) are generating information at a rapid pace, requiring users to increasingly rely and trust the data. Despite remarkable advances of LLM, Information generated by LLM is not completely trustworthy, due to challenges in information quality. Specifically, integrity of Information quality decreases due to unreliable, biased, tokenization during pre-training of LLM. Moreover, due to decreased information quality issues, has led towards hallucination, fabricated information. Unreliable information can lead towards flawed decisions in businesses, which impacts economic activity. In this work, we introduce novel mathematical information quality evaluation of LLM, we furthermore analyze and highlight information quality challenges, scaling laws to systematically scale language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#21360;&#24230;&#35821;&#35328;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;Easy Data Augmentation&#12289;Back Translation&#12289;Paraphrasing&#12289;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20197;&#21450;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#25193;&#23637;&#31561;&#25216;&#26415;&#12290;&#36890;&#36807;&#20108;&#20803;&#21644;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#23454;&#39564;&#65292;&#30740;&#31350;&#21457;&#29616;&#22522;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13085</link><description>&lt;p&gt;
IndiText Boost: &#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#21360;&#24230;&#35821;&#35328;&#30340;&#25991;&#26412;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
IndiText Boost: Text Augmentation for Low Resource India Languages. (arXiv:2401.13085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#21360;&#24230;&#35821;&#35328;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;Easy Data Augmentation&#12289;Back Translation&#12289;Paraphrasing&#12289;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20197;&#21450;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#25193;&#23637;&#31561;&#25216;&#26415;&#12290;&#36890;&#36807;&#20108;&#20803;&#21644;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#23454;&#39564;&#65292;&#30740;&#31350;&#21457;&#29616;&#22522;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22686;&#24378;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23427;&#26377;&#21161;&#20110;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#24212;&#23545;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#22810;&#24180;&#26469;&#65292;&#24050;&#32463;&#23545;&#33521;&#35821;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#22686;&#24378;&#24037;&#20316;&#65292;&#32780;&#22312;&#21360;&#24230;&#35821;&#35328;&#26041;&#38754;&#21364;&#20570;&#24471;&#24456;&#23569;&#12290;&#36825;&#19982;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#22788;&#29702;&#25968;&#25454;&#31232;&#32570;&#30340;&#20107;&#23454;&#30456;&#21453;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#23454;&#26045;Easy Data Augmentation&#12289;Back Translation&#12289;Paraphrasing&#12289;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20197;&#21450;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#25193;&#23637;&#31561;&#25216;&#26415;&#65292;&#29992;&#20110;&#19981;&#21516;&#35821;&#35328;&#30340;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;6&#31181;&#21360;&#24230;&#35821;&#35328;&#65306;&#20449;&#24503;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21476;&#21513;&#25289;&#29305;&#35821;&#12289;&#27888;&#21346;&#22266;&#35821;&#21644;&#26805;&#35821;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#31867;&#20284;&#30340;&#24037;&#20316;&#29992;&#20110;&#21360;&#24230;&#35821;&#35328;&#30340;&#25991;&#26412;&#22686;&#24378;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20108;&#20803;&#21644;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#65292;&#20197;&#20351;&#25105;&#20204;&#30340;&#32467;&#26524;&#26356;&#20855;&#21487;&#27604;&#24615;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#22522;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text Augmentation is an important task for low-resource languages. It helps deal with the problem of data scarcity. A data augmentation strategy is used to deal with the problem of data scarcity. Through the years, much work has been done on data augmentation for the English language. In contrast, very less work has been done on Indian languages. This is contrary to the fact that data augmentation is used to deal with data scarcity. In this work, we focus on implementing techniques like Easy Data Augmentation, Back Translation, Paraphrasing, Text Generation using LLMs, and Text Expansion using LLMs for text classification on different languages. We focus on 6 Indian languages namely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to our knowledge, no such work exists for text augmentation on Indian languages. We carry out binary as well as multi-class text classification to make our results more comparable. We get surprising results as basic data augmentation techniq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#35745;&#31639;&#36229;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#24182;&#36827;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36229;&#22270;&#20013;&#33410;&#28857;&#36317;&#31163;&#35745;&#31639;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25299;&#23637;&#20102;&#36229;&#22270;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2401.13054</link><description>&lt;p&gt;
&#26080;&#35745;&#31639;&#22256;&#38590;&#30340;&#24555;&#36895;&#35745;&#31639;&#36229;&#22270;&#33410;&#28857;&#36317;&#31163;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Frustrated Random Walks: A Fast Method to Compute Node Distances on Hypergraphs. (arXiv:2401.13054v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#35745;&#31639;&#36229;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#24182;&#36827;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36229;&#22270;&#20013;&#33410;&#28857;&#36317;&#31163;&#35745;&#31639;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25299;&#23637;&#20102;&#36229;&#22270;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#22270;&#30340;&#25512;&#24191;&#65292;&#24403;&#32771;&#34385;&#23454;&#20307;&#38388;&#30340;&#23646;&#24615;&#20849;&#20139;&#26102;&#20250;&#33258;&#28982;&#20135;&#29983;&#12290;&#23613;&#31649;&#21487;&#20197;&#36890;&#36807;&#23558;&#36229;&#36793;&#25193;&#23637;&#20026;&#23436;&#20840;&#36830;&#25509;&#30340;&#23376;&#22270;&#26469;&#23558;&#36229;&#22270;&#36716;&#25442;&#20026;&#22270;&#65292;&#20294;&#36870;&#21521;&#25805;&#20316;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#22797;&#26434;&#19988;&#23646;&#20110;NP-complete&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;&#36229;&#22270;&#21253;&#21547;&#27604;&#22270;&#26356;&#22810;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#30452;&#25509;&#25805;&#20316;&#36229;&#22270;&#27604;&#23558;&#20854;&#25193;&#23637;&#20026;&#22270;&#26356;&#20026;&#26041;&#20415;&#12290;&#36229;&#22270;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#26159;&#22914;&#20309;&#31934;&#30830;&#39640;&#25928;&#22320;&#35745;&#31639;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#36890;&#36807;&#20272;&#35745;&#33410;&#28857;&#36317;&#31163;&#65292;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#33410;&#28857;&#30340;&#26368;&#36817;&#37051;&#23621;&#65292;&#24182;&#20351;&#29992;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#26041;&#27861;&#22312;&#36229;&#22270;&#19978;&#25191;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#36229;&#22270;&#19978;&#36827;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#25105;&#20204;&#23558;&#33410;&#28857;&#36317;&#31163;&#20272;&#35745;&#20026;&#38543;&#26426;&#28216;&#36208;&#30340;&#39044;&#26399;&#21040;&#36798;&#26102;&#38388;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#31616;&#21333;&#38543;&#26426;&#28216;&#36208;&#65288;SRW&#65289;&#26080;&#27861;&#20934;&#30830;&#25551;&#36848;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#22240;&#27492;&#25105;&#20204;&#24341;&#20837;&#20102;"frustrated"&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
A hypergraph is a generalization of a graph that arises naturally when attribute-sharing among entities is considered. Although a hypergraph can be converted into a graph by expanding its hyperedges into fully connected subgraphs, going the reverse way is computationally complex and NP-complete. We therefore hypothesize that a hypergraph contains more information than a graph. In addition, it is more convenient to manipulate a hypergraph directly, rather than expand it into a graph. An open problem in hypergraphs is how to accurately and efficiently calculate their node distances. Estimating node distances enables us to find a node's nearest neighbors, and perform label propagation on hypergraphs using a K-nearest neighbors (KNN) approach. In this paper, we propose a novel approach based on random walks to achieve label propagation on hypergraphs. We estimate node distances as the expected hitting times of random walks. We note that simple random walks (SRW) cannot accurately describe 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CIS-UNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20027;&#21160;&#33033;&#21644;&#20027;&#21160;&#33033;&#20998;&#25903;&#30340;&#22810;&#31867;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;CNN&#21644;Swin transformers&#30340;&#20248;&#21183;&#65292;&#37319;&#29992;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24179;&#31227;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20027;&#21160;&#33033;&#30340;&#21508;&#20010;&#20998;&#25903;&#12290;</title><link>http://arxiv.org/abs/2401.13049</link><description>&lt;p&gt;
CIS-UNet: &#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24179;&#31227;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#30340;CTA&#20027;&#21160;&#33033;&#22810;&#31867;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
CIS-UNet: Multi-Class Segmentation of the Aorta in Computed Tomography Angiography via Context-Aware Shifted Window Self-Attention. (arXiv:2401.13049v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CIS-UNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20027;&#21160;&#33033;&#21644;&#20027;&#21160;&#33033;&#20998;&#25903;&#30340;&#22810;&#31867;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;CNN&#21644;Swin transformers&#30340;&#20248;&#21183;&#65292;&#37319;&#29992;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24179;&#31227;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20027;&#21160;&#33033;&#30340;&#21508;&#20010;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#21644;&#20869;&#34924;&#34880;&#31649;&#33180;&#25216;&#26415;&#30340;&#36827;&#27493;&#20419;&#36827;&#20102;&#20027;&#21160;&#33033;&#30142;&#30149;&#30340;&#24494;&#21019;&#27835;&#30103;&#12290;&#20934;&#30830;&#22320;&#23545;&#20027;&#21160;&#33033;&#21450;&#20854;&#20998;&#25903;&#36827;&#34892;3D&#20998;&#21106;&#23545;&#20110;&#24178;&#39044;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#19981;&#20934;&#30830;&#30340;&#20998;&#21106;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#25163;&#26415;&#35268;&#21010;&#21644;&#20869;&#37096;&#25903;&#26550;&#26500;&#36896;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#23558;&#20027;&#21160;&#33033;&#20998;&#21106;&#31616;&#21270;&#20026;&#20108;&#20540;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#65292;&#24573;&#35270;&#20102;&#21306;&#20998;&#21508;&#20010;&#20027;&#21160;&#33033;&#20998;&#25903;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Context Infused Swin-UNet&#65288;CIS-UNet&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20027;&#21160;&#33033;&#21644;&#21313;&#19977;&#20010;&#20027;&#21160;&#33033;&#20998;&#25903;&#30340;&#22810;&#31867;&#20998;&#21106;&#12290;CIS-UNet&#32467;&#21512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#21644;Swin transformers&#30340;&#20248;&#21183;&#65292;&#37319;&#29992;&#20102;&#23618;&#27425;&#21270;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#21253;&#25324;CNN&#32534;&#30721;&#22120;&#12289;&#23545;&#31216;&#35299;&#30721;&#22120;&#12289;&#36339;&#36291;&#36830;&#25509;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24179;&#31227;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65288;CSW-SA&#65289;&#20316;&#20026;&#29942;&#39048;&#27169;&#22359;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CSW-SA&#24341;&#20837;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#22270;&#20687;&#20013;&#30340;&#34917;&#19969;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in medical imaging and endovascular grafting have facilitated minimally invasive treatments for aortic diseases. Accurate 3D segmentation of the aorta and its branches is crucial for interventions, as inaccurate segmentation can lead to erroneous surgical planning and endograft construction. Previous methods simplified aortic segmentation as a binary image segmentation problem, overlooking the necessity of distinguishing between individual aortic branches. In this paper, we introduce Context Infused Swin-UNet (CIS-UNet), a deep learning model designed for multi-class segmentation of the aorta and thirteen aortic branches. Combining the strengths of Convolutional Neural Networks (CNNs) and Swin transformers, CIS-UNet adopts a hierarchical encoder-decoder structure comprising a CNN encoder, symmetric decoder, skip connections, and a novel Context-aware Shifted Window Self-Attention (CSW-SA) as the bottleneck block. Notably, CSW-SA introduces a unique utilization of the patch
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#31070;&#32463;&#20449;&#24687;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#35780;&#20272;&#22899;&#24615;&#36816;&#21160;&#21592;&#33041;&#38663;&#33633;&#30340;&#26041;&#27861;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#20020;&#24202;&#26041;&#27861;&#65292;&#22312;&#22899;&#24615;&#36816;&#21160;&#21592;&#20013;&#35786;&#26029;&#33041;&#38663;&#33633;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#32780;&#36825;&#20123;&#26032;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#20998;&#26512;&#25214;&#20986;&#19982;&#24615;&#21035;&#30456;&#20851;&#30340;&#29983;&#29289;&#26426;&#21046;&#65292;&#20174;&#32780;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.13045</link><description>&lt;p&gt;
&#35780;&#20272;&#22899;&#24615;&#36816;&#21160;&#21592;&#33041;&#38663;&#33633;&#65306;&#31070;&#32463;&#20449;&#24687;&#23398;&#30340;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Assessment of Sports Concussion in Female Athletes: A Role for Neuroinformatics?. (arXiv:2401.13045v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13045
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#31070;&#32463;&#20449;&#24687;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#35780;&#20272;&#22899;&#24615;&#36816;&#21160;&#21592;&#33041;&#38663;&#33633;&#30340;&#26041;&#27861;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#20020;&#24202;&#26041;&#27861;&#65292;&#22312;&#22899;&#24615;&#36816;&#21160;&#21592;&#20013;&#35786;&#26029;&#33041;&#38663;&#33633;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#32780;&#36825;&#20123;&#26032;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#20998;&#26512;&#25214;&#20986;&#19982;&#24615;&#21035;&#30456;&#20851;&#30340;&#29983;&#29289;&#26426;&#21046;&#65292;&#20174;&#32780;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#22899;&#24615;&#36816;&#21160;&#21592;&#33041;&#38663;&#33633;&#30340;&#22797;&#26434;&#24615;&#21464;&#24471;&#26126;&#26174;&#12290;&#20256;&#32479;&#30340;&#20020;&#24202;&#35786;&#26029;&#33041;&#38663;&#33633;&#30340;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#22899;&#24615;&#36816;&#21160;&#21592;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#21040;&#33041;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#32454;&#24494;&#21464;&#21270;&#12290;&#20808;&#36827;&#30340;&#31070;&#32463;&#20449;&#24687;&#23398;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#24050;&#32463;&#25104;&#20026;&#23453;&#36149;&#30340;&#36164;&#20135;&#12290;&#34429;&#28982;&#36825;&#20123;&#25216;&#26415;&#22312;&#29702;&#35299;&#30007;&#24615;&#36816;&#21160;&#21592;&#30340;&#33041;&#38663;&#33633;&#26041;&#38754;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#22312;&#25105;&#20204;&#23545;&#20110;&#23427;&#20204;&#23545;&#22899;&#24615;&#36816;&#21160;&#21592;&#30340;&#26377;&#25928;&#24615;&#30340;&#29702;&#35299;&#19978;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#24378;&#22823;&#25968;&#25454;&#20998;&#26512;&#33021;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#23558;&#35266;&#23519;&#21040;&#30340;&#34920;&#22411;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#32852;&#31995;&#21040;&#29305;&#23450;&#20110;&#24615;&#21035;&#30340;&#29983;&#29289;&#26426;&#21046;&#65292;&#25581;&#31034;&#22899;&#24615;&#36816;&#21160;&#21592;&#33041;&#38663;&#33633;&#30340;&#22885;&#31192;&#12290;&#27492;&#22806;&#65292;&#23884;&#20837;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#22312;&#30740;&#31350;&#20013;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#65292;&#36827;&#19968;&#27493;&#26816;&#39564;&#24615;&#21035;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, the intricacies of sports-related concussions among female athletes have become readily apparent. Traditional clinical methods for diagnosing concussions suffer limitations when applied to female athletes, often failing to capture subtle changes in brain structure and function. Advanced neuroinformatics techniques and machine learning models have become invaluable assets in this endeavor. While these technologies have been extensively employed in understanding concussion in male athletes, there remains a significant gap in our comprehension of their effectiveness for female athletes. With its remarkable data analysis capacity, machine learning offers a promising avenue to bridge this deficit. By harnessing the power of machine learning, researchers can link observed phenotypic neuroimaging data to sex-specific biological mechanisms, unraveling the mysteries of concussions in female athletes. Furthermore, embedding methods within machine learning enable examining b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#31232;&#30095;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25311;&#21512;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.13034</link><description>&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#30340;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Locality Sensitive Sparse Encoding for Learning World Models Online. (arXiv:2401.13034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#31232;&#30095;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25311;&#21512;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#25968;&#25454;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#65292;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#39640;&#25928;&#30340;&#31232;&#30095;&#26356;&#26032;&#65292;&#22312;&#24179;&#34913;&#27169;&#22411;&#23481;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#20248;&#21270;&#25311;&#21512;&#25152;&#26377;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring an accurate world model online for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Proximal Policy Optimization&#30340;&#26426;&#20250;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#30005;&#21378;&#30417;&#25511;&#30340;&#30417;&#30563;&#25511;&#21046;&#12290;&#36890;&#36807;&#20351;&#29992;Lagrangian relaxation&#65292;&#25105;&#20204;&#23558;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#26080;&#32422;&#26463;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#23454;&#26045;&#29366;&#24577;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2401.13020</link><description>&lt;p&gt;
&#30005;&#21378;&#30417;&#25511;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Safe Reinforcement Learning Algorithm for Supervisory Control of Power Plants. (arXiv:2401.13020v1 [cs.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13020
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Proximal Policy Optimization&#30340;&#26426;&#20250;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#30005;&#21378;&#30417;&#25511;&#30340;&#30417;&#30563;&#25511;&#21046;&#12290;&#36890;&#36807;&#20351;&#29992;Lagrangian relaxation&#65292;&#25105;&#20204;&#23558;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#26080;&#32422;&#26463;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#23454;&#26045;&#29366;&#24577;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25511;&#21046;&#29702;&#35770;&#26041;&#27861;&#38656;&#35201;&#20026;&#27599;&#20010;&#31995;&#32479;&#36827;&#34892;&#23450;&#21046;&#21270;&#24037;&#31243;&#21644;&#19981;&#26029;&#30340;&#24494;&#35843;&#12290;&#22312;&#30005;&#21378;&#25511;&#21046;&#20013;&#65292;&#32463;&#24120;&#38656;&#35201;&#31934;&#30830;&#22320;&#33719;&#21462;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#34920;&#31034;&#24182;&#30456;&#24212;&#22320;&#35774;&#35745;&#25511;&#21046;&#26041;&#26696;&#12290;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30001;&#20110;&#20854;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#35797;&#38169;&#20132;&#20114;&#23398;&#20064;&#32780;&#25104;&#20026;&#25511;&#21046;&#20219;&#21153;&#30340;&#26377;&#24076;&#26395;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#28040;&#38500;&#20102;&#23545;&#26126;&#30830;&#24314;&#27169;&#29615;&#22659;&#21160;&#24577;&#30340;&#38656;&#27714;&#65292;&#36825;&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#30005;&#21378;&#25511;&#21046;&#20013;&#30452;&#25509;&#26045;&#21152;&#29366;&#24577;&#32422;&#26463;&#23545;&#26631;&#20934;RL&#26041;&#27861;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#26426;&#20250;&#32422;&#26463;RL&#31639;&#27861;&#65292;&#29992;&#20110;&#30417;&#25511;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#65292;&#23558;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#26080;&#32422;&#26463;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20854;&#20013;&#21487;&#35757;&#32451;&#30340;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#24378;&#21046;&#25191;&#34892;&#29366;&#24577;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional control theory-based methods require tailored engineering for each system and constant fine-tuning. In power plant control, one often needs to obtain a precise representation of the system dynamics and carefully design the control scheme accordingly. Model-free Reinforcement learning (RL) has emerged as a promising solution for control tasks due to its ability to learn from trial-and-error interactions with the environment. It eliminates the need for explicitly modeling the environment's dynamics, which is potentially inaccurate. However, the direct imposition of state constraints in power plant control raises challenges for standard RL methods. To address this, we propose a chance-constrained RL algorithm based on Proximal Policy Optimization for supervisory control. Our method employs Lagrangian relaxation to convert the constrained optimization problem into an unconstrained objective, where trainable Lagrange multipliers enforce the state constraints. Our approach achiev
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#24490;&#29615;&#27169;&#22411;&#20013;&#21547;&#26377;&#38544;&#34255;&#22240;&#21464;&#37327;&#30340;&#22240;&#26524;&#21457;&#29616;&#65292;&#24050;&#32463;&#20986;&#29616;&#20102;&#33021;&#22815;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#30340;&#22810;&#31181;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13009</link><description>&lt;p&gt;
&#24490;&#29615;&#27169;&#22411;&#20013;&#21547;&#26377;&#38544;&#34255;&#22240;&#21464;&#37327;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Study of Causal Discovery Methods for Cyclic Models with Hidden Confounders. (arXiv:2401.13009v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13009
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24490;&#29615;&#27169;&#22411;&#20013;&#21547;&#26377;&#38544;&#34255;&#22240;&#21464;&#37327;&#30340;&#22240;&#26524;&#21457;&#29616;&#65292;&#24050;&#32463;&#20986;&#29616;&#20102;&#33021;&#22815;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#30340;&#22810;&#31181;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#23545;&#22240;&#26524;&#21457;&#29616;&#30340;&#38656;&#27714;&#26080;&#22788;&#19981;&#22312;&#12290;&#29702;&#35299;&#31995;&#32479;&#20013;&#37096;&#20998;&#20043;&#38388;&#30340;&#38543;&#26426;&#20381;&#36182;&#24615;&#20197;&#21450;&#23454;&#38469;&#30340;&#22240;&#26524;&#20851;&#31995;&#23545;&#31185;&#23398;&#30340;&#21508;&#20010;&#37096;&#20998;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#23547;&#25214;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#22240;&#26524;&#26041;&#21521;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#22312;&#36807;&#21435;&#30340;50&#24180;&#37324;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#20165;&#36866;&#29992;&#20110;&#31995;&#32479;&#27809;&#26377;&#21453;&#39304;&#29615;&#36335;&#24182;&#19988;&#20855;&#26377;&#22240;&#26524;&#20805;&#20998;&#24615;&#30340;&#20551;&#35774;&#65292;&#21363;&#27809;&#26377;&#26410;&#27979;&#37327;&#30340;&#23376;&#31995;&#32479;&#33021;&#22815;&#24433;&#21709;&#22810;&#20010;&#24050;&#27979;&#37327;&#21464;&#37327;&#12290;&#36825;&#26159;&#19981;&#24184;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#38480;&#21046;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#19981;&#33021;&#20551;&#23450;&#12290;&#21453;&#39304;&#26159;&#35768;&#22810;&#36807;&#31243;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24615;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#24456;&#23569;&#26159;&#23436;&#20840;&#38548;&#31163;&#21644;&#23436;&#20840;&#27979;&#37327;&#30340;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#24050;&#32463;&#21457;&#23637;&#20102;&#20960;&#31181;&#33021;&#22815;&#22788;&#29702;&#24490;&#29615;&#30340;&#12289;&#22240;&#26524;&#19981;&#20805;&#20998;&#30340;&#31995;&#32479;&#30340;&#25216;&#26415;&#12290;&#38543;&#30528;&#22810;&#31181;&#26041;&#27861;&#30340;&#20986;&#29616;&#65292;&#19968;&#31181;&#23454;&#38469;&#30340;&#24212;&#29992;&#26041;&#27861;&#24320;&#22987;&#21464;&#24471;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the need for causal discovery is ubiquitous. A better understanding of not just the stochastic dependencies between parts of a system, but also the actual cause-effect relations, is essential for all parts of science. Thus, the need for reliable methods to detect causal directions is growing constantly. In the last 50 years, many causal discovery algorithms have emerged, but most of them are applicable only under the assumption that the systems have no feedback loops and that they are causally sufficient, i.e. that there are no unmeasured subsystems that can affect multiple measured variables. This is unfortunate since those restrictions can often not be presumed in practice. Feedback is an integral feature of many processes, and real-world systems are rarely completely isolated and fully measured. Fortunately, in recent years, several techniques, that can cope with cyclic, causally insufficient systems, have been developed. And with multiple methods available, a practical ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#23545;&#21463;&#38480;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#21487;&#25511;&#30340;&#22270;&#20687;&#25805;&#20316;&#12290;&#36890;&#36807;&#20462;&#25913;&#35821;&#20041;&#22320;&#22270;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#25554;&#20837;&#12289;&#21024;&#38500;&#25110;&#26367;&#25442;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#12290;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20266;&#36896;&#21644;&#22270;&#20687;&#32534;&#36753;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13006</link><description>&lt;p&gt;
CIMGEN: &#21463;&#38480;&#25968;&#25454;&#19978;&#23545;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#21487;&#25511;&#22270;&#20687;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
CIMGEN: Controlled Image Manipulation by Finetuning Pretrained Generative Models on Limited Data. (arXiv:2401.13006v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#23545;&#21463;&#38480;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#21487;&#25511;&#30340;&#22270;&#20687;&#25805;&#20316;&#12290;&#36890;&#36807;&#20462;&#25913;&#35821;&#20041;&#22320;&#22270;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#25554;&#20837;&#12289;&#21024;&#38500;&#25110;&#26367;&#25442;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#12290;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20266;&#36896;&#21644;&#22270;&#20687;&#32534;&#36753;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23481;&#21019;&#20316;&#21644;&#22270;&#20687;&#32534;&#36753;&#21487;&#20197;&#21463;&#30410;&#20110;&#29992;&#25143;&#30340;&#28789;&#27963;&#25511;&#21046;&#12290;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#30340;&#24120;&#35265;&#20013;&#38388;&#34920;&#31034;&#26159;&#35821;&#20041;&#22320;&#22270;&#65292;&#20854;&#20013;&#21253;&#21547;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#23545;&#35937;&#30340;&#20449;&#24687;&#12290;&#19982;&#21407;&#22987;RGB&#20687;&#32032;&#30456;&#27604;&#65292;&#20462;&#25913;&#35821;&#20041;&#22320;&#22270;&#35201;&#23481;&#26131;&#24471;&#22810;&#12290;&#21487;&#20197;&#37319;&#29992;&#35821;&#20041;&#22320;&#22270;&#24182;&#36731;&#26494;&#20462;&#25913;&#22320;&#22270;&#20197;&#36873;&#25321;&#24615;&#22320;&#25554;&#20837;&#12289;&#21024;&#38500;&#25110;&#26367;&#25442;&#22320;&#22270;&#20013;&#30340;&#23545;&#35937;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#25509;&#21463;&#20462;&#25913;&#21518;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#24182;&#26681;&#25454;&#20462;&#25913;&#21518;&#30340;&#22320;&#22270;&#35843;&#25972;&#21407;&#22987;&#22270;&#20687;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20256;&#32479;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#22914;CycleGAN&#25110;Pix2Pix GAN&#65292;&#22312;&#19982;&#35821;&#20041;&#22320;&#22270;&#30456;&#20851;&#30340;&#21442;&#32771;&#22270;&#20687;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#24615;&#33021;&#65292;&#20197;&#35828;&#26126;&#23427;&#22312;&#22270;&#20687;&#20266;&#36896;&#21644;&#22270;&#20687;&#32534;&#36753;&#39046;&#22495;&#30340;&#33021;&#21147;&#21644;&#21487;&#33021;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25552;&#35758;&#30340;&#22270;&#20687;&#20266;&#36896;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Content creation and image editing can benefit from flexible user controls. A common intermediate representation for conditional image generation is a semantic map, that has information of objects present in the image. When compared to raw RGB pixels, the modification of semantic map is much easier. One can take a semantic map and easily modify the map to selectively insert, remove, or replace objects in the map. The method proposed in this paper takes in the modified semantic map and alter the original image in accordance to the modified map. The method leverages traditional pre-trained image-to-image translation GANs, such as CycleGAN or Pix2Pix GAN, that are fine-tuned on a limited dataset of reference images associated with the semantic maps. We discuss the qualitative and quantitative performance of our technique to illustrate its capacity and possible applications in the fields of image forgery and image editing. We also demonstrate the effectiveness of the proposed image forgery
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22270;&#29255;&#29983;&#25104;&#25277;&#35937;&#32918;&#20687;&#32472;&#30011;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21333;&#19968;&#30340;&#25163;&#32472;&#22270;&#26696;&#32032;&#25551;&#21644;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#31508;&#35302;&#21464;&#21270;&#65292;&#21019;&#36896;&#20986;&#39118;&#26684;&#29420;&#29305;&#30340;&#20805;&#28385;&#21916;&#24742;&#30340;&#25277;&#35937;&#32472;&#30011;&#12290;</title><link>http://arxiv.org/abs/2401.13001</link><description>&lt;p&gt;
PatternPortrait&#65306;&#29992;&#20320;&#30340;&#28034;&#40486;&#30011;&#20986;&#25105;&#12290; (arXiv:2401.13001v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
PatternPortrait: Draw Me Like One of Your Scribbles. (arXiv:2401.13001v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22270;&#29255;&#29983;&#25104;&#25277;&#35937;&#32918;&#20687;&#32472;&#30011;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21333;&#19968;&#30340;&#25163;&#32472;&#22270;&#26696;&#32032;&#25551;&#21644;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#31508;&#35302;&#21464;&#21270;&#65292;&#21019;&#36896;&#20986;&#39118;&#26684;&#29420;&#29305;&#30340;&#20805;&#28385;&#21916;&#24742;&#30340;&#25277;&#35937;&#32472;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22270;&#29255;&#29983;&#25104;&#25277;&#35937;&#32918;&#20687;&#32472;&#30011;&#30340;&#26041;&#27861;&#12290;&#20854;&#29420;&#29305;&#30340;&#39118;&#26684;&#26159;&#36890;&#36807;&#21033;&#29992;&#21333;&#19968;&#30340;&#25163;&#32472;&#22270;&#26696;&#32032;&#25551;&#20316;&#20026;&#21442;&#32771;&#26469;&#29983;&#25104;&#29992;&#20110;&#38452;&#24433;&#30340;&#29420;&#29305;&#22270;&#26696;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#38754;&#37096;&#21644;&#36523;&#20307;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#21521;&#37327;&#32447;&#26465;&#12290;&#30740;&#31350;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#24320;&#21457;&#19968;&#31181;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#26088;&#22312;&#23398;&#20064;&#21521;&#37327;&#24418;&#24335;&#30340;&#32032;&#25551;&#31508;&#35302;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#31508;&#35302;&#21464;&#21270;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#32452;&#21512;&#21019;&#36896;&#20102;&#20805;&#28385;&#21916;&#24742;&#30340;&#25277;&#35937;&#32472;&#30011;&#65292;&#36890;&#36807;&#38050;&#31508;&#32472;&#22270;&#20202;&#23454;&#29616;&#12290;&#25152;&#20171;&#32461;&#30340;&#36807;&#31243;&#22312;&#22823;&#32422;280&#21517;&#21442;&#19982;&#32773;&#20013;&#33719;&#24471;&#20102;&#31215;&#26497;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a process for generating abstract portrait drawings from pictures. Their unique style is created by utilizing single freehand pattern sketches as references to generate unique patterns for shading. The method involves extracting facial and body features from images and transforming them into vector lines. A key aspect of the research is the development of a graph neural network architecture designed to learn sketch stroke representations in vector form, enabling the generation of diverse stroke variations. The combination of these two approaches creates joyful abstract drawings that are realized via a pen plotter. The presented process garnered positive feedback from an audience of approximately 280 participants.
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#23376;&#23545;&#25509;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#23376;&#29305;&#24615;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#32534;&#30721;&#30340;&#20998;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#30340;&#26799;&#24230;&#65292;&#25552;&#39640;&#20102;&#30450;&#30446;&#23545;&#25509;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12999</link><description>&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#20998;&#23376;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
Quantum-Inspired Machine Learning for Molecular Docking. (arXiv:2401.12999v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12999
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#23376;&#23545;&#25509;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#23376;&#29305;&#24615;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#32534;&#30721;&#30340;&#20998;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#30340;&#26799;&#24230;&#65292;&#25552;&#39640;&#20102;&#30450;&#30446;&#23545;&#25509;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23545;&#25509;&#26159;&#26500;&#24314;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#21487;&#20197;&#21152;&#24555;&#33647;&#29289;&#24320;&#21457;&#30340;&#25928;&#29575;&#12290;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#21160;&#24577;&#32467;&#21512;&#36807;&#31243;&#38656;&#35201;&#22312;&#24191;&#27867;&#30340;&#31354;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#25628;&#32034;&#21644;&#37319;&#26679;&#12290;&#20256;&#32479;&#30340;&#23545;&#25509;&#26041;&#27861;&#36890;&#36807;&#25628;&#32034;&#21487;&#33021;&#30340;&#32467;&#21512;&#20301;&#28857;&#21644;&#26500;&#35937;&#26469;&#23454;&#29616;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#65292;&#22312;&#30450;&#30446;&#23545;&#25509;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#21463;&#21040;&#36825;&#19968;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#37327;&#23376;&#21551;&#21457;&#31639;&#27861;&#19982;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#22312;&#32534;&#30721;&#30340;&#20998;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#30340;&#26799;&#24230;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#30450;&#30446;&#23545;&#25509;&#20013;&#30340;&#25913;&#36827;&#12290;&#25968;&#20540;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20256;&#32479;&#30340;&#23545;&#25509;&#31639;&#27861;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#19978;&#34920;&#29616;&#20986;&#20102;&#36229;&#36807;10%&#30340;&#25552;&#21319;&#12290;&#19982;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23545;&#25509;&#31639;&#27861;DiffDock&#30456;&#27604;&#65292;Top-1&#65288;RMSD&lt;2&#65289;&#30340;&#25104;&#21151;&#29575;&#20174;33%&#25552;&#39640;&#21040;35%&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular docking is an important tool for structure-based drug design, accelerating the efficiency of drug development. Complex and dynamic binding processes between proteins and small molecules require searching and sampling over a wide spatial range. Traditional docking by searching for possible binding sites and conformations is computationally complex and results poorly under blind docking. Quantum-inspired algorithms combining quantum properties and annealing show great advantages in solving combinatorial optimization problems. Inspired by this, we achieve an improved in blind docking by using quantum-inspired combined with gradients learned by deep learning in the encoded molecular space. Numerical simulation shows that our method outperforms traditional docking algorithms and deep learning-based algorithms over 10\%. Compared to the current state-of-the-art deep learning-based docking algorithm DiffDock, the success rate of Top-1 (RMSD&lt;2) achieves an improvement from 33\% to 35
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32763;&#35793;&#20102;&#30340;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21457;&#29616;&#20102;&#23384;&#22312;&#38382;&#39064;&#30340;&#40486;&#29255;&#20351;&#29992;&#30340;&#36864;&#20237;&#20891;&#20154;&#12290;&#19982;&#20165;&#36890;&#36807;&#35786;&#26029;&#20195;&#30721;&#35782;&#21035;&#30340;&#40486;&#29255;&#20351;&#29992;&#38556;&#30861;&#24739;&#32773;&#30456;&#27604;&#65292;&#36825;&#20123;&#24739;&#32773;&#20855;&#26377;&#19981;&#21516;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.12996</link><description>&lt;p&gt;
&#36890;&#36807;&#20020;&#24202;&#35760;&#24405;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;&#20351;&#29992;&#35786;&#26029;&#20195;&#30721;&#23545;&#27604;&#21457;&#29616;&#23384;&#22312;&#38382;&#39064;&#30340;&#30103;&#25928;&#24615;&#40486;&#29255;&#20351;&#29992;&#30340;&#36864;&#20237;&#20891;&#20154;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Veterans with Problematic Opioid Use Identified through Natural Language Processing of Clinical Notes versus Using Diagnostic Codes. (arXiv:2401.12996v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12996
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32763;&#35793;&#20102;&#30340;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21457;&#29616;&#20102;&#23384;&#22312;&#38382;&#39064;&#30340;&#40486;&#29255;&#20351;&#29992;&#30340;&#36864;&#20237;&#20891;&#20154;&#12290;&#19982;&#20165;&#36890;&#36807;&#35786;&#26029;&#20195;&#30721;&#35782;&#21035;&#30340;&#40486;&#29255;&#20351;&#29992;&#38556;&#30861;&#24739;&#32773;&#30456;&#27604;&#65292;&#36825;&#20123;&#24739;&#32773;&#20855;&#26377;&#19981;&#21516;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26159;&#40486;&#29255;&#31867;&#33647;&#29289;&#30740;&#31350;&#30340;&#25968;&#25454;&#26469;&#28304;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#40486;&#29255;&#31867;&#33647;&#29289;&#28389;&#29992;&#38590;&#20197;&#36890;&#36807;&#35786;&#26029;&#20195;&#30721;&#36827;&#34892;&#32534;&#30721;&#65292;&#20294;&#26159;&#23384;&#22312;&#38382;&#39064;&#30340;&#40486;&#29255;&#20351;&#29992;&#21487;&#20197;&#35760;&#24405;&#22312;&#20020;&#24202;&#35760;&#24405;&#20013;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;1&#65289;&#20174;&#21508;&#31181;&#20020;&#24202;&#35760;&#24405;&#20013;&#35782;&#21035;&#23384;&#22312;&#38382;&#39064;&#30340;&#40486;&#29255;&#20351;&#29992;&#65307;2&#65289;&#27604;&#36739;&#20165;&#36890;&#36807;&#20020;&#24202;&#35760;&#24405;&#35760;&#24405;&#23384;&#22312;&#38382;&#39064;&#40486;&#29255;&#20351;&#29992;&#30340;&#24739;&#32773;&#19982;&#20351;&#29992;ICD&#40486;&#29255;&#20351;&#29992;&#38556;&#30861;&#35786;&#26029;&#20195;&#30721;&#30340;&#24739;&#32773;&#30340;&#29305;&#24449;&#12290;&#26448;&#26009;&#19982;&#26041;&#27861;&#65306;&#25105;&#20204;&#24320;&#21457;&#24182;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#65292;&#23545;&#26469;&#33258;&#20004;&#20010;&#36864;&#20237;&#20891;&#20154;&#20107;&#21153;&#25152;&#21306;&#22495;&#30340;&#24739;&#32773;&#38431;&#21015;&#65288;n=222,371&#65289;&#30340;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#20998;&#26512;&#20197;&#35782;&#21035;&#23384;&#22312;&#38382;&#39064;&#30340;&#40486;&#29255;&#20351;&#29992;&#30340;&#24739;&#32773;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#19968;&#32452;ICD&#35786;&#26029;&#20195;&#30721;&#26469;&#35782;&#21035;&#26469;&#33258;&#30456;&#21516;&#38431;&#21015;&#30340;&#24739;&#26377;&#40486;&#29255;&#20351;&#29992;&#38556;&#30861;&#30340;&#24739;&#32773;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20165;&#36890;&#36807;NLP&#35782;&#21035;&#20986;&#30340;&#24739;&#32773;&#19982;&#36890;&#36807;&#35786;&#26029;&#20195;&#30721;&#35782;&#21035;&#20986;&#30340;&#24739;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Electronic health records (EHRs) are a data source for opioid research. Opioid use disorder is known to be under-coded as a diagnosis, yet problematic opioid use can be documented in clinical notes.  Objectives: Our goals were 1) to identify problematic opioid use from a full range of clinical notes; and 2) to compare the characteristics of patients identified as having problematic opioid use, exclusively documented in clinical notes, to those having documented ICD opioid use disorder diagnostic codes.  Materials and Methods: We developed and applied a natural language processing (NLP) tool to the clinical notes of a patient cohort (n=222,371) from two Veteran Affairs service regions to identify patients with problematic opioid use. We also used a set of ICD diagnostic codes to identify patients with opioid use disorder from the same cohort. We compared the demographic and clinical characteristics of patients identified only through NLP, to those of patients identified thro
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20256;&#32479;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#30340;&#36755;&#20986;&#25193;&#23637;&#21040;&#20165;&#38480;&#20110;&#38548;&#31163;&#20196;&#29260;&#21015;&#34920;&#20043;&#22806;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12990</link><description>&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#65306;&#36229;&#36234;&#20196;&#29260;&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
Topic Modelling: Going Beyond Token Outputs. (arXiv:2401.12990v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12990
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20256;&#32479;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#30340;&#36755;&#20986;&#25193;&#23637;&#21040;&#20165;&#38480;&#20110;&#38548;&#31163;&#20196;&#29260;&#21015;&#34920;&#20043;&#22806;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#26159;&#19968;&#31181;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#22810;&#20010;&#25991;&#26723;&#20013;&#35782;&#21035;&#26174;&#33879;&#20027;&#39064;&#12290;&#36890;&#24120;&#36755;&#20986;&#26159;&#30001;&#24120;&#24120;&#20849;&#21516;&#20986;&#29616;&#22312;&#36825;&#20123;&#25991;&#26723;&#20013;&#30340;&#38548;&#31163;&#20196;&#29260;&#32452;&#25104;&#30340;&#20027;&#39064;&#38598;&#21512;&#12290;&#20174;&#20154;&#31867;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#36755;&#20986;&#21487;&#33021;&#19981;&#36275;&#20197;&#20805;&#20998;&#25512;&#26029;&#20027;&#39064;&#30340;&#21547;&#20041;&#65307;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#24120;&#24120;&#34987;&#38169;&#35823;&#22320;&#29702;&#35299;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#30740;&#31350;&#35797;&#22270;&#33258;&#21160;&#25193;&#23637;&#20027;&#39064;&#25551;&#36848;&#20197;&#22686;&#24378;&#20027;&#39064;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#21487;&#33021;&#19981;&#21487;&#29992;&#30340;&#22806;&#37096;&#35821;&#35328;&#36164;&#28304;&#65292;&#24182;&#38656;&#35201;&#20445;&#25345;&#26368;&#26032;&#20197;&#29983;&#25104;&#30456;&#20851;&#32467;&#26524;&#65292;&#24182;&#22312;&#35757;&#32451;&#25110;&#22788;&#29702;&#25968;&#25454;&#26102;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20256;&#32479;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#30340;&#36755;&#20986;&#25193;&#23637;&#21040;&#20165;&#38480;&#20110;&#38548;&#31163;&#20196;&#29260;&#21015;&#34920;&#20043;&#22806;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic modelling is a text mining technique for identifying salient themes from a number of documents. The output is commonly a set of topics consisting of isolated tokens that often co-occur in such documents. Manual effort is often associated with interpreting a topic's description from such tokens. However, from a human's perspective, such outputs may not adequately provide enough information to infer the meaning of the topics; thus, their interpretability is often inaccurately understood. Although several studies have attempted to automatically extend topic descriptions as a means of enhancing the interpretation of topic models, they rely on external language sources that may become unavailable, must be kept up-to-date to generate relevant results, and present privacy issues when training on or processing data. This paper presents a novel approach towards extending the output of traditional topic modelling methods beyond a list of isolated tokens. This approach removes the dependenc
&lt;/p&gt;</description></item><item><title>TelME&#26159;&#19968;&#31181;&#25945;&#24072;&#23548;&#21521;&#30340;&#22810;&#27169;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#23545;&#35805;&#20013;&#24773;&#32490;&#35782;&#21035;&#30340;&#20248;&#21270;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#35828;&#35805;&#20154;&#25968;&#25454;&#38598;MELD&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12987</link><description>&lt;p&gt;
TelME&#65306;&#25945;&#24072;&#23548;&#21521;&#30340;&#22810;&#27169;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation. (arXiv:2401.12987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12987
&lt;/p&gt;
&lt;p&gt;
TelME&#26159;&#19968;&#31181;&#25945;&#24072;&#23548;&#21521;&#30340;&#22810;&#27169;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#23545;&#35805;&#20013;&#24773;&#32490;&#35782;&#21035;&#30340;&#20248;&#21270;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#35828;&#35805;&#20154;&#25968;&#25454;&#38598;MELD&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#22312;&#20351;&#23545;&#35805;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22238;&#24212;&#29992;&#25143;&#35831;&#27714;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#21487;&#20197;&#36890;&#36807;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#31561;&#22810;&#31181;&#27169;&#24577;&#30340;&#34920;&#31034;&#36827;&#34892;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#35821;&#35328;&#27169;&#24577;&#23545;&#35782;&#21035;&#24773;&#32490;&#30340;&#36129;&#29486;&#36739;&#24369;&#65292;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#35805;&#20013;&#24773;&#32490;&#35782;&#21035;&#30340;&#25945;&#24072;&#23548;&#21521;&#22810;&#27169;&#34701;&#21512;&#32593;&#32476;&#65288;TelME&#65289;&#12290;TelME&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#23558;&#20449;&#24687;&#20174;&#20316;&#20026;&#25945;&#24072;&#30340;&#35821;&#35328;&#27169;&#22411;&#20256;&#36882;&#32473;&#38750;&#35821;&#35328;&#30340;&#23398;&#29983;&#65292;&#20174;&#32780;&#20248;&#21270;&#20102;&#24369;&#27169;&#24577;&#30340;&#25928;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#31227;&#21160;&#34701;&#21512;&#26041;&#27861;&#23558;&#22810;&#27169;&#24577;&#29305;&#24449;&#32452;&#21512;&#36215;&#26469;&#65292;&#20854;&#20013;&#23398;&#29983;&#32593;&#32476;&#25903;&#25345;&#25945;&#24072;&#12290;TelME&#22312;MELD&#65288;&#19968;&#31181;&#29992;&#20110;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#30340;&#22810;&#35828;&#35805;&#20154;&#25968;&#25454;&#38598;&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#23454;&#39564;&#35770;&#35777;&#20102;&#25105;&#20204;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation (ERC) plays a crucial role in enabling dialogue systems to effectively respond to user requests. The emotions in a conversation can be identified by the representations from various modalities, such as audio, visual, and text. However, due to the weak contribution of non-verbal modalities to recognize emotions, multimodal ERC has always been considered a challenging task. In this paper, we propose Teacher-leading Multimodal fusion network for ERC (TelME). TelME incorporates cross-modal knowledge distillation to transfer information from a language model acting as the teacher to the non-verbal students, thereby optimizing the efficacy of the weak modalities. We then combine multimodal features using a shifting fusion approach in which student networks support the teacher. TelME achieves state-of-the-art performance in MELD, a multi-speaker conversation dataset for ERC. Finally, we demonstrate the effectiveness of our components through additional expe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#32806;&#21512;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#26681;&#65292;&#24182;&#19988;&#22312;&#24378;&#21333;&#35843;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21270;&#25910;&#25947;&#36895;&#29575;&#20026;$\mathcal{O}(1/k)$&#12290;</title><link>http://arxiv.org/abs/2401.12764</link><description>&lt;p&gt;
&#24555;&#36895;&#38750;&#32447;&#24615;&#30340;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#65306;&#23454;&#29616;$\mathcal{O}(1/k)$&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fast Nonlinear Two-Time-Scale Stochastic Approximation: Achieving $\mathcal{O}(1/k)$ Finite-Sample Complexity. (arXiv:2401.12764v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#32806;&#21512;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#26681;&#65292;&#24182;&#19988;&#22312;&#24378;&#21333;&#35843;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21270;&#25910;&#25947;&#36895;&#29575;&#20026;$\mathcal{O}(1/k)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#32806;&#21512;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#26681;&#65292;&#20165;&#20551;&#35774;&#21487;&#20197;&#35266;&#27979;&#21040;&#36825;&#20123;&#31639;&#23376;&#30340;&#22122;&#22768;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#32463;&#20856;&#30340;Ruppert-Polyak&#24179;&#22343;&#25216;&#26415;&#36890;&#36807;&#26679;&#26412;&#21160;&#24577;&#20272;&#35745;&#31639;&#23376;&#30340;&#20540;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#24179;&#22343;&#27493;&#39588;&#30340;&#20272;&#35745;&#20540;&#23558;&#29992;&#20110;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#26356;&#26032;&#20197;&#25214;&#21040;&#25152;&#38656;&#30340;&#35299;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#26159;&#22312;&#24213;&#23618;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#24378;&#21333;&#35843;&#26465;&#20214;&#19979;&#65292;&#25152;&#25552;&#20986;&#26041;&#27861;&#20135;&#29983;&#30340;&#36845;&#20195;&#30340;&#22343;&#26041;&#35823;&#24046;&#20197;&#20248;&#21270;&#30340;&#36895;&#29575;$\mathcal{O}(1/k)$&#25910;&#25947;&#20110;&#38646;&#65292;&#20854;&#20013;$k$&#20026;&#36845;&#20195;&#27425;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#33879;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#32467;&#26524;&#65292;&#26368;&#20339;&#24050;&#30693;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#36895;&#29575;&#20026;$\mathcal{O}(1/k^{2/3})$&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes to develop a new variant of the two-time-scale stochastic approximation to find the roots of two coupled nonlinear operators, assuming only noisy samples of these operators can be observed. Our key idea is to leverage the classic Ruppert-Polyak averaging technique to dynamically estimate the operators through their samples. The estimated values of these averaging steps will then be used in the two-time-scale stochastic approximation updates to find the desired solution. Our main theoretical result is to show that under the strongly monotone condition of the underlying nonlinear operators the mean-squared errors of the iterates generated by the proposed method converge to zero at an optimal rate $\mathcal{O}(1/k)$, where $k$ is the number of iterations. Our result significantly improves the existing result of two-time-scale stochastic approximation, where the best known finite-time convergence rate is $\mathcal{O}(1/k^{2/3})$.
&lt;/p&gt;</description></item><item><title>Falcon&#26159;&#19968;&#20010;&#20351;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#20844;&#24179;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#26679;&#26412;&#36873;&#25321;&#26469;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#23427;&#36890;&#36807;&#35782;&#21035;&#23545;&#20110;&#25552;&#39640;&#20844;&#24179;&#24615;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#8220;&#30446;&#26631;&#32676;&#20307;&#8221;&#26679;&#26412;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#35797;&#38169;&#26041;&#27861;&#26469;&#35299;&#20915;&#26679;&#26412;&#36873;&#25321;&#20013;&#27809;&#26377;ground truth&#26631;&#31614;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.12722</link><description>&lt;p&gt;
Falcon: &#20351;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#36827;&#34892;&#20844;&#24179;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Falcon: Fair Active Learning using Multi-armed Bandits. (arXiv:2401.12722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12722
&lt;/p&gt;
&lt;p&gt;
Falcon&#26159;&#19968;&#20010;&#20351;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#20844;&#24179;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#26679;&#26412;&#36873;&#25321;&#26469;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#23427;&#36890;&#36807;&#35782;&#21035;&#23545;&#20110;&#25552;&#39640;&#20844;&#24179;&#24615;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#8220;&#30446;&#26631;&#32676;&#20307;&#8221;&#26679;&#26412;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#35797;&#38169;&#26041;&#27861;&#26469;&#35299;&#20915;&#26679;&#26412;&#36873;&#25321;&#20013;&#27809;&#26377;ground truth&#26631;&#31614;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#20506;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24378;&#35843;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#24320;&#22987;&#38454;&#27573;&#23884;&#20837;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#38598;&#30340;&#31579;&#36873;&#21644;&#26631;&#23450;&#36807;&#31243;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20844;&#24179;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;Falcon&#12290;Falcon&#37319;&#29992;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#26679;&#26412;&#36873;&#25321;&#26469;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#32473;&#23450;&#29992;&#25143;&#25351;&#23450;&#30340;&#32676;&#20307;&#20844;&#24179;&#24230;&#37327;&#65292;Falcon&#30830;&#23450;&#20102;&#23545;&#25552;&#39640;&#20844;&#24179;&#24615;&#26368;&#26377;&#20449;&#24687;&#37327;&#30340;&#8220;&#30446;&#26631;&#32676;&#20307;&#8221;&#26679;&#26412;&#65288;&#20363;&#22914;&#65288;&#23646;&#24615;=&#22899;&#24615;&#65292;&#26631;&#31614;=&#27491;&#38754;&#65289;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#26679;&#26412;&#36873;&#25321;&#36807;&#31243;&#20013;&#19981;&#21487;&#29992;ground truth&#26631;&#31614;&#26469;&#23450;&#20041;&#36825;&#20123;&#30446;&#26631;&#32676;&#20307;&#65292;&#20986;&#29616;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35797;&#38169;&#26041;&#27861;&#65292;&#22312;&#39044;&#27979;&#26631;&#31614;&#19982;&#26399;&#26395;&#26631;&#31614;&#19981;&#21516;&#26102;&#24182;&#33853;&#22312;&#30446;&#26631;&#32676;&#20307;&#20043;&#22806;&#26102;&#65292;&#25105;&#20204;&#25512;&#36831;&#20351;&#29992;&#35813;&#26679;&#26412;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#36825;&#26679;&#20570;&#20250;&#20135;&#29983;&#26435;&#34913;&#65292;&#36873;&#25321;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#20250;&#22686;&#21152;&#26679;&#26412;&#36827;&#20837;&#30446;&#26631;&#32676;&#20307;&#20043;&#22806;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biased data can lead to unfair machine learning models, highlighting the importance of embedding fairness at the beginning of data analysis, particularly during dataset curation and labeling. In response, we propose Falcon, a scalable fair active learning framework. Falcon adopts a data-centric approach that improves machine learning model fairness via strategic sample selection. Given a user-specified group fairness measure, Falcon identifies samples from "target groups" (e.g., (attribute=female, label=positive)) that are the most informative for improving fairness. However, a challenge arises since these target groups are defined using ground truth labels that are not available during sample selection. To handle this, we propose a novel trial-and-error method, where we postpone using a sample if the predicted label is different from the expected one and falls outside the target group. We also observe the trade-off that selecting more informative samples results in higher likelihood o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#21644;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#32852;&#21512;&#24433;&#21709;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#65292;&#20013;&#31561;&#20219;&#21153;&#30456;&#20284;&#24615;&#23548;&#33268;&#26368;&#22810;&#30340;&#36951;&#24536;&#65292;&#32780;&#22312;&#25554;&#20540;&#38408;&#20540;&#38468;&#36817;&#65292;&#36951;&#24536;&#38543;&#26399;&#26395;&#20219;&#21153;&#30456;&#20284;&#24615;&#21333;&#35843;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2401.12617</link><description>&lt;p&gt;
&#20219;&#21153;&#30456;&#20284;&#24615;&#21644;&#36807;&#21442;&#25968;&#21270;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#32852;&#21512;&#24433;&#21709; - &#19968;&#31181;&#20998;&#26512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting -- An Analytical Model. (arXiv:2401.12617v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#21644;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#32852;&#21512;&#24433;&#21709;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#65292;&#20013;&#31561;&#20219;&#21153;&#30456;&#20284;&#24615;&#23548;&#33268;&#26368;&#22810;&#30340;&#36951;&#24536;&#65292;&#32780;&#22312;&#25554;&#20540;&#38408;&#20540;&#38468;&#36817;&#65292;&#36951;&#24536;&#38543;&#26399;&#26395;&#20219;&#21153;&#30456;&#20284;&#24615;&#21333;&#35843;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#21463;&#21040;&#22810;&#20010;&#20219;&#21153;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20998;&#21035;&#20998;&#26512;&#20102;&#36951;&#24536;&#21463;&#20219;&#21153;&#30456;&#20284;&#24615;&#25110;&#36807;&#21442;&#25968;&#21270;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#22312;&#21487;&#20998;&#26512;&#30340;&#27169;&#22411;&#20013;&#30740;&#31350;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#21644;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#20849;&#21516;&#24433;&#21709;&#36951;&#24536;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#21452;&#20219;&#21153;&#36830;&#32493;&#32447;&#24615;&#22238;&#24402;&#65292;&#20854;&#20013;&#31532;&#20108;&#20010;&#20219;&#21153;&#26159;&#20219;&#24847;&#31532;&#19968;&#20010;&#20219;&#21153;&#30340;&#38543;&#26426;&#27491;&#20132;&#21464;&#25442;&#65288;&#38543;&#26426;&#25490;&#21015;&#20219;&#21153;&#30340;&#25277;&#35937;&#65289;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#26399;&#26395;&#36951;&#24536;&#30340;&#31934;&#30830;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20010;&#24494;&#22937;&#30340;&#27169;&#24335;&#12290;&#22312;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#65292;&#20013;&#31561;&#20219;&#21153;&#30456;&#20284;&#24615;&#23548;&#33268;&#26368;&#22810;&#30340;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#22312;&#25554;&#20540;&#38408;&#20540;&#38468;&#36817;&#65292;&#36951;&#24536;&#38543;&#26399;&#26395;&#20219;&#21153;&#30456;&#20284;&#24615;&#21333;&#35843;&#20943;&#23569;&#12290;&#25105;&#20204;&#29992;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#32447;&#24615;&#22238;&#24402;&#21644;&#24050;&#24314;&#31435;&#30340;&#25490;&#21015;&#20219;&#21153;&#22522;&#20934;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning, catastrophic forgetting is affected by multiple aspects of the tasks. Previous works have analyzed separately how forgetting is affected by either task similarity or overparameterization. In contrast, our paper examines how task similarity and overparameterization jointly affect forgetting in an analyzable model. Specifically, we focus on two-task continual linear regression, where the second task is a random orthogonal transformation of an arbitrary first task (an abstraction of random permutation tasks). We derive an exact analytical expression for the expected forgetting - and uncover a nuanced pattern. In highly overparameterized models, intermediate task similarity causes the most forgetting. However, near the interpolation threshold, forgetting decreases monotonically with the expected task similarity. We validate our findings with linear regression on synthetic data, and with neural networks on established permutation task benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#24314;&#27169;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#20223;&#30495;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#35823;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#25968;&#23383;&#20811;&#38534;&#24050;&#30693;&#30340;&#35823;&#20449;&#24687;&#20849;&#20139;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#25311;&#30340;&#30495;&#23454;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#24182;&#19988;&#32771;&#34385;&#21040;&#20102;&#35752;&#35770;&#20027;&#39064;&#12289;&#29992;&#25143;&#20559;&#22909;&#21644;&#22312;&#32447;&#31038;&#21306;&#21160;&#24577;&#31561;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2401.12509</link><description>&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#25968;&#23383;&#20811;&#38534;&#30340;&#35821;&#35328;&#25935;&#24863;&#20195;&#29702;&#24314;&#27169;&#30740;&#31350;&#35823;&#20449;&#24687;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Digital cloning of online social networks for language-sensitive agent-based modeling of misinformation spread. (arXiv:2401.12509v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#24314;&#27169;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#20223;&#30495;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#35823;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#25968;&#23383;&#20811;&#38534;&#24050;&#30693;&#30340;&#35823;&#20449;&#24687;&#20849;&#20139;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#25311;&#30340;&#30495;&#23454;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#24182;&#19988;&#32771;&#34385;&#21040;&#20102;&#35752;&#35770;&#20027;&#39064;&#12289;&#29992;&#25143;&#20559;&#22909;&#21644;&#22312;&#32447;&#31038;&#21306;&#21160;&#24577;&#31561;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20223;&#30495;&#26694;&#26550;&#65292;&#23558;&#20195;&#29702;&#24314;&#27169;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#35823;&#20449;&#24687;&#20256;&#25773;&#12290;&#34429;&#28982;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#23384;&#22312;&#35768;&#22810;&#20854;&#20182;&#30340;&#20195;&#29702;&#24314;&#27169;&#20223;&#30495;&#65292;&#20294;&#23427;&#20204;&#22312;&#25552;&#20379;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#26041;&#38754;&#21463;&#38480;&#20110;&#20854;&#23545;&#29616;&#26377;&#32593;&#32476;&#30340;&#30495;&#23454;&#24615;&#21644;&#26222;&#36866;&#24615;&#30340;&#19981;&#36275;&#12290;&#20026;&#20102;&#37096;&#20998;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#19979;&#36733;&#36229;&#36807;&#19968;&#19975;&#21517;&#29992;&#25143;&#30340;&#31038;&#20132;&#23186;&#20307;&#21382;&#21490;&#35760;&#24405;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#24050;&#30693;&#35823;&#20449;&#24687;&#20849;&#20139;&#32593;&#32476;&#30340;&#8220;&#25968;&#23383;&#20811;&#38534;&#8221;&#12290;&#25105;&#20204;&#35299;&#26512;&#36825;&#20123;&#21382;&#21490;&#35760;&#24405;&#65292;&#25552;&#21462;&#20986;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#24182;&#23545;&#25104;&#21592;&#20043;&#38388;&#20449;&#24687;&#20998;&#20139;&#21644;&#20256;&#25773;&#30340;&#24494;&#22937;&#26041;&#24335;&#36827;&#34892;&#24314;&#27169;&#12290;&#19982;&#36825;&#20010;&#39046;&#22495;&#20013;&#35768;&#22810;&#20854;&#20182;&#30340;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#26694;&#26550;&#20013;&#29992;&#25143;&#20043;&#38388;&#30340;&#20449;&#24687;&#20998;&#20139;&#23545;&#35752;&#35770;&#20027;&#39064;&#12289;&#29992;&#25143;&#20559;&#22909;&#21644;&#22312;&#32447;&#31038;&#21306;&#21160;&#24577;&#37117;&#38750;&#24120;&#25935;&#24863;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#30495;&#23454;&#24615;&#65292;&#25105;&#20204;&#29992;&#19968;&#32452;&#35760;&#24405;&#22312; t &#20013;&#30340;&#24086;&#23376;&#23545;&#20811;&#38534;&#32593;&#32476;&#36827;&#34892;&#20102;&#31181;&#23376;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a simulation framework for studying misinformation spread within online social networks that blends agent-based modeling and natural language processing techniques. While many other agent-based simulations exist in this space, their ability to provide actionable insights in in part limited by their lack of fidelity and generalizability to existing networks. To partially address these concerns, we create a 'digital clone' of a known misinformation sharing network by downloading social media histories for over ten thousand of its users. We parse these histories to both extract the structure of the network and model the nuanced ways in which information is shared and spread among its members. Unlike many other agent-based methods in this space, information sharing between users in our framework is sensitive to topic of discussion, user preferences, and online community dynamics. To evaluate the fidelity of our method, we seed our cloned network with a set of posts recorded in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23545;&#32454;&#32990;&#22806;&#38388;&#38553;&#20013;&#20998;&#23376;&#20256;&#36755;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23545;&#20998;&#23376;&#20256;&#36755;&#24418;&#24335;&#19981;&#28165;&#26970;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#35745;&#31639;&#25193;&#25955;&#31995;&#25968;&#21644;&#20998;&#23376;&#36895;&#24230;&#30340;&#20248;&#21270;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12435</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23545;&#32454;&#32990;&#22806;&#38388;&#38553;&#20013;&#30340;&#20998;&#23376;&#20256;&#36755;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Quantitative Analysis of Molecular Transport in the Extracellular Space Using Physics-Informed Neural Network. (arXiv:2401.12435v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23545;&#32454;&#32990;&#22806;&#38388;&#38553;&#20013;&#20998;&#23376;&#20256;&#36755;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23545;&#20998;&#23376;&#20256;&#36755;&#24418;&#24335;&#19981;&#28165;&#26970;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#35745;&#31639;&#25193;&#25955;&#31995;&#25968;&#21644;&#20998;&#23376;&#36895;&#24230;&#30340;&#20248;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#30340;&#32454;&#32990;&#22806;&#38388;&#38553; (ECS)&#26159;&#20301;&#20110;&#32454;&#32990;&#20043;&#38388;&#25110;&#32454;&#32990;&#19982;&#34880;&#31649;&#20043;&#38388;&#30340;&#19981;&#35268;&#21017;&#12289;&#26497;&#20854;&#36802;&#22238;&#30340;&#32435;&#31859;&#32423;&#31354;&#38388;&#65292;&#23545;&#31070;&#32463;&#32454;&#32990;&#30340;&#29983;&#23384;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#22312;&#35760;&#24518;&#12289;&#24773;&#32490;&#21644;&#24863;&#35273;&#31561;&#39640;&#32423;&#33041;&#21151;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;ECS&#20869;&#20998;&#23376;&#20256;&#36755;&#30340;&#20855;&#20307;&#24418;&#24335;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476; (PINN) &#35299;&#20915;&#20174;&#23545;&#27969;-&#25193;&#25955;&#26041;&#31243; (ADE) &#23548;&#20986;&#30340;&#19968;&#20010;&#36870;&#38382;&#39064;&#65292;&#23450;&#37327;&#20998;&#26512;ECS&#20869;&#30340;&#20998;&#23376;&#20256;&#36755;&#12290;PINN&#20026;ADE&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#25968;&#23398;&#20844;&#24335;&#25110;&#32593;&#26684;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;PINN&#30340;&#20248;&#21270;&#21151;&#33021;&#21487;&#33258;&#21160;&#35745;&#31639;&#20915;&#23450;&#38271;&#26399;&#20998;&#23376;&#20256;&#36755;&#30340;&#25193;&#25955;&#31995;&#25968;&#21644;&#30001;&#23545;&#27969;&#39537;&#21160;&#30340;&#20998;&#23376;&#36895;&#24230;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20801;&#35768;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The brain extracellular space (ECS), an irregular, extremely tortuous nanoscale space located between cells or between cells and blood vessels, is crucial for nerve cell survival. It plays a pivotal role in high-level brain functions such as memory, emotion, and sensation. However, the specific form of molecular transport within the ECS remain elusive. To address this challenge, this paper proposes a novel approach to quantitatively analyze the molecular transport within the ECS by solving an inverse problem derived from the advection-diffusion equation (ADE) using a physics-informed neural network (PINN). PINN provides a streamlined solution to the ADE without the need for intricate mathematical formulations or grid settings. Additionally, the optimization of PINN facilitates the automatic computation of the diffusion coefficient governing long-term molecule transport and the velocity of molecules driven by advection. Consequently, the proposed method allows for the quantitative analy
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#21270;&#38382;&#39064;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SSLMem&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#20041;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#24378;&#25968;&#25454;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#65292;&#35760;&#24518;&#21270;&#20173;&#28982;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2401.12233</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#21270;&#25552;&#39640;&#20102;&#19979;&#28216;&#27010;&#25324;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Memorization in Self-Supervised Learning Improves Downstream Generalization. (arXiv:2401.12233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12233
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#21270;&#38382;&#39064;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SSLMem&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#20041;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#24378;&#25968;&#25454;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#65292;&#35760;&#24518;&#21270;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22240;&#20854;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#35757;&#32451;&#39640;&#24615;&#33021;&#32534;&#30721;&#22120;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#26469;&#28304;&#20110;&#20114;&#32852;&#32593;&#30340;&#25235;&#21462;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;SSL&#32534;&#30721;&#22120;&#20250;&#35760;&#24518;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#31169;&#20154;&#20449;&#24687;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#27844;&#38706;&#36825;&#20123;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#30417;&#30563;&#23398;&#20064;&#35760;&#24518;&#21270;&#30340;&#29702;&#35770;&#23450;&#20041;&#20381;&#36182;&#20110;&#26631;&#31614;&#65292;&#22240;&#27492;&#26080;&#27861;&#36866;&#29992;&#20110;SSL&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SSLMem&#65292;&#19968;&#20010;&#22312;SSL&#20869;&#23450;&#20041;&#35760;&#24518;&#21270;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#23450;&#20041;&#36890;&#36807;&#27604;&#36739;&#35757;&#32451;&#22312;&#36825;&#20123;&#25968;&#25454;&#28857;&#19978;&#30340;&#32534;&#30721;&#22120;&#21644;&#26410;&#34987;&#35757;&#32451;&#22312;&#36825;&#20123;&#25968;&#25454;&#28857;&#19978;&#30340;&#32534;&#30721;&#22120;&#36820;&#22238;&#30340;&#25968;&#25454;&#28857;&#21644;&#20182;&#20204;&#30340;&#22686;&#24378;&#35270;&#22270;&#30340;&#34920;&#31034;&#30340;&#23545;&#40784;&#24046;&#24322;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#30340;&#32508;&#21512;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#21363;&#20351;SSL&#20381;&#36182;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#24378;&#22823;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#36825;&#37117;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#20316;&#20026;&#27491;&#21017;&#21270;&#25163;&#27573;&#30340;&#24050;&#30693;&#25216;&#26415;&#65292;&#35760;&#24518;&#21270;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently received significant attention due to its ability to train high-performance encoders purely on unlabeled data-often scraped from the internet. This data can still be sensitive and empirical evidence suggests that SSL encoders memorize private information of their training data and can disclose them at inference time. Since existing theoretical definitions of memorization from supervised learning rely on labels, they do not transfer to SSL. To address this gap, we propose SSLMem, a framework for defining memorization within SSL. Our definition compares the difference in alignment of representations for data points and their augmented views returned by both encoders that were trained on these data points and encoders that were not. Through comprehensive empirical analysis on diverse encoder architectures and datasets we highlight that even though SSL relies on large datasets and strong augmentations-both known in supervised learning as regulari
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#24212;&#29992;&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#24605;&#24819;&#33021;&#22815;&#23454;&#29616;&#22312;&#20943;&#23569;&#21442;&#25968;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#25191;&#34892;&#25928;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#23398;&#29983;&#32593;&#32476;&#23398;&#20064;&#21040;&#22797;&#26434;&#30340;&#20132;&#36890;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.11798</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#20132;&#36890;&#39044;&#27979;&#19978;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction. (arXiv:2401.11798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#24212;&#29992;&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#24605;&#24819;&#33021;&#22815;&#23454;&#29616;&#22312;&#20943;&#23569;&#21442;&#25968;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#25191;&#34892;&#25928;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#23398;&#29983;&#32593;&#32476;&#23398;&#20064;&#21040;&#22797;&#26434;&#30340;&#20132;&#36890;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#23454;&#26102;&#20132;&#36890;&#39044;&#27979;&#23545;&#20943;&#23569;&#20132;&#36890;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#39044;&#27979;&#20132;&#36890;&#29366;&#20917;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31354;&#38388;-&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ST-GNN&#65289;&#23558;&#23454;&#26102;&#20132;&#36890;&#25968;&#25454;&#24314;&#27169;&#20026;&#26102;&#38388;&#22270;&#12290;&#23613;&#31649;ST-GNN&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#20026;&#23454;&#38469;&#20132;&#36890;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#23454;&#26102;&#39044;&#27979;&#26102;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#37492;&#20110;&#23454;&#26102;&#25968;&#25454;&#21160;&#24577;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;ST-GNN&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25104;&#26412;&#20989;&#25968;&#65292;&#26088;&#22312;&#20351;&#29992;&#22797;&#26434;&#32593;&#32476;&#65288;&#25945;&#24072;&#65289;&#30340;&#33976;&#39311;&#25968;&#25454;&#26469;&#35757;&#32451;&#20855;&#26377;&#36739;&#23569;&#21442;&#25968;&#30340;&#32593;&#32476;&#65288;&#23398;&#29983;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20934;&#30830;&#24615;&#25509;&#36817;&#25945;&#24072;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65292;&#23558;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#34701;&#20837;&#23398;&#29983;&#32593;&#32476;&#65292;&#20351;&#23398;&#29983;&#33021;&#22815;&#23398;&#20064;&#21040;&#25945;&#24072;&#24863;&#30693;&#30340;&#22797;&#26434;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#38754;&#20020;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient real-time traffic prediction is crucial for reducing transportation time. To predict traffic conditions, we employ a spatio-temporal graph neural network (ST-GNN) to model our real-time traffic data as temporal graphs. Despite its capabilities, it often encounters challenges in delivering efficient real-time predictions for real-world traffic data. Recognizing the significance of timely prediction due to the dynamic nature of real-time data, we employ knowledge distillation (KD) as a solution to enhance the execution time of ST-GNNs for traffic prediction. In this paper, We introduce a cost function designed to train a network with fewer parameters (the student) using distilled data from a complex network (the teacher) while maintaining its accuracy close to that of the teacher. We use knowledge distillation, incorporating spatial-temporal correlations from the teacher network to enable the student to learn the complex patterns perceived by the teacher. However, a challenge a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20302;&#23433;&#20840;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#20197;&#20934;&#30830;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.11792</link><description>&lt;p&gt;
&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20302;&#23433;&#20840;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#20197;&#20934;&#30830;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26234;&#33021;&#39550;&#39542;&#31995;&#32479;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#24403;&#21069;&#29615;&#22659;&#21644;&#36710;&#36742;&#29366;&#24577;&#21160;&#24577;&#21046;&#23450;&#36866;&#24403;&#30340;&#39550;&#39542;&#31574;&#30053;&#65292;&#21516;&#26102;&#30830;&#20445;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#23433;&#20840;&#24615;&#20302;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#65292;&#32780;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#26159;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#30340;&#21069;&#25552;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#32780;&#22810;&#26679;&#22330;&#26223;&#19979;&#30340;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#12290;&#25105;&#20204;&#30340;SGADS&#19982;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#32467;&#21512;&#65292;&#20351;&#26234;&#33021;&#36710;&#36742;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#31034;&#33539;&#30456;&#32467;&#21512;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
An intelligent driving system should be capable of dynamically formulating appropriate driving strategies based on the current environment and vehicle status, while ensuring the security and reliability of the system. However, existing methods based on reinforcement learning and imitation learning suffer from low safety, poor generalization, and inefficient sampling. Additionally, they cannot accurately predict future driving trajectories, and the accurate prediction of future driving trajectories is a precondition for making optimal decisions. To solve these problems, in this paper, we introduce a Safe and Generalized end-to-end Autonomous Driving System (SGADS) for complex and various scenarios. Our SGADS incorporates variational inference with normalizing flows, enabling the intelligent vehicle to accurately predict future driving trajectories. Moreover, we propose the formulation of robust safety constraints. Furthermore, we combine reinforcement learning with demonstrations to aug
&lt;/p&gt;</description></item><item><title>&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#30697;&#38453;&#26041;&#31243;&#35774;&#35745;&#65292;&#36890;&#36807;&#31616;&#21270;&#22522;&#30784;&#26041;&#27861;&#36827;&#34892;&#36817;&#20284;&#35299;&#21442;&#25968;&#26041;&#31243;&#12290;&#23427;&#21487;&#20197;&#20165;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#35745;&#31639;&#26694;&#26550;&#20869;&#20135;&#29983;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.11694</link><description>&lt;p&gt;
&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Parametric Matrix Models. (arXiv:2401.11694v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11694
&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#30697;&#38453;&#26041;&#31243;&#35774;&#35745;&#65292;&#36890;&#36807;&#31616;&#21270;&#22522;&#30784;&#26041;&#27861;&#36827;&#34892;&#36817;&#20284;&#35299;&#21442;&#25968;&#26041;&#31243;&#12290;&#23427;&#21487;&#20197;&#20165;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#35745;&#31639;&#26694;&#26550;&#20869;&#20135;&#29983;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#30340;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#22522;&#20110;&#30697;&#38453;&#26041;&#31243;&#65292;&#24182;&#19988;&#20854;&#35774;&#35745;&#21463;&#21040;&#20102;&#29992;&#20110;&#36817;&#20284;&#35299;&#21442;&#25968;&#26041;&#31243;&#30340;&#31616;&#21270;&#22522;&#30784;&#26041;&#27861;&#30340;&#25928;&#29575;&#21551;&#21457;&#12290;&#20381;&#36182;&#21464;&#37327;&#21487;&#20197;&#38544;&#24335;&#25110;&#26174;&#24335;&#23450;&#20041;&#65292;&#24182;&#19988;&#26041;&#31243;&#21487;&#20197;&#20351;&#29992;&#20195;&#25968;&#12289;&#24494;&#20998;&#25110;&#31215;&#20998;&#20851;&#31995;&#12290;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#21487;&#20197;&#20165;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#39640;&#20445;&#30495;&#24230;&#27169;&#22411;&#35745;&#31639;&#12290;&#34429;&#28982;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#31185;&#23398;&#35745;&#31639;&#65292;&#20294;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#26159;&#19968;&#31181;&#21487;&#20197;&#24212;&#29992;&#20110;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;&#22312;&#20171;&#32461;&#22522;&#30784;&#29702;&#35770;&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#25152;&#26377;&#22312;&#36825;&#37324;&#27979;&#35797;&#30340;&#25361;&#25112;&#65292;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#22312;&#20801;&#35768;&#35745;&#31639;&#30340;&#26694;&#26550;&#20869;&#20135;&#29983;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a general class of machine learning algorithms called parametric matrix models. Parametric matrix models are based on matrix equations, and the design is motivated by the efficiency of reduced basis methods for approximating solutions of parametric equations. The dependent variables can be defined implicitly or explicitly, and the equations may use algebraic, differential, or integral relations. Parametric matrix models can be trained with empirical data only, and no high-fidelity model calculations are needed. While originally designed for scientific computing, parametric matrix models are universal function approximators that can be applied to general machine learning problems. After introducing the underlying theory, we apply parametric matrix models to a series of different challenges that show their performance for a wide range of problems. For all the challenges tested here, parametric matrix models produce accurate results within a computational framework that allows 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11648</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#20998;&#23618;&#27491;&#21017;&#21270;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11648
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#30340;&#35786;&#26029;&#26159;&#19968;&#39033;&#24517;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#21046;&#23450;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#30340;&#20027;&#21160;&#26410;&#26469;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#35768;&#22810;&#30740;&#31350;&#24182;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;EHR&#25968;&#25454;&#22266;&#26377;&#30340;&#24322;&#26500;&#21644;&#20998;&#23618;&#29305;&#24449;&#65292;&#24517;&#28982;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NECHO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20998;&#23618;&#27491;&#21017;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23450;&#21046;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#19968;&#23545;&#21452;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#34701;&#21512;&#28085;&#30422;&#21307;&#23398;&#20195;&#30721;&#12289;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#20020;&#24202;&#31508;&#35760;&#30340;&#22810;&#26041;&#38754;&#20449;&#24687;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#22260;&#32469;&#30528;&#21307;&#23398;&#20195;&#30721;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#21307;&#23398;&#26412;&#20307;&#20013;&#30340;&#29238;&#32423;&#20449;&#24687;&#26469;&#35268;&#33539;&#29305;&#23450;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#23398;&#20064;EHR&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#23545;MIMIC-III&#25968;&#25454;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#65292;&#20026;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.10841</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#21457;&#29616;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#32534;&#30721;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media. (arXiv:2401.10841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10841
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#65292;&#20026;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20167;&#24680;&#35328;&#35770;&#30340;&#34067;&#24310;&#32473;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24102;&#26469;&#20102;&#19968;&#20010;&#38590;&#39064;&#12290;&#19968;&#20010;&#29305;&#27530;&#30340;&#25361;&#25112;&#19982;&#20351;&#29992;&#32534;&#30721;&#35821;&#35328;&#30340;&#32676;&#20307;&#26377;&#20851;&#65292;&#36825;&#20123;&#32676;&#20307;&#26082;&#24819;&#20026;&#20854;&#29992;&#25143;&#21019;&#36896;&#24402;&#23646;&#24863;&#65292;&#21448;&#24819;&#22238;&#36991;&#26816;&#27979;&#12290;&#32534;&#30721;&#35821;&#35328;&#21457;&#23637;&#36805;&#36895;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#20351;&#29992;&#26041;&#24335;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#30340;&#26041;&#27861;&#35770;&#12290;&#35813;&#26041;&#27861;&#22312;&#22312;&#32447;&#21453;&#29369;&#22826;&#35328;&#35770;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#20174;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#25235;&#21462;&#30340;&#24086;&#23376;&#65292;&#36890;&#24120;&#26159;&#26497;&#31471;&#20027;&#20041;&#29992;&#25143;&#20351;&#29992;&#30340;&#12290;&#24086;&#23376;&#26159;&#20351;&#29992;&#19982;&#20197;&#21069;&#24050;&#30693;&#30340;&#38024;&#23545;&#29369;&#22826;&#20154;&#30340;&#20167;&#24680;&#35328;&#35770;&#30456;&#20851;&#30340;&#31181;&#23376;&#34920;&#36798;&#24335;&#36827;&#34892;&#25235;&#21462;&#30340;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#35782;&#21035;&#27599;&#20010;&#24086;&#23376;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#34920;&#36798;&#24335;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#22312;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#30340;&#39057;&#29575;&#12290;&#36807;&#28388;&#25481;&#35821;&#27861;&#19981;&#19968;&#33268;&#30340;&#34920;&#36798;&#24335;&#21644;&#20043;&#21069;&#36935;&#21040;&#36807;&#30340;&#34920;&#36798;&#24335;&#65292;&#20197;&#20415;&#20851;&#27880;&#26032;&#20986;&#29616;&#30340;&#33391;&#22909;&#24418;&#24335;&#30340;&#26415;&#35821;&#12290;&#28982;&#21518;&#36827;&#34892;&#20102;&#35821;&#20041;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online hate speech proliferation has created a difficult problem for social media platforms. A particular challenge relates to the use of coded language by groups interested in both creating a sense of belonging for its users and evading detection. Coded language evolves quickly and its use varies over time. This paper proposes a methodology for detecting emerging coded hate-laden terminology. The methodology is tested in the context of online antisemitic discourse. The approach considers posts scraped from social media platforms, often used by extremist users. The posts are scraped using seed expressions related to previously known discourse of hatred towards Jews. The method begins by identifying the expressions most representative of each post and calculating their frequency in the whole corpus. It filters out grammatically incoherent expressions as well as previously encountered ones so as to focus on emergent well-formed terminology. This is followed by an assessment of semantic s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36741;&#21161;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#21644;&#27714;&#35299;&#21487;&#34892;&#30340;&#26102;&#38388;&#32858;&#21512;&#20195;&#29702;&#38382;&#39064;&#65292;&#35782;&#21035;&#20986;&#20302;&#25104;&#26412;&#30340;&#35268;&#21010;&#20915;&#31574;&#12290;&#36890;&#36807;&#22312;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#39044;&#27979;&#19978;&#35780;&#20272;&#35299;&#20915;&#30340;&#35268;&#21010;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#38543;&#26426;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#30340;&#21487;&#34892;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2401.10451</link><description>&lt;p&gt;
&#23398;&#20064;&#36741;&#21161;&#30340;&#38543;&#26426;&#23481;&#37327;&#25193;&#23637;&#35268;&#21010;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-assisted Stochastic Capacity Expansion Planning: A Bayesian Optimization Approach. (arXiv:2401.10451v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36741;&#21161;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#21644;&#27714;&#35299;&#21487;&#34892;&#30340;&#26102;&#38388;&#32858;&#21512;&#20195;&#29702;&#38382;&#39064;&#65292;&#35782;&#21035;&#20986;&#20302;&#25104;&#26412;&#30340;&#35268;&#21010;&#20915;&#31574;&#12290;&#36890;&#36807;&#22312;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#39044;&#27979;&#19978;&#35780;&#20272;&#35299;&#20915;&#30340;&#35268;&#21010;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#38543;&#26426;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#30340;&#21487;&#34892;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#22823;&#35268;&#27169;&#30340;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#23545;&#20110;&#21306;&#22495;&#33021;&#28304;&#31995;&#32479;&#30340;&#25104;&#26412;&#25928;&#30410;&#20302;&#30899;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#30830;&#20445;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#30340;&#39044;&#26399;&#32467;&#26524;&#65292;&#24314;&#27169;&#32771;&#34385;&#21040;&#22825;&#27668;&#30456;&#20851;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#20379;&#24212;&#21644;&#33021;&#28304;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#38543;&#26426;&#20248;&#21270;&#27169;&#22411;&#36890;&#24120;&#27604;&#30830;&#23450;&#24615;&#27169;&#22411;&#38590;&#20197;&#35745;&#31639;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36741;&#21161;&#30340;&#36817;&#20284;&#35299;&#27861;&#26469;&#21487;&#34892;&#22320;&#35299;&#20915;&#20004;&#38454;&#27573;&#38543;&#26426;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#21644;&#27714;&#35299;&#19968;&#31995;&#21015;&#21487;&#34892;&#30340;&#26102;&#38388;&#32858;&#21512;&#20195;&#29702;&#38382;&#39064;&#65292;&#35782;&#21035;&#20986;&#20302;&#25104;&#26412;&#30340;&#35268;&#21010;&#20915;&#31574;&#12290;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#25628;&#32034;&#26102;&#38388;&#24207;&#21015;&#32858;&#21512;&#36229;&#21442;&#25968;&#30340;&#31354;&#38388;&#65292;&#24182;&#35745;&#31639;&#22312;&#20379;&#38656;&#39044;&#27979;&#30340;&#39564;&#35777;&#38598;&#19978;&#26368;&#23567;&#21270;&#25104;&#26412;&#30340;&#36817;&#20284;&#35299;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#19968;&#32452;&#20445;&#30041;&#30340;&#27979;&#35797;&#39044;&#27979;&#19978;&#35780;&#20272;&#35299;&#20915;&#30340;&#35268;&#21010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving large-scale capacity expansion problems (CEPs) is central to cost-effective decarbonization of regional-scale energy systems. To ensure the intended outcomes of CEPs, modeling uncertainty due to weather-dependent variable renewable energy (VRE) supply and energy demand becomes crucially important. However, the resulting stochastic optimization models are often less computationally tractable than their deterministic counterparts. Here, we propose a learning-assisted approximate solution method to tractably solve two-stage stochastic CEPs. Our method identifies low-cost planning decisions by constructing and solving a sequence of tractable temporally aggregated surrogate problems. We adopt a Bayesian optimization approach to searching the space of time series aggregation hyperparameters and compute approximate solutions that minimize costs on a validation set of supply-demand projections. Importantly, we evaluate solved planning outcomes on a held-out set of test projections. We 
&lt;/p&gt;</description></item><item><title>PatchAD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22359;&#30340;MLP-Mixer&#20307;&#31995;&#32467;&#26500;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#23427;&#20855;&#26377;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#30340;&#21452;&#39033;&#30446;&#32422;&#26463;&#27169;&#22359;&#26469;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09793</link><description>&lt;p&gt;
PatchAD: &#22522;&#20110;&#22359;&#30340;MLP-Mixer&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection. (arXiv:2401.09793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09793
&lt;/p&gt;
&lt;p&gt;
PatchAD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22359;&#30340;MLP-Mixer&#20307;&#31995;&#32467;&#26500;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#23427;&#20855;&#26377;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#30340;&#21452;&#39033;&#30446;&#32422;&#26463;&#27169;&#22359;&#26469;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#20013;&#30340;&#24322;&#24120;&#20107;&#20214;&#12290;&#36825;&#19968;&#20219;&#21153;&#30340;&#26680;&#24515;&#25361;&#25112;&#22312;&#20110;&#22312;&#32570;&#20047;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#27491;&#24120;&#21644;&#24322;&#24120;&#27169;&#24335;&#30340;&#34920;&#31034;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#20381;&#36182;&#20110;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#34920;&#24449;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#22815;&#36731;&#37327;&#32423;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#35774;&#35745;&#19968;&#20010;&#26356;&#39640;&#25928;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PatchAD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#22522;&#20110;&#22359;&#30340;MLP-Mixer&#20307;&#31995;&#32467;&#26500;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#34920;&#24449;&#25552;&#21462;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PatchAD&#30001;&#22235;&#20010;&#29420;&#29305;&#30340;MLP Mixer&#32452;&#25104;&#65292;&#19987;&#38376;&#21033;&#29992;MLP&#26550;&#26500;&#23454;&#29616;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21019;&#26032;&#22320;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#39033;&#30446;&#32422;&#26463;&#27169;&#22359;&#26469;&#32531;&#35299;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection stands as a crucial aspect of time series analysis, aiming to identify abnormal events in time series samples. The central challenge of this task lies in effectively learning the representations of normal and abnormal patterns in a label-lacking scenario. Previous research mostly relied on reconstruction-based approaches, restricting the representational abilities of the models. In addition, most of the current deep learning-based methods are not lightweight enough, which prompts us to design a more efficient framework for anomaly detection. In this study, we introduce PatchAD, a novel multi-scale patch-based MLP-Mixer architecture that leverages contrastive learning for representational extraction and anomaly detection. Specifically, PatchAD is composed of four distinct MLP Mixers, exclusively utilizing the MLP architecture for high efficiency and lightweight architecture. Additionally, we also innovatively crafted a dual project constraint module to mitigate potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26469;&#23398;&#20064;&#20113;&#36752;&#23556;&#21453;&#39304;&#23545;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#24378;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20869;&#26680;&#28145;&#23545;&#27969;&#21644;&#27973;&#20113;&#30340;&#38271;&#27874;&#36752;&#23556;&#24378;&#36843;&#37117;&#23545;&#24378;&#21270;&#36215;&#21040;&#36129;&#29486;&#65292;&#20854;&#20013;&#28145;&#23545;&#27969;&#30340;&#24433;&#21709;&#26368;&#22823;&#12290;</title><link>http://arxiv.org/abs/2401.09493</link><description>&lt;p&gt;
&#35782;&#21035;&#19982;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#24378;&#21270;&#26377;&#20851;&#30340;&#19977;&#32500;&#36752;&#23556;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Identifying Three-Dimensional Radiative Patterns Associated with Early Tropical Cyclone Intensification. (arXiv:2401.09493v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26469;&#23398;&#20064;&#20113;&#36752;&#23556;&#21453;&#39304;&#23545;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#24378;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20869;&#26680;&#28145;&#23545;&#27969;&#21644;&#27973;&#20113;&#30340;&#38271;&#27874;&#36752;&#23556;&#24378;&#36843;&#37117;&#23545;&#24378;&#21270;&#36215;&#21040;&#36129;&#29486;&#65292;&#20854;&#20013;&#28145;&#23545;&#27969;&#30340;&#24433;&#21709;&#26368;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#36752;&#23556;&#21453;&#39304;&#24433;&#21709;&#20102;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#30340;&#24378;&#21270;&#65292;&#20294;&#29616;&#26377;&#35786;&#26029;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#20351;&#20854;&#26080;&#27861;&#29992;&#26469;&#30740;&#31350;&#19981;&#23545;&#31216;&#25110;&#30636;&#24577;&#30340;&#36752;&#23556;&#21152;&#28909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;VED&#65289;&#26469;&#23398;&#20064;&#36752;&#23556;&#19982;&#23454;&#38469;&#27169;&#25311;&#30340;&#27668;&#26059;&#34920;&#38754;&#24378;&#21270;&#20043;&#38388;&#30340;&#38544;&#34255;&#20851;&#31995;&#12290;&#38480;&#21046;VED&#27169;&#22411;&#30340;&#36755;&#20837;&#21487;&#20197;&#21033;&#29992;&#20854;&#19981;&#30830;&#23450;&#24615;&#26469;&#35782;&#21035;&#36752;&#23556;&#23545;&#24378;&#21270;&#26356;&#37325;&#35201;&#30340;&#26102;&#26399;&#12290;&#23545;&#25552;&#21462;&#30340;&#19977;&#32500;&#36752;&#23556;&#32467;&#26500;&#30340;&#32454;&#33268;&#26816;&#26597;&#34920;&#26126;&#65292;&#20869;&#26680;&#28145;&#23545;&#27969;&#21644;&#27973;&#20113;&#30340;&#38271;&#27874;&#36752;&#23556;&#24378;&#36843;&#37117;&#23545;&#24378;&#21270;&#36215;&#21040;&#36129;&#29486;&#65292;&#20854;&#20013;&#28145;&#23545;&#27969;&#22312;&#25972;&#20307;&#19978;&#20855;&#26377;&#26368;&#22823;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#27973;&#20113;&#30340;&#19979;&#39118;&#22788;&#30340;&#28145;&#23545;&#27969;&#23545;&#28023;&#29141;&#30340;&#24378;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#21457;&#29616;&#28909;&#21147;-&#21160;&#21147;&#23398;&#20851;&#31995;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#36724;&#23545;&#31216;&#25110;&#30830;&#23450;&#24615;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud radiative feedback impacts early tropical cyclone (TC) intensification, but limitations in existing diagnostic frameworks make them unsuitable for studying asymmetric or transient radiative heating. We propose a linear Variational Encoder-Decoder (VED) to learn the hidden relationship between radiation and the surface intensification of realistic simulated TCs. Limiting VED model inputs enables using its uncertainty to identify periods when radiation has more importance for intensification. A close examination of the extracted 3D radiative structures suggests that longwave radiative forcing from inner core deep convection and shallow clouds both contribute to intensification, with the deep convection having the most impact overall. We find that deep convection downwind of the shallow clouds is critical to the intensification of Haiyan. Our work demonstrates that machine learning can discover thermodynamic-kinematic relationships without relying on axisymmetric or deterministic as
&lt;/p&gt;</description></item><item><title>DiConStruct&#26159;&#19968;&#31181;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#25552;&#20379;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.08534</link><description>&lt;p&gt;
DiConStruct: &#22522;&#20110;&#40657;&#30418;&#31934;&#21326;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
DiConStruct: Causal Concept-based Explanations through Black-Box Distillation. (arXiv:2401.08534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08534
&lt;/p&gt;
&lt;p&gt;
DiConStruct&#26159;&#19968;&#31181;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#25552;&#20379;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#22312;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#31995;&#32479;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#35299;&#37322;&#24212;&#35813;&#20351;&#29992;&#20154;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#27010;&#24565;&#26469;&#34920;&#36798;&#12290;&#27492;&#22806;&#65292;&#35299;&#37322;&#22120;&#24212;&#35813;&#25429;&#25417;&#36825;&#20123;&#27010;&#24565;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#20415;&#23545;&#35299;&#37322;&#36827;&#34892;&#25512;&#29702;&#12290;&#26368;&#21518;&#65292;&#35299;&#37322;&#26041;&#27861;&#24212;&#35813;&#39640;&#25928;&#65292;&#24182;&#19981;&#25439;&#23475;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;AI&#35299;&#37322;&#24615;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#33267;&#20170;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#28385;&#36275;&#36825;&#19977;&#20010;&#26465;&#20214;&#12290;&#20107;&#23454;&#19978;&#65292;&#20027;&#27969;&#30340;&#23616;&#37096;&#27010;&#24565;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#19981;&#20135;&#29983;&#22240;&#26524;&#35299;&#37322;&#65292;&#24182;&#22312;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DiConStruct&#65292;&#19968;&#31181;&#26082;&#22522;&#20110;&#27010;&#24565;&#21448;&#20855;&#26377;&#22240;&#26524;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#21019;&#24314;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#22120;&#20316;&#20026;&#19968;&#20010;&#31934;&#21326;&#27169;&#22411;&#36866;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the performance of the predictive task. Despite the rapid advances in AI explainability in recent years, as far as we know to date, no method fulfills these three properties. Indeed, mainstream methods for local concept explainability do not produce causal explanations and incur a trade-off between explainability and prediction performance. We present DiConStruct, an explanation method that is both concept-based and causal, with the goal of creating more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine l
&lt;/p&gt;</description></item><item><title>OpenDPD &#26159;&#19968;&#20010;&#29992;&#20110;&#23485;&#24102;&#21151;&#25918;&#24314;&#27169;&#21644;&#25968;&#23383;&#39044;&#22833;&#30495;&#30340;&#24320;&#28304;&#31471;&#21040;&#31471;&#23398;&#20064;&#21644;&#22522;&#20934;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;DGRU-DPD&#27169;&#22411;&#21644;&#26032;&#22411;&#23398;&#20064;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#26032;&#30340;&#25968;&#23383;&#21457;&#23556;&#22120;&#26550;&#26500;&#19979;&#30340;&#20248;&#20110;&#20197;&#21069;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.08318</link><description>&lt;p&gt;
OpenDPD: &#29992;&#20110;&#23485;&#24102;&#21151;&#25918;&#24314;&#27169;&#21644;&#25968;&#23383;&#39044;&#22833;&#30495;&#30340;&#24320;&#28304;&#31471;&#21040;&#31471;&#23398;&#20064;&#21644;&#22522;&#20934;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
OpenDPD: An Open-Source End-to-End Learning &amp; Benchmarking Framework for Wideband Power Amplifier Modeling and Digital Pre-Distortion. (arXiv:2401.08318v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08318
&lt;/p&gt;
&lt;p&gt;
OpenDPD &#26159;&#19968;&#20010;&#29992;&#20110;&#23485;&#24102;&#21151;&#25918;&#24314;&#27169;&#21644;&#25968;&#23383;&#39044;&#22833;&#30495;&#30340;&#24320;&#28304;&#31471;&#21040;&#31471;&#23398;&#20064;&#21644;&#22522;&#20934;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;DGRU-DPD&#27169;&#22411;&#21644;&#26032;&#22411;&#23398;&#20064;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#26032;&#30340;&#25968;&#23383;&#21457;&#23556;&#22120;&#26550;&#26500;&#19979;&#30340;&#20248;&#20110;&#20197;&#21069;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#20449;&#33021;&#21147;&#30340;&#25552;&#21319;&#65292;&#29992;&#20110;&#32416;&#27491;&#23485;&#24102;&#21151;&#25918;&#22120;&#20013;&#30340;&#38750;&#32447;&#24615;&#30340;&#25968;&#23383;&#39044;&#22833;&#30495;&#65288;DPD&#65289;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21464;&#24471;&#31361;&#20986;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#32570;&#20047;&#19968;&#20010;&#29420;&#31435;&#20110;&#27979;&#37327;&#35774;&#32622;&#12289;&#29992;&#20110;&#24555;&#36895;DPD&#25506;&#32034;&#21644;&#23458;&#35266;DPD&#27169;&#22411;&#27604;&#36739;&#30340;&#24320;&#28304;&#24179;&#21488;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;&#24320;&#28304;&#26694;&#26550;OpenDPD&#65292;&#24182;&#38468;&#24102;&#19968;&#20010;&#29992;&#20110;&#21151;&#25918;&#24314;&#27169;&#21644;DPD&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Dense Gated Recurrent Unit (DGRU)-DPD&#30340;&#26032;&#22411;&#31471;&#21040;&#31471;&#23398;&#20064;&#26550;&#26500;&#65292;&#30456;&#36739;&#20110;&#27169;&#25311;&#21151;&#25918;&#22120;&#65292;&#22312;&#26032;&#30340;&#25968;&#23383;&#21457;&#23556;&#22120;&#65288;DTX&#65289;&#26550;&#26500;&#19979;&#65292;&#20855;&#26377;&#38750;&#24120;&#35268;&#20256;&#36755;&#29305;&#24615;&#65292;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#30340;DPD&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;DGRU-DPD&#23545;&#20110;200 MHz OFDM&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;-44.69/-44.47 dBc&#30340;ACPR&#21644;-35.22 dB&#30340;EVM&#12290;OpenDPD&#30340;&#20195;&#30721;&#12289;&#25968;&#25454;&#38598;&#21644;&#25991;&#26723;&#21487;&#22312;https://github.com/lab-emi/OpenDPD&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise in communication capacity, deep neural networks (DNN) for digital pre-distortion (DPD) to correct non-linearity in wideband power amplifiers (PAs) have become prominent. Yet, there is a void in open-source and measurement-setup-independent platforms for fast DPD exploration and objective DPD model comparison. This paper presents an open-source framework, OpenDPD, crafted in PyTorch, with an associated dataset for PA modeling and DPD learning. We introduce a Dense Gated Recurrent Unit (DGRU)-DPD, trained via a novel end-to-end learning architecture, outperforming previous DPD models on a digital PA (DPA) in the new digital transmitter (DTX) architecture with unconventional transfer characteristics compared to analog PAs. Measurements show our DGRU-DPD achieves an ACPR of -44.69/-44.47 dBc and an EVM of -35.22 dB for 200 MHz OFDM signals. OpenDPD code, datasets, and documentation are publicly available at https://github.com/lab-emi/OpenDPD.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#26102;&#24577;&#36923;&#36753;&#25512;&#29702;&#26469;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#24418;&#24335;&#36923;&#36753;&#21551;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#24322;&#36136;&#24615;&#23458;&#25143;&#35774;&#22791;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#32858;&#21512;&#32676;&#38598;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2401.07448</link><description>&lt;p&gt;
&#20511;&#21161;&#23646;&#24615;&#25512;&#29702;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#24418;&#24335;&#36923;&#36753;&#21551;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Formal Logic Enabled Personalized Federated Learning Through Property Inference. (arXiv:2401.07448v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#26102;&#24577;&#36923;&#36753;&#25512;&#29702;&#26469;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#24418;&#24335;&#36923;&#36753;&#21551;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#24322;&#36136;&#24615;&#23458;&#25143;&#35774;&#22791;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#32858;&#21512;&#32676;&#38598;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#19981;&#26029;&#30740;&#31350;&#36827;&#23637;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#20998;&#24067;&#24335;&#21327;&#20316;&#24212;&#29992;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#29289;&#32852;&#32593;&#20154;&#24037;&#26234;&#33021;&#65288;AIoT&#65289;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30740;&#31350;&#39046;&#22495;&#20013;&#19968;&#20010;&#32570;&#22833;&#30340;&#20851;&#38190;&#26041;&#38754;&#26159;&#20351;&#20855;&#26377;&#31526;&#21495;&#25512;&#29702;&#33021;&#21147;&#30340;&#25968;&#25454;&#39537;&#21160;&#23458;&#25143;&#27169;&#22411;&#33021;&#22815;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#23458;&#25143;&#35774;&#22791;&#30340;&#24322;&#36136;&#24615;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#27599;&#20010;&#23458;&#25143;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#36923;&#36753;&#25512;&#29702;&#29305;&#24615;&#12290;&#24573;&#35270;&#36825;&#20123;&#35774;&#22791;&#29305;&#23450;&#30340;&#35268;&#33539;&#21487;&#33021;&#20250;&#23548;&#33268;&#23458;&#25143;&#31471;&#39044;&#27979;&#20013;&#36951;&#28431;&#20851;&#38190;&#23646;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#26102;&#24577;&#36923;&#36753;&#25512;&#29702;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#36890;&#36807;&#20026;&#27599;&#20010;FL&#23458;&#25143;&#31471;&#21152;&#20837;&#26426;&#26800;&#29983;&#25104;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#26469;&#22686;&#24378;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32858;&#21512;&#32676;&#38598;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in federated learning (FL) have greatly facilitated the development of decentralized collaborative applications, particularly in the domain of Artificial Intelligence of Things (AIoT). However, a critical aspect missing from the current research landscape is the ability to enable data-driven client models with symbolic reasoning capabilities. Specifically, the inherent heterogeneity of participating client devices poses a significant challenge, as each client exhibits unique logic reasoning properties. Failing to consider these device-specific specifications can result in critical properties being missed in the client predictions, leading to suboptimal performance. In this work, we propose a new training paradigm that leverages temporal logic reasoning to address this issue. Our approach involves enhancing the training process by incorporating mechanically generated logic expressions for each FL client. Additionally, we introduce the concept of aggregation clusters 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05535</link><description>&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Accuracy and Interpretability of Random Forests via Forest Pruning. (arXiv:2401.05535v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05535
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25509;&#36817;&#20960;&#21313;&#24180;&#30340;&#21457;&#23637;&#20043;&#21518;&#65292;&#38543;&#26426;&#26862;&#26519;&#20173;&#28982;&#22312;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#36825;&#26041;&#38754;&#36229;&#36234;&#20102;&#20915;&#31574;&#26641;&#29978;&#33267;&#31070;&#32463;&#32593;&#32476;&#31561;&#26367;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#38543;&#26426;&#26862;&#26519;&#22312;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#27604;&#20915;&#31574;&#26641;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#26041;&#27861;&#65292;&#26088;&#22312;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#22312;&#32473;&#23450;&#30340;&#38543;&#26426;&#26862;&#26519;&#20869;&#25214;&#21040;&#26368;&#20339;&#23376;&#26862;&#26519;&#65292;&#28982;&#21518;&#22312;&#36866;&#29992;&#30340;&#24773;&#20917;&#19979;&#23558;&#36873;&#23450;&#30340;&#26641;&#21512;&#24182;&#20026;&#19968;&#26869;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#32422;&#26463;&#31351;&#20030;&#25628;&#32034;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#22522;&#20110;LASSO&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#20013;&#33267;&#23569;&#26377;&#19968;&#31181;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decades after their inception, random forests continue to provide state-of-the-art accuracy in a variety of learning problems, outperforming in this respect alternative machine learning algorithms such as decision trees or even neural networks. However, being an ensemble method, the one aspect where random forests tend to severely underperform decision trees is interpretability. In the present work, we propose a post-hoc approach that aims to have the best of both worlds: the accuracy of random forests and the interpretability of decision trees. To this end, we present two forest-pruning methods to find an optimal sub-forest within a given random forest, and then, when applicable, combine the selected trees into one. Our first method relies on constrained exhaustive search, while our second method is based on an adaptation of the LASSO methodology. Extensive experiments over synthetic and real world datasets show that, in the majority of scenarios, at least one of the two methods propo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SAR-RARP50&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#20844;&#24320;&#30340;&#12289;&#20307;&#20869;&#30340;&#25163;&#26415;&#21160;&#20316;&#35782;&#21035;&#21644;&#35821;&#20041;&#20202;&#22120;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35753;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#31283;&#20581;&#19988;&#20934;&#30830;&#30340;&#21333;&#20219;&#21153;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.00496</link><description>&lt;p&gt;
SAR-RARP50:&#26426;&#22120;&#20154;&#36741;&#21161;&#26681;&#27835;&#24615;&#21069;&#21015;&#33146;&#20999;&#38500;&#26415;&#20013;&#25163;&#26415;&#22120;&#26800;&#30340;&#20998;&#21106;&#21644;&#21160;&#20316;&#35782;&#21035;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
SAR-RARP50: Segmentation of surgical instrumentation and Action Recognition on Robot-Assisted Radical Prostatectomy Challenge. (arXiv:2401.00496v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00496
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SAR-RARP50&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#20844;&#24320;&#30340;&#12289;&#20307;&#20869;&#30340;&#25163;&#26415;&#21160;&#20316;&#35782;&#21035;&#21644;&#35821;&#20041;&#20202;&#22120;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35753;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#31283;&#20581;&#19988;&#20934;&#30830;&#30340;&#21333;&#20219;&#21153;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26415;&#24037;&#20855;&#20998;&#21106;&#21644;&#21160;&#20316;&#35782;&#21035;&#26159;&#35768;&#22810;&#35745;&#31639;&#26426;&#36741;&#21161;&#24178;&#39044;&#24212;&#29992;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#65292;&#20174;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#21040;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#12290;&#29616;&#22312;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#21160;&#20316;&#35782;&#21035;&#21644;&#20998;&#21106;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20294;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#21160;&#20316;&#35782;&#21035;&#21644;&#24037;&#20855;&#20998;&#21106;&#31639;&#27861;&#36890;&#24120;&#26159;&#22312;&#24444;&#27492;&#23396;&#31435;&#22320;&#35757;&#32451;&#21644;&#39044;&#27979;&#65292;&#27809;&#26377;&#21033;&#29992;&#28508;&#22312;&#30340;&#20132;&#21449;&#20219;&#21153;&#20851;&#31995;&#12290;&#36890;&#36807;EndoVis 2022 SAR-RARP50&#25361;&#25112;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#12289;&#20844;&#24320;&#30340;&#12289;&#20307;&#20869;&#30340;&#25163;&#26415;&#21160;&#20316;&#35782;&#21035;&#21644;&#35821;&#20041;&#20202;&#22120;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;50&#20010;&#26426;&#22120;&#20154;&#36741;&#21161;&#26681;&#27835;&#24615;&#21069;&#21015;&#33146;&#20999;&#38500;&#26415;&#65288;RARP&#65289;&#30340;&#32541;&#21512;&#35270;&#39057;&#29255;&#27573;&#12290;&#25361;&#25112;&#30340;&#30446;&#26631;&#26159;&#20004;&#26041;&#38754;&#30340;&#12290;&#39318;&#20808;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#65292;&#24320;&#21457;&#20986;&#31283;&#20581;&#19988;&#39640;&#20934;&#30830;&#24615;&#30340;&#21333;&#20219;&#21153;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surgical tool segmentation and action recognition are fundamental building blocks in many computer-assisted intervention applications, ranging from surgical skills assessment to decision support systems. Nowadays, learning-based action recognition and segmentation approaches outperform classical methods, relying, however, on large, annotated datasets. Furthermore, action recognition and tool segmentation algorithms are often trained and make predictions in isolation from each other, without exploiting potential cross-task relationships. With the EndoVis 2022 SAR-RARP50 challenge, we release the first multimodal, publicly available, in-vivo, dataset for surgical action recognition and semantic instrumentation segmentation, containing 50 suturing video segments of Robotic Assisted Radical Prostatectomy (RARP). The aim of the challenge is twofold. First, to enable researchers to leverage the scale of the provided dataset and develop robust and highly accurate single-task action recognitio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#20934;&#30830;&#33455;&#29255;&#24211;&#29305;&#24449;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33455;&#29255;&#32467;&#26500;&#65292;&#22312;&#21508;&#31181;&#24037;&#33402;&#21442;&#25968;&#19979;&#39044;&#27979;&#31934;&#24230;&#39640;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;100&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2312.12784</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#33455;&#29255;&#24211;&#29305;&#24449;&#21270;&#29992;&#20110;&#35774;&#35745;&#25216;&#26415;&#20849;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fast Cell Library Characterization for Design Technology Co-Optimization Based on Graph Neural Networks. (arXiv:2312.12784v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12784
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#20934;&#30830;&#33455;&#29255;&#24211;&#29305;&#24449;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33455;&#29255;&#32467;&#26500;&#65292;&#22312;&#21508;&#31181;&#24037;&#33402;&#21442;&#25968;&#19979;&#39044;&#27979;&#31934;&#24230;&#39640;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;100&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#25216;&#26415;&#20849;&#20248;&#21270;&#22312;&#20808;&#36827;&#21322;&#23548;&#20307;&#24037;&#33402;&#24320;&#21457;&#20013;&#23454;&#29616;&#21151;&#32791;&#12289;&#24615;&#33021;&#21644;&#38754;&#31215;&#65288;PPA&#65289;&#30340;&#26368;&#20339;&#21270;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#33455;&#29255;&#24211;&#29305;&#24449;&#21270;&#22312;&#35774;&#35745;&#25216;&#26415;&#20849;&#20248;&#21270;&#27969;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20256;&#32479;&#26041;&#27861;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24555;&#36895;&#20934;&#30830;&#30340;&#33455;&#29255;&#24211;&#29305;&#24449;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#33455;&#29255;&#32467;&#26500;&#65292;&#24182;&#22312;&#21508;&#31181;&#24037;&#33402;-&#30005;&#21387;-&#28201;&#24230;&#65288;PVT&#65289;&#35282;&#21644;&#25216;&#26415;&#21442;&#25968;&#19978;&#23637;&#31034;&#20986;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;&#22312;512&#20010;&#26410;&#35265;&#36807;&#30340;&#24037;&#33402;&#35282;&#21644;&#19968;&#30334;&#19975;&#20010;&#27979;&#35797;&#25968;&#25454;&#28857;&#30340;&#39564;&#35777;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#20110;33&#31181;&#31867;&#22411;&#30340;&#21333;&#20803;&#30340;&#24310;&#36831;&#12289;&#21151;&#29575;&#21644;&#36755;&#20837;&#24341;&#33050;&#30005;&#23481;&#20855;&#26377;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#22343;&#26041;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#65288;MAPE&#65289;&#8804; 0.95%&#65292;&#19982;SPICE&#20223;&#30495;&#30456;&#27604;&#21152;&#36895;&#20102;100&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#31995;&#32479;&#32423;&#25351;&#26631;&#65292;&#22914;&#26368;&#24046;&#36127;&#26494;&#24347;&#65288;WNS&#65289;&#12289;&#28431;&#30005;&#21151;&#32791;&#21644;&#21160;&#24577;...
&lt;/p&gt;
&lt;p&gt;
Design technology co-optimization (DTCO) plays a critical role in achieving optimal power, performance, and area (PPA) for advanced semiconductor process development. Cell library characterization is essential in DTCO flow, but traditional methods are time-consuming and costly. To overcome these challenges, we propose a graph neural network (GNN)-based machine learning model for rapid and accurate cell library characterization. Our model incorporates cell structures and demonstrates high prediction accuracy across various process-voltage-temperature (PVT) corners and technology parameters. Validation with 512 unseen technology corners and over one million test data points shows accurate predictions of delay, power, and input pin capacitance for 33 types of cells, with a mean absolute percentage error (MAPE) $\le$ 0.95% and a speed-up of 100X compared with SPICE simulations. Additionally, we investigate system-level metrics such as worst negative slack (WNS), leakage power, and dynamic 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32039;&#20945;&#22411;&#30340;LSTM-SVM&#34701;&#21512;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#27969;&#31243;&#20248;&#21270;&#26041;&#27861;&#23558;&#24515;&#30005;&#22270;&#20449;&#21495;&#39044;&#22788;&#29702;&#20026;&#19968;&#33268;&#30340;10&#31186;&#25345;&#32493;&#26102;&#38388;&#65292;&#21516;&#26102;&#21033;&#29992;LSTM&#21644;SVM&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.09442</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38271;&#26102;&#38388;&#24515;&#34880;&#31649;&#30142;&#30149;&#26816;&#27979;&#30340;&#32039;&#20945;&#22411;LSTM-SVM&#34701;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Compact LSTM-SVM Fusion Model for Long-Duration Cardiovascular Diseases Detection. (arXiv:2312.09442v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32039;&#20945;&#22411;&#30340;LSTM-SVM&#34701;&#21512;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#27969;&#31243;&#20248;&#21270;&#26041;&#27861;&#23558;&#24515;&#30005;&#22270;&#20449;&#21495;&#39044;&#22788;&#29702;&#20026;&#19968;&#33268;&#30340;10&#31186;&#25345;&#32493;&#26102;&#38388;&#65292;&#21516;&#26102;&#21033;&#29992;LSTM&#21644;SVM&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#33539;&#22260;&#20869;&#65292;&#24515;&#34880;&#31649;&#30142;&#30149;&#26159;&#33268;&#27515;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#27599;&#24180;&#32422;&#26377;1790&#19975;&#20154;&#27515;&#20110;&#35813;&#30149;&#12290;&#26089;&#26399;&#26816;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#26159;&#20851;&#38190;&#30340;&#20020;&#24202;&#30446;&#26631;&#65292;&#30005;&#24515;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#26159;&#19968;&#20010;&#21463;&#21040;&#30740;&#31350;&#30028;&#20851;&#27880;&#30340;&#39046;&#22495;&#12290;&#26368;&#36817;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#22312;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#19981;&#36866;&#24403;&#30340;&#27169;&#22411;&#35780;&#20272;&#21644;&#25968;&#25454;&#27844;&#28431;&#30340;&#23454;&#20363;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#24037;&#20316;&#27969;&#31243;&#33539;&#24335;&#65292;&#23558;ECG&#20449;&#21495;&#39044;&#22788;&#29702;&#20026;&#19968;&#33268;&#30340;10&#31186;&#25345;&#32493;&#26102;&#38388;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#29305;&#24449;&#25552;&#21462;/&#24515;&#25615;&#26816;&#27979;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#19982;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#12290;&#35813;&#26550;&#26500;&#21253;&#25324;&#20004;&#20010;LSTM&#23618;&#21644;&#19968;&#20010;SVM&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;SOTA&#32467;&#26524;&#65292;&#24179;&#22343;&#31934;&#30830;&#24230;&#35780;&#20998;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Globally, cardiovascular diseases (CVDs) are the leading cause of mortality, accounting for an estimated 17.9 million deaths annually. One critical clinical objective is the early detection of CVDs using electrocardiogram (ECG) data, an area that has received significant attention from the research community. Recent advancements based on machine learning and deep learning have achieved great progress in this domain. However, existing methodologies exhibit inherent limitations, including inappropriate model evaluations and instances of data leakage. In this study, we present a streamlined workflow paradigm for preprocessing ECG signals into consistent 10-second durations, eliminating the need for manual feature extraction/beat detection. We also propose a hybrid model of Long Short-Term Memory (LSTM) with Support Vector Machine (SVM) for fraud detection. This architecture consists of two LSTM layers and an SVM classifier, which achieves a SOTA results with an Average precision score of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;SpiNNaker 2&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#23454;&#29616;&#35821;&#35328;&#24314;&#27169;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20107;&#20214;&#30340;&#26550;&#26500;&#21644;&#22823;&#35268;&#27169;&#24322;&#27493;&#22788;&#29702;&#30340;&#30828;&#20214;&#65292;&#35813;&#26041;&#27861;&#26377;&#26395;&#22312;&#20943;&#23569;&#33021;&#32791;&#30340;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.09084</link><description>&lt;p&gt;
&#22312;SpiNNaker 2&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Language Modeling on a SpiNNaker 2 Neuromorphic Chip. (arXiv:2312.09084v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09084
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;SpiNNaker 2&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#23454;&#29616;&#35821;&#35328;&#24314;&#27169;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20107;&#20214;&#30340;&#26550;&#26500;&#21644;&#22823;&#35268;&#27169;&#24322;&#27493;&#22788;&#29702;&#30340;&#30828;&#20214;&#65292;&#35813;&#26041;&#27861;&#26377;&#26395;&#22312;&#20943;&#23569;&#33021;&#32791;&#30340;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#36805;&#36895;&#22686;&#38271;&#65292;&#25152;&#38656;&#30340;&#35745;&#31639;&#33021;&#21147;&#20063;&#22312;&#22686;&#21152;&#12290;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#35774;&#22791;&#19978;&#30340;&#20107;&#20214;&#39537;&#21160;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#26174;&#33879;&#38477;&#20302;&#25512;&#29702;&#33021;&#32791;&#30340;&#28508;&#22312;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#21487;&#20197;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#36816;&#34892;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#32593;&#32476;&#65292;&#21253;&#25324;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNN)&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#20219;&#21153;&#24615;&#33021;&#29978;&#33267;&#19981;&#33021;&#19982;LSTM&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#22240;&#27492;&#65292;&#22312;&#31070;&#32463;&#24418;&#24577;&#35774;&#22791;&#19978;&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#20284;&#20046;&#26159;&#19968;&#20010;&#36965;&#36828;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#31070;&#32463;&#24418;&#24577;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411; - &#20855;&#20307;&#26469;&#35828;&#26159;&#22522;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#21517;&#20026;EGRU&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#26550;&#26500;&#30340;SpiNNaker 2&#33455;&#29255;&#12290;SpiNNaker 2&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#22823;&#35268;&#27169;&#24322;&#27493;&#22788;&#29702;&#30340;&#20247;&#26680;&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#65292;&#32780;EGRU&#26159;&#20026;&#20102;&#22312;&#20445;&#25345;&#31454;&#20105;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#39640;&#25928;&#21033;&#29992;&#36825;&#31181;&#30828;&#20214;&#32780;&#35774;&#35745;&#30340;&#12290;&#36825;&#20010;&#23454;&#29616;&#26631;&#24535;&#30528;&#22312;&#31070;&#32463;&#24418;&#24577;&#35774;&#22791;&#19978;&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#30340;&#31532;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
As large language models continue to scale in size rapidly, so too does the computational power required to run them. Event-based networks on neuromorphic devices offer a potential way to reduce energy consumption for inference significantly. However, to date, most event-based networks that can run on neuromorphic hardware, including spiking neural networks (SNNs), have not achieved task performance even on par with LSTM models for language modeling. As a result, language modeling on neuromorphic devices has seemed a distant prospect. In this work, we demonstrate the first-ever implementation of a language model on a neuromorphic device - specifically the SpiNNaker 2 chip based on a recently published event-based architecture called the EGRU. SpiNNaker 2 is a many-core neuromorphic chip designed for large-scale asynchronous processing, while the EGRU is architected to leverage such hardware efficiently while maintaining competitive task performance. This implementation marks the firs
&lt;/p&gt;</description></item><item><title>&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HGPROMPT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36830;&#25509;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#65292;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.01878</link><description>&lt;p&gt;
HGPROMPT: &#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#20013;&#29992;&#20110;&#36830;&#25509;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot Prompt Learning. (arXiv:2312.01878v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01878
&lt;/p&gt;
&lt;p&gt;
&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HGPROMPT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36830;&#25509;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#65292;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#26159;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#31471;&#21040;&#31471;&#30340;&#30417;&#30563;&#26694;&#26550;&#20013;&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20219;&#21153;&#29305;&#23450;&#30417;&#30563;&#30340;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#65292;&#23545;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#30740;&#31350;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#33539;&#24335;&#65292;&#20294;&#26159;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#24120;&#24120;&#23384;&#22312;&#24046;&#36317;&#65292;&#23548;&#33268;&#30446;&#26631;&#30340;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25552;&#31034;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#27491;&#22312;&#23835;&#36215;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#24494;&#35843;&#12290;&#34429;&#28982;&#26089;&#26399;&#24050;&#32463;&#23545;&#22270;&#19978;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#36827;&#34892;&#20102;&#19968;&#20123;&#25506;&#32034;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#28041;&#21450;&#21516;&#36136;&#22270;&#65292;&#24573;&#30053;&#20102;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#24322;&#36136;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HGPROMPT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;&#65292;&#20197;&#32479;&#19968;&#39044;&#20808;&#35757;&#32451;&#30340;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) and heterogeneous graph neural networks (HGNNs) are prominent techniques for homogeneous and heterogeneous graph representation learning, yet their performance in an end-to-end supervised framework greatly depends on the availability of task-specific supervision. To reduce the labeling cost, pre-training on self-supervised pretext tasks has become a popular paradigm,but there is often a gap between the pre-trained model and downstream tasks, stemming from the divergence in their objectives. To bridge the gap, prompt learning has risen as a promising direction especially in few-shot settings, without the need to fully fine-tune the pre-trained model. While there has been some early exploration of prompt-based learning on graphs, they primarily deal with homogeneous graphs, ignoring the heterogeneous graphs that are prevalent in downstream applications. In this paper, we propose HGPROMPT, a novel pre-training and prompting framework to unify not only pre-trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#30452;&#35266;&#29289;&#29702;&#12289;&#22240;&#26524;&#25512;&#29702;&#21644;&#30452;&#35266;&#24515;&#29702;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#23545;&#29031;&#23454;&#39564;&#21457;&#29616;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#35299;&#37322;&#35270;&#35273;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#29087;&#32451;&#24615;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#30452;&#35266;&#29289;&#29702;&#12289;&#22240;&#26524;&#25512;&#29702;&#21644;&#30452;&#35266;&#24515;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#36824;&#26377;&#24453;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2311.16093</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
Visual cognition in multimodal large language models. (arXiv:2311.16093v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#30452;&#35266;&#29289;&#29702;&#12289;&#22240;&#26524;&#25512;&#29702;&#21644;&#30452;&#35266;&#24515;&#29702;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#23545;&#29031;&#23454;&#39564;&#21457;&#29616;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#35299;&#37322;&#35270;&#35273;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#29087;&#32451;&#24615;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#30452;&#35266;&#29289;&#29702;&#12289;&#22240;&#26524;&#25512;&#29702;&#21644;&#30452;&#35266;&#24515;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#36824;&#26377;&#24453;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#26500;&#24314;&#20687;&#20154;&#31867;&#19968;&#26679;&#24605;&#32771;&#30340;&#26426;&#22120;&#12290;&#28982;&#32780;&#25454;&#35748;&#20026;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26080;&#27861;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#30740;&#31350;&#20154;&#21592;&#25351;&#20986;&#36825;&#20123;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#12289;&#30452;&#35266;&#29289;&#29702;&#21644;&#30452;&#35266;&#24515;&#29702;&#31561;&#39046;&#22495;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#38754;&#21521;&#35270;&#35273;&#22788;&#29702;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#37325;&#26032;&#24341;&#36215;&#20102;&#23545;&#27169;&#25311;&#20154;&#31867;&#31867;&#20284;&#35748;&#30693;&#33021;&#21147;&#28508;&#21147;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#30452;&#35266;&#29289;&#29702;&#12289;&#22240;&#26524;&#25512;&#29702;&#21644;&#30452;&#35266;&#24515;&#29702;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#23545;&#29031;&#23454;&#39564;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20123;&#29616;&#20195;&#27169;&#22411;&#22312;&#29702;&#35299;&#22797;&#26434;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12289;&#22240;&#26524;&#20851;&#31995;&#21644;&#23545;&#20182;&#20154;&#20559;&#22909;&#30340;&#30452;&#35266;&#29702;&#35299;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;,&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#35299;&#37322;&#35270;&#35273;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#29087;&#32451;&#24615;,&#28982;&#32780;&#23427;&#20204;&#22312;&#30452;&#35266;&#29289;&#29702;&#12289;&#22240;&#26524;&#25512;&#29702;&#21644;&#30452;&#35266;&#24515;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#36824;&#26377;&#24453;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
A chief goal of artificial intelligence is to build machines that think like people. Yet it has been argued that deep neural network architectures fail to accomplish this. Researchers have asserted these models' limitations in the domains of causal reasoning, intuitive physics, and intuitive psychology. Yet recent advancements, namely the rise of large language models, particularly those designed for visual processing, have rekindled interest in the potential to emulate human-like cognitive abilities. This paper evaluates the current state of vision-based large language models in the domains of intuitive physics, causal reasoning, and intuitive psychology. Through a series of controlled experiments, we investigate the extent to which these modern models grasp complex physical interactions, causal relationships, and intuitive understanding of others' preferences. Our findings reveal that, while these models demonstrate a notable proficiency in processing and interpreting visual data, th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#28508;&#22312;&#21147;&#27169;&#22411;(DLFM)&#30340;&#36890;&#29992;&#22495;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#26680;&#30340;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#65292;&#36890;&#36807;&#36807;&#31243;&#21367;&#31215;&#26041;&#27861;&#20174;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#25512;&#23548;&#20986;&#26469;&#12290;DLFM&#33021;&#22815;&#25429;&#25417;&#39640;&#24230;&#38750;&#32447;&#24615;&#23454;&#38469;&#22810;&#36755;&#20986;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21160;&#24577;&#24615;&#65292;&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#19982;&#19968;&#31995;&#21015;&#38750;&#29289;&#29702;&#32508;&#21512;&#27010;&#29575;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.14828</link><description>&lt;p&gt;
&#28145;&#24230;&#28508;&#22312;&#21147;&#27169;&#22411;&#65306;&#22522;&#20110;ODE&#30340;&#36807;&#31243;&#21367;&#31215;&#29992;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Latent Force Models: ODE-based Process Convolutions for Bayesian Deep Learning. (arXiv:2311.14828v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14828
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#28508;&#22312;&#21147;&#27169;&#22411;(DLFM)&#30340;&#36890;&#29992;&#22495;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#26680;&#30340;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#65292;&#36890;&#36807;&#36807;&#31243;&#21367;&#31215;&#26041;&#27861;&#20174;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#25512;&#23548;&#20986;&#26469;&#12290;DLFM&#33021;&#22815;&#25429;&#25417;&#39640;&#24230;&#38750;&#32447;&#24615;&#23454;&#38469;&#22810;&#36755;&#20986;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21160;&#24577;&#24615;&#65292;&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#19982;&#19968;&#31995;&#21015;&#38750;&#29289;&#29702;&#32508;&#21512;&#27010;&#29575;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#39640;&#24230;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#24182;&#20855;&#26377;&#31283;&#20581;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#19987;&#38376;&#35774;&#35745;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#22495;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#31216;&#20026;&#28145;&#24230;&#28508;&#22312;&#21147;&#27169;&#22411;(DLFM)&#65292;&#23427;&#26159;&#19968;&#20010;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#65292;&#27599;&#20010;&#23618;&#27425;&#37117;&#20855;&#26377;&#29289;&#29702;&#20449;&#24687;&#26680;&#65292;&#20351;&#29992;&#36807;&#31243;&#21367;&#31215;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#25512;&#23548;&#20986;&#12290;&#25552;&#20986;&#20102;DLFM&#30340;&#20004;&#31181;&#19981;&#21516;&#24418;&#24335;&#65292;&#23427;&#20204;&#21033;&#29992;&#22522;&#20110;&#26435;&#31354;&#38388;&#21644;&#22522;&#20110;&#21464;&#20998;&#24863;&#24212;&#28857;&#30340;&#39640;&#26031;&#36807;&#31243;&#36817;&#20284;&#26041;&#27861;&#65292;&#37117;&#36866;&#29992;&#20110;&#21452;&#37325;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;DLFM&#25429;&#25417;&#39640;&#24230;&#38750;&#32447;&#24615;&#23454;&#38469;&#22810;&#36755;&#20986;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#21160;&#24577;&#24615;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;DLFM&#33021;&#22815;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#19982;&#19968;&#31995;&#21015;&#38750;&#29289;&#29702;&#32508;&#21512;&#27010;&#29575;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling the behaviour of highly nonlinear dynamical systems with robust uncertainty quantification is a challenging task which typically requires approaches specifically designed to address the problem at hand. We introduce a domain-agnostic model to address this issue termed the deep latent force model (DLFM), a deep Gaussian process with physics-informed kernels at each layer, derived from ordinary differential equations using the framework of process convolutions. Two distinct formulations of the DLFM are presented which utilise weight-space and variational inducing points-based Gaussian process approximations, both of which are amenable to doubly stochastic variational inference. We present empirical evidence of the capability of the DLFM to capture the dynamics present in highly nonlinear real-world multi-output time series data. Additionally, we find that the DLFM is capable of achieving comparable performance to a range of non-physics-informed probabilistic models on benchmark
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#22870;&#21169;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24230;&#35780;&#20272;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22870;&#21169;&#27169;&#22411;&#23545;&#20110;&#25552;&#31034;&#21644;&#21709;&#24212;&#30340;&#36716;&#31227;&#20855;&#26377;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#21576;&#29616;&#20986;&#26032;&#39062;&#30340;&#26657;&#20934;&#27169;&#24335;&#21644;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#21516;&#26102;&#65292;&#23558;&#24120;&#29992;&#30340;OOD&#26816;&#27979;&#25216;&#26415;&#24341;&#20837;&#21040;&#22870;&#21169;&#27169;&#22411;&#35774;&#32622;&#20013;&#65292;&#29992;&#20110;&#26816;&#27979;&#20998;&#24067;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2311.14743</link><description>&lt;p&gt;
&#22522;&#20934;&#20998;&#26512;&#22870;&#21169;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#20934;&#30830;&#20998;&#26512;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v7 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#22870;&#21169;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24230;&#35780;&#20272;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22870;&#21169;&#27169;&#22411;&#23545;&#20110;&#25552;&#31034;&#21644;&#21709;&#24212;&#30340;&#36716;&#31227;&#20855;&#26377;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#21576;&#29616;&#20986;&#26032;&#39062;&#30340;&#26657;&#20934;&#27169;&#24335;&#21644;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#21516;&#26102;&#65292;&#23558;&#24120;&#29992;&#30340;OOD&#26816;&#27979;&#25216;&#26415;&#24341;&#20837;&#21040;&#22870;&#21169;&#27169;&#22411;&#35774;&#32622;&#20013;&#65292;&#29992;&#20110;&#26816;&#27979;&#20998;&#24067;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#21644;&#24212;&#29992;&#12290;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#21253;&#25324;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#26469;&#25429;&#25417;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#28982;&#21518;&#29992;&#20110;&#23545;&#40784;LLM&#12290;&#36825;&#20123;&#22870;&#21169;&#27169;&#22411;&#36824;&#22312;&#25512;&#26029;&#26102;&#29992;&#20110;&#20272;&#35745;LLM&#21709;&#24212;&#19982;&#26399;&#26395;&#34892;&#20026;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#26469;&#34913;&#37327;&#36825;&#20123;&#22870;&#21169;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36890;&#36807;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24230;&#65288;&#21363;&#20934;&#30830;&#24615;&#21644;&#20449;&#24515;&#30340;&#21305;&#37197;&#31243;&#24230;&#65289;&#34913;&#37327;&#30340;&#22870;&#21169;&#27169;&#22411;&#24615;&#33021;&#22914;&#20309;&#21463;&#21040;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#20110;OOD&#25552;&#31034;&#21644;&#21709;&#24212;&#32780;&#20135;&#29983;&#30340;&#26032;&#22411;&#26657;&#20934;&#27169;&#24335;&#21644;&#20934;&#30830;&#24615;&#19979;&#38477;&#65292;&#24182;&#19988;&#21457;&#29616;&#22870;&#21169;&#27169;&#22411;&#23545;&#21709;&#24212;&#30340;&#36716;&#31227;&#27604;&#25552;&#31034;&#26356;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#24120;&#29992;&#20110;&#20998;&#31867;&#30340;OOD&#26816;&#27979;&#25216;&#26415;&#36866;&#24212;&#21040;&#22870;&#21169;&#27169;&#22411;&#35774;&#32622;&#20013;&#65292;&#20197;&#26816;&#27979;&#36825;&#20123;&#25552;&#31034;&#21644;&#21709;&#24212;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models, specifically Large Language Models (LLMs), have lately gained wide-spread attention and adoption. Reinforcement Learning with Human Feedback (RLHF) involves training a reward model to capture desired behaviors, which is then used to align LLM's. These reward models are additionally used at inference-time to estimate LLM responses' adherence to those desired behaviors. However, there is little work measuring how robust these reward models are to distribution shifts. In this work, we evaluate how reward model performance measured via accuracy and calibration (i.e. alignment between accuracy and confidence) - is affected by distribution shift. We show novel calibration patterns and accuracy drops due to OOD prompts and responses, and that the reward model is more sensitive to shifts in responses than prompts. Additionally, we adapt an OOD detection technique commonly used in classification to the reward model setting to detect these distribution shifts in prompts and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#26469;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.13541</link><description>&lt;p&gt;
&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#19982;&#26080;&#20559;&#38598;&#20013;&#21147;
&lt;/p&gt;
&lt;p&gt;
Linear Log-Normal Attention with Unbiased Concentration. (arXiv:2311.13541v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#26469;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#19982;&#24207;&#21015;&#38271;&#24230;&#30340;&#20108;&#27425;&#20851;&#31995;&#65292;&#20854;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#24403;&#22788;&#29702;&#38271;&#25991;&#26723;&#25110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#26102;&#65292;&#36825;&#19968;&#38480;&#21046;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#65292;&#23545;&#33258;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34913;&#37327;&#36825;&#20123;&#25968;&#37327;&#30340;&#24037;&#20855;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#65292;&#26088;&#22312;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#24120;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#20248;&#20110;&#20854;&#20182;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26367;&#20195;&#26041;&#27861;&#65292;&#20026;&#22686;&#24378;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#38468;&#22312;&#34917;&#20805;&#26448;&#26009;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models. Our code is available in supplementary
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#34394;&#20551;&#25968;&#25454;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#30005;&#21270;&#23398;&#20013;&#30340;&#24433;&#21709;&#12290;&#21457;&#29616;&#32447;&#24615;&#27169;&#22411;&#22312;&#22788;&#29702;&#22122;&#22768;&#26041;&#38754;&#25928;&#26524;&#22909;&#65292;&#32780;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#22312;&#22122;&#22768;&#22788;&#29702;&#26041;&#38754;&#25928;&#26524;&#36739;&#24046;&#65292;&#20294;&#33021;&#25552;&#20379;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.10795</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#30005;&#21270;&#23398;&#20013;&#22914;&#20309;&#21463;&#21040;&#34394;&#20551;&#25968;&#25454;&#30340;&#24433;&#21709;&#65311;
&lt;/p&gt;
&lt;p&gt;
How False Data Affects Machine Learning Models in Electrochemistry?. (arXiv:2311.10795v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.10795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#34394;&#20551;&#25968;&#25454;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#30005;&#21270;&#23398;&#20013;&#30340;&#24433;&#21709;&#12290;&#21457;&#29616;&#32447;&#24615;&#27169;&#22411;&#22312;&#22788;&#29702;&#22122;&#22768;&#26041;&#38754;&#25928;&#26524;&#22909;&#65292;&#32780;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#22312;&#22122;&#22768;&#22788;&#29702;&#26041;&#38754;&#25928;&#26524;&#36739;&#24046;&#65292;&#20294;&#33021;&#25552;&#20379;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20165;&#22522;&#20110;&#25968;&#25454;&#20998;&#24067;&#32780;&#19981;&#32771;&#34385;&#25968;&#25454;&#22122;&#22768;&#36873;&#21462;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21306;&#20998;&#21738;&#20123;&#27169;&#22411;&#22312;&#22122;&#22768;&#25968;&#25454;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#30830;&#23450;&#22534;&#21472;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#21542;&#30830;&#23454;&#20026;&#21407;&#26412;&#23545;&#22122;&#22768;&#25935;&#24863;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;12&#20010;&#29420;&#31435;&#27169;&#22411;&#21644;&#22534;&#21472;&#27169;&#22411;&#23545;&#30005;&#21270;&#23398;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#65292;&#21253;&#25324;XGB&#12289;LGBM&#12289;RF&#12289;GB&#12289;ADA&#12289;NN&#12289;ELAS&#12289;LASS&#12289;RIDGE&#12289;SVM&#12289;KNN&#12289;DT&#21644;&#22534;&#21472;&#27169;&#22411;&#12290;&#21457;&#29616;&#32447;&#24615;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#22788;&#29702;&#22122;&#22768;&#65292;&#24179;&#22343;&#35823;&#24046;&#20026;1.75 F g-1&#65292;&#22312;&#28155;&#21152;100%&#22122;&#22768;&#26102;&#35823;&#24046;&#26368;&#23567;&#65292;&#24179;&#22343;&#20026;60.19 F g-1&#12290;&#32780;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#22312;&#22122;&#22768;&#22788;&#29702;&#26041;&#38754;&#22833;&#36133;&#65288;100%&#22122;&#22768;&#26102;&#24179;&#22343;&#26012;&#29575;&#20026;55.24 F g-1&#65289;&#65292;&#20294;&#23427;&#21487;&#20197;&#25552;&#20379;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65288;&#26368;&#20302;&#35823;&#24046;&#20026;23.9 F g-1&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the selection of machine learning model based on only the data distribution without concerning the noise of the data. This study aims to distinguish, which models perform well under noisy data, and establish whether stacking machine learning models actually provide robustness to otherwise weak-to-noise models. The electrochemical data were tested with 12 standalone models and stacking model. This includes XGB, LGBM, RF, GB, ADA, NN, ELAS, LASS, RIDGE, SVM, KNN, DT, and the stacking model. It is found that linear models handle noise well with the average error of (slope) to 1.75 F g-1 up to error per 100% percent noise added; but it suffers from prediction accuracy due to having an average of 60.19 F g-1 estimated at minimal error at 0% noise added. Tree-based models fail in terms of noise handling (average slope is 55.24 F g-1 at 100% percent noise), but it can provide higher prediction accuracy (lowest error of 23.9 F g-1) than that of linear. To address the controversial be
&lt;/p&gt;</description></item><item><title>UMedNeRF&#26159;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21333;&#35270;&#35282;&#20307;&#31215;&#28210;&#26579;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;2D X&#23556;&#32447;&#22270;&#20687;&#20013;&#23398;&#20064;CT&#25237;&#24433;&#30340;&#36830;&#32493;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#25439;&#22833;&#26435;&#37325;&#30830;&#20445;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.05836</link><description>&lt;p&gt;
UMedNeRF&#65306;&#38024;&#23545;&#21307;&#23398;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21333;&#35270;&#35282;&#20307;&#31215;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;
UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical Neural Radiance Fields. (arXiv:2311.05836v5 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.05836
&lt;/p&gt;
&lt;p&gt;
UMedNeRF&#26159;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21333;&#35270;&#35282;&#20307;&#31215;&#28210;&#26579;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;2D X&#23556;&#32447;&#22270;&#20687;&#20013;&#23398;&#20064;CT&#25237;&#24433;&#30340;&#36830;&#32493;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#25439;&#22833;&#26435;&#37325;&#30830;&#20445;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#21307;&#23398;&#39046;&#22495;&#65292;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#35786;&#26029;&#21508;&#31181;&#30142;&#30149;&#30340;&#26377;&#25928;&#21307;&#23398;&#24433;&#20687;&#27169;&#24577;&#12290;&#19982;X&#23556;&#32447;&#22270;&#20687;&#30456;&#27604;&#65292;CT&#22270;&#20687;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#65292;&#21253;&#25324;&#22810;&#24179;&#38754;&#20999;&#29255;&#21644;&#19977;&#32500;&#32467;&#26500;&#65292;&#29992;&#20110;&#20020;&#24202;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;CT&#25104;&#20687;&#38656;&#35201;&#24739;&#32773;&#38271;&#26102;&#38388;&#26292;&#38706;&#20110;&#22823;&#21058;&#37327;&#30340;&#30005;&#31163;&#36752;&#23556;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#21487;&#36870;&#30340;&#36523;&#20307;&#25439;&#20260;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#30340;&#36752;&#23556;&#22330;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;MedNeRF&#65288;UMedNeRF&#65289;&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#33719;&#21462;&#20869;&#37096;&#32467;&#26500;&#21644;&#28145;&#24230;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#25439;&#22833;&#26435;&#37325;&#26469;&#30830;&#20445;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#23398;&#20064;2D X&#23556;&#32447;&#22270;&#20687;&#30340;CT&#25237;&#24433;&#30340;&#36830;&#32493;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#33181;&#30422;&#21644;&#33016;&#37096;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#21333;&#20010;X&#23556;&#32447;&#36827;&#34892;CT&#25237;&#24433;&#28210;&#26579;&#30340;&#32467;&#26524;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22522;&#20110;&#29983;&#25104;&#30340;&#36752;&#23556;&#22330;&#30340;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of clinical medicine, computed tomography (CT) is an effective medical imaging modality for the diagnosis of various pathologies. Compared with X-ray images, CT images can provide more information, including multi-planar slices and three-dimensional structures for clinical diagnosis. However, CT imaging requires patients to be exposed to large doses of ionizing radiation for a long time, which may cause irreversible physical harm. In this paper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on generated radiation fields. The network can learn a continuous representation of CT projections from 2D X-ray images by obtaining the internal structure and depth information and using adaptive loss weights to ensure the quality of the generated images. Our model is trained on publicly available knee and chest datasets, and we show the results of CT projection rendering with a single X-ray and compare our method with other methods based on generated radiation field
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#39640;&#25928;&#36817;&#20284;&#26041;&#27861;&#65292;&#31216;&#20026;&#20849;&#36717;&#26680;&#65288;CK&#65289;&#65292;&#29992;&#20110;&#26367;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#19982;NTK&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.18612</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#30340;&#39640;&#25928;&#26680;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient kernel surrogates for neural network-based regression. (arXiv:2310.18612v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#39640;&#25928;&#36817;&#20284;&#26041;&#27861;&#65292;&#31216;&#20026;&#20849;&#36717;&#26680;&#65288;CK&#65289;&#65292;&#29992;&#20110;&#26367;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#19982;NTK&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#25191;&#34892;&#21508;&#31181;&#23398;&#20064;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#33267;&#20170;&#20173;&#26080;&#27861;&#29702;&#35299;&#20854;&#23616;&#38480;&#24615;&#12290;&#36825;&#37096;&#20998;&#26159;&#30001;&#20110;&#26080;&#27861;&#30830;&#23450;&#23398;&#20064;&#20989;&#25968;&#30340;&#38381;&#21512;&#24418;&#24335;&#65292;&#36825;&#20351;&#24471;&#22312;&#26410;&#35265;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#23427;&#20204;&#30340;&#27867;&#21270;&#23646;&#24615;&#26356;&#21152;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#19979;&#65292;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20250;&#25910;&#25947;&#21040;&#20381;&#36182;&#20110;&#24050;&#30693;&#38381;&#21512;&#24418;&#24335;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;Neural Tangent Kernel&#65292;NTK&#65289;&#30340;&#26680;&#26426;&#22120;&#12290;&#36825;&#20123;&#32467;&#26524;&#26263;&#31034;&#24182;&#24471;&#21040;&#23454;&#39564;&#35777;&#25454;&#35777;&#26126;&#65292;&#32463;&#39564;&#26680;&#26426;&#22120;&#20063;&#21487;&#20197;&#20316;&#20026;&#26377;&#38480;&#23485;&#24230;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#21697;&#12290;&#28982;&#32780;&#65292;&#32452;&#35013;&#23436;&#25972;&#30340;NTK&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;&#27492;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#19981;&#21487;&#34892;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#38656;&#35201;&#20302;&#25104;&#26412;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;&#22312;&#24403;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20849;&#36717;&#26680;&#65288;Conjugate Kernel&#65292;CK&#65289;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;NTK&#30340;&#39640;&#25928;&#36817;&#20284;&#65292;&#25454;&#35266;&#23519;&#65292;&#23427;&#33021;&#20135;&#29983;&#30456;&#24403;&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their immense promise in performing a variety of learning tasks, a theoretical understanding of the limitations of Deep Neural Networks (DNNs) has so far eluded practitioners. This is partly due to the inability to determine the closed forms of the learned functions, making it harder to study their generalization properties on unseen datasets. Recent work has shown that randomly initialized DNNs in the infinite width limit converge to kernel machines relying on a Neural Tangent Kernel (NTK) with known closed form. These results suggest, and experimental evidence corroborates, that empirical kernel machines can also act as surrogates for finite width DNNs. The high computational cost of assembling the full NTK, however, makes this approach infeasible in practice, motivating the need for low-cost approximations. In the current work, we study the performance of the Conjugate Kernel (CK), an efficient approximation to the NTK that has been observed to yield fairly similar results. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#34394;&#25311;&#20154;&#24037;&#26234;&#33021;&#25945;&#24072;&#22312;&#27169;&#20223;&#20154;&#31867;&#25945;&#32946;&#32773;&#25216;&#26415;&#26041;&#38754;&#22312;&#36816;&#21160;&#25216;&#33021;&#20064;&#24471;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#22235;&#20010;&#25351;&#23548;&#24615;&#20551;&#35774;&#65292;&#35777;&#23454;&#20102;&#22312;&#21512;&#25104;&#23398;&#20064;&#32773;&#19978;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.10280</link><description>&lt;p&gt;
&#27169;&#20223;&#22823;&#24072;&#65306;&#25506;&#32034;&#34394;&#25311;&#20154;&#24037;&#26234;&#33021;&#25945;&#24072;&#22312;&#31934;&#32454;&#36816;&#21160;&#25216;&#33021;&#20064;&#24471;&#20013;&#30340;&#25928;&#33021;
&lt;/p&gt;
&lt;p&gt;
Mimicking the Maestro: Exploring the Efficacy of a Virtual AI Teacher in Fine Motor Skill Acquisition. (arXiv:2310.10280v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#34394;&#25311;&#20154;&#24037;&#26234;&#33021;&#25945;&#24072;&#22312;&#27169;&#20223;&#20154;&#31867;&#25945;&#32946;&#32773;&#25216;&#26415;&#26041;&#38754;&#22312;&#36816;&#21160;&#25216;&#33021;&#20064;&#24471;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#22235;&#20010;&#25351;&#23548;&#24615;&#20551;&#35774;&#65292;&#35777;&#23454;&#20102;&#22312;&#21512;&#25104;&#23398;&#20064;&#32773;&#19978;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#36816;&#21160;&#25216;&#33021;&#65292;&#23588;&#20854;&#26159;&#20070;&#20889;&#31561;&#25216;&#33021;&#65292;&#22312;&#23398;&#26415;&#21644;&#26085;&#24120;&#29983;&#27963;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;&#25945;&#23398;&#26041;&#27861;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#32791;&#26102;&#19988;&#19981;&#19968;&#33268;&#12290;&#38543;&#30528;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#31561;&#20808;&#36827;&#25216;&#26415;&#30340;&#20852;&#36215;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#20851;&#27880;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#33258;&#21160;&#21270;&#25945;&#23398;&#36807;&#31243;&#65292;&#36890;&#36807;&#20154;&#26426;&#21644;&#20154;&#26426;&#20132;&#20114;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#34394;&#25311;&#20154;&#24037;&#26234;&#33021;&#25945;&#24072;&#22312;&#27169;&#20223;&#20154;&#31867;&#25945;&#32946;&#32773;&#25216;&#26415;&#26041;&#38754;&#23545;&#20110;&#36816;&#21160;&#25216;&#33021;&#20064;&#24471;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25429;&#25417;&#21040;&#20154;&#31867;&#25945;&#24072;&#29420;&#29305;&#29305;&#24449;&#30340;&#20154;&#24037;&#26234;&#33021;&#25945;&#24072;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#36866;&#24212;&#20110;&#27169;&#20223;&#25945;&#24072;-&#23398;&#20064;&#32773;&#20132;&#20114;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#27979;&#35797;&#25105;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#22235;&#20010;&#25351;&#23548;&#24615;&#20551;&#35774;&#65292;&#24378;&#35843;&#23398;&#20064;&#32773;&#34920;&#29616;&#30340;&#25552;&#21319;&#12289;&#25216;&#33021;&#20064;&#24471;&#36895;&#24230;&#30340;&#22686;&#24378;&#20197;&#21450;&#23398;&#20064;&#32467;&#26524;&#30340;&#21464;&#24322;&#24615;&#30340;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#22312;&#21512;&#25104;&#23398;&#20064;&#32773;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motor skills, especially fine motor skills like handwriting, play an essential role in academic pursuits and everyday life. Traditional methods to teach these skills, although effective, can be time-consuming and inconsistent. With the rise of advanced technologies like robotics and artificial intelligence, there is increasing interest in automating such teaching processes using these technologies, via human-robot and human-computer interactions. In this study, we examine the potential of a virtual AI teacher in emulating the techniques of human educators for motor skill acquisition. We introduce an AI teacher model that captures the distinct characteristics of human instructors. Using a Reinforcement Learning environment tailored to mimic teacher-learner interactions, we tested our AI model against four guiding hypotheses, emphasizing improved learner performance, enhanced rate of skill acquisition, and reduced variability in learning outcomes. Our findings, validated on synthetic lea
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;MODIS&#22810;&#20809;&#35889;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;LULC&#31867;&#21035;&#30340;&#30450;&#30446;&#20809;&#35889;&#20998;&#31163;&#12290;&#36890;&#36807;&#28155;&#21152;&#22320;&#29702;&#21152;&#22320;&#24418;&#21644;&#27668;&#20505;&#36741;&#21161;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07223</link><description>&lt;p&gt;
&#29992;MODIS&#22810;&#20809;&#35889;&#26102;&#38388;&#24207;&#21015;&#21644;&#36741;&#21161;&#25968;&#25454;&#36827;&#34892;&#30450;&#30446;&#20809;&#35889;&#20998;&#31163;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for blind spectral unmixing of LULC classes with MODIS multispectral time series and ancillary data. (arXiv:2310.07223v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07223
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;MODIS&#22810;&#20809;&#35889;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;LULC&#31867;&#21035;&#30340;&#30450;&#30446;&#20809;&#35889;&#20998;&#31163;&#12290;&#36890;&#36807;&#28155;&#21152;&#22320;&#29702;&#21152;&#22320;&#24418;&#21644;&#27668;&#20505;&#36741;&#21161;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#25968;&#25454;&#20013;LULC&#31867;&#21035;&#36890;&#24120;&#23384;&#22312;&#28151;&#21512;&#24773;&#20917;&#65292;&#20809;&#35889;&#20998;&#31163;&#26159;&#19968;&#31181;&#20174;&#28151;&#21512;&#20687;&#32032;&#20013;&#25552;&#21462;&#20449;&#24687;&#21040;&#20854;&#32452;&#25104;LULC&#31867;&#22411;&#21644;&#30456;&#24212;&#20016;&#24230;&#20998;&#25968;&#30340;&#25216;&#26415;&#12290;&#20256;&#32479;&#19978;&#65292;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#35201;&#20040;&#20381;&#36182;&#20110;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#32463;&#20856;&#26041;&#27861;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#36991;&#20813;&#26174;&#24335;&#25104;&#20998;&#35745;&#31639;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20063;&#23601;&#26159;&#30450;&#30446;&#20809;&#35889;&#20998;&#31163;&#65288;BSU&#65289;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;BSU&#30740;&#31350;&#20391;&#37325;&#20110;&#21333;&#20010;&#26102;&#38388;&#27493;&#30340;&#39640;&#20809;&#35889;&#25968;&#25454;&#65292;&#28982;&#32780;&#19982;&#22810;&#20809;&#35889;&#25968;&#25454;&#30456;&#27604;&#65292;&#20854;&#33719;&#21462;&#25104;&#26412;&#20173;&#28982;&#30456;&#24403;&#39640;&#26114;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20379;&#20102;&#31532;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#22810;&#20809;&#35889;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;DL&#27169;&#22411;&#36827;&#34892;LULC&#31867;&#21035;&#30340;BSU&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#28155;&#21152;&#22320;&#29702;&#21152;&#22320;&#24418;&#65288;geo-topographic&#65289;&#21644;&#27668;&#20505;&#36741;&#21161;&#20449;&#24687;&#26469;&#25552;&#21319;&#22522;&#20110;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20809;&#35889;&#26102;&#38388;&#36755;&#20837;&#25968;&#25454;&#19982;&#22320;&#29702;&#26102;&#24577;&#25968;&#25454;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remotely sensed data are dominated by mixed Land Use and Land Cover (LULC) types. Spectral unmixing is a technique to extract information from mixed pixels into their constituent LULC types and corresponding abundance fractions. Traditionally, solving this task has relied on either classical methods that require prior knowledge of endmembers or machine learning methods that avoid explicit endmembers calculation, also known as blind spectral unmixing (BSU). Most BSU studies based on Deep Learning (DL) focus on one time-step hyperspectral data, yet its acquisition remains quite costly compared with multispectral data. To our knowledge, here we provide the first study on BSU of LULC classes using multispectral time series data with DL models. We further boost the performance of a Long-Short Term Memory (LSTM)-based model by incorporating geographic plus topographic (geo-topographic) and climatic ancillary information. Our experiments show that combining spectral-temporal input data togeth
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19968;&#33268;&#24615;&#31574;&#30053;&#19982;Q-Learning &#65288;CPQL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20174;&#21453;&#21521;&#25193;&#25955;&#36712;&#36857;&#21040;&#26399;&#26395;&#31574;&#30053;&#30340;&#26144;&#23556;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#26041;&#27861;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#20934;&#30830;&#25351;&#23548;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06343</link><description>&lt;p&gt;
&#29992;&#19968;&#33268;&#24615;&#31574;&#30053;&#25552;&#21319;&#36830;&#32493;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Boosting Continuous Control with Consistency Policy. (arXiv:2310.06343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06343
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19968;&#33268;&#24615;&#31574;&#30053;&#19982;Q-Learning &#65288;CPQL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20174;&#21453;&#21521;&#25193;&#25955;&#36712;&#36857;&#21040;&#26399;&#26395;&#31574;&#30053;&#30340;&#26144;&#23556;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#26041;&#27861;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#20934;&#30830;&#25351;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#24378;&#22823;&#34920;&#36798;&#33021;&#21147;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20063;&#24102;&#26469;&#20102;&#20960;&#20010;&#25361;&#25112;&#65306;1&#65289;&#23545;&#22823;&#37327;&#25193;&#25955;&#27493;&#39588;&#30340;&#38656;&#27714;&#20351;&#24471;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#23454;&#26102;&#25511;&#21046;&#20013;&#25928;&#29575;&#20302;&#19979;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#65307;2&#65289;&#22914;&#20309;&#25552;&#20379;&#20934;&#30830;&#25351;&#23548;&#20197;&#23454;&#29616;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31574;&#30053;&#25913;&#36827;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#21463;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#25928;&#29575;&#26041;&#27861;&#65292;&#31216;&#20026;&#19968;&#33268;&#24615;&#31574;&#30053;&#19982;Q-Learning&#65288;CPQL&#65289;&#65292;&#23427;&#36890;&#36807;&#21333;&#27493;&#20174;&#22122;&#22768;&#20013;&#23548;&#20986;&#21160;&#20316;&#12290;&#36890;&#36807;&#24314;&#31435;&#20174;&#21453;&#21521;&#25193;&#25955;&#36712;&#36857;&#21040;&#26399;&#26395;&#31574;&#30053;&#30340;&#26144;&#23556;&#65292;&#25105;&#20204;&#21516;&#26102;&#35299;&#20915;&#20102;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;Q&#20989;&#25968;&#26356;&#26032;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31574;&#30053;&#26102;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#20934;&#30830;&#25351;&#23548;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CPQL&#21487;&#20197;&#36890;&#36807;&#20934;&#30830;&#25351;&#23548;&#23454;&#29616;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to its training stability and strong expression, the diffusion model has attracted considerable attention in offline reinforcement learning. However, several challenges have also come with it: 1) The demand for a large number of diffusion steps makes the diffusion-model-based methods time inefficient and limits their applications in real-time control; 2) How to achieve policy improvement with accurate guidance for diffusion model-based policy is still an open problem. Inspired by the consistency model, we propose a novel time-efficiency method named Consistency Policy with Q-Learning (CPQL), which derives action from noise by a single step. By establishing a mapping from the reverse diffusion trajectories to the desired policy, we simultaneously address the issues of time efficiency and inaccurate guidance when updating diffusion model-based policy with the learned Q-function. We demonstrate that CPQL can achieve policy improvement with accurate guidance for offline reinforcement l
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36716;&#31227;&#21453;&#20107;&#23454;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#20572;&#30005;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#30005;&#21147;&#26381;&#21153;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#20302;&#25910;&#20837;&#21644;&#32769;&#24180;&#20154;&#21475;&#21306;&#22495;&#32463;&#24120;&#36973;&#21463;&#36739;&#38271;&#30340;&#20572;&#30005;&#26102;&#38388;&#65292;&#25581;&#31034;&#20102;&#30005;&#21147;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#23384;&#22312;&#24182;&#24378;&#35843;&#20102;&#25913;&#21892;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.03258</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#35268;&#27169;&#20572;&#30005;&#25968;&#25454;&#38598;&#19978;&#30340;&#36716;&#31227;&#21453;&#20107;&#23454;&#23398;&#20064;&#65292;&#26816;&#27979;&#30005;&#21147;&#26381;&#21153;&#20844;&#24179;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Detecting Electricity Service Equity Issues with Transfer Counterfactual Learning on Large-Scale Outage Datasets. (arXiv:2310.03258v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03258
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#21453;&#20107;&#23454;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#20572;&#30005;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#30005;&#21147;&#26381;&#21153;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#20302;&#25910;&#20837;&#21644;&#32769;&#24180;&#20154;&#21475;&#21306;&#22495;&#32463;&#24120;&#36973;&#21463;&#36739;&#38271;&#30340;&#20572;&#30005;&#26102;&#38388;&#65292;&#25581;&#31034;&#20102;&#30005;&#21147;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#23384;&#22312;&#24182;&#24378;&#35843;&#20102;&#25913;&#21892;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#20844;&#27491;&#26159;&#36328;&#23398;&#31185;&#33021;&#28304;&#30740;&#31350;&#30340;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28151;&#28102;&#21464;&#37327;&#12289;&#27835;&#30103;&#25928;&#24212;&#30340;&#22797;&#26434;&#24322;&#36136;&#24615;&#21644;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#35782;&#21035;&#33021;&#28304;&#37096;&#38376;&#20013;&#30340;&#31995;&#32479;&#20559;&#35265;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22260;&#32469;&#33021;&#28304;&#20844;&#27491;&#30340;&#21453;&#20107;&#23454;&#22240;&#26524;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#23376;&#32452;&#20998;&#26512;&#26469;&#31649;&#29702;&#21508;&#31181;&#22240;&#32032;&#65292;&#24182;&#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#30340;&#24605;&#24819;&#26469;&#32531;&#35299;&#27599;&#20010;&#23376;&#32452;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#25968;&#20540;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23458;&#25143;&#32423;&#20572;&#30005;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20154;&#21475;&#22240;&#32032;&#65288;&#22914;&#25910;&#20837;&#21644;&#24180;&#40836;&#65289;&#23545;&#20572;&#30005;&#25345;&#32493;&#26102;&#38388;&#30340;&#21453;&#20107;&#23454;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20302;&#25910;&#20837;&#21644;&#32769;&#24180;&#20154;&#21475;&#21306;&#22495;&#24635;&#26159;&#32463;&#21382;&#36739;&#38271;&#30340;&#20572;&#30005;&#26102;&#38388;&#65292;&#26080;&#35770;&#22825;&#27668;&#26465;&#20214;&#22914;&#20309;&#12290;&#36825;&#34920;&#26126;&#30005;&#21147;&#31995;&#32479;&#23384;&#22312;&#20559;&#35265;&#65292;&#24182;&#24378;&#35843;&#20102;&#19987;&#27880;&#25913;&#21892;&#36825;&#20123;&#38382;&#39064;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy justice is a growing area of interest in interdisciplinary energy research. However, identifying systematic biases in the energy sector remains challenging due to confounding variables, intricate heterogeneity in treatment effects, and limited data availability. To address these challenges, we introduce a novel approach for counterfactual causal analysis centered on energy justice. We use subgroup analysis to manage diverse factors and leverage the idea of transfer learning to mitigate data scarcity in each subgroup. In our numerical analysis, we apply our method to a large-scale customer-level power outage data set and investigate the counterfactual effect of demographic factors, such as income and age of the population, on power outage durations. Our results indicate that low-income and elderly-populated areas consistently experience longer power outages, regardless of weather conditions. This points to existing biases in the power system and highlights the need for focused im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#23545;&#25239;&#35266;&#23519;&#27169;&#20223;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31163;&#31574;&#30053;&#23545;&#25239;&#23398;&#20064;&#25216;&#26415;&#21644;&#20174;&#35266;&#23519;&#24207;&#21015;&#20013;&#23398;&#20064;&#30340;&#20195;&#29702;&#29366;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#31639;&#27861;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2309.17371</link><description>&lt;p&gt;
&#21033;&#29992;&#28508;&#22312;&#20449;&#24687;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#36827;&#34892;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adversarial Imitation Learning from Visual Observations using Latent Information. (arXiv:2309.17371v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#23545;&#25239;&#35266;&#23519;&#27169;&#20223;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31163;&#31574;&#30053;&#23545;&#25239;&#23398;&#20064;&#25216;&#26415;&#21644;&#20174;&#35266;&#23519;&#24207;&#21015;&#20013;&#23398;&#20064;&#30340;&#20195;&#29702;&#29366;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#31639;&#27861;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#19987;&#27880;&#20110;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#23398;&#20064;&#20195;&#29702;&#21482;&#33021;&#35775;&#38382;&#19987;&#23478;&#30340;&#35270;&#39057;&#20316;&#20026;&#20854;&#21807;&#19968;&#30340;&#23398;&#20064;&#28304;&#12290;&#36825;&#20010;&#26694;&#26550;&#30340;&#25361;&#25112;&#21253;&#25324;&#32570;&#20047;&#19987;&#23478;&#30340;&#21160;&#20316;&#21644;&#29615;&#22659;&#30340;&#23616;&#37096;&#21487;&#35266;&#27979;&#24615;&#65292;&#22240;&#20026;&#22320;&#38754;&#30495;&#23454;&#29366;&#24577;&#21482;&#33021;&#20174;&#20687;&#32032;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#19987;&#23478;&#21644;&#20195;&#29702;&#28508;&#22312;&#29366;&#24577;&#36716;&#25442;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#24230;&#19978;&#24314;&#31435;&#20102;&#23398;&#20064;&#20195;&#29702;&#23376;&#20248;&#24230;&#30340;&#19978;&#30028;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#23545;&#25239;&#35266;&#23519;&#27169;&#20223;&#30340;&#31639;&#27861;&#65292;&#23427;&#23558;&#31163;&#31574;&#30053;&#23545;&#25239;&#23398;&#20064;&#25216;&#26415;&#19982;&#20174;&#35266;&#23519;&#24207;&#21015;&#20013;&#23398;&#20064;&#30340;&#20195;&#29702;&#29366;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#30456;&#32467;&#21512;&#12290;&#22312;&#39640;&#32500;&#36830;&#32493;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the problem of imitation learning from visual observations, where the learning agent has access to videos of experts as its sole learning source. The challenges of this framework include the absence of expert actions and the partial observability of the environment, as the ground-truth states can only be inferred from pixels. To tackle this problem, we first conduct a theoretical analysis of imitation learning in partially observable environments. We establish upper bounds on the suboptimality of the learning agent with respect to the divergence between the expert and the agent latent state-transition distributions. Motivated by this analysis, we introduce an algorithm called Latent Adversarial Imitation from Observations, which combines off-policy adversarial imitation techniques with a learned latent representation of the agent's state from sequences of observations. In experiments on high-dimensional continuous robotic tasks, we show that our algorithm matches state-of-t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.17249</link><description>&lt;p&gt;
&#25209;&#37327;&#26657;&#20934;&#65306;&#37325;&#26032;&#24605;&#32771;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering. (arXiv:2309.17249v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#39640;&#25928;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;LLM&#23384;&#22312;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#21508;&#31181;&#20559;&#35265;&#22240;&#32032;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#26684;&#24335;&#12289;&#36873;&#25321;&#24615;&#30340;&#34920;&#36798;&#26041;&#24335;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#20363;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#26657;&#20934;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#30340;&#24433;&#21709;&#24182;&#24674;&#22797;LLM&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35266;&#28857;&#24182;&#25581;&#31034;&#20102;&#22833;&#36133;&#26696;&#20363;&#12290;&#21463;&#36825;&#20123;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#30452;&#35266;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25209;&#37327;&#36755;&#20837;&#20013;&#25511;&#21046;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#21508;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#36848;&#38382;&#39064;&#12290;BC&#26159;&#38646;-shot&#12289;&#20165;&#25512;&#29702;&#21644;&#39069;&#22806;&#25104;&#26412;&#21487;&#24573;&#30053;&#12290;&#22312;&#23569;-shot&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;BC&#20197;&#23454;&#29616;&#20840;&#37096;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allo
&lt;/p&gt;</description></item><item><title>Timbre-Trap&#26159;&#19968;&#20010;&#20302;&#36164;&#28304;&#26694;&#26550;&#65292;&#23558;&#38899;&#20048;&#36716;&#24405;&#21644;&#38899;&#39057;&#37325;&#26500;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#21033;&#29992;&#38899;&#39640;&#21644;&#38899;&#33394;&#30340;&#24378;&#20998;&#31163;&#24615;&#65292;&#21516;&#26102;&#20272;&#35745;&#38899;&#39640;&#26174;&#33879;&#24230;&#21644;&#37325;&#26500;&#39057;&#35889;&#31995;&#25968;&#65292;&#21462;&#24471;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15717</link><description>&lt;p&gt;
Timbre-Trap:&#19968;&#31181;&#20302;&#36164;&#28304;&#26694;&#26550;&#29992;&#20110;&#19982;&#20048;&#22120;&#26080;&#20851;&#30340;&#38899;&#20048;&#36716;&#24405;
&lt;/p&gt;
&lt;p&gt;
Timbre-Trap: A Low-Resource Framework for Instrument-Agnostic Music Transcription. (arXiv:2309.15717v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15717
&lt;/p&gt;
&lt;p&gt;
Timbre-Trap&#26159;&#19968;&#20010;&#20302;&#36164;&#28304;&#26694;&#26550;&#65292;&#23558;&#38899;&#20048;&#36716;&#24405;&#21644;&#38899;&#39057;&#37325;&#26500;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#21033;&#29992;&#38899;&#39640;&#21644;&#38899;&#33394;&#30340;&#24378;&#20998;&#31163;&#24615;&#65292;&#21516;&#26102;&#20272;&#35745;&#38899;&#39640;&#26174;&#33879;&#24230;&#21644;&#37325;&#26500;&#39057;&#35889;&#31995;&#25968;&#65292;&#21462;&#24471;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38899;&#20048;&#36716;&#24405;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26550;&#26500;&#35774;&#35745;&#21644;&#20048;&#22120;&#29305;&#23450;&#25968;&#25454;&#37319;&#38598;&#19978;&#12290;&#30001;&#20110;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#65292;&#36827;&#23637;&#36890;&#24120;&#20165;&#38480;&#20110;&#38050;&#29748;&#36716;&#24405;&#31561;&#21333;&#20048;&#22120;&#20219;&#21153;&#12290;&#19968;&#20123;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#20048;&#22120;&#36716;&#24405;&#20316;&#20026;&#19968;&#31181;&#22686;&#24378;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#20219;&#21153;&#19978;&#24615;&#33021;&#30340;&#25163;&#27573;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#21516;&#26679;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;Timbre-Trap&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#38899;&#39640;&#21644;&#38899;&#33394;&#20043;&#38388;&#30340;&#24378;&#20998;&#31163;&#24615;&#23558;&#38899;&#20048;&#36716;&#24405;&#21644;&#38899;&#39057;&#37325;&#26500;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;U-Net&#26469;&#21516;&#26102;&#20272;&#35745;&#38899;&#39640;&#26174;&#33879;&#24230;&#21644;&#37325;&#26500;&#22797;&#26434;&#30340;&#39057;&#35889;&#31995;&#25968;&#65292;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#20999;&#25442;&#26426;&#21046;&#22312;&#35299;&#30721;&#38454;&#27573;&#36873;&#25321;&#20004;&#32773;&#20043;&#19968;&#30340;&#36755;&#20986;&#12290;&#36825;&#26679;&#65292;&#27169;&#22411;&#23398;&#20250;&#20102;&#20135;&#29983;&#23545;&#24212;&#20110;&#27809;&#26377;&#38899;&#33394;&#30340;&#38899;&#39057;&#30340;&#31995;&#25968;&#65292;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#38899;&#39640;&#26174;&#33879;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#33021;&#22815;&#21462;&#24471;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, research on music transcription has focused mainly on architecture design and instrument-specific data acquisition. With the lack of availability of diverse datasets, progress is often limited to solo-instrument tasks such as piano transcription. Several works have explored multi-instrument transcription as a means to bolster the performance of models on low-resource tasks, but these methods face the same data availability issues. We propose Timbre-Trap, a novel framework which unifies music transcription and audio reconstruction by exploiting the strong separability between pitch and timbre. We train a single U-Net to simultaneously estimate pitch salience and reconstruct complex spectral coefficients, selecting between either output during the decoding stage via a simple switch mechanism. In this way, the model learns to produce coefficients corresponding to timbre-less audio, which can be interpreted as pitch salience. We demonstrate that the framework leads to perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;Softmax&#25513;&#30721;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#20445;&#25345;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#31283;&#23450;&#24615;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14808</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;Softmax&#25513;&#30721;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting Softmax Masking for Stability in Continual Learning. (arXiv:2309.14808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;Softmax&#25513;&#30721;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#20445;&#25345;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#31283;&#23450;&#24615;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#35768;&#22810;&#20998;&#31867;&#22120;&#20351;&#29992;Softmax&#20989;&#25968;&#26469;&#23398;&#20064;&#32622;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#25351;&#20986;&#20854;&#26080;&#27861;&#20934;&#30830;&#30830;&#23450;&#31163;&#32676;&#20540;&#30340;&#32622;&#20449;&#24230;&#20998;&#24067;&#65292;&#36890;&#24120;&#31216;&#20026;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#22266;&#26377;&#38480;&#21046;&#36824;&#38480;&#21046;&#20102;&#22312;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#36873;&#25321;&#20309;&#26102;&#24536;&#35760;&#21644;&#20445;&#30041;&#20808;&#21069;&#35757;&#32451;&#30340;&#32622;&#20449;&#24230;&#20998;&#24067;&#30340;&#20934;&#30830;&#20915;&#31574;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#25513;&#30721;Softmax&#20989;&#25968;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#26082;&#31616;&#21333;&#21448;&#26222;&#36941;&#65292;&#20294;&#23545;&#20110;&#22312;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#32622;&#20449;&#24230;&#20998;&#24067;&#65288;&#20063;&#31216;&#20026;&#31283;&#23450;&#24615;&#65289;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35843;&#26597;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;Softmax&#25513;&#30721;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#20445;&#25345;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;&#22312;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#35760;&#24518;&#37325;&#25918;&#30340;&#31867;-&#21644;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#22686;&#21152;&#20102;&#31283;&#23450;&#24615;&#21516;&#26102;&#20445;&#25345;&#20102;&#36275;&#22815;&#22823;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning, many classifiers use softmax function to learn confidence. However, numerous studies have pointed out its inability to accurately determine confidence distributions for outliers, often referred to as epistemic uncertainty. This inherent limitation also curtails the accurate decisions for selecting what to forget and keep in previously trained confidence distributions over continual learning process. To address the issue, we revisit the effects of masking softmax function. While this method is both simple and prevalent in literature, its implication for retaining confidence distribution during continual learning, also known as stability, has been under-investigated. In this paper, we revisit the impact of softmax masking, and introduce a methodology to utilize its confidence preservation effects. In class- and task-incremental learning benchmarks with and without memory replay, our approach significantly increases stability while maintaining sufficiently large pla
&lt;/p&gt;</description></item><item><title>Voxtlm&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#21482;&#35299;&#30721;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21512;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#35821;&#38899;&#24310;&#32493;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2309.07937</link><description>&lt;p&gt;
Voxtlm: &#32479;&#19968;&#30340;&#21482;&#35299;&#30721;&#27169;&#22411;&#65292;&#29992;&#20110;&#21512;&#24182;&#35821;&#38899;&#35782;&#21035;/&#21512;&#25104;&#21644;&#35821;&#38899;/&#25991;&#26412;&#34917;&#20805;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks. (arXiv:2309.07937v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07937
&lt;/p&gt;
&lt;p&gt;
Voxtlm&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#21482;&#35299;&#30721;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21512;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#35821;&#38899;&#24310;&#32493;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21482;&#35299;&#30721;&#35821;&#35328;&#27169;&#22411;VoxtLM&#65292;&#33021;&#22815;&#25191;&#34892;&#22235;&#20010;&#20219;&#21153;&#65306;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21512;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#35821;&#38899;&#24310;&#32493;&#12290;VoxtLM&#23558;&#25991;&#26412;&#35789;&#27719;&#19982;&#33258;&#30417;&#30563;&#35821;&#38899;&#29305;&#24449;&#20013;&#30340;&#31163;&#25955;&#35821;&#38899;&#20196;&#29260;&#36827;&#34892;&#25972;&#21512;&#65292;&#24182;&#20351;&#29992;&#29305;&#27530;&#20196;&#29260;&#23454;&#29616;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#19982;&#21333;&#20219;&#21153;&#27169;&#22411;&#30456;&#27604;&#65292;VoxtLM&#22312;&#35821;&#38899;&#21512;&#25104;&#26041;&#38754;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#65292;&#35821;&#38899;&#21487;&#29702;&#35299;&#24615;&#20174;28.9&#25552;&#39640;&#21040;5.6&#65292;&#23458;&#35266;&#36136;&#37327;&#20174;2.68&#25552;&#39640;&#21040;3.90&#12290;VoxtLM&#36824;&#25913;&#21892;&#20102;&#35821;&#38899;&#29983;&#25104;&#21644;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;VoxtLM&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;&#25552;&#20379;&#35757;&#32451;&#33050;&#26412;&#21644;&#27169;&#22411;&#26816;&#26597;&#28857;&#30340;&#24320;&#28304;&#20195;&#30721;&#65292;&#20197;&#23454;&#29616;&#23436;&#20840;&#21487;&#22797;&#29616;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. VoxtLM is trained with publicly available data and training recipes and model checkpoints will be open-sourced to make fully reproducible work.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26631;&#20934;&#21270;&#28508;&#21464;&#37327;&#21644;&#35843;&#25972;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Modified Barlow Twins (MBT) &#26041;&#27861;&#26469;&#25913;&#21892;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35821;&#38899;&#34920;&#31034;&#27867;&#21270;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#26377;&#38480;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#26102;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#21457;&#23637;&#21487;&#37325;&#29992;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2309.03619</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#21464;&#24615;&#21644;&#20887;&#20313;&#20943;&#23569;&#29702;&#35299;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35821;&#38899;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Understanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction. (arXiv:2309.03619v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03619
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26631;&#20934;&#21270;&#28508;&#21464;&#37327;&#21644;&#35843;&#25972;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Modified Barlow Twins (MBT) &#26041;&#27861;&#26469;&#25913;&#21892;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35821;&#38899;&#34920;&#31034;&#27867;&#21270;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#26377;&#38480;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#26102;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#21457;&#23637;&#21487;&#37325;&#29992;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#20989;&#25968;&#30340;&#36873;&#25321;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Barlow Twins (BT) &#30446;&#26631;&#30340;&#19981;&#21516;&#34920;&#36798;&#24418;&#24335;&#22914;&#20309;&#24433;&#21709;&#35821;&#38899;&#25968;&#25454;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#26631;&#20934;&#21270;&#28508;&#21464;&#37327;&#30340;Modified Barlow Twins (MBT) &#26469;&#24378;&#21046;&#23610;&#24230;&#19981;&#21464;&#65292;&#24182;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#24615;&#21035;&#35782;&#21035;&#21644;&#20851;&#38190;&#35789;&#26816;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;MBT&#22312;&#26377;&#38480;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#26102;&#65292;&#25913;&#21892;&#20102;&#34920;&#31034;&#27867;&#21270;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#30456;&#23545;&#20110;&#21407;&#22987;BT&#12290;&#36825;&#20984;&#26174;&#20102;&#35774;&#35745;&#40723;&#21169;&#19981;&#21464;&#24615;&#21644;&#21487;&#20256;&#36755;&#34920;&#31034;&#30340;&#30446;&#26631;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#35843;&#25972;BT&#23398;&#20064;&#30446;&#26631;&#20197;&#29983;&#25104;&#22312;&#36866;&#29992;&#20110;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#35821;&#38899;&#34920;&#31034;&#30340;&#35265;&#35299;&#12290;&#36825;&#39033;&#30740;&#31350;&#26159;&#21457;&#23637;&#21487;&#37325;&#29992;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The choice of the objective function is crucial in emerging high-quality representations from self-supervised learning. This paper investigates how different formulations of the Barlow Twins (BT) objective impact downstream task performance for speech data. We propose Modified Barlow Twins (MBT) with normalized latents to enforce scale-invariance and evaluate on speaker identification, gender recognition and keyword spotting tasks. Our results show MBT improves representation generalization over original BT, especially when fine-tuning with limited target data. This highlights the importance of designing objectives that encourage invariant and transferable representations. Our analysis provides insights into how the BT learning objective can be tailored to produce speech representations that excel when adapted to new downstream tasks. This study is an important step towards developing reusable self-supervised speech representations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#29616;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#33021;&#37327;&#20989;&#25968;&#19982;&#26377;&#25928;&#20234;&#36763;&#33258;&#26059;&#21704;&#23494;&#39039;&#37327;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#26029;&#22797;&#26434;&#25968;&#25454;&#20013;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.02292</link><description>&lt;p&gt;
&#20351;&#29992;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#25512;&#26029;&#26377;&#25928;&#32806;&#21512;
&lt;/p&gt;
&lt;p&gt;
Inferring effective couplings with Restricted Boltzmann Machines. (arXiv:2309.02292v2 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#29616;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#33021;&#37327;&#20989;&#25968;&#19982;&#26377;&#25928;&#20234;&#36763;&#33258;&#26059;&#21704;&#23494;&#39039;&#37327;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#26029;&#22797;&#26434;&#25968;&#25454;&#20013;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#25509;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#26088;&#22312;&#20197;&#27169;&#22411;&#30340;&#29627;&#23572;&#20857;&#26364;&#26435;&#37325;&#30340;&#27700;&#24179;&#20934;&#30830;&#37325;&#29616;&#25968;&#25454;&#20013;&#35266;&#23519;&#21040;&#30340;&#25152;&#26377;&#32479;&#35745;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#25361;&#25112;&#26159;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#29289;&#29702;&#35299;&#37322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#33021;&#37327;&#20989;&#25968;&#19982;&#21253;&#21547;&#33258;&#26059;&#20043;&#38388;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#26377;&#25928;&#20234;&#36763;&#33258;&#26059;&#21704;&#23494;&#39039;&#37327;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26144;&#23556;&#21253;&#25324;&#25152;&#26377;&#21487;&#33021;&#38454;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36229;&#36234;&#20102;&#36870;&#20234;&#36763;&#26041;&#27861;&#20013;&#36890;&#24120;&#32771;&#34385;&#30340;&#20256;&#32479;&#20108;&#27425;&#30456;&#20114;&#20316;&#29992;&#65292;&#20351;&#20854;&#33021;&#22815;&#25551;&#36848;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#35797;&#22270;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#26144;&#23556;&#27809;&#26377;&#27491;&#30830;&#22788;&#29702;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#27809;&#26377;&#20855;&#20307;&#30340;&#23454;&#38469;&#24212;&#29992;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models offer a direct way to model complex data. Among them, energy-based models provide us with a neural network model that aims to accurately reproduce all statistical correlations observed in the data at the level of the Boltzmann weight of the model. However, one challenge is to understand the physical interpretation of such models. In this study, we propose a simple solution by implementing a direct mapping between the energy function of the Restricted Boltzmann Machine and an effective Ising spin Hamiltonian that includes high-order interactions between spins. This mapping includes interactions of all possible orders, going beyond the conventional pairwise interactions typically considered in the inverse Ising approach, and allowing the description of complex datasets. Earlier works attempted to achieve this goal, but the proposed mappings did not do properly treat the complexity of the problem or did not contain direct prescriptions for practical application. To valid
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20849;&#21516;&#37051;&#23621;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21521;&#37327;&#30340;&#27491;&#20132;&#24615;&#26469;&#25429;&#25417;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;MPLP&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20934;&#27491;&#20132;&#21521;&#37327;&#20272;&#35745;&#38142;&#36335;&#32423;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33410;&#28857;&#32423;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00976</link><description>&lt;p&gt;
&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#21487;&#20197;&#20272;&#35745;&#20849;&#21516;&#37051;&#23621;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Pure Message Passing Can Estimate Common Neighbor for Link Prediction. (arXiv:2309.00976v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00976
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20849;&#21516;&#37051;&#23621;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21521;&#37327;&#30340;&#27491;&#20132;&#24615;&#26469;&#25429;&#25417;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;MPLP&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20934;&#27491;&#20132;&#21521;&#37327;&#20272;&#35745;&#38142;&#36335;&#32423;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33410;&#28857;&#32423;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#24050;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#22312;&#38142;&#36335;&#39044;&#27979;&#26041;&#38754;&#65292;&#23427;&#20204;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#34987;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#22914;&#20849;&#21516;&#37051;&#23621;&#65288;CN&#65289;&#25152;&#36229;&#36234;&#12290;&#36825;&#31181;&#24046;&#24322;&#28304;&#20110;&#19968;&#20010;&#26681;&#26412;&#38480;&#21046;&#65306;&#23613;&#31649;MPNN&#22312;&#33410;&#28857;&#32423;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#32534;&#30721;&#38142;&#36335;&#39044;&#27979;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#65288;&#22914;CN&#65289;&#26041;&#38754;&#21017;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21521;&#37327;&#30340;&#27491;&#20132;&#24615;&#65292;&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#30830;&#23454;&#21487;&#20197;&#25429;&#25417;&#21040;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;MPNN&#22312;&#36817;&#20284;CN&#21551;&#21457;&#24335;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;&#28040;&#24687;&#20256;&#36882;&#38142;&#36335;&#39044;&#27979;&#22120;&#65288;MPLP&#65289;&#12290;MPLP&#21033;&#29992;&#20934;&#27491;&#20132;&#21521;&#37327;&#20272;&#35745;&#38142;&#36335;&#32423;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#33410;&#28857;&#32423;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#26126;&#21033;&#29992;&#28040;&#24687;&#20256;&#36882;&#25429;&#25417;&#32467;&#26500;&#29305;&#24449;&#33021;&#22815;&#25913;&#21892;&#38142;&#36335;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message Passing Neural Networks (MPNNs) have emerged as the {\em de facto} standard in graph representation learning. However, when it comes to link prediction, they often struggle, surpassed by simple heuristics such as Common Neighbor (CN). This discrepancy stems from a fundamental limitation: while MPNNs excel in node-level representation, they stumble with encoding the joint structural features essential to link prediction, like CN. To bridge this gap, we posit that, by harnessing the orthogonality of input vectors, pure message-passing can indeed capture joint structural features. Specifically, we study the proficiency of MPNNs in approximating CN heuristics. Based on our findings, we introduce the Message Passing Link Predictor (MPLP), a novel link prediction model. MPLP taps into quasi-orthogonal vectors to estimate link-level structural features, all while preserving the node-level complexities. Moreover, our approach demonstrates that leveraging message-passing to capture stru
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.15812</link><description>&lt;p&gt;
&#36879;&#36807;&#20559;&#22909;&#30475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#33719;&#21462;&#65306;&#25581;&#31034;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#24847;&#22270;&#30340;&#23545;&#40784;&#25215;&#35834;&#28041;&#21450;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25110;&#20154;&#31867;&#21453;&#39304;&#12290;&#31264;&#23494;&#30340;&#21453;&#39304;&#27880;&#37322;&#33719;&#21462;&#21644;&#25972;&#21512;&#25104;&#26412;&#36739;&#39640;&#65292;&#32780;&#31232;&#30095;&#30340;&#21453;&#39304;&#21017;&#28041;&#21450;&#32467;&#26500;&#24615;&#35774;&#35745;&#36873;&#25321;&#65292;&#21363;&#35780;&#20998;&#65288;&#20363;&#22914;&#65292;&#22312;1-7&#30340;&#33539;&#22260;&#20869;&#23545;&#22238;&#31572;A&#36827;&#34892;&#35780;&#20998;&#65289;&#21644;&#25490;&#21517;&#65288;&#20363;&#22914;&#65292;&#22238;&#31572;A&#26159;&#21542;&#27604;&#22238;&#31572;B&#26356;&#22909;&#65311;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#23545;LLMs&#30340;&#23545;&#40784;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#22312;&#20154;&#31867;&#21644;AI&#27880;&#37322;&#32773;&#20013;&#37117;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;60%&#12290;&#25105;&#20204;&#30340;&#21518;&#32493;&#20998;&#26512;&#30830;&#23450;&#20102;&#35299;&#37322;&#36825;&#20010;&#29616;&#35937;&#30340;&#21508;&#31181;&#27880;&#37322;&#32773;&#20559;&#35265;&#26041;&#38754;&#65292;&#27604;&#22914;&#20154;&#31867;&#27880;&#37322;&#32773;&#26356;&#21916;&#27426;&#23494;&#38598;&#22238;&#31572;&#24182;&#22312;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#26356;&#38738;&#30544;&#20934;&#30830;&#24615;&#12290;&#20196;&#25105;&#20204;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#23545;&#23545;&#40784;&#30340;LLMs&#30340;&#35780;&#20272;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#35780;&#20272;&#32467;&#26524;&#22240;&#20026;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs
&lt;/p&gt;</description></item><item><title>CALM&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.12539</link><description>&lt;p&gt;
CALM: &#19968;&#31181;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12539
&lt;/p&gt;
&lt;p&gt;
CALM&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#19981;&#26029;&#22686;&#24378;&#65292;&#37327;&#21270;&#21644;&#27604;&#36739;&#23427;&#20204;&#22312;&#31038;&#20250;&#21644;&#20154;&#21475;&#23398;&#20559;&#35265;&#26041;&#38754;&#30340;&#33021;&#21147;&#20197;&#21450;&#28508;&#22312;&#30340;&#21361;&#23475;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#20559;&#35265;&#27979;&#37327;&#25968;&#25454;&#38598;&#23545;&#20110;&#20154;&#24037;&#35774;&#35745;&#27169;&#26495;&#30340;&#25200;&#21160;&#25935;&#24863;&#65292;&#22240;&#27492;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#65288;CALM&#65289;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#37327;&#21270;LMs&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#26032;&#38395;&#25991;&#31456;&#65289;&#30340;16&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#36807;&#28388;&#20986;224&#20010;&#27169;&#26495;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;78,400&#20010;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24179;&#22343;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#27169;&#26495;&#38271;&#24230;&#30340;&#21464;&#24322;&#31243;&#24230;&#31561;&#25351;&#26631;&#65292;&#27604;&#36739;CALM&#19982;&#20808;&#21069;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#27979;&#35797;&#20854;&#23545;&#32454;&#24494;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30456;&#23545;&#20110;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#22240;&#27492;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;20&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language 
&lt;/p&gt;</description></item><item><title>GaitPT&#26159;&#19968;&#31181;&#21033;&#29992;&#23039;&#21183;&#20272;&#35745;&#39592;&#39612;&#36827;&#34892;&#27493;&#24577;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#22312;&#25511;&#21046;&#22330;&#26223;&#21644;&#37326;&#22806;&#22330;&#26223;&#20013;&#22343;&#23637;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10623</link><description>&lt;p&gt;
GaitPT: &#20351;&#29992;&#39592;&#39612;&#36827;&#34892;&#27493;&#24577;&#35782;&#21035;&#30340;&#20840;&#37096;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
GaitPT: Skeletons Are All You Need For Gait Recognition. (arXiv:2308.10623v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10623
&lt;/p&gt;
&lt;p&gt;
GaitPT&#26159;&#19968;&#31181;&#21033;&#29992;&#23039;&#21183;&#20272;&#35745;&#39592;&#39612;&#36827;&#34892;&#27493;&#24577;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#22312;&#25511;&#21046;&#22330;&#26223;&#21644;&#37326;&#22806;&#22330;&#26223;&#20013;&#22343;&#23637;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27493;&#24577;&#20998;&#26512;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20855;&#26377;&#22312;&#23433;&#20840;&#12289;&#21307;&#30103;&#12289;&#20307;&#32946;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#26041;&#38754;&#30340;&#20247;&#22810;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#27493;&#24577;&#27169;&#24335;&#34987;&#35270;&#20026;&#19968;&#31181;&#36828;&#36317;&#31163;&#33258;&#21160;&#20154;&#21592;&#35782;&#21035;&#30340;&#29420;&#29305;&#25351;&#32441;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gait Pyramid Transformer&#65288;GaitPT&#65289;&#30340;&#26032;&#22411;&#27493;&#24577;&#35782;&#21035;&#26550;&#26500;&#65292;&#23427;&#21033;&#29992;&#23039;&#21183;&#20272;&#35745;&#39592;&#39612;&#26469;&#25429;&#25417;&#29420;&#29305;&#30340;&#27493;&#24577;&#27169;&#24335;&#65292;&#32780;&#19981;&#20381;&#36182;&#22806;&#35980;&#20449;&#24687;&#12290;GaitPT&#37319;&#29992;&#20998;&#23618;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#22312;&#35299;&#21078;&#23398;&#19978;&#19968;&#33268;&#22320;&#25552;&#21462;&#36816;&#21160;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#36890;&#36807;&#20154;&#20307;&#39592;&#39612;&#32467;&#26500;&#36827;&#34892;&#24341;&#23548;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;&#39592;&#39612;&#30340;&#27493;&#24577;&#35782;&#21035;&#26041;&#27861;&#30456;&#27604;&#65292;GaitPT&#22312;&#25511;&#21046;&#22330;&#26223;&#21644;&#37326;&#22806;&#22330;&#26223;&#20013;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;GaitPT&#22312;CASIA-B&#19978;&#33719;&#24471;&#20102;82.6%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20854;&#20182;&#20316;&#21697;6%&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of patterns of walking is an important area of research that has numerous applications in security, healthcare, sports and human-computer interaction. Lately, walking patterns have been regarded as a unique fingerprinting method for automatic person identification at a distance. In this work, we propose a novel gait recognition architecture called Gait Pyramid Transformer (GaitPT) that leverages pose estimation skeletons to capture unique walking patterns, without relying on appearance information. GaitPT adopts a hierarchical transformer architecture that effectively extracts both spatial and temporal features of movement in an anatomically consistent manner, guided by the structure of the human skeleton. Our results show that GaitPT achieves state-of-the-art performance compared to other skeleton-based gait recognition works, in both controlled and in-the-wild scenarios. GaitPT obtains 82.6% average accuracy on CASIA-B, surpassing other works by a margin of 6%. Moreover,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;&#65292;&#22312;&#20505;&#36873;&#20154;&#31579;&#36873;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#20505;&#36873;&#20154;&#27744;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#65292;&#31867;&#20154;&#31579;&#36873;&#32773;&#21487;&#33021;&#23545;&#21463;&#20445;&#25252;&#12289;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#19982;&#19968;&#23478;&#22823;&#20844;&#21496;&#21512;&#20316;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#28508;&#22312;&#30340;&#33258;&#21160;&#21270;&#25307;&#32856;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.15398</link><description>&lt;p&gt;
&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Initial Screening Order Problem. (arXiv:2307.15398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;&#65292;&#22312;&#20505;&#36873;&#20154;&#31579;&#36873;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#20505;&#36873;&#20154;&#27744;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#65292;&#31867;&#20154;&#31579;&#36873;&#32773;&#21487;&#33021;&#23545;&#21463;&#20445;&#25252;&#12289;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#19982;&#19968;&#23478;&#22823;&#20844;&#21496;&#21512;&#20316;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#28508;&#22312;&#30340;&#33258;&#21160;&#21270;&#25307;&#32856;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;&#65292;&#36825;&#26159;&#20505;&#36873;&#20154;&#31579;&#36873;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#23427;&#28041;&#21450;&#19968;&#20010;&#31867;&#20284;&#20154;&#31867;&#30340;&#31579;&#36873;&#32773;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#30340;&#20505;&#36873;&#20154;&#27744;&#20013;&#25214;&#21040;&#21069;k&#20010;&#36866;&#21512;&#30340;&#20505;&#36873;&#20154;&#65292;&#32780;&#19981;&#26159;&#26368;&#22909;&#30340;k&#20010;&#36866;&#21512;&#30340;&#20505;&#36873;&#20154;&#12290;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#34920;&#31034;&#31867;&#20154;&#31579;&#36873;&#32773;&#22312;&#31579;&#36873;&#20043;&#21069;&#22914;&#20309;&#23433;&#25490;&#20505;&#36873;&#20154;&#27744;&#12290;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#30340;&#36873;&#25321;&#23545;&#25152;&#36873;&#30340;k&#20010;&#20505;&#36873;&#20154;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20505;&#36873;&#20154;&#27744;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#65292;&#30007;&#24615;&#20505;&#36873;&#20154;&#22810;&#20110;&#22899;&#24615;&#20505;&#36873;&#20154;&#65289;&#65292;&#31867;&#20154;&#31579;&#36873;&#32773;&#21487;&#33021;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#23545;&#21463;&#20445;&#25252;&#30340;&#12289;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20135;&#29983;&#19981;&#24179;&#31561;&#30340;&#21162;&#21147;&#12290;&#20854;&#20182;&#20844;&#24179;&#24615;&#32467;&#26524;&#20063;&#22312;&#31867;&#20154;&#31579;&#36873;&#32773;&#19979;&#24471;&#21040;&#35777;&#26126;&#12290;&#36825;&#39033;&#30740;&#31350;&#26159;&#19982;&#19968;&#23478;&#22823;&#20844;&#21496;&#21512;&#20316;&#30340;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20102;&#35299;&#20854;&#28508;&#22312;&#33258;&#21160;&#21270;&#30340;&#25307;&#32856;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present the initial screening order problem, a crucial step within candidate screening. It involves a human-like screener with an objective to find the first k suitable candidates rather than the best k suitable candidates in a candidate pool given an initial screening order. The initial screening order represents the way in which the human-like screener arranges the candidate pool prior to screening. The choice of initial screening order has considerable effects on the selected set of k candidates. We prove that under an unbalanced candidate pool (e.g., having more male than female candidates), the human-like screener can suffer from uneven efforts that hinder its decision-making over the protected, under-represented group relative to the non-protected, over-represented group. Other fairness results are proven under the human-like screener. This research is based on a collaboration with a large company to better understand its hiring process for potential automation. 
&lt;/p&gt;</description></item><item><title>EasyTPP&#26159;&#31532;&#19968;&#20010;&#20851;&#20110;&#20107;&#20214;&#24207;&#21015;&#24314;&#27169;&#39046;&#22495;&#30340;&#20013;&#24515;&#36164;&#28304;&#24211;&#65292;&#25552;&#20379;&#32479;&#19968;&#30340;&#25968;&#25454;&#38598;&#20351;&#29992;&#30028;&#38754;&#21644;&#24191;&#27867;&#30340;&#35780;&#20272;&#31243;&#24207;&#65292;&#35299;&#20915;&#20102;&#35813;&#39046;&#22495;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.08097</link><description>&lt;p&gt;
EasyTPP: &#36808;&#21521;&#24320;&#25918;&#22522;&#20934;&#27979;&#35797;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EasyTPP: Towards Open Benchmarking Temporal Point Processes. (arXiv:2307.08097v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08097
&lt;/p&gt;
&lt;p&gt;
EasyTPP&#26159;&#31532;&#19968;&#20010;&#20851;&#20110;&#20107;&#20214;&#24207;&#21015;&#24314;&#27169;&#39046;&#22495;&#30340;&#20013;&#24515;&#36164;&#28304;&#24211;&#65292;&#25552;&#20379;&#32479;&#19968;&#30340;&#25968;&#25454;&#38598;&#20351;&#29992;&#30028;&#38754;&#21644;&#24191;&#27867;&#30340;&#35780;&#20272;&#31243;&#24207;&#65292;&#35299;&#20915;&#20102;&#35813;&#39046;&#22495;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#22312;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#12289;&#22312;&#32447;&#36141;&#29289;&#12289;&#31038;&#20132;&#32593;&#32476;&#31561;&#29616;&#23454;&#19990;&#30028;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#20102;&#23545;&#36825;&#31867;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#65288;TPPs&#65289;&#24050;&#25104;&#20026;&#26368;&#33258;&#28982;&#21644;&#26377;&#31454;&#20105;&#21147;&#30340;&#27169;&#22411;&#65292;&#22312;&#23398;&#26415;&#30028;&#21644;&#24212;&#29992;&#30028;&#37117;&#20135;&#29983;&#20102;&#37325;&#35201;&#24433;&#21709;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#23581;&#35797;&#20043;&#38388;&#32570;&#20047;&#19968;&#20010;&#20013;&#24515;&#22522;&#20934;&#12290;&#36825;&#31181;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#24773;&#20917;&#38459;&#30861;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#23545;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#21644;&#32467;&#26524;&#30340;&#20877;&#29616;&#65292;&#21487;&#33021;&#20250;&#20943;&#24930;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyTPP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20851;&#20110;&#20107;&#20214;&#24207;&#21015;&#24314;&#27169;&#39046;&#22495;&#30340;&#30740;&#31350;&#36164;&#20135;&#65288;&#20363;&#22914;&#25968;&#25454;&#12289;&#27169;&#22411;&#12289;&#35780;&#20272;&#31243;&#24207;&#12289;&#25991;&#26723;&#65289;&#30340;&#38598;&#20013;&#23384;&#20648;&#24211;&#12290;&#25105;&#20204;&#30340;EasyTPP&#23545;&#27492;&#39046;&#22495;&#26377;&#20960;&#20010;&#29420;&#29305;&#30340;&#36129;&#29486;&#65306;&#32479;&#19968;&#30340;&#20351;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#28155;&#21152;&#26032;&#25968;&#25454;&#38598;&#30340;&#30028;&#38754;&#65307;&#24191;&#27867;&#30340;&#35780;&#20272;&#31243;&#24207;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Continuous-time event sequences play a vital role in real-world domains such as healthcare, finance, online shopping, social networks, and so on. To model such data, temporal point processes (TPPs) have emerged as the most natural and competitive models, making a significant impact in both academic and application communities. Despite the emergence of many powerful models in recent years, there hasn't been a central benchmark for these models and future research endeavors. This lack of standardization impedes researchers and practitioners from comparing methods and reproducing results, potentially slowing down progress in this field. In this paper, we present EasyTPP, the first central repository of research assets (e.g., data, models, evaluation programs, documentations) in the area of event sequence modeling. Our EasyTPP makes several unique contributions to this area: a unified interface of using existing datasets and adding new datasets; a wide range of evaluation programs that are
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#20107;&#20214;&#26085;&#24535;&#36716;&#25442;&#20026;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;OCDiGCN&#26469;&#26816;&#27979;&#19968;&#32452;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#20013;&#30340;&#22270;&#32423;&#21035;&#24322;&#24120;&#65292;&#24182;&#25552;&#20379;&#23545;&#24322;&#24120;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00527</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#19982;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks based Log Anomaly Detection and Explanation. (arXiv:2307.00527v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00527
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#20107;&#20214;&#26085;&#24535;&#36716;&#25442;&#20026;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;OCDiGCN&#26469;&#26816;&#27979;&#19968;&#32452;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#20013;&#30340;&#22270;&#32423;&#21035;&#24322;&#24120;&#65292;&#24182;&#25552;&#20379;&#23545;&#24322;&#24120;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#26085;&#24535;&#34987;&#24191;&#27867;&#29992;&#20110;&#35760;&#24405;&#39640;&#31185;&#25216;&#31995;&#32479;&#30340;&#29366;&#24577;&#65292;&#22240;&#27492;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#30417;&#25511;&#36825;&#20123;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#23558;&#26085;&#24535;&#20107;&#20214;&#35745;&#25968;&#30697;&#38453;&#25110;&#26085;&#24535;&#20107;&#20214;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#21033;&#29992;&#26085;&#24535;&#20107;&#20214;&#20043;&#38388;&#30340;&#23450;&#37327;&#21644;/&#25110;&#39034;&#24207;&#20851;&#31995;&#26469;&#26816;&#27979;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#20165;&#32771;&#34385;&#23450;&#37327;&#25110;&#39034;&#24207;&#20851;&#31995;&#21487;&#33021;&#23548;&#33268;&#26816;&#27979;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26080;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;Logs2Graphs&#65292;&#23427;&#39318;&#20808;&#23558;&#20107;&#20214;&#26085;&#24535;&#36716;&#25442;&#20026;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#65292;&#28982;&#21518;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;One-Class Digraph Inception Convolutional Networks&#65288;OCDiGCN&#65289;&#65292;&#29992;&#20110;&#22312;&#19968;&#32452;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#20013;&#26816;&#27979;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#12290;&#36890;&#36807;&#23558;&#22270;&#34920;&#31034;&#19982;&#23646;&#24615;&#34920;&#31034;&#32806;&#21512;&#36215;&#26469;&#65292;&#22312;&#22270;&#32423;&#21035;&#19978;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#24322;&#24120;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event logs are widely used to record the status of high-tech systems, making log anomaly detection important for monitoring those systems. Most existing log anomaly detection methods take a log event count matrix or log event sequences as input, exploiting quantitative and/or sequential relationships between log events to detect anomalies. Unfortunately, only considering quantitative or sequential relationships may result in low detection accuracy. To alleviate this problem, we propose a graph-based method for unsupervised log anomaly detection, dubbed Logs2Graphs, which first converts event logs into attributed, directed, and weighted graphs, and then leverages graph neural networks to perform graph-level anomaly detection. Specifically, we introduce One-Class Digraph Inception Convolutional Networks, abbreviated as OCDiGCN, a novel graph neural network model for detecting graph-level anomalies in a collection of attributed, directed, and weighted graphs. By coupling the graph represe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#20132;&#25442;&#31169;&#26377;&#35266;&#27979;&#20449;&#24687;&#65292;&#20195;&#29702;&#21487;&#20197;&#38598;&#20307;&#20272;&#35745;&#26410;&#30693;&#25968;&#37327;&#65292;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;&#36890;&#36807;&#32447;&#24615;&#32858;&#21512;&#26041;&#26696;&#21644;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35843;&#25972;&#30340;&#38543;&#26426;&#21270;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#39640;&#25928;&#32452;&#21512;&#35266;&#27979;&#25968;&#25454;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15865</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Distributed Estimation and Learning. (arXiv:2306.15865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#20132;&#25442;&#31169;&#26377;&#35266;&#27979;&#20449;&#24687;&#65292;&#20195;&#29702;&#21487;&#20197;&#38598;&#20307;&#20272;&#35745;&#26410;&#30693;&#25968;&#37327;&#65292;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;&#36890;&#36807;&#32447;&#24615;&#32858;&#21512;&#26041;&#26696;&#21644;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35843;&#25972;&#30340;&#38543;&#26426;&#21270;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#39640;&#25928;&#32452;&#21512;&#35266;&#27979;&#25968;&#25454;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#20132;&#25442;&#20449;&#24687;&#26469;&#20272;&#35745;&#20174;&#20854;&#31169;&#19979;&#35266;&#23519;&#30340;&#26679;&#26412;&#20013;&#26410;&#30693;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;&#36890;&#36807;&#20132;&#25442;&#31169;&#26377;&#35266;&#27979;&#20449;&#24687;&#65292;&#20195;&#29702;&#21487;&#20197;&#38598;&#20307;&#20272;&#35745;&#26410;&#30693;&#25968;&#37327;&#65292;&#20294;&#20182;&#20204;&#20063;&#38754;&#20020;&#38544;&#31169;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#32858;&#21512;&#26041;&#26696;&#30340;&#30446;&#26631;&#26159;&#22312;&#26102;&#38388;&#21644;&#32593;&#32476;&#20013;&#39640;&#25928;&#22320;&#32452;&#21512;&#35266;&#27979;&#25968;&#25454;&#65292;&#21516;&#26102;&#28385;&#36275;&#20195;&#29702;&#30340;&#38544;&#31169;&#38656;&#27714;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#36229;&#36234;&#20182;&#20204;&#26412;&#22320;&#38468;&#36817;&#30340;&#21327;&#35843;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#21442;&#19982;&#30340;&#20195;&#29702;&#33021;&#22815;&#20174;&#31163;&#32447;&#25110;&#38543;&#26102;&#38388;&#22312;&#32447;&#33719;&#21462;&#30340;&#31169;&#26377;&#20449;&#21495;&#20013;&#20272;&#35745;&#23436;&#25972;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#24182;&#20445;&#25252;&#20854;&#20449;&#21495;&#21644;&#32593;&#32476;&#38468;&#36817;&#30340;&#38544;&#31169;&#12290;&#36825;&#26159;&#36890;&#36807;&#32447;&#24615;&#32858;&#21512;&#26041;&#26696;&#21644;&#35843;&#25972;&#30340;&#38543;&#26426;&#21270;&#26041;&#26696;&#23454;&#29616;&#30340;&#65292;&#23558;&#22122;&#22768;&#28155;&#21152;&#21040;&#20132;&#25442;&#30340;&#20272;&#35745;&#25968;&#25454;&#20013;&#20197;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distributed estimation and learning problems in a networked environment in which agents exchange information to estimate unknown statistical properties of random variables from their privately observed samples. By exchanging information about their private observations, the agents can collectively estimate the unknown quantities, but they also face privacy risks. The goal of our aggregation schemes is to combine the observed data efficiently over time and across the network, while accommodating the privacy needs of the agents and without any coordination beyond their local neighborhoods. Our algorithms enable the participating agents to estimate a complete sufficient statistic from private signals that are acquired offline or online over time, and to preserve the privacy of their signals and network neighborhoods. This is achieved through linear aggregation schemes with adjusted randomization schemes that add noise to the exchanged estimates subject to differential privacy (DP
&lt;/p&gt;</description></item><item><title>Split learning (SL) enables servers to handle the major training workload while still enhancing data privacy, which is an important approach in 6G edge networks. This article provides an overview of the tailored 6G architecture to support edge SL, the critical design issues for edge SL, and presents future research direction and exciting applications of SL in 6G edge networks.</title><link>http://arxiv.org/abs/2306.12194</link><description>&lt;p&gt;
6G&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;Split Learning
&lt;/p&gt;
&lt;p&gt;
Split Learning in 6G Edge Networks. (arXiv:2306.12194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12194
&lt;/p&gt;
&lt;p&gt;
Split learning (SL) enables servers to handle the major training workload while still enhancing data privacy, which is an important approach in 6G edge networks. This article provides an overview of the tailored 6G architecture to support edge SL, the critical design issues for edge SL, and presents future research direction and exciting applications of SL in 6G edge networks.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20998;&#24067;&#24335;&#36793;&#32536;&#35745;&#31639;&#36164;&#28304;&#30340;&#26222;&#21450;&#65292;6G&#31227;&#21160;&#32593;&#32476;&#23558;&#21457;&#23637;&#25104;&#20026;&#19968;&#20010;&#36830;&#25509;&#26234;&#33021;&#30340;&#32593;&#32476;&#12290;&#22312;&#36825;&#26465;&#32447;&#36335;&#19978;&#65292;&#23558;&#32852;&#37030;&#23398;&#20064;&#32435;&#20837;&#31227;&#21160;&#36793;&#32536;&#30340;&#25552;&#35758;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#24222;&#22823;&#30340;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#20960;&#20046;&#26080;&#27861;&#25903;&#25345;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#36825;&#23548;&#33268;&#20102;Split Learning (SL)&#30340;&#20986;&#29616;&#65292;&#23427;&#20351;&#26381;&#21153;&#22120;&#22788;&#29702;&#20027;&#35201;&#30340;&#35757;&#32451;&#24037;&#20316;&#36127;&#36733;&#65292;&#21516;&#26102;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31616;&#35201;&#27010;&#36848;&#20102;SL&#30340;&#20851;&#38190;&#21457;&#23637;&#65292;&#24182;&#38416;&#36848;&#20102;&#20854;&#19982;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#30340;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#39318;&#20808;&#35828;&#26126;&#20102;&#23450;&#21046;&#30340;6G&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#25903;&#25345;&#36793;&#32536;SL&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36793;&#32536;SL&#30340;&#20851;&#38190;&#35774;&#35745;&#38382;&#39064;&#65292;&#21253;&#25324;&#21019;&#26032;&#30340;&#36164;&#28304;&#39640;&#25928;&#23398;&#20064;&#26694;&#26550;&#21644;&#22312;&#21333;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#19979;&#30340;&#36164;&#28304;&#31649;&#29702;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;Split Learning&#27169;&#22411;&#30340;&#22810;&#36793;&#32536;&#26381;&#21153;&#22120;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#19968;&#20123;&#21050;&#28608;&#20154;&#24515;&#30340;SL&#22312;6G&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the proliferation of distributed edge computing resources, the 6G mobile network will evolve into a network for connected intelligence. Along this line, the proposal to incorporate federated learning into the mobile edge has gained considerable interest in recent years. However, the deployment of federated learning faces substantial challenges as massive resource-limited IoT devices can hardly support on-device model training. This leads to the emergence of split learning (SL) which enables servers to handle the major training workload while still enhancing data privacy. In this article, we offer a brief overview of key advancements in SL and articulate its seamless integration with wireless edge networks. We begin by illustrating the tailored 6G architecture to support edge SL. Then, we examine the critical design issues for edge SL, including innovative resource-efficient learning frameworks and resource management strategies under a single edge server. Additionally, we expand t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$\pi2\text{vec}$&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23558;&#40657;&#30418;&#31574;&#30053;&#34892;&#20026;&#34920;&#31034;&#20026;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#36873;&#25321;&#12290;&#35813;&#26041;&#27861;&#20026;&#29616;&#20195;&#30740;&#31350;&#26041;&#21521;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#12289;&#22522;&#30784;&#27169;&#22411;&#29366;&#24577;&#34920;&#31034;&#21644;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#31574;&#30053;&#36873;&#25321;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.09800</link><description>&lt;p&gt;
$\pi2\text{vec}$&#65306;&#22522;&#20110;&#32487;&#25215;&#29305;&#24449;&#30340;&#31574;&#30053;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
$\pi2\text{vec}$: Policy Representations with Successor Features. (arXiv:2306.09800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\pi2\text{vec}$&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23558;&#40657;&#30418;&#31574;&#30053;&#34892;&#20026;&#34920;&#31034;&#20026;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#36873;&#25321;&#12290;&#35813;&#26041;&#27861;&#20026;&#29616;&#20195;&#30740;&#31350;&#26041;&#21521;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#12289;&#22522;&#30784;&#27169;&#22411;&#29366;&#24577;&#34920;&#31034;&#21644;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#31574;&#30053;&#36873;&#25321;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;$\pi2\text{vec}$&#65292;&#19968;&#31181;&#23558;&#40657;&#30418;&#31574;&#30053;&#34892;&#20026;&#34920;&#31034;&#20026;&#29305;&#24449;&#21521;&#37327;&#30340;&#26041;&#27861;&#12290;&#31574;&#30053;&#34920;&#31034;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#25429;&#25417;&#20102;&#22522;&#30784;&#27169;&#22411;&#29305;&#24449;&#32479;&#35745;&#25968;&#25454;&#22312;&#31574;&#30053;&#34892;&#20026;&#21709;&#24212;&#20013;&#30340;&#21464;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#20026;&#34701;&#21512;&#19977;&#20010;&#29616;&#20195;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25903;&#25345;&#65306;&#20316;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#34917;&#20805;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#20316;&#20026;&#36890;&#29992;&#24378;&#22823;&#29366;&#24577;&#34920;&#31034;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#21450;&#22312;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#26377;&#25928;&#31574;&#30053;&#36873;&#25321;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes $\pi2\text{vec}$, a method for representing behaviors of black box policies as feature vectors. The policy representations capture how the statistics of foundation model features change in response to the policy behavior in a task agnostic way, and can be trained from offline data, allowing them to be used in offline policy selection. This work provides a key piece of a recipe for fusing together three modern lines of research: Offline policy evaluation as a counterpart to offline RL, foundation models as generic and powerful state representations, and efficient policy selection in resource constrained environments.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#22870;&#21169;&#35838;&#31243;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#23454;&#29616;&#40065;&#26834;&#19990;&#30028;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;WAKER&#31639;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#19990;&#30028;&#27169;&#22411;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#20272;&#35745;&#35823;&#24046;&#36873;&#25321;&#25968;&#25454;&#25910;&#38598;&#29615;&#22659;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09205</link><description>&lt;p&gt;
&#26080;&#22870;&#21169;&#35757;&#32451;&#40065;&#26834;&#19990;&#30028;&#27169;&#22411;&#30340;&#26080;&#22870;&#21169;&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;
Reward-Free Curricula for Training Robust World Models. (arXiv:2306.09205v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09205
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#22870;&#21169;&#35838;&#31243;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#23454;&#29616;&#40065;&#26834;&#19990;&#30028;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;WAKER&#31639;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#19990;&#30028;&#27169;&#22411;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#20272;&#35745;&#35823;&#24046;&#36873;&#25321;&#25968;&#25454;&#25910;&#38598;&#29615;&#22659;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#20204;&#23545;&#24320;&#21457;&#33021;&#22815;&#36866;&#24212;&#26032;&#20219;&#21153;&#32780;&#26080;&#38656;&#22312;&#29615;&#22659;&#20013;&#36827;&#34892;&#39069;&#22806;&#35757;&#32451;&#30340;&#36890;&#29992;&#33021;&#21147;&#20195;&#29702;&#20135;&#29983;&#20102;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#20174;&#26080;&#22870;&#21169;&#25506;&#32034;&#20013;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#24819;&#35937;&#30340;&#32463;&#39564;&#26469;&#35757;&#32451;&#26032;&#20219;&#21153;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#19968;&#20010;&#36890;&#29992;&#30340;&#20195;&#29702;&#38656;&#35201;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#26080;&#22870;&#21169;&#35774;&#32622;&#20013;&#29983;&#25104;&#35838;&#31243;&#20197;&#35757;&#32451;&#40065;&#26834;&#24615;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#26497;&#23567;&#26497;&#22823;&#21518;&#24724;&#30340;&#35282;&#24230;&#32771;&#34385;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26497;&#23567;&#26497;&#22823;&#21518;&#24724;&#19982;&#22312;&#19981;&#21516;&#29615;&#22659;&#23454;&#20363;&#20013;&#19990;&#30028;&#27169;&#22411;&#30340;&#26368;&#22823;&#35823;&#24046;&#26368;&#23567;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#36825;&#20010;&#32467;&#26524;&#20026;&#25105;&#20204;&#30340;&#31639;&#27861;WAKER&#65288;&#22522;&#20110;&#40065;&#26834;&#24615;&#30340;&#29615;&#22659;&#20013;&#30340;&#30693;&#35782;&#21152;&#26435;&#33719;&#21462;&#65289;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;WAKER&#26681;&#25454;&#27599;&#20010;&#29615;&#22659;&#20013;&#19990;&#30028;&#27169;&#22411;&#30340;&#20272;&#35745;&#35823;&#24046;&#36873;&#25321;&#25968;&#25454;&#25910;&#38598;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;WAKER&#33021;&#22815;&#25552;&#21319;&#40065;&#26834;&#24615;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a recent surge of interest in developing generally-capable agents that can adapt to new tasks without additional training in the environment. Learning world models from reward-free exploration is a promising approach, and enables policies to be trained using imagined experience for new tasks. However, achieving a general agent requires robustness across different environments. In this work, we address the novel problem of generating curricula in the reward-free setting to train robust world models. We consider robustness in terms of minimax regret over all environment instantiations and show that the minimax regret can be connected to minimising the maximum error in the world model across environment instances. This result informs our algorithm, WAKER: Weighted Acquisition of Knowledge across Environments for Robustness. WAKER selects environments for data collection based on the estimated error of the world model for each environment. Our experiments demonstrate that WA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08013</link><description>&lt;p&gt;
TopP\&amp;R: &#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#25903;&#25345;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
TopP\&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;Inception Score&#65288;IS&#65289;&#65292;Fr\'echet Inception Distance&#65288;FID&#65289;&#20197;&#21450;Precision and Recall&#65288;P\&amp;R&#65289;&#30340;&#21464;&#20307;&#65292;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#26679;&#26412;&#29305;&#24449;&#20272;&#35745;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35780;&#20272;&#30340;&#36136;&#37327;&#23436;&#20840;&#21462;&#20915;&#20110;&#20854;&#21487;&#38752;&#24615;&#65292;&#20294;&#20854;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#24182;&#27809;&#26377;&#24471;&#21040;&#20005;&#32899;&#30340;&#35752;&#35770;&#65288;&#24182;&#34987;&#24573;&#35270;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65288;TopP\&amp;R&#65292;&#21457;&#38899;&#20026;&#8220;topper&#8221;&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#25903;&#25345;&#65292;&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#12290;&#36825;&#19981;&#20165;&#20351;TopP\&amp;R&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TopP\&amp;R&#23545;&#20110;&#31163;&#32676;&#20540;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&amp;R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&amp;R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&amp;R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&amp;R is robust to outliers and non-independent and identically distributed
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#20165;&#26377;&#23569;&#37327;&#26681;&#22240;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;DAGs&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15936</link><description>&lt;p&gt;
&#20174;&#23569;&#37327;&#26681;&#22240;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;DAGs
&lt;/p&gt;
&lt;p&gt;
Learning DAGs from Data with Few Root Causes. (arXiv:2305.15936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15936
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#20165;&#26377;&#23569;&#37327;&#26681;&#22240;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;DAGs&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#21644;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32447;&#24615;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;(SEM)&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#32447;&#24615;SEM&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#32447;&#24615;&#21464;&#25442;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#19968;&#20010;&#30001;&#19982;&#33410;&#28857;&#20851;&#32852;&#30340;&#38543;&#26426;&#20540;&#26681;&#22240;(&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;)&#30340;&#31264;&#23494;&#36755;&#20837;&#21521;&#37327;&#35745;&#31639;&#25968;&#25454;&#12290;&#25105;&#20204;&#32771;&#34385;&#20165;&#23384;&#22312;&#20960;&#20010;&#26681;&#22240;(&#36817;&#20284;)&#30340;&#24773;&#20917;&#65292;&#24182;&#22312;&#25968;&#25454;&#30340;&#27979;&#37327;&#20013;&#24341;&#20837;&#22122;&#22768;&#12290;&#20174;&#30452;&#35273;&#19978;&#35762;&#65292;&#36825;&#24847;&#21619;&#30528;DAG&#25968;&#25454;&#26159;&#30001;&#23569;&#25968;&#25968;&#25454;&#29983;&#25104;&#20107;&#20214;&#20135;&#29983;&#30340;&#65292;&#20854;&#25928;&#26524;&#36890;&#36807;DAG&#20256;&#25773;&#12290;&#25105;&#20204;&#22312;&#36825;&#31181;&#26032;&#35774;&#32622;&#20013;&#35777;&#26126;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#34920;&#26126;&#30495;&#27491;&#30340;DAG&#26159;&#26681;&#22240;&#21521;&#37327;L0-&#33539;&#25968;&#30340;&#20840;&#23616;&#26368;&#23567;&#21270;&#32773;&#12290;&#23545;&#20110;&#20855;&#26377;&#23569;&#37327;&#26681;&#22240;&#30340;&#25968;&#25454;&#65292;&#26377;&#21644;&#27809;&#26377;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#29616;&#20986;&#27604;&#20197;&#21069;&#30340;DAG&#23398;&#20064;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel perspective and algorithm for learning directed acyclic graphs (DAGs) from data generated by a linear structural equation model (SEM). First, we show that a linear SEM can be viewed as a linear transform that, in prior work, computes the data from a dense input vector of random valued root causes (as we will call them) associated with the nodes. Instead, we consider the case of (approximately) few root causes and also introduce noise in the measurement of the data. Intuitively, this means that the DAG data is produced by few data-generating events whose effect percolates through the DAG. We prove identifiability in this new setting and show that the true DAG is the global minimizer of the $L^0$-norm of the vector of root causes. For data with few root causes, with and without noise, we show superior performance compared to prior DAG learning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#22312;&#19968;&#20010;&#20844;&#20849;&#21464;&#37327;&#30340;&#26465;&#20214;&#19979;&#65292;&#30456;&#24212;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110; MMD &#30340;&#26680;&#27491;&#21017;&#21270;&#22120;&#65292;&#20811;&#26381;&#20102;&#26465;&#20214;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#21644;&#20004;&#20010;&#20998;&#24067;&#20013;&#35813;&#21464;&#37327;&#30340;&#36793;&#32536;&#26159;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.15901</link><description>&lt;p&gt;
&#26465;&#20214;&#20998;&#24067;&#20043;&#38388;&#30340;&#32463;&#39564;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Empirical Optimal Transport between Conditional Distributions. (arXiv:2305.15901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#19968;&#20010;&#20844;&#20849;&#21464;&#37327;&#30340;&#26465;&#20214;&#19979;&#65292;&#30456;&#24212;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110; MMD &#30340;&#26680;&#27491;&#21017;&#21270;&#22120;&#65292;&#20811;&#26381;&#20102;&#26465;&#20214;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#21644;&#20004;&#20010;&#20998;&#24067;&#20013;&#35813;&#21464;&#37327;&#30340;&#36793;&#32536;&#26159;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#20004;&#20010;&#32852;&#21512;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#32771;&#34385;&#22312;&#19968;&#20010;&#20844;&#20849;&#21464;&#37327;&#30340;&#26465;&#20214;&#19979;&#65292;&#30456;&#24212;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20272;&#35745;&#20276;&#38543;&#26465;&#20214;&#20540;&#30340;&#36755;&#36816;&#25104;&#26412;&#65288;Wasserstein &#36317;&#31163;&#65289;&#65292;&#20197;&#21450;&#26465;&#20214;&#20998;&#24067;&#38388;&#30340;&#36755;&#36816;&#35745;&#21010;&#12290;&#30001;&#20110;&#21305;&#37197;&#26465;&#20214;&#20998;&#24067;&#26159;&#30417;&#30563;&#35757;&#32451;&#21028;&#21035;&#27169;&#22411;&#21644;&#65288;&#38544;&#24335;&#65289;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26680;&#24515;&#65292;&#26465;&#20214;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#20855;&#26377;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#34987;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28041;&#21450;&#21040;&#38544;&#24335;&#29305;&#23450;&#20110;&#32852;&#21512;&#65288;&#26679;&#26412;&#65289;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#22240;&#27492;&#21046;&#23450;&#36825;&#20010;&#38382;&#39064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#65288;i&#65289;&#26465;&#20214;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#21644;&#65288;ii&#65289;&#20004;&#20010;&#20998;&#24067;&#20013;&#35813;&#21464;&#37327;&#30340;&#36793;&#32536;&#26159;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#29305;&#23450;&#30340;&#22522;&#20110; MMD&#65288;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65289;&#30340;&#26680;&#27491;&#21017;&#21270;&#22120;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given samples from two joint distributions, we consider the problem of Optimal Transportation (OT) between the corresponding distributions conditioned on a common variable. The objective of this work is to estimate the associated transport cost (Wasserstein distance) as well as the transport plan between the conditionals as a function of the conditioned value. Since matching conditional distributions is at the core of supervised training of discriminative models and (implicit) conditional-generative models, OT between conditionals has the potential to be employed in diverse machine learning applications. However, since the conditionals involved in OT are implicitly specified via the joint samples, it is challenging to formulate this problem, especially when (i) the variable conditioned on is continuous and (ii) the marginal of this variable in the two distributions is different. We overcome these challenges by employing a specific kernel MMD (Maximum Mean Discrepancy) based regularizer
&lt;/p&gt;</description></item><item><title>SMT 2.0&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#20195;&#29702;&#27169;&#22411;&#24037;&#20855;&#21253;&#65292;&#24341;&#20837;&#20102;&#22788;&#29702;&#28151;&#21512;&#21464;&#37327;&#21644;&#23618;&#27425;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#37319;&#26679;&#26041;&#27861;&#12289;&#28155;&#21152;&#26032;&#30340;&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#35745;&#31639;Kriging&#30340;&#26041;&#24046;&#21644;&#26680;&#23548;&#25968;&#26469;&#25913;&#36827;&#20102;SMT&#12290;</title><link>http://arxiv.org/abs/2305.13998</link><description>&lt;p&gt;
SMT 2.0&#65306;&#19968;&#20010;&#20851;&#27880;&#23618;&#27425;&#21644;&#28151;&#21512;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#30340;&#20195;&#29702;&#27169;&#22411;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes. (arXiv:2305.13998v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13998
&lt;/p&gt;
&lt;p&gt;
SMT 2.0&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#20195;&#29702;&#27169;&#22411;&#24037;&#20855;&#21253;&#65292;&#24341;&#20837;&#20102;&#22788;&#29702;&#28151;&#21512;&#21464;&#37327;&#21644;&#23618;&#27425;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#37319;&#26679;&#26041;&#27861;&#12289;&#28155;&#21152;&#26032;&#30340;&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#35745;&#31639;Kriging&#30340;&#26041;&#24046;&#21644;&#26680;&#23548;&#25968;&#26469;&#25913;&#36827;&#20102;SMT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Surrogate Modeling Toolbox (SMT)&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#12289;&#37319;&#26679;&#25216;&#26415;&#21644;&#19968;&#22871;&#31034;&#20363;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SMT 2.0&#65292;&#36825;&#26159;SMT&#30340;&#19968;&#20010;&#37325;&#35201;&#26032;&#29256;&#26412;&#65292;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#21319;&#32423;&#21644;&#26032;&#21151;&#33021;&#12290;&#36825;&#20010;&#29256;&#26412;&#22686;&#21152;&#20102;&#22788;&#29702;&#28151;&#21512;&#21464;&#37327;&#20195;&#29702;&#27169;&#22411;&#21644;&#23618;&#27425;&#21464;&#37327;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#31867;&#22411;&#30340;&#21464;&#37327;&#22312;&#22810;&#20010;&#20195;&#29702;&#24314;&#27169;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;SMT 2.0&#36824;&#36890;&#36807;&#25193;&#23637;&#37319;&#26679;&#26041;&#27861;&#12289;&#28155;&#21152;&#26032;&#30340;&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#35745;&#31639;Kriging&#30340;&#26041;&#24046;&#21644;&#26680;&#23548;&#25968;&#26469;&#25913;&#36827;&#20102;SMT&#12290;&#36825;&#20010;&#29256;&#26412;&#36824;&#21253;&#25324;&#20102;&#22788;&#29702;&#24102;&#22122;&#22768;&#21644;&#20351;&#29992;&#22810;&#20445;&#30495;&#24230;&#25968;&#25454;&#30340;&#26032;&#20989;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SMT 2.0&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#23618;&#27425;&#21644;&#28151;&#21512;&#36755;&#20837;&#30340;&#24320;&#28304;&#20195;&#29702;&#24211;&#12290;&#36825;&#20010;&#24320;&#28304;&#36719;&#20214;&#37319;&#29992;New BSD&#35768;&#21487;&#35777;&#36827;&#34892;&#20998;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36870;&#20248;&#21270;&#23398;&#20064;&#30340;&#26032;&#27010;&#24565;&#8212;&#8212;&#20869;&#24515;&#27010;&#24565;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#21487;&#34892;&#20984;&#24418;&#24335;&#65292;&#24182;&#24320;&#21457;&#20102;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;ASL&#20197;&#21450;&#19968;&#38454;&#31639;&#27861;Stochastic Approximate Mirror Descent&#65288;SAM&#65289;&#26469;&#23398;&#20064;&#19987;&#23478;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.07730</link><description>&lt;p&gt;
&#36870;&#20248;&#21270;&#23398;&#20064;&#65306;&#20869;&#24515;&#25104;&#26412;&#12289;&#22686;&#24378;&#27425;&#20248;&#25439;&#22833;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning in Inverse Optimization: Incenter Cost, Augmented Suboptimality Loss, and Algorithms. (arXiv:2305.07730v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36870;&#20248;&#21270;&#23398;&#20064;&#30340;&#26032;&#27010;&#24565;&#8212;&#8212;&#20869;&#24515;&#27010;&#24565;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#21487;&#34892;&#20984;&#24418;&#24335;&#65292;&#24182;&#24320;&#21457;&#20102;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;ASL&#20197;&#21450;&#19968;&#38454;&#31639;&#27861;Stochastic Approximate Mirror Descent&#65288;SAM&#65289;&#26469;&#23398;&#20064;&#19987;&#23478;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36870;&#20248;&#21270;&#23398;&#20064;&#20013;&#65292;&#19987;&#23478;&#20195;&#29702;&#20154;&#35299;&#20915;&#21442;&#25968;&#21270;&#20110;&#22806;&#37096;&#20449;&#21495;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#20174;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#19968;&#20010;&#20449;&#21495;&#21644;&#30456;&#24212;&#26368;&#20248;&#34892;&#21160;&#30340;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#19987;&#23478;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;&#21463;&#21040;&#19982;&#36870;&#20248;&#21270;&#38598;&#19968;&#33268;&#30340;&#25104;&#26412;&#21521;&#37327;&#30340;&#20960;&#20309;&#24418;&#29366;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31867;&#20284;&#20110;Besbes&#31561;&#20154;&#26368;&#36817;&#25552;&#20986;&#30340;&#22806;&#24515;&#27010;&#24565;&#30340; "&#20869;&#24515;"&#27010;&#24565;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20869;&#24515;&#25104;&#26412;&#21521;&#37327;&#30340;&#20960;&#20309;&#21644;&#40065;&#26834;&#24615;&#35299;&#37322;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#21487;&#34892;&#20984;&#24418;&#24335;&#65292;&#19982;&#22806;&#24515;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22806;&#25509;&#22278;&#31561;&#25928;&#20110;&#19968;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#20248;&#21270;&#31243;&#24207;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#22686;&#24378;&#27425;&#20248;&#25439;&#22833;&#65288;ASL&#65289;&#65292;&#20316;&#20026;&#20869;&#24515;&#27010;&#24565;&#30340;&#19968;&#31181;&#26494;&#24347;&#24418;&#24335;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#19968;&#33268;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#21033;&#29992;ASL&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;&#38543;&#26426;&#36924;&#36817;&#38236;&#20687;&#19979;&#38477;&#12290;&#36825;&#31181;&#31639;&#27861;&#24102;&#26469;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Inverse Optimization (IO), an expert agent solves an optimization problem parametric in an exogenous signal. From a learning perspective, the goal is to learn the expert's cost function given a dataset of signals and corresponding optimal actions. Motivated by the geometry of the IO set of consistent cost vectors, we introduce the "incenter" concept, a new notion akin to circumcenter recently proposed by Besbes et al. [2022]. Discussing the geometric and robustness interpretation of the incenter cost vector, we develop corresponding tractable convex reformulations, which are in contrast with the circumcenter, which we show is equivalent to an intractable optimization program. We further propose a novel loss function called Augmented Suboptimality Loss (ASL), as a relaxation of the incenter concept, for problems with inconsistent data. Exploiting the structure of the ASL, we propose a novel first-order algorithm, which we name Stochastic Approximate Mirror Descent. This algorithm com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;PCA&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#23548;&#20986;&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#20197;&#35299;&#20915;&#20197;&#24448;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02803</link><description>&lt;p&gt;
&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#24352;&#37327;PCA
&lt;/p&gt;
&lt;p&gt;
Tensor PCA from basis in tensor space. (arXiv:2305.02803v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;PCA&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#23548;&#20986;&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#20197;&#35299;&#20915;&#20197;&#24448;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;PCA&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#20197;&#21069;&#36890;&#36807;&#36845;&#20195;&#27714;&#35299;&#20248;&#21270;&#38382;&#39064;&#26469;&#25552;&#21462;&#20302;&#32500;&#23376;&#31354;&#38388;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20174;&#23454;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#20013;&#23548;&#20986;&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#65292;&#20174;&#32780;&#23558;&#22522;&#30784;&#30340;&#23548;&#20986;&#38382;&#39064;&#36716;&#21270;&#20026;&#29305;&#24449;&#20540;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#24773;&#20917;&#30340;&#23548;&#20986;&#65306;i&#65289;&#20174;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#20013;&#23548;&#20986;&#22522;&#30784;&#65307;ii&#65289;&#23548;&#20986;&#31209;&#20026;1&#30340;&#22522;&#30784;&#65307;iii&#65289;&#20174;&#23376;&#31354;&#38388;&#20013;&#23548;&#20986;&#22522;&#30784;&#12290;&#29305;&#21035;&#26159;&#65292;&#35777;&#26126;&#20102;&#23454;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#26041;&#31243;&#19982;&#26631;&#20934;&#30697;&#38453;&#29305;&#24449;&#20540;&#26041;&#31243;&#30340;&#31561;&#20215;&#24615;&#12290;&#38024;&#23545;&#25152;&#32771;&#34385;&#30340;&#19977;&#31181;&#24773;&#20917;&#65292;&#37319;&#29992;&#20102;&#23376;&#31354;&#38388;&#26041;&#27861;&#26469;&#23548;&#20986;&#24352;&#37327;PCA&#12290;&#22522;&#20110;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this paper is to present a mathematical framework for tensor PCA. The proposed approach is able to overcome the limitations of previous methods that extract a low dimensional subspace by iteratively solving an optimization problem. The core of the proposed approach is the derivation of a basis in tensor space from a real self-adjoint tensor operator, thus reducing the problem of deriving a basis to an eigenvalue problem. Three different cases have been studied to derive: i) a basis from a self-adjoint tensor operator; ii) a rank-1 basis; iii) a basis in a subspace. In particular, the equivalence between eigenvalue equation for a real self-adjoint tensor operator and standard matrix eigenvalue equation has been proven. For all the three cases considered, a subspace approach has been adopted to derive a tensor PCA. Experiments on image datasets validate the proposed mathematical framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#24322;&#36136;&#24615;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#38598;&#20307;&#20851;&#31995;&#25512;&#26029;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#38598;&#20307;&#22320;&#25512;&#26029;&#19981;&#21516;&#36793;&#30340;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65292;&#20351;&#29992;&#29289;&#29702;&#24863;&#24212;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#20855;&#26377;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;&#25512;&#26029;&#20934;&#30830;&#24615;&#21644;&#20445;&#25345;&#29289;&#29702;&#20445;&#30495;&#24230;&#26041;&#38754;&#19968;&#33268;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.00557</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#24322;&#36136;&#24615;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#38598;&#20307;&#20851;&#31995;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Collective Relational Inference for learning physics-consistent heterogeneous particle interactions. (arXiv:2305.00557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#24322;&#36136;&#24615;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#38598;&#20307;&#20851;&#31995;&#25512;&#26029;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#38598;&#20307;&#22320;&#25512;&#26029;&#19981;&#21516;&#36793;&#30340;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65292;&#20351;&#29992;&#29289;&#29702;&#24863;&#24212;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#20855;&#26377;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;&#25512;&#26029;&#20934;&#30830;&#24615;&#21644;&#20445;&#25345;&#29289;&#29702;&#20445;&#30495;&#24230;&#26041;&#38754;&#19968;&#33268;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#22312;&#33258;&#28982;&#30028;&#21644;&#24037;&#31243;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#25581;&#31034;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#23450;&#24459;&#20855;&#26377;&#22522;&#26412;&#37325;&#35201;&#24615;&#65292;&#20294;&#30001;&#20110;&#24213;&#23618;&#37197;&#32622;&#22797;&#26434;&#24615;&#32780;&#20855;&#26377;&#26497;&#22823;&#30340;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#21457;&#29616;&#21516;&#36136;&#31995;&#32479;&#31890;&#23376;&#36712;&#36857;&#20013;&#30340;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26080;&#27861;&#25581;&#31034;&#24322;&#36136;&#31995;&#32479;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32780;&#36825;&#31181;&#31995;&#32479;&#22312;&#29616;&#23454;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20854;&#20013;&#22810;&#20010;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#21516;&#26102;&#23384;&#22312;&#65292;&#24182;&#19988;&#38656;&#35201;&#20851;&#31995;&#25512;&#26029;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26041;&#27861;&#29992;&#20110;&#20851;&#31995;&#25512;&#26029;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#20004;&#20010;&#29420;&#29305;&#30340;&#29305;&#24449;&#65306;&#39318;&#20808;&#65292;&#23427;&#38598;&#20307;&#22320;&#25512;&#26029;&#19981;&#21516;&#36793;&#30340;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65307;&#20854;&#27425;&#65292;&#23427;&#20351;&#29992;&#29289;&#29702;&#24863;&#24212;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#20855;&#26377;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#25512;&#26029;&#30340;&#30456;&#20114;&#20316;&#29992;&#20934;&#30830;&#24615;&#21644;&#20445;&#25345;&#29289;&#29702;&#20445;&#30495;&#24230;&#26041;&#38754;&#19968;&#33268;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30830;&#23450;&#20102;&#20855;&#26377;&#37325;&#35201;&#29289;&#29702;&#24847;&#20041;&#30340;&#26032;&#22411;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65292;&#25581;&#31034;&#20102;&#32479;&#27835;&#31995;&#32479;&#30340;&#38544;&#34255;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#22312;&#25552;&#39640;&#29289;&#29702;&#24615;&#36136;&#30340;&#39044;&#27979;&#26041;&#38754;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interacting particle systems are ubiquitous in nature and engineering. Revealing particle interaction laws is of fundamental importance but also particularly challenging due to underlying configurational complexities. Recently developed machine learning methods show great potential in discovering pairwise interactions from particle trajectories in homogeneous systems. However, they fail to reveal interactions in heterogeneous systems that are prevalent in reality, where multiple interaction types coexist simultaneously and relational inference is required. Here, we propose a novel probabilistic method for relational inference, which possesses two distinctive characteristics compared to existing methods. First, it infers the interaction types of different edges collectively, and second, it uses a physics-induced graph neural network to learn physics-consistent pairwise interactions. We evaluate the proposed methodology across several benchmark datasets and demonstrate that it is consist
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#12290;&#20026;&#25506;&#31350;EA&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26356;&#25509;&#36817;&#29616;&#23454;&#30340;&#39640;&#24230;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03468</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#65306;&#26032;&#25968;&#25454;&#38598;&#21644;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking GNN-based Entity Alignment on Heterogeneous Knowledge Graphs: New Datasets and A New Method. (arXiv:2304.03468v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03468
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#12290;&#20026;&#25506;&#31350;EA&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26356;&#25509;&#36817;&#29616;&#23454;&#30340;&#39640;&#24230;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#24212;&#29992;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#38656;&#35201;&#20174;&#21508;&#31181;&#26469;&#28304;&#25552;&#21462;&#30340;&#24322;&#26500;KG&#20043;&#38388;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#12290;&#36817;&#26469;&#65292;&#30001;&#20110;GNN&#30340;&#20986;&#33394;&#32467;&#26500;&#20449;&#24687;&#25429;&#25417;&#33021;&#21147;&#65292;&#22312;EA&#20219;&#21153;&#20013;&#24191;&#27867;&#37319;&#29992;GNN&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#24120;&#35265;EA&#25968;&#25454;&#38598;&#30340;&#36807;&#20110;&#31616;&#21333;&#21270;&#30340;&#35774;&#32622;&#19982;&#29616;&#23454;&#22330;&#26223;&#30456;&#36317;&#29978;&#36828;&#65292;&#36825;&#22952;&#30861;&#20102;&#23545;&#26368;&#36817;&#26041;&#27861;&#25152;&#21462;&#24471;&#36827;&#23637;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#36825;&#31181;&#29616;&#35937;&#20351;&#25105;&#20204;&#28145;&#24605;&#65306;&#29616;&#26377;&#22522;&#20110;GNN&#30340;EA&#26041;&#27861;&#26159;&#21542;&#30495;&#30340;&#21462;&#24471;&#20102;&#20255;&#22823;&#36827;&#23637;&#65311;&#20026;&#20102;&#30740;&#31350;EA&#26041;&#27861;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#32858;&#28966;&#20110;&#39640;&#24230;&#24322;&#26500;&#30340;KG&#65288;HHKG&#65289;&#65288;&#20363;&#22914;&#65292;&#20107;&#20214;KG&#21644;&#36890;&#29992;KG&#65289;&#30340;&#23545;&#40784;&#65292;&#36825;&#20123;KG&#22312;&#35268;&#27169;&#21644;&#32467;&#26500;&#19978;&#19981;&#21516;&#65292;&#24182;&#20849;&#20139;&#26356;&#23569;&#30340;&#37325;&#21472;&#23454;&#20307;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28165;&#29702;&#20102;&#19981;&#21512;&#29702;&#30340;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;HHKG&#25968;&#25454;&#38598;&#65292;&#20854;&#23494;&#20999;&#22320;&#27169;&#25311;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of knowledge graph (KG) applications has led to a rising need for entity alignment (EA) between heterogeneous KGs that are extracted from various sources. Recently, graph neural networks (GNNs) have been widely adopted in EA tasks due to GNNs' impressive ability to capture structure information. However, we have observed that the oversimplified settings of the existing common EA datasets are distant from real-world scenarios, which obstructs a full understanding of the advancements achieved by recent methods. This phenomenon makes us ponder: Do existing GNN-based EA methods really make great progress?  In this paper, to study the performance of EA methods in realistic settings, we focus on the alignment of highly heterogeneous KGs (HHKGs) (e.g., event KGs and general KGs) which are different with regard to the scale and structure, and share fewer overlapping entities. First, we sweep the unreasonable settings, and propose two new HHKG datasets that closely mimic real-wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#30340;&#39640;&#25928;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;&#65288;EPSL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#27169;&#22411;&#35757;&#32451;&#12290;EPSL&#24182;&#34892;&#21270;&#23458;&#25143;&#31471;&#27169;&#22411;&#35757;&#32451;&#65292;&#36890;&#36807;&#32858;&#21512;&#26799;&#24230;&#38477;&#20302;&#20102;&#21453;&#21521;&#20256;&#25773;&#30340;&#23616;&#37096;&#26799;&#24230;&#32500;&#24230;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#26381;&#21153;&#22120;&#31471;&#30340;&#35757;&#32451;&#21644;&#36890;&#20449;&#24310;&#36831;&#12290;&#21516;&#26102;&#65292;EPSL&#36824;&#35774;&#35745;&#20102;&#36164;&#28304;&#20998;&#37197;&#31639;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2303.15991</link><description>&lt;p&gt;
&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#30340;&#39640;&#25928;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Parallel Split Learning over Resource-constrained Wireless Edge Networks. (arXiv:2303.15991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#30340;&#39640;&#25928;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;&#65288;EPSL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#27169;&#22411;&#35757;&#32451;&#12290;EPSL&#24182;&#34892;&#21270;&#23458;&#25143;&#31471;&#27169;&#22411;&#35757;&#32451;&#65292;&#36890;&#36807;&#32858;&#21512;&#26799;&#24230;&#38477;&#20302;&#20102;&#21453;&#21521;&#20256;&#25773;&#30340;&#23616;&#37096;&#26799;&#24230;&#32500;&#24230;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#26381;&#21153;&#22120;&#31471;&#30340;&#35757;&#32451;&#21644;&#36890;&#20449;&#24310;&#36831;&#12290;&#21516;&#26102;&#65292;EPSL&#36824;&#35774;&#35745;&#20102;&#36164;&#28304;&#20998;&#37197;&#31639;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#28145;&#65292;&#36825;&#38459;&#30861;&#20102;&#32852;&#21512;&#23398;&#20064;&#31561;&#38544;&#31169;&#22686;&#24378;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#24335;&#65288;&#22914;&#32852;&#37030;&#23398;&#20064;&#65289;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#30340;&#27665;&#20027;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#20513;&#23548;&#23558;&#36793;&#32536;&#35745;&#31639;&#33539;&#24335;&#21644;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;&#65288;PSL&#65289;&#30456;&#32467;&#21512;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#35774;&#22791;&#36890;&#36807;&#36880;&#23618;&#27169;&#22411;&#20998;&#35010;&#23558;&#22823;&#37327;&#30340;&#35757;&#32451;&#24037;&#20316;&#36127;&#36733;&#21368;&#36733;&#21040;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;PSL&#26041;&#26696;&#20250;&#20135;&#29983;&#36807;&#22810;&#30340;&#35757;&#32451;&#24310;&#36831;&#21644;&#22823;&#37327;&#30340;&#25968;&#25454;&#20256;&#36755;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;PSL&#26694;&#26550;&#8212;&#8212;&#39640;&#25928;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;&#65288;EPSL&#65289;&#65292;&#20197;&#21152;&#36895;&#27169;&#22411;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EPSL&#23558;&#23458;&#25143;&#31471;&#27169;&#22411;&#35757;&#32451;&#24182;&#34892;&#21270;&#65292;&#24182;&#36890;&#36807;&#26368;&#21518;&#19968;&#23618;&#26799;&#24230;&#32858;&#21512;&#38477;&#20302;&#20102;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#30340;&#23616;&#37096;&#26799;&#24230;&#32500;&#24230;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#26381;&#21153;&#22120;&#31471;&#30340;&#35757;&#32451;&#21644;&#36890;&#20449;&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32771;&#34385;&#36793;&#32536;&#35774;&#22791;&#30340;&#24322;&#26500;&#36890;&#36947;&#26465;&#20214;&#21644;&#35745;&#31639;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36164;&#28304;&#20998;&#37197;&#31639;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EPSL&#36890;&#36807;&#23558;&#36890;&#20449;&#25104;&#26412;&#21644;&#35757;&#32451;&#26102;&#38388;&#20998;&#21035;&#38477;&#20302;76&#65285;&#21644;63&#65285;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;PSL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasingly deeper neural networks hinder the democratization of privacy-enhancing distributed learning, such as federated learning (FL), to resource-constrained devices. To overcome this challenge, in this paper, we advocate the integration of edge computing paradigm and parallel split learning (PSL), allowing multiple client devices to offload substantial training workloads to an edge server via layer-wise model split. By observing that existing PSL schemes incur excessive training latency and large volume of data transmissions, we propose an innovative PSL framework, namely, efficient parallel split learning (EPSL), to accelerate model training. To be specific, EPSL parallelizes client-side model training and reduces the dimension of local gradients for back propagation (BP) via last-layer gradient aggregation, leading to a significant reduction in server-side training and communication latency. Moreover, by considering the heterogeneous channel conditions and computing capabil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#31232;&#30095;&#20234;&#36763;&#26426;&#22120;&#35757;&#32451;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#32593;&#32476;&#30340;&#26032;&#24212;&#29992;&#39046;&#22495;&#65292;&#36890;&#36807;&#22312;&#30828;&#20214;&#24863;&#30693;&#30340;&#32593;&#32476;&#25299;&#25169;&#20013;&#20351;&#29992;&#20234;&#36763;&#26426;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#20248;&#21270;&#30340;&#36719;&#20214;&#27169;&#22411;&#30456;&#21516;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.10728</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#20234;&#36763;&#26426;&#22120;&#35757;&#32451;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Deep Boltzmann Networks with Sparse Ising Machines. (arXiv:2303.10728v2 [cs.ET] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#31232;&#30095;&#20234;&#36763;&#26426;&#22120;&#35757;&#32451;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#32593;&#32476;&#30340;&#26032;&#24212;&#29992;&#39046;&#22495;&#65292;&#36890;&#36807;&#22312;&#30828;&#20214;&#24863;&#30693;&#30340;&#32593;&#32476;&#25299;&#25169;&#20013;&#20351;&#29992;&#20234;&#36763;&#26426;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#20248;&#21270;&#30340;&#36719;&#20214;&#27169;&#22411;&#30456;&#21516;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25705;&#23572;&#23450;&#24459;&#25918;&#32531;&#65292;&#24320;&#21457;&#38750;&#20256;&#32479;&#35745;&#31639;&#33539;&#24335;&#25104;&#20026;&#36235;&#21183;&#65292;&#20854;&#20013;&#21253;&#25324;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#20234;&#36763;&#26426;&#22120;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#21363;&#20351;&#29992;&#20234;&#36763;&#26426;&#22120;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#31232;&#30095;&#12289;&#24322;&#27493;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#30340;&#20234;&#36763;&#26426;&#22120;&#65292;&#22312;&#28151;&#21512;&#27010;&#29575;-&#32463;&#20856;&#35745;&#31639;&#29615;&#22659;&#20013;&#35757;&#32451;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;&#20013;&#31561;&#35268;&#27169;&#30340;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#19978;&#23454;&#29616;&#30828;&#20214;&#24863;&#30693;&#30340;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65292;&#20351;&#29992;&#23436;&#25972;&#30340;MNIST&#21644;Fashion MNIST&#65288;FMNIST&#65289;&#25968;&#25454;&#38598;&#65292;&#27809;&#26377;&#20219;&#20309;&#38477;&#37319;&#26679;&#65292;&#24182;&#23545;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32553;&#20943;&#12290;&#23545;&#20110;MNIST&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26426;&#22120;&#20165;&#20351;&#29992;4,264&#20010;&#33410;&#28857;&#65288;p-bits&#65289;&#21644;&#22823;&#32422;30,000&#20010;&#21442;&#25968;&#65292;&#23601;&#23454;&#29616;&#20102;&#19982;&#20248;&#21270;&#30340;&#22522;&#20110;&#36719;&#20214;&#30340;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#22120;&#65288;RBM&#65289;&#30456;&#21516;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#65288;90%&#65289;&#65292;&#21518;&#32493;&#30340;FMNIST&#21644;CIFAR-10&#32467;&#26524;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
The slowing down of Moore's law has driven the development of unconventional computing paradigms, such as specialized Ising machines tailored to solve combinatorial optimization problems. In this paper, we show a new application domain for probabilistic bit (p-bit) based Ising machines by training deep generative AI models with them. Using sparse, asynchronous, and massively parallel Ising machines we train deep Boltzmann networks in a hybrid probabilistic-classical computing setup. We use the full MNIST and Fashion MNIST (FMNIST) dataset without any downsampling and a reduced version of CIFAR-10 dataset in hardware-aware network topologies implemented in moderately sized Field Programmable Gate Arrays (FPGA). For MNIST, our machine using only 4,264 nodes (p-bits) and about 30,000 parameters achieves the same classification accuracy (90%) as an optimized software-based restricted Boltzmann Machine (RBM) with approximately 3.25 million parameters. Similar results follow for FMNIST and C
&lt;/p&gt;</description></item><item><title>cito&#26159;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;R&#21253;&#65292;&#20351;&#29992;torch&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#21253;&#25324;&#35768;&#22810;&#23545;&#39044;&#27979;&#21644;&#35780;&#20272;&#27169;&#22411;&#26377;&#29992;&#30340;&#29992;&#25143;&#21451;&#22909;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09599</link><description>&lt;p&gt;
cito: &#20351;&#29992;torch&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;R&#21253;
&lt;/p&gt;
&lt;p&gt;
cito: An R package for training neural networks using torch. (arXiv:2303.09599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09599
&lt;/p&gt;
&lt;p&gt;
cito&#26159;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;R&#21253;&#65292;&#20351;&#29992;torch&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#21253;&#25324;&#35768;&#22810;&#23545;&#39044;&#27979;&#21644;&#35780;&#20272;&#27169;&#22411;&#26377;&#29992;&#30340;&#29992;&#25143;&#21451;&#22909;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#24050;&#25104;&#20026;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#20013;&#24515;&#31639;&#27861;&#31867;&#21035;&#12290;&#34429;&#28982;&#19968;&#20123;&#21253;&#20801;&#35768;&#29992;&#25143;&#22312;R&#20013;&#25351;&#23450;DNN&#65292;&#20294;&#23427;&#20204;&#22312;&#21151;&#33021;&#19978;&#30456;&#24403;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#20381;&#36182;&#20110;&#20027;&#35201;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;PyTorch&#25110;TensorFlow&#26469;&#26500;&#24314;&#21644;&#35757;&#32451;DNN&#12290;&#28982;&#32780;&#65292;&#19982;R&#29615;&#22659;&#20013;&#21487;&#27604;&#30340;&#22238;&#24402;&#25110;&#26426;&#22120;&#23398;&#20064;&#21253;&#30456;&#27604;&#65292;&#20351;&#29992;&#36825;&#20123;&#26694;&#26550;&#38656;&#35201;&#26356;&#22810;&#30340;&#22521;&#35757;&#21644;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#20171;&#32461;&#20102;cito&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;R&#21253;&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#12290;cito&#20801;&#35768;R&#29992;&#25143;&#20351;&#29992;&#22823;&#22810;&#25968;R&#24314;&#27169;&#20989;&#25968;&#20013;&#20351;&#29992;&#30340;&#29087;&#24713;&#30340;&#20844;&#24335;&#35821;&#27861;&#26469;&#25351;&#23450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#21518;&#21488;&#65292;cito&#20351;&#29992;torch&#26469;&#25311;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;torch&#24211;&#30340;&#25152;&#26377;&#25968;&#20540;&#20248;&#21270;&#65292;&#21253;&#25324;&#22312;CPU&#25110;GPU&#19978;&#20999;&#25442;&#35757;&#32451;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;cito&#21253;&#25324;&#35768;&#22810;&#29992;&#20110;&#39044;&#27979;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#29992;&#25143;&#21451;&#22909;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
1. Deep neural networks (DNN) have become a central class of algorithms for regression and classification tasks. Although some packages exist that allow users to specify DNN in R, those are rather limited in their functionality. Most current deep learning applications therefore rely on one of the major deep learning frameworks, PyTorch or TensorFlow, to build and train DNN. However, using these frameworks requires substantially more training and time than comparable regression or machine learning packages in the R environment.  2. Here, we present cito, an user-friendly R package for deep learning. cito allows R users to specify deep neural networks in the familiar formula syntax used by most modeling functions in R. In the background, cito uses torch to fit the models, taking advantage of all the numerical optimizations of the torch library, including the ability to switch between training models on CPUs or GPUs. Moreover, cito includes many user-friendly functions for predictions and
&lt;/p&gt;</description></item><item><title>DeepGD&#26159;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#30446;&#26631;&#40657;&#30418;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#39640;&#38169;&#35823;&#26292;&#38706;&#33021;&#21147;&#30340;&#27979;&#35797;&#36755;&#20837;&#26469;&#38477;&#20302;&#26631;&#35760;&#25104;&#26412;&#65292;&#21516;&#26102;&#36873;&#25321;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#30340;&#27979;&#35797;&#36755;&#20837;&#20197;&#23613;&#21487;&#33021;&#35302;&#21457;&#26356;&#22810;&#30340;&#35823;&#39044;&#27979;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#25581;&#31034;DNN&#27169;&#22411;&#20013;&#19981;&#21516;&#32570;&#38519;&#30340;&#27010;&#29575;&#26469;&#22686;&#21152;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.04878</link><description>&lt;p&gt;
DeepGD: &#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#30446;&#26631;&#40657;&#30418;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks. (arXiv:2303.04878v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04878
&lt;/p&gt;
&lt;p&gt;
DeepGD&#26159;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#30446;&#26631;&#40657;&#30418;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#39640;&#38169;&#35823;&#26292;&#38706;&#33021;&#21147;&#30340;&#27979;&#35797;&#36755;&#20837;&#26469;&#38477;&#20302;&#26631;&#35760;&#25104;&#26412;&#65292;&#21516;&#26102;&#36873;&#25321;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#30340;&#27979;&#35797;&#36755;&#20837;&#20197;&#23613;&#21487;&#33021;&#35302;&#21457;&#26356;&#22810;&#30340;&#35823;&#39044;&#27979;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#25581;&#31034;DNN&#27169;&#22411;&#20013;&#19981;&#21516;&#32570;&#38519;&#30340;&#27010;&#29575;&#26469;&#22686;&#21152;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36755;&#20837;&#22495;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#65292;&#27979;&#35797;DNN&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#27979;&#35797;DNN&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#29983;&#25104;&#25110;&#25506;&#32034;&#22823;&#35268;&#27169;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;DNN&#27979;&#35797;&#31070;&#35861;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#24037;&#20316;&#26469;&#26631;&#35760;&#27979;&#35797;&#25968;&#25454;&#65292;&#21487;&#33021;&#28041;&#21450;&#22810;&#20010;&#19987;&#23478;&#20197;&#30830;&#20445;&#26631;&#35760;&#30340;&#27491;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepGD&#65292;&#19968;&#31181;&#29992;&#20110;DNN&#27169;&#22411;&#30340;&#40657;&#30418;&#22810;&#30446;&#26631;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#39640;&#38169;&#35823;&#26292;&#38706;&#33021;&#21147;&#30340;&#27979;&#35797;&#36755;&#20837;&#26469;&#38477;&#20302;&#26631;&#35760;&#25104;&#26412;&#65292;&#21516;&#26102;&#36873;&#25321;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#30340;&#27979;&#35797;&#36755;&#20837;&#20197;&#23613;&#21487;&#33021;&#35302;&#21457;&#26356;&#22810;&#30340;&#35823;&#39044;&#27979;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#25581;&#31034;DNN&#27169;&#22411;&#20013;&#19981;&#21516;&#32570;&#38519;&#30340;&#27010;&#29575;&#26469;&#22686;&#21152;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are widely used in various application domains such as image processing, speech recognition, and natural language processing. However, testing DNN models may be challenging due to the complexity and size of their input domain. Particularly, testing DNN models often requires generating or exploring large unlabeled datasets. In practice, DNN test oracles, which identify the correct outputs for inputs, often require expensive manual effort to label test data, possibly involving multiple experts to ensure labeling correctness. In this paper, we propose DeepGD, a black-box multi-objective test selection approach for DNN models. It reduces the cost of labeling by prioritizing the selection of test inputs with high fault revealing power from large unlabeled datasets. DeepGD not only selects test inputs with high uncertainty scores to trigger as many mispredicted inputs as possible but also maximizes the probability of revealing distinct faults in the DNN model by s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#22825;&#32447;&#31995;&#32479;&#20013;&#37319;&#29992;&#25968;&#23383;&#35843;&#21046;&#21644;&#31354;&#20013;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#32467;&#21512;&#25968;&#23383;&#35843;&#21046;&#21644;AirComp&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#26080;&#32447;&#20449;&#36947;&#34928;&#33853;&#23548;&#33268;&#30340;&#24635;&#20307;&#22833;&#30495;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.14648</link><description>&lt;p&gt;
&#22810;&#22825;&#32447;&#31995;&#32479;&#20013;&#25968;&#23383;&#26080;&#32447;&#21327;&#20316;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Digital Over-the-Air Federated Learning in Multi-Antenna Systems. (arXiv:2302.14648v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#22825;&#32447;&#31995;&#32479;&#20013;&#37319;&#29992;&#25968;&#23383;&#35843;&#21046;&#21644;&#31354;&#20013;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#32467;&#21512;&#25968;&#23383;&#35843;&#21046;&#21644;AirComp&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#26080;&#32447;&#20449;&#36947;&#34928;&#33853;&#23548;&#33268;&#30340;&#24635;&#20307;&#22833;&#30495;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#38469;&#30340;&#26080;&#32447;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#37319;&#29992;&#25968;&#23383;&#35843;&#21046;&#21644;&#31354;&#20013;&#35745;&#31639;&#65288;AirComp&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#29305;&#21035;&#32771;&#34385;&#20102;&#19968;&#20010;MIMO&#31995;&#32479;&#65292;&#22312;&#35813;&#31995;&#32479;&#20013;&#65292;&#36793;&#32536;&#35774;&#22791;&#20351;&#29992;&#27874;&#26463;&#24418;&#25104;&#23558;&#20854;&#26412;&#22320;&#25910;&#38598;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#26412;&#22320;FL&#27169;&#22411;&#20256;&#36755;&#32473;&#21442;&#25968;&#26381;&#21153;&#22120;&#65288;PS&#65289;&#65292;&#20197;&#26368;&#22823;&#21270;&#21487;&#35843;&#24230;&#20256;&#36755;&#30340;&#35774;&#22791;&#25968;&#37327;&#12290;PS&#20316;&#20026;&#20013;&#22830;&#25511;&#21046;&#22120;&#65292;&#20351;&#29992;&#25509;&#25910;&#21040;&#30340;&#26412;&#22320;FL&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#20840;&#23616;FL&#27169;&#22411;&#24182;&#24191;&#25773;&#32473;&#25152;&#26377;&#35774;&#22791;&#12290;&#30001;&#20110;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24102;&#23485;&#26377;&#38480;&#65292;&#37319;&#29992;&#20102;AirComp&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26080;&#32447;&#25968;&#25454;&#32858;&#21512;&#12290;&#28982;&#32780;&#65292;&#26080;&#32447;&#20449;&#36947;&#30340;&#34928;&#33853;&#20250;&#20135;&#29983;&#22522;&#20110;AirComp&#30340;FL&#26041;&#26696;&#20013;&#30340;&#24635;&#20307;&#22833;&#30495;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#31639;&#27861;&#65292;&#23558;&#25968;&#23383;&#35843;&#21046;&#19982;AirComp&#30456;&#32467;&#21512;&#20197;&#20943;&#36731;&#22833;&#30495;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, the performance optimization of federated learning (FL), when deployed over a realistic wireless multiple-input multiple-output (MIMO) communication system with digital modulation and over-the-air computation (AirComp) is studied. In particular, a MIMO system is considered in which edge devices transmit their local FL models (trained using their locally collected data) to a parameter server (PS) using beamforming to maximize the number of devices scheduled for transmission. The PS, acting as a central controller, generates a global FL model using the received local FL models and broadcasts it back to all devices. Due to the limited bandwidth in a wireless network, AirComp is adopted to enable efficient wireless data aggregation. However, fading of wireless channels can produce aggregate distortions in an AirComp-based FL scheme. To tackle this challenge, we propose a modified federated averaging (FedAvg) algorithm that combines digital modulation with AirComp to mitigate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#20869;&#37096;&#22352;&#26631;&#20013;&#24314;&#27169;&#34507;&#30333;&#36136;&#23494;&#24230;&#12290;&#36890;&#36807;&#32422;&#26463;&#26469;&#35825;&#23548;&#20869;&#37096;&#33258;&#30001;&#24230;&#20043;&#38388;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#28385;&#36275;&#32467;&#26500;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20840;&#21327;&#26041;&#24046;&#36755;&#20986;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#25104;&#21151;&#23558;&#20869;&#37096;&#22352;&#26631;&#30340;&#23494;&#24230;&#27169;&#22411;&#25193;&#23637;&#21040;&#23436;&#25972;&#30340;&#34507;&#30333;&#36136;&#39592;&#26550;&#12290;</title><link>http://arxiv.org/abs/2302.13711</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20869;&#37096;&#22352;&#26631;&#23494;&#24230;&#24314;&#27169;&#65306;&#21327;&#26041;&#24046;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Internal-Coordinate Density Modelling of Protein Structure: Covariance Matters. (arXiv:2302.13711v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#20869;&#37096;&#22352;&#26631;&#20013;&#24314;&#27169;&#34507;&#30333;&#36136;&#23494;&#24230;&#12290;&#36890;&#36807;&#32422;&#26463;&#26469;&#35825;&#23548;&#20869;&#37096;&#33258;&#30001;&#24230;&#20043;&#38388;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#28385;&#36275;&#32467;&#26500;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20840;&#21327;&#26041;&#24046;&#36755;&#20986;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#25104;&#21151;&#23558;&#20869;&#37096;&#22352;&#26631;&#30340;&#23494;&#24230;&#27169;&#22411;&#25193;&#23637;&#21040;&#23436;&#25972;&#30340;&#34507;&#30333;&#36136;&#39592;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#37325;&#22823;&#31361;&#30772;&#21518;&#65292;&#34507;&#30333;&#36136;&#26426;&#22120;&#23398;&#20064;&#20013;&#20173;&#38754;&#20020;&#30528;&#19968;&#20010;&#25361;&#25112;&#65306;&#21487;&#38752;&#22320;&#39044;&#27979;&#32467;&#26500;&#29366;&#24577;&#30340;&#20998;&#24067;&#12290;&#30001;&#20110;&#34507;&#30333;&#36136;&#38142;&#20013;&#33258;&#30001;&#24230;&#20043;&#38388;&#22797;&#26434;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#21442;&#25968;&#27169;&#22411;&#30340;&#25311;&#21512;&#21464;&#24471;&#22256;&#38590;&#65292;&#36890;&#24120;&#23548;&#33268;&#27169;&#22411;&#36829;&#21453;&#23616;&#37096;&#25110;&#20840;&#23616;&#30340;&#32467;&#26500;&#32422;&#26463;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20869;&#37096;&#22352;&#26631;&#20013;&#24314;&#27169;&#34507;&#30333;&#36136;&#23494;&#24230;&#30340;&#26032;&#31574;&#30053;&#65292;&#20854;&#20013;&#21033;&#29992;&#19977;&#32500;&#31354;&#38388;&#20013;&#30340;&#32422;&#26463;&#26469;&#35825;&#23548;&#20869;&#37096;&#33258;&#30001;&#24230;&#20043;&#38388;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#30001;&#19977;&#32500;&#26465;&#20214;&#22343;&#20540;&#25152;&#34164;&#21547;&#30340;&#32422;&#26463;&#24341;&#21457;&#30340;&#20840;&#21327;&#26041;&#24046;&#36755;&#20986;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#20869;&#37096;&#22352;&#26631;&#30340;&#23494;&#24230;&#27169;&#22411;&#25193;&#23637;&#21040;&#23436;&#25972;&#30340;&#34507;&#30333;&#36136;&#39592;&#26550;&#65292;&#26377;&#20004;&#31181;&#35774;&#32622;&#65306;1)&#38024;&#23545;&#23637;&#29616;&#23567;&#27874;&#21160;&#30340;&#34507;&#30333;&#36136;&#30340;&#21333;&#23792;&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
After the recent ground-breaking advances in protein structure prediction, one of the remaining challenges in protein machine learning is to reliably predict distributions of structural states. Parametric models of fluctuations are difficult to fit due to complex covariance structures between degrees of freedom in the protein chain, often causing models to either violate local or global structural constraints. In this paper, we present a new strategy for modelling protein densities in internal coordinates, which uses constraints in 3D space to induce covariance structure between the internal degrees of freedom. We illustrate the potential of the procedure by constructing a variational autoencoder with full covariance output induced by the constraints implied by the conditional mean in 3D, and demonstrate that our approach makes it possible to scale density models of internal coordinates to full protein backbones in two settings: 1) a unimodal setting for proteins exhibiting small fluct
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#29992;&#20110;&#30284;&#30151;&#20998;&#23376;&#20122;&#22411;&#20998;&#31867;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#22312;&#36830;&#25509;&#31867;&#22411;&#12289;GNN&#23618;&#21644;&#22797;&#26434;&#20998;&#31867;&#27979;&#35797;&#26041;&#38754;&#26377;&#19981;&#21516;&#65292;&#20174;&#32780;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.12838</link><description>&lt;p&gt;
&#30284;&#30151;&#20998;&#23376;&#20122;&#22411;&#20998;&#31867;&#30340;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Graph Neural Network Framework of Cancer Molecular Subtype Classification. (arXiv:2302.12838v2 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#29992;&#20110;&#30284;&#30151;&#20998;&#23376;&#20122;&#22411;&#20998;&#31867;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#22312;&#36830;&#25509;&#31867;&#22411;&#12289;GNN&#23618;&#21644;&#22797;&#26434;&#20998;&#31867;&#27979;&#35797;&#26041;&#38754;&#26377;&#19981;&#21516;&#65292;&#20174;&#32780;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36890;&#37327;&#27979;&#24207;&#25216;&#26415;&#30340;&#21457;&#23637;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26356;&#22909;&#22320;&#30740;&#31350;&#30284;&#30151;&#20998;&#23376;&#29305;&#24449;&#21644;&#22522;&#20110;&#20998;&#23376;&#20122;&#22411;&#30340;&#30284;&#30151;&#20998;&#31867;&#12290;&#25972;&#21512;&#22810;&#32452;&#23398;&#25968;&#25454;&#24050;&#34987;&#35777;&#26126;&#23545;&#26500;&#24314;&#26356;&#31934;&#20934;&#30340;&#20998;&#31867;&#27169;&#22411;&#38750;&#24120;&#26377;&#25928;&#12290;&#30446;&#21069;&#30340;&#22810;&#32452;&#23398;&#25972;&#21512;&#27169;&#22411;&#20027;&#35201;&#20351;&#29992;&#26089;&#26399;&#34701;&#21512;&#65288;&#20018;&#32852;&#65289;&#25110;&#32773;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26202;&#26399;&#34701;&#21512;&#12290;&#30001;&#20110;&#29983;&#29289;&#31995;&#32479;&#30340;&#29305;&#24615;&#65292;&#22270;&#26159;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#26356;&#22909;&#34920;&#31034;&#24418;&#24335;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#22810;&#32452;&#23398;&#25972;&#21512;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#19977;&#20010;&#24120;&#35265;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#19968;&#26159;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#20351;&#29992;&#19968;&#31181;&#31867;&#22411;&#30340;&#36830;&#25509;&#65292;&#35201;&#20040;&#26159;&#32452;&#38388;&#36830;&#25509;&#65292;&#35201;&#20040;&#26159;&#32452;&#20869;&#36830;&#25509;&#65307;&#20108;&#26159;&#23427;&#20204;&#21482;&#32771;&#34385;&#19968;&#31181;&#31867;&#22411;&#30340;GNN&#23618;&#65292;&#35201;&#20040;&#26159;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#65292;&#35201;&#20040;&#26159;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#65307;&#19977;&#26159;&#22823;&#22810;&#25968;&#26041;&#27861;&#32570;&#20047;&#26356;&#22797;&#26434;&#30340;&#30284;&#30151;&#20998;&#31867;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development of high-throughput sequencing creates a large collection of multi-omics data, which enables researchers to better investigate cancer molecular profiles and cancer taxonomy based on molecular subtypes. Integrating multi-omics data has been proven to be effective for building more precise classification models. Current multi-omics integrative models mainly use early fusion by concatenation or late fusion based on deep neural networks. Due to the nature of biological systems, graphs are a better representation of bio-medical data. Although few graph neural network (GNN) based multi-omics integrative methods have been proposed, they suffer from three common disadvantages. One is most of them use only one type of connection, either inter-omics or intra-omic connection; second, they only consider one kind of GNN layer, either graph convolution network (GCN) or graph attention network (GAT); and third, most of these methods lack testing on a more complex cancer classifi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32771;&#23519;&#20102;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#65292;&#25910;&#36141;&#21151;&#33021;&#26368;&#22823;&#21270;&#22120;&#21021;&#22987;&#21270;&#23545;&#21033;&#29992;&#25910;&#36141;&#21151;&#33021;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#38543;&#26426;&#21021;&#22987;&#21270;&#26041;&#27861;&#19981;&#33021;&#20805;&#20998;&#21457;&#25381;&#25910;&#36141;&#21151;&#33021;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;&#26469;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2302.08298</link><description>&lt;p&gt;
&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#37322;&#25918;&#25910;&#36141;&#21151;&#33021;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Potential of Acquisition Functions in High-Dimensional Bayesian Optimization. (arXiv:2302.08298v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08298
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32771;&#23519;&#20102;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#65292;&#25910;&#36141;&#21151;&#33021;&#26368;&#22823;&#21270;&#22120;&#21021;&#22987;&#21270;&#23545;&#21033;&#29992;&#25910;&#36141;&#21151;&#33021;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#38543;&#26426;&#21021;&#22987;&#21270;&#26041;&#27861;&#19981;&#33021;&#20805;&#20998;&#21457;&#25381;&#25910;&#36141;&#21151;&#33021;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;&#26469;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;BO&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#26469;&#34920;&#31034;&#30446;&#26631;&#20989;&#25968;&#24182;&#35780;&#20272;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#21518;&#65292;&#23427;&#36890;&#36807;&#26368;&#22823;&#21270;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#25910;&#36141;&#20989;&#25968;&#65288;AF&#65289;&#26469;&#20915;&#23450;&#37319;&#26679;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#38382;&#39064;&#26102;&#65292;&#25214;&#21040;AF&#30340;&#20840;&#23616;&#26368;&#22823;&#20540;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;AF&#26368;&#22823;&#21270;&#22120;&#30340;&#21021;&#22987;&#21270;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#22240;&#20026;&#19981;&#24688;&#24403;&#30340;&#35774;&#32622;&#21487;&#33021;&#20005;&#37325;&#24433;&#21709;AF&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#24456;&#22823;&#31243;&#24230;&#19978;&#26410;&#34987;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#21363;AF&#26368;&#22823;&#21270;&#22120;&#21021;&#22987;&#21270;&#23545;&#21033;&#29992;AF&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#31574;&#30053;&#24448;&#24448;&#26080;&#27861;&#20805;&#20998;&#21457;&#25381;AF&#30340;&#28508;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#21551;&#21457;&#24335;&#20248;&#21270;&#22120;&#26469;&#21033;&#29992;&#40657;&#30418;&#30340;&#21382;&#21490;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is widely used to optimize expensive-to-evaluate black-box functions.BO first builds a surrogate model to represent the objective function and assesses its uncertainty. It then decides where to sample by maximizing an acquisition function (AF) based on the surrogate model. However, when dealing with high-dimensional problems, finding the global maximum of the AF becomes increasingly challenging. In such cases, the initialization of the AF maximizer plays a pivotal role, as an inadequate setup can severely hinder the effectiveness of the AF.  This paper investigates a largely understudied problem concerning the impact of AF maximizer initialization on exploiting AFs' capability. Our large-scale empirical study shows that the widely used random initialization strategy often fails to harness the potential of an AF. In light of this, we propose a better initialization approach by employing multiple heuristic optimizers to leverage the historical data of black-box
&lt;/p&gt;</description></item><item><title>PECAN&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#32463;&#36807;&#35748;&#35777;&#30340;&#21518;&#38376;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#30456;&#20132;&#20998;&#21306;&#19978;&#35757;&#32451;&#19968;&#32452;&#31070;&#32463;&#32593;&#32476;&#24182;&#24212;&#29992;&#27979;&#35797;&#26102;&#38388;&#36867;&#36991;&#35748;&#35777;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38450;&#24481;&#24378;&#24230;&#21644;&#25928;&#29575;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.11824</link><description>&lt;p&gt;
PECAN: &#19968;&#31181;&#38024;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#30830;&#23450;&#24615;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PECAN: A Deterministic Certified Defense Against Backdoor Attacks. (arXiv:2301.11824v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11824
&lt;/p&gt;
&lt;p&gt;
PECAN&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#32463;&#36807;&#35748;&#35777;&#30340;&#21518;&#38376;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#30456;&#20132;&#20998;&#21306;&#19978;&#35757;&#32451;&#19968;&#32452;&#31070;&#32463;&#32593;&#32476;&#24182;&#24212;&#29992;&#27979;&#35797;&#26102;&#38388;&#36867;&#36991;&#35748;&#35777;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38450;&#24481;&#24378;&#24230;&#21644;&#25928;&#29575;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#24694;&#24847;&#27745;&#26579;&#35757;&#32451;&#38598;&#24182;&#22312;&#27979;&#35797;&#36755;&#20837;&#20013;&#25554;&#20837;&#35302;&#21457;&#22120;&#20197;&#25913;&#21464;&#21463;&#23475;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#35201;&#20040;&#27809;&#26377;&#25552;&#20379;&#27491;&#24335;&#30340;&#20445;&#35777;&#65292;&#35201;&#20040;&#20855;&#26377;&#35745;&#31639;&#26114;&#36149;&#19988;&#26080;&#25928;&#30340;&#27010;&#29575;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#32463;&#36807;&#35748;&#35777;&#30340;&#21518;&#38376;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;PECAN&#12290;PECAN&#30340;&#26680;&#24515;&#27934;&#35265;&#26159;&#22312;&#25968;&#25454;&#30340;&#19981;&#30456;&#20132;&#20998;&#21306;&#19978;&#35757;&#32451;&#19968;&#32452;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24212;&#29992;&#29616;&#25104;&#30340;&#27979;&#35797;&#26102;&#38388;&#36867;&#36991;&#35748;&#35777;&#25216;&#26415;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;PECAN&#12290;&#32467;&#26524;&#34920;&#26126;PECAN&#22312;&#38450;&#24481;&#24378;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#32463;&#36807;&#35748;&#35777;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#21518;&#38376;&#25915;&#20987;&#20013;&#65292;&#19982;&#25991;&#29486;&#20013;&#30340;&#19968;&#31995;&#21015;&#22522;&#32447;&#30456;&#27604;&#65292;PECAN&#21487;&#20197;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are vulnerable to backdoor poisoning attacks, where the attackers maliciously poison the training set and insert triggers into the test input to change the prediction of the victim model. Existing defenses for backdoor attacks either provide no formal guarantees or come with expensive-to-compute and ineffective probabilistic guarantees. We present PECAN, an efficient and certified approach for defending against backdoor attacks. The key insight powering PECAN is to apply off-the-shelf test-time evasion certification techniques on a set of neural networks trained on disjoint partitions of the data. We evaluate PECAN on image classification and malware detection datasets. Our results demonstrate that PECAN can (1) significantly outperform the state-of-the-art certified backdoor defense, both in defense strength and efficiency, and (2) on real back-door attacks, PECAN can reduce attack success rate by order of magnitude when compared to a range of baselines from the litera
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20004;&#31181;&#26032;&#22411;&#25915;&#20987;&#65292;COVERT&#21644;TROJANPUZZLE&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#25991;&#26723;&#23383;&#31526;&#20018;&#31561;&#19978;&#19979;&#25991;&#20043;&#22806;&#30340;&#21306;&#22495;&#26893;&#20837;&#24694;&#24847;&#25968;&#25454;&#65292;&#32469;&#36807;&#38745;&#24577;&#20998;&#26512;&#65292;&#23545;&#20195;&#30721;&#24314;&#35758;&#27169;&#22411;&#36827;&#34892;&#38544;&#31192;&#27745;&#26579;&#12290;</title><link>http://arxiv.org/abs/2301.02344</link><description>&lt;p&gt;
TrojanPuzzle: &#38544;&#31192;&#22320;&#27745;&#26579;&#20195;&#30721;&#24314;&#35758;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TrojanPuzzle: Covertly Poisoning Code-Suggestion Models. (arXiv:2301.02344v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20004;&#31181;&#26032;&#22411;&#25915;&#20987;&#65292;COVERT&#21644;TROJANPUZZLE&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#25991;&#26723;&#23383;&#31526;&#20018;&#31561;&#19978;&#19979;&#25991;&#20043;&#22806;&#30340;&#21306;&#22495;&#26893;&#20837;&#24694;&#24847;&#25968;&#25454;&#65292;&#32469;&#36807;&#38745;&#24577;&#20998;&#26512;&#65292;&#23545;&#20195;&#30721;&#24314;&#35758;&#27169;&#22411;&#36827;&#34892;&#38544;&#31192;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31867;&#20284;GitHub Copilot&#30340;&#24037;&#20855;&#65292;&#33258;&#21160;&#20195;&#30721;&#24314;&#35758;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#24050;&#32463;&#19981;&#20877;&#26159;&#19968;&#20010;&#26790;&#24819;&#12290;&#36825;&#20123;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#36890;&#24120;&#22312;&#26410;&#32463;&#23457;&#26680;&#30340;&#20844;&#20849;&#20195;&#30721;&#26469;&#28304;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#21363;&#23545;&#25239;&#25163;&#36890;&#36807;&#27880;&#20837;&#24694;&#24847;&#25968;&#25454;&#26469;&#25805;&#32437;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#27745;&#26579;&#25915;&#20987;&#21487;&#20197;&#34987;&#35774;&#35745;&#25104;&#22312;&#36816;&#34892;&#26102;&#24433;&#21709;&#27169;&#22411;&#23545;&#20110;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#24314;&#35758;&#65292;&#20363;&#22914;&#35825;&#23548;&#27169;&#22411;&#24314;&#35758;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#36127;&#36733;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20808;&#21069;&#30340;&#25915;&#20987;&#20250;&#23558;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#36127;&#36733;&#26126;&#30830;&#22320;&#27880;&#20837;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#36825;&#20351;&#24471;&#24694;&#24847;&#25968;&#25454;&#21487;&#20197;&#34987;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#26816;&#27979;&#24182;&#20174;&#35757;&#32451;&#38598;&#20013;&#21024;&#38500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#65292;COVERT&#21644;TROJANPUZZLE&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#23558;&#24694;&#24847;&#27745;&#26579;&#25968;&#25454;&#26893;&#20837;&#21040;&#19978;&#19979;&#25991;&#20043;&#22806;&#30340;&#21306;&#22495;&#65288;&#22914;&#25991;&#26723;&#23383;&#31526;&#20018;&#65289;&#26469;&#32469;&#36807;&#38745;&#24577;&#20998;&#26512;&#12290;&#25105;&#20204;&#26368;&#26032;&#39062;&#30340;&#25915;&#20987;&#65292;TROJANPUZZLE&#65292;&#20351;&#24471;&#24314;&#35758;&#27169;&#22411;&#26080;&#27861;&#36890;&#36807;&#38745;&#24577;&#20998;&#26512;&#26816;&#27979;&#21040;&#24694;&#24847;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model's training by injecting malicious data. Poisoning attacks could be designed to influence the model's suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior attacks explicitly inject the insecure code payload into the training data, making the poison data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel attacks, COVERT and TROJANPUZZLE, that can bypass static analysis by planting malicious poison data in out-of-context regions such as docstrings. Our most novel attack, TROJANPUZZLE, goes 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32422;&#26463;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#20984;&#20248;&#21270;&#38382;&#39064;&#25110;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#29702;&#35770;&#25910;&#25947;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FLIPS&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#38024;&#23545;&#38382;&#39064;&#32467;&#26500;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#32463;&#20856;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.01068</link><description>&lt;p&gt;
&#29992;&#20110;&#32422;&#26463;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#24555;&#36895;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast Algorithm for Constrained Linear Inverse Problems. (arXiv:2212.01068v6 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32422;&#26463;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#20984;&#20248;&#21270;&#38382;&#39064;&#25110;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#29702;&#35770;&#25910;&#25947;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FLIPS&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#38024;&#23545;&#38382;&#39064;&#32467;&#26500;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#32463;&#20856;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#32422;&#26463;&#32447;&#24615;&#36870;&#38382;&#39064; (LIP)&#65292;&#22312;&#20108;&#27425;&#32422;&#26463;&#26465;&#20214;&#19979;&#26368;&#23567;&#21270;&#26576;&#20010;&#21407;&#23376;&#33539;&#25968;&#65288;&#22914;$\ell_1$&#33539;&#25968;)&#12290;&#36890;&#24120;&#65292;&#36825;&#26679;&#30340;&#25104;&#26412;&#20989;&#25968;&#26159;&#19981;&#21487;&#24494;&#20998;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#19981;&#36866;&#21512;&#29616;&#26377;&#30340;&#24555;&#36895;&#20248;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31561;&#25928;&#30340;&#32422;&#26463;LIP&#25913;&#36827;&#30340;&#20984;&#27491;&#21017;&#21270;&#37325;&#36848;&#65306;(i)&#19968;&#20010;&#20809;&#28369;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#21644;(ii)&#19968;&#20010;&#24378;&#20984;&#30340;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;&#29616;&#26377;&#30340;&#22522;&#20110;&#21152;&#36895;&#30340;&#20984;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#65292;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;$O \left( \frac{1}{k^2} \right)$&#29702;&#35770;&#25910;&#25947;&#20445;&#35777;&#65292;&#25913;&#36827;&#20102;&#24403;&#21069;&#26368;&#20339;&#36895;&#29575; $O \left( \frac{1}{k} \right)$&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#21517;&#20026;&#24555;&#36895;&#32447;&#24615;&#36870;&#38382;&#39064;&#27714;&#35299;&#22120;&#65288;FLIPS&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#29305;&#21035;&#38024;&#23545;&#37325;&#36848;&#32467;&#26500;&#36827;&#34892;&#26368;&#22823;&#21270;&#21033;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FLIPS&#22312;&#32463;&#20856;&#30340;&#20108;&#36827;&#21046;&#36873;&#25321;&#38382;&#39064;&#12289;&#21387;&#32553;&#24863;&#30693;&#21644;&#22270;&#20687;&#24674;&#22797;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the constrained Linear Inverse Problem (LIP), where a certain atomic norm (like the $\ell_1 $ norm) is minimized subject to a quadratic constraint. Typically, such cost functions are non-differentiable which makes them not amenable to the fast optimization methods existing in practice. We propose two equivalent reformulations of the constrained LIP with improved convex regularity: (i) a smooth convex minimization problem, and (ii) a strongly convex min-max problem. These problems could be solved by applying existing acceleration-based convex optimization methods which provide better $ O \left( \frac{1}{k^2} \right) $ theoretical convergence guarantee, improving upon the current best rate of $ O \left( \frac{1}{k} \right) $. We also provide a novel algorithm named the Fast Linear Inverse Problem Solver (FLIPS), which is tailored to maximally exploit the structure of the reformulations. We demonstrate the performance of FLIPS on the classical problems of Binary Selection, Com
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Walsh&#31995;&#25968;&#36924;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#28165;&#26224;&#22270;&#20687;&#21644;&#23545;&#25239;&#22270;&#20687;&#20043;&#38388;&#30340;Walsh&#31995;&#25968;&#36924;&#36817;&#24046;&#24322;&#65292;&#23454;&#29616;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2211.10227</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;&#36793;&#30028;&#36924;&#36817;&#30340;&#23545;&#25239;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Detection by Approximation of Ensemble Boundary. (arXiv:2211.10227v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Walsh&#31995;&#25968;&#36924;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#28165;&#26224;&#22270;&#20687;&#21644;&#23545;&#25239;&#22270;&#20687;&#20043;&#38388;&#30340;Walsh&#31995;&#25968;&#36924;&#36817;&#24046;&#24322;&#65292;&#23454;&#29616;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#38024;&#23545;&#35299;&#20915;&#20004;&#31867;&#27169;&#24335;&#35782;&#21035;&#38382;&#39064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38598;&#25104;&#12290;&#35813;&#38598;&#25104;&#20351;&#29992;Walsh&#31995;&#25968;&#36827;&#34892;&#32452;&#21512;&#65292;&#33021;&#22815;&#36924;&#36817;&#24067;&#23572;&#20989;&#25968;&#24182;&#25511;&#21046;&#38598;&#25104;&#20915;&#31574;&#36793;&#30028;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#30340;&#20551;&#35774;&#26159;&#39640;&#26354;&#29575;&#30340;&#20915;&#31574;&#36793;&#30028;&#20801;&#35768;&#25214;&#21040;&#23545;&#25239;&#25200;&#21160;&#65292;&#20294;&#20250;&#25913;&#21464;&#20915;&#31574;&#36793;&#30028;&#30340;&#26354;&#29575;&#65292;&#32780;&#19982;&#28165;&#26224;&#22270;&#20687;&#30456;&#27604;&#65292;&#20351;&#29992;Walsh&#31995;&#25968;&#23545;&#20854;&#36827;&#34892;&#36924;&#36817;&#30340;&#26041;&#24335;&#20063;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#35266;&#23519;&#28165;&#26224;&#22270;&#20687;&#21644;&#23545;&#25239;&#22270;&#20687;&#20043;&#38388;&#30340;Walsh&#31995;&#25968;&#36924;&#36817;&#24046;&#24322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#36801;&#31227;&#24615;&#21487;&#29992;&#20110;&#26816;&#27979;&#12290;&#27492;&#22806;&#65292;&#36924;&#36817;&#20915;&#31574;&#36793;&#30028;&#21487;&#33021;&#26377;&#21161;&#20110;&#29702;&#35299;DNN&#30340;&#23398;&#20064;&#21644;&#21487;&#36801;&#31227;&#24615;&#29305;&#24615;&#12290;&#23613;&#31649;&#26412;&#25991;&#30340;&#23454;&#39564;&#20351;&#29992;&#22270;&#20687;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20004;&#31867;&#27169;&#24335;&#35782;&#21035;&#38382;&#39064;&#30340;&#38598;&#25104;&#36793;&#30028;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new method of detecting adversarial attacks is proposed for an ensemble of Deep Neural Networks (DNNs) solving two-class pattern recognition problems. The ensemble is combined using Walsh coefficients which are capable of approximating Boolean functions and thereby controlling the complexity of the ensemble decision boundary. The hypothesis in this paper is that decision boundaries with high curvature allow adversarial perturbations to be found, but change the curvature of the decision boundary, which is then approximated in a different way by Walsh coefficients compared to the clean images. By observing the difference in Walsh coefficient approximation between clean and adversarial images, it is shown experimentally that transferability of attack may be used for detection. Furthermore, approximating the decision boundary may aid in understanding the learning and transferability properties of DNNs. While the experiments here use images, the proposed approach of modelling two-class en
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#31867;&#21035;&#30456;&#20851;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#29616;&#26377;&#27169;&#22411;&#22312;&#20998;&#26512;&#21644;&#24037;&#31243;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.08262</link><description>&lt;p&gt;
&#19968;&#31181;&#28151;&#21512;&#31867;&#21035;&#30456;&#20851;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
A mixed-categorical correlation kernel for Gaussian process. (arXiv:2211.08262v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#31867;&#21035;&#30456;&#20851;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#29616;&#26377;&#27169;&#22411;&#22312;&#20998;&#26512;&#21644;&#24037;&#31243;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#30340;&#28151;&#21512;&#31867;&#21035;&#20803;&#27169;&#22411;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20123;&#29616;&#26377;&#30340;&#26041;&#27861;&#20351;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#26680;&#65288;&#20363;&#22914;&#65292;&#36830;&#32493;&#26494;&#24347;&#21644;Gower&#36317;&#31163;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#65289;&#25110;&#36890;&#36807;&#30452;&#25509;&#20272;&#35745;&#30456;&#20851;&#30697;&#38453;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#65292;&#23558;&#36830;&#32493;&#25351;&#25968;&#26680;&#25193;&#23637;&#20026;&#22788;&#29702;&#28151;&#21512;&#31867;&#21035;&#21464;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#26680;&#24341;&#23548;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#26031;&#20195;&#29702;&#65292;&#23427;&#27010;&#25324;&#20102;&#36830;&#32493;&#26494;&#24347;&#21644;Gower&#36317;&#31163;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20998;&#26512;&#21644;&#24037;&#31243;&#38382;&#39064;&#19978;&#35777;&#26126;&#20102;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#27604;&#20854;&#20182;&#22522;&#20110;&#26680;&#30340;&#29616;&#26377;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#21644;&#26356;&#23567;&#30340;&#27531;&#24046;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20351;&#29992;&#24320;&#28304;&#36719;&#20214;SMT&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a growing interest for mixed-categorical meta-models based on Gaussian process (GP) surrogates. In this setting, several existing approaches use different strategies either by using continuous kernels (e.g., continuous relaxation and Gower distance based GP) or by using a direct estimation of the correlation matrix. In this paper, we present a kernel-based approach that extends continuous exponential kernels to handle mixed-categorical variables. The proposed kernel leads to a new GP surrogate that generalizes both the continuous relaxation and the Gower distance based GP models. We demonstrate, on both analytical and engineering problems, that our proposed GP model gives a higher likelihood and a smaller residual error than the other kernel-based state-of-the-art models. Our method is available in the open-source software SMT.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39118;&#38505;&#24847;&#35782;&#32447;&#24615;&#36172;&#21338;&#26426;&#22312;&#26234;&#33021;&#35746;&#21333;&#36335;&#30001;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#26368;&#23567;&#21270;&#36951;&#25022;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#36817;&#20046;&#26368;&#20248;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#32447;&#24615;&#32467;&#26500;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.02389</link><description>&lt;p&gt;
&#39118;&#38505;&#24847;&#35782;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#65306;&#29702;&#35770;&#21644;&#22312;&#26234;&#33021;&#35746;&#21333;&#36335;&#30001;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Risk-Aware Linear Bandits: Theory and Applications in Smart Order Routing. (arXiv:2208.02389v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39118;&#38505;&#24847;&#35782;&#32447;&#24615;&#36172;&#21338;&#26426;&#22312;&#26234;&#33021;&#35746;&#21333;&#36335;&#30001;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#26368;&#23567;&#21270;&#36951;&#25022;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#36817;&#20046;&#26368;&#20248;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#32447;&#24615;&#32467;&#26500;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#37329;&#34701;&#20915;&#31574;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#38469;&#32771;&#34385;&#65288;&#22914;&#39118;&#38505;&#21388;&#24694;&#21644;&#22823;&#22411;&#25805;&#20316;&#31354;&#38388;&#65289;&#30340;&#39537;&#21160;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#26234;&#33021;&#35746;&#21333;&#36335;&#30001;&#65288;SOR&#65289;&#24212;&#29992;&#30340;&#39118;&#38505;&#24847;&#35782;&#36172;&#21338;&#26426;&#20248;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22522;&#20110;&#20174;&#32435;&#26031;&#36798;&#20811;ITCH&#25968;&#25454;&#38598;&#20013;&#23545;&#32447;&#24615;&#20215;&#26684;&#24433;&#21709;&#30340;&#21021;&#27493;&#35266;&#23519;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#39118;&#38505;&#24847;&#35782;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#38754;&#23545;&#19968;&#32452;&#65288;&#26368;&#21021;&#65289;&#26410;&#30693;&#21442;&#25968;&#30340;&#32447;&#24615;&#20989;&#25968;&#20316;&#20026;&#22870;&#21169;&#30340;&#34892;&#21160;&#26102;&#65292;&#36890;&#36807;&#20351;&#29992;&#22343;&#20540;&#26041;&#24046;&#24230;&#37327;&#26469;&#26368;&#23567;&#21270;&#36951;&#25022;&#65292;&#35813;&#24230;&#37327;&#21453;&#26144;&#20102;&#25105;&#20204;&#30340;&#34920;&#29616;&#19982;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22522;&#20110;&#26041;&#24046;&#26368;&#23567;&#21270;&#30340;&#20840;&#23616;&#26368;&#20248;&#65288;G-&#26368;&#20248;&#65289;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29420;&#31435;&#20110;&#23454;&#20363;&#30340;&#20840;&#26032;&#30340;&#39118;&#38505;&#24847;&#35782;&#25506;&#32034;-&#25215;&#35834;&#65288;RISE&#65289;&#31639;&#27861;&#21644;&#20381;&#36182;&#20110;&#23454;&#20363;&#30340;&#39118;&#38505;&#24847;&#35782;&#36830;&#32493;&#28120;&#27760;&#65288;RISE++&#65289;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20005;&#26684;&#20998;&#26512;&#23427;&#20204;&#36817;&#20046;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#32447;&#24615;&#32467;&#26500;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by practical considerations in machine learning for financial decision-making, such as risk aversion and large action space, we consider risk-aware bandits optimization with applications in smart order routing (SOR). Specifically, based on preliminary observations of linear price impacts made from the NASDAQ ITCH dataset, we initiate the study of risk-aware linear bandits. In this setting, we aim at minimizing regret, which measures our performance deficit compared to the optimum's, under the mean-variance metric when facing a set of actions whose rewards are linear functions of (initially) unknown parameters. Driven by the variance-minimizing globally-optimal (G-optimal) design, we propose the novel instance-independent Risk-Aware Explore-then-Commit (RISE) algorithm and the instance-dependent Risk-Aware Successive Elimination (RISE++) algorithm. Then, we rigorously analyze their near-optimal regret upper bounds to show that, by leveraging the linear structure, our algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26641;&#38598;&#21512;&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#35268;&#21017;&#21015;&#34920;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#37322;&#27169;&#22411;&#23545;&#20110;&#23569;&#25968;&#31867;&#21035;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TE2Rules&#26041;&#27861;&#29983;&#25104;&#30340;&#35268;&#21017;&#21015;&#34920;&#20934;&#30830;&#24615;&#36739;&#39640;&#65292;&#24182;&#19988;&#36816;&#34892;&#26102;&#38388;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2206.14359</link><description>&lt;p&gt;
TE2Rules: &#20351;&#29992;&#35268;&#21017;&#35299;&#37322;&#26641;&#38598;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TE2Rules: Explaining Tree Ensembles using Rules. (arXiv:2206.14359v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26641;&#38598;&#21512;&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#35268;&#21017;&#21015;&#34920;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#37322;&#27169;&#22411;&#23545;&#20110;&#23569;&#25968;&#31867;&#21035;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TE2Rules&#26041;&#27861;&#29983;&#25104;&#30340;&#35268;&#21017;&#21015;&#34920;&#20934;&#30830;&#24615;&#36739;&#39640;&#65292;&#24182;&#19988;&#36816;&#34892;&#26102;&#38388;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26641;&#38598;&#21512;&#65288;&#22914;&#26799;&#24230;&#25552;&#21319;&#26641;&#65289;&#36890;&#24120;&#30456;&#27604;&#21333;&#26869;&#20915;&#31574;&#26641;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#26641;&#38598;&#21512;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#20154;&#31867;&#38590;&#20197;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#36923;&#36753;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#20110;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#26641;&#38598;&#21512;&#27169;&#22411;&#36716;&#25442;&#20026;&#25509;&#36817;&#26641;&#38598;&#21512;&#30340;&#21487;&#35299;&#37322;&#35268;&#21017;&#21015;&#34920;&#65292;&#24182;&#19988;&#26377;&#25928;&#22320;&#35299;&#37322;&#27169;&#22411;&#23545;&#20110;&#27169;&#22411;&#39044;&#27979;&#30340;&#23569;&#25968;&#31867;&#21035;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TE2Rules&#29983;&#25104;&#30340;&#35268;&#21017;&#21015;&#34920;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;TE2Rules&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#20854;&#20182;&#31867;&#20284;&#22522;&#32447;&#26041;&#27861;&#30456;&#24403;&#65292;TE2Rules&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#21487;&#20197;&#20197;&#31245;&#24494;&#38477;&#20302;&#30340;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tree Ensemble (TE) models (like Gradient Boosted Trees) often provide higher prediction performance compared to single decision trees. However, TE models generally lack transparency and interpretability, as humans have difficulty understanding their decision logic. This paper presents a novel approach to convert a TE trained for a binary classification task, to a rule list (RL) that closely approximates the TE and is interpretable for a human. This RL can effectively explain the model even on the minority class predicted by the model. Experiments on benchmark datasets demonstrate that, (i) predictions from the RL generated by TE2Rules have higher fidelity (with respect to the original TE) compared to state-of-the-art methods, (ii) the run-time of TE2Rules is comparable to that of some other similar baselines and (iii) the run-time of TE2Rules algorithm can be traded off at the cost of a slightly lower fidelity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#30456;&#23545;&#31574;&#30053;&#36807;&#28193;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#24341;&#29702;&#34913;&#37327;&#20004;&#20010;MDP&#20043;&#38388;&#30340;&#30456;&#23545;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#23545;&#31574;&#30053;&#20248;&#21270;&#21644;&#30456;&#23545;&#36716;&#31227;&#20248;&#21270;&#20004;&#20010;&#31639;&#27861;&#26469;&#23454;&#29616;&#24555;&#36895;&#31574;&#30053;&#36716;&#31227;&#21644;&#21160;&#24577;&#24314;&#27169;&#12290;&#21516;&#26102;&#65292;&#23558;&#36825;&#20004;&#20010;&#31639;&#27861;&#38598;&#25104;&#22312;&#19968;&#36215;&#24418;&#25104;&#23436;&#25972;&#30340;&#30456;&#23545;&#31574;&#30053;&#36807;&#28193;&#20248;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.06009</link><description>&lt;p&gt;
&#24555;&#36895;&#31574;&#30053;&#36716;&#31227;&#30340;&#30456;&#23545;&#31574;&#30053;&#36807;&#28193;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Relative Policy-Transition Optimization for Fast Policy Transfer. (arXiv:2206.06009v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#30456;&#23545;&#31574;&#30053;&#36807;&#28193;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#24341;&#29702;&#34913;&#37327;&#20004;&#20010;MDP&#20043;&#38388;&#30340;&#30456;&#23545;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#23545;&#31574;&#30053;&#20248;&#21270;&#21644;&#30456;&#23545;&#36716;&#31227;&#20248;&#21270;&#20004;&#20010;&#31639;&#27861;&#26469;&#23454;&#29616;&#24555;&#36895;&#31574;&#30053;&#36716;&#31227;&#21644;&#21160;&#24577;&#24314;&#27169;&#12290;&#21516;&#26102;&#65292;&#23558;&#36825;&#20004;&#20010;&#31639;&#27861;&#38598;&#25104;&#22312;&#19968;&#36215;&#24418;&#25104;&#23436;&#25972;&#30340;&#30456;&#23545;&#31574;&#30053;&#36807;&#28193;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#20004;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20043;&#38388;&#30340;&#31574;&#30053;&#36716;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#29616;&#26377;&#29702;&#35770;&#32467;&#26524;&#30340;&#24341;&#29702;&#65292;&#29992;&#20110;&#34913;&#37327;&#20219;&#24847;&#20004;&#20010;MDP&#20043;&#38388;&#30340;&#30456;&#23545;&#24046;&#36317;&#65292;&#21363;&#19981;&#21516;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#19979;&#32047;&#31215;&#39044;&#26399;&#22238;&#25253;&#30340;&#24046;&#24322;&#12290;&#22522;&#20110;&#35813;&#24341;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#31639;&#27861;&#65292;&#20998;&#21035;&#20026;&#30456;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;RPO&#65289;&#21644;&#30456;&#23545;&#36716;&#31227;&#20248;&#21270;&#65288;RTO&#65289;&#65292;&#20998;&#21035;&#25552;&#20379;&#20102;&#24555;&#36895;&#31574;&#30053;&#36716;&#31227;&#21644;&#21160;&#24577;&#24314;&#27169;&#12290;RPO&#36890;&#36807;&#23558;&#22312;&#19968;&#20010;&#29615;&#22659;&#20013;&#35780;&#20272;&#30340;&#31574;&#30053;&#36716;&#31227;&#20197;&#26368;&#22823;&#21270;&#22312;&#21478;&#19968;&#20010;&#29615;&#22659;&#20013;&#30340;&#22238;&#25253;&#65292;&#32780;RTO&#21017;&#36890;&#36807;&#26356;&#26032;&#21442;&#25968;&#21270;&#30340;&#21160;&#24577;&#27169;&#22411;&#26469;&#20943;&#23567;&#20004;&#20010;&#29615;&#22659;&#21160;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23558;&#36825;&#20004;&#20010;&#31639;&#27861;&#38598;&#25104;&#22312;&#19968;&#36215;&#21487;&#20197;&#24471;&#21040;&#23436;&#25972;&#30340;&#30456;&#23545;&#31574;&#30053;&#36807;&#28193;&#20248;&#21270;&#65288;RPTO&#65289;&#31639;&#27861;&#65292;&#20854;&#20013;&#31574;&#30053;&#21516;&#26102;&#19982;&#20004;&#20010;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#20174;&#32780;&#20174;&#20004;&#20010;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of policy transfer between two Markov Decision Processes (MDPs). We introduce a lemma based on existing theoretical results in reinforcement learning to measure the relativity gap between two arbitrary MDPs, that is the difference between any two cumulative expected returns defined on different policies and environment dynamics. Based on this lemma, we propose two new algorithms referred to as Relative Policy Optimization (RPO) and Relative Transition Optimization (RTO), which offer fast policy transfer and dynamics modelling, respectively. RPO transfers the policy evaluated in one environment to maximize the return in another, while RTO updates the parameterized dynamics model to reduce the gap between the dynamics of the two environments. Integrating the two algorithms results in the complete Relative Policy-Transition Optimization (RPTO) algorithm, in which the policy interacts with the two environments simultaneously, such that data collections from two envi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#36716;&#21464;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#24615;&#36136;&#20063;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20986;&#33258;&#21160;&#26816;&#27979;&#36328;&#27169;&#24577;&#19981;&#21327;&#35843;&#30340;&#25216;&#26415;&#65292;&#20294;&#20173;&#38754;&#20020;&#25361;&#25112;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26426;&#20250;&#20063;&#22312;&#31561;&#24453;&#30528;&#25366;&#25496;&#12290;</title><link>http://arxiv.org/abs/2203.13883</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65306;&#26041;&#27861;&#12289;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13883
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#36716;&#21464;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#24615;&#36136;&#20063;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20986;&#33258;&#21160;&#26816;&#27979;&#36328;&#27169;&#24577;&#19981;&#21327;&#35843;&#30340;&#25216;&#26415;&#65292;&#20294;&#20173;&#38754;&#20020;&#25361;&#25112;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26426;&#20250;&#20063;&#22312;&#31561;&#24453;&#30528;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20174;&#25991;&#26412;&#20026;&#20027;&#30340;&#35770;&#22363;&#36716;&#21521;&#22810;&#27169;&#24335;&#29615;&#22659;&#65292;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#34394;&#20551;&#20449;&#24687;&#30340;&#24615;&#36136;&#20063;&#30456;&#24212;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#21033;&#29992;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#35270;&#35273;&#27169;&#24577;&#26356;&#21463;&#29992;&#25143;&#38738;&#30544;&#21644;&#21560;&#24341;&#21147;&#30340;&#20107;&#23454;&#65292;&#20197;&#21450;&#25991;&#26412;&#20869;&#23481;&#26377;&#26102;&#34987;&#31895;&#30053;&#27983;&#35272;&#30340;&#24773;&#20917;&#65292;&#34394;&#20551;&#20449;&#24687;&#20256;&#25773;&#32773;&#26368;&#36817;&#24320;&#22987;&#38024;&#23545;&#27169;&#24577;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#36830;&#25509;&#65292;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20986;&#33258;&#21160;&#26816;&#27979;&#32593;&#39029;&#20869;&#23481;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36328;&#27169;&#24577;&#19981;&#21327;&#35843;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#20998;&#26512;&#12289;&#20998;&#31867;&#21644;&#35782;&#21035;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#23427;&#20204;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#20197;&#25581;&#31034;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#39046;&#22495;&#30340;&#26032;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
As social media platforms are evolving from text-based forums into multi-modal environments, the nature of misinformation in social media is also transforming accordingly. Taking advantage of the fact that visual modalities such as images and videos are more favorable and attractive to the users and textual contents are sometimes skimmed carelessly, misinformation spreaders have recently targeted contextual connections between the modalities e.g., text and image. Hence many researchers have developed automatic techniques for detecting possible cross-modal discordance in web-based content. We analyze, categorize and identify existing approaches in addition to challenges and shortcomings they face in order to unearth new research opportunities in the field of multi-modal misinformation detection.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#22312;&#38797;&#28857;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20998;&#25955;&#30340;&#35774;&#32622;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#26032;&#31639;&#27861;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2106.07289</link><description>&lt;p&gt;
&#20998;&#25955;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#26497;&#20540;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Decentralized Personalized Federated Learning for Min-Max Problems. (arXiv:2106.07289v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07289
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#22312;&#38797;&#28857;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20998;&#25955;&#30340;&#35774;&#32622;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#26032;&#31639;&#27861;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23454;&#29616;&#20102;&#20445;&#25252;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#21019;&#26032;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29702;&#35770;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26497;&#23567;&#21270;&#38382;&#39064;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#19978;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;PFL&#24212;&#29992;&#20110;&#38797;&#28857;&#38382;&#39064;&#65292;&#36825;&#31181;&#38382;&#39064;&#28085;&#30422;&#20102;&#26356;&#24191;&#27867;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#38656;&#35201;&#35299;&#20915;&#30340;&#19981;&#20165;&#20165;&#26159;&#26497;&#23567;&#21270;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;PFL&#35774;&#32622;&#65292;&#20854;&#20013;&#21253;&#25324;&#28151;&#21512;&#30446;&#26631;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21363;&#20840;&#23616;&#27169;&#22411;&#23398;&#20064;&#19982;&#26412;&#22320;&#20998;&#24067;&#24335;&#23398;&#20064;&#22120;&#30456;&#32467;&#21512;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#21482;&#32771;&#34385;&#38598;&#20013;&#24335;&#35774;&#32622;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#26356;&#19968;&#33324;&#21644;&#20998;&#25955;&#30340;&#35774;&#32622;&#20013;&#24037;&#20316;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35774;&#35745;&#21644;&#20998;&#26512;&#26356;&#23454;&#29992;&#21644;&#32852;&#37030;&#21270;&#30340;&#35774;&#22791;&#36830;&#25509;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized Federated Learning (PFL) has witnessed remarkable advancements, enabling the development of innovative machine learning applications that preserve the privacy of training data. However, existing theoretical research in this field has primarily focused on distributed optimization for minimization problems. This paper is the first to study PFL for saddle point problems encompassing a broader range of optimization problems, that require more than just solving minimization problems. In this work, we consider a recently proposed PFL setting with the mixing objective function, an approach combining the learning of a global model together with locally distributed learners. Unlike most previous work, which considered only the centralized setting, we work in a more general and decentralized setup that allows us to design and analyze more practical and federated ways to connect devices to the network. We proposed new algorithms to address this problem and provide a theoretical analy
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#21160;&#24577;&#21830;&#21697;&#36873;&#25321;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#65292;&#21487;&#22312;&#26410;&#30693;&#38656;&#27714;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#24635;&#20307;&#39044;&#26399;&#25910;&#20837;&#12290;&#22312;&#22823;&#24211;&#23384;&#29615;&#22659;&#19979;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2106.01135</link><description>&lt;p&gt;
MNL-&#24102;&#26377;&#32972;&#21253;&#30340;&#21095;&#38598;&#25361;&#36873;&#38382;&#39064;&#65306;&#19968;&#31181;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
MNL-Bandit with Knapsacks: a near-optimal algorithm. (arXiv:2106.01135v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01135
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#21160;&#24577;&#21830;&#21697;&#36873;&#25321;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#65292;&#21487;&#22312;&#26410;&#30693;&#38656;&#27714;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#24635;&#20307;&#39044;&#26399;&#25910;&#20837;&#12290;&#22312;&#22823;&#24211;&#23384;&#29615;&#22659;&#19979;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#21160;&#24577;&#30340;&#21830;&#21697;&#36873;&#25321;&#38382;&#39064;&#65292;&#20854;&#20013;&#21334;&#26041;&#25317;&#26377;&#22266;&#23450;&#24211;&#23384;&#30340;N&#31181;&#21487;&#26367;&#20195;&#20135;&#21697;&#65292;&#24182;&#38754;&#20020;&#22312;T&#20010;&#26102;&#26399;&#20869;&#39034;&#24207;&#21040;&#36798;&#30340;&#26410;&#30693;&#38656;&#27714;&#12290;&#22312;&#27599;&#20010;&#26102;&#26399;&#65292;&#21334;&#26041;&#38656;&#35201;&#20915;&#23450;&#35201;&#21521;&#23458;&#25143;&#25552;&#20379;&#30340;&#20135;&#21697;&#32452;&#21512;&#65288;&#22522;&#25968;&#26368;&#22810;&#20026;K&#65289;&#12290;&#39038;&#23458;&#30340;&#21453;&#24212;&#36981;&#24490;&#20855;&#26377;&#21442;&#25968;v&#30340;&#26410;&#30693;&#22810;&#39033;&#24335;&#23545;&#25968;&#27169;&#22411;&#65288;MNL&#65289;&#12290;&#21334;&#26041;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#22266;&#23450;&#21021;&#22987;&#24211;&#23384;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#24635;&#20307;&#39044;&#26399;&#25910;&#20837;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#36798;&#21040;&#20102;$\tilde O\Big(K \sqrt{KN T}\Big(\sqrt{v_{\text{max}}} + \frac{1}{q_{\text{min}}}\text{OPT}\Big)\Big)$&#30340;&#36951;&#25022;&#20540;&#65292;&#22312;&#27169;&#22411;&#21442;&#25968;&#30340;&#19968;&#31181;&#28201;&#21644;&#20551;&#35774;&#19979;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#22312;&#39640;&#24211;&#23384;&#29615;&#22659;&#19979;&#36798;&#21040;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;$\tilde O(\sqrt{T})$&#36951;&#25022;&#20540;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#22522;&#20110;&#22522;&#20110;UCB&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a dynamic assortment selection problem where a seller has a fixed inventory of $N$ substitutable products and faces an unknown demand that arrives sequentially over $T$ periods. In each period, the seller needs to decide on the assortment of products (of cardinality at most $K$) to offer to the customers. The customer's response follows an unknown multinomial logit model (MNL) with parameters $v$. The goal of the seller is to maximize the total expected revenue given the fixed initial inventory of $N$ products. We give a policy that achieves a regret of $\tilde O\Big(K \sqrt{KN T}\Big(\sqrt{v_{\text{max}}} + \frac{1}{q_{\text{min}}}\text{OPT}\Big)\Big)$, where $v_{\text{max}}\leq 1$ is the maximum utility for any product and $q_{\text{min}}$ the minimum inventory level, under a mild assumption on the model parameters. In particular, our policy achieves a near-optimal $\tilde O(\sqrt{T})$ regret in a large-inventory setting.  Our policy builds upon the UCB-based approach for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;MMD&#27491;&#21017;&#21270;&#30340;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#20984;&#35268;&#21010;&#29992;&#20110;&#20272;&#31639;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#21644;&#35823;&#24046;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2011.05001</link><description>&lt;p&gt;
MMD&#27491;&#21017;&#21270;&#30340;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MMD-regularized Unbalanced Optimal Transport. (arXiv:2011.05001v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.05001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;MMD&#27491;&#21017;&#21270;&#30340;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#20984;&#35268;&#21010;&#29992;&#20110;&#20272;&#31639;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#21644;&#35823;&#24046;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#65288;UOT&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#20351;&#29992;&#26368;&#22823;&#24179;&#22343;&#20559;&#24046;&#65288;MMD&#65289;&#27491;&#21017;&#21270;&#26469;&#23454;&#29616;&#36793;&#38469;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21160;&#26426;&#22312;&#20110;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#30340;UOT&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;$\phi$-&#25955;&#24230;&#65288;&#20363;&#22914;KL&#65289;&#30340;&#27491;&#21017;&#21270;&#12290;MMD&#20316;&#20026;&#20114;&#34917;&#30340;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#65288;IPM&#65289;&#23478;&#26063;&#20043;&#19968;&#65292;&#22312;UOT&#19978;&#30340;&#20316;&#29992;&#20284;&#20046;&#19981;&#22826;&#34987;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#65292;&#21033;&#29992;&#23427;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;MMD&#27491;&#21017;&#21270;&#30340;UOT&#65288;MMD-UOT&#65289;&#30340;&#29305;&#24615;&#12290;&#36825;&#31181;&#23545;&#20598;&#32467;&#26524;&#30340;&#19968;&#20010;&#26377;&#36259;&#32467;&#26524;&#26159;MMD-UOT&#35825;&#23548;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20063;&#23646;&#20110;IPM&#23478;&#26063;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#20984;&#35268;&#21010;&#65292;&#29992;&#20110;&#20272;&#31639;MMD-UOT&#21644;&#30456;&#24212;&#30340;&#37325;&#24515;&#12290;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#20984;&#35268;&#21010;&#30340;&#20272;&#35745;&#37327;&#26159;&#19968;&#33268;&#30340;&#65292;&#32780;&#19988;&#20272;&#35745;&#35823;&#24046;&#20197;$\mathcal{O}(m^{-1/2})$&#30340;&#36895;&#29575;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the unbalanced optimal transport (UOT) problem, where the marginal constraints are enforced using Maximum Mean Discrepancy (MMD) regularization. Our study is motivated by the observation that existing works on UOT have mainly focused on regularization based on $\phi$-divergence (e.g., KL). The role of MMD, which belongs to the complementary family of integral probability metrics (IPMs), as a regularizer in the context of UOT seems to be less understood. Our main result is based on Fenchel duality, using which we are able to study the properties of MMD-regularized UOT (MMD-UOT). One interesting outcome of this duality result is that MMD-UOT induces a novel metric over measures, which again belongs to the IPM family. Further, we present finite-sample-based convex programs for estimating MMD-UOT and the corresponding barycenter. Under mild conditions, we prove that our convex-program-based estimators are consistent, and the estimation error decays at a rate $\mathcal{O}\left(m^{-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#32447;&#24615;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(LRNNs)&#21487;&#20197;&#36924;&#36817;&#20219;&#20309;&#26102;&#21464;&#20989;&#25968;f(t)&#12290;&#36890;&#36807;&#26816;&#26597;&#32593;&#32476;&#36716;&#31227;&#30697;&#38453;&#30340;&#20027;&#35201;&#29305;&#24449;&#20540;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LRNN&#30340;&#35268;&#27169;&#12290;LRNNs&#20855;&#26377;&#20197;&#26925;&#22278;&#36712;&#36857;&#32467;&#26463;&#30340;&#26377;&#36259;&#29305;&#24615;&#65292;&#24182;&#20801;&#35768;&#39044;&#27979;&#36827;&#19968;&#27493;&#30340;&#20540;&#21644;&#20989;&#25968;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/1802.03308</link><description>&lt;p&gt;
&#32447;&#24615;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
The Power of Linear Recurrent Neural Networks. (arXiv:1802.03308v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1802.03308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#32447;&#24615;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(LRNNs)&#21487;&#20197;&#36924;&#36817;&#20219;&#20309;&#26102;&#21464;&#20989;&#25968;f(t)&#12290;&#36890;&#36807;&#26816;&#26597;&#32593;&#32476;&#36716;&#31227;&#30697;&#38453;&#30340;&#20027;&#35201;&#29305;&#24449;&#20540;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LRNN&#30340;&#35268;&#27169;&#12290;LRNNs&#20855;&#26377;&#20197;&#26925;&#22278;&#36712;&#36857;&#32467;&#26463;&#30340;&#26377;&#36259;&#29305;&#24615;&#65292;&#24182;&#20801;&#35768;&#39044;&#27979;&#36827;&#19968;&#27493;&#30340;&#20540;&#21644;&#20989;&#25968;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26159;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;autoregressive linear,&#21363;&#32447;&#24615;&#28608;&#27963;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(LRNNs)&#21487;&#20197;&#36924;&#36817;&#30001;&#22810;&#20010;&#20989;&#25968;&#20540;&#32473;&#20986;&#30340;&#20219;&#20309;&#26102;&#21464;&#20989;&#25968;f(t)&#12290;&#36924;&#36817;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#35299;&#20915;&#19968;&#20010;&#32447;&#24615;&#26041;&#31243;&#32452;&#26469;&#26377;&#25928;&#23398;&#20064;&#65307;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#25110;&#31867;&#20284;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#21487;&#33021;&#26159;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#65292;&#36890;&#36807;&#26816;&#26597;&#32593;&#32476;&#36716;&#31227;&#30697;&#38453;&#30340;&#39057;&#35889;&#65292;&#21363;&#23427;&#30340;&#29305;&#24449;&#20540;&#65292;&#21482;&#21462;&#26368;&#30456;&#20851;&#30340;&#32452;&#20214;&#65292;&#21487;&#20197;&#22312;&#19968;&#27493;&#20013;&#26174;&#33879;&#38477;&#20302;LRNN&#30340;&#35268;&#27169;&#12290;&#22240;&#27492;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#23398;&#20064;&#32593;&#32476;&#26435;&#37325;&#65292;&#36824;&#21487;&#20197;&#23398;&#20064;&#32593;&#32476;&#26550;&#26500;&#12290;LRNNs&#20855;&#26377;&#26377;&#36259;&#30340;&#29305;&#24615;&#65306;&#23427;&#20204;&#26368;&#32456;&#20250;&#20197;&#26925;&#22278;&#36712;&#36857;&#32467;&#26463;&#65292;&#24182;&#20801;&#35768;&#39044;&#27979;&#36827;&#19968;&#27493;&#30340;&#20540;&#21644;&#20989;&#25968;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#28436;&#31034;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent neural networks are a powerful means to cope with time series. We show how autoregressive linear, i.e., linearly activated recurrent neural networks (LRNNs) can approximate any time-dependent function f(t) given by a number of function values. The approximation can effectively be learned by simply solving a linear equation system; no backpropagation or similar methods are needed. Furthermore, and this is probably the main contribution of this article, the size of an LRNN can be reduced significantly in one step after inspecting the spectrum of the network transition matrix, i.e., its eigenvalues, by taking only the most relevant components. Therefore, in contrast to other approaches, we do not only learn network weights but also the network architecture. LRNNs have interesting properties: They end up in ellipse trajectories in the long run and allow the prediction of further values and compact representations of functions. We demonstrate this by several experiments, among the
&lt;/p&gt;</description></item></channel></rss>