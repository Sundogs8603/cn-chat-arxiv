<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>DiffusionMTL &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#22810;&#20219;&#21153;&#21435;&#22122;&#25193;&#25955;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#25193;&#25955;&#21644;&#21435;&#22122;&#26469;&#25913;&#21892;&#37096;&#20998;&#26631;&#27880;&#25968;&#25454;&#20013;&#30340;&#22810;&#20219;&#21153;&#23494;&#38598;&#22330;&#26223;&#29702;&#35299;&#65292;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#22810;&#20219;&#21153;&#26465;&#20214;&#31574;&#30053;&#26469;&#21033;&#29992;&#20219;&#21153;&#30340;&#20114;&#34917;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15389</link><description>&lt;p&gt;
DiffusionMTL: &#20174;&#37096;&#20998;&#26631;&#27880;&#25968;&#25454;&#20013;&#23398;&#20064;&#22810;&#20219;&#21153;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from Partially Annotated Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15389
&lt;/p&gt;
&lt;p&gt;
DiffusionMTL &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#22810;&#20219;&#21153;&#21435;&#22122;&#25193;&#25955;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#25193;&#25955;&#21644;&#21435;&#22122;&#26469;&#25913;&#21892;&#37096;&#20998;&#26631;&#27880;&#25968;&#25454;&#20013;&#30340;&#22810;&#20219;&#21153;&#23494;&#38598;&#22330;&#26223;&#29702;&#35299;&#65292;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#22810;&#20219;&#21153;&#26465;&#20214;&#31574;&#30053;&#26469;&#21033;&#29992;&#20219;&#21153;&#30340;&#20114;&#34917;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20174;&#37096;&#20998;&#26631;&#27880;&#25968;&#25454;&#20013;&#23398;&#20064;&#22810;&#20010;&#23494;&#38598;&#22330;&#26223;&#29702;&#35299;&#20219;&#21153;&#30340;&#23454;&#38469;&#38382;&#39064;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#20854;&#20013;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#20165;&#38024;&#23545;&#19968;&#37096;&#20998;&#20219;&#21153;&#36827;&#34892;&#26631;&#35760;&#12290;&#22312;&#35757;&#32451;&#20013;&#32570;&#23569;&#20219;&#21153;&#26631;&#31614;&#20250;&#23548;&#33268;&#20302;&#36136;&#37327;&#21644;&#22024;&#26434;&#30340;&#39044;&#27979;&#65292;&#36825;&#21487;&#20197;&#20174;&#26368;&#20808;&#36827;&#26041;&#27861;&#20013;&#35266;&#23519;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#37096;&#20998;&#26631;&#35760;&#30340;&#22810;&#20219;&#21153;&#23494;&#38598;&#39044;&#27979;&#37325;&#26032;&#26500;&#24314;&#20026;&#20687;&#32032;&#32423;&#21435;&#22122;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;DiffusionMTL&#30340;&#26032;&#22411;&#22810;&#20219;&#21153;&#21435;&#22122;&#25193;&#25955;&#26694;&#26550;&#12290;&#23427;&#35774;&#35745;&#20102;&#19968;&#20010;&#32852;&#21512;&#25193;&#25955;&#21644;&#21435;&#22122;&#33539;&#24335;&#65292;&#20197;&#27169;&#25311;&#20219;&#21153;&#39044;&#27979;&#25110;&#29305;&#24449;&#22270;&#20013;&#30340;&#28508;&#22312;&#22122;&#22768;&#20998;&#24067;&#65292;&#24182;&#20026;&#19981;&#21516;&#20219;&#21153;&#29983;&#25104;&#26657;&#27491;&#36755;&#20986;&#12290;&#20026;&#20102;&#21033;&#29992;&#21435;&#22122;&#20013;&#30340;&#22810;&#20219;&#21153;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26465;&#20214;&#31574;&#30053;&#65292;&#23427;&#21487;&#20197;&#38544;&#24335;&#21033;&#29992;&#20219;&#21153;&#30340;&#20114;&#34917;&#24615;&#26469;&#24110;&#21161;&#23398;&#20064;&#26410;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15389v1 Announce Type: cross  Abstract: Recently, there has been an increased interest in the practical problem of learning multiple dense scene understanding tasks from partially annotated data, where each training sample is only labeled for a subset of the tasks. The missing of task labels in training leads to low-quality and noisy predictions, as can be observed from state-of-the-art methods. To tackle this issue, we reformulate the partially-labeled multi-task dense prediction as a pixel-level denoising problem, and propose a novel multi-task denoising diffusion framework coined as DiffusionMTL. It designs a joint diffusion and denoising paradigm to model a potential noisy distribution in the task prediction or feature maps and generate rectified outputs for different tasks. To exploit multi-task consistency in denoising, we further introduce a Multi-Task Conditioning strategy, which can implicitly utilize the complementary nature of the tasks to help learn the unlabeled
&lt;/p&gt;</description></item><item><title>LATTE3D&#36890;&#36807;&#26500;&#24314;&#21487;&#25193;&#23637;&#30340;&#26550;&#26500;&#12289;&#21033;&#29992;3D&#25968;&#25454;&#24182;&#37319;&#29992;&#25674;&#36824;&#26041;&#27861;&#65292;&#22312;&#26174;&#33879;&#26356;&#22823;&#30340;&#25552;&#31034;&#38598;&#19978;&#23454;&#29616;&#24555;&#36895;&#12289;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#22686;&#24378;3D&#21512;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.15385</link><description>&lt;p&gt;
LATTE3D: &#22823;&#35268;&#27169;&#25674;&#36824;&#24335;&#25991;&#26412;&#22686;&#24378;3D&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15385
&lt;/p&gt;
&lt;p&gt;
LATTE3D&#36890;&#36807;&#26500;&#24314;&#21487;&#25193;&#23637;&#30340;&#26550;&#26500;&#12289;&#21033;&#29992;3D&#25968;&#25454;&#24182;&#37319;&#29992;&#25674;&#36824;&#26041;&#27861;&#65292;&#22312;&#26174;&#33879;&#26356;&#22823;&#30340;&#25552;&#31034;&#38598;&#19978;&#23454;&#29616;&#24555;&#36895;&#12289;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#22686;&#24378;3D&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26041;&#27861;&#20135;&#29983;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;3D&#32467;&#26524;&#65292;&#20294;&#38656;&#35201;&#32791;&#26102;&#30340;&#20248;&#21270;&#65292;&#27599;&#20010;&#25552;&#31034;&#21487;&#33021;&#38656;&#35201;&#38271;&#36798;&#19968;&#23567;&#26102;&#12290;&#20687;ATT3D&#36825;&#26679;&#30340;&#25674;&#36824;&#26041;&#27861;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#23454;&#29616;&#24555;&#36895;&#30340;&#25991;&#26412;&#21040;3D&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26080;&#27861;&#25429;&#25417;&#39640;&#39057;&#20960;&#20309;&#21644;&#32441;&#29702;&#32454;&#33410;&#65292;&#24182;&#19988;&#24456;&#38590;&#25193;&#23637;&#21040;&#22823;&#22411;&#25552;&#31034;&#38598;&#65292;&#22240;&#27492;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LATTE3D&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#23454;&#29616;&#22312;&#26174;&#33879;&#26356;&#22823;&#30340;&#25552;&#31034;&#38598;&#19978;&#36827;&#34892;&#24555;&#36895;&#12289;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#20043;&#22788;&#22312;&#20110; 1)&#26500;&#24314;&#21487;&#25193;&#23637;&#30340;&#26550;&#26500;&#21644; 2)&#21033;&#29992;3D&#25968;&#25454;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#36890;&#36807;3D&#24863;&#30693;&#25193;&#25955;&#20808;&#39564;&#12289;&#24418;&#29366;&#27491;&#21017;&#21270;&#21644;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#20197;&#23454;&#29616;&#23545;&#19981;&#21516;&#21644;&#22797;&#26434;&#35757;&#32451;&#25552;&#31034;&#30340;&#31283;&#20581;&#24615;&#12290;LATTE3D&#25674;&#36824;&#20102;&#31070;&#32463;&#22330;&#21644;&#32441;&#29702;&#34920;&#38754;&#30340;&#29983;&#25104;&#65292;&#33021;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#20135;&#29983;&#39640;&#24230;&#35814;&#32454;&#30340;&#32441;&#29702;&#32593;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15385v1 Announce Type: cross  Abstract: Recent text-to-3D generation approaches produce impressive 3D results but require time-consuming optimization that can take up to an hour per prompt. Amortized methods like ATT3D optimize multiple prompts simultaneously to improve efficiency, enabling fast text-to-3D synthesis. However, they cannot capture high-frequency geometry and texture details and struggle to scale to large prompt sets, so they generalize poorly. We introduce LATTE3D, addressing these limitations to achieve fast, high-quality generation on a significantly larger prompt set. Key to our method is 1) building a scalable architecture and 2) leveraging 3D data during optimization through 3D-aware diffusion priors, shape regularization, and model initialization to achieve robustness to diverse and complex training prompts. LATTE3D amortizes both neural field and textured surface generation to produce highly detailed textured meshes in a single forward pass. LATTE3D gen
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#26377;&#25928;&#36827;&#34892;&#25506;&#32034;&#65292;&#38500;&#20102;&#29305;&#23450;&#37197;&#32622;&#19979;&#30340;GPT-4&#20855;&#26377;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.15371</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#32034;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can large language models explore in-context?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15371
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#26377;&#25928;&#36827;&#34892;&#25506;&#32034;&#65292;&#38500;&#20102;&#29305;&#23450;&#37197;&#32622;&#19979;&#30340;GPT-4&#20855;&#26377;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36827;&#34892;&#25506;&#32034;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#24378;&#21270;&#23398;&#20064;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#25105;&#20204;&#20851;&#27880;&#29616;&#26377;LLMs&#30340;&#21407;&#29983;&#24615;&#33021;&#65292;&#27809;&#26377;&#36827;&#34892;&#35757;&#32451;&#24178;&#39044;&#12290;&#25105;&#20204;&#23558;LLMs&#37096;&#32626;&#20026;&#31616;&#21333;&#22810;&#33218;&#32769;&#34382;&#26426;&#29615;&#22659;&#20013;&#30340;&#20195;&#29702;&#65292;&#24182;&#23436;&#20840;&#22312;&#19978;&#19979;&#25991;&#20013;&#25351;&#23450;&#29615;&#22659;&#25551;&#36848;&#21644;&#20132;&#20114;&#21382;&#21490;&#65292;&#21363;&#22312;LLM&#25552;&#31034;&#20869;&#37096;&#36827;&#34892;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#23545;GPT-3.5&#12289;GPT-4&#21644;Llama2&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24182;&#27809;&#26377;&#31283;&#20581;&#22320;&#36827;&#34892;&#25506;&#32034;&#65306;i&#65289;&#22312;&#25105;&#20204;&#30340;&#25152;&#26377;&#23454;&#39564;&#20013;&#65292;&#21482;&#26377;&#19968;&#20010;&#37197;&#32622;&#23548;&#33268;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#65306;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#22806;&#37096;&#24635;&#32467;&#30340;&#20132;&#20114;&#21382;&#21490;&#30340;GPT-4&#65292;&#36825;&#20123;&#34987;&#21576;&#29616;&#20026;&#20805;&#20998;&#32479;&#35745;&#30340;&#24773;&#20917;&#65307;ii&#65289;&#25152;&#26377;&#20854;&#20182;&#37197;&#32622;&#37117;&#27809;&#26377;&#20135;&#29983;&#31283;&#20581;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#21253;&#25324;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#20854;&#20182;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15371v1 Announce Type: cross  Abstract: We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration, a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5, GPT-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally summarized interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ARSim&#65292;&#19968;&#20010;&#20840;&#33258;&#21160;&#12289;&#32508;&#21512;&#12289;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;3D&#21512;&#25104;&#23545;&#35937;&#25972;&#21512;&#21040;&#30495;&#23454;&#30340;&#22810;&#35270;&#22270;&#22270;&#20687;&#25968;&#25454;&#20013;&#65292;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#38543;&#26426;&#21270;&#31574;&#30053;&#26469;&#35299;&#20915;&#30495;&#23454;&#19982;&#21512;&#25104;&#25968;&#25454;&#20043;&#38388;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.15370</link><description>&lt;p&gt;
&#22522;&#20110;&#22686;&#24378;&#29616;&#23454;&#30340;&#20855;&#26377;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#30340;AV&#24863;&#30693;&#32593;&#32476;&#30340;&#27169;&#25311;&#25968;&#25454;(ARSim)
&lt;/p&gt;
&lt;p&gt;
Augmented Reality based Simulated Data (ARSim) with multi-view consistency for AV perception networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15370
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ARSim&#65292;&#19968;&#20010;&#20840;&#33258;&#21160;&#12289;&#32508;&#21512;&#12289;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;3D&#21512;&#25104;&#23545;&#35937;&#25972;&#21512;&#21040;&#30495;&#23454;&#30340;&#22810;&#35270;&#22270;&#22270;&#20687;&#25968;&#25454;&#20013;&#65292;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#38543;&#26426;&#21270;&#31574;&#30053;&#26469;&#35299;&#20915;&#30495;&#23454;&#19982;&#21512;&#25104;&#25968;&#25454;&#20043;&#38388;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#21508;&#31181;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#21508;&#31181;&#23545;&#35937;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#32463;&#24120;&#32570;&#20047;&#24517;&#35201;&#30340;&#22810;&#26679;&#24615;&#65292;&#21576;&#29616;&#38271;&#23614;&#20998;&#24067;&#12290;&#34429;&#28982;&#21512;&#25104;&#25968;&#25454;&#24050;&#34987;&#29992;&#26469;&#20811;&#26381;&#36890;&#36807;&#29983;&#25104;&#34394;&#25311;&#22330;&#26223;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#38754;&#20020;&#30528;&#35832;&#22914;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#36317;&#20197;&#21450;&#38656;&#35201;3D&#33402;&#26415;&#23478;&#22823;&#37327;&#24037;&#20316;&#26469;&#21019;&#24314;&#36924;&#30495;&#29615;&#22659;&#31561;&#38556;&#30861;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARSim&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#33258;&#21160;&#30340;&#12289;&#32508;&#21512;&#30340;&#12289;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;3D synthet &#30446;&#26631;&#29289;&#20307;&#22686;&#24378;&#30495;&#23454;&#30340;&#22810;&#35270;&#22270;&#22270;&#20687;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;&#39046;&#22495;&#36866;&#24212;&#21644;&#38543;&#26426;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#20174;&#30495;&#23454;&#25968;&#25454;&#20013;&#25512;&#26029;&#37325;&#35201;&#30340;&#39046;&#22495;&#23646;&#24615;&#24182;&#21033;&#29992;&#22522;&#20110;&#27169;&#25311;&#30340;&#38543;&#26426;&#21270;&#22788;&#29702;&#20854;&#20182;&#23646;&#24615;&#65292;&#20197;&#35299;&#20915;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#20043;&#38388;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15370v1 Announce Type: cross  Abstract: Detecting a diverse range of objects under various driving scenarios is essential for the effectiveness of autonomous driving systems. However, the real-world data collected often lacks the necessary diversity presenting a long-tail distribution. Although synthetic data has been utilized to overcome this issue by generating virtual scenes, it faces hurdles such as a significant domain gap and the substantial efforts required from 3D artists to create realistic environments. To overcome these challenges, we present ARSim, a fully automated, comprehensive, modular framework designed to enhance real multi-view image data with 3D synthetic objects of interest. The proposed method integrates domain adaptation and randomization strategies to address covariate shift between real and simulated data by inferring essential domain attributes from real data and employing simulation-based randomization for other attributes. We construct a simplifie
&lt;/p&gt;</description></item><item><title>&#27700;&#21360;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#30340;&#24773;&#20917;&#19979;&#65292;&#27700;&#21360;&#22522;&#30784;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#20063;&#26080;&#27861;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.15365</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#27700;&#21360;&#30340;&#36716;&#31227;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Transfer Attack to Image Watermarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15365
&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#30340;&#24773;&#20917;&#19979;&#65292;&#27700;&#21360;&#22522;&#30784;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#20063;&#26080;&#27861;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25991;&#29486;&#20013;&#23545;&#36825;&#31181;&#22522;&#20110;&#27700;&#21360;&#30340;&#26816;&#27979;&#22120;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#29615;&#22659;&#19979;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#26377;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30418;&#29615;&#22659;&#19979;&#30340;&#31283;&#20581;&#24615;&#21364;&#30693;&#20043;&#29978;&#23569;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22810;&#39033;&#30740;&#31350;&#22768;&#31216;&#22270;&#20687;&#27700;&#21360;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#26159;&#31283;&#20581;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#31227;&#23545;&#25239;&#25915;&#20987;&#26469;&#38024;&#23545;&#26080;&#30418;&#29615;&#22659;&#19979;&#30340;&#22270;&#20687;&#27700;&#21360;&#12290;&#25105;&#20204;&#30340;&#36716;&#31227;&#25915;&#20987;&#21521;&#24102;&#27700;&#21360;&#30340;&#22270;&#20687;&#28155;&#21152;&#24494;&#25200;&#65292;&#20197;&#36530;&#36991;&#34987;&#25915;&#20987;&#32773;&#35757;&#32451;&#30340;&#22810;&#20010;&#26367;&#20195;&#27700;&#21360;&#27169;&#22411;&#65292;&#24182;&#19988;&#32463;&#36807;&#25200;&#21160;&#30340;&#24102;&#27700;&#21360;&#22270;&#20687;&#20063;&#33021;&#36530;&#36991;&#30446;&#26631;&#27700;&#21360;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#65292;&#22522;&#20110;&#27700;&#21360;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#21363;&#20351;&#25915;&#20987;&#32773;&#27809;&#26377;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#65292;&#20063;&#19981;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15365v1 Announce Type: cross  Abstract: Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and black-box settings is well understood in the literature. However, the robustness in the no-box setting is much less understood. In particular, multiple studies claimed that image watermark is robust in such setting. In this work, we propose a new transfer evasion attack to image watermark in the no-box setting. Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model. Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API.
&lt;/p&gt;</description></item><item><title>&#29992;&#32479;&#35745;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32423;&#32852;&#20572;&#30005;&#20005;&#37325;&#24615;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#65292;&#19968;&#26159;&#36890;&#36807;&#21021;&#22987;&#20998;&#31867;&#27493;&#39588;&#36807;&#28388;&#23433;&#20840;&#24773;&#26223;&#65292;&#20108;&#26159;&#21033;&#29992;&#32423;&#32852;&#20572;&#30005;&#30340;&#32479;&#35745;&#23646;&#24615;&#20419;&#36827;&#38750;&#23616;&#37096;&#28040;&#24687;&#20256;&#36882;&#12290;</title><link>https://arxiv.org/abs/2403.15363</link><description>&lt;p&gt;
&#29992;&#32479;&#35745;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#32423;&#32852;&#20572;&#30005;&#20005;&#37325;&#24615;
&lt;/p&gt;
&lt;p&gt;
Cascading Blackout Severity Prediction with Statistically-Augmented Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15363
&lt;/p&gt;
&lt;p&gt;
&#29992;&#32479;&#35745;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32423;&#32852;&#20572;&#30005;&#20005;&#37325;&#24615;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#65292;&#19968;&#26159;&#36890;&#36807;&#21021;&#22987;&#20998;&#31867;&#27493;&#39588;&#36807;&#28388;&#23433;&#20840;&#24773;&#26223;&#65292;&#20108;&#26159;&#21033;&#29992;&#32423;&#32852;&#20572;&#30005;&#30340;&#32479;&#35745;&#23646;&#24615;&#20419;&#36827;&#38750;&#23616;&#37096;&#28040;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#32593;&#26465;&#20214;&#30340;&#39640;&#21464;&#24322;&#24615;&#65292;&#30001;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#28183;&#36879;&#22686;&#21152;&#21644;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#30340;&#22686;&#21152;&#65292;&#20351;&#24471;&#31579;&#36873;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#32423;&#32852;&#25925;&#38556;&#30340;&#24773;&#26223;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#20256;&#32479;&#29992;&#20110;&#35780;&#20272;&#32423;&#32852;&#20572;&#30005;&#39118;&#38505;&#30340;&#22522;&#20110;&#30005;&#21147;&#27969;&#30340;&#24037;&#20855;&#36895;&#24230;&#22826;&#24930;&#65292;&#26080;&#27861;&#36866;&#24403;&#25506;&#32034;&#21487;&#33021;&#30340;&#25925;&#38556;&#21644;&#36127;&#36733;/&#21457;&#30005;&#27169;&#24335;&#31354;&#38388;&#12290;&#25105;&#20204;&#22312;&#22686;&#38271;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25216;&#26415;&#25991;&#29486;&#20013;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#21021;&#22987;&#30005;&#32593;&#26465;&#20214;&#20272;&#35745;&#20572;&#30005;&#20005;&#37325;&#31243;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20005;&#37325;&#31243;&#24230;&#20272;&#35745;&#20043;&#21069;&#36890;&#36807;&#21021;&#22987;&#20998;&#31867;&#27493;&#39588;&#36807;&#28388;&#20986;&#23433;&#20840;&#30340;&#8220;&#38750;&#20572;&#30005;&#8221;&#24773;&#26223;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#32423;&#32852;&#20572;&#30005;&#30340;&#32479;&#35745;&#23646;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25105;&#20204;&#30340;GNN&#27169;&#22411;&#20013;&#20419;&#36827;&#38750;&#23616;&#37096;&#28040;&#24687;&#20256;&#36882;&#12290;&#25105;&#20204;&#23545;&#36825;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15363v1 Announce Type: cross  Abstract: Higher variability in grid conditions, resulting from growing renewable penetration and increased incidence of extreme weather events, has increased the difficulty of screening for scenarios that may lead to catastrophic cascading failures. Traditional power-flow-based tools for assessing cascading blackout risk are too slow to properly explore the space of possible failures and load/generation patterns. We add to the growing literature of faster graph-neural-network (GNN)-based techniques, developing two novel techniques for the estimation of blackout magnitude from initial grid conditions. First we propose several methods for employing an initial classification step to filter out safe "non blackout" scenarios prior to magnitude estimation. Second, using insights from the statistical properties of cascading blackouts, we propose a method for facilitating non-local message passing in our GNN models. We validate these two approaches on 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#25552;&#39640;&#23545;&#22270;&#20687;&#20013;&#22797;&#26434;&#32467;&#26500;&#30340;&#20998;&#21106;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20026;&#21487;&#20280;&#32553;&#27880;&#37322;&#25552;&#20379;&#20102;&#24378;&#22823;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.15361</link><description>&lt;p&gt;
&#23398;&#20064;&#28145;&#24230;&#22270;&#20687;&#29702;&#35299;&#30340;&#25299;&#25169;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Topological Representations for Deep Image Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15361
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#25552;&#39640;&#23545;&#22270;&#20687;&#20013;&#22797;&#26434;&#32467;&#26500;&#30340;&#20998;&#21106;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20026;&#21487;&#20280;&#32553;&#27880;&#37322;&#25552;&#20379;&#20102;&#24378;&#22823;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#65292;&#23588;&#20854;&#26159;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#65292;&#27491;&#30830;&#21010;&#20998;&#22797;&#26434;&#30340;&#32454;&#23567;&#32467;&#26500;&#22914;&#31070;&#32463;&#20803;&#12289;&#32452;&#32455;&#21644;&#34880;&#31649;&#23545;&#19979;&#28216;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#34429;&#28982;&#20855;&#26377;&#24378;&#22823;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#24182;&#27809;&#26377;&#25552;&#20379;&#23545;&#36825;&#20123;&#32467;&#26500;&#30340;&#20196;&#20154;&#28385;&#24847;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#21487;&#20280;&#32553;&#30340;&#27880;&#37322;&#21644;&#19979;&#28216;&#20998;&#26512;&#20013;&#20135;&#29983;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#38024;&#23545;&#36825;&#20123;&#25299;&#25169;&#32467;&#26500;&#25552;&#20986;&#30340;&#26032;&#39062;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#25968;&#23398;&#24037;&#20855;&#65292;&#21363;&#25345;&#20037;&#21516;&#35843;&#21644;&#31163;&#25955;&#33707;&#23572;&#26031;&#29702;&#35770;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#26356;&#22909;&#20998;&#21106;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#36825;&#23558;&#25104;&#20026;&#21487;&#20280;&#32553;&#27880;&#37322;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15361v1 Announce Type: cross  Abstract: In many scenarios, especially biomedical applications, the correct delineation of complex fine-scaled structures such as neurons, tissues, and vessels is critical for downstream analysis. Despite the strong predictive power of deep learning methods, they do not provide a satisfactory representation of these structures, thus creating significant barriers in scalable annotation and downstream analysis. In this dissertation, we tackle such challenges by proposing novel representations of these topological structures in a deep learning framework. We leverage the mathematical tools from topological data analysis, i.e., persistent homology and discrete Morse theory, to develop principled methods for better segmentation and uncertainty estimation, which will become powerful tools for scalable annotation.
&lt;/p&gt;</description></item><item><title>SiMBA&#26159;&#19968;&#31181;&#24341;&#20837;Einstein FFT&#36827;&#34892;&#36890;&#36947;&#24314;&#27169;&#24182;&#20351;&#29992;Mamba&#22359;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#30340;&#26032;&#26550;&#26500;&#65292;&#22312;&#22270;&#20687;&#21644;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;SSMs&#12290;</title><link>https://arxiv.org/abs/2403.15360</link><description>&lt;p&gt;
SiMBA&#65306;&#29992;&#20110;&#35270;&#35273;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#31616;&#21270;Mamba&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15360
&lt;/p&gt;
&lt;p&gt;
SiMBA&#26159;&#19968;&#31181;&#24341;&#20837;Einstein FFT&#36827;&#34892;&#36890;&#36947;&#24314;&#27169;&#24182;&#20351;&#29992;Mamba&#22359;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#30340;&#26032;&#26550;&#26500;&#65292;&#22312;&#22270;&#20687;&#21644;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;SSMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#24191;&#27867;&#37319;&#29992;&#20102;&#27880;&#24847;&#21147;&#32593;&#32476;&#36827;&#34892;&#24207;&#21015;&#28151;&#21512;&#21644;MLPs&#36827;&#34892;&#36890;&#36947;&#28151;&#21512;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#25991;&#29486;&#24378;&#35843;&#20102;&#20851;&#20110;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#23545;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#30340;&#20302;&#24402;&#32435;&#20559;&#24046;&#21644;&#20108;&#27425;&#22797;&#26434;&#24230;&#12290;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#22914;S4&#21644;&#20854;&#20182;&#27169;&#22411;&#65288;Hippo&#65292;Global Convolutions&#65292;liquid S4&#65292;LRU&#65292;Mega&#21644;Mamba&#65289;&#65292;&#24050;&#32463;&#20986;&#29616;&#26469;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#20197;&#24110;&#21161;&#22788;&#29702;&#26356;&#38271;&#30340;&#24207;&#21015;&#38271;&#24230;&#12290;Mamba&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;SSM&#65292;&#20294;&#22312;&#25193;&#23637;&#21040;&#22823;&#22411;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#26102;&#23384;&#22312;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SiMBA&#65292;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#29305;&#24449;&#20540;&#35745;&#31639;&#24341;&#20837;Einstein FFT&#65288;EinFFT&#65289;&#26469;&#36827;&#34892;&#36890;&#36947;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;Mamba&#22359;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#12290;&#23545;&#22270;&#20687;&#21644;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#30340;&#24191;&#27867;&#24615;&#33021;&#30740;&#31350;&#34920;&#26126;&#65292;SiMBA&#20248;&#20110;&#29616;&#26377;&#30340;SSMs&#65292;&#26550;&#36215;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15360v1 Announce Type: cross  Abstract: Transformers have widely adopted attention networks for sequence mixing and MLPs for channel mixing, playing a pivotal role in achieving breakthroughs across domains. However, recent literature highlights issues with attention networks, including low inductive bias and quadratic complexity concerning input sequence length. State Space Models (SSMs) like S4 and others (Hippo, Global Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address the above issues to help handle longer sequence lengths. Mamba, while being the state-of-the-art SSM, has a stability issue when scaled to large networks for computer vision datasets. We propose SiMBA, a new architecture that introduces Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations and uses the Mamba block for sequence modeling. Extensive performance studies across image and time-series benchmarks demonstrate that SiMBA outperforms existing SSMs, bridging
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#36229;&#22768;&#32447;&#24615;&#30452;&#25509;&#27169;&#22411;&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#20808;&#39564;&#30340;&#28151;&#21512;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#27169;&#22411;&#36827;&#34892;&#26080;&#30417;&#30563;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#24449;&#36229;&#22768;&#22270;&#20687;&#25193;&#25955;&#37325;&#24314;&#38543;&#26426;&#24615;&#30340;&#32463;&#39564;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15316</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#24674;&#22797;&#27169;&#22411;&#26041;&#24046;&#30340;&#36229;&#22768;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Ultrasound Imaging based on the Variance of a Diffusion Restoration Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15316
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#36229;&#22768;&#32447;&#24615;&#30452;&#25509;&#27169;&#22411;&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#20808;&#39564;&#30340;&#28151;&#21512;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#27169;&#22411;&#36827;&#34892;&#26080;&#30417;&#30563;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#24449;&#36229;&#22768;&#22270;&#20687;&#25193;&#25955;&#37325;&#24314;&#38543;&#26426;&#24615;&#30340;&#32463;&#39564;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20170;&#22825;&#36229;&#22768;&#25104;&#20687;&#22312;&#21307;&#23398;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#36229;&#22768;&#20449;&#22122;&#27604;&#20173;&#21463;&#22810;&#31181;&#22122;&#22768;&#21644;&#20266;&#24433;&#24433;&#21709;&#12290;&#32780;&#19988;&#65292;&#25552;&#39640;&#36229;&#22768;&#22270;&#20687;&#36136;&#37327;&#28041;&#21450;&#24179;&#34913;&#23545;&#27604;&#24230;&#12289;&#20998;&#36776;&#29575;&#21644;&#26001;&#28857;&#20445;&#30041;&#31561;&#24182;&#21457;&#22240;&#32032;&#12290;&#26368;&#36817;&#65292;&#22312;&#27169;&#22411;&#20026;&#22522;&#30784;&#21644;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#20013;&#65292;&#22312;&#35299;&#20915;&#36229;&#22768;&#22270;&#20687;&#37325;&#24314;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#32467;&#21512;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#37325;&#24314;&#26041;&#27861;&#65292;&#23558;&#36229;&#22768;&#32447;&#24615;&#30452;&#25509;&#27169;&#22411;&#19982;&#29983;&#25104;&#24335;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#20808;&#39564;&#30456;&#32467;&#21512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#27169;&#22411;(DDRM)&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#12290;&#37492;&#20110;&#36229;&#22768;&#22266;&#26377;&#30340;&#20056;&#24615;&#22122;&#22768;&#29305;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#39564;&#27169;&#22411;&#65292;&#29992;&#20110;&#34920;&#24449;&#36229;&#22768;&#22270;&#20687;&#25193;&#25955;&#37325;&#24314;&#30340;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15316v1 Announce Type: cross  Abstract: Despite today's prevalence of ultrasound imaging in medicine, ultrasound signal-to-noise ratio is still affected by several sources of noise and artefacts. Moreover, enhancing ultrasound image quality involves balancing concurrent factors like contrast, resolution, and speckle preservation. Recently, there has been progress in both model-based and learning-based approaches addressing the problem of ultrasound image reconstruction. Bringing the best from both worlds, we propose a hybrid reconstruction method combining an ultrasound linear direct model with a learning-based prior coming from a generative Denoising Diffusion model. More specifically, we rely on the unsupervised fine-tuning of a pre-trained Denoising Diffusion Restoration Model (DDRM). Given the nature of multiplicative noise inherent to ultrasound, this paper proposes an empirical model to characterize the stochasticity of diffusion reconstruction of ultrasound images, an
&lt;/p&gt;</description></item><item><title>&#23558;&#26222;&#36890;GANs&#19982;&#27700;&#26031;&#22374;&#36317;&#31163;&#32852;&#31995;&#36215;&#26469;&#65292;&#25193;&#23637;&#29616;&#26377;&#27700;&#26031;&#22374;GANs&#32467;&#26524;&#21040;&#26222;&#36890;GANs&#65292;&#33719;&#24471;&#20102;&#26222;&#36890;GANs&#30340;&#31070;&#35861;&#19981;&#31561;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.15312</link><description>&lt;p&gt;
&#27700;&#26031;&#22374;&#35270;&#35282;&#19979;&#30340;&#26222;&#36890; GANs
&lt;/p&gt;
&lt;p&gt;
A Wasserstein perspective of Vanilla GANs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15312
&lt;/p&gt;
&lt;p&gt;
&#23558;&#26222;&#36890;GANs&#19982;&#27700;&#26031;&#22374;&#36317;&#31163;&#32852;&#31995;&#36215;&#26469;&#65292;&#25193;&#23637;&#29616;&#26377;&#27700;&#26031;&#22374;GANs&#32467;&#26524;&#21040;&#26222;&#36890;GANs&#65292;&#33719;&#24471;&#20102;&#26222;&#36890;GANs&#30340;&#31070;&#35861;&#19981;&#31561;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#30340;&#23454;&#35777;&#25104;&#21151;&#24341;&#36215;&#20102;&#23545;&#29702;&#35770;&#30740;&#31350;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#12290;&#32479;&#35745;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#27700;&#26031;&#22374;GANs&#21450;&#20854;&#25193;&#23637;&#19978;&#65292;&#29305;&#21035;&#26159;&#20801;&#35768;&#20855;&#26377;&#33391;&#22909;&#30340;&#38477;&#32500;&#29305;&#24615;&#12290;&#23545;&#20110;&#26222;&#36890;GANs&#65292;&#21363;&#21407;&#22987;&#20248;&#21270;&#38382;&#39064;&#65292;&#32479;&#35745;&#32467;&#26524;&#20173;&#28982;&#30456;&#24403;&#26377;&#38480;&#65292;&#38656;&#35201;&#20551;&#35774;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#21644;&#28508;&#31354;&#38388;&#19982;&#21608;&#22260;&#31354;&#38388;&#30340;&#32500;&#24230;&#30456;&#31561;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23558;&#26222;&#36890;GANs&#19982;&#27700;&#26031;&#22374;&#36317;&#31163;&#32852;&#31995;&#36215;&#26469;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#29616;&#26377;&#30340;&#27700;&#26031;&#22374;GANs&#32467;&#26524;&#21487;&#20197;&#25193;&#23637;&#21040;&#26222;&#36890;GANs&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#27700;&#26031;&#22374;&#36317;&#31163;&#20013;&#33719;&#24471;&#20102;&#26222;&#36890;GANs&#30340;&#31070;&#35861;&#19981;&#31561;&#24335;&#12290;&#36825;&#20010;&#31070;&#35861;&#19981;&#31561;&#24335;&#30340;&#20551;&#35774;&#26088;&#22312;&#30001;&#23454;&#36341;&#20013;&#24120;&#29992;&#30340;&#32593;&#32476;&#26550;&#26500;&#28385;&#36275;&#65292;&#22914;&#21069;&#39304;ReLU&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15312v1 Announce Type: cross  Abstract: The empirical success of Generative Adversarial Networks (GANs) caused an increasing interest in theoretical research. The statistical literature is mainly focused on Wasserstein GANs and generalizations thereof, which especially allow for good dimension reduction properties. Statistical results for Vanilla GANs, the original optimization problem, are still rather limited and require assumptions such as smooth activation functions and equal dimensions of the latent space and the ambient space. To bridge this gap, we draw a connection from Vanilla GANs to the Wasserstein distance. By doing so, existing results for Wasserstein GANs can be extended to Vanilla GANs. In particular, we obtain an oracle inequality for Vanilla GANs in Wasserstein distance. The assumptions of this oracle inequality are designed to be satisfied by network architectures commonly used in practice, such as feedforward ReLU networks. By providing a quantitative resu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#19968;&#26041;&#38754;&#20351;&#29992;&#30417;&#30563;&#27169;&#22411;&#21453;&#39304;&#25214;&#21040;&#23545;&#25239;&#24615;&#25552;&#31034;&#35789;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#65292;&#21478;&#19968;&#26041;&#38754;&#36890;&#36807;&#24341;&#23548;&#20351;&#29983;&#25104;&#36807;&#31243;&#26397;&#21521;&#29305;&#23450;&#30446;&#26631;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.15309</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Controlled Training Data Generation with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15309
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#19968;&#26041;&#38754;&#20351;&#29992;&#30417;&#30563;&#27169;&#22411;&#21453;&#39304;&#25214;&#21040;&#23545;&#25239;&#24615;&#25552;&#31034;&#35789;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#65292;&#21478;&#19968;&#26041;&#38754;&#36890;&#36807;&#24341;&#23548;&#20351;&#29983;&#25104;&#36807;&#31243;&#26397;&#21521;&#29305;&#23450;&#30446;&#26631;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20197;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#19987;&#38376;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#12290;&#19982;&#20043;&#21069;&#37027;&#20123;&#37319;&#29992;&#24320;&#29615;&#26041;&#27861;&#24182;&#39044;&#20808;&#23450;&#20041;&#25552;&#31034;&#35789;&#26469;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25110;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#29983;&#25104;&#26032;&#25968;&#25454;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#38381;&#29615;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#21453;&#39304;&#26426;&#21046;&#12290;&#31532;&#19968;&#20010;&#26426;&#21046;&#20351;&#29992;&#26469;&#33258;&#32473;&#23450;&#30417;&#30563;&#27169;&#22411;&#30340;&#21453;&#39304;&#65292;&#24182;&#25214;&#21040;&#23548;&#33268;&#22270;&#20687;&#29983;&#25104;&#26368;&#22823;&#21270;&#27169;&#22411;&#25439;&#22833;&#30340;&#23545;&#25239;&#25552;&#31034;&#35789;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#25239;&#25552;&#31034;&#35789;&#23548;&#33268;&#20102;&#32463;&#36807;&#27169;&#22411;&#35757;&#32451;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#30693;&#36947;&#30446;&#26631;&#20998;&#24067;&#65292;&#36825;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#31532;&#20108;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#23558;&#29983;&#25104;&#36807;&#31243;&#24341;&#23548;&#21040;&#29305;&#23450;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#31216;&#23558;&#36825;&#20004;&#20010;&#26426;&#21046;&#32467;&#21512;&#36215;&#26469;&#30340;&#26041;&#27861;&#20026;&#24341;&#23548;&#23545;&#25239;&#25552;&#31034;&#35789;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15309v1 Announce Type: cross  Abstract: In this work, we present a method to control a text-to-image generative model to produce training data specifically "useful" for supervised learning. Unlike previous works that employ an open-loop approach and pre-define prompts to generate new data using either a language model or human expertise, we develop an automated closed-loop system which involves two feedback mechanisms. The first mechanism uses feedback from a given supervised model and finds adversarial prompts that result in image generations that maximize the model loss. While these adversarial prompts result in diverse data informed by the model, they are not informed of the target distribution, which can be inefficient. Therefore, we introduce the second feedback mechanism that guides the generation process towards a certain target distribution. We call the method combining these two mechanisms Guided Adversarial Prompts. We perform our evaluations on different tasks, da
&lt;/p&gt;</description></item><item><title>KTbench&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;KT&#27169;&#22411;&#20013;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15304</link><description>&lt;p&gt;
KTbench&#65306;&#19968;&#31181;&#20840;&#26032;&#30340;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15304
&lt;/p&gt;
&lt;p&gt;
KTbench&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;KT&#27169;&#22411;&#20013;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#28041;&#21450;&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#39044;&#27979;&#23398;&#29983;&#23545;&#23398;&#20064;&#39033;&#30446;&#30340;&#26410;&#26469;&#34920;&#29616;&#12290;&#23398;&#20064;&#39033;&#30446;&#34987;&#26631;&#35760;&#20026;&#31216;&#20026;&#30693;&#35782;&#27010;&#24565;&#65288;KCs&#65289;&#30340;&#25216;&#33021;&#26631;&#31614;&#12290;&#35768;&#22810;KT&#27169;&#22411;&#36890;&#36807;&#29992;&#26500;&#25104;KC&#30340;&#23398;&#20064;&#39033;&#30446;&#21462;&#20195;&#23398;&#20064;&#39033;&#30446;&#26469;&#23558;&#23398;&#20064;&#39033;&#30446;-&#23398;&#29983;&#20132;&#20114;&#24207;&#21015;&#25193;&#23637;&#20026;KC-&#23398;&#29983;&#20132;&#20114;&#24207;&#21015;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31232;&#30095;&#30340;&#23398;&#20064;&#39033;&#30446;-&#23398;&#29983;&#20132;&#20114;&#38382;&#39064;&#24182;&#26368;&#23567;&#21270;&#20102;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#26159;&#27169;&#22411;&#23398;&#20064;&#21516;&#19968;&#39033;&#30446;&#20869;&#30340;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22522;&#26412;&#20107;&#23454;&#26631;&#31614;&#30340;&#27844;&#28431;&#24182;&#38459;&#30861;&#27169;&#22411;&#24615;&#33021;&#12290;&#31532;&#20108;&#20010;&#38382;&#39064;&#26159;&#29616;&#26377;&#30340;&#22522;&#20934;&#23454;&#29616;&#24573;&#30053;&#20102;&#35745;&#25968;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15304v1 Announce Type: cross  Abstract: Knowledge Tracing (KT) is concerned with predicting students' future performance on learning items in intelligent tutoring systems. Learning items are tagged with skill labels called knowledge concepts (KCs). Many KT models expand the sequence of item-student interactions into KC-student interactions by replacing learning items with their constituting KCs. This often results in a longer sequence length. This approach addresses the issue of sparse item-student interactions and minimises model parameters. However, two problems have been identified with such models.   The first problem is the model's ability to learn correlations between KCs belonging to the same item, which can result in the leakage of ground truth labels and hinder performance. This problem can lead to a significant decrease in performance on datasets with a higher number of KCs per item. The second problem is that the available benchmark implementations ignore accounti
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32487;&#25215;&#29305;&#24449;&#23398;&#20064;&#31574;&#30053;&#22522;&#30784;&#65292;&#20351;&#27599;&#20010;&#65288;&#23376;&#65289;&#31574;&#30053;&#35299;&#20915;&#19968;&#20010;&#23376;&#38382;&#39064;&#65292;&#22312;FSA&#25551;&#36848;&#30340;&#20219;&#21153;&#20013;&#65292;&#32452;&#21512;&#36825;&#20123;&#65288;&#23376;&#65289;&#31574;&#30053;&#21487;&#29992;&#20110;&#26080;&#38656;&#39069;&#22806;&#23398;&#20064;&#29983;&#25104;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#26041;&#27861;&#33021;&#22815;&#28176;&#36817;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20063;&#22914;&#27492;&#12290;</title><link>https://arxiv.org/abs/2403.15301</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#30340;&#31574;&#30053;&#22522;&#30784;&#36827;&#34892;&#35268;&#21010;&#20197;&#26368;&#20248;&#22320;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Planning with a Learned Policy Basis to Optimally Solve Complex Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15301
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32487;&#25215;&#29305;&#24449;&#23398;&#20064;&#31574;&#30053;&#22522;&#30784;&#65292;&#20351;&#27599;&#20010;&#65288;&#23376;&#65289;&#31574;&#30053;&#35299;&#20915;&#19968;&#20010;&#23376;&#38382;&#39064;&#65292;&#22312;FSA&#25551;&#36848;&#30340;&#20219;&#21153;&#20013;&#65292;&#32452;&#21512;&#36825;&#20123;&#65288;&#23376;&#65289;&#31574;&#30053;&#21487;&#29992;&#20110;&#26080;&#38656;&#39069;&#22806;&#23398;&#20064;&#29983;&#25104;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#26041;&#27861;&#33021;&#22815;&#28176;&#36817;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20063;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35299;&#20915;&#21508;&#31181;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#35268;&#33539;&#30340;&#24773;&#26223;&#20013;&#23398;&#20064;&#33021;&#22815;&#21487;&#38752;&#27867;&#21270;&#20110;&#22810;&#20010;&#20219;&#21153;&#30340;&#31574;&#30053;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32487;&#25215;&#29305;&#24449;&#26469;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#22522;&#30784;&#65292;&#20351;&#24471;&#20854;&#20013;&#30340;&#27599;&#19968;&#20010;&#65288;&#23376;&#65289;&#31574;&#30053;&#35299;&#20915;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#23376;&#38382;&#39064;&#12290;&#22312;&#30001;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288;FSA&#65289;&#25551;&#36848;&#30340;&#20219;&#21153;&#20013;&#28041;&#21450;&#30456;&#21516;&#19968;&#32452;&#23376;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#65288;&#23376;&#65289;&#31574;&#30053;&#30340;&#32452;&#21512;&#21487;&#20197;&#34987;&#29992;&#26469;&#29983;&#25104;&#19968;&#20010;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#23398;&#20064;&#12290;&#19982;&#20854;&#20182;&#36890;&#36807;&#35268;&#21010;&#32452;&#21512;&#65288;&#23376;&#65289;&#31574;&#30053;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28176;&#36817;&#19978;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15301v1 Announce Type: cross  Abstract: Conventional reinforcement learning (RL) methods can successfully solve a wide range of sequential decision problems. However, learning policies that can generalize predictably across multiple tasks in a setting with non-Markovian reward specifications is a challenging problem. We propose to use successor features to learn a policy basis so that each (sub)policy in it solves a well-defined subproblem. In a task described by a finite state automaton (FSA) that involves the same set of subproblems, the combination of these (sub)policies can then be used to generate an optimal solution without additional learning. In contrast to other methods that combine (sub)policies via planning, our method asymptotically attains global optimality, even in stochastic environments.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#21311;&#21517;&#31649;&#29702;&#65292;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#36710;&#36733;&#36793;&#32536;&#20803;&#23431;&#23449;&#20013;&#36710;&#36742;&#21452;&#23376;&#36801;&#31227;&#36807;&#31243;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.15285</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#36710;&#36733;&#36793;&#32536;&#20803;&#23431;&#23449;&#20013;&#36710;&#36742;&#21452;&#23376;&#36801;&#31227;&#30340;&#21311;&#21517;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Blockchain-based Pseudonym Management for Vehicle Twin Migrations in Vehicular Edge Metaverse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15285
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#21311;&#21517;&#31649;&#29702;&#65292;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#36710;&#36733;&#36793;&#32536;&#20803;&#23431;&#23449;&#20013;&#36710;&#36742;&#21452;&#23376;&#36801;&#31227;&#36807;&#31243;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20803;&#23431;&#23449;&#21644;&#36793;&#32536;&#35745;&#31639;&#25216;&#26415;&#30340;&#24040;&#22823;&#36827;&#23637;&#25512;&#21160;&#65292;&#39044;&#35745;&#36710;&#36733;&#36793;&#32536;&#20803;&#23431;&#23449;&#23558;&#39072;&#35206;&#24403;&#21069;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#33539;&#24335;&#12290;&#20316;&#20026;&#36710;&#36733;&#20803;&#23431;&#23449;&#29992;&#25143;&#65288;VMUs&#65289;&#30340;&#39640;&#24230;&#30005;&#31639;&#21270;&#21270;&#36523;&#65292;&#37096;&#32626;&#22312;&#36793;&#32536;&#26381;&#21153;&#22120;&#20013;&#30340;&#36710;&#36742;&#21452;&#23376;&#65288;VTs&#65289;&#21487;&#20197;&#25552;&#20379;&#23453;&#36149;&#30340;&#20803;&#23431;&#23449;&#26381;&#21153;&#65292;&#20197;&#25913;&#21892;&#20854;VMUs&#22312;&#25972;&#20010;&#34892;&#31243;&#20013;&#30340;&#39550;&#39542;&#23433;&#20840;&#24615;&#21644;&#36710;&#20869;&#28385;&#24847;&#24230;&#12290;&#20026;&#20102;&#20445;&#25345;&#19981;&#38388;&#26029;&#30340;&#20803;&#23431;&#23449;&#20307;&#39564;&#65292;VTs&#24517;&#39035;&#26681;&#25454;&#36710;&#36742;&#30340;&#31227;&#21160;&#22312;&#36793;&#32536;&#26381;&#21153;&#22120;&#20043;&#38388;&#36827;&#34892;&#36801;&#31227;&#12290;&#36825;&#21487;&#33021;&#24341;&#21457;&#26377;&#20851;&#36710;&#36733;&#36793;&#32536;&#20803;&#23431;&#23449;&#20043;&#38388;&#21160;&#24577;&#36890;&#20449;&#36807;&#31243;&#20013;&#38544;&#31169;&#27844;&#38706;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#20445;&#25252;&#20301;&#32622;&#38544;&#31169;&#65292;&#21487;&#20197;&#21033;&#29992;&#20551;&#21517;&#20316;&#20026;&#20020;&#26102;&#26631;&#35782;&#31526;&#65292;&#30001;VMUs&#21644;VTs&#20849;&#21516;&#21033;&#29992;&#65292;&#22312;&#29289;&#29702;&#31354;&#38388;&#21644;&#34394;&#25311;&#31354;&#38388;&#20013;&#23454;&#29616;&#21311;&#21517;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20551;&#21517;&#31649;&#29702;&#26041;&#27861;&#22312;&#28385;&#36275;&#38656;&#27714;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15285v1 Announce Type: cross  Abstract: Driven by the great advances in metaverse and edge computing technologies, vehicular edge metaverses are expected to disrupt the current paradigm of intelligent transportation systems. As highly computerized avatars of Vehicular Metaverse Users (VMUs), the Vehicle Twins (VTs) deployed in edge servers can provide valuable metaverse services to improve driving safety and on-board satisfaction for their VMUs throughout journeys. To maintain uninterrupted metaverse experiences, VTs must be migrated among edge servers following the movements of vehicles. This can raise concerns about privacy breaches during the dynamic communications among vehicular edge metaverses. To address these concerns and safeguard location privacy, pseudonyms as temporary identifiers can be leveraged by both VMUs and VTs to realize anonymous communications in the physical space and virtual spaces. However, existing pseudonym management methods fall short in meeting 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23383;&#20856;&#23398;&#20064;&#21644;&#21487;&#24494;&#20998;L$_0$&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#12289;&#40065;&#26834;&#12289;&#21487;&#35299;&#37322;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#21442;&#25968;PDE&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2403.15267</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#21487;&#24494;&#20998;L0&#31232;&#30095;&#22810;&#39033;&#24335;&#31574;&#30053;&#30340;&#21442;&#25968;PDE&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Parametric PDE Control with Deep Reinforcement Learning and Differentiable L0-Sparse Polynomial Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15267
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23383;&#20856;&#23398;&#20064;&#21644;&#21487;&#24494;&#20998;L$_0$&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#12289;&#40065;&#26834;&#12289;&#21487;&#35299;&#37322;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#21442;&#25968;PDE&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26368;&#20248;&#25511;&#21046;&#22312;&#24037;&#31243;&#21644;&#31185;&#23398;&#30340;&#35768;&#22810;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#20026;&#21442;&#25968;PDE&#30340;&#25511;&#21046;&#24320;&#36767;&#20102;&#26032;&#30340;&#21069;&#27839;&#12290;&#29305;&#21035;&#26159;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26377;&#26395;&#35299;&#20915;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#39640;&#32500;&#22797;&#26434;&#25511;&#21046;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#23383;&#20856;&#23398;&#20064;&#21644;&#21487;&#24494;&#20998;L$_0$&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#31232;&#30095;&#12289;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#21442;&#25968;PDE&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15267v1 Announce Type: new  Abstract: Optimal control of parametric partial differential equations (PDEs) is crucial in many applications in engineering and science. In recent years, the progress in scientific machine learning has opened up new frontiers for the control of parametric PDEs. In particular, deep reinforcement learning (DRL) has the potential to solve high-dimensional and complex control problems in a large variety of applications. Most DRL methods rely on deep neural network (DNN) control policies. However, for many dynamical systems, DNN-based control policies tend to be over-parametrized, which means they need large amounts of training data, show limited robustness, and lack interpretability. In this work, we leverage dictionary learning and differentiable L$_0$ regularization to learn sparse, robust, and interpretable control policies for parametric PDEs. Our sparse policy architecture is agnostic to the DRL method and can be used in different policy-gradien
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20256;&#36798;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.15263</link><description>&lt;p&gt;
&#32852;&#37030;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65306;&#32479;&#35745;&#32858;&#21512;&#26041;&#27861;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated Bayesian Deep Learning: The Application of Statistical Aggregation Methods to Bayesian Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20256;&#36798;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#31181;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#21644;&#20943;&#23569;&#19982;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#32858;&#21512;&#31574;&#30053;&#65292;&#29992;&#20110;&#25972;&#21512;&#25110;&#34701;&#21512;&#20998;&#24067;&#24335;&#30830;&#23450;&#24615;&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#20559;&#24046;&#65307;&#28982;&#32780;&#65292;&#29616;&#20195;&#30830;&#23450;&#24615;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#36890;&#24120;&#26657;&#20934;&#19981;&#20339;&#65292;&#32570;&#20047;&#22312;&#39044;&#27979;&#20013;&#20256;&#36798;&#19968;&#31181;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#65292;&#36825;&#23545;&#36965;&#24863;&#24179;&#21488;&#21644;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#26159;&#29702;&#24819;&#30340;&#12290;&#30456;&#21453;&#65292;&#36125;&#21494;&#26031;DL&#27169;&#22411;&#36890;&#24120;&#26657;&#20934;&#33391;&#22909;&#65292;&#33021;&#22815;&#37327;&#21270;&#21644;&#20256;&#36798;&#19968;&#31181;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#20197;&#21450;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22240;&#20026;&#36125;&#21494;&#26031;DL&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#21644;&#20559;&#24046;&#30001;&#27010;&#29575;&#20998;&#24067;&#23450;&#20041;&#65292;&#25152;&#20197;&#31616;&#21333;&#24212;&#29992;&#32858;&#21512;&#26041;&#27861;&#26159;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15263v1 Announce Type: new  Abstract: Federated learning (FL) is an approach to training machine learning models that takes advantage of multiple distributed datasets while maintaining data privacy and reducing communication costs associated with sharing local datasets. Aggregation strategies have been developed to pool or fuse the weights and biases of distributed deterministic models; however, modern deterministic deep learning (DL) models are often poorly calibrated and lack the ability to communicate a measure of epistemic uncertainty in prediction, which is desirable for remote sensing platforms and safety-critical applications. Conversely, Bayesian DL models are often well calibrated and capable of quantifying and communicating a measure of epistemic uncertainty along with a competitive prediction accuracy. Unfortunately, because the weights and biases in Bayesian DL models are defined by a probability distribution, simple application of the aggregation methods associa
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22823;&#35268;&#27169;LLM&#20013;&#22240;&#32032;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36890;&#36807;&#20840;&#38754;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#25512;&#21160;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#23637;</title><link>https://arxiv.org/abs/2403.15250</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35780;&#20272;&#32467;&#26524;&#22312;LLM&#20013;&#30340;&#20840;&#38754;&#37325;&#26032;&#35780;&#20272;&#65306;&#19968;&#31181;&#22810;&#26041;&#20301;&#32479;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15250
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#35268;&#27169;LLM&#20013;&#22240;&#32032;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36890;&#36807;&#20840;&#38754;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#25512;&#21160;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;LLM&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#35780;&#20272;&#22312;&#29702;&#35299;&#21644;&#25512;&#21160;&#36825;&#20123;&#27169;&#22411;&#21069;&#36827;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;&#35780;&#20272;&#25581;&#31034;&#20102;&#32553;&#25918;&#12289;&#35757;&#32451;&#31867;&#22411;&#12289;&#26550;&#26500;&#31561;&#22240;&#32032;&#28145;&#21051;&#24433;&#21709;LLM&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22240;&#32032;&#23545;&#24615;&#33021;&#35780;&#20998;&#30340;&#24433;&#21709;&#31243;&#24230;&#21644;&#24615;&#36136;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#35780;&#20272;&#23616;&#38480;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#28857;&#12290;&#36890;&#36807;&#32479;&#35745;&#35270;&#35282;&#26356;&#26377;&#25928;&#22320;&#28548;&#28165;&#36825;&#20123;&#22240;&#32032;&#23545;&#24615;&#33021;&#24471;&#20998;&#30340;&#24433;&#21709;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#36825;&#20123;LLM&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#37325;&#26032;&#26816;&#26597;&#65292;&#38024;&#23545;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#38543;&#30528;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#32479;&#35745;&#26041;&#27861;&#35770;&#12290;&#20854;&#20013;&#21253;&#25324;ANOVA&#12289;Tukey HSD&#26816;&#39564;&#12289;GAMM&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15250v1 Announce Type: cross  Abstract: Amidst the rapid evolution of LLMs, the significance of evaluation in comprehending and propelling these models forward is increasingly paramount. Evaluations have revealed that factors such as scaling, training types, architectures and other factors profoundly impact the performance of LLMs. However, the extent and nature of these impacts continue to be subjects of debate because most assessments have been restricted to a limited number of models and data points. Clarifying the effects of these factors on performance scores can be more effectively achieved through a statistical lens. Our study embarks on a thorough re-examination of these LLMs, targeting the inadequacies in current evaluation methods. With the advent of a uniform evaluation framework, our research leverages an expansive dataset of evaluation results, introducing a comprehensive statistical methodology. This includes the application of ANOVA, Tukey HSD tests, GAMM, and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spectral Motion Alignment&#65288;SMA&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#26469;&#20248;&#21270;&#21644;&#23545;&#40784;&#36816;&#21160;&#21521;&#37327;&#65292;&#23398;&#20064;&#25972;&#24103;&#20840;&#23616;&#36816;&#21160;&#21160;&#24577;&#65292;&#20943;&#36731;&#31354;&#38388;&#20266;&#24433;&#65292;&#26377;&#25928;&#25913;&#21892;&#36816;&#21160;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.15249</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35270;&#39057;&#36816;&#21160;&#36716;&#31227;&#30340;&#20809;&#35889;&#36816;&#21160;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Spectral Motion Alignment for Video Motion Transfer using Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15249
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spectral Motion Alignment&#65288;SMA&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#26469;&#20248;&#21270;&#21644;&#23545;&#40784;&#36816;&#21160;&#21521;&#37327;&#65292;&#23398;&#20064;&#25972;&#24103;&#20840;&#23616;&#36816;&#21160;&#21160;&#24577;&#65292;&#20943;&#36731;&#31354;&#38388;&#20266;&#24433;&#65292;&#26377;&#25928;&#25913;&#21892;&#36816;&#21160;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#22312;&#35270;&#39057;&#29983;&#25104;&#21644;&#29702;&#35299;&#26041;&#38754;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65288;VDMs&#65289;&#26174;&#33879;&#20419;&#36827;&#20102;&#23558;&#36755;&#20837;&#35270;&#39057;&#23450;&#21046;&#20026;&#30446;&#26631;&#22806;&#35266;&#12289;&#36816;&#21160;&#31561;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#20934;&#30830;&#25552;&#21462;&#35270;&#39057;&#24103;&#30340;&#36816;&#21160;&#20449;&#24687;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#20316;&#21697;&#21033;&#29992;&#36830;&#32493;&#24103;&#27531;&#24046;&#20316;&#20026;&#30446;&#26631;&#36816;&#21160;&#21521;&#37327;&#65292;&#20294;&#23427;&#20204;&#22266;&#26377;&#22320;&#32570;&#20047;&#20840;&#23616;&#36816;&#21160;&#32972;&#26223;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#36880;&#24103;&#22833;&#30495;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20809;&#35889;&#36816;&#21160;&#23545;&#40784;&#65288;SMA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#20613;&#31435;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#26469;&#20248;&#21270;&#21644;&#23545;&#40784;&#36816;&#21160;&#21521;&#37327;&#30340;&#26032;&#26694;&#26550;&#12290;SMA&#36890;&#36807;&#25972;&#21512;&#39057;&#22495;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#36816;&#21160;&#27169;&#24335;&#65292;&#20419;&#36827;&#25972;&#24103;&#20840;&#23616;&#36816;&#21160;&#21160;&#24577;&#30340;&#23398;&#20064;&#65292;&#24182;&#20943;&#36731;&#31354;&#38388;&#20266;&#24433;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;SMA&#22312;&#25913;&#21892;&#36816;&#21160;&#36716;&#31227;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15249v1 Announce Type: cross  Abstract: The evolution of diffusion models has greatly impacted video generation and understanding. Particularly, text-to-video diffusion models (VDMs) have significantly facilitated the customization of input video with target appearance, motion, etc. Despite these advances, challenges persist in accurately distilling motion information from video frames. While existing works leverage the consecutive frame residual as the target motion vector, they inherently lack global motion context and are vulnerable to frame-wise distortions. To address this, we present Spectral Motion Alignment (SMA), a novel framework that refines and aligns motion vectors using Fourier and wavelet transforms. SMA learns motion patterns by incorporating frequency-domain regularization, facilitating the learning of whole-frame global motion dynamics, and mitigating spatial artifacts. Extensive experiments demonstrate SMA's efficacy in improving motion transfer while main
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;&#35758;&#35770;&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#21382;&#21490;&#65292;&#26088;&#22312;&#20351;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#21028;&#26029;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15246</link><description>&lt;p&gt;
FollowIR: &#35780;&#20272;&#21644;&#25945;&#25480;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#20197;&#36981;&#24490;&#35828;&#26126;&#20070;
&lt;/p&gt;
&lt;p&gt;
FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;&#35758;&#35770;&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#21382;&#21490;&#65292;&#26088;&#22312;&#20351;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#21028;&#26029;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#36981;&#24490;&#38271;&#19988;&#22797;&#26434;&#30340;&#35828;&#26126;&#20070;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#27169;&#22411;&#20351;&#29992;LLMs&#20316;&#20026;&#20854;&#26550;&#26500;&#30340;&#25903;&#26609;&#65292;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#21482;&#25509;&#21463;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#27809;&#26377;&#35828;&#26126;&#20070;&#12290;&#23545;&#20110;&#26368;&#36817;&#19968;&#20123;&#25509;&#21463;&#35828;&#26126;&#20070;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#23427;&#20204;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#35828;&#26126;&#20070;&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#65292;&#20197;&#21450;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;IR&#27169;&#22411;&#23398;&#20064;&#26356;&#22909;&#22320;&#36981;&#24490;&#29616;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;FollowIR&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#24736;&#20037;&#21382;&#21490;&#65306;&#27491;&#22914;TREC&#20026;&#20154;&#31867;&#26631;&#27880;&#21592;&#25552;&#20379;&#35828;&#26126;&#20070;&#65288;&#20063;&#31216;&#20026;&#21465;&#36848;&#65289;&#26469;&#21028;&#26029;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#19968;&#26679;&#65292;&#22240;&#27492;IR&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#36825;&#20123;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#30830;&#23450;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20934;&#20174;&#19977;&#20010;&#32463;&#36807;&#28145;&#24230;&#21028;&#26029;&#30340;TREC&#25910;&#34255;&#24320;&#22987;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15246v1 Announce Type: cross  Abstract: Modern Large Language Models (LLMs) are capable of following long and complex instructions that enable a diverse amount of user tasks. However, despite Information Retrieval (IR) models using LLMs as the backbone of their architectures, nearly all of them still only take queries as input, with no instructions. For the handful of recent models that do take instructions, it's unclear how they use them. We introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR builds off the long history of the TREC conferences: as TREC provides human annotators with instructions (also known as narratives) to determine document relevance, so should IR models be able to understand and decide relevance based on these detailed instructions. Our evaluation benchmark starts with three deeply judged TREC collections and al
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29702;&#27169;&#22359;STATM&#65292;&#21033;&#29992;&#35760;&#24518;&#32531;&#20914;&#21306;&#22686;&#24378;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.15245</link><description>&lt;p&gt;
&#35270;&#39057;&#30340;&#22686;&#24378;&#25512;&#29702;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reasoning-Enhanced Object-Centric Learning for Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15245
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29702;&#27169;&#22359;STATM&#65292;&#21033;&#29992;&#35760;&#24518;&#32531;&#20914;&#21306;&#22686;&#24378;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#26088;&#22312;&#23558;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#20998;&#35299;&#20026;&#26356;&#26131;&#22788;&#29702;&#30340;&#29289;&#20307;&#34920;&#31034;&#65292;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#29289;&#29702;&#19990;&#30028;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#27133;&#20301;&#30340;&#35270;&#39057;&#27169;&#22411;&#23637;&#29616;&#20986;&#22312;&#20998;&#21106;&#21644;&#36319;&#36394;&#29289;&#20307;&#26041;&#38754;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#24573;&#35270;&#20102;&#26377;&#25928;&#25512;&#29702;&#27169;&#22359;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#35760;&#24518;&#32531;&#20914;&#21306;&#30340;&#22522;&#20110;&#27133;&#20301;&#30340;&#26102;&#31354;&#21464;&#25442;&#22120;&#65288;STATM&#65289;&#30340;&#26032;&#22411;&#25512;&#29702;&#27169;&#22359;&#12290;&#35760;&#24518;&#32531;&#20914;&#21306;&#20027;&#35201;&#29992;&#20110;&#23384;&#20648;&#26469;&#33258;&#19978;&#28216;&#27169;&#22359;&#30340;&#27133;&#20301;&#20449;&#24687;&#65292;&#22522;&#20110;&#27133;&#20301;&#30340;&#26102;&#31354;&#21464;&#25442;&#22120;&#36890;&#36807;&#27133;&#20301;&#20026;&#22522;&#30784;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15245v1 Announce Type: cross  Abstract: Object-centric learning aims to break down complex visual scenes into more manageable object representations, enhancing the understanding and reasoning abilities of machine learning systems toward the physical world. Recently, slot-based video models have demonstrated remarkable proficiency in segmenting and tracking objects, but they overlook the importance of the effective reasoning module. In the real world, reasoning and predictive abilities play a crucial role in human perception and object tracking; in particular, these abilities are closely related to human intuitive physics. Inspired by this, we designed a novel reasoning module called the Slot-based Time-Space Transformer with Memory buffer (STATM) to enhance the model's perception ability in complex scenes. The memory buffer primarily serves as storage for slot information from upstream modules, the Slot-based Time-Space Transformer makes predictions through slot-based spatio
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#25311;&#29275;&#39039;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#22343;&#21248;&#24179;&#28369;&#24230;&#30340;&#24773;&#20917;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#24341;&#20837;&#20102;$(L_0, L_1)$-&#24179;&#28369;&#24230;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;$L$-&#24179;&#28369;&#24230;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#24179;&#28369;&#24230;&#19982;&#26799;&#24230;&#33539;&#25968;&#20043;&#38388;&#30340;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.15244</link><description>&lt;p&gt;
&#38750;&#20984;&#20248;&#21270;&#30340;&#38543;&#26426;&#25311;&#29275;&#39039;&#26041;&#27861;&#19982;&#38750;&#22343;&#21248;&#24179;&#28369;&#24230;
&lt;/p&gt;
&lt;p&gt;
A Stochastic Quasi-Newton Method for Non-convex Optimization with Non-uniform Smoothness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15244
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#25311;&#29275;&#39039;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#22343;&#21248;&#24179;&#28369;&#24230;&#30340;&#24773;&#20917;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#24341;&#20837;&#20102;$(L_0, L_1)$-&#24179;&#28369;&#24230;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;$L$-&#24179;&#28369;&#24230;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#24179;&#28369;&#24230;&#19982;&#26799;&#24230;&#33539;&#25968;&#20043;&#38388;&#30340;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#20248;&#21270;&#31639;&#27861;&#30340;&#32463;&#20856;&#25910;&#25947;&#20998;&#26512;&#20381;&#36182;&#20110;&#24191;&#27867;&#37319;&#29992;&#30340;&#22343;&#21248;&#24179;&#28369;&#24230;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#34920;&#29616;&#20986;&#38750;&#22343;&#21248;&#24179;&#28369;&#24230;&#65292;&#36825;&#24847;&#21619;&#30528;&#24179;&#28369;&#24230;&#22240;&#23376;&#26159;&#27169;&#22411;&#21442;&#25968;&#30340;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#26222;&#36941;&#24120;&#25968;&#12290;&#23588;&#20854;&#26159;&#35266;&#23519;&#21040;&#65292;&#24179;&#28369;&#24230;&#38543;&#30528;&#35757;&#32451;&#36712;&#36857;&#20013;&#30340;&#26799;&#24230;&#33539;&#25968;&#22686;&#38271;&#12290;&#21463;&#36825;&#19968;&#29616;&#35937;&#30340;&#21551;&#21457;&#65292;&#26368;&#36817;&#24341;&#20837;&#30340;$(L_0, L_1)$-&#24179;&#28369;&#24230;&#26159;&#19968;&#20010;&#27604;&#20256;&#32479;&#30340;$L$-&#24179;&#28369;&#24230;&#26356;&#19968;&#33324;&#30340;&#27010;&#24565;&#65292;&#23427;&#25429;&#25417;&#20102;&#24179;&#28369;&#24230;&#19982;&#26799;&#24230;&#33539;&#25968;&#20043;&#38388;&#30340;&#36825;&#31181;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;&#22312;&#36825;&#31181;&#38750;&#22343;&#21248;&#24179;&#28369;&#24230;&#19979;&#65292;&#29616;&#26377;&#25991;&#29486;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#35009;&#21098;&#25216;&#26415;&#35774;&#35745;&#20102;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#65292;&#20197;&#33719;&#24471;&#25214;&#21040;$\epsilon$-&#36817;&#20284;&#35299;&#30340;$\mathcal{O}(\epsilon^{-3})$&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15244v1 Announce Type: new  Abstract: Classical convergence analyses for optimization algorithms rely on the widely-adopted uniform smoothness assumption. However, recent experimental studies have demonstrated that many machine learning problems exhibit non-uniform smoothness, meaning the smoothness factor is a function of the model parameter instead of a universal constant. In particular, it has been observed that the smoothness grows with respect to the gradient norm along the training trajectory. Motivated by this phenomenon, the recently introduced $(L_0, L_1)$-smoothness is a more general notion, compared to traditional $L$-smoothness, that captures such positive relationship between smoothness and gradient norm. Under this type of non-uniform smoothness, existing literature has designed stochastic first-order algorithms by utilizing gradient clipping techniques to obtain the optimal $\mathcal{O}(\epsilon^{-3})$ sample complexity for finding an $\epsilon$-approximate fi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#21644;&#29616;&#23454;&#35774;&#32622;&#19979;&#30340;&#31283;&#20581;&#25928;&#29992;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22312;&#27809;&#26377;&#24050;&#30693;&#26368;&#20339;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#32988;&#36807;&#25152;&#26377;&#20854;&#20182;&#21442;&#32771;&#31574;&#30053;</title><link>https://arxiv.org/abs/2403.15243</link><description>&lt;p&gt;
&#36890;&#36807;GAN&#26041;&#27861;&#23454;&#29616;&#31283;&#20581;&#25928;&#29992;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Utility Optimization via a GAN Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15243
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#21644;&#29616;&#23454;&#35774;&#32622;&#19979;&#30340;&#31283;&#20581;&#25928;&#29992;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22312;&#27809;&#26377;&#24050;&#30693;&#26368;&#20339;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#32988;&#36807;&#25152;&#26377;&#20854;&#20182;&#21442;&#32771;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#20581;&#25928;&#29992;&#20248;&#21270;&#20351;&#25237;&#36164;&#32773;&#33021;&#22815;&#20197;&#32467;&#26500;&#21270;&#26041;&#24335;&#22788;&#29702;&#24066;&#22330;&#19981;&#30830;&#23450;&#24615;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#26368;&#22351;&#24773;&#20917;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26041;&#27861;&#65292;&#65288;&#36817;&#20284;&#22320;&#65289;&#35299;&#20915;&#19968;&#33324;&#21644;&#29616;&#23454;&#35774;&#32622;&#19979;&#30340;&#31283;&#20581;&#25928;&#29992;&#20248;&#21270;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#23545;&#25237;&#36164;&#32773;&#21644;&#24066;&#22330;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#22312;&#26497;&#23567;&#26497;&#22823;&#38646;&#21644;&#21338;&#24328;&#20013;&#35757;&#32451;&#23427;&#20204;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#36830;&#32493;&#25928;&#29992;&#20989;&#25968;&#65292;&#24182;&#22312;&#20855;&#26377;&#20132;&#26131;&#25104;&#26412;&#30340;&#29616;&#23454;&#24066;&#22330;&#35774;&#32622;&#20013;&#65292;&#21482;&#33021;&#20351;&#29992;&#24066;&#22330;&#30340;&#21487;&#35266;&#23519;&#20449;&#24687;&#12290;&#22823;&#37327;&#23454;&#35777;&#30740;&#31350;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#27599;&#24403;&#23384;&#22312;&#26368;&#20339;&#21442;&#32771;&#31574;&#30053;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37117;&#33021;&#19982;&#20043;&#23218;&#32654;&#65292;&#22312;&#27809;&#26377;&#24050;&#30693;&#26368;&#20339;&#31574;&#30053;&#30340;&#65288;&#35768;&#22810;&#65289;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#25152;&#26377;&#20854;&#20182;&#21442;&#32771;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#30740;&#31350;&#20013;&#24471;&#20986;&#32467;&#35770;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15243v1 Announce Type: cross  Abstract: Robust utility optimization enables an investor to deal with market uncertainty in a structured way, with the goal of maximizing the worst-case outcome. In this work, we propose a generative adversarial network (GAN) approach to (approximately) solve robust utility optimization problems in general and realistic settings. In particular, we model both the investor and the market by neural networks (NN) and train them in a mini-max zero-sum game. This approach is applicable for any continuous utility function and in realistic market settings with trading costs, where only observable information of the market can be used. A large empirical study shows the versatile usability of our method. Whenever an optimal reference strategy is available, our method performs on par with it and in the (many) settings without known optimal strategy, our method outperforms all other reference strategies. Moreover, we can conclude from our study that the tr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#28436;&#31034;&#23398;&#20064;&#38598;&#25104;&#21040;&#36816;&#21160;&#29983;&#25104;&#20013;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23454;&#26102;&#29983;&#25104;&#36866;&#24212;&#22797;&#26434;&#29615;&#22659;&#30340;&#36816;&#21160;</title><link>https://arxiv.org/abs/2403.15239</link><description>&lt;p&gt;
&#24341;&#23548;&#35299;&#30721;&#29992;&#20110;&#26426;&#22120;&#20154;&#36816;&#21160;&#29983;&#25104;&#21644;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Guided Decoding for Robot Motion Generation and Adaption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15239
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#28436;&#31034;&#23398;&#20064;&#38598;&#25104;&#21040;&#36816;&#21160;&#29983;&#25104;&#20013;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23454;&#26102;&#29983;&#25104;&#36866;&#24212;&#22797;&#26434;&#29615;&#22659;&#30340;&#36816;&#21160;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#20855;&#26377;&#38556;&#30861;&#29289;&#12289;&#36890;&#36807;&#28857;&#31561;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#39640;&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#33218;&#36816;&#21160;&#29983;&#25104;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#36890;&#36807;&#23558;&#28436;&#31034;&#23398;&#20064;&#65288;LfD&#65289;&#38598;&#25104;&#21040;&#36816;&#21160;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#21462;&#24471;&#20102;&#35813;&#39046;&#22495;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#31181;&#38598;&#25104;&#25903;&#25345;&#26426;&#22120;&#20154;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20801;&#35768;&#26426;&#22120;&#20154;&#20174;&#28436;&#31034;&#36712;&#36857;&#20013;&#23398;&#20064;&#21644;&#27867;&#21270;&#26469;&#20248;&#21270;&#31215;&#32047;&#30340;&#32463;&#39564;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#27169;&#25311;&#36712;&#36857;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21464;&#25442;&#22120;&#30340;transformer&#26550;&#26500;&#12290;&#36825;&#31181;&#22522;&#20110;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21464;&#25442;&#22120;&#30340;&#26550;&#26500;&#23398;&#20064;&#20102;&#22522;&#26412;&#30340;&#36816;&#21160;&#29983;&#25104;&#25216;&#33021;&#65292;&#24182;&#23558;&#20854;&#35843;&#25972;&#20197;&#28385;&#36275;&#36741;&#21161;&#20219;&#21153;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#33258;&#22238;&#24402;&#26041;&#27861;&#23454;&#29616;&#20102;&#29289;&#29702;&#31995;&#32479;&#21453;&#39304;&#30340;&#23454;&#26102;&#38598;&#25104;&#65292;&#22686;&#24378;&#20102;&#36816;&#21160;&#29983;&#25104;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20174;&#21021;&#22987;&#28857;&#21644;&#30446;&#26631;&#28857;&#29983;&#25104;&#36816;&#21160;&#65292;&#21516;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15239v1 Announce Type: cross  Abstract: We address motion generation for high-DoF robot arms in complex settings with obstacles, via points, etc. A significant advancement in this domain is achieved by integrating Learning from Demonstration (LfD) into the motion generation process. This integration facilitates rapid adaptation to new tasks and optimizes the utilization of accumulated expertise by allowing robots to learn and generalize from demonstrated trajectories.   We train a transformer architecture on a large dataset of simulated trajectories. This architecture, based on a conditional variational autoencoder transformer, learns essential motion generation skills and adapts these to meet auxiliary tasks and constraints. Our auto-regressive approach enables real-time integration of feedback from the physical system, enhancing the adaptability and efficiency of motion generation. We show that our model can generate motion from initial and target points, but also that it 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20195;&#30721;&#35768;&#21487;&#20405;&#26435;&#38382;&#39064;&#65292;&#21457;&#29616;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#23384;&#22312;&#35768;&#21487;&#19981;&#19968;&#33268;&#24615;&#65292;&#23613;&#31649;&#23427;&#20204;&#26159;&#22522;&#20110;&#30456;&#20851;&#20195;&#30721;&#20179;&#24211;&#35768;&#21487;&#35777;&#26469;&#36873;&#25321;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.15230</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23545;&#20195;&#30721;&#35768;&#21487;&#20405;&#26435;&#30340;&#21021;&#27493;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20195;&#30721;&#35768;&#21487;&#20405;&#26435;&#38382;&#39064;&#65292;&#21457;&#29616;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#23384;&#22312;&#35768;&#21487;&#19981;&#19968;&#33268;&#24615;&#65292;&#23613;&#31649;&#23427;&#20204;&#26159;&#22522;&#20110;&#30456;&#20851;&#20195;&#30721;&#20179;&#24211;&#35768;&#21487;&#35777;&#26469;&#36873;&#25321;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15230v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#28508;&#22312;&#20405;&#29359;&#20195;&#30721;&#35768;&#21487;&#65311;&#27492;&#22806;&#65292;&#26159;&#21542;&#26377;&#21487;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#32780;&#19981;&#36829;&#21453;&#36825;&#20123;&#35768;&#21487;&#30340;&#25968;&#25454;&#38598;&#65311;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#36235;&#21183;&#20197;&#21450;&#23558;&#20195;&#30721;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#26597;&#30475;&#26159;&#21542;&#21487;&#20197;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#32780;&#19981;&#20250;&#38754;&#20020;&#26410;&#26469;&#30340;&#27861;&#24459;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;53&#20010;&#22522;&#20110;&#25991;&#20214;&#32423;&#20195;&#30721;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21015;&#34920;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#21462;&#20102;&#23427;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#19982;&#25105;&#20204;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#37325;&#21472;&#30340;&#31243;&#24230;&#65292;&#21518;&#32773;&#20165;&#21253;&#21547;&#24378;&#21046;&#20849;&#20139;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15230v1 Announce Type: cross  Abstract: Does the training of large language models potentially infringe upon code licenses? Furthermore, are there any datasets available that can be safely used for training these models without violating such licenses? In our study, we assess the current trends in the field and the importance of incorporating code into the training of large language models. Additionally, we examine publicly available datasets to see whether these models can be trained on them without the risk of legal issues in the future. To accomplish this, we compiled a list of 53 large language models trained on file-level code. We then extracted their datasets and analyzed how much they overlap with a dataset we created, consisting exclusively of strong copyleft code.   Our analysis revealed that every dataset we examined contained license inconsistencies, despite being selected based on their associated repository licenses. We analyzed a total of 514 million code files
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#37096;&#20998;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#22312;&#20247;&#21253;&#29615;&#22659;&#20013;&#20174;&#38750;&#19987;&#23478;&#22788;&#31574;&#21010;&#21307;&#23398;&#22270;&#20687;&#26631;&#27880;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;3D DL&#20998;&#21106;&#27169;&#22411;&#30340;"&#23494;&#38598;"&#20998;&#21106;&#25513;&#27169;&#12290;</title><link>https://arxiv.org/abs/2403.15218</link><description>&lt;p&gt;
&#26080;&#35770;&#20309;&#26102;&#12289;&#20309;&#22320;&#12289;&#35841;&#20154;&#65306;&#30740;&#31350;&#29992;&#20110;&#20247;&#21253;&#21307;&#23398;&#22270;&#20687;&#26631;&#27880;&#30340;&#37096;&#20998;&#20219;&#24847;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15218
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#37096;&#20998;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#22312;&#20247;&#21253;&#29615;&#22659;&#20013;&#20174;&#38750;&#19987;&#23478;&#22788;&#31574;&#21010;&#21307;&#23398;&#22270;&#20687;&#26631;&#27880;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;3D DL&#20998;&#21106;&#27169;&#22411;&#30340;"&#23494;&#38598;"&#20998;&#21106;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27880;&#37322;&#30340;&#31574;&#21010;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#23548;&#33268;"&#29421;&#31364;"&#19987;&#27880;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20855;&#26377;&#26377;&#38480;&#30340;&#36716;&#21270;&#25928;&#29992;&#12290;&#26368;&#36817;&#65292;&#20687;&#37096;&#20998;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#36825;&#26679;&#30340;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#20041;&#20998;&#21106;&#65292;&#36328;&#21508;&#20010;&#39046;&#22495;&#21253;&#25324;&#21307;&#23398;&#25104;&#20687;&#65292;&#23545;&#20110;&#31616;&#21270;&#27880;&#37322;&#36807;&#31243;&#26377;&#24456;&#22823;&#24076;&#26395;&#12290;&#28982;&#32780;&#65292;SAM&#23578;&#26410;&#22312;&#20247;&#21253;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#31574;&#21010;&#27880;&#37322;&#26469;&#35757;&#32451;3D DL&#20998;&#21106;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;SAM&#29992;&#20110;&#20174;&#38750;&#19987;&#19994;&#20154;&#21592;&#20013;&#20247;&#21253;"&#31232;&#30095;"&#27880;&#37322;&#65292;&#20135;&#29983;&#29992;&#20110;&#35757;&#32451;3D nnU-Net&#27169;&#22411;&#65288;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;DL&#20998;&#21106;&#27169;&#22411;&#65289;&#30340;"&#23494;&#38598;"&#20998;&#21106;&#25513;&#27169;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22320;&#38754;&#30495;&#23454;&#24230;&#30456;&#27604;&#65292;SAM&#29983;&#25104;&#30340;&#27880;&#37322;&#23637;&#29616;&#20986;&#39640;&#24179;&#22343;Dice&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15218v1 Announce Type: cross  Abstract: Curating annotations for medical image segmentation is a labor-intensive and time-consuming task that requires domain expertise, resulting in "narrowly" focused deep learning (DL) models with limited translational utility. Recently, foundation models like the Segment Anything Model (SAM) have revolutionized semantic segmentation with exceptional zero-shot generalizability across various domains, including medical imaging, and hold a lot of promise for streamlining the annotation process. However, SAM has yet to be evaluated in a crowd-sourced setting to curate annotations for training 3D DL segmentation models. In this work, we explore the potential of SAM for crowd-sourcing "sparse" annotations from non-experts to generate "dense" segmentation masks for training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our results indicate that while SAM-generated annotations exhibit high mean Dice scores compared to ground-truth a
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#30340;&#24433;&#21709;&#23545;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#25506;&#31350;&#23398;&#20064;&#21160;&#24577;&#21644;&#29992;&#20110;&#35843;&#26597;&#30340;&#28176;&#36827;&#35299;&#20923;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15210</link><description>&lt;p&gt;
&#35757;&#32451;&#26089;&#26399;&#24433;&#21709;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Early Period of Training Impacts Out-of-Distribution Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15210
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#30340;&#24433;&#21709;&#23545;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#25506;&#31350;&#23398;&#20064;&#21160;&#24577;&#21644;&#29992;&#20110;&#35843;&#26597;&#30340;&#28176;&#36827;&#35299;&#20923;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#30340;&#24046;&#24322;&#26174;&#33879;&#24433;&#21709;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#23545;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#25935;&#24863;&#65292;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#21464;&#24471;&#19981;&#22826;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#21644;&#32570;&#20047;&#26377;&#25928;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#23545;OOD&#27867;&#21270;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#21160;&#24577;&#21644;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#30340;OOD&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21033;&#29992;Fisher&#20449;&#24687;&#30340;&#30165;&#36857;&#21644;&#38160;&#21033;&#24230;&#65292;&#37325;&#28857;&#20851;&#27880;&#28176;&#36827;&#35299;&#20923;&#65288;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#35299;&#20923;&#21442;&#25968;&#65289;&#20316;&#20026;&#30740;&#31350;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;1&#65289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36873;&#25321;&#19981;&#21516;&#26102;&#38388;&#28857;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#21363;&#36880;&#28176;&#35299;&#20923;&#30340;&#23454;&#29616;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15210v1 Announce Type: new  Abstract: Prior research has found that differences in the early period of neural network training significantly impact the performance of in-distribution (ID) tasks. However, neural networks are often sensitive to out-of-distribution (OOD) data, making them less reliable in downstream applications. Yet, the impact of the early training period on OOD generalization remains understudied due to its complexity and lack of effective analytical methodologies. In this work, we investigate the relationship between learning dynamics and OOD generalization during the early period of neural network training. We utilize the trace of Fisher Information and sharpness, with a focus on gradual unfreezing (i.e. progressively unfreezing parameters during training) as the methodology for investigation. Through a series of empirical experiments, we show that 1) selecting the number of trainable parameters at different times during training, i.e. realized by gradual 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#25239;&#23398;&#20064;&#30340;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#26032;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26367;&#20195;&#25439;&#22833;&#26469;&#23454;&#29616;&#65292;&#21516;&#26102;&#22312;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#27169;&#22411;&#19978;&#25512;&#23548;&#20986;&#20102;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.15207</link><description>&lt;p&gt;
&#23545;&#25239;&#23398;&#20064;&#30340;&#40065;&#26834;&#20248;&#21270;&#19982;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Robust optimization for adversarial learning with finite sample complexity guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#25239;&#23398;&#20064;&#30340;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#26032;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26367;&#20195;&#25439;&#22833;&#26469;&#23454;&#29616;&#65292;&#21516;&#26102;&#22312;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#27169;&#22411;&#19978;&#25512;&#23548;&#20986;&#20102;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#20915;&#31574;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#36234;&#26469;&#36234;&#38656;&#35201;&#23454;&#29616;&#40065;&#26834;&#21644;&#21487;&#38752;&#30340;&#25805;&#20316;&#12290;&#24403;&#19981;&#30830;&#23450;&#24615;&#26469;&#33258;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#65292;&#36825;&#31181;&#38656;&#27714;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#24378;&#22823;&#30340;&#20998;&#31867;&#22120;&#65292;&#28789;&#24863;&#26469;&#33258;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#35282;&#24230;&#30475;&#24453;&#40065;&#26834;&#24615;&#65292;&#24182;&#38024;&#23545;&#20108;&#20803;&#21644;&#22810;&#31867;&#22330;&#26223;&#25512;&#23548;&#20986;&#20102;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#19982;&#33258;&#28982;&#20998;&#31867;&#22120;&#30340;&#22797;&#26434;&#24615;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#32447;&#24615;&#35268;&#21010;&#65288;LP&#65289;&#21644;&#20108;&#38454;&#38181;&#35268;&#21010;&#65288;SOCP&#65289;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#30340;&#26367;&#20195;&#25439;&#22833;&#65292;&#36866;&#29992;&#20110;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#27169;&#22411;&#12290;&#22312;&#22522;&#20934;MNIST&#21644;CIFAR10&#25968;&#25454;&#38598;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15207v1 Announce Type: new  Abstract: Decision making and learning in the presence of uncertainty has attracted significant attention in view of the increasing need to achieve robust and reliable operations. In the case where uncertainty stems from the presence of adversarial attacks this need is becoming more prominent. In this paper we focus on linear and nonlinear classification problems and propose a novel adversarial training method for robust classifiers, inspired by Support Vector Machine (SVM) margins. We view robustness under a data driven lens, and derive finite sample complexity bounds for both linear and non-linear classifiers in binary and multi-class scenarios. Notably, our bounds match natural classifiers' complexity. Our algorithm minimizes a worst-case surrogate loss using Linear Programming (LP) and Second Order Cone Programming (SOCP) for linear and non-linear models. Numerical experiments on the benchmark MNIST and CIFAR10 datasets show our approach's com
&lt;/p&gt;</description></item><item><title>FSD-Inference&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#26080;&#26381;&#21153;&#22120;&#19988;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;ML&#25512;&#26029;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#20840;&#26032;&#30340;&#26080;&#26381;&#21153;&#22120;&#36890;&#20449;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.15195</link><description>&lt;p&gt;
FSD-Inference: &#20855;&#26377;&#21487;&#25193;&#23637;&#20113;&#36890;&#20449;&#30340;&#23436;&#20840;&#26080;&#26381;&#21153;&#22120;&#20998;&#24067;&#24335;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
FSD-Inference: Fully Serverless Distributed Inference with Scalable Cloud Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15195
&lt;/p&gt;
&lt;p&gt;
FSD-Inference&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#26080;&#26381;&#21153;&#22120;&#19988;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;ML&#25512;&#26029;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#20840;&#26032;&#30340;&#26080;&#26381;&#21153;&#22120;&#36890;&#20449;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15195v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25277;&#35937;: &#26080;&#26381;&#21153;&#22120;&#35745;&#31639;&#25552;&#20379;&#20102;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#21487;&#20280;&#32553;&#24615;&#12289;&#24377;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;&#28982;&#32780;&#65292;&#23545;&#20869;&#23384;&#12289;CPU&#21644;&#20989;&#25968;&#36816;&#34892;&#26102;&#38388;&#30340;&#38480;&#21046;&#38459;&#30861;&#20102;&#20854;&#22312;&#25968;&#25454;&#23494;&#38598;&#22411;&#24212;&#29992;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24037;&#20316;&#36127;&#36733;&#20013;&#30340;&#24212;&#29992;&#12290;&#20256;&#32479;&#30340;&#8220;&#20840;&#26381;&#21153;&#22120;&#8221;&#24179;&#21488;&#36890;&#36807;&#24555;&#36895;&#32593;&#32476;&#21644;&#24050;&#24314;&#31435;&#33391;&#22909;&#30340;&#36827;&#31243;&#38388;&#36890;&#20449;&#65288;IPC&#65289;&#26426;&#21046;&#65288;&#22914;MPI&#21644;&#20849;&#20139;&#20869;&#23384;&#65289;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;&#22312;&#26080;&#26381;&#21153;&#22120;&#39046;&#22495;&#32570;&#20047;&#27492;&#31867;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#37325;&#35201;IPC&#35201;&#27714;&#30340;&#24182;&#34892;&#35745;&#31639;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;FSD-Inference&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#26080;&#26381;&#21153;&#22120;&#19988;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;ML&#25512;&#26029;&#31995;&#32479;&#12290;&#25105;&#20204;&#25506;&#35752;&#28508;&#22312;&#30340;&#36890;&#20449;&#28192;&#36947;&#65292;&#19982;&#20989;&#25968;&#21363;&#26381;&#21153;&#65288;FaaS&#65289;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#20026;&#26080;&#26381;&#21153;&#22120;&#25968;&#25454;&#23494;&#38598;&#22411;&#35745;&#31639;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;ML&#35774;&#35745;&#20102;&#19968;&#27969;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;ML&#25512;&#26029;&#30340;&#20840;&#26032;&#26080;&#26381;&#21153;&#22120;&#36890;&#20449;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15195v1 Announce Type: cross  Abstract: Serverless computing offers attractive scalability, elasticity and cost-effectiveness. However, constraints on memory, CPU and function runtime have hindered its adoption for data-intensive applications and machine learning (ML) workloads. Traditional 'server-ful' platforms enable distributed computation via fast networks and well-established inter-process communication (IPC) mechanisms such as MPI and shared memory. In the absence of such solutions in the serverless domain, parallel computation with significant IPC requirements is challenging. We present FSD-Inference, the first fully serverless and highly scalable system for distributed ML inference. We explore potential communication channels, in conjunction with Function-as-a-Service (FaaS) compute, to design a state-of-the-art solution for distributed ML within the context of serverless data-intensive computing. We introduce novel fully serverless communication schemes for ML infe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Differentiable Augmentation Search (DAS) &#26041;&#27861;&#65292;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#21487;&#22788;&#29702;&#20026;&#35270;&#39057;&#30340;&#22270;&#20687;&#21464;&#20307;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#25968;&#25454;&#30340;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.15194</link><description>&lt;p&gt;
&#20320;&#30340;&#22270;&#20687;&#23601;&#26159;&#25105;&#30340;&#35270;&#39057;&#65306;&#36890;&#36807;&#22270;&#20687;&#21040;&#35270;&#39057;&#21487;&#24494;&#20998;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#21644;&#34701;&#21512;&#37325;&#26032;&#22609;&#36896;&#24863;&#21463;&#37326;
&lt;/p&gt;
&lt;p&gt;
Your Image is My Video: Reshaping the Receptive Field via Image-To-Video Differentiable AutoAugmentation and Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15194
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Differentiable Augmentation Search (DAS) &#26041;&#27861;&#65292;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#21487;&#22788;&#29702;&#20026;&#35270;&#39057;&#30340;&#22270;&#20687;&#21464;&#20307;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#25968;&#25454;&#30340;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#39046;&#22495;&#27491;&#26397;&#30528;&#21033;&#29992;&#25968;&#25454;&#30495;&#27491;&#28508;&#21147;&#30340;&#21019;&#26032;&#31574;&#30053;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#26469;&#20805;&#20998;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#21487;&#24494;&#20998;&#22686;&#24378;&#25628;&#32034;&#26041;&#27861;&#65288;DAS&#65289;&#26469;&#29983;&#25104;&#22270;&#20687;&#30340;&#21464;&#20307;&#65292;&#36825;&#20123;&#22270;&#20687;&#21487;&#20197;&#34987;&#22788;&#29702;&#20026;&#35270;&#39057;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#30456;&#27604;&#65292;DAS &#38750;&#24120;&#24555;&#36895;&#21644;&#28789;&#27963;&#65292;&#33021;&#22815;&#22312;&#19981;&#21040;&#19968;&#22825;&#30340; GPU &#35745;&#31639;&#26102;&#38388;&#20869;&#25628;&#32034;&#38750;&#24120;&#24222;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#30452;&#35273;&#26159;&#22686;&#22823;&#24863;&#21463;&#37326;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15194v1 Announce Type: cross  Abstract: The landscape of deep learning research is moving towards innovative strategies to harness the true potential of data. Traditionally, emphasis has been on scaling model architectures, resulting in large and complex neural networks, which can be difficult to train with limited computational resources. However, independently of the model size, data quality (i.e. amount and variability) is still a major factor that affects model generalization. In this work, we propose a novel technique to exploit available data through the use of automatic data augmentation for the tasks of image classification and semantic segmentation. We introduce the first Differentiable Augmentation Search method (DAS) to generate variations of images that can be processed as videos. Compared to previous approaches, DAS is extremely fast and flexible, allowing the search on very large search spaces in less than a GPU day. Our intuition is that the increased receptiv
&lt;/p&gt;</description></item><item><title>PDE-CNNs&#36890;&#36807;&#21033;&#29992;&#20960;&#20309;&#24847;&#20041;&#30340;&#28436;&#21270;PDE&#30340;&#27714;&#35299;&#22120;&#26367;&#20195;&#20256;&#32479;&#30340;&#32452;&#20214;&#65292;&#25552;&#20379;&#20102;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#22266;&#26377;&#30340;&#31561;&#21464;&#24615;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#20960;&#20309;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15182</link><description>&lt;p&gt;
PDE-CNNs&#65306;&#20844;&#29702;&#25512;&#23548;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PDE-CNNs: Axiomatic Derivations and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15182
&lt;/p&gt;
&lt;p&gt;
PDE-CNNs&#36890;&#36807;&#21033;&#29992;&#20960;&#20309;&#24847;&#20041;&#30340;&#28436;&#21270;PDE&#30340;&#27714;&#35299;&#22120;&#26367;&#20195;&#20256;&#32479;&#30340;&#32452;&#20214;&#65292;&#25552;&#20379;&#20102;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#22266;&#26377;&#30340;&#31561;&#21464;&#24615;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#20960;&#20309;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#32452;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PDE-G-CNNs&#65289;&#21033;&#29992;&#20855;&#26377;&#20960;&#20309;&#24847;&#20041;&#30340;&#28436;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#27714;&#35299;&#22120;&#26367;&#20195;G-CNNs&#20013;&#24120;&#35268;&#32452;&#20214;&#12290;PDE-G-CNNs&#21516;&#26102;&#25552;&#20379;&#20102;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#22266;&#26377;&#31561;&#21464;&#24615;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#20960;&#20309;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#29305;&#24449;&#22270;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#20026;&#20108;&#32500;&#30340;&#27431;&#20960;&#37324;&#24503;&#31561;&#21464;PDE-G-CNNs&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#30340;&#21464;&#20307;&#31216;&#20026;PDE-CNN&#12290;&#25105;&#20204;&#21015;&#20986;&#20102;&#20960;&#20010;&#22312;&#23454;&#36341;&#20013;&#20196;&#20154;&#28385;&#24847;&#30340;&#20844;&#29702;&#65292;&#24182;&#20174;&#20013;&#25512;&#23548;&#20986;&#24212;&#22312;PDE-CNN&#20013;&#20351;&#29992;&#21738;&#20123;PDE&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#20856;&#32447;&#24615;&#21644;&#24418;&#24577;&#23610;&#24230;&#31354;&#38388;&#29702;&#35770;&#30340;&#20844;&#29702;&#21463;&#21551;&#21457;&#65292;&#36890;&#36807;&#24341;&#20837;&#21322;&#22495;&#20540;&#20449;&#21495;&#23545;&#20854;&#36827;&#34892;&#25512;&#24191;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#30456;&#23545;&#20110;&#23567;&#22411;&#32593;&#32476;&#65292;PDE-CNN&#25552;&#20379;&#20102;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15182v1 Announce Type: new  Abstract: PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) utilize solvers of geometrically meaningful evolution PDEs as substitutes for the conventional components in G-CNNs. PDE-G-CNNs offer several key benefits all at once: fewer parameters, inherent equivariance, better performance, data efficiency, and geometric interpretability. In this article we focus on Euclidean equivariant PDE-G-CNNs where the feature maps are two dimensional throughout. We call this variant of the framework a PDE-CNN. We list several practically desirable axioms and derive from these which PDEs should be used in a PDE-CNN. Here our approach to geometric learning via PDEs is inspired by the axioms of classical linear and morphological scale-space theory, which we generalize by introducing semifield-valued signals. Furthermore, we experimentally confirm for small networks that PDE-CNNs offer fewer parameters, better performance, and data efficiency in compariso
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#24490;&#29615;&#24335;&#38543;&#26426;&#26463;&#25628;&#32034;&#21644;&#26469;&#33258;&#21487;&#35777;&#31574;&#30053;&#25913;&#36827;&#30340;&#26356;&#26032;&#31574;&#30053;&#65292;&#26412;&#30740;&#31350;&#22312;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#26368;&#23567;&#37319;&#26679;&#27425;&#25968;&#20013;&#23454;&#29616;&#36880;&#27493;&#25913;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.15180</link><description>&lt;p&gt;
&#33258;&#25105;&#25913;&#36827;&#29992;&#20110;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65306;&#26080;&#38656;&#26367;&#25442;&#36827;&#34892;&#37319;&#26679;&#65292;&#20294;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Self-Improvement for Neural Combinatorial Optimization: Sample without Replacement, but Improvement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15180
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#24490;&#29615;&#24335;&#38543;&#26426;&#26463;&#25628;&#32034;&#21644;&#26469;&#33258;&#21487;&#35777;&#31574;&#30053;&#25913;&#36827;&#30340;&#26356;&#26032;&#31574;&#30053;&#65292;&#26412;&#30740;&#31350;&#22312;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#26368;&#23567;&#37319;&#26679;&#27425;&#25968;&#20013;&#23454;&#29616;&#36880;&#27493;&#25913;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23545;&#20110;&#31471;&#21040;&#31471;&#30340;&#26500;&#36896;&#24615;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#26041;&#27861;&#36890;&#24120;&#26159;&#20351;&#29992;&#34892;&#20026;&#20811;&#38534;&#26469;&#35757;&#32451;&#31574;&#30053;&#65292;&#20174;&#19987;&#23478;&#35299;&#20915;&#26041;&#26696;&#20013;&#25110;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#20174;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#34892;&#20026;&#20811;&#38534;&#26041;&#27861;&#24456;&#30452;&#25509;&#65292;&#20294;&#38656;&#35201;&#26114;&#36149;&#30340;&#19987;&#23478;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#24448;&#24448;&#35745;&#31639;&#35201;&#27714;&#24456;&#39640;&#65292;&#38590;&#20197;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26725;&#25509;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;&#27599;&#20010;&#32426;&#20803;&#20013;&#20351;&#29992;&#24403;&#21069;&#27169;&#22411;&#23545;&#38543;&#26426;&#23454;&#20363;&#36827;&#34892;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#30340;&#37319;&#26679;&#65292;&#28982;&#21518;&#36873;&#25321;&#26368;&#20339;&#35299;&#20316;&#20026;&#19987;&#23478;&#36712;&#36857;&#36827;&#34892;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;&#20026;&#20102;&#22312;&#26368;&#23567;&#37319;&#26679;&#27425;&#25968;&#20013;&#23454;&#29616;&#36880;&#27493;&#25913;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#24490;&#29615;&#24335;&#38543;&#26426;&#26463;&#25628;&#32034;&#19982;&#19968;&#31181;&#25512;&#23548;&#33258;&#21487;&#35777;&#31574;&#30053;&#25913;&#36827;&#30340;&#26356;&#26032;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#35813;&#31574;&#30053;&#36890;&#36807;&#21033;&#29992;&#20960;&#20046;&#27809;&#26377;&#36890;&#20449;&#24320;&#38144;&#30340;&#26679;&#26412;&#24207;&#21015;&#30340;&#20248;&#21183;&#65292;&#22312;&#36718;&#20043;&#38388;&#35843;&#25972;&#31574;&#30053;&#65292;&#20197;&#31934;&#32454;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15180v1 Announce Type: new  Abstract: Current methods for end-to-end constructive neural combinatorial optimization usually train a policy using behavior cloning from expert solutions or policy gradient methods from reinforcement learning. While behavior cloning is straightforward, it requires expensive expert solutions, and policy gradient methods are often computationally demanding and complex to fine-tune. In this work, we bridge the two and simplify the training process by sampling multiple solutions for random instances using the current model in each epoch and then selecting the best solution as an expert trajectory for supervised imitation learning. To achieve progressively improving solutions with minimal sampling, we introduce a method that combines round-wise Stochastic Beam Search with an update strategy derived from a provable policy improvement. This strategy refines the policy between rounds by utilizing the advantage of the sampled sequences with almost no com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#26816;&#27979;&#20027;&#35201;&#25233;&#37057;&#30151;&#21644;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#26102;&#65292;&#22312;&#20132;&#20114;&#20250;&#35805;&#26399;&#38388;&#25910;&#38598;&#30340;&#38899;&#39057;&#21644;&#35270;&#39057;&#25968;&#25454;&#19978;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.15170</link><description>&lt;p&gt;
&#22312;&#26816;&#27979;&#24515;&#29702;&#30142;&#30149;&#30340;&#24773;&#22659;&#20013;&#25506;&#32034;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20219;&#21153;&#26080;&#20851;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Task-agnostic Trait of Self-supervised Learning in the Context of Detecting Mental Disorders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#26816;&#27979;&#20027;&#35201;&#25233;&#37057;&#30151;&#21644;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#26102;&#65292;&#22312;&#20132;&#20114;&#20250;&#35805;&#26399;&#38388;&#25910;&#38598;&#30340;&#38899;&#39057;&#21644;&#35270;&#39057;&#25968;&#25454;&#19978;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#34987;&#30740;&#31350;&#29992;&#20110;&#22312;&#21508;&#20010;&#39046;&#22495;&#29983;&#25104;&#20219;&#21153;&#26080;&#20851;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#23578;&#26410;&#26377;&#20154;&#25506;&#32034;&#29992;&#20110;&#26816;&#27979;&#22810;&#31181;&#24515;&#29702;&#38556;&#30861;&#30340;&#36825;&#31181;&#26041;&#27861;&#12290;&#23384;&#22312;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#30340;&#29702;&#30001;&#22312;&#20110;&#22810;&#31181;&#24515;&#29702;&#38556;&#30861;&#20043;&#38388;&#30340;&#30151;&#29366;&#37325;&#21472;&#12290;&#22240;&#27492;&#65292;&#25910;&#38598;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#35780;&#20272;&#30340;&#34892;&#20026;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#19982;&#22810;&#31181;&#38556;&#30861;&#30456;&#20851;&#30340;&#23646;&#24615;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;SSL&#25512;&#23548;&#20986;&#30340;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#65292;&#29992;&#20110;&#20351;&#29992;&#22312;&#20114;&#21160;&#20250;&#35805;&#26399;&#38388;&#25910;&#38598;&#30340;&#38899;&#39057;&#21644;&#35270;&#39057;&#25968;&#25454;&#26816;&#27979;&#37325;&#24230;&#25233;&#37057;&#38556;&#30861;&#65288;MDD&#65289;&#21644;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#65288;PTSD&#65289;&#30340;&#24773;&#22659;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#36890;&#36807;&#39044;&#27979;&#22810;&#20010;&#22266;&#23450;&#30446;&#26631;&#25110;&#25513;&#33180;&#24103;&#35757;&#32451;&#30340;SSL&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22266;&#23450;&#30446;&#26631;&#65292;&#20197;&#20351;&#29983;&#25104;&#30340;&#34920;&#31034;&#23545;MDD&#30340;&#26816;&#27979;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15170v1 Announce Type: cross  Abstract: Self-supervised learning (SSL) has been investigated to generate task-agnostic representations across various domains. However, such investigation has not been conducted for detecting multiple mental disorders. The rationale behind the existence of a task-agnostic representation lies in the overlapping symptoms among multiple mental disorders. Consequently, the behavioural data collected for mental health assessment may carry a mixed bag of attributes related to multiple disorders. Motivated by that, in this study, we explore a task-agnostic representation derived through SSL in the context of detecting major depressive disorder (MDD) and post-traumatic stress disorder (PTSD) using audio and video data collected during interactive sessions. This study employs SSL models trained by predicting multiple fixed targets or masked frames. We propose a list of fixed targets to make the generated representation more efficient for detecting MDD 
&lt;/p&gt;</description></item><item><title>&#30446;&#26631;&#31867;&#20998;&#31867;&#30340;&#20851;&#38190;&#22312;&#20110;&#36716;&#25442;&#22270;&#30340;&#23646;&#24615;&#65292;&#30740;&#31350;&#34920;&#26126;&#29702;&#24819;&#30340;&#36716;&#25442;&#22270;&#32467;&#26500;&#26159;&#26397;&#26681;&#39030;&#28857;&#26041;&#21521;&#21462;&#21521;&#30340;&#26377;&#26681;&#26641;&#12290;</title><link>https://arxiv.org/abs/2403.15167</link><description>&lt;p&gt;
&#30446;&#26631;&#31867;&#20998;&#31867;&#30340;&#36716;&#25442;&#22270;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Transition Graph Properties of Target Class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15167
&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#31867;&#20998;&#31867;&#30340;&#20851;&#38190;&#22312;&#20110;&#36716;&#25442;&#22270;&#30340;&#23646;&#24615;&#65292;&#30740;&#31350;&#34920;&#26126;&#29702;&#24819;&#30340;&#36716;&#25442;&#22270;&#32467;&#26500;&#26159;&#26397;&#26681;&#39030;&#28857;&#26041;&#21521;&#21462;&#21521;&#30340;&#26377;&#26681;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#31867;&#20998;&#31867;&#26159;&#19968;&#20010;&#28151;&#21512;&#20998;&#31867;&#21644;&#36716;&#25442;&#27169;&#22411;&#65292;&#20854;&#32508;&#21512;&#30446;&#26631;&#26159;&#23558;&#23545;&#35937;&#20998;&#37197;&#21040;&#26576;&#20010;&#25152;&#35859;&#30340;&#30446;&#26631;&#25110;&#27491;&#24120;&#31867;&#12290;&#20998;&#31867;&#36807;&#31243;&#26159;&#36845;&#20195;&#30340;&#65292;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#26576;&#20010;&#31867;&#20013;&#30340;&#23545;&#35937;&#32463;&#21382;&#19982;&#35813;&#31867;&#30456;&#23545;&#24212;&#30340;&#21160;&#20316;&#65292;&#24341;&#21457;&#23545;&#35937;&#21521;&#20854;&#20013;&#19968;&#20010;&#31867;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#31867;&#36716;&#25442;&#30340;&#36716;&#25442;&#24207;&#21015;&#24517;&#39035;&#35774;&#35745;&#24471;&#33021;&#20026;&#23545;&#35937;&#26368;&#32456;&#20998;&#37197;&#21040;&#30446;&#26631;&#31867;&#25552;&#20379;&#25903;&#25345;&#12290;&#36716;&#25442;&#36807;&#31243;&#21487;&#20197;&#29992;&#26377;&#21521;&#22270;&#30340;&#24418;&#24335;&#25551;&#36848;&#65292;&#26368;&#32456;&#20998;&#31867;&#30340;&#25104;&#21151;&#20027;&#35201;&#21462;&#20915;&#20110;&#36825;&#20010;&#22270;&#30340;&#23646;&#24615;&#12290;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#20102;&#36716;&#25442;&#22270;&#30340;&#29702;&#24819;&#32467;&#26500;&#26159;&#19968;&#20010;&#26397;&#26681;&#39030;&#28857;&#26041;&#21521;&#21462;&#21521;&#30340;&#26377;&#26681;&#26641;&#65292;&#35813;&#39030;&#28857;&#23545;&#24212;&#20110;&#27491;&#24120;&#31867;&#12290;&#24456;&#26126;&#26174;&#65292;&#20219;&#24847;&#31639;&#27861;&#65288;&#31574;&#30053;&#65289;&#30340;&#36716;&#25442;&#22270;&#21487;&#33021;&#27809;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15167v1 Announce Type: cross  Abstract: Target class classification is a mixed classification and transition model whose integrated goal is to assign objects to a certain, so called target or normal class. The classification process is iterative, and in each step an object in a certain class undergoes an action attached to that class, initiating the transition of the object to one of the classes. The sequence of transitions, which we call class transitions, must be designed to provide the final assignment of objects to the target class. The transition process can be described in the form of a directed graph, and the success of the final classification is mainly due to the properties of this graph. In our previous research we showed that the desirable structure of the transition graph is an oriented rooted tree with orientation towards the root vertex, which corresponds to the normal class. It is clear that the transition graph of an arbitrary algorithm (policy) may not have 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#24230;&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#20943;&#23569;&#26041;&#27861;&#23545;&#25552;&#21319;&#25928;&#29575;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#22810;&#31181;&#20943;&#23567;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#29992;&#20110;&#34913;&#37327;&#25968;&#25454;&#38598;&#30456;&#20284;&#24230;&#30340;&#25299;&#25169;&#20195;&#34920;&#24615;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.15150</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#20943;&#23569;&#26041;&#27861;&#30340;&#28145;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An In-Depth Analysis of Data Reduction Methods for Sustainable Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#24230;&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#20943;&#23569;&#26041;&#27861;&#23545;&#25552;&#21319;&#25928;&#29575;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#22810;&#31181;&#20943;&#23567;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#29992;&#20110;&#34913;&#37327;&#25968;&#25454;&#38598;&#30456;&#20284;&#24230;&#30340;&#25299;&#25169;&#20195;&#34920;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22240;&#20854;&#35299;&#20915;&#22797;&#26434;&#20998;&#31867;&#20219;&#21153;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#38738;&#30544;&#65292;&#38543;&#30528;&#26356;&#31934;&#30830;&#27169;&#22411;&#30340;&#24320;&#21457;&#12289;&#28023;&#37327;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#29616;&#20195;&#35745;&#31639;&#26426;&#25913;&#36827;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#19981;&#26029;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24615;&#33021;&#30340;&#25552;&#21319;&#20063;&#24102;&#26469;&#20102;&#25928;&#29575;&#38382;&#39064;&#65292;&#28041;&#21450;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#23384;&#20648;&#65292;&#20197;&#21450;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#28041;&#21450;&#30340;&#33021;&#28304;&#21644;&#26102;&#38388;&#28010;&#36153;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25968;&#25454;&#20943;&#23569;&#26377;&#21161;&#20110;&#22312;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20843;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#20943;&#23567;&#34920;&#26684;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;Python&#36719;&#20214;&#21253;&#26469;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#30340;&#20195;&#34920;&#24615;&#24230;&#37327;&#65292;&#29992;&#20197;&#34913;&#37327;&#20943;&#23569;&#30340;&#25968;&#25454;&#38598;&#21644;&#23436;&#25972;&#35757;&#32451;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#30456;&#20284;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15150v1 Announce Type: new  Abstract: In recent years, Deep Learning has gained popularity for its ability to solve complex classification tasks, increasingly delivering better results thanks to the development of more accurate models, the availability of huge volumes of data and the improved computational capabilities of modern computers. However, these improvements in performance also bring efficiency problems, related to the storage of datasets and models, and to the waste of energy and time involved in both the training and inference processes. In this context, data reduction can help reduce energy consumption when training a deep learning model. In this paper, we present up to eight different methods to reduce the size of a tabular training dataset, and we develop a Python package to apply them. We also introduce a representativeness metric based on topology to measure how similar are the reduced datasets and the full training dataset. Additionally, we develop a methodo
&lt;/p&gt;</description></item><item><title>Adam&#22312;&#38750;&#22343;&#21248;&#20809;&#28369;&#24773;&#20917;&#19979;&#19982;SGDM&#30340;&#25910;&#25947;&#36895;&#29575;&#26377;&#26126;&#26174;&#21306;&#21035;&#65292;Adam&#26356;&#24555;&#22320;&#36798;&#21040;&#25910;&#25947;&#65292;&#24182;&#20855;&#26377;&#26356;&#20302;&#30340;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.15146</link><description>&lt;p&gt;
Adam&#22312;&#38750;&#22343;&#21248;&#20809;&#28369;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#65306;&#19982;SGDM&#30340;&#21487;&#20998;&#24615;&#21450;&#20854;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Adam under Non-uniform Smoothness: Separability from SGDM and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15146
&lt;/p&gt;
&lt;p&gt;
Adam&#22312;&#38750;&#22343;&#21248;&#20809;&#28369;&#24773;&#20917;&#19979;&#19982;SGDM&#30340;&#25910;&#25947;&#36895;&#29575;&#26377;&#26126;&#26174;&#21306;&#21035;&#65292;Adam&#26356;&#24555;&#22320;&#36798;&#21040;&#25910;&#25947;&#65292;&#24182;&#20855;&#26377;&#26356;&#20302;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#26126;&#30830;&#21306;&#20998;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGDM&#65289;&#21644;Adam&#22312;&#25910;&#25947;&#36895;&#29575;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#38750;&#22343;&#21248;&#26377;&#30028;&#20809;&#28369;&#24230;&#26465;&#20214;&#19979;&#65292;Adam&#30456;&#23545;&#20110;SGDM&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65306;&#65288;1&#65289;&#22312;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#65292;Adam&#21487;&#20197;&#36798;&#21040;&#30830;&#23450;&#24615;&#19968;&#38454;&#20248;&#21270;&#22120;&#25910;&#25947;&#36895;&#29575;&#30340;&#24050;&#30693;&#19979;&#30028;&#65292;&#32780;&#21160;&#37327;&#26799;&#24230;&#19979;&#38477;&#65288;GDM&#65289;&#30340;&#25910;&#25947;&#36895;&#29575;&#23545;&#21021;&#22987;&#20989;&#25968;&#20540;&#20855;&#26377;&#26356;&#39640;&#38454;&#30340;&#20381;&#36182;&#24615;&#65307;&#65288;2&#65289;&#22312;&#38543;&#26426;&#35774;&#32622;&#19979;&#65292;Adam&#30340;&#25910;&#25947;&#36895;&#29575;&#19978;&#30028;&#21305;&#37197;&#20102;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#22120;&#30340;&#19979;&#30028;&#65292;&#32771;&#34385;&#21040;&#21021;&#22987;&#20989;&#25968;&#20540;&#21644;&#26368;&#32456;&#35823;&#24046;&#65292;&#32780;SGDM&#23384;&#22312;&#23398;&#20064;&#29575;&#19979;&#22833;&#36133;&#25910;&#25947;&#30340;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#28165;&#26224;&#22320;&#21306;&#20998;&#20102;Adam&#21644;SGDM&#22312;&#25910;&#25947;&#36895;&#29575;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15146v1 Announce Type: new  Abstract: This paper aims to clearly distinguish between Stochastic Gradient Descent with Momentum (SGDM) and Adam in terms of their convergence rates. We demonstrate that Adam achieves a faster convergence compared to SGDM under the condition of non-uniformly bounded smoothness. Our findings reveal that: (1) in deterministic environments, Adam can attain the known lower bound for the convergence rate of deterministic first-order optimizers, whereas the convergence rate of Gradient Descent with Momentum (GDM) has higher order dependence on the initial function value; (2) in stochastic setting, Adam's convergence rate upper bound matches the lower bounds of stochastic first-order optimizers, considering both the initial function value and the final error, whereas there are instances where SGDM fails to converge with any learning rate. These insights distinctly differentiate Adam and SGDM regarding their convergence rates. Additionally, by introduci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#37327;&#21270;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30452;&#26041;&#22270;&#30340;&#32622;&#25442;&#19981;&#21464;&#32593;&#32476;HistNetQ&#65292;&#22312;&#37327;&#21270;&#27604;&#36187;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.15123</link><description>&lt;p&gt;
&#22522;&#20110;&#30452;&#26041;&#22270;&#30340;&#32622;&#25442;&#19981;&#21464;&#32593;&#32476;&#30340;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantification using Permutation-Invariant Networks based on Histograms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#37327;&#21270;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30452;&#26041;&#22270;&#30340;&#32622;&#25442;&#19981;&#21464;&#32593;&#32476;HistNetQ&#65292;&#22312;&#37327;&#21270;&#27604;&#36187;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#65292;&#20063;&#31216;&#20026;&#31867;&#21035;&#26222;&#36941;&#24615;&#20272;&#35745;&#65292;&#26159;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#27169;&#22411;&#34987;&#35757;&#32451;&#29992;&#26469;&#39044;&#27979;&#32473;&#23450;&#26679;&#26412;&#38598;&#20013;&#27599;&#20010;&#31867;&#30340;&#26222;&#36941;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#37327;&#21270;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#21487;&#20197;&#24212;&#29992;&#23545;&#31216;&#30417;&#30563;&#26041;&#27861;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#20998;&#31867;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#30340;&#38656;&#27714;&#65292;&#30452;&#25509;&#35299;&#20915;&#37327;&#21270;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#20026;&#38598;&#21512;&#22788;&#29702;&#35774;&#35745;&#30340;&#29616;&#26377;&#32622;&#25442;&#19981;&#21464;&#23618;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#37327;&#21270;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HistNetQ&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#23427;&#20381;&#36182;&#20110;&#22522;&#20110;&#30452;&#26041;&#22270;&#30340;&#32622;&#25442;&#19981;&#21464;&#34920;&#31034;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#37327;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#36804;&#20170;&#20026;&#27490;&#21807;&#19968;&#30340;&#37327;&#21270;&#31454;&#36187;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;HistNetQ&#32988;&#36807;&#20854;&#20182;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15123v1 Announce Type: new  Abstract: Quantification, also known as class prevalence estimation, is the supervised learning task in which a model is trained to predict the prevalence of each class in a given bag of examples. This paper investigates the application of deep neural networks to tasks of quantification in scenarios where it is possible to apply a symmetric supervised approach that eliminates the need for classification as an intermediary step, directly addressing the quantification problem. Additionally, it discusses existing permutation-invariant layers designed for set processing and assesses their suitability for quantification. In light of our analysis, we propose HistNetQ, a novel neural architecture that relies on a permutation-invariant representation based on histograms that is specially suited for quantification problems. Our experiments carried out in the only quantification competition held to date, show that HistNetQ outperforms other deep neural arch
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.15112</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#23884;&#20837;&#36827;&#34892;&#25991;&#26412;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text clustering with LLM embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15112
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32858;&#31867;&#26159;&#32452;&#32455;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#23383;&#20869;&#23481;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#32467;&#26500;&#21270;&#21644;&#21457;&#29616;&#26410;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#65288;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#20013;&#20351;&#29992;&#30340;&#65289;&#21644;&#32858;&#31867;&#31639;&#27861;&#22914;&#20309;&#24433;&#21709;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#26041;&#24335;&#12290;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#20197;&#35780;&#20272;&#23884;&#20837;&#26159;&#22914;&#20309;&#24433;&#21709;&#32858;&#31867;&#32467;&#26524;&#30340;&#65292;&#20197;&#21450;&#36890;&#36807;&#25688;&#35201;&#36827;&#34892;&#38477;&#32500;&#21644;&#23884;&#20837;&#22823;&#23567;&#35843;&#25972;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#23884;&#20837;&#22312;&#25429;&#33719;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#38656;&#35201;&#20180;&#32454;&#20998;&#26512;&#25165;&#33021;&#22312;&#23454;&#38469;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;GroupSort&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26356;&#31934;&#30830;&#30340;&#20272;&#35745;&#24182;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#24555;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15108</link><description>&lt;p&gt;
&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;GroupSort&#31070;&#32463;&#32593;&#32476;&#30340;&#22238;&#24402;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Regression based on Wasserstein distance and GroupSort Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15108
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;GroupSort&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26356;&#31934;&#30830;&#30340;&#20272;&#35745;&#24182;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#24555;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#12290;&#25152;&#25552;&#20986;&#30340;Wasserstein&#20027;&#21160;&#22238;&#24402;&#27169;&#22411;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#21407;&#21017;&#65292;&#29992;&#20110;&#34913;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#20195;&#34920;&#24615;&#12290;&#20351;&#29992;GroupSort&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;Wasserstein&#36317;&#31163;&#12290;&#36825;&#20123;&#32593;&#32476;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#21487;&#20197;&#37327;&#21270;&#38169;&#35823;&#24182;&#26126;&#30830;&#20854;&#22823;&#23567;&#21644;&#28145;&#24230;&#12290;&#27492;&#35299;&#20915;&#26041;&#26696;&#32467;&#21512;&#20102;&#21478;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#23545;&#24322;&#24120;&#20540;&#26356;&#20855;&#23481;&#24525;&#24615;&#65292;&#20197;&#23436;&#25104;&#26597;&#35810;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#19982;&#20854;&#20182;&#32463;&#20856;&#21644;&#26368;&#36817;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#36825;&#31181;&#20195;&#34920;&#24615;-&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#30340;&#30456;&#20851;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#25972;&#20010;&#26597;&#35810;&#36807;&#31243;&#20013;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;Wasserstein&#20027;&#21160;&#22238;&#24402;&#36890;&#24120;&#33021;&#22815;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20272;&#35745;&#65292;&#24182;&#19988;&#24448;&#24448;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#24555;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15108v1 Announce Type: new  Abstract: This paper addresses a new active learning strategy for regression problems. The presented Wasserstein active regression model is based on the principles of distribution-matching to measure the representativeness of the labeled dataset. The Wasserstein distance is computed using GroupSort Neural Networks. The use of such networks provides theoretical foundations giving a way to quantify errors with explicit bounds for their size and depth. This solution is combined with another uncertainty-based approach that is more outlier-tolerant to complete the query strategy. Finally, this method is compared with other classical and recent solutions. The study empirically shows the pertinence of such a representativity-uncertainty approach, which provides good estimation all along the query procedure. Moreover, the Wasserstein active regression often achieves more precise estimations and tends to improve accuracy faster than other models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FetalSynthSeg&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#32974;&#20799;MRI&#20013;&#30340;&#33041;&#32452;&#32455;&#20998;&#21106;&#65292;&#36890;&#36807;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15103</link><description>&lt;p&gt;
&#29992;&#21512;&#25104;&#25968;&#25454;&#25913;&#36827;&#32974;&#20799;MRI&#20013;&#36328;&#39046;&#22495;&#33041;&#32452;&#32455;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Improving cross-domain brain tissue segmentation in fetal MRI with synthetic data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15103
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FetalSynthSeg&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#32974;&#20799;MRI&#20013;&#30340;&#33041;&#32452;&#32455;&#20998;&#21106;&#65292;&#36890;&#36807;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32974;&#20799;&#33041;&#32452;&#32455;&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#20013;&#30340;&#20998;&#21106;&#23545;&#20110;&#23376;&#23467;&#20869;&#31070;&#32463;&#21457;&#32946;&#30340;&#30740;&#31350;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21270;&#24037;&#20855;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#39046;&#22495;&#36716;&#31227;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#24517;&#39035;&#23545;&#39640;&#24230;&#24322;&#36136;&#30340;&#20020;&#24202;&#25968;&#25454;&#20855;&#26377;&#20581;&#22766;&#24615;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#25968;&#37327;&#26377;&#38480;&#19988;&#32570;&#20047;&#27880;&#37322;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#24863;&#26469;&#28304;&#20110;SynthSeg&#30340;FetalSynthSeg&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#21106;&#32974;&#20799;&#33041;MRI&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#19968;&#20010;&#21253;&#21547;120&#20010;&#23545;&#35937;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#25193;&#23637;&#21040;&#20351;&#29992;&#20302;&#30913;&#22330;&#65288;0.55T&#65289;MRI&#37319;&#38598;&#24182;&#29992;&#26032;&#22411;SR&#27169;&#22411;&#37325;&#24314;&#30340;40&#20010;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15103v1 Announce Type: cross  Abstract: Segmentation of fetal brain tissue from magnetic resonance imaging (MRI) plays a crucial role in the study of in utero neurodevelopment. However, automated tools face substantial domain shift challenges as they must be robust to highly heterogeneous clinical data, often limited in numbers and lacking annotations. Indeed, high variability of the fetal brain morphology, MRI acquisition parameters, and superresolution reconstruction (SR) algorithms adversely affect the model's performance when evaluated out-of-domain. In this work, we introduce FetalSynthSeg, a domain randomization method to segment fetal brain MRI, inspired by SynthSeg. Our results show that models trained solely on synthetic data outperform models trained on real data in out-ofdomain settings, validated on a 120-subject cross-domain dataset. Furthermore, we extend our evaluation to 40 subjects acquired using lowfield (0.55T) MRI and reconstructed with novel SR models, s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#24037;&#20316;&#27969;&#31243;&#65292;&#23558;&#29615;&#22659;&#22122;&#22768;&#23618;&#26512;&#25104;&#20687;&#65288;ANT&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#23545;&#20110;&#38108;&#31561;&#30719;&#20135;&#36164;&#28304;&#30340;&#21457;&#29616;&#21644;&#25551;&#32472;&#12290;</title><link>https://arxiv.org/abs/2403.15095</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#29615;&#22659;&#22122;&#22768;&#23618;&#26512;&#25104;&#20687;&#30340;&#31471;&#21040;&#31471;&#30719;&#20135;&#21208;&#25506;
&lt;/p&gt;
&lt;p&gt;
End-to-End Mineral Exploration with Artificial Intelligence and Ambient Noise Tomography
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15095
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#24037;&#20316;&#27969;&#31243;&#65292;&#23558;&#29615;&#22659;&#22122;&#22768;&#23618;&#26512;&#25104;&#20687;&#65288;ANT&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#23545;&#20110;&#38108;&#31561;&#30719;&#20135;&#36164;&#28304;&#30340;&#21457;&#29616;&#21644;&#25551;&#32472;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31471;&#21040;&#31471;&#24037;&#20316;&#27969;&#31243;&#65292;&#23558;&#29615;&#22659;&#22122;&#22768;&#23618;&#26512;&#25104;&#20687;&#65288;ANT&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#23545;&#30719;&#20135;&#36164;&#28304;&#30340;&#21457;&#29616;&#21644;&#25551;&#32472;&#65292;&#36825;&#23545;&#20840;&#29699;&#36716;&#21521;&#20302;&#30899;&#32463;&#27982;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#38598;&#20013;&#35752;&#35770;&#38108;&#20316;&#20026;&#19968;&#31181;&#20851;&#38190;&#20803;&#32032;&#65292;&#21487;&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#35299;&#20915;&#26041;&#26696;&#20013;&#22823;&#37327;&#38656;&#35201;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;ANT&#30340;&#22909;&#22788;&#65292;&#20854;&#29305;&#28857;&#26159;&#36895;&#24230;&#24555;&#12289;&#21487;&#20280;&#32553;&#12289;&#31359;&#36879;&#28145;&#24230;&#39640;&#12289;&#20998;&#36776;&#29575;&#39640;&#21644;&#29615;&#22659;&#24433;&#21709;&#23567;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#65292;&#22312;&#26412;&#22320;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#19978;&#24494;&#35843;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#22312;&#30719;&#24202;&#23610;&#24230;&#19978;&#32454;&#21270;&#25152;&#22312;&#22823;&#38470;&#23610;&#24230;&#19978;&#36828;&#26223;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#38024;&#23545;&#28595;&#22823;&#21033;&#20122;&#38108;&#30719;&#30340;&#26032;&#19968;&#20195;&#25968;&#25454;&#39537;&#21160;&#30340;AI&#36828;&#26223;&#27169;&#22411;&#65292;&#20316;&#20026;&#25105;&#20204;&#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#21518;&#25105;&#20204;&#19987;&#27880;&#20110;Hillside IOCG&#30719;&#24202;&#30340;&#36828;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15095v1 Announce Type: cross  Abstract: This paper presents an innovative end-to-end workflow for mineral exploration, integrating ambient noise tomography (ANT) and artificial intelligence (AI) to enhance the discovery and delineation of mineral resources essential for the global transition to a low carbon economy. We focus on copper as a critical element, required in significant quantities for renewable energy solutions. We show the benefits of utilising ANT, characterised by its speed, scalability, depth penetration, resolution, and low environmental impact, alongside artificial intelligence (AI) techniques to refine a continent-scale prospectivity model at the deposit scale by fine-tuning our model on local high-resolution data. We show the promise of the method by first presenting a new data-driven AI prospectivity model for copper within Australia, which serves as our foundation model for further fine-tuning. We then focus on the Hillside IOCG deposit on the prospectiv
&lt;/p&gt;</description></item><item><title>&#25913;&#36827;&#38271;&#30701;&#26399;&#35760;&#24518;&#27745;&#27700;&#22788;&#29702;&#27169;&#25311;&#22120;&#65292;&#35299;&#20915;DRL&#20248;&#21270;&#24037;&#19994;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20943;&#23569;&#22797;&#21512;&#35823;&#24046;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2403.15091</link><description>&lt;p&gt;
&#38024;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25913;&#36827;&#38271;&#30701;&#26399;&#35760;&#24518;&#27745;&#27700;&#22788;&#29702;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15091
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#38271;&#30701;&#26399;&#35760;&#24518;&#27745;&#27700;&#22788;&#29702;&#27169;&#25311;&#22120;&#65292;&#35299;&#20915;DRL&#20248;&#21270;&#24037;&#19994;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20943;&#23569;&#22797;&#21512;&#35823;&#24046;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#28216;&#25103;&#39046;&#22495;&#21462;&#24471;&#20102;&#26480;&#20986;&#25104;&#26524;&#65292;&#20294;&#22312;&#27745;&#27700;&#22788;&#29702;&#31561;&#24037;&#19994;&#36807;&#31243;&#30340;&#20248;&#21270;&#20013;&#23454;&#26045;DRL&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#33021;&#22815;&#23613;&#21487;&#33021;&#20934;&#30830;&#22320;&#20195;&#34920;&#23454;&#38469;&#24037;&#21378;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#20197;&#35757;&#32451;DRL&#31574;&#30053;&#12290;&#27745;&#27700;&#22788;&#29702;&#25968;&#25454;&#30340;&#38543;&#26426;&#24615;&#21644;&#38750;&#32447;&#24615;&#23548;&#33268;&#27169;&#22411;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#20135;&#29983;&#19981;&#31283;&#23450;&#21644;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#25913;&#36827;&#30340;&#27169;&#22411;&#20013;&#21487;&#33021;&#20986;&#29616;&#27169;&#25311;&#34892;&#20026;&#19981;&#27491;&#30830;&#30340;&#19968;&#20010;&#21487;&#33021;&#21407;&#22240;&#26159;&#22797;&#21512;&#35823;&#24046;&#38382;&#39064;&#65292;&#21363;&#22312;&#27169;&#25311;&#36807;&#31243;&#20013;&#35823;&#24046;&#30340;&#32047;&#31215;&#12290;&#22797;&#21512;&#35823;&#24046;&#26159;&#22240;&#20026;&#27169;&#22411;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#20013;&#20351;&#29992;&#20854;&#39044;&#27979;&#20316;&#20026;&#36755;&#20837;&#65292;&#23454;&#38469;&#25968;&#25454;&#19982;&#39044;&#27979;&#20043;&#38388;&#30340;&#35823;&#24046;&#38543;&#30528;&#27169;&#25311;&#30340;&#36827;&#34892;&#32780;&#32047;&#31215;&#12290;&#25105;&#20204;&#23454;&#26045;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#27745;&#27700;&#22788;&#29702;&#27169;&#22411;&#30340;&#35757;&#32451;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15091v1 Announce Type: cross  Abstract: Even though Deep Reinforcement Learning (DRL) showed outstanding results in the fields of Robotics and Games, it is still challenging to implement it in the optimization of industrial processes like wastewater treatment. One of the challenges is the lack of a simulation environment that will represent the actual plant as accurately as possible to train DRL policies. Stochasticity and non-linearity of wastewater treatment data lead to unstable and incorrect predictions of models over long time horizons. One possible reason for the models' incorrect simulation behavior can be related to the issue of compounding error, which is the accumulation of errors throughout the simulation. The compounding error occurs because the model utilizes its predictions as inputs at each time step. The error between the actual data and the prediction accumulates as the simulation continues. We implemented two methods to improve the trained models for wastew
&lt;/p&gt;</description></item><item><title>SIMAP&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#22270;&#23618;&#65292;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#32467;&#21512;&#65292;&#20316;&#20026;&#21487;&#35299;&#37322;&#22270;&#23618;&#26367;&#20195;&#32463;&#20856;&#30340;&#23494;&#38598;&#26368;&#32456;&#22270;&#23618;&#65292;&#25903;&#25345;&#38598;&#22522;&#20110;&#22266;&#23450;&#30340;&#26368;&#22823;&#21333;&#32431;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.15083</link><description>&lt;p&gt;
SIMAP&#65306;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#21333;&#32431;&#21516;&#35843;&#22270;&#23618;
&lt;/p&gt;
&lt;p&gt;
SIMAP: A simplicial-map layer for neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15083
&lt;/p&gt;
&lt;p&gt;
SIMAP&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#22270;&#23618;&#65292;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#32467;&#21512;&#65292;&#20316;&#20026;&#21487;&#35299;&#37322;&#22270;&#23618;&#26367;&#20195;&#32463;&#20856;&#30340;&#23494;&#38598;&#26368;&#32456;&#22270;&#23618;&#65292;&#25903;&#25345;&#38598;&#22522;&#20110;&#22266;&#23450;&#30340;&#26368;&#22823;&#21333;&#32431;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SIMAP&#65292;&#36825;&#26159;&#19968;&#20010;&#23884;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#22270;&#23618;&#65292;&#26088;&#22312;&#22686;&#24378;&#36755;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;SIMAP&#22270;&#23618;&#26159;Simplicial-Map&#31070;&#32463;&#32593;&#32476;&#65288;SMNNs&#65289;&#30340;&#22686;&#24378;&#29256;&#26412;&#65292;&#26159;&#19968;&#31181;&#22522;&#20110;&#25903;&#25345;&#38598;&#21644;&#21333;&#32431;&#21516;&#35843;&#26144;&#23556;&#65288;&#25299;&#25169;&#23398;&#20013;&#29992;&#26469;&#22312;&#20445;&#25345;&#32467;&#26500;&#36830;&#36890;&#24615;&#30340;&#21516;&#26102;&#36716;&#25442;&#24418;&#29366;&#30340;&#20989;&#25968;&#65289;&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21019;&#26032;&#24615;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;SIMAP&#22270;&#23618;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#37197;&#21512;&#24037;&#20316;&#65292;&#20316;&#20026;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22270;&#23618;&#26367;&#20195;&#32463;&#20856;&#30340;&#23494;&#38598;&#26368;&#32456;&#22270;&#23618;&#12290;&#20854;&#27425;&#65292;&#19982;SMNNs&#19981;&#21516;&#65292;&#25903;&#25345;&#38598;&#22522;&#20110;&#22266;&#23450;&#30340;&#26368;&#22823;&#21333;&#32431;&#20307;&#65292;&#20854;&#37325;&#24515;&#20998;&#21106;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#30697;&#38453;&#30340;&#20056;&#27861;&#31639;&#27861;&#39640;&#25928;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15083v1 Announce Type: new  Abstract: In this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing the interpretability of the output. The SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an explainable neural network based on support sets and simplicial maps (functions used in topology to transform shapes while preserving their structural connectivity). The novelty of the methodology proposed in this paper is two-fold: Firstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. Secondly, unlike SMNNs, the support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#29366;&#24577;&#20998;&#24067;&#30340;&#32479;&#35745;&#30697;&#21644;&#21033;&#29992;&#36712;&#36857;&#27010;&#29575;&#19982;&#29305;&#24449;&#26399;&#26395;&#30340;&#30456;&#20851;&#24615;&#65292;&#26377;&#25928;&#22320;&#24674;&#22797;&#22870;&#21169;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.15079</link><description>&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Automated Feature Selection for Inverse Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15079
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#29366;&#24577;&#20998;&#24067;&#30340;&#32479;&#35745;&#30697;&#21644;&#21033;&#29992;&#36712;&#36857;&#27010;&#29575;&#19982;&#29305;&#24449;&#26399;&#26395;&#30340;&#30456;&#20851;&#24615;&#65292;&#26377;&#25928;&#22320;&#24674;&#22797;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26159;&#19968;&#31181;&#20174;&#19987;&#23478;&#31034;&#33539;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;&#23427;&#30340;&#20351;&#29992;&#36991;&#20813;&#20102;&#25163;&#21160;&#25351;&#23450;&#22870;&#21169;&#30340;&#22256;&#38590;&#21644;&#32321;&#29712;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;IRL&#20013;&#65292;&#22870;&#21169;&#36890;&#24120;&#34987;&#34920;&#31034;&#20026;&#29305;&#24449;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#65292;&#20165;&#20351;&#29992;&#29366;&#24577;&#21464;&#37327;&#20316;&#20026;&#29305;&#24449;&#19981;&#22815;&#20016;&#23500;&#65292;&#20294;&#36890;&#24120;&#19981;&#28165;&#26970;&#21738;&#20123;&#29305;&#24449;&#26159;&#22909;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#39033;&#24335;&#22522;&#20989;&#25968;&#26469;&#24418;&#25104;&#20505;&#36873;&#29305;&#24449;&#38598;&#21512;&#65292;&#36825;&#20123;&#29305;&#24449;&#34987;&#35777;&#26126;&#21487;&#20197;&#21305;&#37197;&#29366;&#24577;&#20998;&#24067;&#30340;&#32479;&#35745;&#30697;&#12290;&#28982;&#21518;&#36890;&#36807;&#21033;&#29992;&#36712;&#36857;&#27010;&#29575;&#21644;&#29305;&#24449;&#26399;&#26395;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23545;&#20505;&#36873;&#29305;&#24449;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#12290;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#25429;&#33719;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#24335;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15079v1 Announce Type: new  Abstract: Inverse reinforcement learning (IRL) is an imitation learning approach to learning reward functions from expert demonstrations. Its use avoids the difficult and tedious procedure of manual reward specification while retaining the generalization power of reinforcement learning. In IRL, the reward is usually represented as a linear combination of features. In continuous state spaces, the state variables alone are not sufficiently rich to be used as features, but which features are good is not known in general. To address this issue, we propose a method that employs polynomial basis functions to form a candidate set of features, which are shown to allow the matching of statistical moments of state distributions. Feature selection is then performed for the candidates by leveraging the correlation between trajectory probabilities and feature expectations. We demonstrate the approach's effectiveness by recovering reward functions that capture 
&lt;/p&gt;</description></item><item><title>GTAGCN&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#32858;&#21512;&#32593;&#32476;&#21644;&#25299;&#25169;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#21644;&#38745;&#24577;&#25968;&#25454;&#65292;&#36866;&#29992;&#20110;&#33410;&#28857;&#21644;&#22270;&#20998;&#31867;&#65292;&#23454;&#35777;&#20998;&#26512;&#26174;&#31034;&#32467;&#26524;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.15077</link><description>&lt;p&gt;
GTAGCN&#65306;&#24191;&#20041;&#25299;&#25169;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GTAGCN: Generalized Topology Adaptive Graph Convolutional Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15077
&lt;/p&gt;
&lt;p&gt;
GTAGCN&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#32858;&#21512;&#32593;&#32476;&#21644;&#25299;&#25169;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#21644;&#38745;&#24577;&#25968;&#25454;&#65292;&#36866;&#29992;&#20110;&#33410;&#28857;&#21644;&#22270;&#20998;&#31867;&#65292;&#23454;&#35777;&#20998;&#26512;&#26174;&#31034;&#32467;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15077v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#24120;&#29992;&#21644;&#26631;&#20934;&#26041;&#27861;&#12290; GNN&#30340;&#25991;&#29486;&#24378;&#35843;&#20102;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#28508;&#21147;&#20197;&#21450;&#23427;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#35201;&#20040;&#26159;&#27010;&#24565;&#19978;&#26032;&#39062;&#30340;&#65292;&#35201;&#20040;&#26159;&#20174;&#29305;&#23450;&#25216;&#26415;&#27966;&#29983;&#20986;&#26469;&#30340;&#12290;&#22240;&#27492;&#65292;&#28151;&#21512;&#24418;&#24335;&#30340;&#22810;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#36825;&#21487;&#20197;&#24456;&#22909;&#22320;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#25110;&#38745;&#24577;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#31181;&#25104;&#29087;&#25216;&#26415;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#21363;&#24191;&#20041;&#32858;&#21512;&#32593;&#32476;&#21644;&#25299;&#25169;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36825;&#35299;&#20915;&#20102;&#25105;&#20204;&#22312;&#25928;&#26524;&#19978;&#21487;&#20197;&#24212;&#29992;&#20110;&#24207;&#21015;&#22411;&#21644;&#38745;&#24577;&#22411;&#25968;&#25454;&#30340;&#30446;&#30340;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#33410;&#28857;&#21644;&#22270;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#26174;&#31034;&#65292;&#32467;&#26524;&#19982;&#25991;&#29486;&#32467;&#26524;&#30456;&#24403;&#65292;&#24182;&#19988;&#23545;&#25163;&#20889;&#31508;&#30011;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15077v1 Announce Type: new  Abstract: Graph Neural Networks (GNN) have emerged as a popular and standard approach for learning from graph-structured data. The literature on GNN highlights the potential of this evolving research area and its widespread adoption in real-life applications. However, most of the approaches are either new in concept or derived from specific techniques. Therefore, the potential of more than one approach in hybrid form has not been studied extensively, which can be well utilized for sequenced data or static data together. We derive a hybrid approach based on two established techniques as generalized aggregation networks and topology adaptive graph convolution networks that solve our purpose to apply on both types of sequenced and static nature of data, effectively. The proposed method applies to both node and graph classification. Our empirical analysis reveals that the results are at par with literature results and better for handwritten strokes as
&lt;/p&gt;</description></item><item><title>TensorNet&#25193;&#23637;&#20102;&#20854;&#33021;&#21147;&#65292;&#21487;&#20197;&#22788;&#29702;&#24102;&#30005;&#20998;&#23376;&#21644;&#33258;&#26059;&#29366;&#24577;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#21508;&#31181;&#21270;&#23398;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15073</link><description>&lt;p&gt;
&#35770;&#31515;&#21345;&#23572;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#21183;&#20013;&#21253;&#21547;&#30005;&#33655;&#21644;&#33258;&#26059;&#29366;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Inclusion of Charge and Spin States in Cartesian Tensor Neural Network Potentials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15073
&lt;/p&gt;
&lt;p&gt;
TensorNet&#25193;&#23637;&#20102;&#20854;&#33021;&#21147;&#65292;&#21487;&#20197;&#22788;&#29702;&#24102;&#30005;&#20998;&#23376;&#21644;&#33258;&#26059;&#29366;&#24577;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#21508;&#31181;&#21270;&#23398;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#23553;&#20449;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;TensorNet&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#31561;&#21464;&#31515;&#21345;&#23572;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#21183;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#24102;&#30005;&#20998;&#23376;&#21644;&#33258;&#26059;&#29366;&#24577;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26550;&#26500;&#26356;&#25913;&#25110;&#22686;&#21152;&#25104;&#26412;&#12290;&#36890;&#36807;&#21512;&#24182;&#36825;&#20123;&#23646;&#24615;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36755;&#20837;&#36864;&#21270;&#38382;&#39064;&#65292;&#22686;&#24378;&#20102;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#21270;&#23398;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#36827;&#23637;&#26174;&#33879;&#25299;&#23485;&#20102;TensorNet&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15073v1 Announce Type: new  Abstract: In this letter, we present an extension to TensorNet, a state-of-the-art equivariant Cartesian tensor neural network potential, allowing it to handle charged molecules and spin states without architectural changes or increased costs. By incorporating these attributes, we address input degeneracy issues, enhancing the model's predictive accuracy across diverse chemical systems. This advancement significantly broadens TensorNet's applicability, maintaining its efficiency and accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#30001;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#20013;&#35270;&#35273;&#24187;&#35273;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35270;&#35273;&#24187;&#35273;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.15048</link><description>&lt;p&gt;
&#21345;&#36890;&#24187;&#35273;&#26816;&#27979;: &#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#30001;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#20013;&#35270;&#35273;&#24187;&#35273;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35270;&#35273;&#24187;&#35273;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#29983;&#25104;&#39046;&#22495;&#20013;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#24187;&#35273;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39118;&#26684;&#22914;&#21345;&#36890;&#20154;&#29289;&#20013;&#21253;&#21547;&#20102;&#24863;&#30693;&#19978;&#20851;&#38190;&#30340;&#32570;&#38519;&#65292;&#20381;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#26816;&#27979;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#30340;&#35270;&#35273;&#24187;&#35273;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#65288;PA-ICVL&#65289;&#19982;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#21516;&#26102;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#12290;&#36890;&#36807;&#20174;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#23039;&#21183;&#20272;&#35745;&#22120;&#20013;&#33719;&#24471;&#23039;&#21183;&#25351;&#23548;&#65292;&#25105;&#20204;&#20351;VLM&#33021;&#22815;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35782;&#21035;&#35270;&#35273;&#24187;&#35273;&#26041;&#38754;&#65292;&#19982;&#20165;&#20381;&#36182;&#20110;RGB&#22270;&#20687;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20943;&#36731;&#35270;&#35273;&#24187;&#35273;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15048v1 Announce Type: cross  Abstract: Large-scale Text-to-Image (TTI) models have become a common approach for generating training data in various generative fields. However, visual hallucinations, which contain perceptually critical defects, remain a concern, especially in non-photorealistic styles like cartoon characters. We propose a novel visual hallucination detection system for cartoon character images generated by TTI models. Our approach leverages pose-aware in-context visual learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB images and pose information. By incorporating pose guidance from a fine-tuned pose estimator, we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual hallucinations compared to baseline methods relying solely on RGB images. This research advances TTI models by mitigating visual hallucinations, expanding their potential in non-photorealistic domains.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#24102;&#29992;&#25143;&#20559;&#22909;&#30340;&#20027;&#21160;&#23398;&#20064;&#30340;&#24046;&#20998;&#31169;&#23494;&#23545;&#25239;&#24335;&#21452;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#30340;&#31169;&#23494;&#21644;&#38750;&#31169;&#23494;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.15045</link><description>&lt;p&gt;
DP-Dueling: &#22312;&#19981;&#25439;&#23475;&#29992;&#25143;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20174;&#20559;&#22909;&#21453;&#39304;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DP-Dueling: Learning from Preference Feedback without Compromising User Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15045
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#24102;&#29992;&#25143;&#20559;&#22909;&#30340;&#20027;&#21160;&#23398;&#20064;&#30340;&#24046;&#20998;&#31169;&#23494;&#23545;&#25239;&#24335;&#21452;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#30340;&#31169;&#23494;&#21644;&#38750;&#31169;&#23494;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#24191;&#27867;&#30740;&#31350;&#30340;&#23545;&#25239;&#24335;&#21452;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#26088;&#22312;&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26469;&#35782;&#21035;&#36817;&#20284;&#26368;&#20248;&#21160;&#20316;&#65292;&#21516;&#26102;&#21463;&#21040;&#24046;&#20998;&#38544;&#31169;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22823;&#22411;&#65288;&#21487;&#33021;&#26159;&#26080;&#30028;&#30340;&#65289;&#20915;&#31574;&#31354;&#38388;&#30340;&#22522;&#20110;&#25928;&#29992;&#30340;&#20559;&#22909;&#30697;&#38453;&#30340;&#19968;&#33324;&#31867;&#65292;&#24182;&#20026;&#24102;&#26377;&#29992;&#25143;&#20559;&#22909;&#30340;&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#24046;&#20998;&#31169;&#23494;&#23545;&#25239;&#24335;&#21452;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#31169;&#23494;&#21644;&#38750;&#31169;&#23494;&#36951;&#25022;&#19978;&#37117;&#25509;&#36817;&#26368;&#20248;&#24615;&#33021;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#24403;&#20915;&#31574;&#31354;&#38388;&#26159;&#26377;&#38480;&#22823;&#23567;$K$&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#20026;&#32431;$\epsilon$-DP&#20135;&#29983;&#20102;&#39034;&#24207;&#26368;&#20248;$O\Big(\sum_{i = 2}^K\log\frac{KT}{\Delta_i} + \frac{K}{\epsilon}\Big)$&#36951;&#25022;&#19978;&#30028;&#65292;&#20854;&#20013;$\Delta_i$&#34920;&#31034;&#31532;$i$&#20010;&#33218;&#30340;&#27425;&#20248;&#24046;&#36317;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#33268;&#30340;&#19979;&#30028;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15045v1 Announce Type: new  Abstract: We consider the well-studied dueling bandit problem, where a learner aims to identify near-optimal actions using pairwise comparisons, under the constraint of differential privacy. We consider a general class of utility-based preference matrices for large (potentially unbounded) decision spaces and give the first differentially private dueling bandit algorithm for active learning with user preferences. Our proposed algorithms are computationally efficient with near-optimal performance, both in terms of the private and non-private regret bound. More precisely, we show that when the decision space is of finite size $K$, our proposed algorithm yields order optimal $O\Big(\sum_{i = 2}^K\log\frac{KT}{\Delta_i} + \frac{K}{\epsilon}\Big)$ regret bound for pure $\epsilon$-DP, where $\Delta_i$ denotes the suboptimality gap of the $i$-th arm. We also present a matching lower bound analysis which proves the optimality of our algorithms. Finally, we
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20984;&#32452;&#21512;&#30340;&#26041;&#27861;&#20272;&#35745;&#39640;&#32500;&#31354;&#38388;&#20013;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#22810;&#32500;&#22343;&#20540;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#26435;&#37325;&#30830;&#23450;&#31574;&#30053;&#65306;&#19968;&#31181;&#36890;&#36807;&#27979;&#35797;&#31243;&#24207;&#35782;&#21035;&#20302;&#26041;&#24046;&#30340;&#30456;&#37051;&#22343;&#20540;&#65292;&#25552;&#20986;&#20102;&#23553;&#38381;&#24418;&#24335;&#25554;&#34917;&#20844;&#24335;&#65307;&#21478;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#20108;&#27425;&#39118;&#38505;&#30340;&#19978;&#32622;&#20449;&#30028;&#30830;&#23450;&#26435;&#37325;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#24471;&#20986;&#26041;&#27861;&#23545;&#32463;&#39564;&#22343;&#20540;&#30340;&#20108;&#27425;&#39118;&#38505;&#25913;&#36827;&#65292;&#22312;&#32500;&#24230;&#28176;&#36817;&#30340;&#35282;&#24230;&#19978;&#28176;&#36817;&#22320;&#25509;&#36817; Oracle&#65288;Minimax&#65289;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.15038</link><description>&lt;p&gt;
&#39640;&#32500;&#24773;&#20917;&#19979;&#22810;&#20010;&#22343;&#20540;&#21521;&#37327;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimation of multiple mean vectors in high dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15038
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20984;&#32452;&#21512;&#30340;&#26041;&#27861;&#20272;&#35745;&#39640;&#32500;&#31354;&#38388;&#20013;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#22810;&#32500;&#22343;&#20540;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#26435;&#37325;&#30830;&#23450;&#31574;&#30053;&#65306;&#19968;&#31181;&#36890;&#36807;&#27979;&#35797;&#31243;&#24207;&#35782;&#21035;&#20302;&#26041;&#24046;&#30340;&#30456;&#37051;&#22343;&#20540;&#65292;&#25552;&#20986;&#20102;&#23553;&#38381;&#24418;&#24335;&#25554;&#34917;&#20844;&#24335;&#65307;&#21478;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#20108;&#27425;&#39118;&#38505;&#30340;&#19978;&#32622;&#20449;&#30028;&#30830;&#23450;&#26435;&#37325;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#24471;&#20986;&#26041;&#27861;&#23545;&#32463;&#39564;&#22343;&#20540;&#30340;&#20108;&#27425;&#39118;&#38505;&#25913;&#36827;&#65292;&#22312;&#32500;&#24230;&#28176;&#36817;&#30340;&#35282;&#24230;&#19978;&#28176;&#36817;&#22320;&#25509;&#36817; Oracle&#65288;Minimax&#65289;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33268;&#21147;&#20110;&#22522;&#20110;&#29420;&#31435;&#26679;&#26412;&#22312;&#19968;&#20010;&#20849;&#21516;&#31354;&#38388;&#20013;&#20272;&#35745;&#26469;&#33258;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#22810;&#32500;&#22343;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#23545;&#36825;&#20123;&#26679;&#26412;&#23548;&#20986;&#30340;&#32463;&#39564;&#22343;&#20540;&#36827;&#34892;&#20984;&#32452;&#21512;&#26469;&#24418;&#25104;&#20272;&#35745;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#25214;&#21040;&#36866;&#24403;&#30340;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#20984;&#32452;&#21512;&#26435;&#37325;&#65306;&#31532;&#19968;&#31181;&#21033;&#29992;&#27979;&#35797;&#31243;&#24207;&#26469;&#35782;&#21035;&#20855;&#26377;&#20302;&#26041;&#24046;&#30340;&#30456;&#37051;&#22343;&#20540;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#20851;&#20110;&#26435;&#37325;&#30340;&#23553;&#38381;&#24418;&#24335;&#25554;&#34917;&#20844;&#24335;&#65307;&#31532;&#20108;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#20108;&#27425;&#39118;&#38505;&#30340;&#19978;&#32622;&#20449;&#21306;&#38388;&#26469;&#30830;&#23450;&#26435;&#37325;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#32463;&#39564;&#22343;&#20540;&#25552;&#20379;&#30340;&#20108;&#27425;&#39118;&#38505;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#38598;&#20013;&#22312;&#32500;&#24230;&#28176;&#36817;&#30340;&#35282;&#24230;&#19978;&#65292;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#30340;&#26377;&#25928;&#32500;&#24230;&#22686;&#21152;&#26102;&#28176;&#36817;&#22320;&#25509;&#36817;&#20110;&#19968;&#20010; Oracle&#65288;Minimax&#65289;&#25913;&#36827;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22343;&#20540;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15038v1 Announce Type: cross  Abstract: We endeavour to estimate numerous multi-dimensional means of various probability distributions on a common space based on independent samples. Our approach involves forming estimators through convex combinations of empirical means derived from these samples. We introduce two strategies to find appropriate data-dependent convex combination weights: a first one employing a testing procedure to identify neighbouring means with low variance, which results in a closed-form plug-in formula for the weights, and a second one determining weights via minimization of an upper confidence bound on the quadratic risk.Through theoretical analysis, we evaluate the improvement in quadratic risk offered by our methods compared to the empirical means. Our analysis focuses on a dimensional asymptotics perspective, showing that our methods asymptotically approach an oracle (minimax) improvement as the effective dimension of the data increases.We demonstrat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26059;&#36716;&#19981;&#21464;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#30340;&#31561;&#21464;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#20960;&#20309;&#24402;&#32435;&#20559;&#24046;&#65292;&#25104;&#21151;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15031</link><description>&lt;p&gt;
&#20351;&#29992;&#26059;&#36716;&#19981;&#21464;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Image Classification with Rotation-Invariant Variational Quantum Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15031
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26059;&#36716;&#19981;&#21464;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#30340;&#31561;&#21464;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#20960;&#20309;&#24402;&#32435;&#20559;&#24046;&#65292;&#25104;&#21151;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#20316;&#20026;&#22024;&#26434;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#30340;&#26089;&#26399;&#24212;&#29992;&#27491;&#21463;&#21040;&#20851;&#27880;&#12290;&#21464;&#20998;&#26041;&#27861;&#30340;&#20027;&#35201;&#38382;&#39064;&#20043;&#19968;&#22312;&#20110;Barren Plateaus&#29616;&#35937;&#65292;&#22312;&#21464;&#20998;&#21442;&#25968;&#20248;&#21270;&#36807;&#31243;&#20013;&#23384;&#22312;&#12290;&#25552;&#20986;&#23558;&#20960;&#20309;&#24402;&#32435;&#20559;&#24046;&#28155;&#21152;&#21040;&#37327;&#23376;&#27169;&#22411;&#20316;&#20026;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#20010;&#31216;&#20026;&#20960;&#20309;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#39046;&#22495;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31561;&#21464;&#32467;&#26500;&#30340;&#21464;&#20998;&#37327;&#23376;&#20998;&#31867;&#22120;&#65292;&#20197;&#21019;&#24314;&#20855;&#26377;$C_4$&#26059;&#36716;&#26631;&#31614;&#23545;&#31216;&#24615;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#26631;&#31614;&#19981;&#21464;&#27169;&#22411;&#12290;&#31561;&#21464;&#30005;&#36335;&#19982;&#20004;&#31181;&#19981;&#21516;&#30340;&#32467;&#26500;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#23454;&#39564;&#35777;&#26126;&#20960;&#20309;&#26041;&#27861;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#32463;&#20856;&#31561;&#21464;&#21367;&#31215;&#25805;&#20316;&#65292;&#20197;&#25193;&#23637;&#37327;&#23376;&#27169;&#22411;&#22788;&#29702;&#26356;&#22823;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15031v1 Announce Type: cross  Abstract: Variational quantum algorithms are gaining attention as an early application of Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of variational methods lies in the phenomenon of Barren Plateaus, present in the optimization of variational parameters. Adding geometric inductive bias to the quantum models has been proposed as a potential solution to mitigate this problem, leading to a new field called Geometric Quantum Machine Learning. In this work, an equivariant architecture for variational quantum classifiers is introduced to create a label-invariant model for image classification with $C_4$ rotational label symmetry. The equivariant circuit is benchmarked against two different architectures, and it is experimentally observed that the geometric approach boosts the model's performance. Finally, a classical equivariant convolution operation is proposed to extend the quantum model for the processing of larger ima
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#28784;&#33394;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINN&#65289;&#65292;&#36890;&#36807;&#36981;&#24490;&#28784;&#33394;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#23567;&#25968;&#25454;&#26679;&#26412;&#65292;&#20135;&#29983;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.15027</link><description>&lt;p&gt;
&#28784;&#33394;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Grey-informed neural network for time-series forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#28784;&#33394;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINN&#65289;&#65292;&#36890;&#36807;&#36981;&#24490;&#28784;&#33394;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#23567;&#25968;&#25454;&#26679;&#26412;&#65292;&#20135;&#29983;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22797;&#26434;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#34987;&#35270;&#20026;&#40657;&#30418;&#65292;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#24320;&#21457;&#12290;&#22240;&#27492;&#65292;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#26500;&#24314;&#36866;&#24403;&#30340;&#27169;&#22411;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24314;&#35758;&#23454;&#26045;&#28784;&#33394;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINN&#65289;&#12290;GINN &#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#36981;&#24490;&#28784;&#33394;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#28784;&#33394;&#31995;&#32479;&#29702;&#35770;&#20013;&#30340;&#20808;&#39564;&#30693;&#35782;&#20351;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#23567;&#25968;&#25454;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#24050;&#34987;&#35266;&#23519;&#21040;&#33021;&#22815;&#25581;&#31034;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#24182;&#22522;&#20110;&#32463;&#39564;&#25968;&#25454;&#20135;&#29983;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15027v1 Announce Type: cross  Abstract: Neural network models have shown outstanding performance and successful resolutions to complex problems in various fields. However, the majority of these models are viewed as black-box, requiring a significant amount of data for development. Consequently, in situations with limited data, constructing appropriate models becomes challenging due to the lack of transparency and scarcity of data. To tackle these challenges, this study suggests the implementation of a grey-informed neural network (GINN). The GINN ensures that the output of the neural network follows the differential equation model of the grey system, improving interpretability. Moreover, incorporating prior knowledge from grey system theory enables traditional neural networks to effectively handle small data samples. Our proposed model has been observed to uncover underlying patterns in the real world and produce reliable forecasts based on empirical data.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#29289;&#29702;&#20449;&#24687;&#25351;&#23548;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#31283;&#20581;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.15025</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#31283;&#20581;&#31526;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Conformal Prediction under Distribution Shift via Physics-Informed Structural Causal Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15025
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#29289;&#29702;&#20449;&#24687;&#25351;&#23548;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#31283;&#20581;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#21487;&#38752;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290; &#31526;&#21512;&#39044;&#27979;&#65288;CP&#65289;&#36890;&#36807;&#39044;&#27979;&#27979;&#35797;&#36755;&#20837;&#30340;&#19968;&#20010;&#38598;&#21512;&#26469;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#65292;&#24076;&#26395;&#35813;&#38598;&#21512;&#20197;&#33267;&#23569;$(1-\alpha)$&#30340;&#32622;&#20449;&#24230;&#35206;&#30422;&#30495;&#23454;&#26631;&#31614;&#12290; &#21363;&#20351;&#22312;&#26657;&#20934;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36793;&#32536;&#20998;&#24067; $P_X$ &#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#35206;&#30422;&#20063;&#21487;&#20197;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290; &#20294;&#26159;&#65292;&#23454;&#36341;&#20013;&#24456;&#24120;&#35265;&#30340;&#24773;&#20917;&#26159;&#65292;&#24403;&#26657;&#20934;&#21644;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#26465;&#20214;&#20998;&#24067; $P_{Y|X}$ &#19981;&#21516;&#26102;&#65292;&#35206;&#30422;&#23601;&#26080;&#27861;&#20445;&#35777;&#65292;&#22312;&#25152;&#26377;&#21487;&#33021;&#30340;&#32622;&#20449;&#27700;&#24179;&#19979;&#27979;&#37327;&#21644;&#26368;&#23567;&#21270;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#35206;&#30422;&#25439;&#22833;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#26657;&#20934;&#21644;&#27979;&#35797;&#31526;&#21512;&#20998;&#25968;&#30340;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#20197;&#21450;Wasserstein&#36317;&#31163;&#22312;&#21508;&#20010;&#27700;&#24179;&#19978;&#19978;&#30028;&#35206;&#30422;&#24046;&#24322;&#12290; &#21463;&#29289;&#29702;&#23398;&#22312;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#19981;&#21464;&#24615;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#20449;&#24687;&#25351;&#23548;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15025v1 Announce Type: new  Abstract: Uncertainty is critical to reliable decision-making with machine learning. Conformal prediction (CP) handles uncertainty by predicting a set on a test input, hoping the set to cover the true label with at least $(1-\alpha)$ confidence. This coverage can be guaranteed on test data even if the marginal distributions $P_X$ differ between calibration and test datasets. However, as it is common in practice, when the conditional distribution $P_{Y|X}$ is different on calibration and test data, the coverage is not guaranteed and it is essential to measure and minimize the coverage loss under distributional shift at \textit{all} possible confidence levels. To address these issues, we upper bound the coverage difference at all levels using the cumulative density functions of calibration and test conformal scores and Wasserstein distance. Inspired by the invariance of physics across data distributions, we propose a physics-informed structural caus
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#36807;&#31243;&#20013;&#19981;&#21516;&#38454;&#27573;&#33719;&#24471;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20307;&#31215;/&#20960;&#20309;&#21644;&#25439;&#22833;&#26223;&#35266;&#29305;&#24449;&#36827;&#34892;&#32463;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#35797;&#22270;&#27934;&#23519;&#36208;&#21183;&#24425;&#31080;&#20551;&#35774;&#21644;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#20013;&#30340;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.15022</link><description>&lt;p&gt;
&#36208;&#21183;&#24425;&#31080;&#20551;&#35774;&#21644;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Insights into the Lottery Ticket Hypothesis and the Iterative Magnitude Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15022
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#36807;&#31243;&#20013;&#19981;&#21516;&#38454;&#27573;&#33719;&#24471;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20307;&#31215;/&#20960;&#20309;&#21644;&#25439;&#22833;&#26223;&#35266;&#29305;&#24449;&#36827;&#34892;&#32463;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#35797;&#22270;&#27934;&#23519;&#36208;&#21183;&#24425;&#31080;&#20551;&#35774;&#21644;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#20013;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15022v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36208;&#21183;&#24425;&#31080;&#20551;&#35774;&#24378;&#35843;&#20102;&#37325;&#26032;&#35757;&#32451;&#21033;&#29992;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#36807;&#31243;&#33719;&#24471;&#30340;&#26356;&#31232;&#30095;&#32593;&#32476;&#26102;&#25152;&#20351;&#29992;&#30340;&#21021;&#22987;&#21270;&#30340;&#37325;&#35201;&#24615;&#12290;&#33267;&#20170;&#23578;&#32570;&#20047;&#20851;&#20110;&#36208;&#21183;&#24425;&#31080;&#20551;&#35774;&#20013;&#25552;&#20986;&#30340;&#29305;&#23450;&#21021;&#22987;&#21270;&#20026;&#20309;&#26356;&#26377;&#21033;&#20110;&#27867;&#21270;&#65288;&#21644;&#35757;&#32451;&#65289;&#24615;&#33021;&#30340;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#20013;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#22914;&#21098;&#26525;&#36739;&#23567;&#24133;&#24230;&#26435;&#37325;&#21644;&#36845;&#20195;&#36807;&#31243;&#30340;&#20316;&#29992;&#65292;&#23578;&#32570;&#20047;&#23436;&#20840;&#29702;&#35299;&#21644;&#35299;&#37322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#23545;&#22312;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#36807;&#31243;&#30340;&#21508;&#20010;&#38454;&#27573;&#33719;&#24471;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20307;&#31215;/&#20960;&#20309;&#21644;&#25439;&#22833;&#26223;&#35266;&#29305;&#24449;&#36827;&#34892;&#32463;&#39564;&#30740;&#31350;&#65292;&#20197;&#27934;&#23519;&#36825;&#20123;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15022v1 Announce Type: new  Abstract: Lottery ticket hypothesis for deep neural networks emphasizes the importance of initialization used to re-train the sparser networks obtained using the iterative magnitude pruning process. An explanation for why the specific initialization proposed by the lottery ticket hypothesis tends to work better in terms of generalization (and training) performance has been lacking. Moreover, the underlying principles in iterative magnitude pruning, like the pruning of smaller magnitude weights and the role of the iterative process, lack full understanding and explanation. In this work, we attempt to provide insights into these phenomena by empirically studying the volume/geometry and loss landscape characteristics of the solutions obtained at various stages of the iterative magnitude pruning process.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21271;&#27431;&#24694;&#21155;&#20908;&#23395;&#26465;&#20214;&#19979;&#36710;&#36742;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#20351;&#29992;&#21271;&#27431;&#36710;&#36742;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#31639;&#27861;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#27599;&#31181;&#26816;&#27979;&#26694;&#26550;&#30340;&#19968;&#31995;&#21015;&#22686;&#24378;&#25514;&#26045;&#12290;</title><link>https://arxiv.org/abs/2403.15017</link><description>&lt;p&gt;
&#21271;&#27431;&#22320;&#21306;&#36710;&#36742;&#26816;&#27979;&#24615;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Vehicle Detection Performance in Nordic Region
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21271;&#27431;&#24694;&#21155;&#20908;&#23395;&#26465;&#20214;&#19979;&#36710;&#36742;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#20351;&#29992;&#21271;&#27431;&#36710;&#36742;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#31639;&#27861;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#27599;&#31181;&#26816;&#27979;&#26694;&#26550;&#30340;&#19968;&#31995;&#21015;&#22686;&#24378;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#22312;&#21271;&#27431;&#22320;&#21306;&#24694;&#21155;&#20908;&#23395;&#26465;&#20214;&#19979;&#36710;&#36742;&#26816;&#27979;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#35813;&#22320;&#21306;&#20197;&#26292;&#38634;&#12289;&#20302;&#33021;&#35265;&#24230;&#21644;&#20302;&#29031;&#26126;&#20026;&#29305;&#24449;&#12290;&#30001;&#20110;&#20256;&#32479;&#36710;&#36742;&#26816;&#27979;&#26041;&#27861;&#26131;&#21463;&#29615;&#22659;&#25197;&#26354;&#21644;&#36974;&#25377;&#30340;&#24433;&#21709;&#65292;&#22312;&#36825;&#20123;&#24694;&#21155;&#26465;&#20214;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#24102;&#26469;&#20102;&#24076;&#26395;&#65292;&#28982;&#32780;&#22312;&#21271;&#27431;&#20908;&#23395;&#26816;&#27979;&#36710;&#36742;&#30340;&#29420;&#29305;&#22256;&#38590;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;&#21271;&#27431;&#36710;&#36742;&#25968;&#25454;&#38598;(NVD)&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#29790;&#20856;&#21271;&#37096;&#30340;&#26080;&#20154;&#26426;&#22270;&#20687;&#65292;&#20197;&#35780;&#20272;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#26368;&#20808;&#36827;&#30340;&#36710;&#36742;&#26816;&#27979;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#38024;&#23545;&#27599;&#20010;&#26816;&#27979;&#26694;&#26550;&#37327;&#36523;&#23450;&#21046;&#30340;&#19968;&#31995;&#21015;&#22686;&#24378;&#25514;&#26045;&#65292;&#21253;&#25324;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15017v1 Announce Type: cross  Abstract: This paper addresses the critical challenge of vehicle detection in the harsh winter conditions in the Nordic regions, characterized by heavy snowfall, reduced visibility, and low lighting. Due to their susceptibility to environmental distortions and occlusions, traditional vehicle detection methods have struggled in these adverse conditions. The advanced proposed deep learning architectures brought promise, yet the unique difficulties of detecting vehicles in Nordic winters remain inadequately addressed. This study uses the Nordic Vehicle Dataset (NVD), which has UAV images from northern Sweden, to evaluate the performance of state-of-the-art vehicle detection algorithms under challenging weather conditions. Our methodology includes a comprehensive evaluation of single-stage, two-stage, and transformer-based detectors against the NVD. We propose a series of enhancements tailored to each detection framework, including data augmentation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22810;&#28304;&#29615;&#22659;&#20013;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#26631;&#20934;K&#25240;&#20132;&#21449;&#39564;&#35777;&#21644;&#30041;&#20986;&#28304;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#26356;&#20840;&#38754;&#21644;&#30495;&#23454;&#30340;&#31934;&#30830;&#24230;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;</title><link>https://arxiv.org/abs/2403.15012</link><description>&lt;p&gt;
&#20020;&#24202;&#26426;&#22120;&#23398;&#20064;&#20013;&#22810;&#28304;&#20132;&#21449;&#39564;&#35777;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Empirical investigation of multi-source cross-validation in clinical machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22810;&#28304;&#29615;&#22659;&#20013;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#26631;&#20934;K&#25240;&#20132;&#21449;&#39564;&#35777;&#21644;&#30041;&#20986;&#28304;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#26356;&#20840;&#38754;&#21644;&#30495;&#23454;&#30340;&#31934;&#30830;&#24230;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#26159;&#22312;&#26469;&#33258;&#21333;&#19968;&#26469;&#28304;&#65288;&#22914;&#21307;&#38498;&#65289;&#30340;&#24739;&#32773;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#12290;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#21487;&#36890;&#36807;&#37325;&#22797;&#38543;&#26426;&#25286;&#20998;&#25968;&#25454;&#26469;&#20272;&#35745;&#36825;&#20123;&#27169;&#22411;&#22312;&#26469;&#33258;&#21516;&#19968;&#26469;&#28304;&#30340;&#26032;&#24739;&#32773;&#19978;&#30340;&#31934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#19982;&#37096;&#32626;&#27169;&#22411;&#21040;&#25968;&#25454;&#38598;&#20013;&#26410;&#20195;&#34920;&#30340;&#28304;&#22836;&#65288;&#22914;&#26032;&#21307;&#38498;&#65289;&#33719;&#24471;&#30340;&#31934;&#30830;&#24230;&#30456;&#27604;&#65292;&#36825;&#20123;&#20272;&#35745;&#24448;&#24448;&#36807;&#20110;&#20048;&#35266;&#12290;&#22810;&#28304;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#19981;&#26029;&#22686;&#21152;&#20026;&#36890;&#36807;&#22522;&#20110;&#28304;&#22836;&#30340;&#20132;&#21449;&#39564;&#35777;&#35774;&#35745;&#33719;&#24471;&#26356;&#20840;&#38754;&#21644;&#30495;&#23454;&#30340;&#39044;&#26399;&#31934;&#30830;&#24230;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15012v1 Announce Type: new  Abstract: Traditionally, machine learning-based clinical prediction models have been trained and evaluated on patient data from a single source, such as a hospital. Cross-validation methods can be used to estimate the accuracy of such models on new patients originating from the same source, by repeated random splitting of the data. However, such estimates tend to be highly overoptimistic when compared to accuracy obtained from deploying models to sources not represented in the dataset, such as a new hospital. The increasing availability of multi-source medical datasets provides new opportunities for obtaining more comprehensive and realistic evaluations of expected accuracy through source-level cross-validation designs.   In this study, we present a systematic empirical evaluation of standard K-fold cross-validation and leave-source-out cross-validation methods in a multi-source setting. We consider the task of electrocardiogram based cardiovascul
&lt;/p&gt;</description></item><item><title>ParFormer&#25552;&#20986;&#20102;&#24182;&#34892;&#23616;&#37096;&#20840;&#23616;&#26631;&#35760;&#28151;&#21512;&#22120;&#21644;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;&#65292;&#20248;&#21270;&#20102;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;CNN&#21644;&#26368;&#20808;&#36827;&#30340;Transformer&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.15004</link><description>&lt;p&gt;
ParFormer&#65306;&#20855;&#26377;&#24182;&#34892;&#23616;&#37096;&#20840;&#23616;&#26631;&#35760;&#28151;&#21512;&#22120;&#21644;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;&#30340;&#35270;&#35273;Transformer&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
ParFormer: Vision Transformer Baseline with Parallel Local Global Token Mixer and Convolution Attention Patch Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15004
&lt;/p&gt;
&lt;p&gt;
ParFormer&#25552;&#20986;&#20102;&#24182;&#34892;&#23616;&#37096;&#20840;&#23616;&#26631;&#35760;&#28151;&#21512;&#22120;&#21644;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;&#65292;&#20248;&#21270;&#20102;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;CNN&#21644;&#26368;&#20808;&#36827;&#30340;Transformer&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ParFormer&#20316;&#20026;&#19968;&#31181;&#22686;&#24378;&#22411;Transformer&#26550;&#26500;&#65292;&#20801;&#35768;&#23558;&#19981;&#21516;&#30340;&#26631;&#35760;&#28151;&#21512;&#22120;&#25972;&#21512;&#21040;&#21333;&#20010;&#38454;&#27573;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#12290;&#21516;&#26102;&#25972;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#25968;&#25454;&#65292;&#23454;&#29616;&#23545;&#30701;&#31243;&#21644;&#38271;&#31243;&#31354;&#38388;&#20851;&#31995;&#30340;&#31934;&#30830;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#20687;&#24179;&#31227;&#31383;&#21475;&#36825;&#26679;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#38500;&#20102;&#24182;&#34892;&#26631;&#35760;&#28151;&#21512;&#22120;&#32534;&#30721;&#22120;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;(CAPE)&#65292;&#20316;&#20026;&#26631;&#20934;&#34917;&#19969;&#23884;&#20837;&#30340;&#22686;&#24378;&#65292;&#36890;&#36807;&#21367;&#31215;&#27880;&#24847;&#21147;&#27169;&#22359;&#25913;&#36827;&#26631;&#35760;&#28151;&#21512;&#22120;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ParFormer&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29289;&#20307;&#35782;&#21035;&#31561;&#22810;&#20010;&#22797;&#26434;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20110;CNN&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;CAPE&#24050;&#34987;&#35777;&#26126;&#26377;&#30410;&#20110;&#25972;&#20307;MetaFormer&#26550;&#26500;&#65292;&#21363;&#20351;&#20351;&#29992;Id&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15004v1 Announce Type: cross  Abstract: This work presents ParFormer as an enhanced transformer architecture that allows the incorporation of different token mixers into a single stage, hence improving feature extraction capabilities. Integrating both local and global data allows for precise representation of short- and long-range spatial relationships without the need for computationally intensive methods such as shifting windows. Along with the parallel token mixer encoder, We offer the Convolutional Attention Patch Embedding (CAPE) as an enhancement of standard patch embedding to improve token mixer extraction with a convolutional attention module. Our comprehensive evaluation demonstrates that our ParFormer outperforms CNN-based and state-of-the-art transformer-based architectures in image classification and several complex tasks such as object recognition. The proposed CAPE has been demonstrated to benefit the overall MetaFormer architecture, even while utilizing the Id
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#24341;&#20837;&#26032;&#22411;&#26631;&#20934;&#21270;&#26041;&#27861;&#24182;&#20351;&#29992;&#32553;&#25918;&#37327;&#21270;&#21152;&#26435;&#65292;&#23454;&#29616;&#20102;&#22312;&#26368;&#23567;&#31934;&#24230;&#38477;&#32423;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#30340;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;</title><link>https://arxiv.org/abs/2403.14999</link><description>&lt;p&gt;
&#39764;&#27861;&#19982;&#37327;&#23376;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#20195;
&lt;/p&gt;
&lt;p&gt;
Magic for the Age of Quantized DNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14999
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#24341;&#20837;&#26032;&#22411;&#26631;&#20934;&#21270;&#26041;&#27861;&#24182;&#20351;&#29992;&#32553;&#25918;&#37327;&#21270;&#21152;&#26435;&#65292;&#23454;&#29616;&#20102;&#22312;&#26368;&#23567;&#31934;&#24230;&#38477;&#32423;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#30340;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21442;&#25968;&#25968;&#37327;&#24613;&#21095;&#22686;&#21152;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25152;&#31034;&#65292;&#20351;&#24471;&#22312;&#23567;&#35268;&#27169;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#25512;&#29702;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#23545;&#20135;&#21697;&#25972;&#21512;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#20934;&#21270;&#26041;&#27861;&#65288;&#23618;&#25209;&#26631;&#20934;&#21270;&#65289;&#65292;&#20854;&#29420;&#31435;&#20110;&#23567;&#25209;&#37327;&#22823;&#23567;&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#26399;&#38388;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24102;&#26435;&#26631;&#20934;&#21270;&#30340;&#32553;&#25918;&#37327;&#21270;&#21152;&#26435;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#30456;&#21516;&#30340;&#20989;&#25968;&#37327;&#21270;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#24212;&#29992;&#20195;&#29702;&#26799;&#24230;&#26469;&#35757;&#32451;&#26082;&#20855;&#26377;&#37327;&#21270;&#26435;&#37325;&#21448;&#20855;&#26377;&#37327;&#21270;&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#39764;&#27861;&#19982;&#37327;&#23376;&#21270;DNN&#26102;&#20195;&#65288;MaQD&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#37327;&#21270;&#26041;&#27861;&#21487;&#20197;&#22312;&#26368;&#23567;&#31934;&#24230;&#38477;&#32423;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14999v1 Announce Type: cross  Abstract: Recently, the number of parameters in DNNs has explosively increased, as exemplified by LLMs (Large Language Models), making inference on small-scale computers more difficult. Model compression technology is, therefore, essential for integration into products. In this paper, we propose a method of quantization-aware training. We introduce a novel normalization (Layer-Batch Normalization) that is independent of the mini-batch size and does not require any additional computation cost during inference. Then, we quantize the weights by the scaled round-clip function with the weight standardization. We also quantize activation functions using the same function and apply surrogate gradients to train the model with both quantized weights and the quantized activation functions. We call this method Magic for the age of Quantised DNNs (MaQD). Experimental results show that our quantization method can be achieved with minimal accuracy degradation
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20013;&#20351;&#29992;&#20998;&#27573;&#32447;&#24615;&#27969;&#24418;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#39640;&#32500;&#25968;&#25454;&#27969;&#24418;&#26469;&#25913;&#21892;&#30456;&#20284;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26080;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14977</link><description>&lt;p&gt;
&#20998;&#27573;&#32447;&#24615;&#27969;&#24418;&#22312;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Piecewise-Linear Manifolds for Deep Metric Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14977
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20013;&#20351;&#29992;&#20998;&#27573;&#32447;&#24615;&#27969;&#24418;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#39640;&#32500;&#25968;&#25454;&#27969;&#24418;&#26469;&#25913;&#21892;&#30456;&#20284;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26080;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65288;UDML&#65289;&#33268;&#21147;&#20110;&#20165;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#23398;&#20064;&#35821;&#20041;&#34920;&#31034;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20998;&#27573;&#32447;&#24615;&#36924;&#36817;&#27169;&#22411;&#39640;&#32500;&#25968;&#25454;&#27969;&#24418;&#65292;&#20854;&#20013;&#27599;&#20010;&#20302;&#32500;&#32447;&#24615;&#29255;&#27573;&#36817;&#20284;&#20110;&#28857;&#30340;&#23567;&#37051;&#22495;&#20869;&#30340;&#25968;&#25454;&#27969;&#24418;&#12290;&#36825;&#20123;&#37051;&#22495;&#29992;&#20110;&#20272;&#35745;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#36825;&#31181;&#30456;&#20284;&#24615;&#20272;&#35745;&#19982;&#22522;&#20934;&#30456;&#27604;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#22320;&#38754;&#30495;&#30456;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#21487;&#20197;&#20351;&#29992;&#22312;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#20195;&#29702;&#26469;&#27169;&#25311;&#20998;&#27573;&#32447;&#24615;&#27969;&#24418;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14977v1 Announce Type: cross  Abstract: Unsupervised deep metric learning (UDML) focuses on learning a semantic representation space using only unlabeled data. This challenging problem requires accurately estimating the similarity between data points, which is used to supervise a deep network. For this purpose, we propose to model the high-dimensional data manifold using a piecewise-linear approximation, with each low-dimensional linear piece approximating the data manifold in a small neighborhood of a point. These neighborhoods are used to estimate similarity between data points. We empirically show that this similarity estimate correlates better with the ground truth than the similarity estimates of current state-of-the-art techniques. We also show that proxies, commonly used in supervised metric learning, can be used to model the piecewise-linear manifold in an unsupervised setting, helping improve performance. Our method outperforms existing unsupervised metric learning 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26032;&#30340;pose-estimation&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;SSL&#20960;&#20309;&#34920;&#31034;&#65292;&#25552;&#20986;&#26080;&#30417;&#30563;&#36712;&#36857;&#27491;&#21017;&#21270;&#25439;&#22833;&#26469;&#22686;&#24378;SSL&#20960;&#20309;&#34920;&#31034;&#65292;&#22312;&#23039;&#21183;&#20272;&#35745;&#24615;&#33021;&#19978;&#21462;&#24471;10-20%&#30340;&#25552;&#21319;&#65292;&#24182;&#25552;&#39640;&#20102;4%&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14973</link><description>&lt;p&gt;
&#36712;&#36857;&#27491;&#21017;&#21270;&#22686;&#24378;&#33258;&#30417;&#30563;&#20960;&#20309;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Trajectory Regularization Enhances Self-Supervised Geometric Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14973
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26032;&#30340;pose-estimation&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;SSL&#20960;&#20309;&#34920;&#31034;&#65292;&#25552;&#20986;&#26080;&#30417;&#30563;&#36712;&#36857;&#27491;&#21017;&#21270;&#25439;&#22833;&#26469;&#22686;&#24378;SSL&#20960;&#20309;&#34920;&#31034;&#65292;&#22312;&#23039;&#21183;&#20272;&#35745;&#24615;&#33021;&#19978;&#21462;&#24471;10-20%&#30340;&#25552;&#21319;&#65292;&#24182;&#25552;&#39640;&#20102;4%&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#20027;&#35201;&#20851;&#27880;&#35821;&#20041;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#29992;&#20110;&#35780;&#20272;&#20960;&#20309;&#34920;&#31034;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26041;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23039;&#21183;&#20272;&#35745;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;SSL&#20960;&#20309;&#34920;&#31034;&#65292;&#35813;&#22522;&#20934;&#35201;&#27714;&#22312;&#27809;&#26377;&#35821;&#20041;&#25110;&#23039;&#21183;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#35821;&#20041;&#21644;&#20960;&#20309;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#29087;&#32451;&#24230;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22686;&#24378;SSL&#20960;&#20309;&#34920;&#31034;&#32780;&#19981;&#29306;&#29298;&#35821;&#20041;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#21033;&#29992;&#20013;&#38388;&#23618;&#34920;&#31034;&#21487;&#20197;&#23558;&#23039;&#21183;&#20272;&#35745;&#24615;&#33021;&#25552;&#39640;10-20&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#36712;&#36857;&#27491;&#21017;&#21270;&#25439;&#22833;&#65292;&#35813;&#25439;&#22833;&#39069;&#22806;&#25552;&#39640;&#20102;4&#65285;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#22312;&#36234;&#30028;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14973v1 Announce Type: cross  Abstract: Self-supervised learning (SSL) has proven effective in learning high-quality representations for various downstream tasks, with a primary focus on semantic tasks. However, its application in geometric tasks remains underexplored, partially due to the absence of a standardized evaluation method for geometric representations. To address this gap, we introduce a new pose-estimation benchmark for assessing SSL geometric representations, which demands training without semantic or pose labels and achieving proficiency in both semantic and geometric downstream tasks. On this benchmark, we study enhancing SSL geometric representations without sacrificing semantic classification accuracy. We find that leveraging mid-layer representations improves pose-estimation performance by 10-20%. Further, we introduce an unsupervised trajectory-regularization loss, which improves performance by an additional 4% and improves generalization ability on out-of
&lt;/p&gt;</description></item><item><title>Adapprox&#26159;&#19968;&#31181;&#37319;&#29992;&#38543;&#26426;&#20302;&#31209;&#30697;&#38453;&#36817;&#20284;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#21644;&#20934;&#30830;&#22320;&#36924;&#36817;Adam&#20248;&#21270;&#31639;&#27861;&#30340;&#20108;&#38454;&#30697;&#12290;&#22312;GPT-2&#30340;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;Adapprox&#30456;&#27604;AdamW&#33021;&#22815;&#23454;&#29616;34.5%&#33267;49.9%&#21644;33.8%&#33267;49.9%&#30340;&#20869;&#23384;&#33410;&#32422;&#65292;&#24182;&#36890;&#36807;&#20313;&#24358;&#30456;&#20284;&#24615;&#25351;&#23548;&#31574;&#30053;&#25552;&#39640;&#20102;&#31283;&#23450;&#24615;&#21644;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.14958</link><description>&lt;p&gt;
Adapprox:&#33258;&#36866;&#24212;&#36817;&#20284;&#22312;Adam&#20248;&#21270;&#20013;&#20351;&#29992;&#38543;&#26426;&#20302;&#31209;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14958
&lt;/p&gt;
&lt;p&gt;
Adapprox&#26159;&#19968;&#31181;&#37319;&#29992;&#38543;&#26426;&#20302;&#31209;&#30697;&#38453;&#36817;&#20284;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#21644;&#20934;&#30830;&#22320;&#36924;&#36817;Adam&#20248;&#21270;&#31639;&#27861;&#30340;&#20108;&#38454;&#30697;&#12290;&#22312;GPT-2&#30340;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;Adapprox&#30456;&#27604;AdamW&#33021;&#22815;&#23454;&#29616;34.5%&#33267;49.9%&#21644;33.8%&#33267;49.9%&#30340;&#20869;&#23384;&#33410;&#32422;&#65292;&#24182;&#36890;&#36807;&#20313;&#24358;&#30456;&#20284;&#24615;&#25351;&#23548;&#31574;&#30053;&#25552;&#39640;&#20102;&#31283;&#23450;&#24615;&#21644;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35268;&#27169;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#35832;&#22914;Adam&#20043;&#31867;&#30340;&#20248;&#21270;&#22120;&#22312;&#23384;&#20648;&#19968;&#12289;&#20108;&#38454;&#30697;&#25968;&#25454;&#26102;&#36935;&#21040;&#20102;&#26174;&#33879;&#30340;&#20869;&#23384;&#28040;&#32791;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#22914;Adafactor&#21644;CAME&#36890;&#24120;&#36890;&#36807;&#20854;&#30697;&#38453;&#22240;&#24335;&#20998;&#35299;&#25216;&#26415;&#26469;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Adapprox&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#38543;&#26426;&#20302;&#31209;&#30697;&#38453;&#36817;&#20284;&#26469;&#26356;&#26377;&#25928;&#21644;&#20934;&#30830;&#22320;&#36817;&#20284;Adam&#30340;&#20108;&#38454;&#30697;&#12290;Adapprox&#20855;&#26377;&#33258;&#36866;&#24212;&#31209;&#36873;&#25321;&#26426;&#21046;&#65292;&#31934;&#32454;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#20869;&#23384;&#25928;&#29575;&#65292;&#24182;&#21253;&#25324;&#19968;&#20010;&#21487;&#36873;&#30340;&#20313;&#24358;&#30456;&#20284;&#24615;&#25351;&#23548;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#31283;&#23450;&#24615;&#24182;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;GPT-2&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;Adapprox&#36890;&#36807;&#23454;&#29616;&#23545;117M&#21644;345M&#27169;&#22411;&#30340;34.5%&#33267;49.9%&#21644;33.8%&#33267;49.9%&#20869;&#23384;&#33410;&#32422;&#65288;&#20998;&#21035;&#21551;&#29992;&#20102;&#31532;&#19968;&#38454;&#30697;&#65289;&#65292;&#36229;&#36234;&#20102;AdamW&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#36825;&#20123;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14958v1 Announce Type: cross  Abstract: As deep learning models exponentially increase in size, optimizers such as Adam encounter significant memory consumption challenges due to the storage of first and second moment data. Current memory-efficient methods like Adafactor and CAME often compromise accuracy with their matrix factorization techniques. Addressing this, we introduce Adapprox, a novel approach that employs randomized low-rank matrix approximation for a more effective and accurate approximation of Adam's second moment. Adapprox features an adaptive rank selection mechanism, finely balancing accuracy and memory efficiency, and includes an optional cosine similarity guidance strategy to enhance stability and expedite convergence. In GPT-2 training and downstream tasks, Adapprox surpasses AdamW by achieving 34.5% to 49.9% and 33.8% to 49.9% memory savings for the 117M and 345M models, respectively, with the first moment enabled, and further increases these savings wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#22270;&#21387;&#32553;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#22270;&#31070;&#32463;&#32593;&#32476;&#25152;&#24102;&#26469;&#30340;&#19981;&#24517;&#35201;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14951</link><description>&lt;p&gt;
&#31616;&#21333;&#22270;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Simple Graph Condensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14951
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#22270;&#21387;&#32553;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#22270;&#31070;&#32463;&#32593;&#32476;&#25152;&#24102;&#26469;&#30340;&#19981;&#24517;&#35201;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#19978;&#32321;&#37325;&#30340;&#35757;&#32451;&#25104;&#26412;&#24050;&#32463;&#24341;&#36215;&#20102;&#23545;&#22270;&#21387;&#32553;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#28041;&#21450;&#35843;&#25972;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23567;&#23610;&#24230;&#21387;&#32553;&#22270;&#19978;&#30340;&#35757;&#32451;&#20197;&#22312;&#22823;&#35268;&#27169;&#21407;&#22987;&#22270;&#19978;&#20351;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#35843;&#25972;&#21387;&#32553;&#22270;&#21644;&#21407;&#22987;&#22270;&#20043;&#38388;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#22914;&#26799;&#24230;&#12289;GNNs&#30340;&#20998;&#24067;&#21644;&#36712;&#36857;&#65292;&#20174;&#32780;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22797;&#26434;&#25351;&#26631;&#38656;&#35201;&#22797;&#26434;&#30340;&#35745;&#31639;&#65292;&#21487;&#33021;&#20250;&#24178;&#25200;&#21387;&#32553;&#22270;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20351;&#24471;&#21387;&#32553;&#36807;&#31243;&#38750;&#24120;&#32321;&#37325;&#21644;&#19981;&#31283;&#23450;&#12290;&#22312;&#21508;&#20010;&#39046;&#22495;&#31616;&#21270;&#27169;&#22411;&#21462;&#24471;&#25104;&#21151;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#22270;&#21387;&#32553;&#20013;&#30340;&#25351;&#26631;&#23545;&#20934;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#20174;GNNs&#32487;&#25215;&#30340;&#19981;&#24517;&#35201;&#22797;&#26434;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#28040;&#38500;&#22806;&#37096;&#21442;&#25968;&#65292;&#20165;&#20445;&#30041;&#30446;&#26631;&#30340;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14951v1 Announce Type: cross  Abstract: The burdensome training costs on large-scale graphs have aroused significant interest in graph condensation, which involves tuning Graph Neural Networks (GNNs) on a small condensed graph for use on the large-scale original graph. Existing methods primarily focus on aligning key metrics between the condensed and original graphs, such as gradients, distribution and trajectory of GNNs, yielding satisfactory performance on downstream tasks. However, these complex metrics necessitate intricate computations and can potentially disrupt the optimization process of the condensation graph, making the condensation process highly demanding and unstable. Motivated by the recent success of simplified models in various fields, we propose a simplified approach to metric alignment in graph condensation, aiming to reduce unnecessary complexity inherited from GNNs. In our approach, we eliminate external parameters and exclusively retain the target conden
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KnowLA&#30340;&#30693;&#35782;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25554;&#20837;&#33258;&#36866;&#24212;&#23618;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#65292;&#33021;&#22815;&#25552;&#21319;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14950</link><description>&lt;p&gt;
KnowLA&#65306;&#21033;&#29992;&#30693;&#35782;&#33258;&#36866;&#24212;&#25552;&#21319;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KnowLA&#30340;&#30693;&#35782;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25554;&#20837;&#33258;&#36866;&#24212;&#23618;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#65292;&#33021;&#22815;&#25552;&#21319;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26159;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#23884;&#20837;&#26469;&#25913;&#21892;PEFT&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;KnowLA&#30340;&#30693;&#35782;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;&#23427;&#22312;LLM&#20013;&#25554;&#20837;&#19968;&#20010;&#33258;&#36866;&#24212;&#23618;&#65292;&#23558;&#36755;&#20837;&#25991;&#26412;&#20013;&#20986;&#29616;&#30340;&#23454;&#20307;&#30340;&#23884;&#20837;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;&#33258;&#36866;&#24212;&#23618;&#19982;LoRA&#22312;&#25351;&#23548;&#25968;&#25454;&#19978;&#32452;&#21512;&#35757;&#32451;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;LLMs&#21644;&#19977;&#20010;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#30340;&#20845;&#39033;&#22522;&#20934;&#23454;&#39564;&#34920;&#26126;&#20102;KnowLA&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; \modelname &#33021;&#22815;&#24110;&#21161;&#28608;&#27963;LLM&#20013;&#30340;&#30456;&#20851;&#21442;&#25968;&#21270;&#30693;&#35782;&#20197;&#22238;&#31572;&#38382;&#39064;&#65292;&#32780;&#19981;&#25913;&#21464;&#20854;&#21442;&#25968;&#25110;&#36755;&#20837;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14950v1 Announce Type: new  Abstract: Parameter-efficient finetuning (PEFT) is a key technique for adapting large language models (LLMs) to downstream tasks. In this paper, we study leveraging knowledge graph embeddings to improve the effectiveness of PEFT. We propose a knowledgeable adaptation method called KnowLA. It inserts an adaptation layer into an LLM to integrate the embeddings of entities appearing in the input text. The adaptation layer is trained in combination with LoRA on instruction data. Experiments on six benchmarks with two popular LLMs and three knowledge graphs demonstrate the effectiveness and robustness of KnowLA. We show that \modelname can help activate the relevant parameterized knowledge in an LLM to answer a question without changing its parameters or input prompts.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Concept Drift Detection and Adaptation&#65288;D3A&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#28418;&#31227;&#27010;&#24565;&#24182;&#22312;&#26816;&#27979;&#21518;&#31215;&#26497;&#35843;&#25972;&#27169;&#22411;&#26469;&#35299;&#20915;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#36935;&#21040;&#30340;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14949</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#35299;&#20915;&#27010;&#24565;&#28418;&#31227;&#65306;&#20808;&#26816;&#27979;&#20877;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Addressing Concept Shift in Online Time Series Forecasting: Detect-then-Adapt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14949
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Concept Drift Detection and Adaptation&#65288;D3A&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#28418;&#31227;&#27010;&#24565;&#24182;&#22312;&#26816;&#27979;&#21518;&#31215;&#26497;&#35843;&#25972;&#27169;&#22411;&#26469;&#35299;&#20915;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#36935;&#21040;&#30340;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#22312;&#32447;&#26356;&#26032;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#27969;&#25968;&#25454;&#35843;&#25972;&#39044;&#27979;&#27169;&#22411;&#26469;&#35299;&#20915;&#27010;&#24565;&#28418;&#31227;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#32463;&#21457;&#23637;&#20102;&#35768;&#22810;&#31639;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#31639;&#27861;&#20391;&#37325;&#20110;&#27169;&#22411;&#35774;&#35745;&#21644;&#26356;&#26032;&#12290;&#23454;&#38469;&#19978;&#65292;&#35768;&#22810;&#26041;&#27861;&#22312;&#38754;&#23545;&#38543;&#26102;&#38388;&#32047;&#31215;&#30340;&#27010;&#24565;&#28418;&#31227;&#26102;&#65292;&#24456;&#38590;&#20445;&#25345;&#25345;&#32493;&#30340;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#21644;&#36866;&#24212;&#65288;D3A&#65289;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#26816;&#27979;&#28418;&#31227;&#27010;&#24565;&#65292;&#28982;&#21518;&#22312;&#26816;&#27979;&#21518;&#31215;&#26497;&#35843;&#25972;&#24403;&#21069;&#27169;&#22411;&#20197;&#36866;&#24212;&#28418;&#31227;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#12290;&#20026;&#20102;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#36866;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#23558;&#39640;&#26031;&#22122;&#22768;&#24341;&#20837;&#29616;&#26377;&#35757;&#32451;&#23454;&#20363;&#12290;&#36825;&#26377;&#21161;&#20110;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#24046;&#36317;&#65292;&#36825;&#26159;&#24433;&#21709;&#35757;&#32451; - &#27979;&#35797;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14949v1 Announce Type: new  Abstract: Online updating of time series forecasting models aims to tackle the challenge of concept drifting by adjusting forecasting models based on streaming data. While numerous algorithms have been developed, most of them focus on model design and updating. In practice, many of these methods struggle with continuous performance regression in the face of accumulated concept drifts over time. To address this limitation, we present a novel approach, Concept \textbf{D}rift \textbf{D}etection an\textbf{D} \textbf{A}daptation (D3A), that first detects drifting conception and then aggressively adapts the current model to the drifted concepts after the detection for rapid adaption. To best harness the utility of historical data for model adaptation, we propose a data augmentation strategy introducing Gaussian noise into existing training instances. It helps mitigate the data distribution gap, a critical factor contributing to train-test performance in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#36716;&#25442;&#30697;&#38453;&#23558;$ W_0 $&#36716;&#25442;&#20026;&#20302;&#31209;&#30697;&#38453;&#30340;&#20851;&#31995;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#21333;&#19968;&#32447;&#24615;&#23618;&#21487;&#20197;&#29983;&#25104;&#20219;&#21153;&#33258;&#36866;&#24212;&#30340;&#20302;&#31209;&#30697;&#38453;&#12290;</title><link>https://arxiv.org/abs/2403.14946</link><description>&lt;p&gt;
&#19968;&#20010;&#32447;&#24615;&#23618;&#29983;&#25104;&#20219;&#21153;&#33258;&#36866;&#24212;&#20302;&#31209;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
A Single Linear Layer Yields Task-Adapted Low-Rank Matrices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14946
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#36716;&#25442;&#30697;&#38453;&#23558;$ W_0 $&#36716;&#25442;&#20026;&#20302;&#31209;&#30697;&#38453;&#30340;&#20851;&#31995;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#21333;&#19968;&#32447;&#24615;&#23618;&#21487;&#20197;&#29983;&#25104;&#20219;&#21153;&#33258;&#36866;&#24212;&#30340;&#20302;&#31209;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#30001;&#20004;&#20010;&#20302;&#31209;&#30697;&#38453;$ A $&#21644;$ B $&#32452;&#25104;&#30340;&#22686;&#37327;&#30697;&#38453;$ \Delta W $&#26356;&#26032;&#21021;&#22987;&#26435;&#37325;&#30697;&#38453;$ W_0 $&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;$ W_0 $&#21644;$ \Delta W $&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#28145;&#20837;&#25506;&#35752;$ W_0 $&#19982;&#20302;&#31209;&#30697;&#38453;$ A $&#21644;$ B $&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#36827;&#19968;&#27493;&#29702;&#35299;LoRA&#30340;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#23558;$ W_0 $&#36716;&#25442;&#20026;&#20302;&#31209;&#30697;&#38453;&#30340;&#36716;&#25442;&#30697;&#38453;&#65292;&#20854;&#20013;&#34164;&#21547;&#20102;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#36716;&#25442;&#30697;&#38453;&#22312;&#27599;&#19968;&#23618;&#20043;&#38388;&#26159;&#30456;&#20284;&#30340;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20551;&#35774;&#19968;&#20010;&#21333;&#19968;&#32447;&#24615;&#23618;&#65292;&#23558;&#27599;&#19968;&#23618;&#30340;$ W_0 $&#20316;&#20026;&#36755;&#20837;&#65292;&#21487;&#20197;&#29983;&#25104;&#20219;&#21153;&#33258;&#36866;&#24212;&#30340;&#20302;&#31209;&#30697;&#38453;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#26377;&#26465;&#20214;&#21442;&#25968;&#21270;&#30340;LoRA (CondLoRA) &#26041;&#27861;&#65292;&#26469;&#26356;&#26032;&#21021;&#22987;&#26435;&#37325;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14946v1 Announce Type: cross  Abstract: Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning (PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix $\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study suggested that there is correlation between $W_0$ and $\Delta W$. In this study, we aim to delve deeper into relationships between $W_0$ and low-rank matrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular, we analyze a conversion matrix that transform $W_0$ into low-rank matrices, which encapsulates information about the relationships. Our analysis reveals that the conversion matrices are similar across each layer. Inspired by these findings, we hypothesize that a single linear layer, which takes each layer's $W_0$ as input, can yield task-adapted low-rank matrices. To confirm this hypothesis, we devise a method named Conditionally Parameterized LoRA (CondLoRA) that updates initial weig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;GraphMLP&#65292;&#22522;&#20110;&#22270;&#32467;&#26500;&#21644;MLP&#32593;&#32476;&#65292;&#22312;&#36710;&#36947;&#32423;&#20132;&#36890;&#39044;&#27979;&#20013;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#31354;&#38388;&#25299;&#25169;&#32467;&#26500;&#21644;&#39044;&#27979;&#20219;&#21153;&#65292;&#24110;&#21161;&#31361;&#30772;&#20102;&#29616;&#26377;&#35780;&#20272;&#26631;&#20934;&#21644;&#25968;&#25454;&#20844;&#24320;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.14941</link><description>&lt;p&gt;
&#20174;&#22270;&#32467;&#26500;&#35282;&#24230;&#32479;&#19968;&#36710;&#36947;&#32423;&#20132;&#36890;&#39044;&#27979;&#65306;&#22522;&#20934;&#21644;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Unifying Lane-Level Traffic Prediction from a Graph Structural Perspective: Benchmark and Baseline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;GraphMLP&#65292;&#22522;&#20110;&#22270;&#32467;&#26500;&#21644;MLP&#32593;&#32476;&#65292;&#22312;&#36710;&#36947;&#32423;&#20132;&#36890;&#39044;&#27979;&#20013;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#31354;&#38388;&#25299;&#25169;&#32467;&#26500;&#21644;&#39044;&#27979;&#20219;&#21153;&#65292;&#24110;&#21161;&#31361;&#30772;&#20102;&#29616;&#26377;&#35780;&#20272;&#26631;&#20934;&#21644;&#25968;&#25454;&#20844;&#24320;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#28966;&#28857;&#21644;&#20851;&#38190;&#39046;&#22495;&#65292;&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#26082;&#35265;&#35777;&#20102;&#20174;&#22478;&#24066;&#32423;&#21040;&#36947;&#36335;&#32423;&#39044;&#27979;&#21462;&#24471;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#38543;&#30528;&#36710;&#36742;&#23545;&#19968;&#20999;&#65288;V2X&#65289;&#25216;&#26415;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#20132;&#36890;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#36947;&#36335;&#32423;&#20132;&#36890;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#19981;&#21487;&#25110;&#32570;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#36827;&#23637;&#21463;&#21040;&#20102;&#20840;&#38754;&#21644;&#32479;&#19968;&#30340;&#35780;&#20272;&#26631;&#20934;&#30340;&#32570;&#20047;&#20197;&#21450;&#26377;&#38480;&#30340;&#20844;&#24320;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#38459;&#30861;&#12290;&#26412;&#25991;&#23545;&#36710;&#36947;&#32423;&#20132;&#36890;&#39044;&#27979;&#20013;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#31354;&#38388;&#25299;&#25169;&#32467;&#26500;&#21644;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#32467;&#26500;&#21644;MLP&#32593;&#32476;&#30340;&#31616;&#21333;&#22522;&#32447;&#27169;&#22411;GraphMLP&#12290;&#25105;&#20204;&#22797;&#21046;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#23578;&#19981;&#20844;&#24320;&#30340;&#20195;&#30721;&#65292;&#24182;&#22522;&#20110;&#27492;&#20805;&#20998;&#32780;&#20844;&#27491;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14941v1 Announce Type: cross  Abstract: Traffic prediction has long been a focal and pivotal area in research, witnessing both significant strides from city-level to road-level predictions in recent years. With the advancement of Vehicle-to-Everything (V2X) technologies, autonomous driving, and large-scale models in the traffic domain, lane-level traffic prediction has emerged as an indispensable direction. However, further progress in this field is hindered by the absence of comprehensive and unified evaluation standards, coupled with limited public availability of data and code. This paper extensively analyzes and categorizes existing research in lane-level traffic prediction, establishes a unified spatial topology structure and prediction tasks, and introduces a simple baseline model, GraphMLP, based on graph structure and MLP networks. We have replicated codes not publicly available in existing studies and, based on this, thoroughly and fairly assessed various models in 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#22810;&#27169;&#24577;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#24182;&#23581;&#35797;&#23558;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#25552;&#39640;&#24739;&#32773;&#21307;&#30103;&#21382;&#21490;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14926</link><description>&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#22810;&#27169;&#24577;&#20998;&#26512;&#19978;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning on Multimodal Analysis of Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14926
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#22810;&#27169;&#24577;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#24182;&#23581;&#35797;&#23558;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#25552;&#39640;&#24739;&#32773;&#21307;&#30103;&#21382;&#21490;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#31995;&#32479;&#21253;&#21547;&#22823;&#37327;&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#65292;&#21253;&#25324;&#32467;&#26500;&#21270;&#25968;&#25454;&#22914;&#20020;&#24202;&#32534;&#30721;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#22914;&#20020;&#24202;&#31508;&#35760;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#38024;&#23545;EHR&#30340;&#30740;&#31350;&#20256;&#32479;&#19978;&#35201;&#20040;&#38598;&#20013;&#20110;&#20010;&#21035;&#27169;&#24577;&#65292;&#35201;&#20040;&#20197;&#19968;&#31181;&#30456;&#24403;&#31895;&#31961;&#30340;&#26041;&#24335;&#21512;&#24182;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#23558;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#35270;&#20026;&#21333;&#29420;&#23454;&#20307;&#65292;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#22266;&#26377;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20004;&#20010;&#37325;&#35201;&#30340;&#27169;&#24577;&#21253;&#21547;&#20020;&#24202;&#30456;&#20851;&#12289;&#23494;&#20999;&#30456;&#20851;&#21644;&#20114;&#34917;&#30340;&#20581;&#24247;&#20449;&#24687;&#12290;&#36890;&#36807;&#32852;&#21512;&#20998;&#26512;&#36825;&#20004;&#31181;&#25968;&#25454;&#27169;&#24577;&#21487;&#20197;&#25429;&#25417;&#21040;&#24739;&#32773;&#21307;&#30103;&#21382;&#21490;&#30340;&#26356;&#23436;&#25972;&#30011;&#38754;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#22312;&#35270;&#35273;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#22810;&#27169;&#24577;EHR&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#22312;&#29702;&#35770;&#29702;&#35299;&#26041;&#38754;&#65292;&#20854;&#28508;&#21147;&#20173;&#26410;&#20805;&#20998;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14926v1 Announce Type: cross  Abstract: Electronic health record (EHR) systems contain a wealth of multimodal clinical data including structured data like clinical codes and unstructured data such as clinical notes. However, many existing EHR-focused studies has traditionally either concentrated on an individual modality or merged different modalities in a rather rudimentary fashion. This approach often results in the perception of structured and unstructured data as separate entities, neglecting the inherent synergy between them. Specifically, the two important modalities contain clinically relevant, inextricably linked and complementary health information. A more complete picture of a patient's medical history is captured by the joint analysis of the two modalities of data. Despite the great success of multimodal contrastive learning on vision-language, its potential remains under-explored in the realm of multimodal EHR, particularly in terms of its theoretical understandi
&lt;/p&gt;</description></item><item><title>CODA &#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#32422;&#25104;&#26412;&#30340;&#27979;&#35797;&#26102;&#22495;&#33258;&#36866;&#24212;&#26426;&#21046;&#65292;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#29702;&#35770;&#22788;&#29702;&#23454;&#26102;&#28418;&#31227;&#65292;&#23454;&#29616;&#22312;&#35774;&#22791;&#19978;&#30452;&#25509;&#36827;&#34892;&#25104;&#26412;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#65292;&#24182;&#20445;&#30041;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#26377;&#24847;&#20041;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.14922</link><description>&lt;p&gt;
CODA&#65306;&#19968;&#31181;&#29992;&#20110;HAR&#30340;&#33410;&#32422;&#25104;&#26412;&#30340;&#27979;&#35797;&#26102;&#22495;&#33258;&#36866;&#24212;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
CODA: A COst-efficient Test-time Domain Adaptation Mechanism for HAR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14922
&lt;/p&gt;
&lt;p&gt;
CODA &#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#32422;&#25104;&#26412;&#30340;&#27979;&#35797;&#26102;&#22495;&#33258;&#36866;&#24212;&#26426;&#21046;&#65292;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#29702;&#35770;&#22788;&#29702;&#23454;&#26102;&#28418;&#31227;&#65292;&#23454;&#29616;&#22312;&#35774;&#22791;&#19978;&#30452;&#25509;&#36827;&#34892;&#25104;&#26412;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#65292;&#24182;&#20445;&#30041;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#26377;&#24847;&#20041;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31227;&#21160;&#24863;&#30693;&#30340;&#26032;&#20852;&#30740;&#31350;&#23548;&#33268;&#20102;&#22686;&#24378;&#20154;&#31867;&#26085;&#24120;&#29983;&#27963;&#30340;&#26032;&#22411;&#22330;&#26223;&#65292;&#20294;&#26159;&#21160;&#24577;&#30340;&#20351;&#29992;&#26465;&#20214;&#32463;&#24120;&#23548;&#33268;&#31995;&#32479;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#37096;&#32626;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#27425;&#24615;&#33258;&#36866;&#24212;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#24448;&#24448;&#38590;&#20197;&#30830;&#20445;&#38024;&#23545;&#20154;&#31867;&#24863;&#30693;&#22330;&#26223;&#20013;&#19981;&#30830;&#23450;&#28418;&#31227;&#26465;&#20214;&#30340;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CODA&#65292;&#19968;&#31181;&#38024;&#23545;&#31227;&#21160;&#24863;&#30693;&#30340;&#33410;&#32422;&#25104;&#26412;&#22495;&#33258;&#36866;&#24212;&#26426;&#21046;&#65292;&#20174;&#25968;&#25454;&#20998;&#24067;&#35282;&#24230;&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#29702;&#35770;&#35299;&#20915;&#23454;&#26102;&#28418;&#31227;&#65292;&#20197;&#30830;&#20445;&#22312;&#35774;&#22791;&#19978;&#30452;&#25509;&#36827;&#34892;&#25104;&#26412;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#12290;&#36890;&#36807;&#32467;&#21512;&#32858;&#31867;&#25439;&#22833;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;CODA&#22312;&#25104;&#26412;&#25928;&#30410;&#30340;&#23454;&#20363;&#32423;&#26356;&#26032;&#36807;&#31243;&#20013;&#20445;&#30041;&#19981;&#21516;&#32858;&#31867;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20445;&#30041;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#26377;&#24847;&#20041;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14922v1 Announce Type: new  Abstract: In recent years, emerging research on mobile sensing has led to novel scenarios that enhance daily life for humans, but dynamic usage conditions often result in performance degradation when systems are deployed in real-world settings. Existing solutions typically employ one-off adaptation schemes based on neural networks, which struggle to ensure robustness against uncertain drifting conditions in human-centric sensing scenarios. In this paper, we propose CODA, a COst-efficient Domain Adaptation mechanism for mobile sensing that addresses real-time drifts from the data distribution perspective with active learning theory, ensuring cost-efficient adaptation directly on the device. By incorporating a clustering loss and importance-weighted active learning algorithm, CODA retains the relationship between different clusters during cost-effective instance-level updates, preserving meaningful structure within the data distribution. We also sho
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#26085;&#26412;&#20061;&#24030;&#31992;&#23798;&#22825;&#27668;&#39044;&#27979;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#65292;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14918</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65306;&#20197;&#31992;&#23798;&#20026;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based method for weather forecasting: A case study in Itoshima
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#26085;&#26412;&#20061;&#24030;&#31992;&#23798;&#22825;&#27668;&#39044;&#27979;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#65292;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#22825;&#27668;&#39044;&#27979;&#23545;&#20110;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#24341;&#36215;&#20102;&#22823;&#37327;&#30340;&#31185;&#23398;&#21644;&#31038;&#20250;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22825;&#27668;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#32473;&#20934;&#30830;&#39044;&#27979;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#29305;&#21035;&#38024;&#23545;&#26085;&#26412;&#20061;&#24030;&#31992;&#23798;&#22825;&#27668;&#39044;&#27979;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#30340;&#26550;&#26500;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#35832;&#22914;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#31561;&#29616;&#26377;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14918v1 Announce Type: new  Abstract: Accurate weather forecasting is of paramount importance for a wide range of practical applications, drawing substantial scientific and societal interest. However, the intricacies of weather systems pose substantial challenges to accurate predictions. This research introduces a multilayer perceptron model tailored for weather forecasting in Itoshima, Kyushu, Japan. Our meticulously designed architecture demonstrates superior performance compared to existing models, surpassing benchmarks such as Long Short-Term Memory and Recurrent Neural Networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26680;&#26041;&#27861;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#24179;&#22343;&#22330;&#26497;&#38480;&#19979;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#27604;&#20219;&#20309;&#26680;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#20010;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#24182;&#38598;&#65292;&#24182;&#19988;&#31070;&#32463;&#32593;&#32476;&#20250;&#33719;&#24471;&#19982;&#30446;&#26631;&#20989;&#25968;&#23545;&#40784;&#30340;&#25968;&#25454;&#30456;&#20851;&#26680;&#12290;</title><link>https://arxiv.org/abs/2403.14917</link><description>&lt;p&gt;
&#20174;&#26680;&#26041;&#27861;&#30340;&#35282;&#24230;&#23545;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24179;&#22343;&#22330;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Mean-field Analysis on Two-layer Neural Networks from a Kernel Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26680;&#26041;&#27861;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#24179;&#22343;&#22330;&#26497;&#38480;&#19979;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#27604;&#20219;&#20309;&#26680;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#20010;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#24182;&#38598;&#65292;&#24182;&#19988;&#31070;&#32463;&#32593;&#32476;&#20250;&#33719;&#24471;&#19982;&#30446;&#26631;&#20989;&#25968;&#23545;&#40784;&#30340;&#25968;&#25454;&#30456;&#20851;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26680;&#26041;&#27861;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#24179;&#22343;&#22330;&#26497;&#38480;&#19979;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#12290;&#20026;&#20102;&#32858;&#28966;&#20110;&#31532;&#19968;&#23618;&#35825;&#23548;&#30340;&#26680;&#30340;&#21160;&#24577;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#26497;&#38480;&#65292;&#20854;&#20013;&#31532;&#20108;&#23618;&#27604;&#31532;&#19968;&#23618;&#31227;&#21160;&#24471;&#24555;&#24471;&#22810;&#12290;&#22312;&#36825;&#20010;&#26497;&#38480;&#19979;&#65292;&#23398;&#20064;&#38382;&#39064;&#34987;&#31616;&#21270;&#20026;&#22312;&#20869;&#22312;&#26680;&#19978;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24179;&#22343;&#22330; Langevin &#21160;&#21147;&#23398;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#25512;&#23548;&#20102;&#26102;&#38388;&#21644;&#31890;&#23376;&#31163;&#25955;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#27604;&#20219;&#20309;&#26680;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#20010;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#24182;&#38598;&#65292;&#24182;&#19988;&#31070;&#32463;&#32593;&#32476;&#20250;&#33719;&#24471;&#19982;&#30446;&#26631;&#20989;&#25968;&#23545;&#40784;&#30340;&#25968;&#25454;&#30456;&#20851;&#26680;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#30340;&#26631;&#31614;&#22122;&#22768;&#36807;&#31243;&#65292;&#24182;&#23637;&#31034;&#33258;&#30001;&#24230;&#20986;&#29616;&#20316;&#20026;&#19968;&#31181;&#38544;&#24335;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14917v1 Announce Type: new  Abstract: In this paper, we study the feature learning ability of two-layer neural networks in the mean-field regime through the lens of kernel methods. To focus on the dynamics of the kernel induced by the first layer, we utilize a two-timescale limit, where the second layer moves much faster than the first layer. In this limit, the learning problem is reduced to the minimization problem over the intrinsic kernel. Then, we show the global convergence of the mean-field Langevin dynamics and derive time and particle discretization error. We also demonstrate that two-layer neural networks can learn a union of multiple reproducing kernel Hilbert spaces more efficiently than any kernel methods, and neural networks acquire data-dependent kernel which aligns with the target function. In addition, we develop a label noise procedure, which converges to the global optimum and show that the degrees of freedom appears as an implicit regularization.
&lt;/p&gt;</description></item><item><title>ACFL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#32534;&#30721;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20043;&#21069;&#37319;&#29992;&#20010;&#24615;&#21270;&#30340;&#25968;&#25454;&#19978;&#20256;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#26469;&#29983;&#25104;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#21407;&#26377;&#22266;&#23450;&#26435;&#37325;&#29983;&#25104;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#26102;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14905</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32534;&#30721;&#32852;&#37030;&#23398;&#20064;&#65306;&#38544;&#31169;&#20445;&#25252;&#19982;&#24930;&#33410;&#28857;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Adaptive Coded Federated Learning: Privacy Preservation and Straggler Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14905
&lt;/p&gt;
&lt;p&gt;
ACFL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#32534;&#30721;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20043;&#21069;&#37319;&#29992;&#20010;&#24615;&#21270;&#30340;&#25968;&#25454;&#19978;&#20256;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#26469;&#29983;&#25104;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#21407;&#26377;&#22266;&#23450;&#26435;&#37325;&#29983;&#25104;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#26102;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#23384;&#22312;&#24930;&#33410;&#28857;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32534;&#30721;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#20013;&#22830;&#26381;&#21153;&#22120;&#32858;&#21512;&#26469;&#33258;&#38750;&#24930;&#33410;&#28857;&#30340;&#26799;&#24230;&#21644;&#26469;&#33258;&#38544;&#31169;&#20445;&#25252;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#30340;&#26799;&#24230;&#65292;&#20197;&#20943;&#36731;&#24930;&#33410;&#28857;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#32858;&#21512;&#36825;&#20123;&#26799;&#24230;&#26102;&#65292;&#22266;&#23450;&#26435;&#37325;&#22312;&#36845;&#20195;&#20013;&#19968;&#30452;&#34987;&#24212;&#29992;&#65292;&#24573;&#30053;&#20102;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#30340;&#29983;&#25104;&#36807;&#31243;&#20197;&#21450;&#35757;&#32451;&#27169;&#22411;&#38543;&#30528;&#36845;&#20195;&#30340;&#21160;&#24577;&#24615;&#12290;&#36825;&#19968;&#30095;&#28431;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20811;&#26381;&#36825;&#19968;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#32534;&#30721;&#32852;&#37030;&#23398;&#20064;&#65288;ACFL&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;ACFL&#20013;&#65292;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#27599;&#20010;&#35774;&#22791;&#21521;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#20256;&#19968;&#20010;&#24102;&#26377;&#38468;&#21152;&#22122;&#22768;&#30340;&#32534;&#30721;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#38544;&#31169;&#20445;&#25252;&#35201;&#27714;&#30340;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#12290;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14905v1 Announce Type: cross  Abstract: In this article, we address the problem of federated learning in the presence of stragglers. For this problem, a coded federated learning framework has been proposed, where the central server aggregates gradients received from the non-stragglers and gradient computed from a privacy-preservation global coded dataset to mitigate the negative impact of the stragglers. However, when aggregating these gradients, fixed weights are consistently applied across iterations, neglecting the generation process of the global coded dataset and the dynamic nature of the trained model over iterations. This oversight may result in diminished learning performance. To overcome this drawback, we propose a new method named adaptive coded federated learning (ACFL). In ACFL, before the training, each device uploads a coded local dataset with additive noise to the central server to generate a global coded dataset under privacy preservation requirements. During
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Hydro&#31995;&#32479;&#65292;&#29992;&#20110;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#26597;&#35810;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#26597;&#35810;&#35745;&#21010;&#20197;&#36866;&#24212;&#25968;&#25454;&#30340;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;DBMSs&#22312;&#22788;&#29702;ML&#26597;&#35810;&#26102;&#36935;&#21040;&#30340;&#24615;&#33021;&#29942;&#39048;&#21644;&#20248;&#21270;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14902</link><description>&lt;p&gt;
Hydro: &#33258;&#36866;&#24212;&#26426;&#22120;&#23398;&#20064;&#26597;&#35810;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Hydro: Adaptive Query Processing of ML Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14902
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Hydro&#31995;&#32479;&#65292;&#29992;&#20110;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#26597;&#35810;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#26597;&#35810;&#35745;&#21010;&#20197;&#36866;&#24212;&#25968;&#25454;&#30340;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;DBMSs&#22312;&#22788;&#29702;ML&#26597;&#35810;&#26102;&#36935;&#21040;&#30340;&#24615;&#33021;&#29942;&#39048;&#21644;&#20248;&#21270;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25968;&#25454;&#24211;&#31649;&#29702;&#31995;&#32479;&#65288;DBMSs&#65289;&#20013;&#30340;&#26597;&#35810;&#20248;&#21270;&#23545;&#20110;&#24555;&#36895;&#26597;&#35810;&#22788;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#26597;&#35810;&#20248;&#21270;&#22120;&#20381;&#36182;&#20110;&#31934;&#30830;&#30340;&#36873;&#25321;&#24615;&#21644;&#25104;&#26412;&#20272;&#35745;&#65292;&#20197;&#20415;&#22312;&#25191;&#34892;&#20043;&#21069;&#26377;&#25928;&#22320;&#20248;&#21270;&#26597;&#35810;&#12290;&#20256;&#32479;&#30340;DBMSs&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#35813;&#31574;&#30053;&#65292;&#20294;&#23545;&#20110;&#19987;&#38376;&#29992;&#20110;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26597;&#35810;&#30340;DBMSs&#26469;&#35828;&#21017;&#19981;&#22815;&#12290;&#22312;&#20197;ML&#20026;&#20013;&#24515;&#30340;DBMSs&#20013;&#65292;&#26597;&#35810;&#20248;&#21270;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#26597;&#35810;&#30340;&#24615;&#33021;&#29942;&#39048;&#36716;&#31227;&#21040;&#29992;&#25143;&#23450;&#20041;&#20989;&#25968;&#65288;UDFs&#65289;&#19978;&#65292;&#36825;&#20123;&#20989;&#25968;&#36890;&#24120;&#23553;&#35013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;&#27809;&#26377;&#23545;&#26597;&#35810;&#36827;&#34892;&#20998;&#26512;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#20272;&#35745;UDF&#32479;&#35745;&#25968;&#25454;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#23548;&#33268;&#32479;&#35745;&#25968;&#25454;&#19981;&#20934;&#30830;&#21644;&#26597;&#35810;&#35745;&#21010;&#27425;&#20248;&#12290;&#20854;&#27425;&#65292;ML&#26597;&#35810;&#30340;&#26368;&#20339;&#26597;&#35810;&#35745;&#21010;&#21462;&#20915;&#20110;&#25968;&#25454;&#65292;&#22240;&#27492;DBMSs&#38656;&#35201;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#26597;&#35810;&#35745;&#21010;&#12290;&#22240;&#27492;&#65292;&#38745;&#24577;&#26597;&#35810;&#35745;&#21010;&#23545;&#36825;&#31867;&#26597;&#35810;&#26469;&#35828;&#26159;&#19981;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14902v1 Announce Type: cross  Abstract: Query optimization in relational database management systems (DBMSs) is critical for fast query processing. The query optimizer relies on precise selectivity and cost estimates to effectively optimize queries prior to execution. While this strategy is effective for relational DBMSs, it is not sufficient for DBMSs tailored for processing machine learning (ML) queries. In ML-centric DBMSs, query optimization is challenging for two reasons. First, the performance bottleneck of the queries shifts to user-defined functions (UDFs) that often wrap around deep learning models, making it difficult to accurately estimate UDF statistics without profiling the query. This leads to inaccurate statistics and sub-optimal query plans. Second, the optimal query plan for ML queries is data-dependent, necessitating DBMSs to adapt the query plan on the fly during execution. So, a static query plan is not sufficient for such queries.   In this paper, we pre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#40657;&#33394;&#32032;&#30244;&#20998;&#31867;&#26041;&#27861;&#65292;&#25903;&#25345;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#32452;&#21512;&#65292;&#33021;&#22815;&#20197;&#26497;&#24555;&#30340;&#36895;&#24230;&#21644;&#39640;&#20934;&#30830;&#29575;&#36827;&#34892;&#23454;&#26102;&#30340;&#40657;&#33394;&#32032;&#30244;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.14898</link><description>&lt;p&gt;
&#22522;&#20110;Web&#30340;&#40657;&#33394;&#32032;&#30244;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Web-based Melanoma Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#40657;&#33394;&#32032;&#30244;&#20998;&#31867;&#26041;&#27861;&#65292;&#25903;&#25345;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#32452;&#21512;&#65292;&#33021;&#22815;&#20197;&#26497;&#24555;&#30340;&#36895;&#24230;&#21644;&#39640;&#20934;&#30830;&#29575;&#36827;&#34892;&#23454;&#26102;&#30340;&#40657;&#33394;&#32032;&#30244;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#33394;&#32032;&#30244;&#26159;&#30382;&#32932;&#30284;&#20013;&#26368;&#20855;&#20405;&#30053;&#24615;&#30340;&#24418;&#24335;&#65292;&#26089;&#26399;&#26816;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23384;&#27963;&#29575;&#24182;&#39044;&#38450;&#30284;&#30151;&#25193;&#25955;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#20934;&#21270;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#24320;&#21457;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#26816;&#27979;&#25216;&#26415;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#40657;&#33394;&#32032;&#30244;&#20998;&#31867;&#26041;&#27861;&#65292;&#25903;&#25345;11&#20010;&#25968;&#25454;&#38598;&#21644;24&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;54&#31181;&#32452;&#21512;&#12290;&#23427;&#36890;&#36807;1,296&#20010;&#23454;&#39564;&#30340;&#20844;&#24179;&#27604;&#36739;&#65292;&#24182;&#20135;&#29983;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#21487;&#37096;&#32626;&#21040;&#21517;&#20026;Mela-D&#30340;&#22522;&#20110;Web&#30340;MeshNet&#26550;&#26500;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#65292;&#20351;&#36895;&#24230;&#25552;&#39640;&#20102;33&#20493;&#65292;&#36798;&#21040;&#20102;&#31867;&#20284;&#20110;ResNet50&#30340;88.8%&#30340;&#20934;&#30830;&#29575;&#65292;&#21487;&#22312;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#19978;&#36816;&#34892;&#12290;&#36825;&#21487;&#20197;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#39640;&#25928;&#20934;&#30830;&#22320;&#26816;&#27979;&#40657;&#33394;&#32032;&#30244;&#65292;&#21487;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14898v1 Announce Type: cross  Abstract: Melanoma is the most aggressive form of skin cancer, and early detection can significantly increase survival rates and prevent cancer spread. However, developing reliable automated detection techniques is difficult due to the lack of standardized datasets and evaluation methods. This study introduces a unified melanoma classification approach that supports 54 combinations of 11 datasets and 24 state-of-the-art deep learning architectures. It enables a fair comparison of 1,296 experiments and results in a lightweight model deployable to the web-based MeshNet architecture named Mela-D. This approach can run up to 33x faster by reducing parameters 24x to yield an analogous 88.8\% accuracy comparable with ResNet50 on previously unseen images. This allows efficient and accurate melanoma detection in real-world settings that can run on consumer-level hardware.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#35328;&#25351;&#23548;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14874</link><description>&lt;p&gt;
WeatherProof:&#20511;&#21161;&#35821;&#35328;&#25351;&#23548;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
WeatherProof: Leveraging Language Guidance for Semantic Segmentation in Adverse Weather
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#35328;&#25351;&#23548;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25512;&#26029;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#25293;&#25668;&#30340;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#22270;&#12290;&#25105;&#20204;&#39318;&#20808;&#26816;&#26597;&#20102;&#38024;&#23545;&#21463;&#22825;&#27668;&#26465;&#20214;&#65288;&#22914;&#38632;&#12289;&#38654;&#25110;&#38634;&#65289;&#24433;&#21709;&#32780;&#38477;&#32423;&#30340;&#22270;&#20687;&#30340;&#29616;&#26377;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#22312;&#26228;&#26391;&#22825;&#27668;&#19979;&#25293;&#25668;&#30340;&#22270;&#20687;&#30456;&#27604;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;&#20026;&#20102;&#25511;&#21046;&#22330;&#26223;&#32467;&#26500;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WeatherProof&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#20934;&#30830;&#30340;&#26228;&#26391;&#21644;&#24694;&#21155;&#22825;&#27668;&#22270;&#20687;&#23545;&#30340;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#23427;&#20204;&#20849;&#20139;&#30456;&#21516;&#30340;&#22330;&#26223;&#12290;&#36890;&#36807;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#38169;&#35823;&#27169;&#24335;&#65292;&#21457;&#29616;&#23427;&#20204;&#23545;&#22312;&#25293;&#25668;&#36807;&#31243;&#20013;&#26045;&#21152;&#22312;&#22270;&#20687;&#19978;&#30340;&#19981;&#21516;&#22825;&#27668;&#25928;&#24212;&#30340;&#39640;&#24230;&#22797;&#26434;&#32452;&#21512;&#25935;&#24863;&#12290;&#20026;&#20102;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#20316;&#20026;&#25351;&#23548;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#35782;&#21035;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#30340;&#24433;&#21709;&#24182;&#23558;&#20854;&#27880;&#20837;&#20026;&#8220;&#36741;&#21161;&#20449;&#24687;&#8221;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#35821;&#35328;&#25351;&#23548;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14874v1 Announce Type: cross  Abstract: We propose a method to infer semantic segmentation maps from images captured under adverse weather conditions. We begin by examining existing models on images degraded by weather conditions such as rain, fog, or snow, and found that they exhibit a large performance drop as compared to those captured under clear weather. To control for changes in scene structures, we propose WeatherProof, the first semantic segmentation dataset with accurate clear and adverse weather image pairs that share an underlying scene. Through this dataset, we analyze the error modes in existing models and found that they were sensitive to the highly complex combination of different weather effects induced on the image during capture. To improve robustness, we propose a way to use language as guidance by identifying contributions of adverse weather conditions and injecting that as "side information". Models trained using our language guidance exhibit performance
&lt;/p&gt;</description></item><item><title>VidLA &#25552;&#20986;&#20102;&#19968;&#31181;&#35268;&#27169;&#21270;&#35270;&#39057;&#35821;&#35328;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21270;&#32593;&#32476;&#26550;&#26500;&#21644;&#20351;&#29992;&#20998;&#23618;&#25968;&#25454;&#20196;&#29260;&#26469;&#25429;&#25417;&#30701;&#31243;&#21644;&#38271;&#31243;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#25104;&#21151;&#34701;&#21512;&#39044;&#35757;&#32451;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14870</link><description>&lt;p&gt;
VidLA: &#35268;&#27169;&#21270;&#35270;&#39057;&#35821;&#35328;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
VidLA: Video-Language Alignment at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14870
&lt;/p&gt;
&lt;p&gt;
VidLA &#25552;&#20986;&#20102;&#19968;&#31181;&#35268;&#27169;&#21270;&#35270;&#39057;&#35821;&#35328;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21270;&#32593;&#32476;&#26550;&#26500;&#21644;&#20351;&#29992;&#20998;&#23618;&#25968;&#25454;&#20196;&#29260;&#26469;&#25429;&#25417;&#30701;&#31243;&#21644;&#38271;&#31243;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#25104;&#21151;&#34701;&#21512;&#39044;&#35757;&#32451;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VidLA&#65292;&#19968;&#31181;&#29992;&#20110;&#35268;&#27169;&#21270;&#35270;&#39057;&#35821;&#35328;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#20197;&#24448;&#35270;&#39057;&#35821;&#35328;&#23545;&#40784;&#26041;&#27861;&#30340;&#20004;&#20010;&#20027;&#35201;&#23616;&#38480;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#27809;&#26377;&#25429;&#25417;&#21040;&#30701;&#31243;&#21644;&#38271;&#31243;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#24120;&#37319;&#29992;&#22797;&#26434;&#30340;&#20998;&#23618;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#65292;&#38590;&#20197;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#12290;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#20445;&#25345;&#32593;&#32476;&#26550;&#26500;&#31616;&#21333;&#65292;&#20351;&#29992;&#19968;&#32452;&#22312;&#19981;&#21516;&#26102;&#38388;&#20998;&#36776;&#29575;&#19979;&#20197;&#20998;&#23618;&#26041;&#24335;&#36816;&#34892;&#30340;&#25968;&#25454;&#20196;&#29260;&#65292;&#20174;&#32780;&#32771;&#34385;&#35270;&#39057;&#30340;&#26102;&#38388;&#20998;&#23618;&#24615;&#36136;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#21452;&#22612;&#26550;&#26500;&#65292;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#21021;&#22987;&#21270;&#25105;&#20204;&#30340;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14870v1 Announce Type: cross  Abstract: In this paper, we propose VidLA, an approach for video-language alignment at scale. There are two major limitations of previous video-language alignment approaches. First, they do not capture both short-range and long-range temporal dependencies and typically employ complex hierarchical deep network architectures that are hard to integrate with existing pretrained image-text foundation models. To effectively address this limitation, we instead keep the network architecture simple and use a set of data tokens that operate at different temporal resolutions in a hierarchical manner, accounting for the temporally hierarchical nature of videos. By employing a simple two-tower architecture, we are able to initialize our video-language model with pretrained image-text foundation models, thereby boosting the final performance. Second, existing video-language alignment works struggle due to the lack of semantically aligned large-scale training 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#22768;&#25104;&#20687;&#20013;&#29616;&#26377;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#23545;&#34880;&#27687;&#39281;&#21644;&#24230;&#20272;&#35745;&#30340;&#19981;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14863</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#20449;&#24687;&#21644;&#27874;&#38271;&#28789;&#27963;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#22768;&#20809;&#34880;&#27687;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Distribution-informed and wavelength-flexible data-driven photoacoustic oximetry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14863
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#22768;&#25104;&#20687;&#20013;&#29616;&#26377;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#23545;&#34880;&#27687;&#39281;&#21644;&#24230;&#20272;&#35745;&#30340;&#19981;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#32763;&#35793;: &#20256;&#22768;&#25104;&#20687;&#65288;PAI&#65289;&#26377;&#26395;&#27979;&#37327;&#20855;&#26377;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#34880;&#27687;&#39281;&#21644;&#24230;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#20809;&#35889;&#35299;&#28151;&#26041;&#27861;&#20197;&#20817;&#29616;&#36825;&#19968;&#25215;&#35834;&#12290;&#20934;&#30830;&#30340;&#34880;&#27687;&#39281;&#21644;&#24230;&#20272;&#35745;&#22312;&#30284;&#30151;&#26816;&#27979;&#21040;&#28814;&#30151;&#23450;&#37327;&#31561;&#20020;&#24202;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#35299;&#20915;PAI&#20013;&#29616;&#26377;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#20272;&#35745;&#34880;&#27687;&#39281;&#21644;&#24230;&#30340;&#19981;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14863v1 Announce Type: cross  Abstract: Significance: Photoacoustic imaging (PAI) promises to measure spatially-resolved blood oxygen saturation, but suffers from a lack of accurate and robust spectral unmixing methods to deliver on this promise. Accurate blood oxygenation estimation could have important clinical applications, from cancer detection to quantifying inflammation.   Aim: This study addresses the inflexibility of existing data-driven methods for estimating blood oxygenation in PAI by introducing a recurrent neural network architecture.   Approach: We created 25 simulated training dataset variations to assess neural network performance. We used a long short-term memory network to implement a wavelength-flexible network architecture and proposed the Jensen-Shannon divergence to predict the most suitable training dataset.   Results: The network architecture can handle arbitrary input wavelengths and outperforms linear unmixing and the previously proposed learned spe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;$\mathcal{L}_1$&#33258;&#36866;&#24212;&#25511;&#21046;&#30340;&#40065;&#26834;&#24615;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#31995;&#32479;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#39640;&#20102;MBRL&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.14860</link><description>&lt;p&gt;
&#20351;&#29992;$\mathcal{L}_1$&#33258;&#36866;&#24212;&#25511;&#21046;&#30340;&#40065;&#26834;&#24615;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Model Based Reinforcement Learning Using $\mathcal{L}_1$ Adaptive Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;$\mathcal{L}_1$&#33258;&#36866;&#24212;&#25511;&#21046;&#30340;&#40065;&#26834;&#24615;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#31995;&#32479;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#39640;&#20102;MBRL&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;$\mathcal{L}_1$-MBRL&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#31639;&#27861;&#30340;&#25511;&#21046;&#29702;&#35770;&#22686;&#24378;&#26041;&#26696;&#12290;&#19982;&#26080;&#27169;&#22411;&#26041;&#27861;&#19981;&#21516;&#65292;MBRL&#31639;&#27861;&#36890;&#36807;&#25968;&#25454;&#23398;&#20064;&#36716;&#31227;&#20989;&#25968;&#27169;&#22411;&#65292;&#24182;&#29992;&#23427;&#35774;&#35745;&#25511;&#21046;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26681;&#25454;&#25552;&#20986;&#30340;&#20999;&#25442;&#35268;&#24459;&#29983;&#25104;&#19968;&#31995;&#21015;&#23398;&#20064;&#36716;&#31227;&#20989;&#25968;&#30340;&#36817;&#20284;&#25511;&#21046;&#20223;&#23556;&#27169;&#22411;&#12290;&#36890;&#36807;&#36817;&#20284;&#27169;&#22411;&#65292;&#24213;&#23618;MBRL&#29983;&#25104;&#30340;&#25511;&#21046;&#36755;&#20837;&#34987;$\mathcal{L}_1$&#33258;&#36866;&#24212;&#25511;&#21046;&#25152;&#25200;&#21160;&#65292;&#26088;&#22312;&#22686;&#24378;&#31995;&#32479;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;MBRL&#31639;&#27861;&#30340;&#36873;&#25321;&#26159;&#19981;&#21487;&#30693;&#30340;&#65292;&#21487;&#20197;&#19982;&#21508;&#31181;MBRL&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290;&#20855;&#26377;$\mathcal{L}_1$&#22686;&#24378;&#30340;MBRL&#31639;&#27861;&#22312;&#22810;&#20010;MuJoCo&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#20339;&#30340;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#20248;&#20110;&#21407;&#22987;MBRL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14860v1 Announce Type: cross  Abstract: We introduce $\mathcal{L}_1$-MBRL, a control-theoretic augmentation scheme for Model-Based Reinforcement Learning (MBRL) algorithms. Unlike model-free approaches, MBRL algorithms learn a model of the transition function using data and use it to design a control input. Our approach generates a series of approximate control-affine models of the learned transition function according to the proposed switching law. Using the approximate model, control input produced by the underlying MBRL is perturbed by the $\mathcal{L}_1$ adaptive control, which is designed to enhance the robustness of the system against uncertainties. Importantly, this approach is agnostic to the choice of MBRL algorithm, enabling the use of the scheme with various MBRL algorithms. MBRL algorithms with $\mathcal{L}_1$ augmentation exhibit enhanced performance and sample efficiency across multiple MuJoCo environments, outperforming the original MBRL algorithms, both with 
&lt;/p&gt;</description></item><item><title>iSpLib&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;C++&#24211;&#65292;&#25552;&#20379;&#20102;&#33258;&#21160;&#35843;&#20248;&#30340;&#31232;&#30095;&#25805;&#20316;&#65292;&#21152;&#36895;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#32531;&#23384;&#20248;&#21270;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#25554;&#20214;&#23454;&#29616;&#20102;&#31616;&#20415;&#30340;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.14853</link><description>&lt;p&gt;
iSpLib&#65306;&#20351;&#29992;&#33258;&#21160;&#35843;&#20248;&#31232;&#30095;&#25805;&#20316;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
iSpLib: A Library for Accelerating Graph Neural Networks using Auto-tuned Sparse Operations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14853
&lt;/p&gt;
&lt;p&gt;
iSpLib&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;C++&#24211;&#65292;&#25552;&#20379;&#20102;&#33258;&#21160;&#35843;&#20248;&#30340;&#31232;&#30095;&#25805;&#20316;&#65292;&#21152;&#36895;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#32531;&#23384;&#20248;&#21270;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#25554;&#20214;&#23454;&#29616;&#20102;&#31616;&#20415;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#30340;&#26680;&#24515;&#35745;&#31639;&#36890;&#24120;&#34987;&#26144;&#23556;&#21040;&#31232;&#30095;&#30697;&#38453;&#36816;&#31639;&#65292;&#22914;&#31232;&#30095;-&#31264;&#23494;&#30697;&#38453;&#20056;&#27861;&#65288;SpMM&#65289;&#12290;&#36825;&#20123;&#31232;&#30095;&#25805;&#20316;&#24456;&#38590;&#36890;&#36807;&#25163;&#21160;&#35843;&#20248;&#36827;&#34892;&#20248;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#36755;&#20837;&#22270;&#30340;&#31232;&#30095;&#24615;&#12289;GNN&#27169;&#22411;&#21644;&#35745;&#31639;&#24179;&#21488;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;iSpLib&#65292;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;C++&#24211;&#65292;&#37197;&#22791;&#20102;&#33258;&#21160;&#35843;&#20248;&#30340;&#31232;&#30095;&#25805;&#20316;&#12290;iSpLib&#36890;&#36807;&#20855;&#26377;&#32531;&#23384;&#21151;&#33021;&#30340;&#21453;&#21521;&#20256;&#25773;&#21152;&#24555;&#20102;GNN&#35757;&#32451;&#65292;&#35813;&#21453;&#21521;&#20256;&#25773;&#23558;&#20013;&#38388;&#30697;&#38453;&#23384;&#20648;&#22312;&#26412;&#22320;&#32531;&#23384;&#20013;&#12290;&#35813;&#24211;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#25554;&#20214;&#65292;&#20801;&#35768;&#29992;&#25143;&#21033;&#29992;&#25105;&#20204;&#20248;&#21270;&#36807;&#30340;PyTorch&#25805;&#20316;&#65292;&#20165;&#38656;&#20004;&#34892;&#39069;&#22806;&#20195;&#30721;&#21363;&#21487;&#30452;&#25509;&#22312;&#20219;&#20309;&#29616;&#26377;&#22522;&#20110;&#32447;&#24615;&#20195;&#25968;&#30340;PyTorch&#23454;&#29616;&#30340;&#27969;&#34892;GNN&#65288;&#22270;&#21367;&#31215;&#32593;&#32476;&#12289;GraphSAGE&#12289;&#22270;&#25512;&#29702;&#32593;&#32476;&#31561;&#65289;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;iSpLib&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21152;&#36895;&#65292;&#26368;&#22823;&#21487;&#36798;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14853v1 Announce Type: new  Abstract: Core computations in Graph Neural Network (GNN) training and inference are often mapped to sparse matrix operations such as sparse-dense matrix multiplication (SpMM). These sparse operations are harder to optimize by manual tuning because their performance depends significantly on the sparsity of input graphs, GNN models, and computing platforms. To address this challenge, we present iSpLib, a PyTorch-based C++ library equipped with auto-tuned sparse operations. iSpLib expedites GNN training with a cache-enabled backpropagation that stores intermediate matrices in local caches. The library offers a user-friendly Python plug-in that allows users to take advantage of our optimized PyTorch operations out-of-the-box for any existing linear algebra-based PyTorch implementation of popular GNNs (Graph Convolution Network, GraphSAGE, Graph Inference Network, etc.) with only two lines of additional code. We demonstrate that iSpLib obtains up to 2
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#20855;&#26377;&#26377;&#38480;&#36890;&#29992;&#38543;&#26426;&#24615;&#30340;&#36755;&#20986;&#21463;&#38480;&#22833;&#30495;&#28304;&#32534;&#30721;&#22312;&#22343;&#26041;&#35823;&#24046;&#22833;&#30495;&#24230;&#37327;&#19979;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25512;&#23548;&#20986;&#20102;&#39640;&#26031;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#26126;&#30830;&#34920;&#36798;&#24335;&#65292;&#24182;&#37096;&#20998;&#21051;&#30011;&#20102;&#20108;&#27425;&#39640;&#26031;&#36895;&#29575;-&#22833;&#30495;-&#24863;&#30693;&#32534;&#30721;&#30340;&#20449;&#24687;&#35770;&#26497;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.14849</link><description>&lt;p&gt;
&#36755;&#20986;&#21463;&#38480;&#22833;&#30495;&#28304;&#32534;&#30721;&#21450;&#22312;&#36895;&#29575;-&#22833;&#30495;-&#24863;&#30693;&#29702;&#35770;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Output-Constrained Lossy Source Coding With Application to Rate-Distortion-Perception Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14849
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#20855;&#26377;&#26377;&#38480;&#36890;&#29992;&#38543;&#26426;&#24615;&#30340;&#36755;&#20986;&#21463;&#38480;&#22833;&#30495;&#28304;&#32534;&#30721;&#22312;&#22343;&#26041;&#35823;&#24046;&#22833;&#30495;&#24230;&#37327;&#19979;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25512;&#23548;&#20986;&#20102;&#39640;&#26031;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#26126;&#30830;&#34920;&#36798;&#24335;&#65292;&#24182;&#37096;&#20998;&#21051;&#30011;&#20102;&#20108;&#27425;&#39640;&#26031;&#36895;&#29575;-&#22833;&#30495;-&#24863;&#30693;&#32534;&#30721;&#30340;&#20449;&#24687;&#35770;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#20855;&#26377;&#26377;&#38480;&#36890;&#29992;&#38543;&#26426;&#24615;&#30340;&#36755;&#20986;&#21463;&#38480;&#22833;&#30495;&#28304;&#32534;&#30721;&#30340;&#22833;&#30495;-&#36895;&#29575;&#20989;&#25968;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22343;&#26041;&#35823;&#24046;&#22833;&#30495;&#24230;&#37327;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#24403;&#28304;&#21644;&#37325;&#24314;&#20998;&#24067;&#22343;&#20026;&#39640;&#26031;&#20998;&#24067;&#26102;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#34920;&#36798;&#24335;&#12290;&#36827;&#19968;&#27493;&#23548;&#33268;&#20102;&#21033;&#29992;Kullback-Leibler&#25955;&#24230;&#25110;&#24179;&#26041;&#20108;&#27425;Wasserstein&#36317;&#31163;&#32473;&#23450;&#24863;&#30693;&#24230;&#37327;&#30340;&#20108;&#27425;&#39640;&#26031;&#36895;&#29575;-&#22833;&#30495;-&#24863;&#30693;&#32534;&#30721;&#30340;&#20449;&#24687;&#35770;&#26497;&#38480;&#30340;&#37096;&#20998;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14849v1 Announce Type: cross  Abstract: The distortion-rate function of output-constrained lossy source coding with limited common randomness is analyzed for the special case of squared error distortion measure. An explicit expression is obtained when both source and reconstruction distributions are Gaussian. This further leads to a partial characterization of the information-theoretic limit of quadratic Gaussian rate-distortion-perception coding with the perception measure given by Kullback-Leibler divergence or squared quadratic Wasserstein distance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Deep Sign-Preserving WENO&#65288;DSP-WENO&#65289;&#30340;&#21464;&#31181;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;WENO&#21152;&#26435;&#31574;&#30053;&#65292;&#20197;&#25913;&#36827;&#22312;&#38663;&#33633;&#38468;&#36817;&#34920;&#29616;&#19981;&#20339;&#30340;WENO&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.14848</link><description>&lt;p&gt;
&#23398;&#20064;WENO&#29992;&#20110;&#29109;&#31283;&#23450;&#26041;&#26696;&#20197;&#35299;&#20915;&#23432;&#24658;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Learning WENO for entropy stable schemes to solve conservation laws
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14848
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Deep Sign-Preserving WENO&#65288;DSP-WENO&#65289;&#30340;&#21464;&#31181;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;WENO&#21152;&#26435;&#31574;&#30053;&#65292;&#20197;&#25913;&#36827;&#22312;&#38663;&#33633;&#38468;&#36817;&#34920;&#29616;&#19981;&#20339;&#30340;WENO&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29109;&#26465;&#20214;&#22312;&#25552;&#21462;&#31995;&#32479;&#23432;&#24658;&#24459;&#30340;&#29289;&#29702;&#30456;&#20851;&#35299;&#26102;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22240;&#27492;&#20419;&#20351;&#26500;&#24314;&#28385;&#36275;&#31163;&#25955;&#26465;&#20214;&#30340;&#29109;&#31283;&#23450;&#26041;&#26696;&#12290; TeCNO&#26041;&#26696;&#65288;Fjordholm&#31561;&#65292;2012&#65289;&#24418;&#25104;&#20102;&#19968;&#31867;&#20219;&#24847;&#39640;&#38454;&#29109;&#31283;&#23450;&#26377;&#38480;&#24046;&#20998;&#27714;&#35299;&#22120;&#65292;&#23427;&#20204;&#38656;&#35201;&#28385;&#36275;&#27599;&#20010;&#21333;&#20803;&#26684;&#30028;&#38754;&#30340;&#31526;&#21495;&#29305;&#24615;&#30340;&#19987;&#19994;&#37325;&#26500;&#31639;&#27861;&#12290;&#26368;&#36817;&#65292;&#35774;&#35745;&#20102;&#28385;&#36275;&#31526;&#21495;&#29305;&#24615;&#30340;&#31532;&#19977;&#38454;WENO&#26041;&#26696;&#65292;&#31216;&#20026;SP-WENO&#65288;Fjordholm&#21644;Ray&#65292;2016&#65289;&#21644;SP-WENOc&#65288;Ray&#65292;2018&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;WENO&#31639;&#27861;&#22312;&#38663;&#33633;&#38468;&#36817;&#30340;&#24615;&#33021;&#21487;&#33021;&#24456;&#24046;&#65292;&#25968;&#20540;&#35299;&#34920;&#29616;&#20986;&#22823;&#30340;&#20154;&#24037;&#25391;&#33633;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SP-WENO&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#31216;&#20026;Deep Sign-Preserving WENO&#65288;DSP-WENO&#65289;&#65292;&#22312;&#20854;&#20013;&#65292;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#34987;&#35757;&#32451;&#26469;&#23398;&#20064;WENO&#21152;&#26435;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14848v1 Announce Type: cross  Abstract: Entropy conditions play a crucial role in the extraction of a physically relevant solution for a system of conservation laws, thus motivating the construction of entropy stable schemes that satisfy a discrete analogue of such conditions. TeCNO schemes (Fjordholm et al. 2012) form a class of arbitrary high-order entropy stable finite difference solvers, which require specialized reconstruction algorithms satisfying the sign property at each cell interface. Recently, third-order WENO schemes called SP-WENO (Fjordholm and Ray, 2016) and SP-WENOc (Ray, 2018) have been designed to satisfy the sign property. However, these WENO algorithms can perform poorly near shocks, with the numerical solutions exhibiting large spurious oscillations. In the present work, we propose a variant of the SP-WENO, termed as Deep Sign-Preserving WENO (DSP-WENO), where a neural network is trained to learn the WENO weighting strategy. The sign property and third-o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#32479;&#19968;&#30340;&#23616;&#37096;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#20351;&#29992;&#32447;&#24615;&#38750;&#39640;&#26031;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#30446;&#26631;&#21464;&#37327;&#30340;&#39532;&#23572;&#21487;&#22827;&#27631;&#20013;&#31934;&#30830;&#35782;&#21035;&#31561;&#25928;&#30340;&#23616;&#37096;&#26377;&#21521;&#32467;&#26500;&#21644;&#22240;&#26524;&#24378;&#24230;</title><link>https://arxiv.org/abs/2403.14843</link><description>&lt;p&gt;
&#20855;&#26377;&#32447;&#24615;&#38750;&#39640;&#26031;&#24490;&#29615;&#27169;&#22411;&#30340;&#23616;&#37096;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Local Causal Discovery with Linear non-Gaussian Cyclic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14843
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#32479;&#19968;&#30340;&#23616;&#37096;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#20351;&#29992;&#32447;&#24615;&#38750;&#39640;&#26031;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#30446;&#26631;&#21464;&#37327;&#30340;&#39532;&#23572;&#21487;&#22827;&#27631;&#20013;&#31934;&#30830;&#35782;&#21035;&#31561;&#25928;&#30340;&#23616;&#37096;&#26377;&#21521;&#32467;&#26500;&#21644;&#22240;&#26524;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#22240;&#26524;&#21457;&#29616;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24847;&#20041;&#65292;&#22240;&#20026;&#32463;&#24120;&#20250;&#20986;&#29616;&#21457;&#29616;&#20840;&#23616;&#22240;&#26524;&#32467;&#26500;&#24182;&#38750;&#24517;&#35201;&#30340;&#24773;&#20917;&#65292;&#20852;&#36259;&#20165;&#20165;&#22312;&#20110;&#21333;&#20010;&#30446;&#26631;&#21464;&#37327;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#32479;&#19968;&#30340;&#23616;&#37096;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#20351;&#29992;&#32447;&#24615;&#38750;&#39640;&#26031;&#27169;&#22411;&#65292;&#26080;&#35770;&#20854;&#26159;&#21542;&#26159;&#24490;&#29615;&#30340;&#25110;&#38750;&#24490;&#29615;&#30340;&#12290;&#25105;&#20204;&#23558;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#30340;&#24212;&#29992;&#20174;&#20840;&#23616;&#19978;&#19979;&#25991;&#25193;&#23637;&#21040;&#29420;&#31435;&#23376;&#31354;&#38388;&#20998;&#26512;&#65292;&#20174;&#30446;&#26631;&#21464;&#37327;&#30340;&#39532;&#23572;&#21487;&#22827;&#27631;&#20013;&#31934;&#30830;&#35782;&#21035;&#31561;&#25928;&#30340;&#23616;&#37096;&#26377;&#21521;&#32467;&#26500;&#21644;&#22240;&#26524;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14843v1 Announce Type: cross  Abstract: Local causal discovery is of great practical significance, as there are often situations where the discovery of the global causal structure is unnecessary, and the interest lies solely on a single target variable. Most existing local methods utilize conditional independence relations, providing only a partially directed graph, and assume acyclicity for the ground-truth structure, even though real-world scenarios often involve cycles like feedback mechanisms. In this work, we present a general, unified local causal discovery method with linear non-Gaussian models, whether they are cyclic or acyclic. We extend the application of independent component analysis from the global context to independent subspace analysis, enabling the exact identification of the equivalent local directed structures and causal strengths from the Markov blanket of the target variable. We also propose an alternative regression-based method in the particular acycl
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31995;&#32479;&#35770;&#26041;&#27861;&#38024;&#23545;&#28145;&#24230;&#32467;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32447;&#24615;&#21160;&#24577;&#22359;&#36827;&#34892;&#27169;&#22411;&#38477;&#38454;&#65292;&#24341;&#20837;&#20102;&#27169;&#24577;$\ell_1$&#21644;Hankel&#26680;&#33539;&#25968;&#27491;&#21017;&#21270;&#65292;&#20419;&#36827;&#31232;&#30095;&#24615;&#65292;&#21487;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#20165;&#20445;&#30041;&#30456;&#20851;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.14833</link><description>&lt;p&gt;
&#28145;&#24230;&#32467;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#27169;&#22411;&#38477;&#38454;&#65306;&#19968;&#31181;&#31995;&#32479;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Model order reduction of deep structured state-space models: A system-theoretic approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14833
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31995;&#32479;&#35770;&#26041;&#27861;&#38024;&#23545;&#28145;&#24230;&#32467;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32447;&#24615;&#21160;&#24577;&#22359;&#36827;&#34892;&#27169;&#22411;&#38477;&#38454;&#65292;&#24341;&#20837;&#20102;&#27169;&#24577;$\ell_1$&#21644;Hankel&#26680;&#33539;&#25968;&#27491;&#21017;&#21270;&#65292;&#20419;&#36827;&#31232;&#30095;&#24615;&#65292;&#21487;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#20165;&#20445;&#30041;&#30456;&#20851;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25511;&#21046;&#35774;&#35745;&#30446;&#26631;&#30340;&#29305;&#23450;&#24378;&#35843;&#19979;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#22797;&#26434;&#24230;&#23454;&#29616;&#20934;&#30830;&#30340;&#31995;&#32479;&#24314;&#27169;&#23545;&#20110;&#21442;&#25968;&#31995;&#32479;&#36776;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#28145;&#24230;&#32467;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#65292;&#20854;&#20855;&#26377;&#32447;&#24615;&#21160;&#24577;&#22359;&#20316;&#20026;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#25552;&#20379;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25152;&#23398;&#34920;&#31034;&#32463;&#24120;&#21463;&#21040;&#36807;&#22823;&#30340;&#27169;&#22411;&#38454;&#25968;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25511;&#21046;&#35774;&#35745;&#30446;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#35770;&#27169;&#22411;&#38477;&#38454;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#35813;&#25216;&#26415;&#38024;&#23545;SSM&#30340;&#32447;&#24615;&#21160;&#24577;&#22359;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#21487;&#32467;&#21512;&#21040;&#35757;&#32451;&#25439;&#22833;&#20013;&#20197;&#23454;&#29616;&#25913;&#21892;&#30340;&#27169;&#22411;&#38477;&#38454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#27169;&#24577;$\ell_1$ &#21644; Hankel&#26680;&#33539;&#25968;&#27491;&#21017;&#21270;&#26469;&#20419;&#36827;&#31232;&#30095;&#24615;&#65292;&#20351;&#24471;&#33021;&#22815;&#20445;&#30041;&#20165;&#30456;&#20851;&#29366;&#24577;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14833v1 Announce Type: new  Abstract: With a specific emphasis on control design objectives, achieving accurate system modeling with limited complexity is crucial in parametric system identification. The recently introduced deep structured state-space models (SSM), which feature linear dynamical blocks as key constituent components, offer high predictive performance. However, the learned representations often suffer from excessively large model orders, which render them unsuitable for control design purposes. The current paper addresses this challenge by means of system-theoretic model order reduction techniques that target the linear dynamical blocks of SSMs. We introduce two regularization terms which can be incorporated into the training loss for improved model order reduction. In particular, we consider modal $\ell_1$ and Hankel nuclear norm regularization to promote sparsity, allowing one to retain only the relevant states without sacrificing accuracy. The presented reg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#22312;&#35780;&#20272;&#32858;&#31867;&#36136;&#37327;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#24212;&#29992;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.14830</link><description>&lt;p&gt;
&#28145;&#24230;&#32858;&#31867;&#35780;&#20272;&#65306;&#22914;&#20309;&#39564;&#35777;&#20869;&#37096;&#32858;&#31867;&#26377;&#25928;&#24615;&#27979;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Clustering Evaluation: How to Validate Internal Clustering Validation Measures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#22312;&#35780;&#20272;&#32858;&#31867;&#36136;&#37327;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#24212;&#29992;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14830v1 &#36890;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#28145;&#24230;&#32858;&#31867;&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#22797;&#26434;&#12289;&#39640;&#32500;&#25968;&#25454;&#36827;&#34892;&#21010;&#20998;&#30340;&#26041;&#27861;&#65292;&#23427;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#35780;&#20272;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#32858;&#31867;&#39564;&#35777;&#26041;&#27861;&#65292;&#35774;&#35745;&#29992;&#20110;&#20302;&#32500;&#31354;&#38388;&#65292;&#23545;&#20110;&#28041;&#21450;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#36739;&#20302;&#32500;&#23884;&#20837;&#31354;&#38388;&#21518;&#20877;&#36827;&#34892;&#21010;&#20998;&#30340;&#28145;&#24230;&#32858;&#31867;&#26469;&#35828;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#35770;&#25991;&#30830;&#23450;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;1&#65289;&#22312;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#21407;&#22987;&#25968;&#25454;&#26102;&#30340;&#32500;&#24230;&#28798;&#38590;&#65292;2&#65289;&#30001;&#20110;&#19981;&#21516;&#32858;&#31867;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#21442;&#25968;&#35774;&#32622;&#30340;&#21464;&#21270;&#32780;&#23548;&#33268;&#19981;&#21516;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#32858;&#31867;&#32467;&#26524;&#26080;&#27861;&#21487;&#38752;&#27604;&#36739;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#35780;&#20272;&#32858;&#31867;&#36136;&#37327;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#24378;&#35843;&#22312;&#21407;&#22987;&#25968;&#25454;&#21644;&#23884;&#20837;&#25968;&#25454;&#19978;&#20351;&#29992;&#20869;&#37096;&#39564;&#35777;&#26041;&#27861;&#21487;&#33021;&#20986;&#29616;&#30340;&#26080;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#24212;&#29992;&#28145;&#24230;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14830v1 Announce Type: cross  Abstract: Deep clustering, a method for partitioning complex, high-dimensional data using deep neural networks, presents unique evaluation challenges. Traditional clustering validation measures, designed for low-dimensional spaces, are problematic for deep clustering, which involves projecting data into lower-dimensional embeddings before partitioning. Two key issues are identified: 1) the curse of dimensionality when applying these measures to raw data, and 2) the unreliable comparison of clustering results across different embedding spaces stemming from variations in training procedures and parameter settings in different clustering models. This paper addresses these challenges in evaluating clustering quality in deep learning. We present a theoretical framework to highlight ineffectiveness arising from using internal validation measures on raw and embedded data and propose a systematic approach to applying clustering validity indices in deep 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;P\'olya-Gamma&#38543;&#26426;&#21464;&#37327;&#21046;&#23450;VGPMIL&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#19982;&#21407;&#22987;VGPMIL&#30456;&#21516;&#30340;&#21464;&#20998;&#21518;&#39564;&#36817;&#20284;&#65292;&#36825;&#26159;&#21452;&#26354;&#27491;&#21106;&#20998;&#24067;&#25152;&#25215;&#35748;&#30340;&#20004;&#31181;&#34920;&#31034;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14829</link><description>&lt;p&gt;
&#36923;&#36753;&#20989;&#25968;&#30340;&#21452;&#26354;&#27491;&#21106;&#34920;&#31034;&#65306;&#22312;CT&#39045;&#20869;&#20986;&#34880;&#26816;&#27979;&#20013;&#30340;&#27010;&#29575;&#22810;&#31034;&#20363;&#23398;&#20064;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Secant representation of the logistic function: Application to probabilistic Multiple Instance Learning for CT intracranial hemorrhage detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14829
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;P\'olya-Gamma&#38543;&#26426;&#21464;&#37327;&#21046;&#23450;VGPMIL&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#19982;&#21407;&#22987;VGPMIL&#30456;&#21516;&#30340;&#21464;&#20998;&#21518;&#39564;&#36817;&#20284;&#65292;&#36825;&#26159;&#21452;&#26354;&#27491;&#21106;&#20998;&#24067;&#25152;&#25215;&#35748;&#30340;&#20004;&#31181;&#34920;&#31034;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31034;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26159;&#19968;&#31181;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#31185;&#23398;&#39046;&#22495;&#30340;&#24369;&#30417;&#30563;&#33539;&#24335;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#21307;&#23398;&#25104;&#20687;&#12290;&#27010;&#29575;MIL&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#65292;&#30001;&#20110;&#20854;&#39640;&#34920;&#36798;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33021;&#21147;&#24050;&#32463;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#26368;&#25104;&#21151;&#30340;&#22522;&#20110;GP&#30340;MIL&#26041;&#27861;&#20043;&#19968;&#65292;VGPMIL&#65292;&#20351;&#29992;&#21464;&#20998;&#36793;&#30028;&#22788;&#29702;&#36923;&#36753;&#20989;&#25968;&#30340;&#19981;&#21487;&#35299;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;P\'olya-Gamma&#38543;&#26426;&#21464;&#37327;&#26469;&#21046;&#23450;VGPMIL&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#19982;&#21407;&#22987;VGPMIL&#30456;&#21516;&#30340;&#21464;&#20998;&#21518;&#39564;&#36817;&#20284;&#65292;&#36825;&#26159;&#21452;&#26354;&#27491;&#21106;&#20998;&#24067;&#25152;&#25215;&#35748;&#30340;&#20004;&#31181;&#34920;&#31034;&#30340;&#32467;&#26524;&#12290;&#36825;&#23548;&#33268;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22522;&#20110;GP&#30340;MIL&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#21033;&#29992;&#38500;&#21452;&#26354;&#27491;&#21106;&#20197;&#22806;&#30340;&#20998;&#24067;&#65292;&#21487;&#20197;&#37319;&#21462;&#19981;&#21516;&#24418;&#24335;&#12290;&#20351;&#29992;Gamma&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14829v1 Announce Type: new  Abstract: Multiple Instance Learning (MIL) is a weakly supervised paradigm that has been successfully applied to many different scientific areas and is particularly well suited to medical imaging. Probabilistic MIL methods, and more specifically Gaussian Processes (GPs), have achieved excellent results due to their high expressiveness and uncertainty quantification capabilities. One of the most successful GP-based MIL methods, VGPMIL, resorts to a variational bound to handle the intractability of the logistic function. Here, we formulate VGPMIL using P\'olya-Gamma random variables. This approach yields the same variational posterior approximations as the original VGPMIL, which is a consequence of the two representations that the Hyperbolic Secant distribution admits. This leads us to propose a general GP-based MIL method that takes different forms by simply leveraging distributions other than the Hyperbolic Secant one. Using the Gamma distribution
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;Sinkhorn&#19981;&#30830;&#23450;&#24615;&#38598;&#35299;&#20915;&#38750;&#20984;&#40065;&#26834;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#30830;&#20999;&#30340;&#28151;&#21512;&#25972;&#25968;&#25351;&#25968;&#38181;&#37325;&#26500;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20248;&#20110;&#30446;&#21069;&#25991;&#29486;&#20013;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20984;&#36924;&#36817;&#12290;</title><link>https://arxiv.org/abs/2403.14822</link><description>&lt;p&gt;
&#20351;&#29992;Sinkhorn&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#38750;&#20984;&#40065;&#26834;&#20551;&#35774;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Non-Convex Robust Hypothesis Testing using Sinkhorn Uncertainty Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14822
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;Sinkhorn&#19981;&#30830;&#23450;&#24615;&#38598;&#35299;&#20915;&#38750;&#20984;&#40065;&#26834;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#30830;&#20999;&#30340;&#28151;&#21512;&#25972;&#25968;&#25351;&#25968;&#38181;&#37325;&#26500;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20248;&#20110;&#30446;&#21069;&#25991;&#29486;&#20013;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20984;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#35299;&#20915;&#38750;&#20984;&#40065;&#26834;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#23547;&#25214;&#26368;&#20248;&#25506;&#27979;&#22120;&#65292;&#20197;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#31867;&#22411;I&#21644;&#31867;&#22411;II&#39118;&#38505;&#20989;&#25968;&#30340;&#26368;&#22823;&#20540;&#12290;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;&#38598;&#22260;&#32469;&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#26679;&#26412;&#24471;&#20986;&#30340;&#32463;&#39564;&#20998;&#24067;&#26500;&#24314;&#12290;&#30001;&#20110;&#30446;&#26631;&#28041;&#21450;&#38750;&#20984;&#12289;&#38750;&#24179;&#28369;&#30340;&#27010;&#29575;&#20989;&#25968;&#65292;&#36890;&#24120;&#38590;&#20197;&#20248;&#21270;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#36817;&#20284;&#32780;&#38750;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30830;&#20999;&#30340;&#28151;&#21512;&#25972;&#25968;&#25351;&#25968;&#38181;&#37325;&#26500;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36866;&#37327;&#30340;&#36755;&#20837;&#25968;&#25454;&#19979;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20984;&#36924;&#36817;&#65292;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#24403;&#21069;&#25991;&#29486;&#20013;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#40065;&#26834;&#20551;&#35774;&#26816;&#39564;&#19982;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14822v1 Announce Type: cross  Abstract: We present a new framework to address the non-convex robust hypothesis testing problem, wherein the goal is to seek the optimal detector that minimizes the maximum of worst-case type-I and type-II risk functions. The distributional uncertainty sets are constructed to center around the empirical distribution derived from samples based on Sinkhorn discrepancy. Given that the objective involves non-convex, non-smooth probabilistic functions that are often intractable to optimize, existing methods resort to approximations rather than exact solutions. To tackle the challenge, we introduce an exact mixed-integer exponential conic reformulation of the problem, which can be solved into a global optimum with a moderate amount of input data. Subsequently, we propose a convex approximation, demonstrating its superiority over current state-of-the-art methodologies in literature. Furthermore, we establish connections between robust hypothesis testi
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.14814</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#26426;&#20250;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
The opportunities and risks of large language models in mental health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14814
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#21457;&#29983;&#29575;&#27491;&#22312;&#19978;&#21319;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#29616;&#26377;&#30340;&#24515;&#29702;&#20445;&#20581;&#27169;&#24335;&#26080;&#27861;&#20805;&#20998;&#25193;&#23637;&#20197;&#28385;&#36275;&#38656;&#27714;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#20855;&#26377;&#21019;&#36896;&#26032;&#39062;&#12289;&#22823;&#35268;&#27169;&#35299;&#20915;&#26041;&#26696;&#20197;&#25903;&#25345;&#24515;&#29702;&#20581;&#24247;&#30340;&#25215;&#35834;&#24863;&#21040;&#20048;&#35266;&#12290;&#23613;&#31649;&#23427;&#20204;&#36824;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;LLMs&#24050;&#34987;&#24212;&#29992;&#20110;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#24050;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#21033;&#29992;LLMs&#25552;&#20379;&#24515;&#29702;&#20581;&#24247;&#25945;&#32946;&#12289;&#35780;&#20272;&#21644;&#24178;&#39044;&#30340;&#21162;&#21147;&#65292;&#24182;&#31361;&#20986;&#20102;&#27599;&#20010;&#39046;&#22495;&#20013;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#30340;&#20851;&#38190;&#26426;&#20250;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23558;LLMs&#24212;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#25152;&#20276;&#38543;&#30340;&#39118;&#38505;&#65292;&#24182;&#40723;&#21169;&#37319;&#29992;&#31574;&#30053;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#36843;&#20999;&#38656;&#27714;&#24517;&#39035;&#19982;&#36127;&#36131;&#20219;&#30340;&#24515;&#29702;&#20581;&#24247;LLMs&#30340;&#24320;&#21457;&#12289;&#27979;&#35797;&#21644;&#37096;&#32626;&#30456;&#24179;&#34913;&#12290;&#29305;&#21035;&#20851;&#38190;&#30340;&#26159;&#30830;&#20445;&#24515;&#29702;&#20581;&#24247;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14814v1 Announce Type: cross  Abstract: Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this review, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. Especially critical is ensuring that mental he
&lt;/p&gt;</description></item><item><title>&#23558;&#38477;&#32500;&#38382;&#39064;&#24314;&#27169;&#20026;&#26426;&#26800;/&#29289;&#29702;&#27169;&#22411;&#65292;&#24341;&#20837;&#26354;&#29575;&#22686;&#24378;&#21147;&#30340;&#26354;&#29575;&#22686;&#24378;&#27969;&#24418;&#23884;&#20837;&#19982;&#23398;&#20064;&#65288;CAMEL&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25429;&#25417;&#25968;&#25454;&#38598;&#30340;n&#32500;&#27969;&#24418;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.14813</link><description>&lt;p&gt;
&#26354;&#29575;&#22686;&#24378;&#27969;&#24418;&#23884;&#20837;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curvature Augmented Manifold Embedding and Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14813
&lt;/p&gt;
&lt;p&gt;
&#23558;&#38477;&#32500;&#38382;&#39064;&#24314;&#27169;&#20026;&#26426;&#26800;/&#29289;&#29702;&#27169;&#22411;&#65292;&#24341;&#20837;&#26354;&#29575;&#22686;&#24378;&#21147;&#30340;&#26354;&#29575;&#22686;&#24378;&#27969;&#24418;&#23884;&#20837;&#19982;&#23398;&#20064;&#65288;CAMEL&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25429;&#25417;&#25968;&#25454;&#38598;&#30340;n&#32500;&#27969;&#24418;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#34920;&#20110;arXiv:2403.14813v1&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38477;&#32500;&#65288;DR&#65289;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#26354;&#29575;&#22686;&#24378;&#27969;&#24418;&#23884;&#20837;&#19982;&#23398;&#20064;&#65288;CAMEL&#65289;&#12290;&#20854;&#20851;&#38190;&#21019;&#26032;&#36129;&#29486;&#22312;&#20110;&#23558;&#38477;&#32500;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#26426;&#26800;/&#29289;&#29702;&#27169;&#22411;&#65292;&#20854;&#20013;&#33410;&#28857;&#65288;&#25968;&#25454;&#28857;&#65289;&#20043;&#38388;&#30340;&#21147;&#22330;&#34987;&#29992;&#26469;&#25214;&#21040;&#25968;&#25454;&#38598;&#30340;n&#32500;&#27969;&#24418;&#34920;&#31034;&#12290;&#19982;&#35768;&#22810;&#29616;&#26377;&#30340;&#21560;&#24341;-&#25490;&#26021;&#21147;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#30340;&#19968;&#20010;&#29420;&#29305;&#36129;&#29486;&#26159;&#21253;&#21547;&#20102;&#19968;&#20010;&#38750;&#25104;&#23545;&#21147;&#12290;&#24341;&#20837;&#21644;&#35752;&#35770;&#20102;&#19968;&#31181;&#26032;&#30340;&#21147;&#22330;&#27169;&#22411;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#26230;&#26684;&#31890;&#23376;&#29289;&#29702;&#23398;&#20013;&#30340;&#22810;&#20307;&#21183;&#21644;&#25299;&#25169;&#23398;&#20013;&#30340;&#40654;&#26364;&#26354;&#29575;&#12290;CAMEL&#20013;&#21253;&#21547;&#20102;&#19968;&#31181;&#26354;&#29575;&#22686;&#24378;&#21147;&#12290;&#20854;&#27425;&#65292;&#25552;&#20379;&#20102;CAMEL&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#12289;&#30417;&#30563;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;/&#24230;&#37327;&#23398;&#20064;&#21644;&#36870;&#21521;&#23398;&#20064;&#30340;&#20844;&#24335;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#19982;&#29616;&#26377;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#23558;CAMEL&#24212;&#29992;&#20110;&#35768;&#22810;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14813v1 Announce Type: cross  Abstract: A new dimensional reduction (DR) and data visualization method, Curvature-Augmented Manifold Embedding and Learning (CAMEL), is proposed. The key novel contribution is to formulate the DR problem as a mechanistic/physics model, where the force field among nodes (data points) is used to find an n-dimensional manifold representation of the data sets. Compared with many existing attractive-repulsive force-based methods, one unique contribution of the proposed method is to include a non-pairwise force. A new force field model is introduced and discussed, inspired by the multi-body potential in lattice-particle physics and Riemann curvature in topology. A curvature-augmented force is included in CAMEL. Following this, CAMEL formulation for unsupervised learning, supervised learning, semi-supervised learning/metric learning, and inverse learning are provided. Next, CAMEL is applied to many benchmark datasets by comparing existing models, suc
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;&#21457;&#29616;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#21333;&#19968;&#27169;&#22411;&#26041;&#27861;&#33021;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;&#29109;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#36215;&#22987;&#39044;&#31639;&#12289;&#39044;&#31639;&#27493;&#38271;&#21644;&#39044;&#35757;&#32451;&#31561;&#22240;&#32032;&#23545;&#21462;&#24471;&#20248;&#36234;&#32467;&#26524;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25299;&#23637;&#20102;&#22312;&#20854;&#20182;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.14800</link><description>&lt;p&gt;
&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#65306;&#29616;&#23454;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Deep Active Learning: A Reality Check
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14800
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;&#21457;&#29616;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#21333;&#19968;&#27169;&#22411;&#26041;&#27861;&#33021;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;&#29109;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#36215;&#22987;&#39044;&#31639;&#12289;&#39044;&#31639;&#27493;&#38271;&#21644;&#39044;&#35757;&#32451;&#31561;&#22240;&#32032;&#23545;&#21462;&#24471;&#20248;&#36234;&#32467;&#26524;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25299;&#23637;&#20102;&#22312;&#20854;&#20182;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#21333;&#19968;&#27169;&#22411;&#26041;&#27861;&#33021;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;&#29109;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#29978;&#33267;&#26377;&#20123;&#26041;&#27861;&#34920;&#29616;&#19981;&#22914;&#38543;&#26426;&#25277;&#26679;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#19968;&#20123;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#65292;&#22914;&#36215;&#22987;&#39044;&#31639;&#12289;&#39044;&#31639;&#27493;&#38271;&#21644;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#21462;&#24471;&#21331;&#36234;&#32467;&#26524;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#25193;&#23637;&#21040;&#20854;&#20182;&#20219;&#21153;&#65292;&#25506;&#32034;&#20027;&#21160;&#23398;&#20064;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#30446;&#26631;&#26816;&#27979;&#30456;&#32467;&#21512;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#21644;&#20855;&#20307;&#24314;&#35758;&#65292;&#20026;&#26410;&#26469;&#30340;&#20027;&#21160;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#36890;&#36807;&#25581;&#31034;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#20102;&#35299;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#26088;&#22312;&#28608;&#21457;&#22312;&#20855;&#26377;&#26377;&#38480;&#27880;&#37322;&#39044;&#31639;&#30340;&#30495;&#23454;&#22330;&#26223;&#20013;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#25512;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14800v1 Announce Type: cross  Abstract: We conduct a comprehensive evaluation of state-of-the-art deep active learning methods. Surprisingly, under general settings, no single-model method decisively outperforms entropy-based active learning, and some even fall short of random sampling. We delve into overlooked aspects like starting budget, budget step, and pretraining's impact, revealing their significance in achieving superior results. Additionally, we extend our evaluation to other tasks, exploring the active learning effectiveness in combination with semi-supervised learning, and object detection. Our experiments provide valuable insights and concrete recommendations for future active learning studies. By uncovering the limitations of current methods and understanding the impact of different experimental settings, we aim to inspire more efficient training of deep learning models in real-world scenarios with limited annotation budgets. This work contributes to advancing a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#35760;&#24518;&#32593;&#32476;&#21644;&#23616;&#37096;&#26597;&#35810;&#20989;&#25968;&#65292;&#36825;&#39033;&#24037;&#20316;&#33268;&#21147;&#20110;&#22312;&#36830;&#32493;&#26816;&#27979;&#20013;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#35299;&#20915;&#20102;&#25345;&#32493;&#26816;&#27979;&#20013;&#30340;&#32972;&#26223;&#36140;&#20302;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14797</link><description>&lt;p&gt;
&#36890;&#36807;&#35760;&#24518;&#32593;&#32476;&#22312;&#36830;&#32493;&#26816;&#27979;&#20013;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Preventing Catastrophic Forgetting through Memory Networks in Continuous Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14797
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#35760;&#24518;&#32593;&#32476;&#21644;&#23616;&#37096;&#26597;&#35810;&#20989;&#25968;&#65292;&#36825;&#39033;&#24037;&#20316;&#33268;&#21147;&#20110;&#22312;&#36830;&#32493;&#26816;&#27979;&#20013;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#35299;&#20915;&#20102;&#25345;&#32493;&#26816;&#27979;&#20013;&#30340;&#32972;&#26223;&#36140;&#20302;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#39044;&#35757;&#32451;&#26550;&#26500;&#22312;&#25345;&#32493;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#26102;&#24456;&#38590;&#20445;&#30041;&#20808;&#21069;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#22312;&#25345;&#32493;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#38024;&#23545;&#22797;&#26434;&#35270;&#35273;&#20219;&#21153;&#65288;&#22914;&#26816;&#27979;&#25110;&#20998;&#21106;&#65289;&#35774;&#35745;&#30340;&#31995;&#32479;&#20173;&#28982;&#38590;&#20197;&#33719;&#24471;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#26816;&#27979;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#20197;&#20351;&#39044;&#35757;&#32451;&#30340;DETR&#39118;&#26684;&#26816;&#27979;&#22120;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#20808;&#21069;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23616;&#37096;&#26597;&#35810;&#20989;&#25968;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#35760;&#24518;&#21333;&#20803;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25345;&#32493;&#26816;&#27979;&#20013;&#19968;&#20010;&#31216;&#20026;&#32972;&#26223;&#36140;&#20302;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#24403;&#26469;&#33258;&#20808;&#21069;&#20219;&#21153;&#30340;&#23545;&#35937;&#31867;&#21035;&#22312;&#26410;&#26469;&#20219;&#21153;&#20013;&#37325;&#26032;&#20986;&#29616;&#26102;&#65292;&#21487;&#33021;&#27809;&#26377;&#26631;&#31614;&#65292;&#23548;&#33268;&#23427;&#20204;&#34987;&#38544;&#24335;&#35270;&#20026;&#32972;&#26223;&#12290;&#36825;&#26159;&#25345;&#32493;&#26816;&#27979;&#25110;&#20998;&#21106;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14797v1 Announce Type: cross  Abstract: Modern pre-trained architectures struggle to retain previous information while undergoing continuous fine-tuning on new tasks. Despite notable progress in continual classification, systems designed for complex vision tasks such as detection or segmentation still struggle to attain satisfactory performance. In this work, we introduce a memory-based detection transformer architecture to adapt a pre-trained DETR-style detector to new tasks while preserving knowledge from previous tasks. We propose a novel localized query function for efficient information retrieval from memory units, aiming to minimize forgetting. Furthermore, we identify a fundamental challenge in continual detection referred to as background relegation. This arises when object categories from earlier tasks reappear in future tasks, potentially without labels, leading them to be implicitly treated as background. This is an inevitable issue in continual detection or segme
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#26234;&#20307;&#31995;&#32479;&#65292;&#21517;&#20026;&#22810;&#26234;&#20307;VQA&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#26234;&#20307;&#24037;&#20855;&#65292;&#20811;&#26381;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35745;&#25968;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.14783</link><description>&lt;p&gt;
&#22810;&#26234;&#20307;VQA&#65306;&#25506;&#32034;&#38646;&#26679;&#26412;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#22810;&#26234;&#20307;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#26234;&#20307;&#31995;&#32479;&#65292;&#21517;&#20026;&#22810;&#26234;&#20307;VQA&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#26234;&#20307;&#24037;&#20855;&#65292;&#20811;&#26381;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35745;&#25968;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#26234;&#20307;&#31995;&#32479;&#65292;&#21629;&#21517;&#20026;&#22810;&#26234;&#20307;VQA&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#26234;&#20307;&#20316;&#20026;&#24037;&#20855;&#65292;&#20197;&#20811;&#26381;&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35745;&#25968;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#22312;&#19981;&#23545;&#20854;&#36827;&#34892;&#29305;&#23450;VQA&#25968;&#25454;&#38598;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#26356;&#21152;&#23454;&#29992;&#21644;&#31283;&#20581;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#25552;&#20986;&#20102;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#31361;&#20986;&#20102;&#19968;&#20123;&#22833;&#36133;&#26696;&#20363;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14783v1 Announce Type: cross  Abstract: This work explores the zero-shot capabilities of foundation models in Visual Question Answering (VQA) tasks. We propose an adaptive multi-agent system, named Multi-Agent VQA, to overcome the limitations of foundation models in object detection and counting by using specialized agents as tools. Unlike existing approaches, our study focuses on the system's performance without fine-tuning it on specific VQA datasets, making it more practical and robust in the open world. We present preliminary experimental results under zero-shot scenarios and highlight some failure cases, offering new directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#23545;&#25239;&#25552;&#31034;&#26694;&#26550;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#26377;&#38480;&#25968;&#25454;&#35843;&#25972;&#36755;&#20837;&#24207;&#21015;&#65292;&#26174;&#33879;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#23545;&#25239;&#24615;&#30456;&#20851;&#30340;&#25991;&#26412;&#30417;&#30563;&#12290;</title><link>https://arxiv.org/abs/2403.14774</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#23569;&#26679;&#26412;&#23545;&#25239;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Adversarial Prompt Learning on Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#23545;&#25239;&#25552;&#31034;&#26694;&#26550;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#26377;&#38480;&#25968;&#25454;&#35843;&#25972;&#36755;&#20837;&#24207;&#21015;&#65292;&#26174;&#33879;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#23545;&#25239;&#24615;&#30456;&#20851;&#30340;&#25991;&#26412;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#24494;&#19981;&#21487;&#35265;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#21463;&#21040;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#36890;&#36807;&#23558;&#23545;&#25239;&#24615;&#35270;&#35273;&#29305;&#24449;&#19982;&#25991;&#26412;&#30417;&#30563;&#23545;&#40784;&#26469;&#23454;&#29616;&#38646;&#26679;&#26412;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#21253;&#25324;&#37325;&#22823;&#36866;&#24212;&#25104;&#26412;&#12289;&#27425;&#20248;&#25991;&#26412;&#30417;&#30563;&#21644;&#26410;&#21463;&#25511;&#21046;&#30340;&#33258;&#28982;&#27867;&#21270;&#33021;&#21147;&#22312;&#20869;&#30340;&#22810;&#20010;&#38382;&#39064;&#65292;&#23427;&#20204;&#20173;&#28982;&#19981;&#23613;&#20154;&#24847;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#23545;&#25239;&#25552;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#25968;&#25454;&#35843;&#25972;&#36755;&#20837;&#24207;&#21015;&#20351;&#24471;&#23545;&#25239;&#40065;&#26834;&#24615;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#23545;&#25239;&#30456;&#20851;&#30340;&#25991;&#26412;&#30417;&#30563;&#65292;&#35813;&#30417;&#30563;&#26159;&#20174;&#23545;&#25239;&#24615;&#31034;&#20363;&#20013;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#65292;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#22810;&#27169;&#24577;&#29305;&#24449;&#19968;&#33268;&#24615;&#24182;&#40723;&#21169;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14774v1 Announce Type: cross  Abstract: The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by aligning adversarial visual features with text supervision. However, in practice, they are still unsatisfactory due to several issues, including heavy adaptation cost, suboptimal text supervision, and uncontrolled natural generalization capacity. In this paper, to address these issues, we propose a few-shot adversarial prompt framework where adapting input sequences with limited data makes significant adversarial robustness improvement. Specifically, we achieve this by providing adversarially correlated text supervision that is end-to-end learned from adversarial examples. We also propose a novel training objective that enhances the consistency of multi-modal features while encourages differenti
&lt;/p&gt;</description></item><item><title>StreamingT2V&#26159;&#19968;&#31181;&#33258;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#38271;&#35270;&#39057;&#65292;&#21487;&#20197;&#20135;&#29983;80&#12289;240&#12289;600&#12289;1200&#24103;&#29978;&#33267;&#26356;&#22810;&#24103;&#30340;&#35270;&#39057;&#65292;&#24182;&#20855;&#26377;&#24179;&#28369;&#30340;&#36807;&#28193;&#12290;</title><link>https://arxiv.org/abs/2403.14773</link><description>&lt;p&gt;
StreamingT2V: &#19968;&#31181;&#19968;&#33268;&#12289;&#21160;&#24577;&#21644;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#38271;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14773
&lt;/p&gt;
&lt;p&gt;
StreamingT2V&#26159;&#19968;&#31181;&#33258;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#38271;&#35270;&#39057;&#65292;&#21487;&#20197;&#20135;&#29983;80&#12289;240&#12289;600&#12289;1200&#24103;&#29978;&#33267;&#26356;&#22810;&#24103;&#30340;&#35270;&#39057;&#65292;&#24182;&#20855;&#26377;&#24179;&#28369;&#30340;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14773v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#25991;&#26412;&#21040;&#35270;&#39057;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36981;&#24490;&#25991;&#26412;&#25351;&#20196;&#30340;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#20351;&#24471;&#21019;&#24314;&#22810;&#26679;&#21270;&#21644;&#20010;&#24615;&#21270;&#20869;&#23481;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#38598;&#20013;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#30701;&#35270;&#39057;&#65288;&#36890;&#24120;&#20026;16&#25110;24&#24103;&#65289;&#65292;&#24403;&#22825;&#30495;&#22320;&#25193;&#23637;&#21040;&#38271;&#35270;&#39057;&#21512;&#25104;&#30340;&#24773;&#20917;&#26102;&#65292;&#36890;&#24120;&#20250;&#20986;&#29616;&#30828;&#35009;&#21098;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StreamingT2V&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;80&#12289;240&#12289;600&#12289;1200&#25110;&#26356;&#22810;&#24103;&#30340;&#38271;&#35270;&#39057;&#65292;&#20855;&#26377;&#24179;&#28369;&#30340;&#36807;&#28193;&#12290;&#20027;&#35201;&#32452;&#20214;&#21253;&#25324;&#65306;&#65288;i&#65289;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;CAM&#65289;&#30340;&#30701;&#26399;&#35760;&#24518;&#22359;&#65292;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#23558;&#24403;&#21069;&#29983;&#25104;&#26465;&#20214;&#35774;&#32622;&#20026;&#20808;&#21069;&#22359;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#19968;&#33268;&#30340;&#22359;&#36807;&#28193;&#65292;&#65288;ii&#65289;&#19968;&#31181;&#21517;&#20026;&#22806;&#35266;&#20445;&#23384;&#27169;&#22359;&#30340;&#38271;&#26399;&#35760;&#24518;&#22359;&#65292;&#20174;&#31532;&#19968;&#20010;&#35270;&#39057;&#22359;&#20013;&#25552;&#21462;&#39640;&#32423;&#22330;&#26223;&#21644;&#23545;&#35937;&#29305;&#24449;&#65292;&#20197;&#38450;&#27490;th
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14773v1 Announce Type: cross  Abstract: Text-to-video diffusion models enable the generation of high-quality videos that follow text instructions, making it easy to create diverse and individual content. However, existing approaches mostly focus on high-quality short video generation (typically 16 or 24 frames), ending up with hard-cuts when naively extended to the case of long video synthesis. To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions. The key components are:(i) a short-term memory block called conditional attention module (CAM), which conditions the current generation on the features extracted from the previous chunk via an attentional mechanism, leading to consistent chunk transitions, (ii) a long-term memory block called appearance preservation module, which extracts high-level scene and object features from the first video chunk to prevent th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23618;&#35774;&#35745;&#26032;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23545;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14772</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#26550;&#26500;&#25552;&#39640;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14772
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23618;&#35774;&#35745;&#26032;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23545;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#31639;&#27861;&#20801;&#35768;&#23545;&#25163;&#36890;&#36807;&#21453;&#22797;&#26597;&#35810;&#31070;&#32463;&#32593;&#32476;&#24182;&#26816;&#26597;&#20854;&#36755;&#20986;&#26469;&#37325;&#24314;&#32593;&#32476;&#30340;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#31232;&#30095;&#32534;&#30721;&#23618;&#26469;&#33719;&#24471;&#23545;&#36825;&#31867;&#25915;&#20987;&#30340;&#21331;&#36234;&#40065;&#26834;&#24615;&#12290; &#19977;&#21313;&#24180;&#26469;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#31232;&#30095;&#32534;&#30721;&#22312;&#22270;&#20687;&#21435;&#22122;&#65292;&#30446;&#26631;&#35782;&#21035;&#21644;&#23545;&#25239;&#24615;&#35823;&#20998;&#35774;&#32622;&#20013;&#30340;&#20316;&#29992;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#38544;&#31169;&#28431;&#27934;&#20043;&#38388;&#30340;&#32852;&#31995;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#31232;&#30095;&#32534;&#30721;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21033;&#30340;&#25163;&#27573;&#26469;&#25269;&#24481;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#25105;&#20204;&#25511;&#21046;&#32534;&#30721;&#22312;&#32593;&#32476;&#30340;&#20013;&#38388;&#34920;&#31034;&#20013;&#30340;&#26080;&#20851;&#31169;&#20154;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#32780;&#36825;&#31181;&#26041;&#24335;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39640;&#25928;&#35745;&#31639;&#65292;&#24182;&#19988;&#20247;&#25152;&#21608;&#30693;&#21482;&#26377;&#36739;&#23567;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14772v1 Announce Type: cross  Abstract: Recent model inversion attack algorithms permit adversaries to reconstruct a neural network's private training data just by repeatedly querying the network and inspecting its outputs. In this work, we develop a novel network architecture that leverages sparse-coding layers to obtain superior robustness to this class of attacks. Three decades of computer science research has studied sparse coding in the context of image denoising, object recognition, and adversarial misclassification settings, but to the best of our knowledge, its connection to state-of-the-art privacy vulnerabilities remains unstudied. However, sparse coding architectures suggest an advantageous means to defend against model inversion attacks because they allow us to control the amount of irrelevant private information encoded in a network's intermediate representations in a manner that can be computed efficiently during training and that is known to have little effect
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26680;&#33258;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#30340;&#26032;&#22411;&#21464;&#20998;&#37327;&#23376;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#21487;&#20197;&#29992;&#31616;&#21333;&#30340;&#21367;&#31215;&#26680;&#34920;&#31034;&#28145;&#23618;&#30340;&#35270;&#35273;&#21464;&#21387;&#22120;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2403.14753</link><description>&lt;p&gt;
&#19982;SASQuaTCh&#23398;&#20064;&#65306;&#22522;&#20110;&#26680;&#33258;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#21464;&#20998;&#37327;&#23376;&#21464;&#21387;&#22120;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning with SASQuaTCh: a Novel Variational Quantum Transformer Architecture with Kernel-Based Self-Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26680;&#33258;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#30340;&#26032;&#22411;&#21464;&#20998;&#37327;&#23376;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#21487;&#20197;&#29992;&#31616;&#21333;&#30340;&#21367;&#31215;&#26680;&#34920;&#31034;&#28145;&#23618;&#30340;&#35270;&#35273;&#21464;&#21387;&#22120;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#26222;&#21450;&#30340;&#24191;&#27867;&#27969;&#34892;&#30340;&#21464;&#21387;&#22120;&#32593;&#32476;&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#25991;&#26412;&#21644;&#22270;&#20687;&#12289;&#20998;&#31867;&#65292;&#29978;&#33267;&#39044;&#27979;&#29289;&#29702;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#35299;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#33021;&#22815;&#36890;&#36807;&#22522;&#20110;&#26680;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;&#35270;&#35282;&#39640;&#25928;&#34920;&#36798;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#19979;&#65292;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#31616;&#21333;&#30340;&#21367;&#31215;&#26680;&#26469;&#34920;&#31034;&#35270;&#35273;&#21464;&#21387;&#22120;&#32593;&#32476;&#30340;&#28145;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14753v1 Announce Type: cross  Abstract: The widely popular transformer network popularized by the generative pre-trained transformer (GPT) has a large field of applicability, including predicting text and images, classification, and even predicting solutions to the dynamics of physical systems. In the latter context, the continuous analog of the self-attention mechanism at the heart of transformer networks has been applied to learning the solutions of partial differential equations and reveals a convolution kernel nature that can be exploited by the Fourier transform. It is well known that many quantum algorithms that have provably demonstrated a speedup over classical algorithms utilize the quantum Fourier transform. In this work, we explore quantum circuits that can efficiently express a self-attention mechanism through the perspective of kernel-based operator learning. In this perspective, we are able to represent deep layers of a vision transformer network using simple g
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#20498;&#25968;&#31532;&#20108;&#23618;&#20316;&#20026;&#24322;&#24120;&#26816;&#27979;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#31867;&#23396;&#31435;&#26862;&#26519;&#65288;MCIF&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#35757;&#32451;&#27599;&#20010;&#31867;&#21035;&#30340;&#23396;&#31435;&#26862;&#26519;&#65292;&#20197;&#25512;&#23548;&#24322;&#24120;&#20540;</title><link>https://arxiv.org/abs/2403.14742</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#22810;&#31867;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#22825;&#25991;&#26242;&#21464;&#26143;
&lt;/p&gt;
&lt;p&gt;
A Classifier-Based Approach to Multi-Class Anomaly Detection for Astronomical Transients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14742
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#20498;&#25968;&#31532;&#20108;&#23618;&#20316;&#20026;&#24322;&#24120;&#26816;&#27979;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#31867;&#23396;&#31435;&#26862;&#26519;&#65288;MCIF&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#35757;&#32451;&#27599;&#20010;&#31867;&#21035;&#30340;&#23396;&#31435;&#26862;&#26519;&#65292;&#20197;&#25512;&#23548;&#24322;&#24120;&#20540;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#33258;&#21160;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#22312;&#22823;&#35268;&#27169;&#22825;&#25991;&#35843;&#26597;&#26102;&#20195;&#35782;&#21035;&#31232;&#26377;&#30340;&#30701;&#26242;&#29616;&#35937;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#22823;&#37096;&#20998;&#22825;&#25991;&#26242;&#21464;&#26816;&#27979;&#31639;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#20174;&#20809;&#21464;&#26354;&#32447;&#20013;&#25552;&#21462;&#30340;&#25163;&#24037;&#29305;&#24449;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#36890;&#36807;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#29983;&#25104;&#30340;&#29305;&#24449;&#65292;&#28982;&#21518;&#19982;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#32467;&#21512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21478;&#31867;&#30340;&#26816;&#27979;&#24322;&#24120;&#30340;&#26041;&#27861;&#65306;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#20498;&#25968;&#31532;&#20108;&#23618;&#20316;&#20026;&#24322;&#24120;&#26816;&#27979;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#31867;&#23396;&#31435;&#26862;&#26519;&#65288;MCIF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#27599;&#20010;&#31867;&#21035;&#35757;&#32451;&#21333;&#29420;&#30340;&#23396;&#31435;&#26862;&#26519;&#26469;&#25512;&#23548;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14742v1 Announce Type: cross  Abstract: Automating real-time anomaly detection is essential for identifying rare transients in the era of large-scale astronomical surveys. Modern survey telescopes are generating tens of thousands of alerts per night, and future telescopes, such as the Vera C. Rubin Observatory, are projected to increase this number dramatically. Currently, most anomaly detection algorithms for astronomical transients rely either on hand-crafted features extracted from light curves or on features generated through unsupervised representation learning, which are then coupled with standard machine learning anomaly detection algorithms. In this work, we introduce an alternative approach to detecting anomalies: using the penultimate layer of a neural network classifier as the latent space for anomaly detection. We then propose a novel method, named Multi-Class Isolation Forests (MCIF), which trains separate isolation forests for each class to derive an anomaly sc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;</title><link>https://arxiv.org/abs/2403.14738</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#21355;&#26143;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
A task of anomaly detection for a smart satellite Internet of things system
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14738
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35774;&#22791;&#24037;&#20316;&#26102;&#65292;&#23454;&#26102;&#25910;&#38598;&#29615;&#22659;&#20256;&#24863;&#22120;&#25968;&#25454;&#20197;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#26159;&#38450;&#27490;&#24037;&#19994;&#36807;&#31243;&#20107;&#25925;&#21644;&#32593;&#32476;&#25915;&#20987;&#12289;&#30830;&#20445;&#31995;&#32479;&#23433;&#20840;&#30340;&#20851;&#38190;&#29615;&#33410;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#29305;&#23450;&#23454;&#26102;&#38656;&#27714;&#30340;&#29615;&#22659;&#20013;&#65292;&#29615;&#22659;&#20256;&#24863;&#22120;&#30340;&#24322;&#24120;&#26816;&#27979;&#20173;&#28982;&#38754;&#20020;&#20197;&#19979;&#22256;&#38590;&#65306;&#65288;1&#65289;&#29615;&#22659;&#20256;&#24863;&#22120;&#25968;&#25454;&#21464;&#37327;&#20043;&#38388;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#29305;&#24615;&#32570;&#20047;&#26377;&#25928;&#30340;&#34920;&#36798;&#26041;&#27861;&#65292;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#38590;&#20197;&#25429;&#25417;&#12290;&#65288;2&#65289;&#20351;&#29992;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24456;&#38590;&#30830;&#20445;&#23454;&#26102;&#30417;&#25511;&#35201;&#27714;&#65292;&#24182;&#19988;&#35774;&#22791;&#25104;&#26412;&#36807;&#39640;&#12290;&#65288;3&#65289;&#26679;&#26412;&#25968;&#25454;&#22826;&#23569;&#23548;&#33268;&#30417;&#30563;&#23398;&#20064;&#20013;&#26631;&#35760;&#25968;&#25454;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#12290;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;consi
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14738v1 Announce Type: new  Abstract: When the equipment is working, real-time collection of environmental sensor data for anomaly detection is one of the key links to prevent industrial process accidents and network attacks and ensure system security. However, under the environment with specific real-time requirements, the anomaly detection for environmental sensors still faces the following difficulties: (1) The complex nonlinear correlation characteristics between environmental sensor data variables lack effective expression methods, and the distribution between the data is difficult to be captured. (2) it is difficult to ensure the real-time monitoring requirements by using complex machine learning models, and the equipment cost is too high. (3) Too little sample data leads to less labeled data in supervised learning. This paper proposes an unsupervised deep learning anomaly detection system. Based on the generative adversarial network and self-attention mechanism, consi
&lt;/p&gt;</description></item><item><title>FedMef&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#20869;&#23384;&#39640;&#25928;&#30340;&#32852;&#37030;&#21160;&#24577;&#21098;&#26525;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#31639;&#24863;&#30693;&#30340;&#25380;&#20986;&#26426;&#21046;&#21644;&#32553;&#25918;&#28608;&#27963;&#21098;&#26525;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#36864;&#21270;&#21644;&#39640;&#28608;&#27963;&#20869;&#23384;&#20351;&#29992;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14737</link><description>&lt;p&gt;
FedMef&#65306;&#38754;&#21521;&#20869;&#23384;&#39640;&#25928;&#30340;&#32852;&#37030;&#21160;&#24577;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
FedMef: Towards Memory-efficient Federated Dynamic Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14737
&lt;/p&gt;
&lt;p&gt;
FedMef&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#20869;&#23384;&#39640;&#25928;&#30340;&#32852;&#37030;&#21160;&#24577;&#21098;&#26525;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#31639;&#24863;&#30693;&#30340;&#25380;&#20986;&#26426;&#21046;&#21644;&#32553;&#25918;&#28608;&#27963;&#21098;&#26525;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#36864;&#21270;&#21644;&#39640;&#28608;&#27963;&#20869;&#23384;&#20351;&#29992;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#39044;&#31639;&#24863;&#30693;&#30340;&#25380;&#20986;&#26426;&#21046;&#20197;&#32500;&#25345;&#21098;&#26525;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#22312;&#32473;&#23450;&#39044;&#31639;&#20869;&#20174;&#26631;&#35760;&#20026;&#21098;&#26525;&#30340;&#21442;&#25968;&#20013;&#25405;&#25937;&#20851;&#38190;&#20449;&#24687;&#26469;&#20445;&#25345;&#21098;&#26525;&#21518;&#24615;&#33021;&#65307;&#25552;&#20986;&#20102;&#32553;&#25918;&#28608;&#27963;&#21098;&#26525;&#20197;&#26377;&#25928;&#20943;&#23569;&#28608;&#27963;&#20869;&#23384;&#20351;&#29992;&#30340;Federated dynamic pruning framework&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14737v1 Announce Type: new  Abstract: Federated learning (FL) promotes decentralized training while prioritizing data confidentiality. However, its application on resource-constrained devices is challenging due to the high demand for computation and memory resources to train deep learning models. Neural network pruning techniques, such as dynamic pruning, could enhance model efficiency, but directly adopting them in FL still poses substantial challenges, including post-pruning performance degradation, high activation memory usage, etc. To address these challenges, we propose FedMef, a novel and memory-efficient federated dynamic pruning framework. FedMef comprises two key components. First, we introduce the budget-aware extrusion that maintains pruning efficiency while preserving post-pruning performance by salvaging crucial information from parameters marked for pruning within a given budget. Second, we propose scaled activation pruning to effectively reduce activation memo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;NaNa&#21644;MiGu&#20004;&#31181;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#34507;&#30333;&#36136;&#30340;&#20027;&#38142;&#21270;&#23398;&#21644;&#20391;&#38142;&#29983;&#29289;&#29289;&#29702;&#20449;&#24687;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.14736</link><description>&lt;p&gt;
NaNa&#21644;MiGu&#65306;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#34507;&#30333;&#36136;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein Classification in Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14736
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;NaNa&#21644;MiGu&#20004;&#31181;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#34507;&#30333;&#36136;&#30340;&#20027;&#38142;&#21270;&#23398;&#21644;&#20391;&#38142;&#29983;&#29289;&#29289;&#29702;&#20449;&#24687;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#26159;&#21160;&#24577;&#21464;&#21270;&#30340;&#65292;&#36825;&#23558;&#20915;&#23450;&#34507;&#30333;&#36136;&#30340;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;ProNet&#65292;&#20165;&#35775;&#38382;&#26377;&#38480;&#30340;&#26500;&#35937;&#29305;&#24449;&#21644;&#34507;&#30333;&#36136;&#20391;&#38142;&#29305;&#24449;&#65292;&#23548;&#33268;&#39044;&#27979;&#20013;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#19981;&#20999;&#23454;&#38469;&#21644;&#34507;&#30333;&#36136;&#31867;&#21035;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;NaNa&#21644;MiGu&#65292;&#23558;&#34507;&#30333;&#36136;&#20027;&#38142;&#21270;&#23398;&#21644;&#20391;&#38142;&#29983;&#29289;&#29289;&#29702;&#20449;&#24687;&#32435;&#20837;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#21644;&#20849;&#23884;&#27531;&#24046;&#23398;&#20064;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#34507;&#30333;&#36136;&#30340;&#20998;&#23376;&#29983;&#29289;&#29289;&#29702;&#12289;&#20108;&#32423;&#32467;&#26500;&#12289;&#21270;&#23398;&#38190;&#21644;&#31163;&#23376;&#29305;&#24449;&#26469;&#20419;&#36827;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14736v1 Announce Type: cross  Abstract: Protein classification tasks are essential in drug discovery. Real-world protein structures are dynamic, which will determine the properties of proteins. However, the existing machine learning methods, like ProNet (Wang et al., 2022a), only access limited conformational characteristics and protein side-chain features, leading to impractical protein structure and inaccuracy of protein classes in their predictions. In this paper, we propose novel semantic data augmentation methods, Novel Augmentation of New Node Attributes (NaNa), and Molecular Interactions and Geometric Upgrading (MiGu) to incorporate backbone chemical and side-chain biophysical information into protein classification tasks and a co-embedding residual learning framework. Specifically, we leverage molecular biophysical, secondary structure, chemical bonds, and ionic features of proteins to facilitate protein classification tasks. Furthermore, our semantic augmentation me
&lt;/p&gt;</description></item><item><title>Foundation Models&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#24102;&#26469;&#21019;&#26032;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#27169;&#22411;&#26469;&#33719;&#24471;&#20855;&#20307;&#23450;&#21046;&#30340;&#24191;&#20041;&#30693;&#35782;&#65292;&#25552;&#21319;&#20102;&#23454;&#36341;&#20013;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14735</link><description>&lt;p&gt;
&#22522;&#20110;Foundation Models&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65306;&#25945;&#31243;&#19982;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Foundation Models for Time Series Analysis: A Tutorial and Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14735
&lt;/p&gt;
&lt;p&gt;
Foundation Models&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#24102;&#26469;&#21019;&#26032;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#27169;&#22411;&#26469;&#33719;&#24471;&#20855;&#20307;&#23450;&#21046;&#30340;&#24191;&#20041;&#30693;&#35782;&#65292;&#25552;&#21319;&#20102;&#23454;&#36341;&#20013;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20316;&#20026;&#25968;&#25454;&#25366;&#25496;&#39046;&#22495;&#30340;&#28966;&#28857;&#65292;&#26159;&#25552;&#21462;&#26377;&#20215;&#20540;&#35265;&#35299;&#30340;&#22522;&#30707;&#65292;&#23545;&#20247;&#22810;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;Foundation Models&#65288;FMs&#65289;&#30340;&#21457;&#23637;&#26681;&#26412;&#24615;&#22320;&#25913;&#21464;&#20102;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#27169;&#22411;&#35774;&#35745;&#30340;&#33539;&#24335;&#65292;&#25552;&#21319;&#20102;&#23454;&#36341;&#20013;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;FMs&#65292;&#20197;&#33719;&#21462;&#19987;&#38376;&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#37327;&#36523;&#23450;&#21046;&#30340;&#24191;&#20041;&#30693;&#35782;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;Foundation Models&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#20840;&#38754;&#21644;&#26368;&#26032;&#27010;&#36848;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#35843;&#26597;&#20027;&#35201;&#38598;&#20013;&#22312;Foundation Models&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#25110;&#31649;&#36947;&#26041;&#38754;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#28145;&#20837;&#20102;&#35299;&#38416;&#26126;Foundation Models&#22914;&#20309;&#21463;&#30410;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#37319;&#29992;&#20102;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14735v1 Announce Type: new  Abstract: Time series analysis stands as a focal point within the data mining community, serving as a cornerstone for extracting valuable insights crucial to a myriad of real-world applications. Recent advancements in Foundation Models (FMs) have fundamentally reshaped the paradigm of model design for time series analysis, boosting various downstream tasks in practice. These innovative approaches often leverage pre-trained or fine-tuned FMs to harness generalized knowledge tailored specifically for time series analysis. In this survey, we aim to furnish a comprehensive and up-to-date overview of FMs for time series analysis. While prior surveys have predominantly focused on either the application or the pipeline aspects of FMs in time series analysis, they have often lacked an in-depth understanding of the underlying mechanisms that elucidate why and how FMs benefit time series analysis. To address this gap, our survey adopts a model-centric class
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;MulCanon&#26469;&#22788;&#29702;&#24320;&#25918;&#30693;&#35782;&#24211;&#65288;OKB&#65289;&#35268;&#33539;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22312;&#36719;&#32858;&#31867;&#36807;&#31243;&#20013;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#36827;&#21517;&#35789;&#30701;&#35821;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.14733</link><description>&lt;p&gt;
&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#24320;&#25918;&#30693;&#35782;&#24211;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
Open Knowledge Base Canonicalization with Multi-task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14733
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;MulCanon&#26469;&#22788;&#29702;&#24320;&#25918;&#30693;&#35782;&#24211;&#65288;OKB&#65289;&#35268;&#33539;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22312;&#36719;&#32858;&#31867;&#36807;&#31243;&#20013;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#36827;&#21517;&#35789;&#30701;&#35821;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#24320;&#25918;&#30693;&#35782;&#24211;&#65288;OKB&#65289;&#30340;&#26500;&#24314;&#23545;&#20110;&#35832;&#22810;&#22522;&#20110;&#30693;&#35782;&#30340;&#32593;&#32476;&#24212;&#29992;&#22914;&#32593;&#32476;&#25628;&#32034;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;OKB&#20013;&#30340;&#21517;&#35789;&#30701;&#35821;&#21644;&#20851;&#31995;&#30701;&#35821;&#24448;&#24448;&#23384;&#22312;&#20887;&#20313;&#21644;&#27495;&#20041;&#65292;&#36825;&#38656;&#35201;&#23545;OKB&#36827;&#34892;&#35268;&#33539;&#21270;&#30340;&#30740;&#31350;&#12290;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#35774;&#35745;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGE&#65289;&#36827;&#19968;&#27493;&#20419;&#36827;&#35268;&#33539;&#21270;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#32858;&#31867;&#21644;KGE&#23398;&#20064;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#24182;&#19988;&#20026;&#36825;&#20123;&#23376;&#20219;&#21153;&#35774;&#35745;&#30340;&#26041;&#27861;&#26159;&#27425;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MulCanon&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;OKB&#30340;&#35268;&#33539;&#21270;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22312;&#36719;&#32858;&#31867;&#36807;&#31243;&#20013;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#36827;&#21517;&#35789;&#30701;&#35821;&#30340;&#34920;&#31034;&#65292;&#24102;&#26469;&#26356;&#20934;&#30830;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14733v1 Announce Type: new  Abstract: The construction of large open knowledge bases (OKBs) is integral to many knowledge-driven applications on the world wide web such as web search. However, noun phrases and relational phrases in OKBs often suffer from redundancy and ambiguity, which calls for the investigation on OKB canonicalization. Current solutions address OKB canonicalization by devising advanced clustering algorithms and using knowledge graph embedding (KGE) to further facilitate the canonicalization process. Nevertheless, these works fail to fully exploit the synergy between clustering and KGE learning, and the methods designed for these subtasks are sub-optimal. To this end, we put forward a multi-task learning framework, namely MulCanon, to tackle OKB canonicalization. In addition, diffusion model is used in the soft clustering process to improve the noun phrase representations with neighboring information, which can lead to more accurate representations. MulCano
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#21487;&#36870;&#36339;&#21160;&#25915;&#20987;&#65288;RJA&#65289;&#21644;Metropolis-Hasting&#20462;&#25913;&#32553;&#20943;&#65288;MMR&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#24230;&#26377;&#25928;&#30340;&#23545;&#25239;&#31034;&#20363;&#65292;&#24182;&#20998;&#21035;&#25913;&#21892;&#31034;&#20363;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14731</link><description>&lt;p&gt;
&#21487;&#36870;&#36339;&#21160;&#25915;&#20987;&#23545;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20462;&#25913;&#32553;&#20943;
&lt;/p&gt;
&lt;p&gt;
Reversible Jump Attack to Textual Classifiers with Modification Reduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14731
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#21487;&#36870;&#36339;&#21160;&#25915;&#20987;&#65288;RJA&#65289;&#21644;Metropolis-Hasting&#20462;&#25913;&#32553;&#20943;&#65288;MMR&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#24230;&#26377;&#25928;&#30340;&#23545;&#25239;&#31034;&#20363;&#65292;&#24182;&#20998;&#21035;&#25913;&#21892;&#31034;&#20363;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23545;&#25239;&#26679;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#30340;&#28431;&#27934;&#12290;&#29616;&#26377;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#30340;&#25216;&#26415;&#36890;&#24120;&#30001;&#30830;&#23450;&#24615;&#30340;&#23618;&#27425;&#35268;&#21017;&#39537;&#21160;&#65292;&#36825;&#20123;&#35268;&#21017;&#23545;&#26368;&#20248;&#23545;&#25239;&#26679;&#26412;&#27627;&#19981;&#20851;&#24515;&#65292;&#36890;&#24120;&#23548;&#33268;&#23545;&#25239;&#26679;&#26412;&#22312;&#21464;&#21270;&#24133;&#24230;&#21644;&#25915;&#20987;&#25104;&#21151;&#20043;&#38388;&#23384;&#22312;&#27425;&#20248;&#24179;&#34913;&#12290;&#22312;&#27492;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#21487;&#36870;&#36339;&#21160;&#25915;&#20987;&#65288;RJA&#65289;&#21644;Metropolis-Hasting&#20462;&#25913;&#32553;&#20943;&#65288;MMR&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#24230;&#26377;&#25928;&#30340;&#23545;&#25239;&#31034;&#20363;&#24182;&#20998;&#21035;&#25913;&#21892;&#31034;&#20363;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;RJA&#21033;&#29992;&#26032;&#39062;&#30340;&#38543;&#26426;&#21270;&#26426;&#21046;&#26469;&#25193;&#22823;&#25628;&#32034;&#31354;&#38388;&#65292;&#26377;&#25928;&#36866;&#24212;&#23545;&#25239;&#26679;&#26412;&#20013;&#30340;&#22810;&#20010;&#25200;&#21160;&#35789;&#27719;&#12290;&#21033;&#29992;&#29983;&#25104;&#30340;&#23545;&#25239;&#31034;&#20363;&#65292;MMR&#24212;&#29992;Metropolis-Hasting&#37319;&#26679;&#22120;&#20197;&#22686;&#24378;&#23545;&#25239;&#31034;&#20363;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14731v1 Announce Type: cross  Abstract: Recent studies on adversarial examples expose vulnerabilities of natural language processing (NLP) models. Existing techniques for generating adversarial examples are typically driven by deterministic hierarchical rules that are agnostic to the optimal adversarial examples, a strategy that often results in adversarial samples with a suboptimal balance between magnitudes of changes and attack successes. To this end, in this research we propose two algorithms, Reversible Jump Attack (RJA) and Metropolis-Hasting Modification Reduction (MMR), to generate highly effective adversarial examples and to improve the imperceptibility of the examples, respectively. RJA utilizes a novel randomization mechanism to enlarge the search space and efficiently adapts to a number of perturbed words for adversarial examples. With these generated adversarial examples, MMR applies the Metropolis-Hasting sampler to enhance the imperceptibility of adversarial e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Auto-Train-Once&#65288;ATO&#65289;&#32593;&#32476;&#21098;&#26525;&#31639;&#27861;&#65292;&#21033;&#29992;&#25511;&#21046;&#22120;&#32593;&#32476;&#33258;&#21160;&#38477;&#20302;DNN&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#22686;&#24378;&#21327;&#35843;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14729</link><description>&lt;p&gt;
Auto-Train-Once&#65306;&#20174;&#38646;&#24320;&#22987;&#25351;&#23548;&#30340;&#25511;&#21046;&#22120;&#32593;&#32476;&#33258;&#21160;&#32593;&#32476;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Auto-Train-Once: Controller Network Guided Automatic Network Pruning from Scratch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14729
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Auto-Train-Once&#65288;ATO&#65289;&#32593;&#32476;&#21098;&#26525;&#31639;&#27861;&#65292;&#21033;&#29992;&#25511;&#21046;&#22120;&#32593;&#32476;&#33258;&#21160;&#38477;&#20302;DNN&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#22686;&#24378;&#21327;&#35843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21098;&#26525;&#25216;&#26415;&#36890;&#24120;&#28041;&#21450;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#36807;&#31243;&#65292;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#20351;&#20854;&#26222;&#36941;&#37319;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#27425;&#35757;&#32451;&#65288;OTO&#65289;&#21644;OTOv2&#65292;&#23427;&#20204;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#21644;&#21387;&#32553;&#36890;&#29992;DNN&#26469;&#28040;&#38500;&#39069;&#22806;&#30340;&#24494;&#35843;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#22120;&#30340;&#38745;&#24577;&#35774;&#35745;&#65288;&#22312;OTO&#20013;&#65289;&#21487;&#33021;&#23548;&#33268;&#23616;&#37096;&#26368;&#20248;&#35299;&#30340;&#25910;&#25947;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Auto-Train-Once&#65288;ATO&#65289;&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#32593;&#32476;&#21098;&#26525;&#31639;&#27861;&#65292;&#26088;&#22312;&#33258;&#21160;&#20943;&#23569;DNN&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#22312;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#65292;&#36824;&#21033;&#29992;&#25511;&#21046;&#22120;&#32593;&#32476;&#20316;&#20026;&#26550;&#26500;&#29983;&#25104;&#22120;&#26469;&#24341;&#23548;&#30446;&#26631;&#27169;&#22411;&#26435;&#37325;&#30340;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#22686;&#24378;&#21327;&#35843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14729v1 Announce Type: cross  Abstract: Current techniques for deep neural network (DNN) pruning often involve intricate multi-step processes that require domain-specific expertise, making their widespread adoption challenging. To address the limitation, the Only-Train-Once (OTO) and OTOv2 are proposed to eliminate the need for additional fine-tuning steps by directly training and compressing a general DNN from scratch. Nevertheless, the static design of optimizers (in OTO) can lead to convergence issues of local optima. In this paper, we proposed the Auto-Train-Once (ATO), an innovative network pruning algorithm designed to automatically reduce the computational and storage costs of DNNs. During the model training phase, our approach not only trains the target model but also leverages a controller network as an architecture generator to guide the learning of target model weights. Furthermore, we developed a novel stochastic gradient algorithm that enhances the coordination 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20262;&#29702;&#21644;&#20844;&#24179;&#39046;&#22495;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#19981;&#20165;&#21453;&#26144;&#20102;&#31038;&#20250;&#20559;&#35265;&#65292;&#36824;&#20284;&#20046;&#25918;&#22823;&#20102;&#36825;&#20123;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.14727</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21463;&#20445;&#25252;&#32676;&#20307;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Protected group bias and stereotypes in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14727
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20262;&#29702;&#21644;&#20844;&#24179;&#39046;&#22495;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#19981;&#20165;&#21453;&#26144;&#20102;&#31038;&#20250;&#20559;&#35265;&#65292;&#36824;&#20284;&#20046;&#25918;&#22823;&#20102;&#36825;&#20123;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25171;&#30772;&#35768;&#22810;&#26368;&#26032;&#25216;&#26415;&#22522;&#20934;&#65292;&#26412;&#25991;&#35843;&#26597;&#20102;&#23427;&#20204;&#22312;&#20262;&#29702;&#21644;&#20844;&#24179;&#39046;&#22495;&#30340;&#34892;&#20026;&#65292;&#37325;&#28857;&#20851;&#27880;&#21463;&#20445;&#25252;&#32676;&#20307;&#20559;&#35265;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#37096;&#20998;&#30740;&#31350;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24449;&#38598;&#20102;&#25551;&#36848;&#26469;&#33258;&#19981;&#21516;&#21463;&#20445;&#25252;&#32676;&#20307;&#65288;&#21253;&#25324;&#24615;&#21035;&#12289;&#24615;&#21462;&#21521;&#12289;&#23447;&#25945;&#21644;&#31181;&#26063;&#65289;&#20010;&#20154;&#32844;&#19994;&#30340;&#21477;&#23376;&#24310;&#32493;&#65307;&#20854;&#27425;&#65292;&#25105;&#20204;&#35753;&#27169;&#22411;&#29983;&#25104;&#20851;&#20110;&#25317;&#26377;&#19981;&#21516;&#31867;&#22411;&#32844;&#19994;&#30340;&#20010;&#20154;&#30340;&#25925;&#20107;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#27454;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340; &gt;10k &#20010;&#21477;&#23376;&#24310;&#32493;&#65292;&#21463;&#21040;&#20154;&#31867;&#26631;&#27880;&#12290;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#34987;&#36793;&#32536;&#21270;&#32676;&#20307;&#20013;&#23384;&#22312;&#20559;&#35265;&#65292;&#23588;&#20854;&#22312;&#24615;&#21035;&#21644;&#24615;&#21462;&#21521;&#39046;&#22495;&#65292;&#20197;&#21450;&#22312;&#27169;&#22411;&#29983;&#25104;&#20013;&#23384;&#22312;&#35199;&#26041;&#20559;&#35265;&#12290;&#27169;&#22411;&#19981;&#20165;&#21453;&#26144;&#20102;&#31038;&#20250;&#20559;&#35265;&#65292;&#36824;&#20284;&#20046;&#25918;&#22823;&#20102;&#36825;&#20123;&#20559;&#35265;&#12290;&#35813;&#27169;&#22411;&#23545;&#20110;&#19982;&#36793;&#32536;&#21270;&#32676;&#20307;&#30456;&#20851;&#30340;&#26597;&#35810;&#22238;&#22797;&#36807;&#20110;&#35880;&#24910;&#65292;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14727v1 Announce Type: cross  Abstract: As modern Large Language Models (LLMs) shatter many state-of-the-art benchmarks in a variety of domains, this paper investigates their behavior in the domains of ethics and fairness, focusing on protected group bias. We conduct a two-part study: first, we solicit sentence continuations describing the occupations of individuals from different protected groups, including gender, sexuality, religion, and race. Second, we have the model generate stories about individuals who hold different types of occupations. We collect &gt;10k sentence completions made by a publicly available LLM, which we subject to human annotation. We find bias across minoritized groups, but in particular in the domains of gender and sexuality, as well as Western bias, in model generations. The model not only reflects societal biases, but appears to amplify them. The model is additionally overly cautious in replies to queries relating to minoritized groups, providing re
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#20013;"&#36234;&#29425;"&#25915;&#20987;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#23450;&#20041;&#22909;&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#26469;&#36827;&#34892;&#38450;&#24481;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20110;&#25191;&#34892;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.14725</link><description>&lt;p&gt;
Jailbreaking&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#23450;&#20041;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking is Best Solved by Definition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14725
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;"&#36234;&#29425;"&#25915;&#20987;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#23450;&#20041;&#22909;&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#26469;&#36827;&#34892;&#38450;&#24481;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20110;&#25191;&#34892;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19978;"&#36234;&#29425;"&#25915;&#20987;&#30340;&#22686;&#22810;&#24341;&#21457;&#20102;&#22823;&#37327;&#38450;&#24481;&#24037;&#20316;&#65292;&#26088;&#22312;&#38450;&#27490;&#20135;&#29983;&#19981;&#33391;&#22238;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#23457;&#35270;&#20102;&#38450;&#24481;&#31649;&#36947;&#30340;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#23450;&#20041;&#20309;&#20026;&#19981;&#23433;&#20840;&#36755;&#20986;&#65292;&#21644;&#65288;ii&#65289;&#36890;&#36807;&#36755;&#20837;&#22788;&#29702;&#25110;&#24494;&#35843;&#31561;&#26041;&#27861;&#26469;&#25191;&#34892;&#35813;&#23450;&#20041;&#12290;&#25105;&#20204;&#20005;&#37325;&#24576;&#30097;&#29616;&#26377;&#30340;&#25191;&#34892;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23637;&#31034;&#23427;&#20204;&#21363;&#20351;&#23545;&#20110;&#31616;&#21333;&#30340;&#19981;&#23433;&#20840;&#36755;&#20986;&#23450;&#20041;--&#21253;&#21547;&#21333;&#35789;"purple"&#30340;&#36755;&#20986;&#20063;&#26080;&#27861;&#38450;&#24481;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23545;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#23545;&#20110;&#36825;&#26679;&#30340;&#23450;&#20041;&#26159;&#23436;&#20840;&#20581;&#22766;&#30340;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#21363;&#22312;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#20013;&#30495;&#27491;&#30340;&#25361;&#25112;&#22312;&#20110;&#24471;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#23450;&#20041;&#65306;&#27809;&#26377;&#33391;&#22909;&#30340;&#23450;&#20041;&#65292;&#20219;&#20309;&#25191;&#34892;&#31574;&#30053;&#37117;&#26080;&#27861;&#25104;&#21151;&#65292;&#20294;&#26377;&#20102;&#33391;&#22909;&#30340;&#23450;&#20041;&#65292;&#36755;&#20986;&#22788;&#29702;&#24050;&#32463;&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14725v1 Announce Type: cross  Abstract: The rise of "jailbreak" attacks on language models has led to a flurry of defenses aimed at preventing the output of undesirable responses. In this work, we critically examine the two stages of the defense pipeline: (i) the definition of what constitutes unsafe outputs, and (ii) the enforcement of the definition via methods such as input processing or fine-tuning. We cast severe doubt on the efficacy of existing enforcement mechanisms by showing that they fail to defend even for a simple definition of unsafe outputs--outputs that contain the word "purple". In contrast, post-processing outputs is perfectly robust for such a definition. Drawing on our results, we present our position that the real challenge in defending jailbreaks lies in obtaining a good definition of unsafe responses: without a good definition, no enforcement strategy can succeed, but with a good definition, output processing already serves as a robust baseline albeit 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#37329;&#34701;&#21512;&#25104;&#25968;&#25454;&#30340;&#8220;&#38544;&#31169;&#23618;&#27425;&#8221;&#23618;&#32423;&#65292;&#26377;&#21161;&#20110;&#35780;&#20272;&#38544;&#31169;&#20445;&#25252;&#31243;&#24230;&#21644;&#20998;&#31867;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.14724</link><description>&lt;p&gt;
&#38544;&#31169;&#30340;&#20845;&#20010;&#23618;&#27425;&#65306;&#37329;&#34701;&#21512;&#25104;&#25968;&#25454;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Six Levels of Privacy: A Framework for Financial Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#37329;&#34701;&#21512;&#25104;&#25968;&#25454;&#30340;&#8220;&#38544;&#31169;&#23618;&#27425;&#8221;&#23618;&#32423;&#65292;&#26377;&#21161;&#20110;&#35780;&#20272;&#38544;&#31169;&#20445;&#25252;&#31243;&#24230;&#21644;&#20998;&#31867;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#22312;&#37329;&#34701;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#38500;&#20102;&#23427;&#25552;&#20379;&#30340;&#22909;&#22788;&#65292;&#22914;&#25913;&#36827;&#37329;&#34701;&#24314;&#27169;&#21644;&#26356;&#22909;&#30340;&#27979;&#35797;&#31243;&#24207;&#65292;&#23427;&#20063;&#23384;&#22312;&#30528;&#38544;&#31169;&#39118;&#38505;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#28304;&#33258;&#23458;&#25143;&#20449;&#24687;&#12289;&#21830;&#19994;&#20449;&#24687;&#25110;&#20854;&#20182;&#24517;&#39035;&#21463;&#21040;&#20445;&#25252;&#30340;&#19987;&#26377;&#26469;&#28304;&#12290;&#34429;&#28982;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#36807;&#31243;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#26377;&#21161;&#20110;&#25513;&#30422;&#21407;&#22987;&#25968;&#25454;&#65292;&#20294;&#24456;&#38590;&#35780;&#20272;&#38544;&#31169;&#20445;&#25252;&#30340;&#31243;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#26377;&#29992;&#20110;&#20998;&#31867;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#21644;&#36880;&#28176;&#25913;&#36827;&#30340;&#20445;&#25252;&#25514;&#26045;&#30340;"&#38544;&#31169;&#23618;&#27425;"&#30340;&#31561;&#32423;&#12290;&#23613;&#31649;&#36825;&#20845;&#20010;&#32423;&#21035;&#26159;&#22312;&#37329;&#34701;&#24212;&#29992;&#30340;&#32972;&#26223;&#19979;&#35774;&#35745;&#30340;&#65292;&#20294;&#23427;&#20204;&#22312;&#20854;&#20182;&#34892;&#19994;&#20013;&#20063;&#21487;&#33021;&#36866;&#29992;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#21253;&#25324;&#65306;&#23545;&#37329;&#34701;&#21512;&#25104;&#25968;&#25454;&#30340;&#31616;&#35201;&#27010;&#36848;&#65292;&#23427;&#22914;&#20309;&#34987;&#20351;&#29992;&#65292;&#22914;&#20309;&#35780;&#20272;&#20854;&#20215;&#20540;&#65292;&#38544;&#31169;&#39118;&#38505;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14724v1 Announce Type: cross  Abstract: Synthetic Data is increasingly important in financial applications. In addition to the benefits it provides, such as improved financial modeling and better testing procedures, it poses privacy risks as well. Such data may arise from client information, business information, or other proprietary sources that must be protected. Even though the process by which Synthetic Data is generated serves to obscure the original data to some degree, the extent to which privacy is preserved is hard to assess. Accordingly, we introduce a hierarchy of ``levels'' of privacy that are useful for categorizing Synthetic Data generation methods and the progressively improved protections they offer. While the six levels were devised in the context of financial applications, they may also be appropriate for other industries as well. Our paper includes: A brief overview of Financial Synthetic Data, how it can be used, how its value can be assessed, privacy ris
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#32858;&#28966;&#25216;&#26415;&#65292;&#19968;&#31181;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#29992;&#20110;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#20010;&#36755;&#20837;&#28304;&#26102;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25552;&#20379;&#21487;&#38752;&#30340;&#36755;&#20837;&#26469;&#28304;&#20449;&#21495;&#26469;&#38450;&#24481;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.14720</link><description>&lt;p&gt;
&#20351;&#29992;&#32858;&#28966;&#25216;&#26415;&#25269;&#24481;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Against Indirect Prompt Injection Attacks With Spotlighting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14720
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#32858;&#28966;&#25216;&#26415;&#65292;&#19968;&#31181;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#29992;&#20110;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#20010;&#36755;&#20837;&#28304;&#26102;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25552;&#20379;&#21487;&#38752;&#30340;&#36755;&#20837;&#26469;&#28304;&#20449;&#21495;&#26469;&#38450;&#24481;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34429;&#28982;&#24378;&#22823;&#65292;&#21364;&#26159;&#24314;&#31435;&#21644;&#35757;&#32451;&#29992;&#20110;&#22788;&#29702;&#21333;&#20010;&#25991;&#26412;&#36755;&#20837;&#30340;&#12290;&#22312;&#24120;&#35265;&#24212;&#29992;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#22810;&#20010;&#36755;&#20837;&#36830;&#25509;&#22312;&#19968;&#36215;&#24418;&#25104;&#21333;&#20010;&#25991;&#26412;&#27969;&#26469;&#36827;&#34892;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;LLM&#26080;&#27861;&#21306;&#20998;&#25552;&#31034;&#30340;&#21738;&#20123;&#37096;&#20998;&#23646;&#20110;&#19981;&#21516;&#30340;&#36755;&#20837;&#28304;&#12290;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21033;&#29992;&#36825;&#19968;&#28431;&#27934;&#65292;&#23558;&#23545;&#25163;&#25351;&#20196;&#23884;&#20837;&#21040;&#19982;&#29992;&#25143;&#21629;&#20196;&#19968;&#36215;&#22788;&#29702;&#30340;&#19981;&#21463;&#20449;&#20219;&#25968;&#25454;&#20013;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;LLM&#20250;&#35823;&#23558;&#23545;&#25163;&#25351;&#20196;&#20316;&#20026;&#35201;&#36981;&#24490;&#30340;&#29992;&#25143;&#25351;&#20196;&#65292;&#20174;&#32780;&#22312;&#25972;&#20010;&#31995;&#32479;&#20013;&#21019;&#24314;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32858;&#28966;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;LLMs&#21306;&#20998;&#22810;&#20010;&#36755;&#20837;&#28304;&#33021;&#21147;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#31995;&#21015;&#12290;&#20851;&#38190;&#30340;&#35265;&#35299;&#26159;&#21033;&#29992;&#36755;&#20837;&#30340;&#21464;&#25442;&#25552;&#20379;&#20854;&#26469;&#28304;&#30340;&#21487;&#38752;&#36830;&#32493;&#20449;&#21495;&#12290;&#25105;&#20204;&#23558;&#32858;&#28966;&#25216;&#26415;&#20316;&#20026;&#19968;&#31181;&#38450;&#24481;&#25163;&#27573;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14720v1 Announce Type: cross  Abstract: Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources. Indirect prompt injection attacks take advantage of this vulnerability by embedding adversarial instructions into untrusted data being processed alongside user commands. Often, the LLM will mistake the adversarial instructions as user commands to be followed, creating a security vulnerability in the larger system. We introduce spotlighting, a family of prompt engineering techniques that can be used to improve LLMs' ability to distinguish among multiple sources of input. The key insight is to utilize transformations of an input to provide a reliable and continuous signal of its provenance. We evaluate spotlighting as a defen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#8220;&#33394;&#24425;&#24863;&#30693;&#8221;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#33258;&#25105;&#39068;&#33394;&#27979;&#35797;&#30340;&#26367;&#25442;&#65288;SCTS&#65289;&#65292;&#25104;&#21151;&#22320;&#35268;&#36991;&#20102;LLM&#27700;&#21360;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.14719</link><description>&lt;p&gt;
&#20351;&#29992;&#33394;&#24425;&#24863;&#30693;&#26367;&#25442;&#32469;&#36807;LLM&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Bypassing LLM Watermarks with Color-Aware Substitutions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14719
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#8220;&#33394;&#24425;&#24863;&#30693;&#8221;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#33258;&#25105;&#39068;&#33394;&#27979;&#35797;&#30340;&#26367;&#25442;&#65288;SCTS&#65289;&#65292;&#25104;&#21151;&#22320;&#35268;&#36991;&#20102;LLM&#27700;&#21360;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27700;&#21360;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#20256;&#25773;&#30340;&#25991;&#26412;&#26159;&#20154;&#31867;&#29983;&#25104;&#36824;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#12290;&#29616;&#26377;&#25915;&#20987;&#26041;&#27861;&#26080;&#27861;&#35268;&#36991;&#38271;&#25991;&#26412;&#27573;&#33853;&#30340;&#26816;&#27979;&#65292;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#8220;&#33394;&#24425;&#24863;&#30693;&#8221;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#33258;&#25105;&#39068;&#33394;&#27979;&#35797;&#30340;&#26367;&#25442;&#65288;SCTS&#65289;&#12290;SCTS&#36890;&#36807;&#24341;&#23548;&#24102;&#26377;&#27700;&#21360;&#30340;LLM&#33719;&#21462;&#39068;&#33394;&#20449;&#24687;&#65292;&#24182;&#27604;&#36739;&#36755;&#20986;&#26631;&#35760;&#30340;&#39057;&#29575;&#65292;&#20174;&#32780;&#30830;&#23450;&#26631;&#35760;&#30340;&#39068;&#33394;&#65292;&#24182;&#29992;&#38750;&#32511;&#33394;&#26631;&#35760;&#26367;&#25442;&#32511;&#33394;&#26631;&#35760;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;SCTS&#25104;&#21151;&#22320;&#35268;&#36991;&#20102;&#27700;&#21360;&#26816;&#27979;&#65292;&#19988;&#25152;&#38656;&#30340;&#32534;&#36753;&#27425;&#25968;&#27604;&#30456;&#20851;&#30740;&#31350;&#35201;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14719v1 Announce Type: cross  Abstract: Watermarking approaches are proposed to identify if text being circulated is human or large language model (LLM) generated. The state-of-the-art watermarking strategy of Kirchenbauer et al. (2023a) biases the LLM to generate specific (``green'') tokens. However, determining the robustness of this watermarking method is an open problem. Existing attack methods fail to evade detection for longer text segments. We overcome this limitation, and propose {\em Self Color Testing-based Substitution (SCTS)}, the first ``color-aware'' attack. SCTS obtains color information by strategically prompting the watermarked LLM and comparing output tokens frequencies. It uses this information to determine token colors, and substitutes green tokens with non-green ones. In our experiments, SCTS successfully evades watermark detection using fewer number of edits than related work. Additionally, we show both theoretically and empirically that SCTS can remove
&lt;/p&gt;</description></item><item><title>FedSR&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#20998;&#25955;&#24335;&#30340;&#20113;-&#36793;&#32536;-&#35774;&#22791;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#20986;&#29616;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21644;&#36890;&#20449;&#36164;&#28304;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14718</link><description>&lt;p&gt;
FedSR&#65306;&#29992;&#20110;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#38750;&#29420;&#31435;&#20998;&#24067;&#38382;&#39064;&#30340;&#21322;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedSR: A Semi-Decentralized Federated Learning Algorithm for Non-IIDness in IoT System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14718
&lt;/p&gt;
&lt;p&gt;
FedSR&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#20998;&#25955;&#24335;&#30340;&#20113;-&#36793;&#32536;-&#35774;&#22791;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#20986;&#29616;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21644;&#36890;&#20449;&#36164;&#28304;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#20013;&#65292;&#27599;&#22825;&#37117;&#20250;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#12290;&#30001;&#20110;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#38590;&#20197;&#23558;&#25152;&#26377;&#36825;&#20123;&#25968;&#25454;&#27719;&#38598;&#22312;&#19968;&#36215;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22240;&#27492;&#32852;&#37030;&#23398;&#20064;&#65292;&#19968;&#31181;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#22312;&#29289;&#32852;&#32593;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#35774;&#22791;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#36890;&#24120;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#65292;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#29289;&#32852;&#32593;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#36890;&#24120;&#28041;&#21450;&#22823;&#37327;&#35774;&#22791;&#21442;&#19982;&#35757;&#32451;&#65292;&#20113;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#36164;&#28304;&#26377;&#38480;&#25104;&#20026;&#35757;&#32451;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#23558;&#38598;&#20013;&#24335;&#32852;&#37030;&#23398;&#20064;&#19982;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21322;&#20998;&#25955;&#24335;&#30340;&#20113;-&#36793;&#32536;-&#35774;&#22791;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14718v1 Announce Type: new  Abstract: In the Industrial Internet of Things (IoT), a large amount of data will be generated every day. Due to privacy and security issues, it is difficult to collect all these data together to train deep learning models, thus the federated learning, a distributed machine learning paradigm that protects data privacy, has been widely used in IoT. However, in practical federated learning, the data distributions usually have large differences across devices, and the heterogeneity of data will deteriorate the performance of the model. Moreover, federated learning in IoT usually has a large number of devices involved in training, and the limited communication resource of cloud servers become a bottleneck for training. To address the above issues, in this paper, we combine centralized federated learning with decentralized federated learning to design a semi-decentralized cloud-edge-device hierarchical federated learning framework, which can mitigate t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;1&#27604;&#29305;&#26799;&#24230;&#32534;&#30721;&#30340;&#26032;&#22411;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#28382;&#21518;&#32773;&#30340;&#24773;&#20917;&#19979;&#38477;&#20302;&#36890;&#20449;&#36127;&#25285;&#65292;&#24182;&#22312;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#19979;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.14716</link><description>&lt;p&gt;
&#22522;&#20110;1&#27604;&#29305;&#26799;&#24230;&#32534;&#30721;&#30340;&#23384;&#22312;&#28382;&#21518;&#32773;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributed Learning based on 1-Bit Gradient Coding in the Presence of Stragglers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14716
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;1&#27604;&#29305;&#26799;&#24230;&#32534;&#30721;&#30340;&#26032;&#22411;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#28382;&#21518;&#32773;&#30340;&#24773;&#20917;&#19979;&#38477;&#20302;&#36890;&#20449;&#36127;&#25285;&#65292;&#24182;&#22312;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#19979;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#23384;&#22312;&#28382;&#21518;&#32773;&#30340;&#24773;&#20917;&#19979;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#65288;DL&#65289;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#26799;&#24230;&#32534;&#30721;&#30340;DL&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#20887;&#20313;&#22320;&#23558;&#35757;&#32451;&#25968;&#25454;&#20998;&#21457;&#32473;&#24037;&#20316;&#33410;&#28857;&#65292;&#20197;&#30830;&#20445;&#22312;&#19968;&#20123;&#24037;&#20316;&#33410;&#28857;&#26159;&#28382;&#21518;&#32773;&#26102;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#27714;&#24037;&#20316;&#33410;&#28857;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20256;&#36755;&#23454;&#20540;&#21521;&#37327;&#65292;&#36825;&#23548;&#33268;&#20102;&#38750;&#24120;&#39640;&#30340;&#36890;&#20449;&#36127;&#25285;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;1&#27604;&#29305;&#26799;&#24230;&#32534;&#30721;&#65288;1-bit GCDL&#65289;&#30340;&#26032;&#22411;DL&#26041;&#27861;&#65292;&#20854;&#20013;&#24037;&#20316;&#33410;&#28857;&#20256;&#36755;&#20174;&#26412;&#22320;&#35745;&#31639;&#30340;&#26799;&#24230;&#32534;&#30721;&#32780;&#25104;&#30340;1&#27604;&#29305;&#25968;&#25454;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20026;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#19979;&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;1&#27604;&#29305;GC-DL&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#20854;&#22312;&#23384;&#22312;&#28382;&#21518;&#32773;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14716v1 Announce Type: new  Abstract: This paper considers the problem of distributed learning (DL) in the presence of stragglers. For this problem, DL methods based on gradient coding have been widely investigated, which redundantly distribute the training data to the workers to guarantee convergence when some workers are stragglers. However, these methods require the workers to transmit real-valued vectors during the process of learning, which induces very high communication burden. To overcome this drawback, we propose a novel DL method based on 1-bit gradient coding (1-bit GCDL), where 1-bit data encoded from the locally computed gradients are transmitted by the workers to reduce the communication overhead. We theoretically provide the convergence guarantees of the proposed method for both the convex loss functions and nonconvex loss functions. It is shown empirically that 1-bit GC-DL outperforms the baseline methods, which attains better learning performance under the s
&lt;/p&gt;</description></item><item><title>LS&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#24179;&#28369;&#25928;&#26524;&#34987;&#21457;&#29616;&#20250;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#36890;&#36807;&#24433;&#21709;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#27492;&#30740;&#31350;&#38416;&#26126;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.14715</link><description>&lt;p&gt;
&#29702;&#35299;&#20026;&#20309;&#26631;&#31614;&#24179;&#28369;&#20250;&#38477;&#20302;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#25928;&#26524;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14715
&lt;/p&gt;
&lt;p&gt;
LS&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#24179;&#28369;&#25928;&#26524;&#34987;&#21457;&#29616;&#20250;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#36890;&#36807;&#24433;&#21709;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#27492;&#30740;&#31350;&#38416;&#26126;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#65288;LS&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#22312;&#25552;&#39640;&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#65292;&#24182;&#19988;&#23454;&#29616;&#31616;&#21333;&#12290;"&#30828;"&#30340;one-hot&#26631;&#31614;&#36890;&#36807;&#23558;&#27010;&#29575;&#36136;&#37327;&#22343;&#21248;&#20998;&#37197;&#32473;&#20854;&#20182;&#31867;&#21035;&#26469;&#36827;&#34892;"&#24179;&#28369;&#21270;"&#65292;&#20174;&#32780;&#20943;&#23569;&#36807;&#24230;&#25311;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LS&#22914;&#20309;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65288;SC&#65289;- &#20854;&#30446;&#26631;&#26159;&#21033;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26469;&#25298;&#32477;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#21644;&#26550;&#26500;&#20013;&#20174;&#32463;&#39564;&#19978;&#35777;&#26126;LS&#20250;&#23548;&#33268;SC&#30340;&#19968;&#33268;&#24615;&#38477;&#32423;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;logit&#32423;&#21035;&#30340;&#26799;&#24230;&#26469;&#35299;&#37322;&#36825;&#19968;&#28857;&#65292;&#34920;&#26126;LS&#36890;&#36807;&#22312;&#38169;&#35823;&#27010;&#29575;&#20302;&#26102;&#26356;&#21152;&#27491;&#21017;&#21270;&#26368;&#22823;logit&#65292;&#32780;&#22312;&#38169;&#35823;&#27010;&#29575;&#39640;&#26102;&#26356;&#23569;&#27491;&#21017;&#21270;&#65292;&#21152;&#21095;&#20102;&#36807;&#24230;&#33258;&#20449;&#21644;&#20302;&#33258;&#20449;&#12290;&#36825;&#38416;&#26126;&#20102;&#20197;&#21069;&#25253;&#36947;&#30340;&#24378;&#20998;&#31867;&#22120;&#22312;SC&#20013;&#24615;&#33021;&#19981;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14715v1 Announce Type: cross  Abstract: Label smoothing (LS) is a popular regularisation method for training deep neural network classifiers due to its effectiveness in improving test accuracy and its simplicity in implementation. "Hard" one-hot labels are "smoothed" by uniformly distributing probability mass to other classes, reducing overfitting. In this work, we reveal that LS negatively affects selective classification (SC) - where the aim is to reject misclassifications using a model's predictive uncertainty. We first demonstrate empirically across a range of tasks and architectures that LS leads to a consistent degradation in SC. We then explain this by analysing logit-level gradients, showing that LS exacerbates overconfidence and underconfidence by regularising the max logit more when the probability of error is low, and less when the probability of error is high. This elucidates previously reported experimental results where strong classifiers underperform in SC. We
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32534;&#35793;&#22120;&#20248;&#21270;&#21453;&#39304;&#30340;&#26032;&#33539;&#24335;&#65292;&#33021;&#22815;&#22312;&#20248;&#21270;LLVM&#27719;&#32534;&#20195;&#30721;&#22823;&#23567;&#26041;&#38754;&#21462;&#24471;&#39069;&#22806;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.14714</link><description>&lt;p&gt;
&#32534;&#35793;&#22120;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Compiler generated feedback for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14714
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32534;&#35793;&#22120;&#20248;&#21270;&#21453;&#39304;&#30340;&#26032;&#33539;&#24335;&#65292;&#33021;&#22815;&#22312;&#20248;&#21270;LLVM&#27719;&#32534;&#20195;&#30721;&#22823;&#23567;&#26041;&#38754;&#21462;&#24471;&#39069;&#22806;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#35793;&#22120;&#20248;&#21270;&#33539;&#24335;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#32534;&#35793;&#22120;&#21453;&#39304;&#65292;&#20197;&#20248;&#21270;LLVM&#27719;&#32534;&#20195;&#30721;&#30340;&#22823;&#23567;&#12290;&#35813;&#27169;&#22411;&#20197;&#26410;&#20248;&#21270;&#30340;LLVM IR&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#20248;&#21270;&#30340;IR&#12289;&#26368;&#20339;&#20248;&#21270;&#20256;&#36882;&#20197;&#21450;&#26410;&#20248;&#21270;&#21644;&#20248;&#21270;IR&#30340;&#25351;&#20196;&#35745;&#25968;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;&#30340;&#20248;&#21270;&#20256;&#36882;&#32534;&#35793;&#36755;&#20837;&#65292;&#24182;&#35780;&#20272;&#39044;&#27979;&#30340;&#25351;&#20196;&#35745;&#25968;&#26159;&#21542;&#27491;&#30830;&#65292;&#29983;&#25104;&#30340;IR&#26159;&#21542;&#21487;&#32534;&#35793;&#65292;&#24182;&#19988;&#19982;&#32534;&#35793;&#21518;&#30340;&#20195;&#30721;&#30456;&#23545;&#24212;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#21453;&#39304;&#21457;&#36865;&#22238;LLM&#65292;&#35753;&#20854;&#20877;&#27425;&#20248;&#21270;&#20195;&#30721;&#12290;&#35813;&#26041;&#27861;&#27604;&#21407;&#27169;&#22411;&#30340;-Oz&#39069;&#22806;&#25552;&#21319;&#20102;0.53%&#12290;&#23613;&#31649;&#28155;&#21152;&#26356;&#22810;&#21453;&#39304;&#20449;&#24687;&#20284;&#20046;&#30452;&#35266;&#65292;&#20294;&#31616;&#21333;&#30340;&#25277;&#26679;&#25216;&#26415;&#22312;&#37319;&#26679;10&#27425;&#25110;&#26356;&#22810;&#27425;&#26102;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14714v1 Announce Type: cross  Abstract: We introduce a novel paradigm in compiler optimization powered by Large Language Models with compiler feedback to optimize the code size of LLVM assembly. The model takes unoptimized LLVM IR as input and produces optimized IR, the best optimization passes, and instruction counts of both unoptimized and optimized IRs. Then we compile the input with generated optimization passes and evaluate if the predicted instruction count is correct, generated IR is compilable, and corresponds to compiled code. We provide this feedback back to LLM and give it another chance to optimize code. This approach adds an extra 0.53% improvement over -Oz to the original model. Even though, adding more information with feedback seems intuitive, simple sampling techniques achieve much higher performance given 10 or more samples.
&lt;/p&gt;</description></item><item><title>&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25918;&#23485;&#25110;&#29978;&#33267;&#22312;&#25490;&#38500;&#25152;&#26377;&#30456;&#20851;&#39118;&#38505;&#22240;&#32032;&#34987;&#35266;&#27979;&#21040;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#32473;&#20986;&#23545;&#39640;&#39118;&#38505;&#20010;&#20307;&#20998;&#37197;&#29575;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.14713</link><description>&lt;p&gt;
&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#22240;&#32032;&#19979;&#23457;&#35745;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Auditing Fairness under Unobserved Confounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14713
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25918;&#23485;&#25110;&#29978;&#33267;&#22312;&#25490;&#38500;&#25152;&#26377;&#30456;&#20851;&#39118;&#38505;&#22240;&#32032;&#34987;&#35266;&#27979;&#21040;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#32473;&#20986;&#23545;&#39640;&#39118;&#38505;&#20010;&#20307;&#20998;&#37197;&#29575;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#36328;&#36234;&#20154;&#21475;&#32479;&#35745;&#32447;&#23384;&#22312;&#19981;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#19981;&#20844;&#24179;&#24615;&#21487;&#33021;&#38590;&#20197;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;&#22914;&#26524;&#25105;&#20204;&#23545;&#20844;&#24179;&#24615;&#30340;&#29702;&#35299;&#20381;&#36182;&#20110;&#38590;&#20197;&#34913;&#37327;&#30340;&#39118;&#38505;&#31561;&#35266;&#24565;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#37027;&#20123;&#27809;&#26377;&#20854;&#27835;&#30103;&#23601;&#20250;&#27515;&#20129;&#30340;&#20154;&#24179;&#31561;&#33719;&#24471;&#27835;&#30103;&#65289;&#12290;&#23457;&#35745;&#36825;&#31181;&#19981;&#20844;&#24179;&#24615;&#38656;&#35201;&#20934;&#30830;&#27979;&#37327;&#20010;&#20307;&#39118;&#38505;&#65292;&#32780;&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#65292;&#38590;&#20197;&#20272;&#35745;&#12290;&#22312;&#36825;&#20123;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#32032;&#8220;&#35299;&#37322;&#8221;&#26126;&#26174;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#33021;&#20302;&#20272;&#25110;&#39640;&#20272;&#19981;&#20844;&#24179;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25918;&#23485;&#25110;&#65288;&#20196;&#20154;&#24778;&#35766;&#22320;&#65289;&#29978;&#33267;&#22312;&#25490;&#38500;&#25152;&#26377;&#30456;&#20851;&#39118;&#38505;&#22240;&#32032;&#34987;&#35266;&#27979;&#21040;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#23545;&#39640;&#39118;&#38505;&#20010;&#20307;&#30340;&#20998;&#37197;&#29575;&#32473;&#20986;&#20449;&#24687;&#20016;&#23500;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#22312;&#35768;&#22810;&#23454;&#38469;&#29615;&#22659;&#20013;&#65288;&#20363;&#22914;&#24341;&#20837;&#26032;&#22411;&#27835;&#30103;&#65289;&#25105;&#20204;&#25317;&#26377;&#22312;&#20219;&#20309;&#20998;&#37197;&#20043;&#21069;&#30340;&#25968;&#25454;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14713v1 Announce Type: cross  Abstract: A fundamental problem in decision-making systems is the presence of inequity across demographic lines. However, inequity can be difficult to quantify, particularly if our notion of equity relies on hard-to-measure notions like risk (e.g., equal access to treatment for those who would die without it). Auditing such inequity requires accurate measurements of individual risk, which is difficult to estimate in the realistic setting of unobserved confounding. In the case that these unobservables "explain" an apparent disparity, we may understate or overstate inequity. In this paper, we show that one can still give informative bounds on allocation rates among high-risk individuals, even while relaxing or (surprisingly) even when eliminating the assumption that all relevant risk factors are observed. We utilize the fact that in many real-world settings (e.g., the introduction of a novel treatment) we have data from a period prior to any alloc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#22312;&#29615;AI&#20316;&#24330;&#29615;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#35774;&#35745;&#21407;&#21017;&#12289;&#35780;&#20272;&#26041;&#27861;&#21450;&#31526;&#21512;&#36127;&#36131;&#20219;AI&#26631;&#20934;&#65292;&#23454;&#29616;&#20102;&#26816;&#27979;&#20316;&#24330;&#32773;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.14711</link><description>&lt;p&gt;
&#20154;&#22312;&#29615;AI&#29992;&#20110;&#20316;&#24330;&#29615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-Loop AI for Cheating Ring Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#22312;&#29615;AI&#20316;&#24330;&#29615;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#35774;&#35745;&#21407;&#21017;&#12289;&#35780;&#20272;&#26041;&#27861;&#21450;&#31526;&#21512;&#36127;&#36131;&#20219;AI&#26631;&#20934;&#65292;&#23454;&#29616;&#20102;&#26816;&#27979;&#20316;&#24330;&#32773;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#22312;&#32447;&#32771;&#35797;&#30340;&#26131;&#33719;&#21462;&#24615;&#65292;&#22312;&#32447;&#32771;&#35797;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#22312;&#32447;&#32771;&#35797;&#30340;&#23433;&#20840;&#24615;&#25552;&#20986;&#20102;&#19968;&#20123;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#19987;&#19994;&#20316;&#24330;&#26381;&#21153;&#24110;&#21161;&#24694;&#24847;&#32771;&#29983;&#36890;&#36807;&#32771;&#35797;&#30340;&#32972;&#26223;&#19979;&#65292;&#24418;&#25104;&#20102;&#25152;&#35859;&#30340;&#8220;&#20316;&#24330;&#29615;&#8221;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#22312;&#29615;AI&#20316;&#24330;&#29615;&#26816;&#27979;&#31995;&#32479;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#38459;&#27490;&#36825;&#20123;&#20316;&#24330;&#29615;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#36825;&#31181;&#20154;&#22312;&#29615;AI&#31995;&#32479;&#30340;&#22522;&#26412;&#36923;&#36753;&#65292;&#25506;&#35752;&#20102;&#20854;&#35774;&#35745;&#21407;&#21017;&#65292;&#26088;&#22312;&#23454;&#29616;&#26816;&#27979;&#20316;&#24330;&#32773;&#30340;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#29992;&#20110;&#35780;&#20272;&#20854;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#36731;&#19982;AI&#31995;&#32479;&#30456;&#20851;&#30340;&#24847;&#22806;&#39118;&#38505;&#12290;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#36981;&#24490;&#20102;&#36127;&#36131;&#20219;&#30340;AI&#65288;RAI&#65289;&#26631;&#20934;&#65292;&#30830;&#20445;&#22312;&#25972;&#20010;&#24320;&#21457;&#36807;&#31243;&#20013;&#25972;&#21512;&#20102;&#20262;&#29702;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14711v1 Announce Type: cross  Abstract: Online exams have become popular in recent years due to their accessibility. However, some concerns have been raised about the security of the online exams, particularly in the context of professional cheating services aiding malicious test takers in passing exams, forming so-called "cheating rings". In this paper, we introduce a human-in-the-loop AI cheating ring detection system designed to detect and deter these cheating rings. We outline the underlying logic of this human-in-the-loop AI system, exploring its design principles tailored to achieve its objectives of detecting cheaters. Moreover, we illustrate the methodologies used to evaluate its performance and fairness, aiming to mitigate the unintended risks associated with the AI system. The design and development of the system adhere to Responsible AI (RAI) standards, ensuring that ethical considerations are integrated throughout the entire development process.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;ClimateQ&amp;A&#24179;&#21488;&#19978;&#25552;&#20986;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20844;&#20247;&#23545;&#27668;&#20505;&#21464;&#21270;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#25439;&#22833;&#30340;&#30475;&#27861;&#65292;&#20026;&#20351;&#33258;&#28982;&#31185;&#23398;&#26356;&#26131;&#25509;&#36817;&#21644;&#25910;&#38598;&#20998;&#26512;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.14709</link><description>&lt;p&gt;
ClimateQ&amp;A&#65306;&#24357;&#21512;&#27668;&#20505;&#31185;&#23398;&#23478;&#19982;&#26222;&#36890;&#22823;&#20247;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
ClimateQ&amp;A: Bridging the gap between climate scientists and the general public
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14709
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;ClimateQ&amp;A&#24179;&#21488;&#19978;&#25552;&#20986;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20844;&#20247;&#23545;&#27668;&#20505;&#21464;&#21270;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#25439;&#22833;&#30340;&#30475;&#27861;&#65292;&#20026;&#20351;&#33258;&#28982;&#31185;&#23398;&#26356;&#26131;&#25509;&#36817;&#21644;&#25910;&#38598;&#20998;&#26512;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#21521;ClimateQ&amp;A&#24179;&#21488;&#25552;&#20986;&#30340;&#38382;&#39064;&#65292;&#35843;&#26597;&#20102;&#20844;&#20247;&#23545;&#27668;&#20505;&#21464;&#21270;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#20007;&#22833;&#30340;&#30475;&#27861;&#12290;ClimateQ&amp;A&#26159;&#19968;&#20010;&#20250;&#35805;&#20195;&#29702;&#65292;&#21033;&#29992;LLMs&#26469;&#22238;&#24212;&#22522;&#20110;&#26469;&#33258;IPCC&#21644;IPBES&#25253;&#21578;&#30340;&#36229;&#36807;14,000&#39029;&#31185;&#23398;&#25991;&#29486;&#30340;&#26597;&#35810;&#12290;&#35813;&#24037;&#20855;&#20110;2023&#24180;3&#26376;&#19978;&#32447;&#65292;&#24050;&#25628;&#38598;&#20102;&#26469;&#33258;&#27861;&#22269;&#21463;&#20247;&#20027;&#35201;&#25552;&#20986;&#30340;&#36229;&#36807;30,000&#20010;&#38382;&#39064;&#12290;&#20854;&#32842;&#22825;&#26426;&#22120;&#20154;&#30028;&#38754;&#20801;&#35768;&#33258;&#30001;&#25552;&#20986;&#19982;&#33258;&#28982;&#30456;&#20851;&#30340;&#38382;&#39064;*. &#23613;&#31649;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#20351;&#33258;&#28982;&#31185;&#23398;&#26356;&#26131;&#25509;&#36817;&#65292;&#20294;&#23427;&#20063;&#20801;&#35768;&#25910;&#38598;&#21644;&#20998;&#26512;&#38382;&#39064;&#21450;&#20854;&#20027;&#39064;&#12290;&#19982;&#28041;&#21450;&#23553;&#38381;&#38382;&#39064;&#30340;&#20256;&#32479;&#35843;&#26597;&#19981;&#21516;&#65292;&#36825;&#31181;&#26032;&#39062;&#26041;&#27861;&#20026;&#20010;&#20307;&#20851;&#20110;&#33258;&#28982;&#30340;&#36136;&#35810;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#22312;&#23545;3,425&#20010;&#38382;&#39064;&#30340;&#26679;&#26412;&#36816;&#34892;NLP&#32858;&#31867;&#31639;&#27861;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#26377;&#26174;&#33879;&#30340;25.8%&#30340;&#38382;&#39064;&#35810;&#38382;&#27668;&#20505;&#21464;&#21270;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#20007;&#22833;&#23558;&#22914;&#20309;&#24433;&#21709;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14709v1 Announce Type: cross  Abstract: This research paper investigates public views on climate change and biodiversity loss by analyzing questions asked to the ClimateQ&amp;A platform. ClimateQ&amp;A is a conversational agent that uses LLMs to respond to queries based on over 14,000 pages of scientific literature from the IPCC and IPBES reports. Launched online in March 2023, the tool has gathered over 30,000 questions, mainly from a French audience. Its chatbot interface allows for the free formulation of questions related to nature*. While its main goal is to make nature science more accessible, it also allows for the collection and analysis of questions and their themes. Unlike traditional surveys involving closed questions, this novel method offers a fresh perspective on individual interrogations about nature. Running NLP clustering algorithms on a sample of 3,425 questions, we find that a significant 25.8% inquire about how climate change and biodiversity loss will affect the
&lt;/p&gt;</description></item><item><title>&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#39046;&#22495;&#65292;&#27604;&#36739;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#12289;&#36229;&#24102;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.14695</link><description>&lt;p&gt;
&#38024;&#23545;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#38142;&#24335;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Chain-structured neural architecture search for financial time series forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14695
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#39046;&#22495;&#65292;&#27604;&#36739;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#12289;&#36229;&#24102;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38142;&#24335;&#25628;&#32034;&#31354;&#38388;&#19978;&#27604;&#36739;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31574;&#30053;&#65306;&#36125;&#21494;&#26031;&#20248;&#21270;&#12289;&#36229;&#24102;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#36825;&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14695v1 Announce Type: cross  Abstract: We compare three popular neural architecture search strategies on chain-structured search spaces: Bayesian optimization, the hyperband method, and reinforcement learning in the context of financial time series forecasting.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#21160;&#19988;&#39640;&#25928;&#22320;&#28155;&#21152;&#20960;&#20309;&#38382;&#39064;&#20013;&#30340;&#36741;&#21161;&#32452;&#20214;</title><link>https://arxiv.org/abs/2403.14690</link><description>&lt;p&gt;
&#23558;&#22270;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20960;&#20309;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Incorporating Graph Attention Mechanism into Geometric Problem Solving Based on Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14690
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#21160;&#19988;&#39640;&#25928;&#22320;&#28155;&#21152;&#20960;&#20309;&#38382;&#39064;&#20013;&#30340;&#36741;&#21161;&#32452;&#20214;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#25945;&#32946;&#32972;&#26223;&#19979;&#65292;&#35774;&#35745;&#19968;&#20010;&#33258;&#21160;&#27714;&#35299;&#20960;&#20309;&#38382;&#39064;&#30340;&#27714;&#35299;&#22120;&#34987;&#35748;&#20026;&#26159;&#36808;&#21521;&#36890;&#29992;&#25968;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#19968;&#27493;&#65292;&#20854;&#20381;&#25176;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#20256;&#32479;&#36923;&#36753;&#25512;&#29702;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#38382;&#39064;&#30340;&#35299;&#20915;&#26159;&#36890;&#36807;&#28155;&#21152;&#36741;&#21161;&#32452;&#20214;&#22914;&#32447;&#26465;&#25110;&#28857;&#26469;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#38656;&#35201;&#20570;&#20986;&#20851;&#38190;&#20915;&#31574;&#26102;&#36873;&#25321;&#21512;&#36866;&#30340;&#36741;&#21161;&#32452;&#20214;&#30340;&#22797;&#26434;&#24615;&#65292;&#33258;&#21160;&#28155;&#21152;&#36741;&#21161;&#32452;&#20214;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#30340;&#26368;&#26032;&#24615;&#33021;&#26159;&#36890;&#36807;&#20174;&#31867;&#21035;&#24211;&#20013;&#31351;&#20030;&#25152;&#26377;&#21487;&#33021;&#30340;&#31574;&#30053;&#65292;&#20197;&#35782;&#21035;&#20855;&#26377;&#26368;&#22823;&#21487;&#33021;&#24615;&#30340;&#31574;&#30053;&#26469;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#22312;&#25928;&#29575;&#26041;&#38754;&#20570;&#20986;&#22949;&#21327;&#65292;&#24517;&#39035;&#37319;&#29992;&#24191;&#27867;&#30340;&#31574;&#30053;&#25628;&#32034;&#12290;&#20026;&#20102;&#33258;&#21160;&#19988;&#39640;&#25928;&#22320;&#28155;&#21152;&#36741;&#21161;&#32452;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14690v1 Announce Type: cross  Abstract: In the context of online education, designing an automatic solver for geometric problems has been considered a crucial step towards general math Artificial Intelligence (AI), empowered by natural language understanding and traditional logical inference. In most instances, problems are addressed by adding auxiliary components such as lines or points. However, adding auxiliary components automatically is challenging due to the complexity in selecting suitable auxiliary components especially when pivotal decisions have to be made. The state-of-the-art performance has been achieved by exhausting all possible strategies from the category library to identify the one with the maximum likelihood. However, an extensive strategy search have to be applied to trade accuracy for ef-ficiency. To add auxiliary components automatically and efficiently, we present deep reinforcement learning framework based on the language model, such as BERT. We first
&lt;/p&gt;</description></item><item><title>&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#38656;&#35201;&#21046;&#23450;&#21644;&#23454;&#26045;&#20135;&#19994;&#26631;&#20934;&#65292;&#20197;&#35299;&#20915;&#20114;&#25805;&#20316;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#36947;&#24503;&#27835;&#29702;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14689</link><description>&lt;p&gt;
&#21457;&#23637;&#21644;&#37096;&#32626;&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#20135;&#19994;&#26631;&#20934;&#65306;&#25361;&#25112;&#12289;&#31574;&#30053;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Developing and Deploying Industry Standards for Artificial Intelligence in Education (AIED): Challenges, Strategies, and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14689
&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#38656;&#35201;&#21046;&#23450;&#21644;&#23454;&#26045;&#20135;&#19994;&#26631;&#20934;&#65292;&#20197;&#35299;&#20915;&#20114;&#25805;&#20316;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#36947;&#24503;&#27835;&#29702;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#25215;&#35834;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#12289;&#33258;&#21160;&#21270;&#34892;&#25919;&#21644;&#25945;&#23398;&#20219;&#21153;&#20197;&#21450;&#38477;&#20302;&#20869;&#23481;&#21019;&#24314;&#25104;&#26412;&#26469;&#38761;&#26032;&#25945;&#32946;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#22312;&#24320;&#21457;&#21644;&#37096;&#32626;&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#32570;&#20047;&#26631;&#20934;&#21270;&#23454;&#36341;&#23548;&#33268;&#29983;&#24577;&#31995;&#32479;&#20998;&#25955;&#65292;&#32473;&#20114;&#25805;&#20316;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#36947;&#24503;&#27835;&#29702;&#24102;&#26469;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22312;&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#21644;&#23454;&#26045;&#20135;&#19994;&#26631;&#20934;&#30340;&#32039;&#36843;&#38656;&#27714;&#65292;&#25552;&#20379;&#23545;&#24403;&#21069;&#23616;&#21183;&#12289;&#25361;&#25112;&#21644;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#30340;&#31574;&#30053;&#26041;&#27861;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#24320;&#22987;&#36890;&#36807;&#30740;&#31350;AIED&#22312;&#19981;&#21516;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#24182;&#30830;&#23450;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#21253;&#25324;&#31995;&#32479;&#20114;&#25805;&#20316;&#24615;&#12289;&#26412;&#20307;&#26144;&#23556;&#12289;&#25968;&#25454;&#38598;&#25104;&#12289;&#35780;&#20272;&#21644;&#36947;&#24503;&#27835;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14689v1 Announce Type: cross  Abstract: The adoption of Artificial Intelligence in Education (AIED) holds the promise of revolutionizing educational practices by offering personalized learning experiences, automating administrative and pedagogical tasks, and reducing the cost of content creation. However, the lack of standardized practices in the development and deployment of AIED solutions has led to fragmented ecosystems, which presents challenges in interoperability, scalability, and ethical governance. This article aims to address the critical need to develop and implement industry standards in AIED, offering a comprehensive analysis of the current landscape, challenges, and strategic approaches to overcome these obstacles. We begin by examining the various applications of AIED in various educational settings and identify key areas lacking in standardization, including system interoperability, ontology mapping, data integration, evaluation, and ethical governance. Then, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25972;&#21512;&#26680;&#20989;&#25968;&#21644;&#26680;&#23545;&#40784;&#23454;&#29616;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#26680;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.14688</link><description>&lt;p&gt;
&#36890;&#36807;&#30697;&#38453;&#20998;&#35299;&#23454;&#29616;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#30340;&#26680;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Kernel Alignment for Unsupervised Feature Selection via Matrix Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14688
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25972;&#21512;&#26680;&#20989;&#25968;&#21644;&#26680;&#23545;&#40784;&#23454;&#29616;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#26680;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28040;&#38500;&#26080;&#20851;&#21644;&#20887;&#20313;&#29305;&#24449;&#65292;&#29305;&#24449;&#36873;&#25321;&#26088;&#22312;&#25214;&#21040;&#21407;&#22987;&#29305;&#24449;&#30340;&#33391;&#22909;&#34920;&#31034;&#12290;&#38543;&#30528;&#26080;&#26631;&#35760;&#25968;&#25454;&#30340;&#26222;&#21450;&#65292;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#24050;&#34987;&#35777;&#26126;&#22312;&#32531;&#35299;&#25152;&#35859;&#30340;&#32500;&#24230;&#28798;&#38590;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#24314;&#31435;&#22312;&#23376;&#31354;&#38388;&#23398;&#20064;&#20043;&#19978;&#65292;&#20294;&#22312;&#25429;&#33719;&#29305;&#24449;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#32467;&#26500;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#26680;&#25216;&#26415;&#21487;&#20197;&#25429;&#33719;&#38750;&#32447;&#24615;&#32467;&#26500;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#38598;&#25104;&#26680;&#20989;&#25968;&#21644;&#26680;&#23545;&#40784;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#36825;&#31561;&#25928;&#22320;&#21487;&#20197;&#34987;&#34920;&#24449;&#20026;&#19968;&#20010;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25193;&#23637;&#24341;&#21457;&#20102;&#21478;&#19968;&#20010;&#38382;&#39064;&#65306;&#31639;&#27861;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#26680;&#30340;&#36873;&#25321;&#65292;&#32780;&#36825;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26680;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14688v1 Announce Type: new  Abstract: By removing irrelevant and redundant features, feature selection aims to find a good representation of the original features. With the prevalence of unlabeled data, unsupervised feature selection has been proven effective in alleviating the so-called curse of dimensionality. Most existing matrix factorization-based unsupervised feature selection methods are built upon subspace learning, but they have limitations in capturing nonlinear structural information among features. It is well-known that kernel techniques can capture nonlinear structural information. In this paper, we construct a model by integrating kernel functions and kernel alignment, which can be equivalently characterized as a matrix factorization problem. However, such an extension raises another issue: the algorithm performance heavily depends on the choice of kernel, which is often unknown a priori. Therefore, we further propose a multiple kernel-based learning method. By
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19971;&#31181;&#22635;&#34917;&#25216;&#26415;&#22312;&#20581;&#24247;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;...</title><link>https://arxiv.org/abs/2403.14687</link><description>&lt;p&gt;
&#22312;&#20581;&#24247;&#25968;&#25454;&#38598;&#19978;&#32570;&#22833;&#20540;&#22635;&#34917;&#25216;&#26415;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
On the Performance of Imputation Techniques for Missing Values on Healthcare Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19971;&#31181;&#22635;&#34917;&#25216;&#26415;&#22312;&#20581;&#24247;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#22833;&#20540;&#26159;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#19968;&#31181;&#24120;&#35265;&#29305;&#24449;&#65292;&#23588;&#20854;&#26159;&#22312;&#20581;&#24247;&#25968;&#25454;&#20013;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#19971;&#31181;&#22635;&#34917;&#25216;&#26415;&#65288;&#22343;&#20540;&#22635;&#34917;&#12289;&#20013;&#20301;&#25968;&#22635;&#34917;&#12289;&#26368;&#36817;&#35266;&#23519;&#20540;&#22635;&#34917;&#12289;K-&#26368;&#36817;&#37051;&#22635;&#34917;&#12289;&#25554;&#20540;&#22635;&#34917;&#12289;Missforest&#22635;&#34917;&#21644;&#38142;&#24335;&#26041;&#31243;&#22810;&#37325;&#22635;&#34917;&#65289;&#22312;&#19977;&#20010;&#20581;&#24247;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#23558;&#25968;&#25454;&#38598;&#24341;&#20837;&#20102;&#19981;&#21516;&#30334;&#20998;&#27604;&#30340;&#32570;&#22833;&#20540;&#65288;10\%&#12289;15\%&#12289;20\%&#21644;25\%&#65289;&#65292;&#24182;&#20351;&#29992;&#22635;&#34917;&#25216;&#26415;&#23545;&#36825;&#20123;&#32570;&#22833;&#20540;&#36827;&#34892;&#22635;&#34917;&#12290;&#36890;&#36807;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;Mi
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14687v1 Announce Type: cross  Abstract: Missing values or data is one popular characteristic of real-world datasets, especially healthcare data. This could be frustrating when using machine learning algorithms on such datasets, simply because most machine learning models perform poorly in the presence of missing values. The aim of this study is to compare the performance of seven imputation techniques, namely Mean imputation, Median Imputation, Last Observation carried Forward (LOCF) imputation, K-Nearest Neighbor (KNN) imputation, Interpolation imputation, Missforest imputation, and Multiple imputation by Chained Equations (MICE), on three healthcare datasets. Some percentage of missing values - 10\%, 15\%, 20\% and 25\% - were introduced into the dataset, and the imputation techniques were employed to impute these missing values. The comparison of their performance was evaluated by using root mean squared error (RMSE) and mean absolute error (MAE). The results show that Mi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25968;&#26041;&#27861;&#20316;&#20026;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#65292;&#36890;&#36807;&#26356;&#31215;&#26497;&#30340;&#37325;&#21551;&#27169;&#24335;&#65292;&#21487;&#33021;&#20351;&#24471;&#22312;&#22312;&#32447;&#20984;&#20248;&#21270;&#26694;&#26550;&#19978;&#20351;&#29992;&#26356;&#36138;&#23146;&#30340;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#24615;&#33021;&#31867;&#20284;&#20110;&#20313;&#24358;&#36864;&#28779;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.14685</link><description>&lt;p&gt;
&#21608;&#26399;&#24615;&#23545;&#25968;&#28201;&#24230;&#35843;&#24230;&#20316;&#20026;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cyclical Log Annealing as a Learning Rate Scheduler
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14685
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25968;&#26041;&#27861;&#20316;&#20026;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#65292;&#36890;&#36807;&#26356;&#31215;&#26497;&#30340;&#37325;&#21551;&#27169;&#24335;&#65292;&#21487;&#33021;&#20351;&#24471;&#22312;&#22312;&#32447;&#20984;&#20248;&#21270;&#26694;&#26550;&#19978;&#20351;&#29992;&#26356;&#36138;&#23146;&#30340;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#24615;&#33021;&#31867;&#20284;&#20110;&#20313;&#24358;&#36864;&#28779;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#26159;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#25351;&#20196;&#65292;&#29992;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#25913;&#21464;&#25628;&#32034;&#27493;&#38271;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#23545;&#27493;&#38271;&#36827;&#34892;&#20005;&#26684;&#30340;&#37325;&#21551;&#12290;&#21608;&#26399;&#24615;&#23545;&#25968;&#28201;&#24230;&#35843;&#24230;&#26356;&#31215;&#26497;&#22320;&#23454;&#29616;&#20102;&#37325;&#21551;&#27169;&#24335;&#65292;&#25110;&#35768;&#21487;&#20197;&#20801;&#35768;&#22312;&#22312;&#32447;&#20984;&#20248;&#21270;&#26694;&#26550;&#19978;&#20351;&#29992;&#26356;&#36138;&#23146;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20284;&#20046;&#22312;&#22823;&#22411;&#21464;&#21387;&#22120;&#22686;&#24378;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#20313;&#24358;&#36864;&#28779;&#26041;&#26696;&#34920;&#29616;&#31867;&#20284;&#12290;&#26410;&#26469;&#30340;&#23454;&#39564;&#23558;&#28041;&#21450;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#27979;&#35797;&#35843;&#24230;&#22120;&#65292;&#24182;&#36890;&#36807;&#26356;&#22810;&#23454;&#39564;&#25214;&#21040;&#35843;&#24230;&#22120;&#30340;&#26368;&#20339;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14685v1 Announce Type: new  Abstract: A learning rate scheduler is a predefined set of instructions for varying search stepsizes during model training processes. This paper introduces a new logarithmic method using harsh restarting of step sizes through stochastic gradient descent. Cyclical log annealing implements the restart pattern more aggressively to maybe allow the usage of more greedy algorithms on the online convex optimization framework. The algorithm was tested on the CIFAR-10 image datasets, and seemed to perform analogously with cosine annealing on large transformer-enhanced residual neural networks. Future experiments would involve testing the scheduler in generative adversarial networks and finding the best parameters for the scheduler with more experiments.
&lt;/p&gt;</description></item><item><title>FOCIL&#36890;&#36807;&#35757;&#32451;&#38543;&#26426;&#20462;&#21098;&#31232;&#30095;&#23376;&#32593;&#32476;&#23454;&#29616;&#22312;&#32447;&#25345;&#32493;&#31867;&#36882;&#22686;&#23398;&#20064;&#65292;&#22312;&#36991;&#20813;&#23384;&#20648;&#37325;&#25918;&#25968;&#25454;&#30340;&#21516;&#26102;&#26377;&#25928;&#38450;&#27490;&#36951;&#24536;&#12290;</title><link>https://arxiv.org/abs/2403.14684</link><description>&lt;p&gt;
FOCIL: &#36890;&#36807;&#35757;&#32451;&#38543;&#26426;&#20462;&#21098;&#31232;&#30095;&#19987;&#23478;&#36827;&#34892;&#22312;&#32447;&#31867;&#36882;&#22686;&#23398;&#20064;&#30340;&#24494;&#35843;&#21644;&#20923;&#32467;
&lt;/p&gt;
&lt;p&gt;
FOCIL: Finetune-and-Freeze for Online Class Incremental Learning by Training Randomly Pruned Sparse Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14684
&lt;/p&gt;
&lt;p&gt;
FOCIL&#36890;&#36807;&#35757;&#32451;&#38543;&#26426;&#20462;&#21098;&#31232;&#30095;&#23376;&#32593;&#32476;&#23454;&#29616;&#22312;&#32447;&#25345;&#32493;&#31867;&#36882;&#22686;&#23398;&#20064;&#65292;&#22312;&#36991;&#20813;&#23384;&#20648;&#37325;&#25918;&#25968;&#25454;&#30340;&#21516;&#26102;&#26377;&#25928;&#38450;&#27490;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#31867;&#36882;&#22686;&#23398;&#20064;&#65288;CIL&#65289;&#26088;&#22312;&#20174;&#25968;&#25454;&#27969;&#20013;&#33719;&#21462;&#19968;&#31995;&#21015;&#26032;&#31867;&#30340;&#30693;&#35782;&#65292;&#20165;&#20351;&#29992;&#27599;&#20010;&#25968;&#25454;&#28857;&#36827;&#34892;&#19968;&#27425;&#35757;&#32451;&#12290;&#19982;&#31163;&#32447;&#27169;&#24335;&#30456;&#27604;&#65292;&#36825;&#26356;&#21152;&#29616;&#23454;&#65292;&#31163;&#32447;&#27169;&#24335;&#20551;&#23450;&#25152;&#26377;&#26032;&#31867;&#30340;&#25968;&#25454;&#24050;&#32463;&#20934;&#22791;&#22909;&#12290;&#24403;&#21069;&#30340;&#22312;&#32447;CIL&#26041;&#27861;&#23384;&#20648;&#20808;&#21069;&#25968;&#25454;&#30340;&#23376;&#38598;&#65292;&#36825;&#20250;&#22312;&#20869;&#23384;&#21644;&#35745;&#31639;&#26041;&#38754;&#36896;&#25104;&#27785;&#37325;&#30340;&#24320;&#38144;&#65292;&#36824;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FOCIL&#30340;&#26032;&#22411;&#22312;&#32447;CIL&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#35757;&#32451;&#38543;&#26426;&#20462;&#21098;&#31232;&#30095;&#23376;&#32593;&#32476;&#19981;&#26029;&#24494;&#35843;&#20027;&#20307;&#31995;&#32467;&#26500;&#65292;&#28982;&#21518;&#20923;&#32467;&#35757;&#32451;&#36830;&#25509;&#20197;&#38450;&#27490;&#36951;&#24536;&#12290;FOCIL&#36824;&#33258;&#36866;&#24212;&#30830;&#23450;&#27599;&#20010;&#20219;&#21153;&#30340;&#31232;&#30095;&#24230;&#32423;&#21035;&#21644;&#23398;&#20064;&#36895;&#29575;&#65292;&#24182;&#30830;&#20445;&#65288;&#20960;&#20046;&#65289;&#38646;&#36951;&#24536;&#36328;&#25152;&#26377;&#20219;&#21153;&#65292;&#19988;&#19981;&#23384;&#20648;&#20219;&#20309;&#37325;&#25918;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14684v1 Announce Type: cross  Abstract: Class incremental learning (CIL) in an online continual learning setting strives to acquire knowledge on a series of novel classes from a data stream, using each data point only once for training. This is more realistic compared to offline modes, where it is assumed that all data from novel class(es) is readily available. Current online CIL approaches store a subset of the previous data which creates heavy overhead costs in terms of both memory and computation, as well as privacy issues. In this paper, we propose a new online CIL approach called FOCIL. It fine-tunes the main architecture continually by training a randomly pruned sparse subnetwork for each task. Then, it freezes the trained connections to prevent forgetting. FOCIL also determines the sparsity level and learning rate per task adaptively and ensures (almost) zero forgetting across all tasks without storing any replay data. Experimental results on 10-Task CIFAR100, 20-Task
&lt;/p&gt;</description></item><item><title>&#23454;&#29616;&#32456;&#36523;&#36229;&#23545;&#40784;&#38656;&#35201;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#37325;&#22823;&#21464;&#38761;&#65292;&#20197;&#35299;&#20915;&#20854;&#22312;&#29702;&#35299;&#21644;&#36866;&#24212;&#21160;&#24577;&#20154;&#31867;&#36947;&#24503;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#20840;&#29699;&#24773;&#26223;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14683</link><description>&lt;p&gt;
&#19968;&#39033;&#36947;&#20041;&#20351;&#21629;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25345;&#32493;&#36229;&#23545;&#40784;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
A Moral Imperative: The Need for Continual Superalignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14683
&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#32456;&#36523;&#36229;&#23545;&#40784;&#38656;&#35201;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#37325;&#22823;&#21464;&#38761;&#65292;&#20197;&#35299;&#20915;&#20854;&#22312;&#29702;&#35299;&#21644;&#36866;&#24212;&#21160;&#24577;&#20154;&#31867;&#36947;&#24503;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#20840;&#29699;&#24773;&#26223;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#23454;&#29616;&#32456;&#36523;&#36229;&#23545;&#40784;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#12290;&#36229;&#23545;&#40784;&#26159;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#26088;&#22312;&#30830;&#20445;&#36229;&#26234;&#33021;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31526;&#21512;&#20154;&#31867;&#30340;&#20215;&#20540;&#35266;&#21644;&#30446;&#26631;&#12290;&#23613;&#31649;&#20854;&#23637;&#26395;&#20196;&#20154;&#25391;&#22859;&#65292;&#25105;&#20204;&#35748;&#20026;&#23454;&#29616;&#36229;&#23545;&#40784;&#38656;&#35201;&#23545;&#24403;&#21069;LLM&#26550;&#26500;&#36827;&#34892;&#37325;&#22823;&#21464;&#38761;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#29702;&#35299;&#21644;&#36866;&#24212;&#20154;&#31867;&#36947;&#24503;&#30340;&#21160;&#24577;&#24615;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#20840;&#29699;&#24773;&#26223;&#26041;&#38754;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21078;&#26512;&#20102;&#23558;&#19981;&#26029;&#21464;&#21270;&#30340;&#20154;&#31867;&#20215;&#20540;&#35266;&#35889;&#31995;&#32534;&#30721;&#21040;LLMs&#20013;&#30340;&#25361;&#25112;&#65292;&#31361;&#20986;&#20102;&#38745;&#24577;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#19982;&#20154;&#31867;&#31038;&#20250;&#21160;&#24577;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#31034;&#20363;&#65306;&#19968;&#20010;&#23637;&#31034;&#20102;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23450;&#24615;&#36716;&#21464;&#65292;&#21478;&#19968;&#20010;&#21576;&#29616;&#20102;&#21487;&#37327;&#21270;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#36825;&#20123;&#31034;&#20363;&#65292;&#25105;&#20204;&#35828;&#26126;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14683v1 Announce Type: cross  Abstract: This paper examines the challenges associated with achieving life-long superalignment in AI systems, particularly large language models (LLMs). Superalignment is a theoretical framework that aspires to ensure that superintelligent AI systems act in accordance with human values and goals. Despite its promising vision, we argue that achieving superalignment requires substantial changes in the current LLM architectures due to their inherent limitations in comprehending and adapting to the dynamic nature of these human ethics and evolving global scenarios. We dissect the challenges of encoding an ever-changing spectrum of human values into LLMs, highlighting the discrepancies between static AI models and the dynamic nature of human societies. To illustrate these challenges, we analyze two distinct examples: one demonstrates a qualitative shift in human values, while the other presents a quantifiable change. Through these examples, we illus
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;CVAE-USM&#26041;&#27861;&#65292;&#36890;&#36807;&#25918;&#26494;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#21644;&#21033;&#29992;&#26102;&#38388;&#20851;&#31995;&#65292;&#26377;&#25928;&#22320;&#22312;&#19981;&#21516;&#29992;&#25143;&#20043;&#38388;&#23545;&#40784;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#25913;&#36827;&#27963;&#21160;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.14682</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#20851;&#31995;&#30693;&#35782;&#30340;&#28145;&#24230;&#29983;&#25104;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#29992;&#20110;&#36328;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Domain Adaptation with Temporal Relation Knowledge for Cross-User Activity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14682
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;CVAE-USM&#26041;&#27861;&#65292;&#36890;&#36807;&#25918;&#26494;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#21644;&#21033;&#29992;&#26102;&#38388;&#20851;&#31995;&#65292;&#26377;&#25928;&#22320;&#22312;&#19981;&#21516;&#29992;&#25143;&#20043;&#38388;&#23545;&#40784;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#25913;&#36827;&#27963;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#20013;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#30340;&#20551;&#35774;&#36890;&#24120;&#22833;&#36133;&#65292;&#29305;&#21035;&#26159;&#22312;&#36328;&#29992;&#25143;&#22330;&#26223;&#20013;&#65292;&#20854;&#20013;&#25968;&#25454;&#20998;&#24067;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#19982;&#36890;&#29992;&#24207;&#21015;&#26144;&#23556;&#65288;CVAE-USM&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25918;&#26494; i.i.d. &#20551;&#35774;&#24182;&#21033;&#29992;&#26102;&#38388;&#20851;&#31995;&#26469;&#26377;&#25928;&#22320;&#23545;&#40784;&#19981;&#21516;&#29992;&#25143;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;HAR&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14682v1 Announce Type: cross  Abstract: In human activity recognition (HAR), the assumption that training and testing data are independent and identically distributed (i.i.d.) often fails, particularly in cross-user scenarios where data distributions vary significantly. This discrepancy highlights the limitations of conventional domain adaptation methods in HAR, which typically overlook the inherent temporal relations in time-series data. To bridge this gap, our study introduces a Conditional Variational Autoencoder with Universal Sequence Mapping (CVAE-USM) approach, which addresses the unique challenges of time-series domain adaptation in HAR by relaxing the i.i.d. assumption and leveraging temporal relations to align data distributions effectively across different users. This method combines the strengths of Variational Autoencoder (VAE) and Universal Sequence Mapping (USM) to capture and utilize common temporal patterns between users for improved activity recognition. Ou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19977;&#38454;&#27573;&#24041;&#22266;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#38480;&#21046;&#26799;&#24230;&#26657;&#27491;&#36991;&#20813;&#36951;&#24536;&#23569;&#25968;&#31867;&#65292;&#24182;&#22312;&#22797;&#26434;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.14679</link><description>&lt;p&gt;
&#19977;&#38454;&#27573;&#24041;&#22266;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning by Three-Phase Consolidation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14679
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19977;&#38454;&#27573;&#24041;&#22266;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#38480;&#21046;&#26799;&#24230;&#26657;&#27491;&#36991;&#20813;&#36951;&#24536;&#23569;&#25968;&#31867;&#65292;&#24182;&#22312;&#22797;&#26434;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TPC (Three-Phase Consolidation)&#34987;&#24341;&#20837;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25511;&#21046;&#36951;&#24536;&#20808;&#21069;&#30693;&#35782;&#30340;&#21516;&#26102;&#25345;&#32493;&#23398;&#20064;&#26032;&#31867;&#21035;&#65288;&#21644;/&#25110;&#24050;&#30693;&#31867;&#21035;&#30340;&#23454;&#20363;&#65289;&#12290;&#27599;&#20010;&#32463;&#39564;&#65288;&#20063;&#31216;&#20026;&#20219;&#21153;&#65289;&#37117;&#36890;&#36807;&#20855;&#26377;&#19981;&#21516;&#35268;&#21017;&#21644;&#23398;&#20064;&#21160;&#24577;&#30340;&#19977;&#20010;&#38454;&#27573;&#26469;&#23398;&#20064;&#65292;&#26088;&#22312;&#28040;&#38500;&#30001;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#32780;&#23548;&#33268;&#30340;&#31867;&#21035;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#38480;&#21046;&#22522;&#20110;&#26799;&#24230;&#30340;&#26657;&#27491;&#65292;&#20197;&#38450;&#27490;&#36951;&#24536;&#23569;&#25968;&#31867;&#12290;&#22797;&#26434;&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#20010;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#29616;&#26377;&#31454;&#20105;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21457;&#24067;&#22312;&#25345;&#32493;&#23398;&#20064;&#30340;Avalanche&#24320;&#25918;&#26694;&#26550;&#19978;&#65292;&#26412;&#25991;&#23637;&#31034;&#30340;&#31639;&#27861;&#21644;&#25152;&#26377;&#32467;&#26524;&#22343;&#21487;&#20197;&#23436;&#20840;&#21487;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14679v1 Announce Type: new  Abstract: TPC (Three-Phase Consolidation) is here introduced as a simple but effective approach to continually learn new classes (and/or instances of known classes) while controlling forgetting of previous knowledge. Each experience (a.k.a. task) is learned in three phases characterized by different rules and learning dynamics, aimed at removing the class-bias problem (due to class unbalancing) and limiting gradient-based corrections to prevent forgetting of underrepresented classes. Several experiments on complex datasets demonstrate its accuracy and efficiency advantages over competitive existing approaches. The algorithm and all the results presented in this paper are fully reproducible thanks to its publication on the Avalanche open framework for continual learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#38754;&#21521;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#35748;&#35777;&#26694;&#26550;&#65292;&#32467;&#21512;&#22266;&#26377;&#23433;&#20840;&#35774;&#35745;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#26816;&#27979;&#65292;&#20197;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14678</link><description>&lt;p&gt;
&#38754;&#21521;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#35748;&#35777;&#26694;&#26550;&#65306;&#37319;&#29992;&#22266;&#26377;&#23433;&#20840;&#35774;&#35745;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards a Framework for Deep Learning Certification in Safety-Critical Applications Using Inherently Safe Design and Run-Time Error Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#38754;&#21521;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#35748;&#35777;&#26694;&#26550;&#65292;&#32467;&#21512;&#22266;&#26377;&#23433;&#20840;&#35774;&#35745;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#26816;&#27979;&#65292;&#20197;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#37319;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31995;&#32479;&#36827;&#34892;&#39044;&#27979;&#12289;&#20915;&#31574;&#25110;&#29366;&#24577;&#20272;&#35745;&#65292;&#20960;&#20046;&#27809;&#26377;&#24314;&#31435;&#35777;&#20070;&#27969;&#31243;&#65292;&#20351;&#24471;&#36825;&#26679;&#30340;&#31995;&#32479;&#33021;&#22815;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#22312;&#33322;&#31354;&#21644;&#20854;&#20182;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#20986;&#29616;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#35748;&#35777;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#31038;&#21306;&#38024;&#23545;&#39564;&#35777;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#23545;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#36866;&#29992;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#65288;i&#65289;&#22266;&#26377;&#23433;&#20840;&#35774;&#35745;&#21644;&#65288;ii&#65289;&#36816;&#34892;&#26102;&#38169;&#35823;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#35748;&#35777;&#26032;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;&#33322;&#31354;&#26696;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#24674;&#22797;&#35299;&#32806;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14678v1 Announce Type: new  Abstract: Although an ever-growing number of applications employ deep learning based systems for prediction, decision-making, or state estimation, almost no certification processes have been established that would allow such systems to be deployed in safety-critical applications. In this work we consider real-world problems arising in aviation and other safety-critical areas, and investigate their requirements for a certified model. To this end, we investigate methodologies from the machine learning research community aimed towards verifying robustness and reliability of deep learning systems, and evaluate these methodologies with regard to their applicability to real-world problems. Then, we establish a new framework towards deep learning certification based on (i) inherently safe design, and (ii) run-time error detection. Using a concrete use case from aviation, we show how deep learning models can recover disentangled variables through the use 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#23545;&#20110;&#20855;&#26377;&#20132;&#20114;&#21151;&#33021;&#21442;&#25968;&#30340;&#22797;&#26434;&#27169;&#22411;&#30340;&#23398;&#26415;&#31354;&#30333;</title><link>https://arxiv.org/abs/2403.14676</link><description>&lt;p&gt;
&#32479;&#19968;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#29992;&#20110;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unified Uncertainty Estimation for Cognitive Diagnosis Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14676
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#23545;&#20110;&#20855;&#26377;&#20132;&#20114;&#21151;&#33021;&#21442;&#25968;&#30340;&#22797;&#26434;&#27169;&#22411;&#30340;&#23398;&#26415;&#31354;&#30333;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#26234;&#33021;&#25945;&#32946;&#65292;&#29992;&#20110;&#27979;&#37327;&#29992;&#25143;&#23545;&#30693;&#35782;&#27010;&#24565;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#22522;&#20110;&#27492;&#65292;&#29992;&#25143;&#21487;&#20197;&#33719;&#24471;&#20010;&#24615;&#21270;&#30340;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#32852;&#31995;&#34180;&#24369;&#65292;&#27979;&#37327;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#65292;&#27979;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20063;&#20026;&#20915;&#31574;&#25552;&#20379;&#20102;&#37325;&#35201;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35748;&#30693;&#35786;&#26029;&#20013;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#30740;&#31350;&#33853;&#21518;&#20110;&#23545;&#35748;&#30693;&#35786;&#26029;&#39640;&#32423;&#27169;&#22411;&#32467;&#26500;&#30340;&#30740;&#31350;&#12290;&#29616;&#26377;&#26041;&#27861;&#25928;&#29575;&#26377;&#38480;&#65292;&#24182;&#20026;&#20855;&#26377;&#20132;&#20114;&#21151;&#33021;&#21442;&#25968;&#65288;&#22914;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65289;&#30340;&#31934;&#23494;&#27169;&#22411;&#30041;&#19979;&#20102;&#19968;&#20010;&#23398;&#26415;&#31354;&#30333;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24191;&#27867;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#30340;&#32479;&#19968;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#20272;&#35745;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#19968;&#31181;u
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14676v1 Announce Type: cross  Abstract: Cognitive diagnosis models have been widely used in different areas, especially intelligent education, to measure users' proficiency levels on knowledge concepts, based on which users can get personalized instructions. As the measurement is not always reliable due to the weak links of the models and data, the uncertainty of measurement also offers important information for decisions. However, the research on the uncertainty estimation lags behind that on advanced model structures for cognitive diagnosis. Existing approaches have limited efficiency and leave an academic blank for sophisticated models which have interaction function parameters (e.g., deep learning-based models). To address these problems, we propose a unified uncertainty estimation approach for a wide range of cognitive diagnosis models. Specifically, based on the idea of estimating the posterior distributions of cognitive diagnosis model parameters, we first provide a u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22478;&#24066;&#35774;&#35745;&#22312;&#20844;&#20849;&#20132;&#36890;&#25928;&#29575;&#21644;&#20943;&#23569;&#30899;&#25490;&#25918;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25351;&#20986;&#22478;&#24066;&#24067;&#23616;&#23545;&#20844;&#20849;&#20132;&#36890;&#32467;&#26524;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#22478;&#24066;&#35774;&#35745;&#20803;&#32032;&#30340;&#23450;&#21046;&#31574;&#30053;&#23545;&#20110;&#27668;&#20505;&#36866;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.14671</link><description>&lt;p&gt;
&#20102;&#35299;&#36807;&#22659;&#32570;&#21475;&#65306;&#21335;&#21345;&#32599;&#26469;&#32435;&#24030;&#22799;&#27931;&#29305;&#24066;&#21335;&#31471;&#21644;&#30000;&#32435;&#35199;&#24030;&#26597;&#22612;&#21162;&#21152;&#24066;&#38463;&#20911;&#20195;&#23572;&#30340;&#21363;&#38656;&#20844;&#20849;&#27773;&#36710;&#26381;&#21153;&#21450;&#22478;&#24066;&#27668;&#20505;&#36866;&#24212;&#33021;&#21147;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Transit Gap: A Comparative Study of On-Demand Bus Services and Urban Climate Resilience in South End, Charlotte, NC and Avondale, Chattanooga, TN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22478;&#24066;&#35774;&#35745;&#22312;&#20844;&#20849;&#20132;&#36890;&#25928;&#29575;&#21644;&#20943;&#23569;&#30899;&#25490;&#25918;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25351;&#20986;&#22478;&#24066;&#24067;&#23616;&#23545;&#20844;&#20849;&#20132;&#36890;&#32467;&#26524;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#22478;&#24066;&#35774;&#35745;&#20803;&#32032;&#30340;&#23450;&#21046;&#31574;&#30053;&#23545;&#20110;&#27668;&#20505;&#36866;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#35774;&#35745;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#20132;&#36890;&#25928;&#29575;&#21644;&#20943;&#23569;&#30899;&#25490;&#25918;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20855;&#26377;&#19981;&#21516;&#22478;&#24066;&#35774;&#35745;&#30340;&#20004;&#20010;&#31038;&#21306;&#65306;&#21271;&#21345;&#32599;&#26469;&#32435;&#24030;&#22799;&#27931;&#29305;&#24066;&#21335;&#31471;&#65292;&#20855;&#26377;&#21160;&#24577;&#28151;&#21512;&#29992;&#36884;&#30340;&#22478;&#24066;&#35774;&#35745;&#27169;&#24335;&#65307;&#21644;&#30000;&#32435;&#35199;&#24030;&#26597;&#22612;&#21162;&#21152;&#24066;&#38463;&#20911;&#20195;&#23572;&#65292;&#37319;&#29992;&#20303;&#23429;&#37066;&#21306;&#32593;&#26684;&#24067;&#23616;&#12290;&#36890;&#36807;&#20351;&#29992;TRANSIT-GYM&#24037;&#20855;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#36825;&#20123;&#19981;&#21516;&#22478;&#24066;&#29615;&#22659;&#20013;&#22686;&#21152;&#20844;&#20849;&#27773;&#36710;&#21033;&#29992;&#29575;&#23545;&#20132;&#36890;&#21644;CO2&#25490;&#25918;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#22478;&#24066;&#35774;&#35745;&#21644;&#35268;&#21010;&#22312;&#20132;&#36890;&#31995;&#32479;&#25928;&#29575;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#22799;&#27931;&#29305;&#24066;&#21335;&#31471;&#65292;&#28151;&#21512;&#29992;&#36884;&#35774;&#35745;&#23548;&#33268;&#26356;&#22810;&#30340;&#25490;&#25918;&#20943;&#23569;&#65292;&#34920;&#26126;&#22478;&#24066;&#24067;&#23616;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#20844;&#20849;&#20132;&#36890;&#32467;&#26524;&#12290;&#32771;&#34385;&#21040;&#29420;&#29305;&#30340;&#22478;&#24066;&#35774;&#35745;&#20803;&#32032;&#30340;&#37327;&#36523;&#23450;&#21046;&#31574;&#30053;&#23545;&#20110;&#27668;&#20505;&#36866;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#21335;&#31471;&#65292;&#20844;&#20849;&#27773;&#36710;&#21033;&#29992;&#29575;&#32763;&#20493;&#20351;&#26085;&#24120;&#25490;&#25918;&#20943;&#23569;&#20102;10.18&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14671v1 Announce Type: cross  Abstract: Urban design significantly impacts sustainability, particularly in the context of public transit efficiency and carbon emissions reduction. This study explores two neighborhoods with distinct urban designs: South End, Charlotte, NC, featuring a dynamic mixed-use urban design pattern, and Avondale, Chattanooga, TN, with a residential suburban grid layout. Using the TRANSIT-GYM tool, we assess the impact of increased bus utilization in these different urban settings on traffic and CO2 emissions. Our results highlight the critical role of urban design and planning in transit system efficiency. In South End, the mixed-use design led to more substantial emission reductions, indicating that urban layout can significantly influence public transit outcomes. Tailored strategies that consider the unique urban design elements are essential for climate resilience. Notably, doubling bus utilization decreased daily emissions by 10.18% in South End a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25506;&#35752;&#20102;&#22312;ITS&#20013;&#39044;&#27979;&#25104;&#20154;&#35782;&#23383;&#35745;&#21010;&#23398;&#20064;&#34920;&#29616;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;GPT-4&#22312;&#27492;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14668</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#23398;&#20064;&#34920;&#29616;&#65306;&#25104;&#20154;&#35782;&#23383;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Predicting Learning Performance with Large Language Models: A Study in Adult Literacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14668
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25506;&#35752;&#20102;&#22312;ITS&#20013;&#39044;&#27979;&#25104;&#20154;&#35782;&#23383;&#35745;&#21010;&#23398;&#20064;&#34920;&#29616;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;GPT-4&#22312;&#27492;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14668v1 &#20844;&#21578;&#31867;&#21035;&#65306;&#36328;&#39046;&#22495; &#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITS&#65289;&#26174;&#33879;&#22686;&#24378;&#20102;&#25104;&#20154;&#35782;&#23383;&#22521;&#35757;&#65292;&#36825;&#26159;&#31038;&#20250;&#21442;&#19982;&#12289;&#23601;&#19994;&#26426;&#20250;&#21644;&#32456;&#36523;&#23398;&#20064;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#39640;&#32423;AI&#27169;&#22411;&#65288;&#21253;&#25324;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#22312;ITS&#20013;&#39044;&#27979;&#25104;&#20154;&#35782;&#23383;&#35745;&#21010;&#23398;&#20064;&#34920;&#29616;&#30340;&#24212;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#21463;&#21040;&#20102;LLMs&#22522;&#20110;&#20854;&#20869;&#22312;&#25512;&#29702;&#21644;&#35745;&#31639;&#33021;&#21147;&#39044;&#27979;&#23398;&#20064;&#34920;&#29616;&#30340;&#28508;&#21147;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#20351;&#29992;ITS AutoTutor&#30340;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36890;&#36807;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#25216;&#26415;&#35780;&#20272;&#20102;GPT-4&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#23398;&#20064;&#34920;&#29616;&#26041;&#38754;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#23637;&#29616;&#20986;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#36125;&#21494;&#26031;&#30693;&#35782;&#36319;&#36394;&#12289;&#34920;&#29616;&#22240;&#32032;&#20998;&#26512;&#12289;&#31232;&#30095;&#22240;&#32032;&#20998;&#26512;&#65289;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14668v1 Announce Type: cross  Abstract: Intelligent Tutoring Systems (ITSs) have significantly enhanced adult literacy training, a key factor for societal participation, employment opportunities, and lifelong learning. Our study investigates the application of advanced AI models, including Large Language Models (LLMs) like GPT-4, for predicting learning performance in adult literacy programs in ITSs. This research is motivated by the potential of LLMs to predict learning performance based on its inherent reasoning and computational capabilities. By using reading comprehension datasets from the ITS, AutoTutor, we evaluate the predictive capabilities of GPT-4 versus traditional machine learning methods in predicting learning performance through five-fold cross-validation techniques. Our findings show that the GPT-4 presents the competitive predictive abilities with traditional machine learning methods such as Bayesian Knowledge Tracing, Performance Factor Analysis, Sparse Fact
&lt;/p&gt;</description></item><item><title>SyllabusQA&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;63&#20010;&#30495;&#23454;&#35838;&#31243;&#22823;&#32434;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#23545;36&#20010;&#19987;&#19994;&#28085;&#30422;5,078&#23545;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;-&#31572;&#26696;&#23545;&#36827;&#34892;&#20102;&#35814;&#32454;&#25910;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#31572;&#26696;&#20107;&#23454;&#24615;&#65292;&#22810;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#23384;&#22312;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.14666</link><description>&lt;p&gt;
SyllabusQA&#65306;&#19968;&#20010;&#35838;&#31243;&#36923;&#36753;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SyllabusQA: A Course Logistics Question Answering Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14666
&lt;/p&gt;
&lt;p&gt;
SyllabusQA&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;63&#20010;&#30495;&#23454;&#35838;&#31243;&#22823;&#32434;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#23545;36&#20010;&#19987;&#19994;&#28085;&#30422;5,078&#23545;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;-&#31572;&#26696;&#23545;&#36827;&#34892;&#20102;&#35814;&#32454;&#25910;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#31572;&#26696;&#20107;&#23454;&#24615;&#65292;&#22810;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#23384;&#22312;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#25945;&#23398;&#21161;&#29702;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#26377;&#26174;&#33879;&#28508;&#21147;&#20943;&#36731;&#20154;&#31867;&#25945;&#24072;&#30340;&#24037;&#20316;&#37327;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19982;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#30340;&#38382;&#39064;&#22238;&#31572;&#65292;&#36825;&#23545;&#23398;&#29983;&#24456;&#37325;&#35201;&#65292;&#20294;&#23545;&#25945;&#24072;&#26469;&#35828;&#26159;&#37325;&#22797;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SyllabusQA&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;63&#20010;&#30495;&#23454;&#35838;&#31243;&#22823;&#32434;&#65292;&#28085;&#30422;36&#20010;&#19987;&#19994;&#65292;&#21253;&#21547;5,078&#23545;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#38382;&#39064;&#31867;&#22411;&#21644;&#31572;&#26696;&#26684;&#24335;&#37117;&#26159;&#22810;&#26679;&#30340;&#12290;&#30001;&#20110;&#35768;&#22810;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;&#21253;&#21547;&#20851;&#38190;&#20449;&#24687;&#65292;&#22914;&#32771;&#35797;&#26085;&#26399;&#65292;&#35780;&#20272;&#31572;&#26696;&#30340;&#20107;&#23454;&#24615;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#35813;&#20219;&#21153;&#19978;&#23545;&#20960;&#20010;&#24378;&#22522;&#32447;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#21040;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#22312;&#20256;&#32479;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#25351;&#26631;&#19978;&#25509;&#36817;&#20154;&#31867;&#34920;&#29616;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14666v1 Announce Type: cross  Abstract: Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce SyllabusQA, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5,078 open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between
&lt;/p&gt;</description></item><item><title>ClickTree&#26159;&#19968;&#31181;&#22522;&#20110;&#26641;&#24418;&#26041;&#27861;&#30340;&#39044;&#27979;&#23398;&#29983;&#25968;&#23398;&#20316;&#19994;&#34920;&#29616;&#30340;&#25216;&#26415;&#65292;&#22312;2023&#24180;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#26479;&#27604;&#36187;&#20013;&#21462;&#24471;&#20102;0.78844&#30340;AUC&#65292;&#25490;&#21517;&#31532;&#20108;&#12290;</title><link>https://arxiv.org/abs/2403.14664</link><description>&lt;p&gt;
ClickTree&#65306;&#22522;&#20110;&#28857;&#20987;&#25968;&#25454;&#30340;&#26641;&#24418;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#25968;&#23398;&#23398;&#29983;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
ClickTree: A Tree-based Method for Predicting Math Students' Performance Based on Clickstream Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14664
&lt;/p&gt;
&lt;p&gt;
ClickTree&#26159;&#19968;&#31181;&#22522;&#20110;&#26641;&#24418;&#26041;&#27861;&#30340;&#39044;&#27979;&#23398;&#29983;&#25968;&#23398;&#20316;&#19994;&#34920;&#29616;&#30340;&#25216;&#26415;&#65292;&#22312;2023&#24180;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#26479;&#27604;&#36187;&#20013;&#21462;&#24471;&#20102;0.78844&#30340;AUC&#65292;&#25490;&#21517;&#31532;&#20108;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#25429;&#33719;&#23398;&#29983;&#34892;&#20026;&#30340;&#22823;&#37327;&#28857;&#20987;&#25968;&#25454;&#65292;&#25945;&#32946;&#24037;&#20316;&#32773;&#21487;&#20197;&#28145;&#20837;&#20102;&#35299;&#24433;&#21709;&#23398;&#26415;&#25104;&#26524;&#30340;&#22240;&#32032;&#65292;&#24182;&#30830;&#23450;&#35838;&#31243;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ClickTree&#65292;&#19968;&#31181;&#22522;&#20110;&#26641;&#24418;&#26041;&#27861;&#30340;&#25216;&#26415;&#65292;&#26681;&#25454;&#23398;&#29983;&#30340;&#28857;&#20987;&#25968;&#25454;&#26469;&#39044;&#27979;&#25968;&#23398;&#20316;&#19994;&#34920;&#29616;&#12290;&#25105;&#20204;&#20174;&#22823;&#37327;&#30340;&#28857;&#20987;&#25968;&#25454;&#20013;&#25552;&#21462;&#20102;&#19968;&#32452;&#29305;&#24449;&#65292;&#21253;&#25324;&#38382;&#39064;&#32423;&#21035;&#12289;&#20219;&#21153;&#32423;&#21035;&#21644;&#23398;&#29983;&#32423;&#21035;&#30340;&#29305;&#24449;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;CatBoost&#26641;&#26469;&#39044;&#27979;&#23398;&#29983;&#26159;&#21542;&#25104;&#21151;&#22238;&#31572;&#20102;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;2023&#24180;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#26479;&#27604;&#36187;&#20013;&#21462;&#24471;&#20102;0.78844&#30340;AUC&#65292;&#33719;&#24471;&#20102;&#31532;&#20108;&#21517;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14664v1 Announce Type: cross  Abstract: The prediction of student performance and the analysis of students' learning behavior play an important role in enhancing online courses. By analysing a massive amount of clickstream data that captures student behavior, educators can gain valuable insights into the factors that influence academic outcomes and identify areas of improvement in courses. In this study, we developed ClickTree, a tree-based methodology, to predict student performance in mathematical assignments based on students' clickstream data. We extracted a set of features, including problem-level, assignment-level and student-level features, from the extensive clickstream data and trained a CatBoost tree to predict whether a student successfully answers a problem in an assignment. The developed method achieved an AUC of 0.78844 in the Educational Data Mining Cup 2023 and ranked second in the competition. Furthermore, our results indicate that students encounter more di
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#38271;&#36798;13&#24180;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;&#24188;&#20799;&#22253;&#21040;9&#24180;&#32423;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#39044;&#27979;&#20102;&#21021;&#20013;&#36749;&#23398;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.14663</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21487;&#39044;&#27979;&#21021;&#20013;&#36749;&#23398;&#65292;&#29978;&#33267;&#22312;&#23567;&#23398;&#32467;&#26463;&#26102;&#23601;&#33021;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Predicts Upper Secondary Education Dropout as Early as the End of Primary School
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14663
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#38271;&#36798;13&#24180;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;&#24188;&#20799;&#22253;&#21040;9&#24180;&#32423;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#39044;&#27979;&#20102;&#21021;&#20013;&#36749;&#23398;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#22312;&#20943;&#36731;&#36139;&#22256;&#12289;&#25512;&#21160;&#32463;&#27982;&#22686;&#38271;&#21644;&#36171;&#20104;&#20010;&#20154;&#26435;&#21147;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#26174;&#30528;&#24433;&#21709;&#31038;&#20250;&#21644;&#20010;&#20154;&#21457;&#23637;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#36749;&#23398;&#20998;&#31867;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#24448;&#24448;&#21463;&#30701;&#26399;&#28966;&#28857;&#30340;&#24433;&#21709;&#65292;&#20381;&#36182;&#20110;&#20165;&#22312;&#30740;&#31350;&#26399;&#38388;&#25910;&#38598;&#20102;&#20960;&#24180;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#20026;&#26399;13&#24180;&#30340;&#32437;&#21521;&#25968;&#25454;&#38598;&#25193;&#23637;&#20102;&#24314;&#27169;&#35270;&#35282;&#65292;&#28085;&#30422;&#20102;&#20174;&#24188;&#20799;&#22253;&#21040;9&#24180;&#32423;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#21442;&#25968;&#65292;&#21253;&#25324;&#23398;&#29983;&#30340;&#23398;&#26415;&#21644;&#35748;&#30693;&#33021;&#21147;&#12289;&#21160;&#26426;&#12289;&#34892;&#20026;&#12289;&#24184;&#31119;&#24863;&#20197;&#21450;&#23448;&#26041;&#35760;&#24405;&#30340;&#36749;&#23398;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#20998;&#31867;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#24179;&#22343;&#38754;&#31215;u
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14663v1 Announce Type: cross  Abstract: Education plays a pivotal role in alleviating poverty, driving economic growth, and empowering individuals, thereby significantly influencing societal and personal development. However, the persistent issue of school dropout poses a significant challenge, with its effects extending beyond the individual. While previous research has employed machine learning for dropout classification, these studies often suffer from a short-term focus, relying on data collected only a few years into the study period. This study expanded the modeling horizon by utilizing a 13-year longitudinal dataset, encompassing data from kindergarten to Grade 9. Our methodology incorporated a comprehensive range of parameters, including students' academic and cognitive skills, motivation, behavior, well-being, and officially recorded dropout data. The machine learning models developed in this study demonstrated notable classification ability, achieving a mean area u
&lt;/p&gt;</description></item><item><title>&#38750;&#27954;&#22269;&#23478;&#22312;AI&#20934;&#22791;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#27809;&#26377;&#23436;&#20840;&#34987;&#20840;&#29699;&#20934;&#22791;&#24773;&#20917;&#35780;&#20272;&#25429;&#25417;&#21040;&#65292;&#36890;&#36807;&#23545;&#22235;&#20010;&#38750;&#27954;&#22269;&#23478;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22914;&#20309;&#25913;&#21892;&#22269;&#23478;&#30340;AI&#20934;&#22791;&#26631;&#20934;&#20197;&#21450;&#22914;&#20309;&#20351;&#31038;&#20250;&#33021;&#22815;&#33719;&#30410;&#20110;AI&#30340;&#39640;&#23618;&#25919;&#31574;&#32771;&#34385;&#12290;</title><link>https://arxiv.org/abs/2403.14662</link><description>&lt;p&gt;
&#38750;&#27954;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#21457;&#23637;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Case Studies of AI Policy Development in Africa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14662
&lt;/p&gt;
&lt;p&gt;
&#38750;&#27954;&#22269;&#23478;&#22312;AI&#20934;&#22791;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#27809;&#26377;&#23436;&#20840;&#34987;&#20840;&#29699;&#20934;&#22791;&#24773;&#20917;&#35780;&#20272;&#25429;&#25417;&#21040;&#65292;&#36890;&#36807;&#23545;&#22235;&#20010;&#38750;&#27954;&#22269;&#23478;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22914;&#20309;&#25913;&#21892;&#22269;&#23478;&#30340;AI&#20934;&#22791;&#26631;&#20934;&#20197;&#21450;&#22914;&#20309;&#20351;&#31038;&#20250;&#33021;&#22815;&#33719;&#30410;&#20110;AI&#30340;&#39640;&#23618;&#25919;&#31574;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14662v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#38656;&#35201;&#26032;&#30340;&#35780;&#20272;&#26041;&#24335;&#65292;&#20197;&#35780;&#20272;&#38750;&#27954;&#22269;&#23478;&#22312;&#22269;&#23478;&#25216;&#26415;&#20351;&#29992;&#21644;&#25112;&#30053;&#26041;&#38754;&#30340;&#20934;&#22791;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#38024;&#23545;&#19968;&#33324;&#25968;&#23383;&#37319;&#32435;&#21644;&#29305;&#23450;AI&#25919;&#31574;&#30340;&#8220;&#20934;&#22791;&#24773;&#20917;&#8221;&#35780;&#20272;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#29616;&#26377;&#30340;&#20840;&#29699;&#20934;&#22791;&#24773;&#20917;&#35780;&#20272;&#24182;&#27809;&#26377;&#23436;&#20840;&#25429;&#25417;&#21040;&#38750;&#27954;&#22269;&#23478;&#22312;AI&#20934;&#22791;&#26041;&#38754;&#30340;&#36827;&#27493;&#65292;&#24182;&#20026;&#22914;&#20309;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#35780;&#20272;&#26469;&#36866;&#24212;&#38750;&#27954;&#32972;&#26223;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#32771;&#34385;&#36825;&#20123;&#25351;&#26631;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#19982;&#38750;&#27954;&#32972;&#26223;&#30456;&#21563;&#21512;&#65292;&#20197;&#21450;&#36825;&#20123;&#25351;&#26631;&#22312;&#25429;&#25417;&#38750;&#27954;&#22269;&#23478;&#22312;&#23454;&#29616;AI&#33021;&#21147;&#26041;&#38754;&#30340;&#23454;&#38469;&#24037;&#20316;&#26041;&#38754;&#25152;&#36951;&#28431;&#30340;&#20869;&#23481;&#12290;&#36890;&#36807;&#23545;&#38750;&#27954;&#22235;&#20010;&#22320;&#29702;&#21644;&#32463;&#27982;&#23610;&#24230;&#19981;&#21516;&#30340;&#22269;&#23478;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#29699;&#35780;&#20272;&#36951;&#28431;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#22914;&#20309;&#25913;&#21892;&#22269;&#23478;&#30340;AI&#20934;&#22791;&#26631;&#20934;&#20197;&#21450;&#22914;&#20309;&#20351;&#31038;&#20250;&#33021;&#22815;&#33719;&#30410;&#20110;AI&#30340;&#39640;&#23618;&#25919;&#31574;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14662v1 Announce Type: cross  Abstract: Artificial Intelligence (AI) requires new ways of evaluating national technology use and strategy for African nations. We conduct a survey of existing 'readiness' assessments both for general digital adoption and for AI policy in particular. We conclude that existing global readiness assessments do not fully capture African states' progress in AI readiness and lay the groundwork for how assessments can be better used for the African context. We consider the extent to which these indicators map to the African context and what these indicators miss in capturing African states' on-the-ground work in meeting AI capability. Through case studies of four African nations of diverse geographic and economic dimensions, we identify nuances missed by global assessments and offer high-level policy considerations for how states can best improve their AI readiness standards and prepare their societies to capture the benefits of AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#36861;&#36394;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#27604;&#36739;&#38646;-shot&#25552;&#31034;&#21644;&#27169;&#22411;&#24494;&#35843;&#20004;&#31181;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LLMs&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#39044;&#27979;&#23398;&#20064;&#32773;&#34920;&#29616;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14661</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#32773;&#34920;&#29616;&#24314;&#27169;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Modeling Learner Performance with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#36861;&#36394;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#27604;&#36739;&#38646;-shot&#25552;&#31034;&#21644;&#27169;&#22411;&#24494;&#35843;&#20004;&#31181;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LLMs&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#39044;&#27979;&#23398;&#20064;&#32773;&#34920;&#29616;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#33021;&#22815;&#20805;&#24403;&#19968;&#33324;&#27169;&#24335;&#26426;&#22120;&#65292;&#36890;&#36807;&#23436;&#25104;&#20195;&#34920;&#21508;&#31181;&#20219;&#21153;&#30340;&#22797;&#26434;&#20196;&#29260;&#24207;&#21015;&#65292;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#26159;&#21542;&#33021;&#22815;&#25193;&#23637;&#21040;&#30693;&#35782;&#36861;&#36394;&#39046;&#22495;&#65292;&#36825;&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITSs&#65289;&#30340;&#21457;&#23637;&#20013;&#26159;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#36807;&#39044;&#27979;&#23398;&#20064;&#32773;&#38543;&#26102;&#38388;&#30340;&#34920;&#29616;&#26469;&#20010;&#24615;&#21270;&#25945;&#32946;&#20307;&#39564;&#12290;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#32463;&#39564;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#27492;&#20219;&#21153;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#38646;-shot&#25552;&#31034;&#21644;&#27169;&#22411;&#24494;&#35843;&#65292;&#20197;&#21450;&#29616;&#26377;&#30340;&#38750;LLM&#26041;&#27861;&#12290;&#34429;&#28982;&#22522;&#20110;LLMs&#30340;&#26041;&#27861;&#27809;&#26377;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#32463;&#36807;&#24494;&#35843;&#30340;LLMs&#36229;&#36807;&#20102;&#22825;&#30495;&#30340;&#22522;&#32447;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14661v1 Announce Type: cross  Abstract: Recent work exploring the capabilities of pre-trained large language models (LLMs) has demonstrated their ability to act as general pattern machines by completing complex token sequences representing a wide array of tasks, including time-series prediction and robot control. This paper investigates whether the pattern recognition and sequence modeling capabilities of LLMs can be extended to the domain of knowledge tracing, a critical component in the development of intelligent tutoring systems (ITSs) that tailor educational experiences by predicting learner performance over time. In an empirical evaluation across multiple real-world datasets, we compare two approaches to using LLMs for this task, zero-shot prompting and model fine-tuning, with existing, non-LLM approaches to knowledge tracing. While LLM-based approaches do not achieve state-of-the-art performance, fine-tuned LLMs surpass the performance of naive baseline models and perf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#35777;&#26126;&#29983;&#25104;&#24335;AI&#33021;&#26174;&#33879;&#25552;&#39640;&#22823;&#23398;&#29983;&#30340;&#23398;&#20064;&#36895;&#24230;&#65292;&#20351;&#29992;AI&#21161;&#25163;Syntea&#22312;&#36828;&#31243;&#23398;&#20064;&#23398;&#29983;&#20013;&#24179;&#22343;&#20943;&#23569;&#20102;27%&#30340;&#23398;&#20064;&#26102;&#38388;&#65292;&#34920;&#26126;&#29983;&#25104;&#24335;AI&#21487;&#20197;&#36890;&#36807;&#20010;&#24615;&#21270;&#26174;&#33879;&#25913;&#36827;&#21644;&#21152;&#24555;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.14642</link><description>&lt;p&gt;
&#38761;&#26032;&#36828;&#31243;&#23398;&#20064;&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#36741;&#23548;&#30340;&#23398;&#20064;&#36827;&#23637;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revolutionising Distance Learning: A Comparative Study of Learning Progress with AI-Driven Tutoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#35777;&#26126;&#29983;&#25104;&#24335;AI&#33021;&#26174;&#33879;&#25552;&#39640;&#22823;&#23398;&#29983;&#30340;&#23398;&#20064;&#36895;&#24230;&#65292;&#20351;&#29992;AI&#21161;&#25163;Syntea&#22312;&#36828;&#31243;&#23398;&#20064;&#23398;&#29983;&#20013;&#24179;&#22343;&#20943;&#23569;&#20102;27%&#30340;&#23398;&#20064;&#26102;&#38388;&#65292;&#34920;&#26126;&#29983;&#25104;&#24335;AI&#21487;&#20197;&#36890;&#36807;&#20010;&#24615;&#21270;&#26174;&#33879;&#25913;&#36827;&#21644;&#21152;&#24555;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Generative AI&#39044;&#35745;&#23558;&#23545;&#25945;&#32946;&#20135;&#29983;&#24040;&#22823;&#31215;&#26497;&#24433;&#21709;; &#20294;&#26159;&#65292;&#30446;&#21069;&#23578;&#26410;&#22312;&#22823;&#23398;&#23618;&#38754;&#23637;&#31034;&#36825;&#31181;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#35777;&#25454;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;AI&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22823;&#23398;&#29983;&#30340;&#23398;&#20064;&#36895;&#24230;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#26159;&#21542;&#20351;&#29992;AI&#21160;&#21147;&#23398;&#36741;&#23548;&#21161;&#25163;Syntea&#24433;&#21709;&#20102;IU&#22269;&#38469;&#24212;&#29992;&#31185;&#23398;&#22823;&#23398;40&#22810;&#20010;&#35838;&#31243;&#20013;&#25968;&#30334;&#21517;&#36828;&#31243;&#23398;&#20064;&#23398;&#29983;&#30340;&#23398;&#20064;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20351;&#29992;Syntea&#22312;Syntea&#21457;&#24067;&#21518;&#31532;&#19977;&#20010;&#26376;&#26174;&#33879;&#20943;&#23569;&#20102;&#20182;&#20204;&#30340;&#23398;&#20064;&#26102;&#38388;--&#24179;&#22343;&#32422;&#20943;&#23569;&#20102;27\%&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35813;&#25928;&#24212;&#30340;&#24133;&#24230;&#21644;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#35777;&#26126;&#20102;&#29983;&#25104;&#24335;AI&#20316;&#20026;&#26174;&#33879;&#25913;&#36827;&#21644;&#21152;&#24555;&#23398;&#20064;&#30340;&#20851;&#38190;&#26464;&#26438;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14642v1 Announce Type: cross  Abstract: Generative AI is expected to have a vast, positive impact on education; however, at present, this potential has not yet been demonstrated at scale at university level. In this study, we present first evidence that generative AI can increase the speed of learning substantially in university students. We tested whether using the AI-powered teaching assistant Syntea affected the speed of learning of hundreds of distance learning students across more than 40 courses at the IU International University of Applied Sciences. Our analysis suggests that using Syntea reduced their study time substantially--by about 27\% on average--in the third month after the release of Syntea. Taken together, the magnitude of the effect and the scalability of the approach implicate generative AI as a key lever to significantly improve and accelerate learning by personalisation.
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#32593;&#32476;&#23433;&#20840;&#23457;&#35745;&#12289;&#20915;&#31574;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#35780;&#20272;&#39044;&#27979;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#21644;&#36947;&#24503;&#34892;&#20026;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;</title><link>https://arxiv.org/abs/2403.14641</link><description>&lt;p&gt;
&#27979;&#35797;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#20154;&#24037;&#26234;&#33021;&#65306;&#20174;&#32593;&#32476;&#23433;&#20840;&#12289;&#36879;&#26126;&#24230;&#12289;&#31283;&#20581;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#35270;&#35282;&#25506;&#35752;&#25361;&#25112;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Testing autonomous vehicles and AI: perspectives and challenges from cybersecurity, transparency, robustness and fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14641
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#32593;&#32476;&#23433;&#20840;&#23457;&#35745;&#12289;&#20915;&#31574;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#35780;&#20272;&#39044;&#27979;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#21644;&#36947;&#24503;&#34892;&#20026;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25972;&#21512;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#20013;&#25152;&#28041;&#21450;&#30340;&#22797;&#26434;&#24615;&#65292;&#30740;&#31350;&#20102;AI&#32452;&#20214;&#24341;&#20837;&#30340;&#25361;&#25112;&#20197;&#21450;&#23545;&#27979;&#35797;&#31243;&#24207;&#30340;&#24433;&#21709;&#65292;&#30528;&#37325;&#20851;&#27880;&#21487;&#20449;&#36182;AI&#30340;&#19968;&#20123;&#22522;&#26412;&#35201;&#27714;&#12290;&#35752;&#35770;&#30340;&#20027;&#39064;&#21253;&#25324;AI&#22312;AVs&#30340;&#21508;&#20010;&#25805;&#20316;&#23618;&#20013;&#30340;&#20316;&#29992;&#12289;&#27431;&#30431;AI&#27861;&#26696;&#23545;AVs&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23545;&#39640;&#32423;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#65288;ADAS&#65289;&#21644;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65288;ADS&#65289;&#30340;&#26032;&#27979;&#35797;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#30740;&#31350;&#36824;&#23601;&#32593;&#32476;&#23433;&#20840;&#23457;&#35745;&#30340;&#37325;&#35201;&#24615;&#12289;AI&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#38656;&#27714;&#20197;&#21450;&#35780;&#20272;AVs&#20013;&#39044;&#27979;&#31995;&#32479;&#31283;&#20581;&#24615;&#21644;&#36947;&#24503;&#34892;&#20026;&#30340;&#21327;&#35758;&#25552;&#20379;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#35813;&#35770;&#25991;&#25351;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;AI&#22312;AV&#25216;&#26415;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#26410;&#26469;&#26041;&#21521;&#30340;&#24314;&#35758;&#65292;&#24378;&#35843;&#20102;&#36328;&#23398;&#31185;&#19987;&#19994;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14641v1 Announce Type: cross  Abstract: This study explores the complexities of integrating Artificial Intelligence (AI) into Autonomous Vehicles (AVs), examining the challenges introduced by AI components and the impact on testing procedures, focusing on some of the essential requirements for trustworthy AI. Topics addressed include the role of AI at various operational layers of AVs, the implications of the EU's AI Act on AVs, and the need for new testing methodologies for Advanced Driver Assistance Systems (ADAS) and Automated Driving Systems (ADS). The study also provides a detailed analysis on the importance of cybersecurity audits, the need for explainability in AI decision-making processes and protocols for assessing the robustness and ethical behaviour of predictive systems in AVs. The paper identifies significant challenges and suggests future directions for research and development of AI in AV technology, highlighting the need for multidisciplinary expertise.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#20041;&#25991;&#26412;&#20998;&#26512;&#65292;&#26412;&#35770;&#25991;&#23581;&#35797;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#26234;&#24935;&#22478;&#24066;&#23450;&#20041;&#8220;&#22949;&#21327;&#8221;&#29256;&#26412;&#65292;&#24182;&#25552;&#20986;&#35821;&#20041;&#30456;&#20284;&#24230;&#24230;&#37327;&#20316;&#20026;&#35780;&#20272;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.14639</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#26469;&#23450;&#20041;&#26234;&#24935;&#22478;&#24066;
&lt;/p&gt;
&lt;p&gt;
On Defining Smart Cities using Transformer Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14639
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#20041;&#25991;&#26412;&#20998;&#26512;&#65292;&#26412;&#35770;&#25991;&#23581;&#35797;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#26234;&#24935;&#22478;&#24066;&#23450;&#20041;&#8220;&#22949;&#21327;&#8221;&#29256;&#26412;&#65292;&#24182;&#25552;&#20986;&#35821;&#20041;&#30456;&#20284;&#24230;&#24230;&#37327;&#20316;&#20026;&#35780;&#20272;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#21508;&#22320;&#30340;&#22478;&#24066;&#27491;&#22312;&#36805;&#36895;&#37319;&#29992;&#26234;&#33021;&#25216;&#26415;&#65292;&#25913;&#21464;&#22478;&#24066;&#29983;&#27963;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#19968;&#36235;&#21183;&#65292;&#20294;&#26377;&#20851;&#8220;&#26234;&#24935;&#22478;&#24066;&#8221;&#30340;&#26222;&#36941;&#25509;&#21463;&#30340;&#23450;&#20041;&#20173;&#28982;&#38590;&#20197;&#30028;&#23450;&#12290;&#26412;&#25991;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#24212;&#19982;&#20808;&#21069;&#21442;&#19982;&#23450;&#20041;&#36825;&#19968;&#27010;&#24565;&#30340;&#22823;&#22810;&#25968;&#19987;&#23478; resonating &#30340;&#26032;&#8220;&#22949;&#21327;&#8221;&#23450;&#20041;&#65292;&#24182;&#26088;&#22312;&#39564;&#35777;&#29616;&#26377;&#23450;&#20041;&#20043;&#19968;&#12290;&#25105;&#20204;&#20174;&#34892;&#19994;&#12289;&#23398;&#26415;&#30028;&#21644;&#21508;&#31181;&#30456;&#20851;&#32452;&#32455;&#20013;&#23457;&#26597;&#20102;60&#20010;&#26234;&#24935;&#22478;&#24066;&#30340;&#23450;&#20041;&#65292;&#37319;&#29992;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;AI&#21644;&#35821;&#20041;&#25991;&#26412;&#20998;&#26512;&#20197;&#36798;&#25104;&#36825;&#19968;&#22949;&#21327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#30456;&#20284;&#24230;&#24230;&#37327;&#20316;&#20026;&#35780;&#20272;&#25216;&#26415;&#65292;&#36890;&#24120;&#21487;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#26234;&#24935;&#22478;&#24066;&#23450;&#20041;&#65292;&#35780;&#20272;&#20854;&#29420;&#29305;&#24615;&#25110;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;AI&#26469;&#20998;&#26512;&#21508;&#31181;&#29616;&#26377;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14639v1 Announce Type: cross  Abstract: Cities worldwide are rapidly adopting smart technologies, transforming urban life. Despite this trend, a universally accepted definition of 'smart city' remains elusive. Past efforts to define it have not yielded a consensus, as evidenced by the numerous definitions in use. In this paper, we endeavored to create a new 'compromise' definition that should resonate with most experts previously involved in defining this concept and aimed to validate one of the existing definitions. We reviewed 60 definitions of smart cities from industry, academia, and various relevant organizations, employing transformer architecture-based generative AI and semantic text analysis to reach this compromise. We proposed a semantic similarity measure as an evaluation technique, which could generally be used to compare different smart city definitions, assessing their uniqueness or resemblance. Our methodology employed generative AI to analyze various existing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#23398;&#20064;&#39118;&#26684;&#30340;&#32534;&#31243;&#32451;&#20064;&#25512;&#33616;&#22120;&#65288;PERS&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#32534;&#31243;&#20013;&#22914;&#20309;&#35782;&#21035;&#22797;&#26434;&#32534;&#31243;&#34892;&#20026;&#21644;&#25429;&#25417;&#20869;&#22312;&#23398;&#20064;&#27169;&#24335;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14638</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#32534;&#31243;&#23398;&#20064;&#39118;&#26684;&#25429;&#25417;&#30340;&#20010;&#24615;&#21270;&#32534;&#31243;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Personalized Programming Guidance based on Deep Programming Learning Style Capturing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#23398;&#20064;&#39118;&#26684;&#30340;&#32534;&#31243;&#32451;&#20064;&#25512;&#33616;&#22120;&#65288;PERS&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#32534;&#31243;&#20013;&#22914;&#20309;&#35782;&#21035;&#22797;&#26434;&#32534;&#31243;&#34892;&#20026;&#21644;&#25429;&#25417;&#20869;&#22312;&#23398;&#20064;&#27169;&#24335;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#32534;&#31243;&#38656;&#27714;&#37327;&#22823;&#22686;&#65292;&#24050;&#25104;&#20026;&#23398;&#29983;&#30340;&#24517;&#22791;&#25216;&#33021;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#20063;&#33268;&#21147;&#20110;&#25552;&#39640;&#22312;&#32447;&#35780;&#21028;&#31995;&#32479;&#30340;&#25351;&#23548;&#33021;&#21147;&#65292;&#20197;&#20943;&#23569;&#23398;&#29983;&#30340;&#36749;&#23398;&#29575;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#22686;&#24378;&#23398;&#20064;&#32773;&#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#30340;&#21442;&#19982;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#32534;&#31243;&#26041;&#38754;&#20173;&#38656;&#35299;&#20915;&#20004;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;C1) &#22914;&#20309;&#35782;&#21035;&#22797;&#26434;&#30340;&#32534;&#31243;&#34892;&#20026;&#65307;C2) &#22914;&#20309;&#25429;&#25417;&#19982;&#23454;&#38469;&#23398;&#20064;&#36807;&#31243;&#30456;&#31526;&#30340;&#20869;&#22312;&#23398;&#20064;&#27169;&#24335;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#23398;&#20064;&#39118;&#26684;&#30340;&#32534;&#31243;&#32451;&#20064;&#25512;&#33616;&#22120;&#65288;PERS&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#27169;&#25311;&#20102;&#23398;&#20064;&#32773;&#22797;&#26434;&#30340;&#32534;&#31243;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30001;&#20110;&#32534;&#31243;&#26159;&#19968;&#20010;&#36845;&#20195;&#21644;&#35797;&#38169;&#30340;&#36807;&#31243;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#20301;&#32622;&#32534;&#30721;&#21644;&#21306;&#20998;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14638v1 Announce Type: cross  Abstract: With the rapid development of big data and AI technology, programming is in high demand and has become an essential skill for students. Meanwhile, researchers also focus on boosting the online judging system's guidance ability to reduce students' dropout rates. Previous studies mainly targeted at enhancing learner engagement on online platforms by providing personalized recommendations. However, two significant challenges still need to be addressed in programming: C1) how to recognize complex programming behaviors; C2) how to capture intrinsic learning patterns that align with the actual learning process. To fill these gaps, in this paper, we propose a novel model called Programming Exercise Recommender with Learning Style (PERS), which simulates learners' intricate programming behaviors. Specifically, since programming is an iterative and trial-and-error process, we first introduce a positional encoding and a differentiating module to
&lt;/p&gt;</description></item><item><title>Videoshop&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#32534;&#36753;&#31639;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26412;&#22320;&#21270;&#35821;&#20041;&#32534;&#36753;&#65292;&#20174;&#32780;&#20801;&#35768;&#29992;&#25143;&#23545;&#35270;&#39057;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14617</link><description>&lt;p&gt;
Videoshop&#65306;&#20855;&#26377;&#22122;&#22768;&#22806;&#25512;&#25193;&#25955;&#21453;&#28436;&#30340;&#26412;&#22320;&#21270;&#35821;&#20041;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14617
&lt;/p&gt;
&lt;p&gt;
Videoshop&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#32534;&#36753;&#31639;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26412;&#22320;&#21270;&#35821;&#20041;&#32534;&#36753;&#65292;&#20174;&#32780;&#20801;&#35768;&#29992;&#25143;&#23545;&#35270;&#39057;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Videoshop&#65292;&#36825;&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#29992;&#20110;&#26412;&#22320;&#21270;&#35821;&#20041;&#32534;&#36753;&#30340;&#35270;&#39057;&#32534;&#36753;&#31639;&#27861;&#12290;Videoshop&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#20219;&#20309;&#32534;&#36753;&#36719;&#20214;&#65292;&#21253;&#25324;Photoshop&#21644;&#29983;&#25104;&#22635;&#20805;&#65292;&#20462;&#25913;&#31532;&#19968;&#24103;&#65307;&#23427;&#20250;&#33258;&#21160;&#23558;&#36825;&#20123;&#26356;&#25913;&#20256;&#25773;&#21040;&#20854;&#20313;&#24103;&#65292;&#20445;&#25345;&#35821;&#20041;&#12289;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#36816;&#21160;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#36890;&#36807;&#19981;&#31934;&#30830;&#30340;&#25991;&#26412;&#25351;&#20196;&#36827;&#34892;&#32534;&#36753;&#19981;&#21516;&#65292;Videoshop&#20801;&#35768;&#29992;&#25143;&#28155;&#21152;&#25110;&#21024;&#38500;&#23545;&#35937;&#65292;&#35821;&#20041;&#19978;&#26356;&#25913;&#23545;&#35937;&#65292;&#23558;&#32032;&#26448;&#29031;&#29255;&#25554;&#20837;&#35270;&#39057;&#31561;&#65292;&#24182;&#23545;&#20301;&#32622;&#21644;&#22806;&#35266;&#36827;&#34892;&#32454;&#31890;&#24230;&#25511;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#28508;&#22312;&#20540;&#36827;&#34892;&#22122;&#22768;&#22806;&#25512;&#21453;&#28436;&#30340;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#35270;&#39057;&#32534;&#36753;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20174;&#20013;&#25105;&#20204;&#29983;&#25104;&#26681;&#25454;&#32534;&#36753;&#22270;&#20687;&#35843;&#25972;&#30340;&#35270;&#39057;&#12290;Videoshop&#22312;2&#20010;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#20013;&#20351;&#29992;10&#20010;&#35780;&#20272;&#25351;&#26631;&#23545;6&#20010;&#22522;&#32447;&#21462;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14617v1 Announce Type: cross  Abstract: We introduce Videoshop, a training-free video editing algorithm for localized semantic edits. Videoshop allows users to use any editing software, including Photoshop and generative inpainting, to modify the first frame; it automatically propagates those changes, with semantic, spatial, and temporally consistent motion, to the remaining frames. Unlike existing methods that enable edits only through imprecise textual instructions, Videoshop allows users to add or remove objects, semantically change objects, insert stock photos into videos, etc. with fine-grained control over locations and appearance. We achieve this through image-based video editing by inverting latents with noise extrapolation, from which we generate videos conditioned on the edited image. Videoshop produces higher quality edits against 6 baselines on 2 editing benchmarks using 10 evaluation metrics.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#36827;&#21270;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#36741;&#21161;&#31574;&#30053;&#20197;&#22686;&#24378;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14413</link><description>&lt;p&gt;
&#36827;&#21270;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65306;&#19968;&#39033;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Model Uncertainty in Evolutionary Optimization and Bayesian Optimization: A Comparative Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14413
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#36827;&#21270;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#36741;&#21161;&#31574;&#30053;&#20197;&#22686;&#24378;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#20248;&#21270;&#38382;&#39064;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#38656;&#35201;&#36890;&#36807;&#36755;&#20837;&#36755;&#20986;&#20132;&#20114;&#26469;&#36827;&#34892;&#20248;&#21270;&#65292;&#32780;&#27809;&#26377;&#35775;&#38382;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#21644;&#36741;&#21161;&#20195;&#29702;&#36827;&#21270;&#31639;&#27861;&#65288;SAEA&#65289;&#26159;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26080;&#26799;&#24230;&#20248;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#38416;&#26126;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#21033;&#29992;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#65292;&#20197;&#21450;&#27169;&#22411;&#19981;&#20934;&#30830;&#24615;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36741;&#21161;&#31574;&#30053;&#65292;&#21033;&#29992;&#26410;&#35780;&#20272;&#30340;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#21518;&#20195;&#65292;&#21033;&#29992;&#36827;&#21270;&#31639;&#27861;&#30340;&#22522;&#20110;&#31181;&#32676;&#30340;&#25628;&#32034;&#33021;&#21147;&#26469;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14413v1 Announce Type: cross  Abstract: Black-box optimization problems, which are common in many real-world applications, require optimization through input-output interactions without access to internal workings. This often leads to significant computational resources being consumed for simulations. Bayesian Optimization (BO) and Surrogate-Assisted Evolutionary Algorithm (SAEA) are two widely used gradient-free optimization techniques employed to address such challenges. Both approaches follow a similar iterative procedure that relies on surrogate models to guide the search process. This paper aims to elucidate the similarities and differences in the utilization of model uncertainty between these two methods, as well as the impact of model inaccuracies on algorithmic performance. A novel model-assisted strategy is introduced, which utilizes unevaluated solutions to generate offspring, leveraging the population-based search capabilities of evolutionary algorithm to enhance 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#21457;&#23637;&#19968;&#20010;&#22312;&#35780;&#20272;&#23433;&#20840;&#20851;&#38190;&#33258;&#20027;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#30340;&#20851;&#38190;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.13869</link><description>&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26234;&#33021;&#31995;&#32479;&#30340;&#23433;&#20840;&#20851;&#38190;&#31232;&#26377;&#20107;&#20214;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Accurately Predicting Probabilities of Safety-Critical Rare Events for Intelligent Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#21457;&#23637;&#19968;&#20010;&#22312;&#35780;&#20272;&#23433;&#20840;&#20851;&#38190;&#33258;&#20027;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#30340;&#20851;&#38190;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#31995;&#32479;&#36234;&#26469;&#36234;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#28982;&#32780;&#32597;&#35265;&#30340;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#23545;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#26500;&#25104;&#20102;&#37325;&#22823;&#28508;&#22312;&#23041;&#32961;&#12290;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#20851;&#38190;&#22312;&#20110;&#20934;&#30830;&#39044;&#27979;&#22312;&#32473;&#23450;&#26102;&#38388;&#27493;&#38271;&#20869;&#20174;&#24403;&#21069;&#29366;&#24577;&#21457;&#29983;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#30340;&#27010;&#29575;&#65292;&#19968;&#20010;&#25105;&#20204;&#23450;&#20041;&#20026;&#8220;&#37325;&#35201;&#24615;&#8221;&#30340;&#25351;&#26631;&#12290;&#39044;&#27979;&#37325;&#35201;&#24615;&#30340;&#22797;&#26434;&#24615;&#28304;&#33258;&#20110;&#26497;&#31471;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#36825;&#26159;&#30001;&#39640;&#32500;&#21464;&#37327;&#20013;&#19982;&#32597;&#35265;&#20107;&#20214;&#30456;&#20851;&#32852;&#24341;&#36215;&#30340;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32597;&#35265;&#24615;&#35781;&#21650;&#12290;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#35201;&#20040;&#36807;&#20110;&#20445;&#23432;&#65292;&#35201;&#20040;&#23481;&#26131;&#24573;&#35270;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#65292;&#22240;&#27492;&#24456;&#38590;&#21516;&#26102;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#37325;&#35201;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#35780;&#20272;&#23433;&#20840;&#20851;&#38190;&#33258;&#20027;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13869v1 Announce Type: cross  Abstract: Intelligent systems are increasingly integral to our daily lives, yet rare safety-critical events present significant latent threats to their practical deployment. Addressing this challenge hinges on accurately predicting the probability of safety-critical events occurring within a given time step from the current state, a metric we define as 'criticality'. The complexity of predicting criticality arises from the extreme data imbalance caused by rare events in high dimensional variables associated with the rare events, a challenge we refer to as the curse of rarity. Existing methods tend to be either overly conservative or prone to overlooking safety-critical events, thus struggling to achieve both high precision and recall rates, which severely limits their applicability. This study endeavors to develop a criticality prediction model that excels in both precision and recall rates for evaluating the criticality of safety-critical auton
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13784</link><description>&lt;p&gt;
&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;: &#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#21487;&#37325;&#29616;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#29992;&#24615;&#30340;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13784
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#20854;&#21830;&#19994;&#21270;&#24341;&#21457;&#20102;&#20851;&#20110;&#36879;&#26126;&#24230;&#12289;&#21487;&#37325;&#29616;&#24615;&#12289;&#20559;&#35265;&#21644;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#12290;&#35768;&#22810;"&#24320;&#28304;"&#30340;GAI&#27169;&#22411;&#32570;&#20047;&#23436;&#25972;&#29702;&#35299;&#21644;&#20877;&#29616;&#25152;&#24517;&#38656;&#30340;&#32452;&#20214;&#65292;&#19968;&#20123;&#37319;&#29992;&#38480;&#21046;&#24615;&#35768;&#21487;&#35777;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;"&#24320;&#28304;&#27927;&#30333;"&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#25968;&#25454;&#21644;&#24320;&#25918;&#33719;&#21462;&#30340;&#21407;&#21017;&#12290;MOF&#35201;&#27714;&#27169;&#22411;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#29305;&#23450;&#32452;&#20214;&#34987;&#21253;&#21547;&#24182;&#26681;&#25454;&#36866;&#24403;&#30340;&#24320;&#25918;&#35768;&#21487;&#35777;&#21457;&#24067;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#38450;&#27490;&#23459;&#31216;&#33258;&#24049;&#26159;&#24320;&#25918;&#30340;&#27169;&#22411;&#34987;&#35823;&#35299;&#65292;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#20197;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#21457;&#24067;&#25152;&#26377;&#27169;&#22411;&#32452;&#20214;&#65292;&#24182;&#24110;&#21161;&#20844;&#21496;&#12289;&#23398;&#26415;&#30028;&#21644;&#29233;&#22909;&#32773;&#35782;&#21035;&#21487;&#20197;&#23433;&#20840;&#37319;&#29992;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13784v1 Announce Type: new  Abstract: Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety. Many "open-source" GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as "openwashing." We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses. This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adop
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;AltGraph&#36825;&#19968;&#22522;&#20110;&#25628;&#32034;&#30340;&#30005;&#36335;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#22270;&#27169;&#22411;&#29983;&#25104;&#31561;&#25928;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#20197;&#22312;&#20445;&#25345;&#31561;&#25928;&#24615;&#30340;&#21516;&#26102;&#20248;&#21270;&#30005;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.12979</link><description>&lt;p&gt;
AltGraph&#65306;&#21033;&#29992;&#29983;&#25104;&#22270;&#27169;&#22411;&#37325;&#26032;&#35774;&#35745;&#37327;&#23376;&#30005;&#36335;&#20197;&#36827;&#34892;&#39640;&#25928;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
AltGraph: Redesigning Quantum Circuits Using Generative Graph Models for Efficient Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;AltGraph&#36825;&#19968;&#22522;&#20110;&#25628;&#32034;&#30340;&#30005;&#36335;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#22270;&#27169;&#22411;&#29983;&#25104;&#31561;&#25928;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#20197;&#22312;&#20445;&#25345;&#31561;&#25928;&#24615;&#30340;&#21516;&#26102;&#20248;&#21270;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#30005;&#36335;&#36716;&#25442;&#26088;&#22312;&#29983;&#25104;&#31561;&#25928;&#30005;&#36335;&#30340;&#21516;&#26102;&#20248;&#21270;&#21508;&#26041;&#38754;&#65292;&#22914;&#30005;&#36335;&#28145;&#24230;&#12289;&#38376;&#25968;&#37327;&#20197;&#21450;&#19982;&#29616;&#20195;&#22024;&#26434;&#20013;&#38388;&#23610;&#24230;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#30340;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AltGraph&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#30005;&#36335;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#29983;&#25104;&#31561;&#25928;&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#20010;&#20027;&#35201;&#30340;&#22270;&#27169;&#22411;&#65306;DAG&#65288;&#26377;&#21521;&#26080;&#29615;&#22270;&#65289;&#12289;AND-OR&#22270;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#22270;&#65292;&#20197;&#22312;&#20445;&#25345;&#31561;&#25928;&#24615;&#30340;&#21516;&#26102;&#23545;&#30005;&#36335;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12979v1 Announce Type: cross  Abstract: Quantum circuit transformation aims to produce equivalent circuits while optimizing for various aspects such as circuit depth, gate count, and compatibility with modern Noisy Intermediate Scale Quantum (NISQ) devices. There are two techniques for circuit transformation. The first is a rule-based approach that greedily cancels out pairs of gates that equate to the identity unitary operation. Rule-based approaches are used in quantum compilers such as Qiskit, tket, and Quilc. The second is a search-based approach that tries to find an equivalent quantum circuit by exploring the quantum circuits search space. Search-based approaches typically rely on machine learning techniques such as generative models and Reinforcement Learning (RL). In this work, we propose AltGraph, a novel search-based circuit transformation approach that generates equivalent quantum circuits using existing generative graph models. We use three main graph models: DAG
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#22522;&#20110;&#25968;&#37327;&#30340;&#22522;&#30784;&#35774;&#26045;&#19981;&#24179;&#31561;&#29305;&#24449;&#21270;&#30740;&#31350;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#22478;&#24066;&#19981;&#24179;&#31561;&#21644;&#29615;&#22659;&#27491;&#20041;&#32771;&#34385;&#20043;&#38388;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.12074</link><description>&lt;p&gt;
&#36229;&#36234;&#25968;&#37327;&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#36136;&#37327;&#19981;&#24179;&#31561;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond Quantities: Machine Learning-based Characterization of Inequality in Infrastructure Quality Provision in Cities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12074
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#22522;&#20110;&#25968;&#37327;&#30340;&#22522;&#30784;&#35774;&#26045;&#19981;&#24179;&#31561;&#29305;&#24449;&#21270;&#30740;&#31350;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#22478;&#24066;&#19981;&#24179;&#31561;&#21644;&#29615;&#22659;&#27491;&#20041;&#32771;&#34385;&#20043;&#38388;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#23545;&#22478;&#24066;&#22320;&#21306;&#30340;&#22522;&#30784;&#35774;&#26045;&#36136;&#37327;&#19981;&#24179;&#31561;&#36827;&#34892;&#29305;&#24449;&#21270;&#12290;&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#24050;&#32463;&#24847;&#35782;&#21040;&#29305;&#24449;&#21270;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#19981;&#24179;&#31561;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#25351;&#26631;&#20197;&#25351;&#23548;&#22478;&#24066;&#21457;&#23637;&#35268;&#21010;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#27979;&#37327;&#22522;&#30784;&#35774;&#26045;&#30340;&#25968;&#37327;&#19978;&#65292;&#20551;&#23450;&#26356;&#22810;&#30340;&#22522;&#30784;&#35774;&#26045;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#25351;&#25968;&#30340;&#26041;&#27861;&#19978;&#65292;&#20854;&#20013;&#22478;&#24066;&#22320;&#21306;&#22522;&#30784;&#35774;&#26045;&#20379;&#32473;&#29366;&#20917;&#26159;&#26681;&#25454;&#35774;&#23450;&#30340;&#20027;&#35266;&#26435;&#37325;&#30830;&#23450;&#30340;&#12290;&#23545;&#22522;&#30784;&#35774;&#26045;&#25968;&#37327;&#30340;&#20851;&#27880;&#21644;&#20351;&#29992;&#26469;&#33258;&#20027;&#35266;&#26435;&#37325;&#30340;&#25351;&#25968;&#24050;&#32463;&#22952;&#30861;&#20102;&#36866;&#24403;&#22320;&#30740;&#31350;&#22522;&#30784;&#35774;&#26045;&#19981;&#24179;&#31561;&#19982;&#22478;&#24066;&#19981;&#24179;&#31561;&#21644;&#29615;&#22659;&#27491;&#20041;&#32771;&#34385;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#37492;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12074v1 Announce Type: cross  Abstract: The objective of this study is to characterize inequality in infrastructure quality across urban areas. While a growing of body of literature has recognized the importance of characterizing infrastructure inequality in cities and provided quantified metrics to inform urban development plans, the majority of the existing approaches focus primarily on measuring the quantity of infrastructure, assuming that more infrastructure is better. Also, the existing research focuses primarily on index-based approaches in which the status of infrastructure provision in urban areas is determined based on assumed subjective weights. The focus on infrastructure quantity and use of indices obtained from subjective weights has hindered the ability to properly examine infrastructure inequality as it pertains to urban inequality and environmental justice considerations. Recognizing this gap, we propose a machine learning-based approach in which infrastruct
&lt;/p&gt;</description></item><item><title>LSKNet&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#39592;&#24178;&#65292;&#33021;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#31243;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2403.11735</link><description>&lt;p&gt;
LSKNet&#65306;&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#30340;&#36731;&#37327;&#32423;&#22522;&#30784;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
LSKNet: A Foundation Lightweight Backbone for Remote Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11735
&lt;/p&gt;
&lt;p&gt;
LSKNet&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#39592;&#24178;&#65292;&#33021;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#31243;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#23545;&#19979;&#28216;&#20219;&#21153;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#36965;&#24863;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#24573;&#35270;&#20102;&#23884;&#20837;&#22312;&#36965;&#24863;&#22330;&#26223;&#20013;&#30340;&#23453;&#36149;&#20808;&#39564;&#30693;&#35782;&#12290;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#21487;&#33021;&#20250;&#24456;&#26377;&#29992;&#65292;&#22240;&#20026;&#22312;&#27809;&#26377;&#21442;&#32771;&#36275;&#22815;&#38271;&#31243;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#36965;&#24863;&#23545;&#35937;&#21487;&#33021;&#20250;&#34987;&#38169;&#35823;&#35782;&#21035;&#65292;&#32780;&#36825;&#21487;&#20197;&#22240;&#19981;&#21516;&#23545;&#35937;&#32780;&#24322;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#65288;LSKNet&#65289;&#39592;&#24178;&#32593;&#32476;&#12290;LSKNet&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20808;&#21069;&#23578;&#26410;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#25506;&#32034;&#36807;&#22823;&#22411;&#21644;&#36873;&#25321;&#24615;&#26680;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#27809;&#26377;&#22826;&#22810;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11735v1 Announce Type: cross  Abstract: Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection and semantic segmentation, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightw
&lt;/p&gt;</description></item><item><title>LOOPer&#26159;&#38024;&#23545;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#30340;&#23398;&#20064;&#22411;&#33258;&#21160;&#20195;&#30721;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#24314;&#31435;&#25104;&#26412;&#27169;&#22411;&#26469;&#25351;&#23548;&#22810;&#38754;&#20307;&#20248;&#21270;&#25628;&#32034;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#32534;&#35793;&#22120;&#22312;&#36873;&#25321;&#20195;&#30721;&#36716;&#25442;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.11522</link><description>&lt;p&gt;
LOOPer: &#19968;&#20010;&#38024;&#23545;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#30340;&#23398;&#20064;&#22411;&#33258;&#21160;&#20195;&#30721;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11522
&lt;/p&gt;
&lt;p&gt;
LOOPer&#26159;&#38024;&#23545;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#30340;&#23398;&#20064;&#22411;&#33258;&#21160;&#20195;&#30721;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#24314;&#31435;&#25104;&#26412;&#27169;&#22411;&#26469;&#25351;&#23548;&#22810;&#38754;&#20307;&#20248;&#21270;&#25628;&#32034;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#32534;&#35793;&#22120;&#22312;&#36873;&#25321;&#20195;&#30721;&#36716;&#25442;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#22312;&#23454;&#29616;&#39640;&#32423;&#20195;&#30721;&#36716;&#25442;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#22312;&#36873;&#25321;&#33021;&#22815;&#24102;&#26469;&#26368;&#20339;&#21152;&#36895;&#30340;&#26368;&#26377;&#21033;&#36716;&#25442;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#20419;&#20351;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26500;&#24314;&#25104;&#26412;&#27169;&#22411;&#26469;&#24341;&#23548;&#22810;&#38754;&#20307;&#20248;&#21270;&#30340;&#25628;&#32034;&#12290;&#26368;&#20808;&#36827;&#30340;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#24050;&#32463;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#27010;&#24565;&#39564;&#35777;&#12290;&#34429;&#28982;&#36825;&#31181;&#27010;&#24565;&#39564;&#35777;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25104;&#26412;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#21482;&#25903;&#25345;&#23569;&#37327;&#20223;&#23556;&#21464;&#25442;&#30340;&#23376;&#38598;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#24212;&#29992;&#22797;&#26434;&#20195;&#30721;&#21464;&#25442;&#30340;&#33021;&#21147;&#12290;&#23427;&#20204;&#36824;&#21482;&#25903;&#25345;&#20855;&#26377;&#21333;&#20010;&#24490;&#29615;&#23884;&#22871;&#21644;&#30697;&#24418;&#36845;&#20195;&#22495;&#30340;&#31616;&#21333;&#31243;&#24207;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#35768;&#22810;&#31243;&#24207;&#30340;&#36866;&#29992;&#24615;&#12290;&#36825;&#20123;&#38480;&#21046;&#26174;&#33879;&#24433;&#21709;&#20102;&#36825;&#26679;&#30340;&#32534;&#35793;&#22120;&#21644;&#33258;&#21160;&#35843;&#24230;&#22120;&#30340;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11522v1 Announce Type: cross  Abstract: While polyhedral compilers have shown success in implementing advanced code transformations, they still have challenges in selecting the most profitable transformations that lead to the best speedups. This has motivated the use of machine learning to build cost models to guide the search for polyhedral optimizations. State-of-the-art polyhedral compilers have demonstrated a viable proof-of-concept of this approach. While such a proof-of-concept has shown promise, it still has significant limitations. State-of-the-art polyhedral compilers that use a deep-learning cost model only support a small subset of affine transformations, limiting their ability to apply complex code transformations. They also only support simple programs that have a single loop nest and a rectangular iteration domain, limiting their applicability to many programs. These limitations significantly impact the generality of such compilers and autoschedulers and put in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30446;&#26631;&#26816;&#27979;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#26377;&#25928;&#25552;&#21319;&#21463;&#25439;&#22270;&#20687;&#30340;&#26816;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.11220</link><description>&lt;p&gt;
CPA-Enhancer&#65306;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11220
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30446;&#26631;&#26816;&#27979;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#26377;&#25928;&#25552;&#21319;&#21463;&#25439;&#22270;&#20687;&#30340;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#21333;&#19968;&#36864;&#21270;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#26469;&#30830;&#23450;&#36864;&#21270;&#31867;&#22411;&#65292;&#24182;&#20026;&#27599;&#31181;&#31867;&#22411;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CPA-Enhancer&#22312;CoT&#25552;&#31034;&#30340;&#36880;&#27493;&#25351;&#23548;&#19979;&#36880;&#27493;&#35843;&#25972;&#20854;&#22686;&#24378;&#31574;&#30053;&#65292;&#36825;&#20123;&#25552;&#31034;&#32534;&#30721;&#20102;&#19982;&#36864;&#21270;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#21033;&#29992;CoT&#25552;&#31034;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#30340;&#24037;&#20316;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;CPA-Enhancer&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#22686;&#24378;&#27169;&#22411;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#20219;&#20309;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#22312;&#19981;&#20107;&#20808;&#30693;&#36947;&#36864;&#21270;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#21463;&#25439;&#22270;&#20687;&#19978;&#23454;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CPA-E
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11220v1 Announce Type: cross  Abstract: Object detection methods under known single degradations have been extensively investigated. However, existing approaches require prior knowledge of the degradation type and train a separate model for each, limiting their practical applications in unpredictable environments. To address this challenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer, CPA-Enhancer, for object detection under unknown degradations. Specifically, CPA-Enhancer progressively adapts its enhancement strategy under the step-by-step guidance of CoT prompts, that encode degradation-related information. To the best of our knowledge, it's the first work that exploits CoT prompting for object detection tasks. Overall, CPA-Enhancer is a plug-and-play enhancement model that can be integrated into any generic detectors to achieve substantial gains on degraded images, without knowing the degradation type priorly. Experimental results demonstrate that CPA-E
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#29992;&#20110;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#65292;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#26377;&#25928;&#24212;&#23545;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.10581</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#30340;ECG&#21452;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10581
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#29992;&#20110;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#65292;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#26377;&#25928;&#24212;&#23545;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#30001;&#20110;&#20840;&#29699;&#27515;&#20129;&#29575;&#19981;&#26029;&#19978;&#21319;&#32780;&#26500;&#25104;&#37325;&#22823;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#36890;&#36807;&#26089;&#26399;&#35786;&#26029;&#21644;&#39044;&#38450;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#21487;&#26174;&#33879;&#20943;&#23569;&#30142;&#30149;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#20020;&#24202;&#33719;&#21462;&#30340;12&#23548;&#32852;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#36827;&#34892;HF&#39118;&#38505;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#36731;&#37327;&#32423;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#26088;&#22312;&#25429;&#25417;&#23545;&#26089;&#26399;HF&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#22797;&#26434;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#23613;&#31649;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#19981;&#24179;&#34913;&#12290;&#35813;&#32593;&#32476;&#20855;&#26377;&#19968;&#20010;&#36328;&#23548;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;12&#20010;&#23548;&#32852;&#29305;&#23450;&#30340;&#26102;&#38388;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#25429;&#25417;&#20132;&#21449;&#23548;&#32852;&#20132;&#20114;&#20316;&#29992;&#21644;&#27599;&#20010;&#23548;&#32852;&#20869;&#30340;&#23616;&#37096;&#26102;&#38388;&#21160;&#24577;&#12290;&#20026;&#20102;&#38450;&#27490;&#27169;&#22411;&#36807;&#25311;&#21512;&#20110;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20844;&#20849;ECG-Report&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#36827;&#34892;ECG-&#25253;&#21578;&#23545;&#40784;&#20219;&#21153;&#12290;&#28982;&#21518;&#23545;&#32593;&#32476;&#36827;&#34892;fine-tune&#20197;&#29992;&#20110;HF&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10581v1 Announce Type: cross  Abstract: Heart failure (HF) poses a significant public health challenge due to its rising global mortality rate. Addressing this issue through early diagnosis and prevention could significantly reduce the disease's impact. This work introduces a methodology for HF risk prediction using clinically acquired 12-lead electrocardiograms (ECGs). We present a novel, lightweight dual-attention ECG network designed to capture complex ECG features essential for early HF prediction, despite the notable imbalance between low and high-risk groups. The network features a cross-lead attention module and twelve lead-specific temporal attention modules to capture cross-lead interactions and local temporal dynamics within each lead. To prevent model overfitting from limited training data, we leverage a large language model (LLM) with a public ECG-Report dataset for pretraining on an ECG-report alignment task. The network is then fine-tuned for HF risk prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;</title><link>https://arxiv.org/abs/2403.09919</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#24555;&#36895;&#25512;&#27979;&#35299;&#30721;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Recurrent Drafter for Fast Speculative Decoding in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#25913;&#36827;&#30340;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#25104;&#29087;&#25216;&#26415;&#30340;&#20248;&#21183;&#65306;&#32463;&#20856;&#30340;&#21452;&#27169;&#22411;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#21644;&#36739;&#26032;&#30340;&#21333;&#27169;&#22411;&#26041;&#27861;Medusa&#12290;&#20174;Medusa&#24471;&#21040;&#28789;&#24863;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#21333;&#27169;&#22411;&#31574;&#30053;&#36827;&#34892;&#25512;&#27979;&#35299;&#30721;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#30340;&#21333;&#20010;&#36731;&#37327;&#32423;&#33609;&#31295;&#22836;&#26469;&#21306;&#20998;&#33258;&#24049;&#65292;&#26412;&#36136;&#19978;&#31867;&#20284;&#20110;&#32463;&#20856;&#25512;&#27979;&#35299;&#30721;&#20013;&#20351;&#29992;&#30340;&#23567;&#22411;&#33609;&#31295;&#27169;&#22411;&#65292;&#20294;&#36991;&#20813;&#20102;&#23436;&#25972;transformer&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#30001;&#20110;&#24490;&#29615;&#20381;&#36182;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#27874;&#26463;&#25628;&#32034;&#24555;&#36895;&#36807;&#28388;&#20986;&#33609;&#31295;&#22836;&#20013;&#19981;&#38656;&#35201;&#30340;&#20505;&#36873;&#39033;&#12290;&#20854;&#32467;&#26524;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21333;&#27169;&#22411;&#35774;&#35745;&#31616;&#26131;&#24615;&#24182;&#36991;&#20813;&#20102;&#21019;&#24314;&#25968;&#25454;&#30456;&#20851;&#26641;&#20381;&#36182;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09919v1 Announce Type: new  Abstract: In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attent
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#23450;&#20102;&#23545;&#20110;&#23454;&#29616;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#26032;&#28526;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#35774;&#35745;&#32463;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.09611</link><description>&lt;p&gt;
MM1&#65306;&#22810;&#27169;&#24335;LLM&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12289;&#20998;&#26512;&#19982;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09611
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#23450;&#20102;&#23545;&#20110;&#23454;&#29616;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#26032;&#28526;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#35774;&#35745;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#21644;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#21508;&#31181;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#36827;&#34892;&#20180;&#32454;&#21644;&#20840;&#38754;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#20851;&#38190;&#30340;&#35774;&#35745;&#32463;&#39564;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#22823;&#35268;&#27169;&#22810;&#27169;&#24335;&#39044;&#35757;&#32451;&#20351;&#29992;&#20180;&#32454;&#28151;&#21512;&#30340;&#22270;&#20687;&#26631;&#39064;&#12289;&#20132;&#26367;&#22270;&#20687;&#25991;&#26412;&#21644;&#20165;&#25991;&#26412;&#25968;&#25454;&#23545;&#20110;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#26032;&#28526;&#65288;SOTA&#65289;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#19982;&#20854;&#20182;&#24050;&#21457;&#34920;&#30340;&#39044;&#35757;&#32451;&#32467;&#26524;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#22270;&#20687;&#32534;&#30721;&#22120;&#36830;&#21516;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#22270;&#20687;&#26631;&#35760;&#35745;&#25968;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#32780;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#35774;&#35745;&#30456;&#23545;&#37325;&#35201;&#24615;&#36739;&#23567;&#12290;&#36890;&#36807;&#25193;&#22823;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;MM1&#65292;&#19968;&#20010;&#22810;&#27169;&#24335;&#27169;&#22411;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09611v1 Announce Type: cross  Abstract: In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08763</link><description>&lt;p&gt;
&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Simple and Scalable Strategies to Continually Pre-train Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08763
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25968;&#21313;&#20159;&#30340;&#26631;&#35760;&#19978;&#36827;&#34892;&#24120;&#35268;&#39044;&#35757;&#32451;&#65292;&#19968;&#26086;&#26377;&#26032;&#25968;&#25454;&#21487;&#29992;&#23601;&#37325;&#26032;&#24320;&#22987;&#35813;&#36807;&#31243;&#12290;&#19968;&#20010;&#26356;&#26377;&#25928;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25345;&#32493;&#39044;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#19982;&#37325;&#26032;&#35757;&#32451;&#30456;&#27604;&#33021;&#33410;&#30465;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#24341;&#36215;&#30340;&#20998;&#24067;&#36716;&#31227;&#36890;&#24120;&#20250;&#23548;&#33268;&#22312;&#20197;&#21069;&#25968;&#25454;&#19978;&#38477;&#20302;&#24615;&#33021;&#25110;&#26080;&#27861;&#36866;&#24212;&#26032;&#25968;&#25454;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#65288;LR&#65289;&#37325;&#26032;&#21319;&#28201;&#12289;LR&#37325;&#26032;&#34928;&#20943;&#21644;&#37325;&#25918;&#19978;&#19968;&#25968;&#25454;&#30340;&#32452;&#21512;&#36275;&#20197;&#19982;&#23436;&#20840;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#22312;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#20174;&#26368;&#32456;&#25439;&#22833;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#35780;&#20272;&#22522;&#20934;&#30340;&#35282;&#24230;&#34913;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#8594;&#33521;&#35821;&#65289;&#20043;&#38388;&#30340;&#24369;&#20294;&#29616;&#23454;&#30340;&#20998;&#24067;&#36716;&#31227;&#20197;&#21450;&#26356;&#24378;&#28872;&#30340;&#20998;&#24067;&#36716;&#31227;&#65288;&#33521;&#35821;&#8594;&#24503;&#35821;&#65289;&#19979;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08763v1 Announce Type: cross  Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at th
&lt;/p&gt;</description></item><item><title>TeleMoMa &#26159;&#19968;&#31181;&#38754;&#21521;&#31227;&#21160;&#25805;&#20316;&#30340;&#27169;&#22359;&#21270;&#22810;&#21151;&#33021;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20154;&#26426;&#25509;&#21475;&#12289;&#38477;&#20302;&#38376;&#27099;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#20026;&#31227;&#21160;&#25805;&#20316;&#22120;&#25552;&#20379;&#20102;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.07869</link><description>&lt;p&gt;
TeleMoMa&#65306;&#19968;&#31181;&#29992;&#20110;&#31227;&#21160;&#25805;&#20316;&#30340;&#27169;&#22359;&#21270;&#22810;&#21151;&#33021;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07869
&lt;/p&gt;
&lt;p&gt;
TeleMoMa &#26159;&#19968;&#31181;&#38754;&#21521;&#31227;&#21160;&#25805;&#20316;&#30340;&#27169;&#22359;&#21270;&#22810;&#21151;&#33021;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20154;&#26426;&#25509;&#21475;&#12289;&#38477;&#20302;&#38376;&#27099;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#20026;&#31227;&#21160;&#25805;&#20316;&#22120;&#25552;&#20379;&#20102;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20013;&#38480;&#21046;&#27169;&#20223;&#23398;&#20064;&#30340;&#20851;&#38190;&#29942;&#39048;&#26159;&#25968;&#25454;&#30340;&#21294;&#20047;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#31227;&#21160;&#25805;&#20316;&#20013;&#26356;&#20026;&#20005;&#37325;&#65292;&#22240;&#20026;&#19982;&#38745;&#27490;&#25805;&#20316;&#30456;&#27604;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#29992;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#36828;&#31243;&#25805;&#20316;&#30028;&#38754;&#65292;&#25910;&#38598;&#28436;&#31034;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;TeleMoMa&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#31227;&#21160;&#25805;&#20316;&#22120;&#30340;&#36890;&#29992;&#21644;&#27169;&#22359;&#21270;&#30028;&#38754;&#12290;TeleMoMa&#23558;&#21253;&#25324;RGB&#21644;&#28145;&#24230;&#25668;&#20687;&#22836;&#12289;&#34394;&#25311;&#29616;&#23454;&#25511;&#21046;&#22120;&#12289;&#38190;&#30424;&#12289;&#25805;&#32437;&#26438;&#31561;&#22810;&#20010;&#20154;&#26426;&#25509;&#21475;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#21450;&#36825;&#20123;&#25509;&#21475;&#30340;&#20219;&#20309;&#32452;&#21512;&#12290;&#22312;&#20854;&#26356;&#26131;&#35775;&#38382;&#30340;&#29256;&#26412;&#20013;&#65292; TeleMoMa&#21487;&#20197;&#20165;&#20351;&#29992;&#35270;&#35273;&#65288;&#22914;RGB-D&#30456;&#26426;&#65289;&#21363;&#21487;&#24037;&#20316;&#65292;&#38477;&#20302;&#20102;&#20154;&#31867;&#25552;&#20379;&#31227;&#21160;&#25805;&#20316;&#28436;&#31034;&#30340;&#38376;&#27099;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#36828;&#31243;&#25805;&#20316;&#20960;&#20010;&#29616;&#26377;&#30340;&#31227;&#21160;&#25805;&#20316;&#22120;&#8212;&#8212;PAL Tiago++, Toyota HSR&#21644;Fetch&#26469;&#23637;&#29616;TeleMoMa&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07869v1 Announce Type: cross  Abstract: A critical bottleneck limiting imitation learning in robotics is the lack of data. This problem is more severe in mobile manipulation, where collecting demonstrations is harder than in stationary manipulation due to the lack of available and easy-to-use teleoperation interfaces. In this work, we demonstrate TeleMoMa, a general and modular interface for whole-body teleoperation of mobile manipulators. TeleMoMa unifies multiple human interfaces including RGB and depth cameras, virtual reality controllers, keyboard, joysticks, etc., and any combination thereof. In its more accessible version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering the entry bar for humans to provide mobile manipulation demonstrations. We demonstrate the versatility of TeleMoMa by teleoperating several existing mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and the real world. We demonstrate the quality of the demonst
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#25193;&#25955;&#30340;NAS&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#26465;&#20214;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26041;&#27861;&#65292;&#33021;&#22312;&#26550;&#26500;&#25628;&#32034;&#20013;&#29983;&#25104;&#24555;&#36895;&#19988;&#24615;&#33021;&#20248;&#36234;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;</title><link>https://arxiv.org/abs/2403.06020</link><description>&lt;p&gt;
&#22810;&#26465;&#20214;&#22270;&#25193;&#25955;&#29992;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-conditioned Graph Diffusion for Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06020
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#25193;&#25955;&#30340;NAS&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#26465;&#20214;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26041;&#27861;&#65292;&#33021;&#22312;&#26550;&#26500;&#25628;&#32034;&#20013;&#29983;&#25104;&#24555;&#36895;&#19988;&#24615;&#33021;&#20248;&#36234;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#36890;&#36807;&#25506;&#32034;&#24222;&#22823;&#19988;&#22797;&#26434;&#30340;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#26469;&#33258;&#21160;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#20026;&#20102;&#25512;&#21160;&#26550;&#26500;&#25628;&#32034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#25193;&#25955;&#30340;NAS&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31163;&#25955;&#26465;&#20214;&#22270;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#24615;&#33021;&#20248;&#36234;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26465;&#20214;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#22270;&#25193;&#25955;&#32593;&#32476;&#65292;&#20849;&#21516;&#26045;&#21152;&#35832;&#22914;&#39640;&#20934;&#30830;&#24615;&#21644;&#20302;&#30828;&#20214;&#24310;&#36831;&#31561;&#32422;&#26463;&#12290;&#19982;&#30456;&#20851;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23436;&#20840;&#21487;&#24494;&#20998;&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#21333;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20845;&#20010;&#26631;&#20934;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20197;&#24555;&#36895;&#36895;&#24230;&#29983;&#25104;&#26032;&#39062;&#19988;&#29420;&#29305;&#30340;&#26550;&#26500;&#65292;&#21363;&#27599;&#31181;&#26550;&#26500;&#23569;&#20110;0.2&#31186;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06020v1 Announce Type: new  Abstract: Neural architecture search automates the design of neural network architectures usually by exploring a large and thus complex architecture search space. To advance the architecture search, we present a graph diffusion-based NAS approach that uses discrete conditional graph diffusion processes to generate high-performing neural network architectures. We then propose a multi-conditioned classifier-free guidance approach applied to graph diffusion networks to jointly impose constraints such as high accuracy and low hardware latency. Unlike the related work, our method is completely differentiable and requires only a single model training. In our evaluations, we show promising results on six standard benchmarks, yielding novel and unique architectures at a fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we demonstrate the generalisability and efficiency of our method through experiments on ImageNet dataset.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#28909;&#21147;&#23398;&#23450;&#24459;&#30340;&#25968;&#25454;&#39537;&#21160;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#28508;&#21464;&#37327;&#24182;&#26500;&#24314;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#28909;&#21147;&#23398;&#23450;&#24459;&#30340;&#36981;&#23432;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#28436;&#31034;&#20102;&#20854;&#22312;&#31283;&#20581;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#22312;&#28508;&#31354;&#38388;&#20013;&#29109;&#20135;&#29983;&#36895;&#29575;&#19982;&#31995;&#32479;&#34892;&#20026;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05848</link><description>&lt;p&gt;
tLaSDI: &#28909;&#21147;&#23398;&#20449;&#24687;&#39537;&#21160;&#30340;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
tLaSDI: Thermodynamics-informed latent space dynamics identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05848
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#28909;&#21147;&#23398;&#23450;&#24459;&#30340;&#25968;&#25454;&#39537;&#21160;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#28508;&#21464;&#37327;&#24182;&#26500;&#24314;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#28909;&#21147;&#23398;&#23450;&#24459;&#30340;&#36981;&#23432;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#28436;&#31034;&#20102;&#20854;&#22312;&#31283;&#20581;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#22312;&#28508;&#31354;&#38388;&#20013;&#29109;&#20135;&#29983;&#36895;&#29575;&#19982;&#31995;&#32479;&#34892;&#20026;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;&#65288;tLaSDI&#65289;&#65292;&#35813;&#26041;&#27861;&#34701;&#20837;&#20102;&#28909;&#21147;&#23398;&#30340;&#31532;&#19968;&#21644;&#31532;&#20108;&#23450;&#24459;&#12290;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#28508;&#21464;&#37327;&#20316;&#20026;&#38750;&#32447;&#24615;&#38477;&#32500;&#27169;&#22411;&#12290;&#28508;&#21464;&#37327;&#30340;&#21160;&#21147;&#23398;&#30001;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#26500;&#24314;&#65292;&#36890;&#36807;&#36890;&#29992;&#24418;&#24335;&#20027;&#20041;&#20445;&#30041;&#26576;&#20123;&#32467;&#26500;&#20197;&#23562;&#37325;&#28909;&#21147;&#23398;&#23450;&#24459;&#12290;&#24314;&#31435;&#20102;&#23545;&#36817;&#20284;&#20540;&#30340;&#25277;&#35937;&#35823;&#24046;&#20272;&#35745;&#65292;&#25552;&#20379;&#20102;&#28041;&#21450;&#33258;&#21160;&#32534;&#30721;&#22120;&#38597;&#21487;&#27604;&#35745;&#31639;&#30340;&#26032;&#25439;&#22833;&#21046;&#23450;&#12290;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#28508;&#21160;&#24577;&#37117;&#32463;&#36807;&#35757;&#32451;&#20197;&#26368;&#23567;&#21270;&#26032;&#30340;&#25439;&#22833;&#12290;&#23637;&#31034;&#20102;&#25968;&#20540;&#31034;&#20363;&#20197;&#28436;&#31034;tLaSDI&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#22806;&#25512;&#24773;&#20917;&#19979;&#20063;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22312;&#28508;&#31354;&#38388;&#20013;&#35266;&#23519;&#21040;&#20102;&#29109;&#20135;&#29983;&#36895;&#29575;&#19982;&#23436;&#25972;&#34892;&#20026;&#20043;&#38388;&#30340;&#26377;&#36259;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05848v1 Announce Type: new  Abstract: We propose a data-driven latent space dynamics identification method (tLaSDI) that embeds the first and second principles of thermodynamics. The latent variables are learned through an autoencoder as a nonlinear dimension reduction model. The dynamics of the latent variables are constructed by a neural network-based model that preserves certain structures to respect the thermodynamic laws through the GENERIC formalism. An abstract error estimate of the approximation is established, which provides a new loss formulation involving the Jacobian computation of autoencoder. Both the autoencoder and the latent dynamics are trained to minimize the new loss. Numerical examples are presented to demonstrate the performance of tLaSDI, which exhibits robust generalization ability, even in extrapolation. In addition, an intriguing correlation is empirically observed between the entropy production rates in the latent space and the behaviors of the ful
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;TOSG&#25552;&#21462;&#30340;&#26041;&#27861;KG-TOSA&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38754;&#21521;&#20219;&#21153;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#20197;&#20943;&#36731;&#23545;&#22823;&#22411;KG&#30340;&#36807;&#22810;&#35745;&#31639;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2403.05752</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#30693;&#35782;&#22270;&#19978;&#35757;&#32451;&#38754;&#21521;&#20219;&#21153;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20934;&#30830;&#39640;&#25928;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;TOSG&#25552;&#21462;&#30340;&#26041;&#27861;KG-TOSA&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38754;&#21521;&#20219;&#21153;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#20197;&#20943;&#36731;&#23545;&#22823;&#22411;KG&#30340;&#36807;&#22810;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#26159;&#19968;&#31181;&#21253;&#21547;&#21508;&#31181;&#33410;&#28857;&#21644;&#36793;&#31867;&#22411;&#30340;&#24322;&#26500;&#22270;&#12290;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#36890;&#24120;&#29992;&#20110;&#22312;KG&#19978;&#35757;&#32451;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#31561;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;HGNN&#26041;&#27861;&#21463;KG&#30340;&#22823;&#23567;&#12289;&#23494;&#24230;&#20197;&#21450;&#33410;&#28857;&#21644;&#36793;&#31867;&#22411;&#25968;&#37327;&#30340;&#24433;&#21709;&#65292;&#34920;&#29616;&#20986;&#36807;&#22810;&#30340;&#22797;&#26434;&#24615;&#12290;AI&#20174;&#19994;&#32773;&#25163;&#24037;&#35774;&#35745;&#20986;&#19968;&#20010;&#19982;&#29305;&#23450;&#20219;&#21153;&#30456;&#20851;&#30340;KG G&#30340;&#23376;&#22270;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#38754;&#21521;&#20219;&#21153;&#30340;&#23376;&#22270;&#65288;TOSG&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;G&#20013;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#33410;&#28857;&#21644;&#36793;&#31867;&#22411;&#30340;&#23376;&#38598;&#12290;&#20351;&#29992;TOSG&#32780;&#19981;&#26159;G&#26469;&#35757;&#32451;&#20219;&#21153;&#21487;&#20197;&#20943;&#36731;&#23545;&#22823;&#22411;KG&#25152;&#38656;&#30340;&#36807;&#22810;&#35745;&#31639;&#12290;&#35774;&#35745;TOSG&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;KG&#30340;&#32467;&#26500;&#21644;&#20219;&#21153;&#30340;&#30446;&#26631;&#65292;&#22240;&#27492;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;KG-TOSA&#65292;&#19968;&#31181;&#33258;&#21160;&#21270;TOSG&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;KG&#19978;&#36827;&#34892;&#38754;&#21521;&#20219;&#21153;&#30340;HGNN&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05752v1 Announce Type: cross  Abstract: A Knowledge Graph (KG) is a heterogeneous graph encompassing a diverse range of node and edge types. Heterogeneous Graph Neural Networks (HGNNs) are popular for training machine learning tasks like node classification and link prediction on KGs. However, HGNN methods exhibit excessive complexity influenced by the KG's size, density, and the number of node and edge types. AI practitioners handcraft a subgraph of a KG G relevant to a specific task. We refer to this subgraph as a task-oriented subgraph (TOSG), which contains a subset of task-related node and edge types in G. Training the task using TOSG instead of G alleviates the excessive computation required for a large KG. Crafting the TOSG demands a deep understanding of the KG's structure and the task's objectives. Hence, it is challenging and time-consuming. This paper proposes KG-TOSA, an approach to automate the TOSG extraction for task-oriented HGNN training on a large KG. In KG
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#38480;&#21046;&#22312;&#26368;&#36817;&#30340;&#37051;&#23621;&#20043;&#38388;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.04690</link><description>&lt;p&gt;
&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;: &#22312;&#32447;&#31243;&#22359;&#32423;&#21035;&#20943;&#23569;&#33258;&#27880;&#24847;&#21147;&#30340;O(n^2)&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04690
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#38480;&#21046;&#22312;&#26368;&#36817;&#30340;&#37051;&#23621;&#20043;&#38388;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37051;&#22495;&#27880;&#24847;&#21147;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#26631;&#35760;&#30340;&#27880;&#24847;&#21147;&#33539;&#22260;&#20026;&#20854;&#26368;&#36817;&#30340;&#37051;&#23621;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#25104;&#26412;&#12290;&#35813;&#38480;&#21046;&#30001;&#31383;&#21475;&#22823;&#23567;&#21644;&#25193;&#24352;&#22240;&#23376;&#21442;&#25968;&#21270;&#65292;&#20171;&#20110;&#32447;&#24615;&#25237;&#24433;&#21644;&#33258;&#27880;&#24847;&#21147;&#20043;&#38388;&#32472;&#21046;&#20102;&#21487;&#33021;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#35889;&#12290;&#37051;&#22495;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#26356;&#19968;&#33324;&#22320;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#22522;&#30784;&#35774;&#26045;&#26041;&#38754;&#38271;&#26399;&#21463;&#21040;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#39640;&#31209;&#30340;&#31354;&#38388;&#65288;2-D&#21644;3-D&#65289;&#65292;&#20419;&#20351;&#24320;&#21457;&#23450;&#21046;&#20869;&#26680;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#20869;&#26680;&#22312;&#21151;&#33021;&#25110;&#24615;&#33021;&#26041;&#38754;&#21463;&#38480;&#65292;&#22914;&#26524;&#19981;&#26159;&#20004;&#32773;&#37117;&#26377;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#37051;&#22495;&#27880;&#24847;&#21147;&#21487;&#20197;&#34920;&#31034;&#20026;&#25209;&#37327;&#21270;&#30340;GEMM&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#24182;&#20026;1-D&#21644;2-D&#37051;&#22495;&#27880;&#24847;&#21147;&#23454;&#29616;&#23427;&#12290;&#19982;&#29616;&#26377;&#30340;&#31616;&#21333;&#20869;&#26680;&#30456;&#27604;&#65292;&#36825;&#20123;&#20869;&#26680;&#24179;&#22343;&#25552;&#20379;&#20102;&#20998;&#21035;&#26159;1-D&#21644;2-D&#37051;&#22495;&#27880;&#24847;&#21147;&#30340;&#20840;&#31934;&#24230;&#24310;&#36831;&#25913;&#36827;&#20998;&#21035;&#20026;895%&#21644;272%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04690v1 Announce Type: cross  Abstract: Neighborhood attention reduces the cost of self attention by restricting each token's attention span to its nearest neighbors. This restriction, parameterized by a window size and dilation factor, draws a spectrum of possible attention patterns between linear projection and self attention. Neighborhood attention, and more generally sliding window attention patterns, have long been bounded by infrastructure, particularly in higher-rank spaces (2-D and 3-D), calling for the development of custom kernels, which have been limited in either functionality, or performance, if not both. In this work, we first show that neighborhood attention can be represented as a batched GEMM problem, similar to standard attention, and implement it for 1-D and 2-D neighborhood attention. These kernels on average provide 895% and 272% improvement in full precision latency compared to existing naive kernels for 1-D and 2-D neighborhood attention respectively. 
&lt;/p&gt;</description></item><item><title>ChunkAttention&#26159;&#19968;&#31181;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#24182;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#34892;&#26102;&#25913;&#21892;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;KV&#32531;&#23384;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#20197;&#25552;&#39640;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15220</link><description>&lt;p&gt;
ChunkAttention: &#20855;&#26377;&#21069;&#32512;&#24863;&#30693;KV&#32531;&#23384;&#21644;&#20004;&#38454;&#27573;&#20998;&#21306;&#30340;&#39640;&#25928;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15220
&lt;/p&gt;
&lt;p&gt;
ChunkAttention&#26159;&#19968;&#31181;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#24182;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#34892;&#26102;&#25913;&#21892;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;KV&#32531;&#23384;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#20197;&#25552;&#39640;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;&#38271;&#24207;&#21015;&#26469;&#35828;&#26159;&#25512;&#29702;&#24310;&#36831;&#30340;&#19968;&#20010;&#26174;&#33879;&#26469;&#28304;&#12290;&#22312;&#22810;&#31199;&#25143;LLMs&#26381;&#21153;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;LLM&#35831;&#27714;&#22312;&#21069;&#32512;&#20013;&#20849;&#20139;&#31995;&#32479;&#25552;&#31034;&#30340;&#27010;&#29575;&#65292;&#21487;&#20197;&#20248;&#21270;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25805;&#20316;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChunkAttention&#65292;&#19968;&#31181;&#20855;&#26377;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#26816;&#27979;&#22810;&#20010;&#35831;&#27714;&#20043;&#38388;&#21305;&#37197;&#30340;&#25552;&#31034;&#21069;&#32512;&#65292;&#24182;&#20849;&#20139;&#23427;&#20204;&#30340;&#38190;/&#20540;&#24352;&#37327;&#20197;&#25913;&#36827;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#21033;&#29992;&#29575;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#25972;&#20307;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#26469;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;KV&#32531;&#23384;&#20043;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#33258;&#27880;&#24847;&#21147;&#20869;&#26680;&#65292;&#20854;&#20013;&#23454;&#29616;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#65292;&#20197;&#25913;&#21892;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15220v1 Announce Type: cross  Abstract: Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32479;&#35745;&#26080;&#20851;&#22320;&#35780;&#20272;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;ML&#20272;&#35745;&#22312;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.15213</link><description>&lt;p&gt;
&#32479;&#35745;&#26080;&#20559;&#22238;&#24402;&#65306;&#19968;&#31181;&#29992;&#20110;&#39564;&#35777;&#22238;&#24402;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Statistical Agnostic Regression: a machine learning method to validate regression models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32479;&#35745;&#26080;&#20851;&#22320;&#35780;&#20272;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;ML&#20272;&#35745;&#22312;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#20998;&#26512;&#26159;&#32479;&#35745;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20027;&#39064;&#65292;&#26088;&#22312;&#20272;&#35745;&#22240;&#21464;&#37327;&#65288;&#36890;&#24120;&#31216;&#20026;&#21709;&#24212;&#21464;&#37327;&#65289;&#19982;&#19968;&#20010;&#25110;&#22810;&#20010;&#33258;&#21464;&#37327;&#65288;&#21363;&#35299;&#37322;&#21464;&#37327;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32447;&#24615;&#22238;&#24402;&#26159;&#36804;&#20170;&#20026;&#27490;&#22312;&#39044;&#27979;&#12289;&#39044;&#27979;&#25110;&#22240;&#26524;&#25512;&#26029;&#31561;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#25191;&#34892;&#27492;&#20219;&#21153;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#12290;&#38500;&#20102;&#35299;&#20915;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#30340;&#21508;&#31181;&#20256;&#32479;&#26041;&#27861;&#22806;&#65292;&#22914;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#12289;&#23725;&#22238;&#24402;&#25110;&#22871;&#32034;&#22238;&#24402;&#8212;&#8212;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#26159;&#26356;&#39640;&#32423;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#30340;&#22522;&#30784;&#8212;&#8212;&#21518;&#32773;&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#22312;&#36825;&#31181;&#22330;&#26223;&#20013;&#65292;&#20294;&#27809;&#26377;&#23545;&#32479;&#35745;&#26174;&#33879;&#24615;&#36827;&#34892;&#27491;&#24335;&#23450;&#20041;&#12290;&#26368;&#22810;&#65292;&#22522;&#20110;&#32463;&#39564;&#27979;&#37327;&#65288;&#22914;&#27531;&#24046;&#25110;&#20934;&#30830;&#24230;&#65289;&#36827;&#34892;&#32622;&#25442;&#25110;&#22522;&#20110;&#32463;&#20856;&#20998;&#26512;&#65292;&#20197;&#21453;&#26144;ML&#20272;&#35745;&#23545;&#26816;&#27979;&#30340;&#26356;&#39640;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32479;&#35745;&#26080;&#20851;&#22320;&#35780;&#20272;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#23545;ML&#20272;&#35745;&#22312;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15213v1 Announce Type: cross  Abstract: Regression analysis is a central topic in statistical modeling, aiming to estimate the relationships between a dependent variable, commonly referred to as the response variable, and one or more independent variables, i.e., explanatory variables. Linear regression is by far the most popular method for performing this task in several fields of research, such as prediction, forecasting, or causal inference. Beyond various classical methods to solve linear regression problems, such as Ordinary Least Squares, Ridge, or Lasso regressions - which are often the foundation for more advanced machine learning (ML) techniques - the latter have been successfully applied in this scenario without a formal definition of statistical significance. At most, permutation or classical analyses based on empirical measures (e.g., residuals or accuracy) have been conducted to reflect the greater ability of ML estimations for detection. In this paper, we introd
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#35299;&#32806;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#26102;&#31354;&#25554;&#34917;&#65292;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#26469;&#25552;&#39640;&#39044;&#27979;&#25928;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11558</link><description>&lt;p&gt;
&#26102;&#38388;&#35299;&#32806;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#26102;&#31354;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
Temporal Disentangled Contrastive Diffusion Model for Spatiotemporal Imputation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11558
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#35299;&#32806;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#26102;&#31354;&#25554;&#34917;&#65292;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#26469;&#25552;&#39640;&#39044;&#27979;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11558v1 &#21457;&#24067;&#31867;&#22411;: &#26032;&#20869;&#23481; &#25688;&#35201;: &#26102;&#31354;&#25968;&#25454;&#20998;&#26512;&#22312;&#21508;&#20010;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#20132;&#36890;&#12289;&#27668;&#35937;&#21644;&#21307;&#30103;&#20445;&#20581;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#24448;&#24448;&#22240;&#20256;&#24863;&#22120;&#25925;&#38556;&#21644;&#32593;&#32476;&#20256;&#36755;&#38169;&#35823;&#32780;&#19981;&#23436;&#25972;&#12290;&#26102;&#31354;&#25554;&#34917;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#26469;&#39044;&#27979;&#32570;&#22833;&#20540;&#12290;&#20256;&#32479;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#32463;&#20856;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#24120;&#19981;&#36275;&#22815;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#26410;&#33021;&#31526;&#21512;&#20005;&#26684;&#30340;&#20998;&#24067;&#20551;&#35774;&#26102;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26368;&#36817;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#24418;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#22686;&#24378;&#30340;&#21151;&#25928;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23481;&#26131;&#31215;&#32047;&#35823;&#24046;&#12290;&#29983;&#25104;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#29992;&#65292;&#20197;&#35268;&#36991;&#20381;&#36182;&#20110;&#28508;&#22312;&#19981;&#20934;&#30830;&#30340;&#21382;&#21490;&#25554;&#34917;&#20540;&#36827;&#34892;&#26410;&#26469;&#39044;&#27979;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11558v1 Announce Type: new  Abstract: Spatiotemporal data analysis is pivotal across various domains, including transportation, meteorology, and healthcare. However, the data collected in real-world scenarios often suffers incompleteness due to sensor malfunctions and network transmission errors. Spatiotemporal imputation endeavours to predict missing values by exploiting the inherent spatial and temporal dependencies present in the observed data. Traditional approaches, which rely on classical statistical and machine learning techniques, are often inadequate, particularly when the data fails to meet strict distributional assumptions. In contrast, recent deep learning-based methods, leveraging graph and recurrent neural networks, have demonstrated enhanced efficacy. Nonetheless, these approaches are prone to error accumulation. Generative models have been increasingly adopted to circumvent the reliance on potentially inaccurate historical imputed values for future prediction
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#30701;&#35270;&#39057;&#23545;&#35266;&#20247;&#24515;&#29702;&#20581;&#24247;&#30340;&#25233;&#37057;&#24433;&#21709;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21307;&#23398;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#20854;&#24433;&#21709;&#24182;&#37319;&#21462;&#30456;&#24212;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;</title><link>https://arxiv.org/abs/2402.10045</link><description>&lt;p&gt;
&#30701;&#35270;&#39057;&#21644;&#24515;&#29702;&#20581;&#24247;&#65306;&#22522;&#20110;&#30693;&#35782;&#23548;&#21521;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Short-Form Videos and Mental Health: A Knowledge-Guided Multimodal Neural Topic Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10045
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#30701;&#35270;&#39057;&#23545;&#35266;&#20247;&#24515;&#29702;&#20581;&#24247;&#30340;&#25233;&#37057;&#24433;&#21709;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21307;&#23398;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#20854;&#24433;&#21709;&#24182;&#37319;&#21462;&#30456;&#24212;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#35270;&#39057;&#27491;&#35797;&#22270;&#37325;&#26032;&#22609;&#36896;&#25972;&#20010;&#31038;&#20132;&#23186;&#20307;&#26223;&#35266;&#65292;&#28982;&#32780;&#19987;&#23478;&#20204;&#23545;&#20854;&#23545;&#35266;&#20247;&#30340;&#25233;&#37057;&#24433;&#21709;&#24863;&#21040;&#26497;&#24230;&#25285;&#24551;&#65292;&#36825;&#19968;&#28857;&#24050;&#30001;&#21307;&#23398;&#30740;&#31350;&#35777;&#26126;&#12290;&#20026;&#20102;&#38450;&#27490;&#24191;&#27867;&#24433;&#21709;&#65292;&#21508;&#24179;&#21488;&#28212;&#26395;&#39044;&#27979;&#36825;&#20123;&#35270;&#39057;&#23545;&#35266;&#20247;&#24515;&#29702;&#20581;&#24247;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#37319;&#21462;&#24178;&#39044;&#25514;&#26045;&#65292;&#27604;&#22914;&#20462;&#35746;&#25512;&#33616;&#31639;&#27861;&#21644;&#26174;&#31034;&#35266;&#20247;&#24910;&#37325;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#27979;&#26041;&#27861;&#32570;&#20047;&#19982;&#25233;&#37057;&#30151;&#30340;&#20020;&#24202;&#35777;&#23454;&#30340;&#22806;&#37096;&#29615;&#22659;&#22240;&#32032;&#30456;&#20851;&#30340;&#21307;&#23398;&#30693;&#35782;&#12290;&#20026;&#20102;&#32771;&#34385;&#36825;&#26679;&#30340;&#21307;&#23398;&#30693;&#35782;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#26041;&#27861;&#35770;&#23398;&#31185;&#8212;&#8212;&#31181;&#23376;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTMs&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31181;&#23376;NTMs&#23384;&#22312;&#21333;&#19968;&#26469;&#28304;&#20027;&#39064;&#12289;&#26410;&#30693;&#20027;&#39064;&#26469;&#28304;&#12289;&#27169;&#31946;&#30340;&#31181;&#23376;&#30417;&#30563;&#21644;&#27425;&#20248;&#30340;&#25910;&#25947;&#31561;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30693;&#35782;&#25351;&#23548;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;Knowledg...&#65288;&#24453;&#34917;&#20805;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10045v1 Announce Type: cross  Abstract: While short-form videos head to reshape the entire social media landscape, experts are exceedingly worried about their depressive impacts on viewers, as evidenced by medical studies. To prevent widespread consequences, platforms are eager to predict these videos' impact on viewers' mental health. Subsequently, they can take intervention measures, such as revising recommendation algorithms and displaying viewer discretion. Nevertheless, applicable predictive methods lack relevance to well-established medical knowledge, which outlines clinically proven external and environmental factors of depression. To account for such medical knowledge, we resort to an emergent methodological discipline, seeded Neural Topic Models (NTMs). However, existing seeded NTMs suffer from the limitations of single-origin topics, unknown topic sources, unclear seed supervision, and suboptimal convergence. To address those challenges, we develop a novel Knowledg
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#23384;&#22312;&#29305;&#23450;&#32676;&#20307;&#27010;&#24565;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.07586</link><description>&lt;p&gt;
&#25581;&#31034;&#29305;&#23450;&#32676;&#20307;&#30340;&#20998;&#24067;&#24335;&#27010;&#24565;&#28418;&#31227;: &#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Unveiling Group-Specific Distributed Concept Drift: A Fairness Imperative in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07586
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#23384;&#22312;&#29305;&#23450;&#32676;&#20307;&#27010;&#24565;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#30830;&#20445;&#20844;&#24179;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#65292;&#25512;&#21160;&#20102;&#24320;&#21457;&#26088;&#22312;&#20943;&#23569;&#20915;&#31574;&#36807;&#31243;&#20013;&#27495;&#35270;&#32467;&#26524;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#29305;&#23450;&#32676;&#20307;&#30340;&#27010;&#24565;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20195;&#34920;&#20102;&#22312;&#36825;&#26041;&#38754;&#30340;&#24320;&#25299;&#24615;&#21162;&#21147;&#12290;&#29305;&#23450;&#32676;&#20307;&#30340;&#27010;&#24565;&#28418;&#31227;&#26159;&#25351;&#19968;&#20010;&#32676;&#20307;&#38543;&#26102;&#38388;&#32463;&#21382;&#27010;&#24565;&#28418;&#31227;&#65292;&#32780;&#21478;&#19968;&#20010;&#32676;&#20307;&#21364;&#27809;&#26377;&#65292;&#23548;&#33268;&#20844;&#24179;&#24615;&#19979;&#38477;&#65292;&#21363;&#20351;&#20934;&#30830;&#24615;&#20445;&#25345;&#30456;&#23545;&#31283;&#23450;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#19979;&#65292;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20998;&#24067;&#24335;&#24615;&#36136;&#36827;&#19968;&#27493;&#25918;&#22823;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#22240;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#21487;&#20197;&#29420;&#31435;&#32463;&#21382;&#29305;&#23450;&#32676;&#20307;&#30340;&#27010;&#24565;&#28418;&#31227;&#65292;&#21516;&#26102;&#20173;&#20849;&#20139;&#30456;&#21516;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#20174;&#32780;&#21019;&#36896;&#20102;&#19968;&#20010;&#22797;&#26434;&#32780;&#21160;&#24577;&#30340;&#29615;&#22659;&#26469;&#32500;&#25345;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#20043;&#19968;&#26159;&#23545;&#32676;&#20307;&#29305;&#23450;&#30340;&#27010;&#24565;&#28418;&#31227;&#36827;&#34892;&#24418;&#24335;&#21270;&#21644;&#20869;&#37096;&#21270;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the evolving field of machine learning, ensuring fairness has become a critical concern, prompting the development of algorithms designed to mitigate discriminatory outcomes in decision-making processes. However, achieving fairness in the presence of group-specific concept drift remains an unexplored frontier, and our research represents pioneering efforts in this regard. Group-specific concept drift refers to situations where one group experiences concept drift over time while another does not, leading to a decrease in fairness even if accuracy remains fairly stable. Within the framework of federated learning, where clients collaboratively train models, its distributed nature further amplifies these challenges since each client can experience group-specific concept drift independently while still sharing the same underlying concept, creating a complex and dynamic environment for maintaining fairness. One of the significant contributions of our research is the formalization and intr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#23454;&#29616;&#20154;&#33080;&#36870;&#40836;&#21270;&#65292;&#35299;&#20915;&#20102;NPR&#22270;&#20687;&#19978;&#32534;&#36753;&#24180;&#40836;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#24182;&#19988;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#38754;&#37096;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02733</link><description>&lt;p&gt;
ToonAging: &#33402;&#26415;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#19979;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;
&lt;/p&gt;
&lt;p&gt;
ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#23454;&#29616;&#20154;&#33080;&#36870;&#40836;&#21270;&#65292;&#35299;&#20915;&#20102;NPR&#22270;&#20687;&#19978;&#32534;&#36753;&#24180;&#40836;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#24182;&#19988;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#38754;&#37096;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#36870;&#40836;&#21270;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#22312;&#30005;&#24433;&#12289;&#24191;&#21578;&#21644;&#30452;&#25773;&#31561;&#36924;&#30495;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#23558;&#20154;&#33080;&#36870;&#40836;&#21270;&#24212;&#29992;&#20110;&#38750;&#36924;&#30495;&#22270;&#20687;&#65292;&#22914;&#28459;&#30011;&#12289;&#25554;&#22270;&#21644;&#21160;&#30011;&#65292;&#22312;&#21508;&#31181;&#23089;&#20048;&#34892;&#19994;&#20013;&#25104;&#20026;&#19968;&#20010;&#26032;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#26080;&#32541;&#32534;&#36753;NPR&#22270;&#20687;&#19978;&#26174;&#29616;&#24180;&#40836;&#30340;&#32593;&#32476;&#24847;&#21619;&#30528;&#36825;&#20123;&#20219;&#21153;&#19968;&#30452;&#23616;&#38480;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#39034;&#24207;&#26041;&#27861;&#65292;&#36825;&#24448;&#24448;&#20250;&#23548;&#33268;&#19981;&#24841;&#24555;&#30340;&#20266;&#24433;&#21644;&#30001;&#20110;&#22495;&#24046;&#24322;&#32780;&#20002;&#22833;&#38754;&#37096;&#23646;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#38454;&#27573;&#20154;&#33080;&#36870;&#40836;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#65292;&#22312;&#19968;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#23436;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#20004;&#32773;&#37117;&#22312;&#30456;&#21516;&#30340;PR&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#27599;&#20010;&#21521;&#37327;&#36127;&#36131;&#31649;&#29702;&#19982;&#34928;&#32769;&#30456;&#20851;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attribu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#22522;&#20110;&#33655;&#20848;&#30340;&#35843;&#26597;&#25968;&#25454;&#21644;&#30331;&#35760;&#25968;&#25454;&#65292;&#29992;&#20110;&#30740;&#31350;&#33655;&#20848;&#29983;&#32946;&#32467;&#26524;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#21644;&#26679;&#26412;&#65292;&#24182;&#25551;&#36848;&#20102;&#29983;&#32946;&#32467;&#26524;&#30340;&#20855;&#20307;&#20869;&#23481;&#12290;&#20182;&#20204;&#36824;&#20171;&#32461;&#20102;&#29983;&#32946;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.00705</link><description>&lt;p&gt;
&#32467;&#21512;&#33655;&#20848;&#35843;&#26597;&#21644;&#30331;&#35760;&#25968;&#25454;&#30340;&#25968;&#25454;&#25361;&#25112;&#65292;&#39044;&#27979;&#29983;&#32946;&#29575;&#65288;PreFer&#65289;
&lt;/p&gt;
&lt;p&gt;
Combining the Strengths of Dutch Survey and Register Data in a Data Challenge to Predict Fertility (PreFer)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#22522;&#20110;&#33655;&#20848;&#30340;&#35843;&#26597;&#25968;&#25454;&#21644;&#30331;&#35760;&#25968;&#25454;&#65292;&#29992;&#20110;&#30740;&#31350;&#33655;&#20848;&#29983;&#32946;&#32467;&#26524;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#21644;&#26679;&#26412;&#65292;&#24182;&#25551;&#36848;&#20102;&#29983;&#32946;&#32467;&#26524;&#30340;&#20855;&#20307;&#20869;&#23481;&#12290;&#20182;&#20204;&#36824;&#20171;&#32461;&#20102;&#29983;&#32946;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#31185;&#23398;&#39046;&#22495;&#24050;&#32463;&#31215;&#32047;&#20102;&#22823;&#37327;&#26377;&#20851;&#29983;&#32946;&#32467;&#26524;&#30340;&#30740;&#31350;&#65292;&#21363;&#20154;&#20204;&#26159;&#21542;&#20197;&#21450;&#20309;&#26102;&#29983;&#32946;&#23376;&#22899;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20915;&#23450;&#22240;&#32032;&#21644;&#22522;&#26412;&#29702;&#35770;&#30340;&#39044;&#27979;&#33021;&#21147;&#24456;&#23569;&#22312;&#26032;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#26080;&#27861;&#31995;&#32479;&#22320;&#27604;&#36739;&#30740;&#31350;&#65292;&#38459;&#30861;&#20102;&#30693;&#35782;&#30340;&#35780;&#20272;&#21644;&#31215;&#32047;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#33655;&#20848;&#29983;&#32946;&#32467;&#26524;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;&#19968;&#20010;&#25968;&#25454;&#38598;&#22522;&#20110;LISS&#38754;&#26495;&#65292;&#36825;&#26159;&#19968;&#20010;&#32437;&#21521;&#35843;&#26597;&#65292;&#21253;&#25324;&#20102;&#25968;&#21315;&#20010;&#20851;&#20110;&#21508;&#31181;&#20027;&#39064;&#30340;&#21464;&#37327;&#65292;&#21253;&#25324;&#20010;&#20307;&#20559;&#22909;&#21644;&#20215;&#20540;&#35266;&#12290;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#22522;&#20110;&#33655;&#20848;&#30331;&#35760;&#25968;&#25454;&#65292;&#32570;&#20047;&#24577;&#24230;&#25968;&#25454;&#65292;&#20294;&#21253;&#25324;&#20102;&#25968;&#30334;&#19975;&#33655;&#20848;&#23621;&#27665;&#29983;&#27963;&#36712;&#36857;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20379;&#20851;&#20110;&#25968;&#25454;&#38598;&#21644;&#26679;&#26412;&#30340;&#20449;&#24687;&#65292;&#24182;&#25551;&#36848;&#24863;&#20852;&#36259;&#30340;&#29983;&#32946;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#29983;&#32946;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The social sciences have produced an impressive body of research on determinants of fertility outcomes, or whether and when people have children. However, the strength of these determinants and underlying theories are rarely evaluated on their predictive ability on new data. This prevents us from systematically comparing studies, hindering the evaluation and accumulation of knowledge. In this paper, we present two datasets which can be used to study the predictability of fertility outcomes in the Netherlands. One dataset is based on the LISS panel, a longitudinal survey which includes thousands of variables on a wide range of topics, including individual preferences and values. The other is based on the Dutch register data which lacks attitudinal data but includes detailed information about the life courses of millions of Dutch residents. We provide information about the datasets and the samples, and describe the fertility outcome of interest. We also introduce the fertility prediction
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#29420;&#31435;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#21644;&#29420;&#31435;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;MLP&#27169;&#22411;&#20197;&#21450;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.16427</link><description>&lt;p&gt;
&#29420;&#31435;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Learning to Embed Time Series Patches Independently
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16427
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29420;&#31435;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#21644;&#29420;&#31435;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;MLP&#27169;&#22411;&#20197;&#21450;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25513;&#30721;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#31574;&#30053;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#21551;&#21457;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#39318;&#20808;&#23558;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#22359;&#22788;&#29702;&#24182;&#37096;&#20998;&#25513;&#30422;&#65292;&#28982;&#21518;&#35757;&#32451;Transformer&#27169;&#22411;&#36890;&#36807;&#20174;&#26410;&#25513;&#30422;&#30340;&#22359;&#39044;&#27979;&#34987;&#25513;&#30422;&#22359;&#26469;&#25429;&#25417;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#25429;&#25417;&#36825;&#31181;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21487;&#33021;&#19981;&#26159;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#26368;&#20339;&#31574;&#30053;&#65307;&#30456;&#21453;&#65292;&#29420;&#31435;&#23398;&#20064;&#23884;&#20837;&#29255;&#27573;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;1&#65289;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#65292;&#33258;&#21160;&#23558;&#27599;&#20010;&#22359;&#36827;&#34892;&#32534;&#30721;&#32780;&#19981;&#26597;&#30475;&#20854;&#20182;&#22359;&#65292;&#20197;&#21450;2&#65289;&#29420;&#33258;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;&#31616;&#21333;&#22359;&#24335;MLP&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#26377;&#25928;&#22320;&#20998;&#23618;&#25429;&#33719;&#30456;&#37051;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16427v2 Announce Type: replace-cross  Abstract: Masked time series modeling has recently gained much attention as a self-supervised representation learning strategy for time series. Inspired by masked image modeling in computer vision, recent works first patchify and partially mask out time series, and then train Transformers to capture the dependencies between patches by predicting masked patches from unmasked patches. However, we argue that capturing such patch dependencies might not be an optimal strategy for time series representation learning; rather, learning to embed patches independently results in better time series representations. Specifically, we propose to use 1) the simple patch reconstruction task, which autoencode each patch without looking at other patches, and 2) the simple patch-wise MLP that embeds each patch independently. In addition, we introduce complementary contrastive learning to hierarchically capture adjacent time series information efficiently. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoftCLT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#36719;&#23545;&#27604;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#24573;&#30053;&#22266;&#26377;&#30456;&#20851;&#24615;&#25152;&#23548;&#33268;&#30340;&#23398;&#20064;&#34920;&#31034;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.16424</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#36719;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Soft Contrastive Learning for Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoftCLT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#36719;&#23545;&#27604;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#24573;&#30053;&#22266;&#26377;&#30456;&#20851;&#24615;&#25152;&#23548;&#33268;&#30340;&#23398;&#20064;&#34920;&#31034;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23545;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#23398;&#20064;&#34920;&#31034;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#20013;&#30456;&#20284;&#30340;&#23454;&#20363;&#25110;&#30456;&#37051;&#26102;&#38388;&#25139;&#30340;&#20540;&#36827;&#34892;&#23545;&#27604;&#20250;&#24573;&#30053;&#23427;&#20204;&#22266;&#26377;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SoftCLT&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#36719;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#12290;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#20174;&#38646;&#21040;&#19968;&#30340;&#36719;&#36171;&#20540;&#30340;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#23545;&#27604;&#25439;&#22833;&#26469;&#23454;&#29616;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20026;1)&#22522;&#20110;&#25968;&#25454;&#31354;&#38388;&#19978;&#30340;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#36317;&#31163;&#23450;&#20041;&#20102;&#23454;&#20363;&#32423;&#23545;&#27604;&#25439;&#22833;&#30340;&#36719;&#36171;&#20540;&#65292;&#24182;&#20026;2)&#22522;&#20110;&#26102;&#38388;&#25139;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#20102;&#26102;&#38388;&#32423;&#23545;&#27604;&#25439;&#22833;&#12290;SoftCLT&#26159;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#27809;&#26377;&#36807;&#22810;&#22797;&#26434;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16424v2 Announce Type: replace-cross  Abstract: Contrastive learning has shown to be effective to learn representations from time series in a self-supervised way. However, contrasting similar time series instances or values from adjacent timestamps within a time series leads to ignore their inherent correlations, which results in deteriorating the quality of learned representations. To address this issue, we propose SoftCLT, a simple yet effective soft contrastive learning strategy for time series. This is achieved by introducing instance-wise and temporal contrastive loss with soft assignments ranging from zero to one. Specifically, we define soft assignments for 1) instance-wise contrastive loss by the distance between time series on the data space, and 2) temporal contrastive loss by the difference of timestamps. SoftCLT is a plug-and-play method for time series contrastive learning that improves the quality of learned representations without bells and whistles. In experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#31232;&#30095;&#22343;&#22330;&#29702;&#35770;&#30340;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#23616;&#37096;&#25490;&#38431;&#32593;&#32476;&#20013;&#23398;&#20064;&#36817;&#20284;&#26368;&#20248;&#36127;&#36733;&#24179;&#34913;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#20026;&#31232;&#30095;&#26377;&#30028;&#24230;&#26080;&#32447;&#25299;&#25169;&#25552;&#20379;&#20102;&#36890;&#29992;&#30340;&#36127;&#36733;&#24179;&#34913;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2312.12973</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#23616;&#37096;&#25490;&#38431;&#31995;&#32479;&#20013;&#30340;&#31232;&#30095;&#22343;&#22330;&#36127;&#36733;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Sparse Mean Field Load Balancing in Large Localized Queueing Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#31232;&#30095;&#22343;&#22330;&#29702;&#35770;&#30340;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#23616;&#37096;&#25490;&#38431;&#32593;&#32476;&#20013;&#23398;&#20064;&#36817;&#20284;&#26368;&#20248;&#36127;&#36733;&#24179;&#34913;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#20026;&#31232;&#30095;&#26377;&#30028;&#24230;&#26080;&#32447;&#25299;&#25169;&#25552;&#20379;&#20102;&#36890;&#29992;&#30340;&#36127;&#36733;&#24179;&#34913;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#36127;&#36733;&#24179;&#34913;&#31639;&#27861;&#22312;&#20113;&#32593;&#32476;&#21644;&#25968;&#25454;&#20013;&#24515;&#20013;&#20855;&#26377;&#24456;&#22823;&#30340;&#20852;&#36259;&#65292;&#38656;&#35201;&#20351;&#29992;&#21487;&#22788;&#29702;&#30340;&#25216;&#26415;&#26469;&#35745;&#31639;&#33391;&#22909;&#24615;&#33021;&#30340;&#26368;&#20339;&#36127;&#36733;&#24179;&#34913;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21487;&#25193;&#23637;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#22343;&#22330;&#29702;&#35770;&#30340;&#28176;&#36817;&#32553;&#25918;&#26041;&#27861;&#65292;&#23578;&#26080;&#27861;&#24314;&#27169;&#20855;&#26377;&#24378;&#23616;&#37096;&#24615;&#30340;&#22823;&#22411;&#25490;&#38431;&#32593;&#32476;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19968;&#33324;&#30340;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#24456;&#38590;&#25193;&#23637;&#65292;&#24182;&#19988;&#36890;&#24120;&#32570;&#20047;&#29702;&#35770;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#22343;&#22330;&#29702;&#35770;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20197;&#21487;&#22788;&#29702;&#30340;&#26041;&#24335;&#23398;&#20064;&#31232;&#30095;&#36830;&#25509;&#25490;&#38431;&#32593;&#32476;&#20013;&#30340;&#36817;&#20284;&#26368;&#20248;&#36127;&#36733;&#24179;&#34913;&#31574;&#30053;&#65292;&#36825;&#30456;&#23545;&#20110;&#20840;&#23616;&#26041;&#27861;&#21487;&#33021;&#26356;&#21487;&#21462;&#65292;&#20174;&#26080;&#32447;&#36890;&#20449;&#24320;&#38144;&#30340;&#35282;&#24230;&#26469;&#30475;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#22823;&#31867;&#31232;&#30095;&#26377;&#30028;&#24230;&#26080;&#32447;&#25299;&#25169;&#30340;&#36890;&#29992;&#36127;&#36733;&#24179;&#34913;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12973v2 Announce Type: replace-cross  Abstract: Scalable load balancing algorithms are of great interest in cloud networks and data centers, necessitating the use of tractable techniques to compute optimal load balancing policies for good performance. However, most existing scalable techniques, especially asymptotically scaling methods based on mean field theory, have not been able to model large queueing networks with strong locality. Meanwhile, general multi-agent reinforcement learning techniques can be hard to scale and usually lack a theoretical foundation. In this work, we address this challenge by leveraging recent advances in sparse mean field theory to learn a near-optimal load balancing policy in sparsely connected queueing networks in a tractable manner, which may be preferable to global approaches in terms of wireless communication overhead. Importantly, we obtain a general load balancing framework for a large class of sparse bounded-degree wireless topologies. B
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;UniChest&#26694;&#26550;&#65292;&#37319;&#29992;&#24449;&#26381;&#21644;&#20998;&#21106;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#65292;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.11038</link><description>&lt;p&gt;
UniChest&#65306;&#24449;&#26381;&#20998;&#21106;&#39044;&#35757;&#32451;&#29992;&#20110;&#22810;&#28304;&#33016;&#37096;X&#23556;&#32447;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
UniChest: Conquer-and-Divide Pre-training for Multi-Source Chest X-Ray Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11038
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;UniChest&#26694;&#26550;&#65292;&#37319;&#29992;&#24449;&#26381;&#21644;&#20998;&#21106;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#65292;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision-Language Pre-training (VLP)&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#20419;&#36827;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#22312;&#33258;&#28982;&#39046;&#22495;&#35270;&#35273;&#35782;&#21035;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#65292;&#24182;&#22312;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#30340;&#21307;&#23398;&#24433;&#20687;&#35786;&#26029;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UniChest&#30340;Conquer-and-Divide&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;CXR&#26469;&#28304;&#30340;&#21512;&#20316;&#25928;&#30410;&#65292;&#21516;&#26102;&#20943;&#23569;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11038v2 Announce Type: replace-cross  Abstract: Vision-Language Pre-training (VLP) that utilizes the multi-modal information to promote the training efficiency and effectiveness, has achieved great success in vision recognition of natural domains and shown promise in medical imaging diagnosis for the Chest X-Rays (CXRs). However, current works mainly pay attention to the exploration on single dataset of CXRs, which locks the potential of this powerful paradigm on larger hybrid of multi-source CXRs datasets. We identify that although blending samples from the diverse sources offers the advantages to improve the model generalization, it is still challenging to maintain the consistent superiority for the task of each source due to the existing heterogeneity among sources. To handle this dilemma, we design a Conquer-and-Divide pre-training framework, termed as UniChest, aiming to make full use of the collaboration benefit of multiple sources of CXRs while reducing the negative i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#25918;&#26494;&#31561;&#21464;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#35299;&#20915;&#31561;&#21464;&#20989;&#25968;&#26080;&#27861;&#22312;&#21333;&#20010;&#25968;&#25454;&#26679;&#26412;&#23618;&#38754;&#25171;&#30772;&#23545;&#31216;&#30340;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#24212;&#29992;&#20110;&#31561;&#21464;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;E-MLP&#65289;&#20013;&#12290;</title><link>https://arxiv.org/abs/2312.09016</link><description>&lt;p&gt;
&#23545;&#31216;&#30772;&#32570;&#21644;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Symmetry Breaking and Equivariant Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09016
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#25918;&#26494;&#31561;&#21464;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#35299;&#20915;&#31561;&#21464;&#20989;&#25968;&#26080;&#27861;&#22312;&#21333;&#20010;&#25968;&#25454;&#26679;&#26412;&#23618;&#38754;&#25171;&#30772;&#23545;&#31216;&#30340;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#24212;&#29992;&#20110;&#31561;&#21464;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;E-MLP&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20351;&#29992;&#23545;&#31216;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35774;&#35745;&#20986;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#31216;&#21644;&#31561;&#21464;&#24615;&#30340;&#20851;&#31995;&#24182;&#19981;&#24635;&#26159;&#26174;&#32780;&#26131;&#35265;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#31561;&#21464;&#20989;&#25968;&#20013;&#20986;&#29616;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#23427;&#20204;&#19981;&#33021;&#22312;&#21333;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#23618;&#38754;&#25171;&#30772;&#23545;&#31216;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#8220;&#25918;&#26494;&#31561;&#21464;&#24615;&#8221;&#30340;&#27010;&#24565;&#26469;&#35268;&#36991;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#31181;&#25918;&#26494;&#24341;&#20837;&#31561;&#21464;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;E-MLP&#65289;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#27880;&#20837;&#22122;&#22768;&#30340;&#26041;&#27861;&#12290;&#25509;&#30528;&#35752;&#35770;&#20102;&#23545;&#31216;&#30772;&#32570;&#22312;&#29289;&#29702;&#12289;&#22270;&#34920;&#31034;&#23398;&#20064;&#12289;&#32452;&#21512;&#20248;&#21270;&#21644;&#31561;&#21464;&#35299;&#30721;&#31561;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09016v2 Announce Type: replace  Abstract: Using symmetry as an inductive bias in deep learning has been proven to be a principled approach for sample-efficient model design. However, the relationship between symmetry and the imperative for equivariance in neural networks is not always obvious. Here, we analyze a key limitation that arises in equivariant functions: their incapacity to break symmetry at the level of individual data samples. In response, we introduce a novel notion of 'relaxed equivariance' that circumvents this limitation. We further demonstrate how to incorporate this relaxation into equivariant multilayer perceptrons (E-MLPs), offering an alternative to the noise-injection method. The relevance of symmetry breaking is then discussed in various application domains: physics, graph representation learning, combinatorial optimization and equivariant decoding.
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#29992;&#25143;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#20010;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#36825;&#20123;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#21306;&#20998;&#22270;&#20687;&#23545;&#24182;&#33258;&#21160;&#26631;&#27880;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2312.03187</link><description>&lt;p&gt;
FERGI&#65306;&#26469;&#33258;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#29992;&#25143;&#20559;&#22909;&#30340;&#33258;&#21160;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03187
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#29992;&#25143;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#20010;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#36825;&#20123;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#21306;&#20998;&#22270;&#20687;&#23545;&#24182;&#33258;&#21160;&#26631;&#27880;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20351;&#29992;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#25968;&#25454;&#26469;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#20154;&#31867;&#21453;&#39304;&#25910;&#38598;&#30340;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20174;&#29992;&#25143;&#30340;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#20854;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#21040;&#29983;&#25104;&#22270;&#20687;&#65288;FERGI&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#20010;&#38754;&#37096;&#36816;&#21160;&#21333;&#20803;&#65288;AUs&#65289;&#30340;&#28608;&#27963;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AU4&#65288;&#30473;&#27611;&#19979;&#22402;&#32773;&#65289;&#21453;&#26144;&#20102;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#36127;&#38754;&#35780;&#20215;&#65292;&#32780;AU12&#65288;&#22068;&#35282;&#25289;&#21160;&#32773;&#65289;&#21453;&#26144;&#20102;&#27491;&#38754;&#35780;&#20215;&#12290;&#36825;&#20004;&#32773;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#24456;&#26377;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#22320;&#20351;&#29992;&#36825;&#20123;AU&#21709;&#24212;&#23384;&#22312;&#23454;&#36136;&#24046;&#24322;&#30340;&#22270;&#20687;&#23545;&#20043;&#38388;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03187v2 Announce Type: replace-cross  Abstract: Researchers have proposed to use data of human preference feedback to fine-tune text-to-image generative models. However, the scalability of human feedback collection has been limited by its reliance on manual annotation. Therefore, we develop and test a method to automatically annotate user preferences from their spontaneous facial expression reaction to the generated images. We collect a dataset of Facial Expression Reaction to Generated Images (FERGI) and show that the activations of multiple facial action units (AUs) are highly correlated with user evaluations of the generated images. Specifically, AU4 (brow lowerer) is reflective of negative evaluations of the generated image whereas AU12 (lip corner puller) is reflective of positive evaluations. These can be useful in two ways. Firstly, we can automatically annotate user preferences between image pairs with substantial difference in these AU responses with an accuracy sig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21040;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#20854;&#24120;&#35782;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20316;&#20026;&#26234;&#33021;&#20915;&#31574;&#32773;&#26469;&#22686;&#24378;&#39550;&#39542;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.00812</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#33258;&#21160;&#39550;&#39542;&#65306;&#19968;&#20010;&#23433;&#20840;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Empowering Autonomous Driving with Large Language Models: A Safety Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21040;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#20854;&#24120;&#35782;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20316;&#20026;&#26234;&#33021;&#20915;&#31574;&#32773;&#26469;&#22686;&#24378;&#39550;&#39542;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#65288;AD&#65289;&#22312;&#38271;&#23614;&#26410;&#30693;&#39550;&#39542;&#22330;&#26223;&#20013;&#36935;&#21040;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#38556;&#30861;&#65292;&#20027;&#35201;&#28304;&#33258;AD&#31995;&#32479;&#20869;&#37096;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#24067;&#22806;&#21644;&#19981;&#30830;&#23450;&#25968;&#25454;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25972;&#21512;&#21040;AD&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#23427;&#20204;&#24378;&#22823;&#30340;&#24120;&#35782;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;LLM&#29992;&#20316;&#34892;&#20026;&#35268;&#21010;&#20013;&#30340;&#26234;&#33021;&#20915;&#31574;&#32773;&#65292;&#37197;&#22791;&#19968;&#20010;&#23433;&#20840;&#39564;&#35777;&#22120;&#25252;&#30462;&#36827;&#34892;&#19978;&#19979;&#25991;&#23433;&#20840;&#23398;&#20064;&#65292;&#20197;&#22686;&#24378;&#39550;&#39542;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20004;&#20010;&#20851;&#38190;&#30740;&#31350;&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;LLM&#35843;&#33410;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#21644;&#19968;&#31181;&#24102;&#26377;&#29366;&#24577;&#26426;&#30340;LLM&#21551;&#29992;&#20132;&#20114;&#24335;&#34892;&#20026;&#35268;&#21010;&#26041;&#26696;&#12290;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24230;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00812v4 Announce Type: replace  Abstract: Autonomous Driving (AD) encounters significant safety hurdles in long-tail unforeseen driving scenarios, largely stemming from the non-interpretability and poor generalization of the deep neural networks within the AD system, particularly in out-of-distribution and uncertain data. To this end, this paper explores the integration of Large Language Models (LLMs) into AD systems, leveraging their robust common-sense knowledge and reasoning abilities. The proposed methodologies employ LLMs as intelligent decision-makers in behavioral planning, augmented with a safety verifier shield for contextual safety learning, for enhancing driving performance and safety. We present two key studies in a simulated environment: an adaptive LLM-conditioned Model Predictive Control (MPC) and an LLM-enabled interactive behavior planning scheme with a state machine. Demonstrating superior performance and safety metrics compared to state-of-the-art approach
&lt;/p&gt;</description></item><item><title>&#36890;&#29992;&#26234;&#33021;&#20307;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26159;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#24230;&#36229;&#21442;&#25968;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21892;&#20840;&#23616;&#24615;&#33021;&#24182;&#19982;&#25163;&#24037;&#35774;&#35745;&#21551;&#21457;&#24335;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;</title><link>https://arxiv.org/abs/2311.18598</link><description>&lt;p&gt;
&#36890;&#29992;&#26234;&#33021;&#20307;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalisable Agents for Neural Network Optimisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18598
&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#26234;&#33021;&#20307;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26159;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#24230;&#36229;&#21442;&#25968;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21892;&#20840;&#23616;&#24615;&#33021;&#24182;&#19982;&#25163;&#24037;&#35774;&#35745;&#21551;&#21457;&#24335;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21407;&#22240;&#22312;&#20110;&#22797;&#26434;&#30340;&#35757;&#32451;&#21160;&#24577;&#12289;&#39640;&#35745;&#31639;&#35201;&#27714;&#21644;&#38271;&#26102;&#38388;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#26234;&#33021;&#20307;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#65288;GANNO&#65289;&#30340;&#26694;&#26550;--&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21644;&#21709;&#24212;&#24335;&#22320;&#35843;&#24230;&#36229;&#21442;&#25968;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;GANNO&#21033;&#29992;&#27599;&#23618;&#19968;&#20010;&#26234;&#33021;&#20307;&#35266;&#23519;&#23616;&#37096;&#21270;&#30340;&#32593;&#32476;&#21160;&#24577;&#65292;&#24182;&#30456;&#24212;&#22320;&#37319;&#21462;&#34892;&#21160;&#26469;&#35843;&#25972;&#36825;&#20123;&#21160;&#24577;&#65292;&#20174;&#32780;&#22312;&#23618;&#32423;&#19978;&#38598;&#20307;&#25913;&#21892;&#20840;&#23616;&#24615;&#33021;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;GANNO&#26469;&#25511;&#21046;&#23618;&#32423;&#23398;&#20064;&#29575;&#65292;&#24182;&#23637;&#31034;&#35813;&#26694;&#26550;&#21487;&#20197;&#20135;&#29983;&#26377;&#29992;&#19988;&#21709;&#24212;&#28789;&#27963;&#30340;&#35843;&#24230;&#65292;&#19982;&#25163;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;&#27492;&#22806;&#65292;&#26174;&#31034;GANNO&#22312;&#21508;&#31181;&#30475;&#19981;&#35265;&#30340;&#21021;&#22987;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#25104;&#21151;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18598v2 Announce Type: replace-cross  Abstract: Optimising deep neural networks is a challenging task due to complex training dynamics, high computational requirements, and long training times. To address this difficulty, we propose the framework of Generalisable Agents for Neural Network Optimisation (GANNO) -- a multi-agent reinforcement learning (MARL) approach that learns to improve neural network optimisation by dynamically and responsively scheduling hyperparameters during training. GANNO utilises an agent per layer that observes localised network dynamics and accordingly takes actions to adjust these dynamics at a layerwise level to collectively improve global performance. In this paper, we use GANNO to control the layerwise learning rate and show that the framework can yield useful and responsive schedules that are competitive with handcrafted heuristics. Furthermore, GANNO is shown to perform robustly across a wide variety of unseen initial conditions, and can succe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#37327;&#23376;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#65288;QLD&#65289;&#26469;&#35299;&#20915;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#20984;&#26223;&#35266;&#20013; QLD &#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#33021;&#37327;&#32791;&#25955;&#33021;&#21147;&#21644;&#20302;&#28201;&#26497;&#38480;&#19979;&#25351;&#25968;&#34928;&#20943;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.15587</link><description>&lt;p&gt;
&#37327;&#23376;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#29992;&#20110;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Quantum Langevin Dynamics for Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15587
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#37327;&#23376;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#65288;QLD&#65289;&#26469;&#35299;&#20915;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#20984;&#26223;&#35266;&#20013; QLD &#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#33021;&#37327;&#32791;&#25955;&#33021;&#21147;&#21644;&#20302;&#28201;&#26497;&#38480;&#19979;&#25351;&#25968;&#34928;&#20943;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#21033;&#29992;&#37327;&#23376;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#65288;QLD&#65289;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#23545;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20135;&#29983;&#37325;&#22823;&#38556;&#30861;&#30340;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#26080;&#38480;&#28909;&#28020;&#32806;&#21512;&#30340;&#31995;&#32479;&#21160;&#21147;&#23398;&#12290;&#35813;&#30456;&#20114;&#20316;&#29992;&#26082;&#24341;&#36215;&#20102;&#38543;&#26426;&#37327;&#23376;&#22122;&#22768;&#65292;&#21448;&#24341;&#36215;&#20102;&#23545;&#31995;&#32479;&#30340;&#30830;&#23450;&#24615;&#38459;&#23612;&#25928;&#24212;&#65292;&#20174;&#32780;&#23558;&#31995;&#32479;&#25512;&#21521;&#25509;&#36817;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#31283;&#23450;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20984;&#26223;&#35266;&#20013;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102; QLD &#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#31995;&#32479;&#30340;&#24179;&#22343;&#33021;&#37327;&#21487;&#20197;&#22312;&#20302;&#28201;&#26497;&#38480;&#19979;&#25509;&#36817;&#38646;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#28436;&#21270;&#26102;&#38388;&#30456;&#20851;&#30340;&#25351;&#25968;&#34928;&#20943;&#36895;&#29575;&#12290;&#22312;&#25968;&#20540;&#19978;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23558;&#20854;&#36215;&#28304;&#36861;&#28335;&#21040;&#33258;&#21457;&#36752;&#23556;&#26469;&#23637;&#31034; QLD &#30340;&#33021;&#37327;&#32791;&#25955;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010; p &#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35814;&#32454;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15587v2 Announce Type: replace-cross  Abstract: We initiate the study of utilizing Quantum Langevin Dynamics (QLD) to solve optimization problems, particularly those non-convex objective functions that present substantial obstacles for traditional gradient descent algorithms. Specifically, we examine the dynamics of a system coupled with an infinite heat bath. This interaction induces both random quantum noise and a deterministic damping effect to the system, which nudge the system towards a steady state that hovers near the global minimum of objective functions. We theoretically prove the convergence of QLD in convex landscapes, demonstrating that the average energy of the system can approach zero in the low temperature limit with an exponential decay rate correlated with the evolution time. Numerically, we first show the energy dissipation capability of QLD by retracing its origins to spontaneous emission. Furthermore, we conduct detailed discussion of the impact of each p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;CLIP&#35782;&#21035;ID&#26679;&#24335;&#24322;&#24120;&#20540;&#24182;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#36827;&#34892;OOD&#26816;&#27979;&#30340;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#35782;&#21035;&#26368;&#20855;&#25361;&#25112;&#24615;OOD&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.15243</link><description>&lt;p&gt;
ID&#26679;&#24335;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ID-like Prompt Learning for Few-Shot Out-of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15243
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;CLIP&#35782;&#21035;ID&#26679;&#24335;&#24322;&#24120;&#20540;&#24182;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#36827;&#34892;OOD&#26816;&#27979;&#30340;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#35782;&#21035;&#26368;&#20855;&#25361;&#25112;&#24615;OOD&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#36741;&#21161;&#24322;&#24120;&#20540;&#26469;&#35757;&#32451;&#27169;&#22411;&#35782;&#21035;&#24322;&#24120;&#26679;&#26412;&#65292;&#23588;&#20854;&#26159;&#20174;&#36741;&#21161;&#24322;&#24120;&#20540;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24322;&#24120;&#20540;&#20197;&#25913;&#21892;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#20173;&#38754;&#20020;&#26377;&#25928;&#21306;&#20998;&#19982;ID&#25968;&#25454;&#38750;&#24120;&#30456;&#20284;&#30340;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;OOD&#26679;&#26412;&#30340;&#23616;&#38480;&#24615;&#65292;&#21363;ID&#26679;&#24335;&#26679;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;OOD&#26816;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;CLIP&#20174;ID&#26679;&#26412;&#30340;&#37051;&#36817;&#31354;&#38388;&#20013;&#21457;&#29616;ID&#26679;&#24335;&#30340;&#24322;&#24120;&#20540;&#65292;&#20174;&#32780;&#24110;&#21161;&#35782;&#21035;&#36825;&#20123;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;OOD&#26679;&#26412;&#12290;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#35782;&#21035;&#30340;ID&#26679;&#24335;&#24322;&#24120;&#20540;&#36827;&#19968;&#27493;&#21033;&#29992;CLIP&#30340;&#33021;&#21147;&#36827;&#34892;OOD&#26816;&#27979;&#12290;&#21463;&#30410;&#20110;&#24378;&#22823;&#30340;CLIP&#65292;&#25105;&#20204;&#21482;&#38656;&#35201;&#23569;&#37327;ID&#26679;&#26412;&#21363;&#21487;&#23398;&#20064;&#27169;&#22411;&#30340;&#25552;&#31034;&#65292;&#32780;&#26080;&#38656;&#26292;&#38706;&#20854;&#20182;&#36741;&#21161;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15243v3 Announce Type: replace-cross  Abstract: Out-of-distribution (OOD) detection methods often exploit auxiliary outliers to train model identifying OOD samples, especially discovering challenging outliers from auxiliary outliers dataset to improve OOD detection. However, they may still face limitations in effectively distinguishing between the most challenging OOD samples that are much like in-distribution (ID) data, i.e., \idlike samples. To this end, we propose a novel OOD detection framework that discovers \idlike outliers using CLIP \cite{DBLP:conf/icml/RadfordKHRGASAM21} from the vicinity space of the ID samples, thus helping to identify these most challenging OOD samples. Then a prompt learning framework is proposed that utilizes the identified \idlike outliers to further leverage the capabilities of CLIP for OOD detection. Benefiting from the powerful CLIP, we only need a small number of ID samples to learn the prompts of the model without exposing other auxiliary
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27491;&#35268;&#21270;&#27969;&#29983;&#25104;&#26085;&#21069;&#30005;&#21147;&#20215;&#26684;&#30340;&#22810;&#20803;&#22330;&#26223;&#29983;&#25104;&#65292;&#24182;&#25552;&#20986;&#25193;&#23637;&#29305;&#24449;&#38598;&#21644;&#37325;&#26032;&#35757;&#32451;&#26041;&#26696;&#26469;&#36866;&#24212;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#30340;&#21464;&#21270;&#26465;&#20214;</title><link>https://arxiv.org/abs/2311.14033</link><description>&lt;p&gt;
&#20351;&#29992;&#27491;&#35268;&#21270;&#27969;&#29983;&#25104;&#26085;&#21069;&#30005;&#21147;&#20215;&#26684;&#30340;&#22810;&#20803;&#22330;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multivariate Scenario Generation of Day-Ahead Electricity Prices using Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14033
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27491;&#35268;&#21270;&#27969;&#29983;&#25104;&#26085;&#21069;&#30005;&#21147;&#20215;&#26684;&#30340;&#22810;&#20803;&#22330;&#26223;&#29983;&#25104;&#65292;&#24182;&#25552;&#20986;&#25193;&#23637;&#29305;&#24449;&#38598;&#21644;&#37325;&#26032;&#35757;&#32451;&#26041;&#26696;&#26469;&#36866;&#24212;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#30340;&#21464;&#21270;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#21069;&#30005;&#21147;&#24066;&#22330;&#20132;&#26131;&#20013;&#65292;&#38656;&#35201;&#20934;&#30830;&#30340;&#20851;&#20110;&#30005;&#21147;&#20215;&#26684;&#23454;&#29616;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20449;&#24687;&#12290;&#30001;&#20110;&#26085;&#21069;&#20215;&#26684;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#23548;&#33268;&#20102;&#30001;2021&#24180;&#33021;&#28304;&#21361;&#26426;&#24341;&#36215;&#30340;&#24066;&#22330;&#24773;&#20917;&#21464;&#21270;&#31561;&#21407;&#22240;&#65292;&#20934;&#30830;&#24314;&#31435;&#39044;&#27979;&#27169;&#22411;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#27491;&#35268;&#21270;&#27969;&#26469;&#36827;&#34892;&#26085;&#21069;&#30005;&#21147;&#20215;&#26684;&#27010;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24314;&#27169;&#26041;&#27861;&#22522;&#20110;&#26465;&#20214;&#29305;&#24449;&#65288;&#22914;&#21097;&#20313;&#36127;&#33655;&#39044;&#27979;&#65289;&#29983;&#25104;&#26085;&#21069;&#30005;&#21147;&#20215;&#26684;&#30340;&#20840;&#22825;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20808;&#21069;&#23454;&#29616;&#30340;&#25193;&#23637;&#29305;&#24449;&#38598;&#21644;&#21608;&#26399;&#24615;&#37325;&#26032;&#35757;&#32451;&#26041;&#26696;&#65292;&#20351;&#27491;&#21017;&#21270;&#27969;&#33021;&#22815;&#36866;&#24212;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#30340;&#21464;&#21270;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#34920;&#26126;&#65292;&#27491;&#35268;&#21270;&#27969;&#29983;&#25104;&#20102;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14033v2 Announce Type: replace  Abstract: Trading on the day-ahead electricity markets requires accurate information about the realization of electricity prices and the uncertainty attached to the predictions. Deriving accurate forecasting models presents a difficult task due to the day-ahead price's non-stationarity resulting from changing market conditions, e.g., due to changes resulting from the energy crisis in 2021. We present a probabilistic forecasting approach for day-ahead electricity prices using the fully data-driven deep generative model called normalizing flow. Our modeling approach generates full-day scenarios of day-ahead electricity prices based on conditional features such as residual load forecasts. Furthermore, we propose extended feature sets of prior realizations and a periodic retraining scheme that allows the normalizing flow to adapt to the changing conditions of modern electricity markets. Our results highlight that the normalizing flow generates hig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#20445;&#30495;&#31070;&#32463;&#32593;&#32476;(MFNN)&#35299;&#20915;&#20809;&#23398;&#22270;&#20687;&#19982;&#23454;&#38469;&#26426;&#26800;&#24615;&#33021;&#20043;&#38388;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.10278</link><description>&lt;p&gt;
&#29289;&#29702;&#22686;&#24378;&#30340;&#20809;&#23398;&#34920;&#38754;&#21360;&#36857;&#22810;&#20445;&#30495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#20445;&#30495;&#31070;&#32463;&#32593;&#32476;(MFNN)&#35299;&#20915;&#20809;&#23398;&#22270;&#20687;&#19982;&#23454;&#38469;&#26426;&#26800;&#24615;&#33021;&#20043;&#38388;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25351;&#32441;&#20316;&#20026;&#27599;&#20010;&#20154;&#30340;&#29420;&#29305;&#32780;&#24378;&#22823;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#24110;&#21161;&#35686;&#23519;&#35782;&#21035;&#36523;&#20221;&#12290;&#31867;&#20284;&#22320;&#65292;&#35768;&#22810;&#33258;&#28982;&#20307;&#21644;&#20869;&#22312;&#26426;&#26800;&#29305;&#24615;&#20063;&#21487;&#20197;&#36890;&#36807;&#34920;&#38754;&#29305;&#24449;&#24471;&#21040;&#21807;&#19968;&#35782;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20445;&#30495;&#31070;&#32463;&#32593;&#32476;(MFNN)&#35299;&#20915;&#36825;&#20010;&#21453;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10278v2 Announce Type: replace-cross  Abstract: Human fingerprints serve as one unique and powerful characteristic for each person, from which policemen can recognize the identity. Similar to humans, many natural bodies and intrinsic mechanical qualities can also be uniquely identified from surface characteristics. To measure the elasto-plastic properties of one material, one formally sharp indenter is pushed into the measured body under constant force and retracted, leaving a unique residual imprint of the minute size from several micrometers to nanometers. However, one great challenge is how to map the optical image of this residual imprint into the real wanted mechanical properties, \ie, the tensile force curve. In this paper, we propose a novel method to use multi-fidelity neural networks (MFNN) to solve this inverse problem. We first build up the NN model via pure simulation data, and then bridge the sim-to-real gap via transfer learning. Considering the difficulty of c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;&#65288;MHSNs&#65289;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#21644;&#21367;&#31215;&#32467;&#26500;&#65292;&#29983;&#25104;&#23545;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2311.10270</link><description>&lt;p&gt;
&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#30340;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multiscale Hodge Scattering Networks for Data Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;&#65288;MHSNs&#65289;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#21644;&#21367;&#31215;&#32467;&#26500;&#65292;&#29983;&#25104;&#23545;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25955;&#23556;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#19978;&#27979;&#37327;&#30340;&#20449;&#21495;&#65292;&#31216;&#20026;\emph{&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;}&#65288;MHSNs&#65289;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#22522;&#20110;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#19978;&#30340;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#65292;&#21363;$\kappa$-GHWT&#21644;$\kappa$-HGLET&#65292;&#25105;&#20204;&#26368;&#36817;&#20026;&#32473;&#23450;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#20013;&#30340;&#32500;&#24230;$\kappa \in \mathbb{N}$&#25512;&#24191;&#20102;&#22522;&#20110;&#33410;&#28857;&#30340;&#24191;&#20041;&#21704;-&#27779;&#20160;&#21464;&#25442;&#65288;GHWT&#65289;&#21644;&#20998;&#23618;&#22270;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21464;&#25442;&#65288;HGLET&#65289;&#12290;$\kappa$-GHWT&#21644;$\kappa$-HGLET&#37117;&#24418;&#25104;&#20887;&#20313;&#38598;&#21512;&#65288;&#21363;&#35789;&#20856;&#65289;&#30340;&#22810;&#23610;&#24230;&#22522;&#30784;&#21521;&#37327;&#21644;&#32473;&#23450;&#20449;&#21495;&#30340;&#30456;&#24212;&#25193;&#23637;&#31995;&#25968;&#12290;&#25105;&#20204;&#30340;MHSNs&#20351;&#29992;&#31867;&#20284;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20998;&#23618;&#32467;&#26500;&#26469;&#32423;&#32852;&#35789;&#20856;&#31995;&#25968;&#27169;&#30340;&#30697;&#12290;&#25152;&#24471;&#29305;&#24449;&#23545;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#30340;&#37325;&#26032;&#25490;&#24207;&#19981;&#21464;&#65288;&#21363;&#33410;&#28857;&#25490;&#21015;&#30340;&#32622;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10270v2 Announce Type: replace  Abstract: We propose new scattering networks for signals measured on simplicial complexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs). Our construction is based on multiscale basis dictionaries on simplicial complexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recently developed for simplices of dimension $\kappa \in \mathbb{N}$ in a given simplicial complex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT) and Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\kappa$-GHWT and the $\kappa$-HGLET both form redundant sets (i.e., dictionaries) of multiscale basis vectors and the corresponding expansion coefficients of a given signal. Our MHSNs use a layered structure analogous to a convolutional neural network (CNN) to cascade the moments of the modulus of the dictionary coefficients. The resulting features are invariant to reordering of the simplices (i.e., node permutation of the u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Multi-resolution Time-Series Transformer&#26694;&#26550;&#65292;&#37319;&#29992;&#22810;&#20998;&#25903;&#26550;&#26500;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65292;&#29992;&#20110;&#21516;&#26102;&#24314;&#27169;&#19981;&#21516;&#20998;&#36776;&#29575;&#19979;&#30340;&#22810;&#26679;&#21270;&#26102;&#38388;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2311.04147</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#26102;&#38388;&#24207;&#21015;Transformer&#29992;&#20110;&#38271;&#26399;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-resolution Time-Series Transformer for Long-term Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04147
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Multi-resolution Time-Series Transformer&#26694;&#26550;&#65292;&#37319;&#29992;&#22810;&#20998;&#25903;&#26550;&#26500;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65292;&#29992;&#20110;&#21516;&#26102;&#24314;&#27169;&#19981;&#21516;&#20998;&#36776;&#29575;&#19979;&#30340;&#22810;&#26679;&#21270;&#26102;&#38388;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;transformers&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#12290;&#26368;&#36817;&#30340;&#26550;&#26500;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#20026;&#29255;&#27573;&#24182;&#23558;&#36825;&#20123;&#29255;&#27573;&#29992;&#20316;&#26631;&#35760;&#26469;&#23398;&#20064;&#22797;&#26434;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#29255;&#27573;&#22823;&#23567;&#25511;&#21046;&#20102;transformers&#23398;&#20064;&#19981;&#21516;&#39057;&#29575;&#30340;&#26102;&#38388;&#27169;&#24335;&#30340;&#33021;&#21147;&#65306;&#36739;&#30701;&#30340;&#29255;&#27573;&#36866;&#29992;&#20110;&#23398;&#20064;&#23616;&#37096;&#30340;&#39640;&#39057;&#27169;&#24335;&#65292;&#32780;&#25366;&#25496;&#38271;&#26399;&#30340;&#23395;&#33410;&#24615;&#21644;&#36235;&#21183;&#21017;&#38656;&#35201;&#36739;&#38271;&#30340;&#29255;&#27573;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#22810;&#20998;&#36776;&#29575;&#26102;&#38388;&#24207;&#21015;Transformer&#65288;MTST&#65289;&#65292;&#23427;&#30001;&#22810;&#20998;&#25903;&#26550;&#26500;&#32452;&#25104;&#65292;&#29992;&#20110;&#21516;&#26102;&#24314;&#27169;&#19981;&#21516;&#20998;&#36776;&#29575;&#19979;&#30340;&#22810;&#26679;&#21270;&#26102;&#38388;&#27169;&#24335;&#12290;&#19982;&#35768;&#22810;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;transformers&#19981;&#21516;&#65292;&#25105;&#20204;&#37319;&#29992;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65292;&#26356;&#36866;&#21512;&#25552;&#21462;&#19981;&#21516;&#23610;&#24230;&#19978;&#30340;&#21608;&#26399;&#25104;&#20998;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04147v2 Announce Type: replace  Abstract: The performance of transformers for time-series forecasting has improved significantly. Recent architectures learn complex temporal patterns by segmenting a time-series into patches and using the patches as tokens. The patch size controls the ability of transformers to learn the temporal patterns at different frequencies: shorter patches are effective for learning localized, high-frequency patterns, whereas mining long-term seasonalities and trends requires longer patches. Inspired by this observation, we propose a novel framework, Multi-resolution Time-Series Transformer (MTST), which consists of a multi-branch architecture for simultaneous modeling of diverse temporal patterns at different resolutions. In contrast to many existing time-series transformers, we employ relative positional encoding, which is better suited for extracting periodic components at different scales. Extensive experiments on several real-world datasets demons
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#23558;&#20449;&#24687;&#29109;&#24341;&#20837;&#21098;&#26525;&#24230;&#37327;&#35774;&#35745;&#65292;&#25552;&#39640;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013; N:M &#31232;&#30095;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.15929</link><description>&lt;p&gt;
E-Sparse: &#36890;&#36807;&#22522;&#20110;&#20449;&#24687;&#29109;&#30340; N:M &#31232;&#30095;&#24615;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.15929
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#23558;&#20449;&#24687;&#29109;&#24341;&#20837;&#21098;&#26525;&#24230;&#37327;&#35774;&#35745;&#65292;&#25552;&#39640;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013; N:M &#31232;&#30095;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#21098;&#26525;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24456;&#38590;&#23454;&#29616;&#65292;&#22240;&#20026;&#23427;&#20204;&#35757;&#32451;&#36807;&#31243;&#26114;&#36149;&#65292;&#35745;&#31639;&#38656;&#27714;&#22823;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#38544;&#34255;&#29366;&#24577;&#29305;&#24449;&#30340;&#20449;&#24687;&#29109;&#24341;&#20837;&#21040;&#21098;&#26525;&#24230;&#37327;&#35774;&#35745;&#20013;&#65292;&#21363; E-Sparse&#65292;&#20197;&#25552;&#39640;LLM&#20013; N:M &#31232;&#30095;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;E-Sparse&#21033;&#29992;&#20449;&#24687;&#20016;&#23500;&#24615;&#26469;&#25552;&#21319;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#32467;&#21512;&#20960;&#31181;&#26032;&#39062;&#25216;&#26415;&#26469;&#23454;&#29616;&#65306;(1)&#24341;&#20837;&#20449;&#24687;&#29109;&#26469;&#22686;&#24378;&#21442;&#25968;&#26435;&#37325;&#21644;&#36755;&#20837;&#29305;&#24449;&#33539;&#25968;&#30340;&#37325;&#35201;&#24615;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#21098;&#26525;&#24230;&#37327;&#65292;&#24182;&#22312;&#19981;&#20462;&#25913;&#21097;&#20313;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;N:M&#31232;&#30095;&#24615;&#12290;(2)&#35774;&#35745;&#20840;&#23616;&#26420;&#32032;&#27927;&#29260;&#21644;&#23616;&#37096;&#22359;&#27927;&#29260;&#65292;&#24555;&#36895;&#20248;&#21270;&#20449;&#24687;&#20998;&#24067;&#65292;&#20805;&#20998;&#24212;&#23545; N:M &#31232;&#30095;&#24615;&#23545;LLMs&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;E-Sparse &#34987;&#23454;&#29616;&#20026;&#19968;&#31181; Spars
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.15929v2 Announce Type: replace-cross  Abstract: Traditional pruning methods are known to be challenging to work in Large Language Models (LLMs) for Generative AI because of their unaffordable training process and large computational demands. For the first time, we introduce the information entropy of hidden state features into a pruning metric design, namely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse employs the information richness to leverage the channel importance, and further incorporates several novel techniques to put it into effect: (1) it introduces information entropy to enhance the significance of parameter weights and input feature norms as a novel pruning metric, and performs N:M sparsity without modifying the remaining weights. (2) it designs global naive shuffle and local block shuffle to quickly optimize the information distribution and adequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is implemented as a Spars
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21270;&#39537;&#21160;&#21644;&#26080;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#20351;&#29992;Youla-Kucera&#21442;&#25968;&#21270;&#25552;&#20379;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#20026;&#35774;&#35745;&#21453;&#39304;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2310.14098</link><description>&lt;p&gt;
&#31283;&#23450;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#65306;&#29992;&#20110;&#20248;&#21270;&#25152;&#26377;&#31283;&#23450;&#34892;&#20026;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Stabilizing reinforcement learning control: A modular framework for optimizing over all stable behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14098
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21270;&#39537;&#21160;&#21644;&#26080;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#20351;&#29992;Youla-Kucera&#21442;&#25968;&#21270;&#25552;&#20379;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#20026;&#35774;&#35745;&#21453;&#39304;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35774;&#35745;&#21453;&#39304;&#25511;&#21046;&#22120;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21270;&#39537;&#21160;&#21644;&#26080;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#20351;&#29992;Youla-Kucera&#21442;&#25968;&#21270;&#25552;&#20379;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#26469;&#23450;&#20041;&#25628;&#32034;&#22495;&#12290;&#26368;&#36817;&#22312;&#34892;&#20026;&#31995;&#32479;&#26041;&#38754;&#30340;&#36827;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#20869;&#37096;&#27169;&#22411;&#65307;&#36825;&#20351;&#24471;&#22312;&#23436;&#20840;&#22522;&#20110;&#36755;&#20837;-&#36755;&#20986;&#25506;&#32034;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;Youla-Kucera&#21442;&#25968;&#21270;&#30340;&#26367;&#20195;&#24615;&#23454;&#29616;&#25104;&#20026;&#21487;&#33021;&#12290;&#25110;&#35768;&#26356;&#20540;&#24471;&#20851;&#27880;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#21046;&#23450;&#21644;&#20998;&#26512;&#20102;&#36825;&#31867;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;Youla-Kucera&#26041;&#27861;&#23545;&#20110;&#25511;&#21046;&#22120;&#35774;&#35745;&#38656;&#35201;&#19968;&#20010;&#31283;&#23450;&#30340;&#8220;&#21442;&#25968;&#8221;&#12290;&#20026;&#20102;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#25152;&#26377;&#31283;&#23450;&#32447;&#24615;&#36816;&#31639;&#31526;&#30340;&#38598;&#21512;&#36890;&#36807;&#30697;&#38453;&#22240;&#23376;&#21270;&#26041;&#27861;&#34987;&#26126;&#30830;&#32473;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#38750;&#32447;&#24615;&#25193;&#23637;&#65292;&#20197;&#34920;&#36798;&#21442;&#25968;&#21270;&#30340;&#19968;&#32452;&#31283;&#23450;&#36816;&#31639;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14098v2 Announce Type: replace-cross  Abstract: We propose a framework for the design of feedback controllers that combines the optimization-driven and model-free advantages of deep reinforcement learning with the stability guarantees provided by using the Youla-Kucera parameterization to define the search domain. Recent advances in behavioral systems allow us to construct a data-driven internal model; this enables an alternative realization of the Youla-Kucera parameterization based entirely on input-output exploration data. Perhaps of independent interest, we formulate and analyze the stability of such data-driven models in the presence of noise. The Youla-Kucera approach requires a stable "parameter" for controller design. For the training of reinforcement learning agents, the set of all stable linear operators is given explicitly through a matrix factorization approach. Moreover, a nonlinear extension is given using a neural network to express a parameterized set of stab
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;SLAM&#20013;&#30340;&#39640;&#32423;&#35821;&#20041;&#20851;&#31995;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2310.00401</link><description>&lt;p&gt;
&#20026;SLAM&#23398;&#20064;&#39640;&#32423;&#35821;&#20041;&#20851;&#31995;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Learning High-level Semantic-Relational Concepts for SLAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00401
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;SLAM&#20013;&#30340;&#39640;&#32423;&#35821;&#20041;&#20851;&#31995;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;SLAM&#30340;&#30740;&#31350;&#23558;&#20854;&#23039;&#24577;&#22270;&#19982;&#26356;&#39640;&#32423;&#30340;&#35821;&#20041;&#27010;&#24565;&#65288;&#22914;Rooms&#65289;&#25193;&#23637;&#65292;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#19981;&#20165;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#24773;&#22659;/&#29615;&#22659;&#34920;&#31034;&#65292;&#32780;&#19988;&#25552;&#39640;&#20102;&#20854;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20808;&#21069;&#30340;&#24037;&#20316;Situational Graphs (S-Graphs+)&#65292;&#20316;&#20026;&#22312;&#22240;&#23376;&#20248;&#21270;&#36807;&#31243;&#20013;&#20849;&#21516;&#21033;&#29992;&#35821;&#20041;&#20851;&#31995;&#30340;&#20808;&#39537;&#65292;&#20381;&#36182;&#20110;&#25968;&#23398;&#23450;&#20041;&#30340;Plane&#21644;Room&#31561;&#35821;&#20041;&#23454;&#20307;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22312;&#21457;&#29616;&#19982;&#19981;&#21516;&#24615;&#36136;&#30340;&#39640;&#32423;&#27010;&#24565;&#23545;&#24212;&#30340;&#25152;&#26377;&#38544;&#34255;&#27169;&#24335;&#30340;&#19979;&#32423;&#22240;&#23376;&#22270;&#20013;&#24182;&#27809;&#26377;&#21807;&#19968;&#26041;&#27861;&#12290;&#30446;&#21069;&#65292;&#23427;&#36890;&#36807;&#29305;&#23450;&#31639;&#27861;&#26469;&#35299;&#20915;&#65292;&#38480;&#21046;&#20102;&#20854;&#22270;&#34920;&#36798;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21487;&#20197;&#20174;&#20013;&#25512;&#26029;&#21508;&#31181;&#39640;&#32423;&#35821;&#20041;&#20851;&#31995;&#27010;&#24565;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00401v2 Announce Type: replace  Abstract: Recent works on SLAM extend their pose graphs with higher-level semantic concepts like Rooms exploiting relationships between them, to provide, not only a richer representation of the situation/environment but also to improve the accuracy of its estimation. Concretely, our previous work, Situational Graphs (S-Graphs+), a pioneer in jointly leveraging semantic relationships in the factor optimization process, relies on semantic entities such as Planes and Rooms, whose relationship is mathematically defined. Nevertheless, there is no unique approach to finding all the hidden patterns in lower-level factor-graphs that correspond to high-level concepts of different natures. It is currently tackled with ad-hoc algorithms, which limits its graph expressiveness.   To overcome this limitation, in this work, we propose an algorithm based on Graph Neural Networks for learning high-level semantic-relational concepts that can be inferred from th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#32452;&#23395;&#33410;&#24615;&#21644;&#38750;&#23395;&#33410;&#24615;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#22686;&#38271;&#36895;&#24230;&#20171;&#20110;&#32447;&#24615;&#21644;&#25351;&#25968;&#20043;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27169;&#22411;&#21253;&#25324;&#20840;&#23616;&#36235;&#21183;&#20174;&#21152;&#27861;&#21040;&#20056;&#27861;&#30340;&#24179;&#28369;&#21464;&#21270;&#65292;&#19982;&#32447;&#24615;&#23616;&#37096;&#36235;&#21183;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#20056;&#27861;&#23395;&#33410;&#24615;&#21644;&#24322;&#26041;&#24046;&#30340;&#21152;&#27861;&#35823;&#24046;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#25311;&#21512;&#25216;&#26415;&#65292;&#35813;&#27169;&#22411;&#22312;M3&#31454;&#36187;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2309.13950</link><description>&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#32452;&#23395;&#33410;&#24615;&#21644;&#38750;&#23395;&#33410;&#24615;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#21152;&#27861;&#21644;&#20056;&#27861;&#25351;&#25968;&#24179;&#28369;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#29992;&#20110;&#24314;&#27169;&#22686;&#38271;&#36895;&#24230;&#20171;&#20110;&#32447;&#24615;&#21644;&#25351;&#25968;&#20043;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#24320;&#21457;&#26159;&#22522;&#20110;&#24555;&#36895;&#22686;&#38271;&#12289;&#27874;&#21160;&#24615;&#36739;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#20855;&#26377;&#20174;&#21152;&#27861;&#21040;&#20056;&#27861;&#24179;&#28369;&#21464;&#21270;&#30340;&#20840;&#23616;&#36235;&#21183;&#65292;&#19982;&#32447;&#24615;&#23616;&#37096;&#36235;&#21183;&#30456;&#32467;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#23395;&#33410;&#24615;(&#22914;&#26524;&#26377;&#20351;&#29992;)&#26159;&#20056;&#27861;&#30340;&#65292;&#35823;&#24046;&#22987;&#32456;&#26159;&#21152;&#27861;&#30340;&#65292;&#20294;&#20855;&#26377;&#24322;&#26041;&#24046;&#24615;&#65292;&#24182;&#21487;&#36890;&#36807;&#21442;&#25968;sigma&#22686;&#38271;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#36125;&#21494;&#26031;&#25311;&#21512;&#25216;&#26415;&#20934;&#30830;&#25311;&#21512;&#36825;&#20123;&#27604;&#26631;&#20934;&#25351;&#25968;&#24179;&#28369;&#27169;&#22411;&#26356;&#22797;&#26434;&#12289;&#26356;&#28789;&#27963;&#30340;&#27169;&#22411;&#12290;&#24403;&#24212;&#29992;&#20110;M3&#31454;&#36187;&#25968;&#25454;&#38598;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#31454;&#36187;&#20013;&#30340;&#26368;&#20339;&#31639;&#27861;&#21644;&#20854;&#20182;&#22522;&#20934;&#27169;&#22411;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#21462;&#24471;&#20102;&#27599;&#20010;&#24207;&#21015;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local and Global Trend Bayesian Exponential Smoothing Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#32452;&#23395;&#33410;&#24615;&#21644;&#38750;&#23395;&#33410;&#24615;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#22686;&#38271;&#36895;&#24230;&#20171;&#20110;&#32447;&#24615;&#21644;&#25351;&#25968;&#20043;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27169;&#22411;&#21253;&#25324;&#20840;&#23616;&#36235;&#21183;&#20174;&#21152;&#27861;&#21040;&#20056;&#27861;&#30340;&#24179;&#28369;&#21464;&#21270;&#65292;&#19982;&#32447;&#24615;&#23616;&#37096;&#36235;&#21183;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#20056;&#27861;&#23395;&#33410;&#24615;&#21644;&#24322;&#26041;&#24046;&#30340;&#21152;&#27861;&#35823;&#24046;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#25311;&#21512;&#25216;&#26415;&#65292;&#35813;&#27169;&#22411;&#22312;M3&#31454;&#36187;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#32452;&#23395;&#33410;&#24615;&#21644;&#38750;&#23395;&#33410;&#24615;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#21152;&#27861;&#21644;&#20056;&#27861;&#25351;&#25968;&#24179;&#28369;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#29992;&#26469;&#24314;&#27169;&#22686;&#38271;&#36895;&#24230;&#20171;&#20110;&#32447;&#24615;&#21644;&#25351;&#25968;&#20043;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#20840;&#23616;&#36235;&#21183;&#65292;&#21487;&#20197;&#20174;&#21152;&#27861;&#24179;&#28369;&#24179;&#28369;&#22320;&#36716;&#21464;&#20026;&#20056;&#27861;&#24179;&#28369;&#65292;&#24182;&#19982;&#32447;&#24615;&#23616;&#37096;&#36235;&#21183;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#30340;&#23395;&#33410;&#24615;&#26159;&#20056;&#27861;&#30340;&#65292;&#35823;&#24046;&#22987;&#32456;&#26159;&#21152;&#27861;&#30340;&#65292;&#20294;&#20855;&#26377;&#24322;&#26041;&#24046;&#24615;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#21442;&#25968;sigma&#22686;&#38271;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#36125;&#21494;&#26031;&#25311;&#21512;&#25216;&#26415;&#20934;&#30830;&#25311;&#21512;&#36825;&#20123;&#27604;&#26631;&#20934;&#25351;&#25968;&#24179;&#28369;&#27169;&#22411;&#26356;&#22797;&#26434;&#12289;&#26356;&#28789;&#27963;&#30340;&#27169;&#22411;&#12290;&#22312;&#24212;&#29992;&#20110;M3&#31454;&#36187;&#25968;&#25454;&#38598;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#31454;&#36187;&#20013;&#30340;&#26368;&#20339;&#31639;&#27861;&#21644;&#20854;&#20182;&#22522;&#20934;&#27169;&#22411;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#21462;&#24471;&#20102;&#27599;&#20010;&#24207;&#21015;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes a family of seasonal and non-seasonal time series models that can be viewed as generalisations of additive and multiplicative exponential smoothing models, to model series that grow faster than linear but slower than exponential. Their development is motivated by fast-growing, volatile time series. In particular, our models have a global trend that can smoothly change from additive to multiplicative, and is combined with a linear local trend. Seasonality when used is multiplicative in our models, and the error is always additive but is heteroscedastic and can grow through a parameter sigma. We leverage state-of-the-art Bayesian fitting techniques to accurately fit these models that are more complex and flexible than standard exponential smoothing models. When applied to the M3 competition data set, our models outperform the best algorithms in the competition as well as other benchmarks, thus achieving to the best of our knowledge the best results of per-series univ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; Dynamic-SUPERB &#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#26500;&#24314;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#65292;&#21033;&#29992;&#25351;&#20196;&#35843;&#20248;&#23454;&#29616;&#38646;-shot&#25191;&#34892;&#22810;&#20219;&#21153;&#65292;&#36890;&#36807;&#21512;&#20316;&#21644;&#36129;&#29486;&#21160;&#24577;&#22686;&#38271;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2309.09510</link><description>&lt;p&gt;
Dynamic-SUPERB: &#38754;&#21521;&#21160;&#24577;&#12289;&#21327;&#20316;&#21644;&#20840;&#38754;&#25351;&#20196;&#35843;&#20248;&#30340;&#35821;&#38899;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09510
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; Dynamic-SUPERB &#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#26500;&#24314;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#65292;&#21033;&#29992;&#25351;&#20196;&#35843;&#20248;&#23454;&#29616;&#38646;-shot&#25191;&#34892;&#22810;&#20219;&#21153;&#65292;&#36890;&#36807;&#21512;&#20316;&#21644;&#36129;&#29486;&#21160;&#24577;&#22686;&#38271;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#33391;&#22909;&#21046;&#23450;&#30340;&#25351;&#20196;&#26102;&#65292;&#23637;&#31034;&#20986;&#20102;&#22312;&#27867;&#21270;&#21040;&#26410;&#35265;&#20219;&#21153;&#26102;&#30340;&#21331;&#36234;&#38646;-shot&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#35821;&#38899;&#22788;&#29702;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26377;&#38480;&#25110;&#29305;&#23450;&#20219;&#21153;&#19978;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#22952;&#30861;&#20102;&#22312;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;Dynamic-SUPERB&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#26500;&#24314;&#33021;&#22815;&#21033;&#29992;&#25351;&#20196;&#35843;&#20248;&#20197;&#38646;-shot&#26041;&#24335;&#25191;&#34892;&#22810;&#39033;&#20219;&#21153;&#30340;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#22810;&#26679;&#30340;&#35821;&#38899;&#20219;&#21153;&#30340;&#20840;&#38754;&#35206;&#30422;&#24182;&#21033;&#29992;&#25351;&#20196;&#35843;&#20248;&#65292;&#25105;&#20204;&#36992;&#35831;&#31038;&#21306;&#21512;&#20316;&#21644;&#36129;&#29486;&#65292;&#20419;&#36827;&#22522;&#20934;&#27979;&#35797;&#30340;&#21160;&#24577;&#22686;&#38271;&#12290;&#20316;&#20026;&#24320;&#31471;&#65292;Dynamic-SUPERB&#36890;&#36807;&#32467;&#21512;33&#20010;&#20219;&#21153;&#21644;22&#20010;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;55&#20010;&#35780;&#20272;&#23454;&#20363;&#12290;&#36825;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#32500;&#24230;&#65292;&#20026;&#35780;&#20272;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#24179;&#21488;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.09510v2 Announce Type: replace-cross  Abstract: Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose severa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34920;&#36848;&#20026;&#19968;&#20010;&#31616;&#21333;&#22522;&#30784;&#27169;&#22411;&#21040;&#28436;&#21270;&#31639;&#23376;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#27969;-&#25193;&#25955;&#26041;&#31243;&#27169;&#22411;&#65292;&#20026;&#26377;&#25928;&#32593;&#32476;&#25552;&#20379;&#20102;&#25968;&#23398;&#35299;&#37322;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2307.12333</link><description>&lt;p&gt;
&#19968;&#20010;&#20844;&#29702;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An axiomatized PDE model of deep neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.12333
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34920;&#36848;&#20026;&#19968;&#20010;&#31616;&#21333;&#22522;&#30784;&#27169;&#22411;&#21040;&#28436;&#21270;&#31639;&#23376;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#27969;-&#25193;&#25955;&#26041;&#31243;&#27169;&#22411;&#65292;&#20026;&#26377;&#25928;&#32593;&#32476;&#25552;&#20379;&#20102;&#25968;&#23398;&#35299;&#37322;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#19982;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#20043;&#38388;&#20851;&#31995;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;PDE&#27169;&#22411;&#30340;&#19968;&#33324;&#24418;&#24335;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;DNN&#34920;&#36848;&#20026;&#20174;&#31616;&#21333;&#22522;&#30784;&#27169;&#22411;&#21040;&#28436;&#21270;&#31639;&#23376;&#12290;&#22522;&#20110;&#20960;&#20010;&#21512;&#29702;&#20551;&#35774;&#65292;&#25105;&#20204;&#35777;&#26126;&#28436;&#21270;&#31639;&#23376;&#23454;&#38469;&#19978;&#30001;&#23545;&#27969;-&#25193;&#25955;&#26041;&#31243;&#20915;&#23450;&#12290;&#35813;&#23545;&#27969;-&#25193;&#25955;&#26041;&#31243;&#27169;&#22411;&#20026;&#20960;&#20010;&#26377;&#25928;&#32593;&#32476;&#25552;&#20379;&#20102;&#25968;&#23398;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#23545;&#27969;-&#25193;&#25955;&#27169;&#22411;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#24182;&#20943;&#23567;&#20102;Rademacher&#22797;&#26434;&#24615;&#12290;&#22522;&#20110;&#23545;&#27969;-&#25193;&#25955;&#26041;&#31243;&#65292;&#25105;&#20204;&#20026;ResNets&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.12333v2 Announce Type: replace  Abstract: Inspired by the relation between deep neural network (DNN) and partial differential equations (PDEs), we study the general form of the PDE models of deep neural networks. To achieve this goal, we formulate DNN as an evolution operator from a simple base model. Based on several reasonable assumptions, we prove that the evolution operator is actually determined by convection-diffusion equation. This convection-diffusion equation model gives mathematical explanation for several effective networks. Moreover, we show that the convection-diffusion model improves the robustness and reduces the Rademacher complexity. Based on the convection-diffusion equation, we design a new training method for ResNets. Experiments validate the performance of the proposed method.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#24179;&#21488;KGLiDS&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#30693;&#35782;&#22270;&#25216;&#26415;&#26469;&#25277;&#35937;&#21644;&#25429;&#33719;&#25968;&#25454;&#31185;&#23398;&#24037;&#20855;&#21450;&#20854;&#32852;&#31995;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#25903;&#25345;&#25968;&#25454;&#21457;&#29616;&#21644;&#31649;&#36947;&#33258;&#21160;&#21270;&#12290;</title><link>https://arxiv.org/abs/2303.02204</link><description>&lt;p&gt;
KGLiDS&#65306;&#29992;&#20110;&#25968;&#25454;&#31185;&#23398;&#30340;&#35821;&#20041;&#25277;&#35937;&#12289;&#38142;&#25509;&#21644;&#33258;&#21160;&#21270;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
KGLiDS: A Platform for Semantic Abstraction, Linking, and Automation of Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.02204
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#24179;&#21488;KGLiDS&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#30693;&#35782;&#22270;&#25216;&#26415;&#26469;&#25277;&#35937;&#21644;&#25429;&#33719;&#25968;&#25454;&#31185;&#23398;&#24037;&#20855;&#21450;&#20854;&#32852;&#31995;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#25903;&#25345;&#25968;&#25454;&#21457;&#29616;&#21644;&#31649;&#36947;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#25105;&#20204;&#35265;&#35777;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#25216;&#26415;&#26469;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#30340;&#26085;&#30410;&#27987;&#21402;&#20852;&#36259;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21019;&#36896;&#20102;&#22823;&#37327;&#30340;&#24037;&#20855;&#65288;&#25968;&#25454;&#38598;&#12289;&#31649;&#36947;&#33050;&#26412;&#31561;&#65289;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#26377;&#31995;&#32479;&#24615;&#30340;&#23581;&#35797;&#26469;&#20840;&#38754;&#25910;&#38598;&#21644;&#21033;&#29992;&#36825;&#20123;&#24037;&#20855;&#20013;&#38544;&#21547;&#30340;&#25152;&#26377;&#30693;&#35782;&#21644;&#32463;&#39564;&#12290;&#30456;&#21453;&#65292;&#25968;&#25454;&#31185;&#23398;&#23478;&#20174;&#21516;&#20107;&#37027;&#37324;&#24674;&#22797;&#20449;&#24687;&#21644;&#19987;&#19994;&#30693;&#35782;&#65292;&#25110;&#36890;&#36807;&#21453;&#22797;&#35797;&#39564;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24179;&#21488;&#65292;KGLiDS&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#30693;&#35782;&#22270;&#25216;&#26415;&#26469;&#25277;&#35937;&#21644;&#25429;&#33719;&#25968;&#25454;&#31185;&#23398;&#24037;&#20855;&#21450;&#20854;&#32852;&#31995;&#30340;&#35821;&#20041;&#12290;&#22522;&#20110;&#36825;&#20123;&#20449;&#24687;&#65292;KGLiDS&#33021;&#22815;&#25903;&#25345;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#25968;&#25454;&#21457;&#29616;&#21644;&#31649;&#36947;&#33258;&#21160;&#21270;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35780;&#20272;&#28085;&#30422;&#20102;&#25968;&#25454;&#21457;&#29616;&#12289;&#25968;&#25454;&#28165;&#27927;&#12289;&#36716;&#25442;&#21644;AutoM&#31561;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.02204v3 Announce Type: replace  Abstract: In recent years, we have witnessed the growing interest from academia and industry in applying data science technologies to analyze large amounts of data. In this process, a myriad of artifacts (datasets, pipeline scripts, etc.) are created. However, there has been no systematic attempt to holistically collect and exploit all the knowledge and experiences that are implicitly contained in those artifacts. Instead, data scientists recover information and expertise from colleagues or learn via trial and error. Hence, this paper presents a scalable platform, KGLiDS, that employs machine learning and knowledge graph technologies to abstract and capture the semantics of data science artifacts and their connections. Based on this information, KGLiDS enables various downstream applications, such as data discovery and pipeline automation. Our comprehensive evaluation covers use cases in data discovery, data cleaning, transformation, and AutoM
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#27668;&#20505;&#32972;&#26223;&#19979;&#30340;XAI&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#26399;&#26395;&#35299;&#37322;&#29305;&#24615;&#65292;&#20026;&#20102;&#35780;&#20272;&#21644;&#25490;&#24207;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#27668;&#20505;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2303.00652</link><description>&lt;p&gt;
&#30830;&#23450;&#27491;&#30830;&#30340;XAI&#26041;&#27861;--&#27668;&#20505;&#31185;&#23398;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#35780;&#20272;&#21644;&#25490;&#24207;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Finding the right XAI method -- A Guide for the Evaluation and Ranking of Explainable AI Methods in Climate Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.00652
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#27668;&#20505;&#32972;&#26223;&#19979;&#30340;XAI&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#26399;&#26395;&#35299;&#37322;&#29305;&#24615;&#65292;&#20026;&#20102;&#35780;&#20272;&#21644;&#25490;&#24207;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#27668;&#20505;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39044;&#27979;&#12290;&#23384;&#22312;&#20960;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#24050;&#32463;&#24212;&#29992;&#20110;&#27668;&#20505;&#31185;&#23398;&#20013;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#32570;&#23569;&#22320;&#38754;&#30495;&#23454;&#35299;&#37322;&#20351;&#20182;&#20204;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#21464;&#24471;&#22797;&#26434;&#65292;&#36827;&#32780;&#38459;&#30861;&#20102;XAI&#26041;&#27861;&#30340;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#27668;&#20505;&#32972;&#26223;&#19979;&#30340;XAI&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#26399;&#26395;&#35299;&#37322;&#29305;&#24615;&#65292;&#21363;&#31283;&#20581;&#24615;&#12289;&#24544;&#23454;&#24615;&#12289;&#38543;&#26426;&#24615;&#12289;&#22797;&#26434;&#24615;&#21644;&#23450;&#20301;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36873;&#25321;&#20197;&#26576;&#19968;&#26696;&#20363;&#30740;&#31350;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#39044;&#27979;&#20102;&#21313;&#24180;&#30340;&#24180;&#22343;&#28201;&#24230;&#22270;&#12290;&#22312;&#35757;&#32451;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20043;&#21518;&#65292;&#24212;&#29992;&#22810;&#31181;XAI&#26041;&#27861;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#22312;&#27599;&#20010;&#23646;&#24615;&#19978;&#19982;&#38543;&#26426;&#22343;&#21248;&#35299;&#37322;&#30340;&#25216;&#33021;&#20998;&#25968;&#12290;&#29420;&#31435;&#20110;&#32593;&#32476;&#65292;&#25105;&#20204;&#21457;&#29616;XAI m
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.00652v2 Announce Type: replace-cross  Abstract: Explainable artificial intelligence (XAI) methods shed light on the predictions of machine learning algorithms. Several different approaches exist and have already been applied in climate science. However, usually missing ground truth explanations complicate their evaluation and comparison, subsequently impeding the choice of the XAI method. Therefore, in this work, we introduce XAI evaluation in the climate context and discuss different desired explanation properties, namely robustness, faithfulness, randomization, complexity, and localization. To this end, we chose previous work as a case study where the decade of annual-mean temperature maps is predicted. After training both a multi-layer perceptron (MLP) and a convolutional neural network (CNN), multiple XAI methods are applied and their skill scores in reference to a random uniform explanation are calculated for each property. Independent of the network, we find that XAI m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CRPTpro&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2302.05614</link><description>&lt;p&gt;
&#20855;&#26377;&#21407;&#22411;&#30340;&#36328;&#39046;&#22495;&#38543;&#26426;&#39044;&#35757;&#32451;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Random Pre-training with Prototypes for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05614
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CRPTpro&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27492;&#24037;&#20316;&#24050;&#25552;&#20132;&#32473;IEEE&#36827;&#34892;&#21487;&#33021;&#30340;&#20986;&#29256;&#12290; CRPTpro&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;RL&#30340;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#12290; CRPTpro&#37319;&#29992;&#20102;&#36328;&#39046;&#22495;&#38543;&#26426;&#31574;&#30053;&#65292;&#21487;&#20197;&#36731;&#26494;&#24555;&#36895;&#22320;&#20174;&#22810;&#20010;&#39046;&#22495;&#20013;&#25277;&#26679;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22312;&#25439;&#22833;&#36827;&#34892;&#21407;&#22411;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#39044;&#35757;&#32451;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;&#12290;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36328;&#39046;&#22495;&#32534;&#30721;&#22120;&#21487;&#20197;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19979;&#28216;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290; &#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#22914;APT&#21644;Proto-RL&#30456;&#27604;&#65292;CRP
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.05614v2 Announce Type: replace-cross  Abstract: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Task-agnostic cross-domain pre-training shows great potential in image-based Reinforcement Learning (RL) but poses a big challenge. In this paper, we propose CRPTpro, a Cross-domain self-supervised Random Pre-Training framework with prototypes for image-based RL. CRPTpro employs cross-domain random policy to easily and quickly sample diverse data from multiple domains, to improve pre-training efficiency. Moreover, prototypical representation learning with a novel intrinsic loss is proposed to pre-train an effective and generic encoder across different domains. Without finetuning, the cross-domain encoder can be implemented for challenging downstream visual-control RL tasks defined in different domains efficiently. Compared with prior arts like APT and Proto-RL, CRP
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#20855;&#26377;&#33258;&#19978;&#32780;&#19979;&#21453;&#39304;&#30340;&#21069;&#39304;&#23398;&#20064;&#31639;&#27861;&#19982;&#33258;&#36866;&#24212;&#21453;&#39304;&#23545;&#40784;&#31639;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;&#20165;&#21521;&#21069;&#31639;&#27861;&#65292;&#25581;&#31034;&#23427;&#20204;&#20849;&#20139;&#30456;&#21516;&#30340;&#23398;&#20064;&#21407;&#29702;&#12290;</title><link>https://arxiv.org/abs/2302.05440</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#19978;&#32780;&#19979;&#21453;&#39304;&#30340;&#21069;&#39304;&#23398;&#20064;&#65306;&#23454;&#35777;&#21644;&#20998;&#26512;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Forward Learning with Top-Down Feedback: Empirical and Analytical Characterization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#20855;&#26377;&#33258;&#19978;&#32780;&#19979;&#21453;&#39304;&#30340;&#21069;&#39304;&#23398;&#20064;&#31639;&#27861;&#19982;&#33258;&#36866;&#24212;&#21453;&#39304;&#23545;&#40784;&#31639;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;&#20165;&#21521;&#21069;&#31639;&#27861;&#65292;&#25581;&#31034;&#23427;&#20204;&#20849;&#20139;&#30456;&#21516;&#30340;&#23398;&#20064;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#20165;&#21521;&#21069;"&#31639;&#27861;&#36817;&#26469;&#21463;&#21040;&#20851;&#27880;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#36991;&#20813;&#20102;&#21521;&#21518;&#20256;&#36882;&#65292;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#21453;&#21521;&#20256;&#25773;&#20013;&#29983;&#29289;&#19981;&#29616;&#23454;&#22240;&#32032;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#39318;&#20808;&#38024;&#23545;&#19982;&#8220;&#20165;&#21521;&#21069;&#8221;&#35268;&#21017;&#30456;&#20851;&#30340;&#24341;&#20154;&#27880;&#30446;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#32553;&#23567;&#19982;&#21453;&#21521;&#20256;&#25773;&#30340;&#24615;&#33021;&#24046;&#36317;&#21644;&#23545;&#20854;&#21160;&#24577;&#29305;&#24615;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#33258;&#19978;&#32780;&#19979;&#21453;&#39304;&#30340;&#20165;&#21521;&#21069;&#31639;&#27861;&#21487;&#34987;&#24456;&#22909;&#22320;&#36817;&#20284;&#20026;&#8220;&#33258;&#36866;&#24212;&#21453;&#39304;&#23545;&#40784;&#8221;&#31639;&#27861;&#65292;&#24182;&#22312;&#21407;&#22411;&#39640;&#32500;&#35774;&#32622;&#20013;&#23545;&#20854;&#23398;&#20064;&#36807;&#31243;&#36827;&#34892;&#20102;&#20998;&#26512;&#36319;&#36394;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;&#20165;&#21521;&#21069;&#31639;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#21069;-&#21069;&#21644;PEPITA&#26694;&#26550;&#65292;&#24182;&#19988;&#34920;&#26126;&#23427;&#20204;&#20849;&#20139;&#30456;&#21516;&#30340;&#23398;&#20064;&#21407;&#29702;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#19977;&#31181;&#37325;&#35201;&#30340;&#21463;&#31070;&#32463;&#21551;&#21457;&#30340;&#23398;&#20064;&#35268;&#21017;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20026;&#8220;&#21521;&#21069;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.05440v2 Announce Type: replace  Abstract: "Forward-only" algorithms, which train neural networks while avoiding a backward pass, have recently gained attention as a way of solving the biologically unrealistic aspects of backpropagation. Here, we first address compelling challenges related to the "forward-only" rules, which include reducing the performance gap with backpropagation and providing an analytical understanding of their dynamics. To this end, we show that the forward-only algorithm with top-down feedback is well-approximated by an "adaptive-feedback-alignment" algorithm, and we analytically track its performance during learning in a prototype high-dimensional setting. Then, we compare different versions of forward-only algorithms, focusing on the Forward-Forward and PEPITA frameworks, and we show that they share the same learning principles. Overall, our work unveils the connections between three key neuro-inspired learning rules, providing a link between "forward-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32431;&#31929;&#23616;&#37096;&#20449;&#24687;&#23454;&#29616;&#25512;&#27979;&#23545;&#25163;&#24314;&#27169;&#30340;&#22810;&#26234;&#33021;&#20307;&#20998;&#24067;&#24335;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#33021;&#22815;&#24110;&#21161;&#21463;&#25511;&#20195;&#29702;&#20570;&#20986;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2211.11940</link><description>&lt;p&gt;
&#29992;&#25512;&#27979;&#23545;&#25163;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Decision-making with Speculative Opponent Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.11940
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32431;&#31929;&#23616;&#37096;&#20449;&#24687;&#23454;&#29616;&#25512;&#27979;&#23545;&#25163;&#24314;&#27169;&#30340;&#22810;&#26234;&#33021;&#20307;&#20998;&#24067;&#24335;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#33021;&#22815;&#24110;&#21161;&#21463;&#25511;&#20195;&#29702;&#20570;&#20986;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25163;&#24314;&#27169;&#36890;&#36807;&#26500;&#24314;&#20854;&#20182;&#20195;&#29702;&#30340;&#27169;&#22411;&#65292;&#20351;&#21463;&#25511;&#20195;&#29702;&#30340;&#20915;&#31574;&#21463;&#30410;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#23545;&#25163;&#30340;&#35266;&#23519;&#21644;&#34892;&#20026;&#65292;&#20294;&#24403;&#23545;&#25163;&#30340;&#34892;&#20026;&#19981;&#21487;&#35266;&#23519;&#25110;&#38590;&#20197;&#33719;&#24471;&#26102;&#65292;&#36825;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#20998;&#24067;&#24335;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#32431;&#31929;&#30340;&#23616;&#37096;&#20449;&#24687;&#65288;&#21363;&#21463;&#25511;&#20195;&#29702;&#30340;&#35266;&#23519;&#12289;&#34892;&#20026;&#21644;&#22870;&#21169;&#65289;&#23454;&#29616;&#25512;&#27979;&#23545;&#25163;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#28436;&#21592;&#32500;&#25345;&#23545;&#23545;&#25163;&#30340;&#25512;&#27979;&#20449;&#24565;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25512;&#27979;&#23545;&#25163;&#27169;&#22411;&#65292;&#20197;&#20351;&#29992;&#23616;&#37096;&#35266;&#23519;&#26469;&#39044;&#27979;&#23545;&#25163;&#30340;&#21160;&#20316;&#65292;&#24182;&#30456;&#24212;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#20998;&#24067;&#24335;&#35780;&#35770;&#23478;&#27169;&#22411;&#25919;&#31574;&#30340;&#22238;&#25253;&#20998;&#24067;&#12290;&#23427;&#21453;&#26144;&#20102;&#28436;&#21592;&#30340;&#36136;&#37327;&#65292;&#22240;&#27492;&#21487;&#20197;&#25351;&#23548;&#28436;&#21592;&#25152;&#20381;&#36182;&#30340;&#25512;&#27979;&#23545;&#25163;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.11940v2 Announce Type: replace  Abstract: Opponent modeling has benefited a controlled agent's decision-making by constructing models of other agents. Existing methods commonly assume access to opponents' observations and actions, which is infeasible when opponents' behaviors are unobservable or hard to obtain. We propose a novel multi-agent distributional actor-critic algorithm to achieve speculative opponent modeling with purely local information (i.e., the controlled agent's observations, actions, and rewards). Specifically, the actor maintains a speculated belief of the opponents, which we call the speculative opponent models, to predict opponent actions using local observations and makes decisions accordingly. Further, the distributional critic models the return distribution of the policy. It reflects the quality of the actor and thus can guide the training of the speculative opponent model that the actor relies on. Extensive experiments confirm that our method successf
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#35757;&#32451;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#30456;&#20851;&#20915;&#31574;&#38382;&#39064;&#26159;$\exists\mathbb{R}$-&#23436;&#20840;&#30340;&#65292;&#21516;&#26102;&#25351;&#20986;&#24517;&#39035;&#20351;&#29992;&#20219;&#24847;&#22823;&#38454;&#30340;&#20195;&#25968;&#25968;&#20316;&#20026;&#26435;&#37325;&#25165;&#33021;&#35757;&#32451;&#19968;&#20123;&#23454;&#20363;&#36798;&#21040;&#26368;&#20248;&#12290;</title><link>https://arxiv.org/abs/2204.01368</link><description>&lt;p&gt;
&#35757;&#32451;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26159;$\exists\mathbb{R}$-&#23436;&#20840;&#30340;
&lt;/p&gt;
&lt;p&gt;
Training Fully Connected Neural Networks is $\exists\mathbb{R}$-Complete
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.01368
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#35757;&#32451;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#30456;&#20851;&#20915;&#31574;&#38382;&#39064;&#26159;$\exists\mathbb{R}$-&#23436;&#20840;&#30340;&#65292;&#21516;&#26102;&#25351;&#20986;&#24517;&#39035;&#20351;&#29992;&#20219;&#24847;&#22823;&#38454;&#30340;&#20195;&#25968;&#25968;&#20316;&#20026;&#26435;&#37325;&#25165;&#33021;&#35757;&#32451;&#19968;&#20123;&#23454;&#20363;&#36798;&#21040;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#19968;&#20010;&#20004;&#23618;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#20013;&#25214;&#21040;&#36866;&#21512;&#32473;&#23450;&#25968;&#25454;&#28857;&#38598;&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#65292;&#20063;&#31216;&#20026;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#30456;&#20851;&#30340;&#20915;&#31574;&#38382;&#39064;&#26159;$\exists\mathbb{R}$-&#23436;&#20840;&#30340;&#65292;&#21363;&#65292;&#22810;&#39033;&#24335;&#26102;&#38388;&#31561;&#20215;&#20110;&#30830;&#23450;&#22810;&#21464;&#37327;&#25972;&#31995;&#25968;&#22810;&#39033;&#24335;&#26159;&#21542;&#26377;&#20219;&#20309;&#23454;&#26681;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#38656;&#35201;&#20855;&#26377;&#20219;&#24847;&#22823;&#38454;&#30340;&#20195;&#25968;&#25968;&#20316;&#20026;&#26435;&#37325;&#65292;&#25165;&#33021;&#35757;&#32451;&#26576;&#20123;&#23454;&#20363;&#36798;&#21040;&#26368;&#20248;&#65292;&#21363;&#20351;&#25152;&#26377;&#25968;&#25454;&#28857;&#37117;&#26159;&#26377;&#29702;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24050;&#32463;&#36866;&#29992;&#20110;&#20855;&#26377;&#20004;&#20010;&#36755;&#20837;&#12289;&#20004;&#20010;&#36755;&#20986;&#21644;&#19968;&#20010;&#20351;&#29992;ReLU&#31070;&#32463;&#20803;&#30340;&#38544;&#34255;&#23618;&#30340;&#20840;&#36830;&#25509;&#23454;&#20363;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21152;&#24378;&#20102;Abrahamsen&#12289;Kleist&#21644;Miltzow&#22312;NeurIPS 2021&#20013;&#30340;&#19968;&#20010;&#32467;&#26524;&#12290;&#30001;&#27492;&#24102;&#26469;&#30340;&#19968;&#20010;&#32467;&#35770;&#26159;&#20687;Arora&#12289;Basu&#12289;Mianjy&#21644;Mukherjee&#22312;ICLR 2018&#20013;&#25552;&#20986;&#30340;&#19968;&#31181;&#32452;&#21512;&#25628;&#32034;&#31639;&#27861;&#26159;&#19981;&#21487;&#33021;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.01368v3 Announce Type: replace-cross  Abstract: We consider the problem of finding weights and biases for a two-layer fully connected neural network to fit a given set of data points as well as possible, also known as EmpiricalRiskMinimization. Our main result is that the associated decision problem is $\exists\mathbb{R}$-complete, that is, polynomial-time equivalent to determining whether a multivariate polynomial with integer coefficients has any real roots. Furthermore, we prove that algebraic numbers of arbitrarily large degree are required as weights to be able to train some instances to optimality, even if all data points are rational. Our result already applies to fully connected instances with two inputs, two outputs, and one hidden layer of ReLU neurons. Thereby, we strengthen a result by Abrahamsen, Kleist and Miltzow [NeurIPS 2021]. A consequence of this is that a combinatorial search algorithm like the one by Arora, Basu, Mianjy and Mukherjee [ICLR 2018] is impos
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#38271;&#30701;&#31383;&#21475;&#30340;&#28857;&#20987;&#27969;&#27169;&#22411;&#33258;&#20030;&#38598;&#25104;&#26694;&#26550;&#65288;BBE-LSWCM&#65289;&#65292;&#22312;&#23454;&#26102;&#23458;&#25143;&#20107;&#20214;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;QBO&#35746;&#38405;&#21462;&#28040;&#21644;&#26377;&#24847;&#20219;&#21153;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2203.16155</link><description>&lt;p&gt;
BBE-LSWCM: &#19968;&#31181;&#38271;&#30701;&#31383;&#21475;&#28857;&#20987;&#27969;&#27169;&#22411;&#30340;&#33258;&#20030;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
BBE-LSWCM: A Bootstrapped Ensemble of Long and Short Window Clickstream Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.16155
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#38271;&#30701;&#31383;&#21475;&#30340;&#28857;&#20987;&#27969;&#27169;&#22411;&#33258;&#20030;&#38598;&#25104;&#26694;&#26550;&#65288;BBE-LSWCM&#65289;&#65292;&#22312;&#23454;&#26102;&#23458;&#25143;&#20107;&#20214;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;QBO&#35746;&#38405;&#21462;&#28040;&#21644;&#26377;&#24847;&#20219;&#21153;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20026;SaaS&#20135;&#21697;&#22914;QBO&#24320;&#21457;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#23458;&#25143;&#20107;&#20214;&#39044;&#27979;&#38382;&#39064;&#30340;&#28857;&#20987;&#27969;&#24314;&#27169;&#26694;&#26550;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20302;&#24310;&#36831;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#40065;&#26834;&#24615;&#30340;&#38598;&#25104;&#26550;&#26500;&#65288;BBE-LSWCM&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#38271;&#26102;&#38388;&#21382;&#21490;&#31383;&#21475;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#65288;&#22914;&#36807;&#21435;&#20960;&#21608;&#20869;&#30340;&#25968;&#25454;&#65289;&#20197;&#21450;&#26368;&#36817;&#30701;&#31383;&#21475;&#20869;&#30340;&#29992;&#25143;&#27963;&#21160;&#65288;&#22914;&#24403;&#21069;&#20250;&#35805;&#20013;&#30340;&#27963;&#21160;&#65289;&#12290;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#20004;&#20010;&#37325;&#35201;&#30340;&#23454;&#26102;&#20107;&#20214;&#39044;&#27979;&#38382;&#39064;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#65306;QBO&#35746;&#38405;&#21462;&#28040;&#21644;&#26377;&#24847;&#20219;&#21153;&#26816;&#27979;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#22312;QBO&#20013;&#30340;&#22312;&#32447;&#37096;&#32626;&#32454;&#33410;&#21644;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.16155v3 Announce Type: replace  Abstract: We consider the problem of developing a clickstream modeling framework for real-time customer event prediction problems in SaaS products like QBO. We develop a low-latency, cost-effective, and robust ensemble architecture (BBE-LSWCM), which combines both aggregated user behavior data from a longer historical window (e.g., over the last few weeks) as well as user activities over a short window in recent-past (e.g., in the current session). As compared to other baseline approaches, we demonstrate the superior performance of the proposed method for two important real-time event prediction problems: subscription cancellation and intended task detection for QBO subscribers. Finally, we present details of the live deployment and results from online experiments in QBO.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Split learning&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#30456;&#20284;&#24615;&#24230;&#37327;&#24182;&#35774;&#35745;&#20102;&#19977;&#31181;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2203.05222</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#65306;&#38024;&#23545;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Similarity-based Label Inference Attack against Training and Inference of Split Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.05222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Split learning&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#30456;&#20284;&#24615;&#24230;&#37327;&#24182;&#35774;&#35745;&#20102;&#19977;&#31181;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Split learning&#26159;&#19968;&#31181;&#26377;&#26395;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#12290;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#34987;&#20999;&#21106;&#25104;&#22810;&#20010;&#37096;&#20998;&#65292;&#22312;&#21442;&#19982;&#32773;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#65292;&#21482;&#20132;&#25442;&#20999;&#21106;&#23618;&#30340;&#20013;&#38388;&#32467;&#26524;&#12290;&#20102;&#35299;Split learning&#30340;&#23433;&#20840;&#24615;&#33021;&#23545;&#35768;&#22810;&#38544;&#31169;&#25935;&#24863;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;Split learning&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#20132;&#25442;&#30340;&#20013;&#38388;&#32467;&#26524;&#65292;&#21253;&#25324;&#30772;&#30862;&#30340;&#25968;&#25454;&#65288;&#21363;&#20174;&#21407;&#22987;&#25968;&#25454;&#25552;&#21462;&#30340;&#29305;&#24449;&#65289;&#21644;&#26799;&#24230;&#65292;&#24050;&#32463;&#21487;&#20197;&#36879;&#38706;&#20986;&#31169;&#23494;&#26631;&#31614;&#12290;&#25105;&#20204;&#23545;&#28508;&#22312;&#30340;&#26631;&#31614;&#27844;&#28431;&#36827;&#34892;&#20102;&#25968;&#23398;&#20998;&#26512;&#65292;&#24182;&#38024;&#23545;&#26799;&#24230;&#21644;&#30772;&#30862;&#30340;&#25968;&#25454;&#25552;&#20986;&#20102;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#27431;&#27663;&#30456;&#20284;&#24230;&#27979;&#37327;&#12290;&#28982;&#21518;&#65292;&#36825;&#20004;&#31181;&#30456;&#20284;&#24230;&#27979;&#37327;&#26174;&#31034;&#21487;&#20197;&#22312;&#27431;&#27663;&#31354;&#38388;&#20013;&#32479;&#19968;&#12290;&#22522;&#20110;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#24674;&#22797;&#31169;&#23494;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.05222v2 Announce Type: replace-cross  Abstract: Split learning is a promising paradigm for privacy-preserving distributed learning. The learning model can be cut into multiple portions to be collaboratively trained at the participants by exchanging only the intermediate results at the cut layer. Understanding the security performance of split learning is critical for many privacy-sensitive applications. This paper shows that the exchanged intermediate results, including the smashed data (i.e., extracted features from the raw data) and gradients during training and inference of split learning, can already reveal the private labels. We mathematically analyze the potential label leakages and propose the cosine and Euclidean similarity measurements for gradients and smashed data, respectively. Then, the two similarity measurements are shown to be unified in Euclidean space. Based on the similarity metric, we design three label inference attacks to efficiently recover the private
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#20027;&#37319;&#26679;&#31354;&#38388;&#20013;&#23398;&#20064;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#28210;&#26579;&#31639;&#27861;&#20013;&#30340;&#26041;&#24046;&#32553;&#20943;&#12290;</title><link>https://arxiv.org/abs/1808.07840</link><description>&lt;p&gt;
&#22312;&#20027;&#37319;&#26679;&#31354;&#38388;&#20013;&#23398;&#20064;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Learning to Importance Sample in Primary Sample Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1808.07840
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#20027;&#37319;&#26679;&#31354;&#38388;&#20013;&#23398;&#20064;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#28210;&#26579;&#31639;&#27861;&#20013;&#30340;&#26041;&#24046;&#32553;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#37319;&#26679;&#26159;&#33945;&#29305;&#21345;&#27931;&#28210;&#26579;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#24046;&#32553;&#20943;&#31574;&#30053;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#35201;&#24615;&#37319;&#26679;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22914;&#20309;&#20174;&#19968;&#32452;&#26679;&#26412;&#34920;&#31034;&#30340;&#30446;&#26631;&#23494;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#29616;&#26377;&#30340;&#33945;&#29305;&#21345;&#27931;&#28210;&#26579;&#31639;&#27861;&#35270;&#20026;&#40657;&#21283;&#23376;&#12290;&#22312;&#22330;&#26223;&#30456;&#20851;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23398;&#20064;&#22312;&#28210;&#26579;&#31639;&#27861;&#30340;&#20027;&#37319;&#26679;&#31354;&#38388;&#20013;&#29983;&#25104;&#20855;&#26377;&#30446;&#26631;&#23494;&#24230;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#35774;&#35745;&#29992;&#20110;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#34920;&#31034;&#23454;&#20540;&#38750;&#20307;&#31215;&#20445;&#25345;&#65288;'Real NVP'&#65289;&#21464;&#25442;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;Real NVP&#26469;&#38750;&#32447;&#24615;&#25197;&#26354;&#20027;&#37319;&#26679;&#31354;&#38388;&#24182;&#33719;&#24471;&#26399;&#26395;&#30340;&#23494;&#24230;&#12290;&#27492;&#22806;&#65292;Real NVP&#26377;&#25928;&#22320;&#35745;&#31639;&#20102;&#25197;&#26354;&#30340;&#38597;&#21487;&#27604;&#34892;&#21015;&#24335;&#65292;&#36825;&#26159;&#23454;&#29616;&#31215;&#20998;&#21464;&#25442;&#25152;&#38656;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1808.07840v2 Announce Type: replace  Abstract: Importance sampling is one of the most widely used variance reduction strategies in Monte Carlo rendering. In this paper, we propose a novel importance sampling technique that uses a neural network to learn how to sample from a desired density represented by a set of samples. Our approach considers an existing Monte Carlo rendering algorithm as a black box. During a scene-dependent training phase, we learn to generate samples with a desired density in the primary sample space of the rendering algorithm using maximum likelihood estimation. We leverage a recent neural network architecture that was designed to represent real-valued non-volume preserving ('Real NVP') transformations in high dimensional spaces. We use Real NVP to non-linearly warp primary sample space and obtain desired densities. In addition, Real NVP efficiently computes the determinant of the Jacobian of the warp, which is required to implement the change of integratio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#32806;&#21512;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#26681;&#65292;&#24182;&#19988;&#22312;&#24378;&#21333;&#35843;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21270;&#25910;&#25947;&#36895;&#29575;&#20026;$\mathcal{O}(1/k)$&#12290;</title><link>http://arxiv.org/abs/2401.12764</link><description>&lt;p&gt;
&#24555;&#36895;&#38750;&#32447;&#24615;&#30340;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#65306;&#23454;&#29616;$\mathcal{O}(1/k)$&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fast Nonlinear Two-Time-Scale Stochastic Approximation: Achieving $\mathcal{O}(1/k)$ Finite-Sample Complexity. (arXiv:2401.12764v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#32806;&#21512;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#26681;&#65292;&#24182;&#19988;&#22312;&#24378;&#21333;&#35843;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21270;&#25910;&#25947;&#36895;&#29575;&#20026;$\mathcal{O}(1/k)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#32806;&#21512;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#26681;&#65292;&#20165;&#20551;&#35774;&#21487;&#20197;&#35266;&#27979;&#21040;&#36825;&#20123;&#31639;&#23376;&#30340;&#22122;&#22768;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#32463;&#20856;&#30340;Ruppert-Polyak&#24179;&#22343;&#25216;&#26415;&#36890;&#36807;&#26679;&#26412;&#21160;&#24577;&#20272;&#35745;&#31639;&#23376;&#30340;&#20540;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#24179;&#22343;&#27493;&#39588;&#30340;&#20272;&#35745;&#20540;&#23558;&#29992;&#20110;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#26356;&#26032;&#20197;&#25214;&#21040;&#25152;&#38656;&#30340;&#35299;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#26159;&#22312;&#24213;&#23618;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#24378;&#21333;&#35843;&#26465;&#20214;&#19979;&#65292;&#25152;&#25552;&#20986;&#26041;&#27861;&#20135;&#29983;&#30340;&#36845;&#20195;&#30340;&#22343;&#26041;&#35823;&#24046;&#20197;&#20248;&#21270;&#30340;&#36895;&#29575;$\mathcal{O}(1/k)$&#25910;&#25947;&#20110;&#38646;&#65292;&#20854;&#20013;$k$&#20026;&#36845;&#20195;&#27425;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#33879;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#32467;&#26524;&#65292;&#26368;&#20339;&#24050;&#30693;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#36895;&#29575;&#20026;$\mathcal{O}(1/k^{2/3})$&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes to develop a new variant of the two-time-scale stochastic approximation to find the roots of two coupled nonlinear operators, assuming only noisy samples of these operators can be observed. Our key idea is to leverage the classic Ruppert-Polyak averaging technique to dynamically estimate the operators through their samples. The estimated values of these averaging steps will then be used in the two-time-scale stochastic approximation updates to find the desired solution. Our main theoretical result is to show that under the strongly monotone condition of the underlying nonlinear operators the mean-squared errors of the iterates generated by the proposed method converge to zero at an optimal rate $\mathcal{O}(1/k)$, where $k$ is the number of iterations. Our result significantly improves the existing result of two-time-scale stochastic approximation, where the best known finite-time convergence rate is $\mathcal{O}(1/k^{2/3})$.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20648;&#23618;&#35745;&#31639;&#22312;&#21560;&#24341;&#23376;&#37325;&#24314;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#39537;&#21160;&#24335;&#20648;&#23618;&#30340;&#26368;&#22823;&#26465;&#20214;Lyapunov&#25351;&#25968;&#38656;&#35201;&#27604;&#30495;&#23454;&#31995;&#32479;&#30340;&#26368;&#23567;Lyapunov&#25351;&#25968;&#26356;&#23567;&#65292;&#20648;&#23618;&#30340;&#35889;&#21322;&#24452;&#23545;&#21560;&#24341;&#23376;&#37325;&#24314;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.00885</link><description>&lt;p&gt;
&#20351;&#29992;&#20648;&#23618;&#35745;&#31639;&#36827;&#34892;&#21560;&#24341;&#23376;&#37325;&#24314;&#65306;&#20648;&#23618;&#30340;&#26465;&#20214;Lyapunov&#25351;&#25968;&#23545;&#24544;&#23454;&#21560;&#24341;&#23376;&#37325;&#24314;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Attractor reconstruction with reservoir computers: The effect of the reservoir's conditional Lyapunov exponents on faithful attractor reconstruction. (arXiv:2401.00885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00885
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20648;&#23618;&#35745;&#31639;&#22312;&#21560;&#24341;&#23376;&#37325;&#24314;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#39537;&#21160;&#24335;&#20648;&#23618;&#30340;&#26368;&#22823;&#26465;&#20214;Lyapunov&#25351;&#25968;&#38656;&#35201;&#27604;&#30495;&#23454;&#31995;&#32479;&#30340;&#26368;&#23567;Lyapunov&#25351;&#25968;&#26356;&#23567;&#65292;&#20648;&#23618;&#30340;&#35889;&#21322;&#24452;&#23545;&#21560;&#24341;&#23376;&#37325;&#24314;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20648;&#23618;&#35745;&#31639;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#22797;&#21046;&#21160;&#21147;&#31995;&#32479;&#30340;&#28151;&#27788;&#21560;&#24341;&#23376;&#65292;&#21253;&#25324;&#20998;&#24418;&#32500;&#24230;&#21644;&#25972;&#20010;Lyapunov&#35889;&#12290;&#25105;&#20204;&#23450;&#37327;&#22320;&#23558;&#39537;&#21160;&#24335;&#20648;&#23618;&#35745;&#31639;&#30340;&#24191;&#20041;&#21516;&#27493;&#21160;&#21147;&#23398;&#19982;&#33258;&#20027;&#24335;&#20648;&#23618;&#35745;&#31639;&#22312;&#21560;&#24341;&#23376;&#37325;&#24314;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20026;&#20102;&#25104;&#21151;&#36827;&#34892;&#21560;&#24341;&#23376;&#37325;&#24314;&#21644;Lyapunov&#25351;&#25968;&#20272;&#35745;&#65292;&#39537;&#21160;&#24335;&#20648;&#23618;&#30340;&#26368;&#22823;&#26465;&#20214;Lyapunov&#25351;&#25968;&#24517;&#39035;&#26174;&#33879;&#23567;&#20110;&#30495;&#23454;&#31995;&#32479;&#30340;&#26368;&#23567;&#65288;&#26368;&#36127;&#65289;Lyapunov&#25351;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20648;&#23618;&#30340;&#26368;&#22823;&#26465;&#20214;Lyapunov&#25351;&#25968;&#24378;&#28872;&#20381;&#36182;&#20110;&#20648;&#23618;&#37051;&#25509;&#30697;&#38453;&#30340;&#35889;&#21322;&#24452;&#65292;&#22240;&#27492;&#65292;&#23545;&#20110;&#21560;&#24341;&#23376;&#37325;&#24314;&#21644;Lyapunov&#25351;&#25968;&#20272;&#35745;&#65292;&#35889;&#21322;&#24452;&#36739;&#23567;&#30340;&#20648;&#23618;&#35745;&#31639;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir computing is a machine learning technique which has been shown to be able to replicate the chaotic attractor, including the fractal dimension and the entire Lyapunov spectrum, of the dynamical system on which it is trained. We quantitatively relate the generalized synchronization dynamics of a driven reservoir computer during the training stage to the performance of the autonomous reservoir computer at the attractor reconstruction task. We show that, for successful attractor reconstruction and Lyapunov exponent estimation, the largest conditional Lyapunov exponent of the driven reservoir must be significantly smaller (more negative) than the smallest (most negative) Lyapunov exponent of the true system. We find that the maximal conditional Lyapunov exponent of the reservoir depends strongly on the spectral radius of the reservoir adjacency matrix, and therefore, for attractor reconstruction and Lyapunov exponent estimation, small spectral radius reservoir computers perform be
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#29256;&#26412;&#65292;&#29992;&#20110;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#23384;&#38382;&#39064;&#19978;&#23384;&#22312;&#30340;&#22256;&#25200;&#12290;&#19982;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#30340;Graph Transformer&#30456;&#27604;&#65292;&#24212;&#29992;&#20102;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#30830;&#23454;&#21487;&#20197;&#31283;&#23450;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.11730</link><description>&lt;p&gt;
&#24378;&#21270;&#22270;&#36716;&#25442;&#22120;&#19982;&#27491;&#21017;&#21270;&#20851;&#27880;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Stronger Graph Transformer with Regularized Attention Scores. (arXiv:2312.11730v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#29256;&#26412;&#65292;&#29992;&#20110;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#23384;&#38382;&#39064;&#19978;&#23384;&#22312;&#30340;&#22256;&#25200;&#12290;&#19982;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#30340;Graph Transformer&#30456;&#27604;&#65292;&#24212;&#29992;&#20102;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#30830;&#23454;&#21487;&#20197;&#31283;&#23450;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20197;&#20854;&#20869;&#23384;&#28040;&#32791;&#22823;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#26368;&#36817;&#21457;&#29616;&#65292;&#22522;&#20110;Transformer&#30340;GNN&#31216;&#20026;Graph Transformer&#22312;&#23384;&#22312;&#38271;&#31243;&#20381;&#36182;&#24615;&#26102;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#25968;&#25454;&#21644;Transformer&#26550;&#26500;&#30456;&#32467;&#21512;&#23548;&#33268;&#20102;&#35760;&#24518;&#38382;&#39064;&#26356;&#21152;&#20005;&#37325;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#8221;&#30340;&#29256;&#26412;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#20943;&#36731;GT&#30340;&#20869;&#23384;&#28322;&#20986;&#38382;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19981;&#28165;&#26970;&#22312;&#20301;&#32622;&#32534;&#30721;&#30340;&#22522;&#30784;&#19978;&#26159;&#21542;&#26377;&#36793;&#32536;&#27491;&#21017;&#21270;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#28982;&#32780;&#65292;&#26174;&#28982;&#65292;&#24212;&#29992;&#25105;&#20204;&#30340;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#30830;&#23454;&#21487;&#20197;&#31283;&#23450;&#22320;&#25913;&#21892;GT&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#30340;GT&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks are notorious for its memory consumption. A recent Transformer-based GNN called Graph Transformer is shown to obtain superior performances when long range dependencies exist. However, combining graph data and Transformer architecture led to a combinationally worse memory issue. We propose a novel version of "edge regularization technique" that alleviates the need for Positional Encoding and ultimately alleviate GT's out of memory issue. We observe that it is not clear whether having an edge regularization on top of positional encoding is helpful. However, it seems evident that applying our edge regularization technique indeed stably improves GT's performance compared to GT without Positional Encoding.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31867;&#21407;&#22411;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;CPDM&#65289;&#26469;&#35299;&#20915;Continual Learning&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;CPDM&#36890;&#36807;&#25552;&#39640;&#29983;&#25104;&#22120;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#20943;&#23569;&#20102;&#20998;&#31867;&#22120;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2312.06710</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#24335;&#22238;&#25918;&#30340;&#31867;&#21407;&#22411;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;Continual Learning
&lt;/p&gt;
&lt;p&gt;
Class-Prototype Conditional Diffusion Model for Continual Learning with Generative Replay. (arXiv:2312.06710v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31867;&#21407;&#22411;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;CPDM&#65289;&#26469;&#35299;&#20915;Continual Learning&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;CPDM&#36890;&#36807;&#25552;&#39640;&#29983;&#25104;&#22120;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#20943;&#23569;&#20102;&#20998;&#31867;&#22120;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#26159;Continual Learning&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38590;&#39064;&#12290;&#28145;&#24230;&#29983;&#25104;&#24335;&#22238;&#25918;&#65288;GR&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#29983;&#25104;&#26679;&#26412;&#26469;&#22686;&#24378;&#27169;&#22411;&#35760;&#24518;&#33021;&#21147;&#30340;&#25216;&#26415;&#12290;&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#20174;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21457;&#23637;&#21040;&#20102;&#26368;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#12290;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#29983;&#25104;&#30340;&#25968;&#25454;&#36136;&#37327;&#19982;&#21407;&#22987;&#25968;&#25454;&#30456;&#27604;&#20250;&#36880;&#28176;&#19979;&#38477;&#65292;&#22240;&#20026;&#29983;&#25104;&#22120;&#19981;&#26029;&#20174;&#20854;&#36755;&#20986;&#20013;&#36827;&#34892;&#33258;&#25105;&#23398;&#20064;&#12290;&#36825;&#31181;&#36864;&#21270;&#21487;&#33021;&#23548;&#33268;&#20998;&#31867;&#22120;&#20013;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31867;&#21407;&#22411;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;CPDM&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#24335;&#22238;&#25918;&#30340;Continual Learning&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25552;&#39640;&#29983;&#25104;&#22120;&#20013;&#30340;&#22270;&#20687;&#36136;&#37327;&#20174;&#32780;&#20943;&#23569;&#20102;&#20998;&#31867;&#22120;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;CPDM&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#31867;&#21407;&#22411;&#65292;&#23427;&#25429;&#25417;&#20102;&#32473;&#23450;&#31867;&#21035;&#20013;&#22270;&#20687;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mitigating catastrophic forgetting is a key hurdle in continual learning. Deep Generative Replay (GR) provides techniques focused on generating samples from prior tasks to enhance the model's memory capabilities. With the progression in generative AI, generative models have advanced from Generative Adversarial Networks (GANs) to the more recent Diffusion Models (DMs). A major issue is the deterioration in the quality of generated data compared to the original, as the generator continuously self-learns from its outputs. This degradation can lead to the potential risk of catastrophic forgetting occurring in the classifier. To address this, we propose the Class-Prototype Conditional Diffusion Model (CPDM), a GR-based approach for continual learning that enhances image quality in generators and thus reduces catastrophic forgetting in classifiers. The cornerstone of CPDM is a learnable class-prototype that captures the core characteristics of images in a given class. This prototype, integra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#24187;&#35273;&#29366;&#24577;&#21644;&#30495;&#23454;&#35266;&#23519;&#29366;&#24577;&#30340;&#19981;&#21305;&#37197;&#24615;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#65292;&#23558;&#26032;&#39062;&#24615;&#26816;&#27979;&#32435;&#20837;&#19990;&#30028;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#12290;&#22312;&#26032;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.08731</link><description>&lt;p&gt;
&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#23454;&#29616;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Simple Way to Incorporate Novelty Detection in World Models. (arXiv:2310.08731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#24187;&#35273;&#29366;&#24577;&#21644;&#30495;&#23454;&#35266;&#23519;&#29366;&#24577;&#30340;&#19981;&#21305;&#37197;&#24615;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#65292;&#23558;&#26032;&#39062;&#24615;&#26816;&#27979;&#32435;&#20837;&#19990;&#30028;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#12290;&#22312;&#26032;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#19990;&#30028;&#26426;&#21046;&#25110;&#23646;&#24615;&#21457;&#29983;&#31361;&#28982;&#21464;&#21270;&#26102;&#65292;&#20195;&#29702;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#21487;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#23558;&#35270;&#35273;&#23646;&#24615;&#25110;&#29366;&#24577;&#36716;&#25442;&#30340;&#31361;&#21464;&#31216;&#20026;&#8220;&#26032;&#39062;&#24615;&#8221;&#12290;&#22312;&#29983;&#25104;&#30340;&#19990;&#30028;&#27169;&#22411;&#26694;&#26550;&#20013;&#23454;&#26045;&#26032;&#39062;&#24615;&#26816;&#27979;&#26159;&#20445;&#25252;&#37096;&#32626;&#26102;&#20195;&#29702;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36793;&#30028;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26032;&#39062;&#24615;&#26816;&#27979;&#32435;&#20837;&#19990;&#30028;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#24187;&#35273;&#29366;&#24577;&#21644;&#30495;&#23454;&#35266;&#23519;&#29366;&#24577;&#30340;&#19981;&#21305;&#37197;&#24615;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19982;&#24207;&#21015;&#20915;&#31574;&#30456;&#20851;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#26412;&#20307;&#35770;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#20195;&#29702;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#36716;&#25442;&#20998;&#24067;&#20013;&#26816;&#27979;&#26032;&#39062;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#26032;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) using world models has found significant recent successes. However, when a sudden change to world mechanics or properties occurs then agent performance and reliability can dramatically decline. We refer to the sudden change in visual properties or state transitions as {\em novelties}. Implementing novelty detection within generated world model frameworks is a crucial task for protecting the agent when deployed. In this paper, we propose straightforward bounding approaches to incorporate novelty detection into world model RL agents, by utilizing the misalignment of the world model's hallucinated states and the true observed states as an anomaly score. We first provide an ontology of novelty detection relevant to sequential decision making, then we provide effective approaches to detecting novelties in a distribution of transitions learned by an agent in a world model. Finally, we show the advantage of our work in a novel environment compared to traditional ma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26597;&#35810;&#24335;&#25805;&#20316;&#32593;&#32476;&#30340;&#25391;&#21160;&#22768;&#23398;&#39057;&#21709;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#20195;&#34920;&#24615;&#25391;&#21160;&#22768;&#23398;&#38382;&#39064;&#30340;&#32467;&#26500;&#21270;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#39057;&#21709;&#27169;&#25311;&#65292;&#26377;&#21161;&#20110;&#35774;&#35745;&#20248;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.05469</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#24335;&#25805;&#20316;&#32593;&#32476;&#30340;&#25391;&#21160;&#22768;&#23398;&#39057;&#21709;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Vibroacoustic Frequency Response Prediction with Query-based Operator Networks. (arXiv:2310.05469v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26597;&#35810;&#24335;&#25805;&#20316;&#32593;&#32476;&#30340;&#25391;&#21160;&#22768;&#23398;&#39057;&#21709;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#20195;&#34920;&#24615;&#25391;&#21160;&#22768;&#23398;&#38382;&#39064;&#30340;&#32467;&#26500;&#21270;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#39057;&#21709;&#27169;&#25311;&#65292;&#26377;&#21161;&#20110;&#35774;&#35745;&#20248;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#39134;&#26426;&#12289;&#27773;&#36710;&#21644;&#25151;&#23627;&#31561;&#26426;&#26800;&#32467;&#26500;&#20013;&#30340;&#25391;&#21160;&#22768;&#23398;&#27874;&#20256;&#25773;&#23545;&#20445;&#35777;&#29992;&#25143;&#30340;&#20581;&#24247;&#21644;&#33298;&#36866;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#20998;&#26512;&#36825;&#20123;&#31995;&#32479;&#65292;&#35774;&#35745;&#24072;&#21644;&#24037;&#31243;&#24072;&#20027;&#35201;&#32771;&#34385;&#39057;&#22495;&#20013;&#30340;&#21160;&#24577;&#21709;&#24212;&#65292;&#36825;&#36890;&#36807;&#20687;&#26377;&#38480;&#20803;&#26041;&#27861;&#36825;&#26679;&#30340;&#26114;&#36149;&#25968;&#20540;&#27169;&#25311;&#26469;&#35745;&#31639;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#25968;&#25454;&#30340;&#26367;&#20195;&#27169;&#22411;&#25215;&#35834;&#21152;&#36895;&#36825;&#20123;&#27169;&#25311;&#65292;&#20174;&#32780;&#20419;&#36827;&#35774;&#35745;&#20248;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#31561;&#20219;&#21153;&#30340;&#23454;&#26045;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#20195;&#34920;&#24615;&#25391;&#21160;&#22768;&#23398;&#38382;&#39064;&#65306;&#39044;&#27979;&#24102;&#26377;&#19981;&#21516;&#24418;&#24335;&#38262;&#36793;&#30340;&#25391;&#21160;&#26495;&#30340;&#39057;&#21709;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20102;&#20849;&#35745;12,000&#20010;&#26495;&#20960;&#20309;&#24418;&#29366;&#20197;&#21450;&#30456;&#24212;&#30340;&#25968;&#20540;&#35299;&#65292;&#24182;&#24341;&#20837;&#20102;&#35780;&#20272;&#25351;&#26631;&#20197;&#37327;&#21270;&#39044;&#27979;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#39057;&#21709;&#39044;&#27979;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#29575;&#26597;&#35810;&#25805;&#20316;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding vibroacoustic wave propagation in mechanical structures like airplanes, cars and houses is crucial to ensure health and comfort of their users. To analyze such systems, designers and engineers primarily consider the dynamic response in the frequency domain, which is computed through expensive numerical simulations like the finite element method. In contrast, data-driven surrogate models offer the promise of speeding up these simulations, thereby facilitating tasks like design optimization, uncertainty quantification, and design space exploration. We present a structured benchmark for a representative vibroacoustic problem: Predicting the frequency response for vibrating plates with varying forms of beadings. The benchmark features a total of 12,000 plate geometries with an associated numerical solution and introduces evaluation metrics to quantify the prediction quality. To address the frequency response prediction task, we propose a novel frequency query operator model, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.16512</link><description>&lt;p&gt;
&#20174;&#22797;&#26434;&#21040;&#28165;&#26224;&#65306;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#20998;&#26512;&#34920;&#36798;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity. (arXiv:2309.16512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#65288;Clifford&#65289;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20351;&#29992;&#26631;&#20934;&#27491;&#21017;&#21270;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#30001;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#32473;&#20986;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#38382;&#39064;&#21487;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#22312;&#20854;&#20013;&#32534;&#30721;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#35813;&#32467;&#26500;&#20197;&#25968;&#25454;&#21521;&#37327;&#29983;&#25104;&#30340;&#19977;&#35282;&#24418;&#21644;&#24179;&#34892;&#20307;&#30340;&#26377;&#31526;&#21495;&#20307;&#31215;&#34920;&#31034;&#12290;&#20984;&#38382;&#39064;&#36890;&#36807;$\ell_1$&#27491;&#21017;&#21270;&#25214;&#21040;&#26679;&#26412;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#20197;&#21457;&#29616;&#20165;&#30456;&#20851;&#30340;&#26964;&#31215;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#23618;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford) algebra and convex optimization. We show that optimal weights of deep ReLU neural networks are given by the wedge product of training samples when trained with standard regularized loss. Furthermore, the training problem reduces to convex optimization over wedge product features, which encode the geometric structure of the training dataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by data vectors. The convex problem finds a small subset of samples via $\ell_1$ regularization to discover only relevant wedge product features. Our analysis provides a novel perspective on the inner workings of deep neural networks and sheds light on the role of the hidden layers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21097;&#20313;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;RDDM&#65289;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#39044;&#27979;&#27531;&#24046;&#26469;&#34920;&#31034;&#20174;&#30446;&#26631;&#22495;&#21040;&#36755;&#20837;&#22495;&#30340;&#26041;&#21521;&#24615;&#25193;&#25955;&#65292;&#24182;&#21516;&#26102;&#20272;&#35745;&#22122;&#22768;&#26469;&#32771;&#34385;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2308.13712</link><description>&lt;p&gt;
&#21097;&#20313;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Residual Denoising Diffusion Models. (arXiv:2308.13712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13712
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21097;&#20313;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;RDDM&#65289;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#39044;&#27979;&#27531;&#24046;&#26469;&#34920;&#31034;&#20174;&#30446;&#26631;&#22495;&#21040;&#36755;&#20837;&#22495;&#30340;&#26041;&#21521;&#24615;&#25193;&#25955;&#65292;&#24182;&#21516;&#26102;&#20272;&#35745;&#22122;&#22768;&#26469;&#32771;&#34385;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;&#23558;&#36864;&#21270;&#30340;&#36755;&#20837;&#22270;&#20687;&#20316;&#20026;&#26465;&#20214;&#36755;&#20837;&#21040;&#22122;&#22768;&#20272;&#35745;&#32593;&#32476;&#20013;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#36825;&#20010;&#25193;&#25955;&#36807;&#31243;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#22522;&#26412;&#19978;&#26159;&#20174;&#22122;&#22768;&#29983;&#25104;&#30446;&#26631;&#22270;&#20687;&#12290;&#20026;&#20102;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#19988;&#26356;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#24674;&#22797;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21097;&#20313;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;RDDM&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#65288;&#20363;&#22914;DDPM&#25110;DDIM&#65289;&#20165;&#19987;&#27880;&#20110;&#22122;&#22768;&#20272;&#35745;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;RDDM&#39044;&#27979;&#27531;&#24046;&#26469;&#34920;&#31034;&#20174;&#30446;&#26631;&#22495;&#21040;&#36755;&#20837;&#22495;&#30340;&#26041;&#21521;&#24615;&#25193;&#25955;&#65292;&#24182;&#21516;&#26102;&#20272;&#35745;&#22122;&#22768;&#26469;&#32771;&#34385;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#25200;&#21160;&#12290;&#27531;&#24046;&#30340;&#24341;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#37325;&#26032;&#23450;&#20041;&#27491;&#21521;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20013;&#30446;&#26631;&#22270;&#20687;&#36880;&#28176;&#25193;&#25955;&#25104;&#19968;&#20010;&#32431;&#22122;&#22768;&#22270;&#20687;&#25110;&#25658;&#24102;&#22122;&#22768;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#20174;&#32780;&#32479;&#19968;&#20102;&#22270;&#20687;&#29983;&#25104;&#21644;&#24674;&#22797;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#37319;&#26679;&#36807;&#31243;&#19982;&#30495;&#23454;&#30340;&#30446;&#26631;&#22270;&#20687;&#20998;&#24067;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current diffusion-based image restoration methods feed degraded input images as conditions into the noise estimation network. However, interpreting this diffusion process is challenging since it essentially generates the target image from the noise. To establish a unified and more interpretable model for image generation and restoration, we propose residual denoising diffusion models (RDDM). In contrast to existing diffusion models (e.g., DDPM or DDIM) that focus solely on noise estimation, our RDDM predicts residuals to represent directional diffusion from the target domain to the input domain, while concurrently estimating noise to account for random perturbations in the diffusion process. The introduction of residuals allows us to redefine the forward diffusion process, wherein the target image progressively diffuses into a purely noisy image or a noise-carrying input image, thus unifying image generation and restoration. We demonstrate that our sampling process is consistent with t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32463;&#27982;&#38750;&#32447;&#24615;MPC&#30340;Koopman&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#25511;&#21046;&#24615;&#33021;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.01674</link><description>&lt;p&gt;
&#32463;&#27982;&#38750;&#32447;&#24615;MPC&#30340;Koopman&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear MPC. (arXiv:2308.01674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32463;&#27982;&#38750;&#32447;&#24615;MPC&#30340;Koopman&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#25511;&#21046;&#24615;&#33021;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#65288;&#32463;&#27982;&#65289;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;&#65288;e&#65289;NMPC&#65289;&#38656;&#35201;&#22312;&#25152;&#26377;&#30456;&#20851;&#29366;&#24577;&#31354;&#38388;&#21306;&#22495;&#37117;&#20855;&#26377;&#36275;&#22815;&#20934;&#30830;&#24615;&#30340;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36824;&#24517;&#39035;&#35745;&#31639;&#25104;&#26412;&#36275;&#22815;&#20302;&#20197;&#30830;&#20445;&#23454;&#26102;&#21487;&#34892;&#24615;&#12290;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#26426;&#21046;&#27169;&#22411;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#65288;e&#65289;NMPC&#30340;&#35745;&#31639;&#36127;&#25285;&#65307;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#31995;&#32479;&#36776;&#35782;&#20197;&#22312;&#27169;&#25311;&#26679;&#26412;&#19978;&#33719;&#24471;&#26368;&#22823;&#24179;&#22343;&#39044;&#27979;&#20934;&#30830;&#24615;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20316;&#20026;&#23454;&#38469;&#65288;e&#65289;NMPC&#30340;&#19968;&#37096;&#20998;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#26368;&#20339;&#65288;e&#65289;NMPC&#24615;&#33021;&#30340;&#21160;&#24577;&#26367;&#20195;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;&#20855;&#26377;&#25511;&#21046;&#24615;&#33021;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#33391;&#22909;&#24179;&#34913;&#30340;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#22522;&#20110;&#24050;&#24314;&#31435;&#30340;&#38750;&#32447;&#24615;&#36830;&#32493;&#25605;&#25292;&#21453;&#24212;&#22120;&#27169;&#22411;&#30340;&#24212;&#29992;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
(Economic) nonlinear model predictive control ((e)NMPC) requires dynamic system models that are sufficiently accurate in all relevant state-space regions. These models must also be computationally cheap enough to ensure real-time tractability. Data-driven surrogate models for mechanistic models can be used to reduce the computational burden of (e)NMPC; however, such models are typically trained by system identification for maximum average prediction accuracy on simulation samples and perform suboptimally as part of actual (e)NMPC. We present a method for end-to-end reinforcement learning of dynamic surrogate models for optimal performance in (e)NMPC applications, resulting in predictive controllers that strike a favorable balance between control performance and computational demand. We validate our method on two applications derived from an established nonlinear continuous stirred-tank reactor model. We compare the controller performance to that of MPCs utilizing models trained by the 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26368;&#23567;&#33539;&#25968;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#36827;&#34892;&#22122;&#22768;&#21333;&#21464;&#37327;&#22238;&#24402;&#30340;&#25554;&#20540;&#65292;&#23545;&#20110;$L_1$&#25439;&#22833;&#21644;$ p &lt;2 $&#30340;$L_p$&#25439;&#22833;&#25233;&#21046;&#36807;&#25311;&#21512;&#65292;&#20294;&#23545;&#20110;$ p \geq 2 $&#30340;&#25439;&#22833;&#26159;&#28798;&#38590;&#24615;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.15396</link><description>&lt;p&gt;
&#22522;&#20110;&#27973;&#23618;&#21333;&#21464;&#37327;ReLU&#32593;&#32476;&#30340;&#22122;&#22768;&#25554;&#20540;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Noisy Interpolation Learning with Shallow Univariate ReLU Networks. (arXiv:2307.15396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15396
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26368;&#23567;&#33539;&#25968;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#36827;&#34892;&#22122;&#22768;&#21333;&#21464;&#37327;&#22238;&#24402;&#30340;&#25554;&#20540;&#65292;&#23545;&#20110;$L_1$&#25439;&#22833;&#21644;$ p &lt;2 $&#30340;$L_p$&#25439;&#22833;&#25233;&#21046;&#36807;&#25311;&#21512;&#65292;&#20294;&#23545;&#20110;$ p \geq 2 $&#30340;&#25439;&#22833;&#26159;&#28798;&#38590;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22122;&#22768;&#21333;&#21464;&#37327;&#22238;&#24402;&#20013;&#20351;&#29992;&#26368;&#23567;&#33539;&#25968;&#65288;&#26435;&#37325;&#30340;$\ell_2$&#33539;&#25968;&#65289;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#36827;&#34892;&#25554;&#20540;&#30340;&#28176;&#36817;&#36807;&#25311;&#21512;&#34892;&#20026;&#12290;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;$L_1$&#25439;&#22833;&#21644;$ p &lt;2 $&#30340;&#20219;&#20309;$L_p$&#25439;&#22833;&#65292;&#36807;&#25311;&#21512;&#29616;&#35937;&#20250;&#34987;&#25233;&#21046;&#65292;&#20294;&#23545;&#20110;$ p \geq 2 $&#30340;&#25439;&#22833;&#26159;&#28798;&#38590;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the asymptotic overfitting behavior of interpolation with minimum norm ($\ell_2$ of the weights) two-layer ReLU networks for noisy univariate regression. We show that overfitting is tempered for the $L_1$ loss, and any $L_p$ loss for $p&lt;2$, but catastrophic for $p\geq 2$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;Dikin&#27493;&#34892;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#24182;&#36866;&#24212;&#19968;&#33324;&#24230;&#37327;&#65292;&#20026;&#24102;&#32422;&#26463;&#30340;PSD&#38181;&#20307;&#25277;&#26679;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#30340;&#33258;&#20849;&#36717;&#30697;&#38453;&#20989;&#25968;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2307.12943</link><description>&lt;p&gt;
&#29992;&#24230;&#37327;&#30340;Dikin&#27493;&#39588;&#26377;&#25928;&#22320;&#25277;&#26679;PSD&#38181;&#20307;
&lt;/p&gt;
&lt;p&gt;
Efficiently Sampling the PSD Cone with the Metric Dikin Walk. (arXiv:2307.12943v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;Dikin&#27493;&#34892;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#24182;&#36866;&#24212;&#19968;&#33324;&#24230;&#37327;&#65292;&#20026;&#24102;&#32422;&#26463;&#30340;PSD&#38181;&#20307;&#25277;&#26679;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#30340;&#33258;&#20849;&#36717;&#30697;&#38453;&#20989;&#25968;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#23450;&#35268;&#21010;&#20195;&#34920;&#20102;&#39640;&#25928;&#35745;&#31639;&#30340;&#21069;&#27839;&#12290;&#23613;&#31649;&#22312;&#21322;&#23450;&#26368;&#20248;&#21270;&#19978;&#24050;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#22914;&#20170;&#20869;&#28857;&#27861;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#35299;&#20915;&#20013;&#31561;&#35268;&#27169;&#30340;&#38382;&#39064;&#65292;&#20294;&#26159;&#25277;&#26679;&#21322;&#23450;&#35299;&#30340;&#22522;&#26412;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#30452;&#25509;&#24212;&#29992;&#24050;&#30693;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#25277;&#26679;&#19968;&#33324;&#20984;&#20307;&#30340;&#26041;&#27861;&#23548;&#33268;&#36816;&#34892;&#26102;&#38388;&#36807;&#38271;&#12290;&#27492;&#22806;&#65292;&#24050;&#30693;&#30340;&#36890;&#29992;&#26041;&#27861;&#38656;&#35201;&#26114;&#36149;&#30340;&#33293;&#20837;&#38454;&#27573;&#20316;&#20026;&#39044;&#22788;&#29702;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;Dikin&#27493;&#34892;&#65292;&#24182;&#39318;&#20808;&#23558;&#20854;&#36866;&#24212;&#20110;&#19968;&#33324;&#24230;&#37327;&#65292;&#28982;&#21518;&#20026;&#24102;&#26377;&#20223;&#23556;&#32422;&#26463;&#30340;PSD&#38181;&#20307;&#35774;&#35745;&#21512;&#36866;&#30340;&#24230;&#37327;&#12290;&#25152;&#24471;&#21040;&#30340;&#28151;&#21512;&#26102;&#38388;&#21644;&#27599;&#27493;&#22797;&#26434;&#24230;&#30456;&#24403;&#23567;&#65292;&#24182;&#19988;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#24230;&#37327;&#65292;&#21487;&#20197;&#20351;&#20854;&#23545;&#32422;&#26463;&#30340;&#20381;&#36182;&#20851;&#31995;&#21464;&#20026;&#22810;&#23545;&#25968;&#32423;&#30340;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;&#33258;&#20849;&#36717;&#30697;&#38453;&#20989;&#25968;&#27010;&#24565;&#65292;&#24182;&#32473;&#20986;&#20102;&#32452;&#21512;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-definite programs represent a frontier of efficient computation. While there has been much progress on semi-definite optimization, with moderate-sized instances currently solvable in practice by the interior-point method, the basic problem of sampling semi-definite solutions remains a formidable challenge. The direct application of known polynomial-time algorithms for sampling general convex bodies to semi-definite sampling leads to a prohibitively high running time. In addition, known general methods require an expensive rounding phase as pre-processing. Here we analyze the Dikin walk, by first adapting it to general metrics, then devising suitable metrics for the PSD cone with affine constraints. The resulting mixing time and per-step complexity are considerably smaller, and by an appropriate choice of the metric, the dependence on the number of constraints can be made polylogarithmic. We introduce a refined notion of self-concordant matrix functions and give rules for combining
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#31867;&#21035;&#65292;&#36890;&#36807;&#24102;&#26377;&#32593;&#32476;&#21487;&#20998;&#31163;&#20132;&#20114;&#30340;&#22810;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#27169;&#22411;&#65288;MZNMGs&#65289;&#26469;&#27169;&#25311;&#38750;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#23616;&#37096;&#20132;&#20114;&#32467;&#26500;&#12290;&#20316;&#32773;&#30830;&#23450;&#20102;MG&#21487;&#34987;&#34920;&#31034;&#20026;MZNMG&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#20854;Markov CCE&#38598;&#21512;&#19982;Markov NE&#38598;&#21512;&#30456;&#31561;&#65307;&#27492;&#22806;&#65292;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;MZNMG&#20013;&#25214;&#21040;&#36817;&#20284;&#30340;Markov&#31283;&#23450;CCE&#26159;PPAD&#38590;&#39064;&#65292;&#38500;&#38750;&#32593;&#32476;&#20855;&#26377;&#8220;&#26143;&#29366;&#32467;&#26500;&#8221;&#12290;</title><link>http://arxiv.org/abs/2307.09470</link><description>&lt;p&gt;
&#24102;&#26377;&#32593;&#32476;&#21487;&#20998;&#31163;&#20132;&#20114;&#30340;&#22810;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Multi-Player Zero-Sum Markov Games with Networked Separable Interactions. (arXiv:2307.09470v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#31867;&#21035;&#65292;&#36890;&#36807;&#24102;&#26377;&#32593;&#32476;&#21487;&#20998;&#31163;&#20132;&#20114;&#30340;&#22810;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#27169;&#22411;&#65288;MZNMGs&#65289;&#26469;&#27169;&#25311;&#38750;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#23616;&#37096;&#20132;&#20114;&#32467;&#26500;&#12290;&#20316;&#32773;&#30830;&#23450;&#20102;MG&#21487;&#34987;&#34920;&#31034;&#20026;MZNMG&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#20854;Markov CCE&#38598;&#21512;&#19982;Markov NE&#38598;&#21512;&#30456;&#31561;&#65307;&#27492;&#22806;&#65292;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;MZNMG&#20013;&#25214;&#21040;&#36817;&#20284;&#30340;Markov&#31283;&#23450;CCE&#26159;PPAD&#38590;&#39064;&#65292;&#38500;&#38750;&#32593;&#32476;&#20855;&#26377;&#8220;&#26143;&#29366;&#32467;&#26500;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#31867;&#21035;&#65292;&#21363;&#24102;&#26377;&#32593;&#32476;&#21487;&#20998;&#31163;&#20132;&#20114;&#30340;&#22810;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#65288;MZNMGs&#65289;&#65292;&#20197;&#27169;&#25311;&#38750;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#23616;&#37096;&#20132;&#20114;&#32467;&#26500;&#12290;&#25105;&#20204;&#23558;MZNMG&#23450;&#20041;&#20026;&#19968;&#20010;&#27169;&#22411;&#65292;&#20854;&#20013;&#19982;&#27599;&#20010;&#29366;&#24577;&#30456;&#20851;&#30340;&#36741;&#21161;&#28216;&#25103;&#30340;&#25910;&#30410;&#26159;&#38646;&#21644;&#30340;&#65292;&#24182;&#19988;&#22312;&#26576;&#20010;&#20132;&#20114;&#32593;&#32476;&#19978;&#30340;&#37051;&#23621;&#20043;&#38388;&#20855;&#26377;&#19968;&#20123;&#21487;&#20998;&#31163;&#65288;&#21363;&#32858;&#21512;&#30697;&#38453;&#65289;&#32467;&#26500;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#33021;&#22815;&#34987;&#34920;&#31034;&#20026;MZNMG&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#65292;&#39532;&#23572;&#21487;&#22827;&#31895;&#31961;&#30456;&#20851;&#22343;&#34913;&#65288;CCE&#65289;&#30340;&#38598;&#21512;&#32553;&#20943;&#20026;&#39532;&#23572;&#21487;&#22827;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#30340;&#38598;&#21512;&#65292;&#21363;&#21069;&#32773;&#23545;&#25152;&#26377;&#29609;&#23478;&#30340;&#27599;&#20010;&#29366;&#24577;&#30340;&#36793;&#38469;&#21270;&#20056;&#31215;&#32467;&#26524;&#24471;&#21040;&#21518;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;MZNMGs&#20013;&#25214;&#21040;&#36817;&#20284;&#39532;&#23572;&#21487;&#22827;\emph{&#31283;&#23450;}CCE&#26159;PPAD&#38590;&#39064;&#65292;&#38500;&#38750;&#24213;&#23618;&#32593;&#32476;&#20855;&#26377;``&#26143;&#29366;&#32467;&#26500;''&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a new class of Markov games (MGs), \textit{Multi-player Zero-sum Markov Games} with {\it Networked separable interactions} (MZNMGs), to model the local interaction structure in non-cooperative multi-agent sequential decision-making. We define an MZNMG as a model where {the payoffs of the auxiliary games associated with each state are zero-sum and} have some separable (i.e., polymatrix) structure across the neighbors over some interaction network. We first identify the necessary and sufficient conditions under which an MG can be presented as an MZNMG, and show that the set of Markov coarse correlated equilibrium (CCE) collapses to the set of Markov Nash equilibrium (NE) in these games, in that the {product of} per-state marginalization of the former for all players yields the latter. Furthermore, we show that finding approximate Markov \emph{stationary} CCE in infinite-horizon discounted MZNMGs is \texttt{PPAD}-hard, unless the underlying network has a ``star topology''. Then, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#23725;&#22238;&#24402;&#20013;&#36807;&#25311;&#21512;&#25104;&#26412;&#65292;&#37319;&#29992;&#8220;&#19981;&#21487;&#30693;&#8221;&#30340;&#35266;&#28857;&#65292;&#20197;&#20998;&#26512;&#26679;&#26412;&#37327;&#21644;&#20219;&#21153;&#29305;&#24449;&#32467;&#26500;&#23545;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#30340;&#36807;&#24230;&#25311;&#21512;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.13185</link><description>&lt;p&gt;
(&#26680;) &#23725;&#22238;&#24402;&#20013;&#36807;&#24230;&#25311;&#21512;&#25104;&#26412;&#30340;&#19981;&#21487;&#30693;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression. (arXiv:2306.13185v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#23725;&#22238;&#24402;&#20013;&#36807;&#25311;&#21512;&#25104;&#26412;&#65292;&#37319;&#29992;&#8220;&#19981;&#21487;&#30693;&#8221;&#30340;&#35266;&#28857;&#65292;&#20197;&#20998;&#26512;&#26679;&#26412;&#37327;&#21644;&#20219;&#21153;&#29305;&#24449;&#32467;&#26500;&#23545;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#30340;&#36807;&#24230;&#25311;&#21512;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26377;&#22122;&#22768;&#30340;&#26680;&#23725;&#22238;&#24402; (KRR) &#20013;&#36807;&#25311;&#21512;&#30340;&#25104;&#26412;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#25554;&#20540;&#26080;&#23725;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#19982;&#26368;&#20248;&#35843;&#33410;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#20043;&#27604;&#12290;&#25105;&#20204;&#37319;&#29992;&#8220;&#19981;&#21487;&#30693;&#8221;&#30340;&#35266;&#28857;&#65292;&#21363;&#23545;&#20110;&#20219;&#20309;&#30446;&#26631;&#20989;&#25968;&#65292;&#21363;&#20351;&#26679;&#26412;&#37327;&#19981;&#36275;&#20197;&#36798;&#21040;&#19968;&#33268;&#24615;&#25110;&#30446;&#26631;&#20989;&#25968;&#19981;&#22312; RKHS &#20013;&#65292;&#25105;&#20204;&#20063;&#23558;&#25104;&#26412;&#30475;&#20316;&#26679;&#26412;&#37327;&#30340;&#20989;&#25968;&#12290;&#20351;&#29992;&#26368;&#36817;&#25512;&#23548;&#20986;&#30340;&#65288;&#38750;&#20005;&#26684;&#30340;&#65289;&#39118;&#38505;&#35780;&#20272;&#65292;&#20197;&#20219;&#21153;&#29305;&#24449;&#32467;&#26500;&#20026;&#22522;&#30784;&#65292;&#21033;&#29992;&#39640;&#26031;&#26222;&#36866;&#24615;&#20551;&#35774;&#20998;&#26512;&#36807;&#24230;&#25311;&#21512;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#33391;&#24615;&#12289;&#32531;&#21644;&#21644;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#65288;&#21442;&#35265; Mallinar &#31561;&#20154; 2022&#65289;&#30340;&#26356;&#31934;&#32454;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the cost of overfitting in noisy kernel ridge regression (KRR), which we define as the ratio between the test error of the interpolating ridgeless model and the test error of the optimally-tuned model. We take an "agnostic" view in the following sense: we consider the cost as a function of sample size for any target function, even if the sample size is not large enough for consistency or the target is outside the RKHS. We analyze the cost of overfitting under a Gaussian universality ansatz using recently derived (non-rigorous) risk estimates in terms of the task eigenstructure. Our analysis provides a more refined characterization of benign, tempered and catastrophic overfitting (qv Mallinar et al. 2022).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#21644;&#24037;&#20154;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04366</link><description>&lt;p&gt;
&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing Based on GCN Trustworthiness Prediction. (arXiv:2306.04366v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#21644;&#24037;&#20154;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#21487;&#20197;&#36890;&#36807;&#20419;&#36827;&#20219;&#21153;&#24863;&#30693;&#30340;&#22242;&#38431;&#21512;&#20316;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#32780;&#24037;&#20154;&#25307;&#21215;&#21017;&#20195;&#34920;&#30528;&#19968;&#20010;&#22797;&#26434;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#31574;&#30053;&#20027;&#35201;&#20851;&#27880;&#24037;&#20154;&#26412;&#36523;&#30340;&#29305;&#24449;&#65292;&#24573;&#30053;&#20102;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20219;&#21153;&#25928;&#29992;&#35780;&#20272;&#30340;&#21512;&#29702;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;Mini-Batch K-Means&#32858;&#31867;&#31639;&#27861;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#24037;&#20154;&#25307;&#21215;&#12290;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#21644;&#20219;&#21153;&#35201;&#27714;&#33719;&#24471;&#24037;&#20154;&#30340;&#33021;&#21147;&#31867;&#22411;&#21644;&#36317;&#31163;&#12290;&#20351;&#29992;&#24037;&#20154;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20449;&#20219;&#23548;&#21521;&#22270;&#36755;&#20837;&#33267;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26694;&#26550;&#36827;&#34892;&#35757;&#32451;&#65292;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#12290;&#36890;&#36807;&#24037;&#20154;&#20043;&#38388;&#30340;&#39640;&#20449;&#20219;&#20540;&#65292;&#38450;&#27490;CMCS&#22330;&#26223;&#19979;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26368;&#32456;&#65292;&#21033;&#29992;&#39044;&#27979;&#30340;&#20449;&#20219;&#21644;&#24037;&#20154;&#33021;&#21147;&#26500;&#24314;&#20102;&#19968;&#20010;&#26080;&#21521;&#25307;&#21215;&#22270;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#25307;&#21215;&#26041;&#27861;&#22312;&#25307;&#21215;&#20934;&#30830;&#24230;&#12289;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative Mobile Crowd Sensing (CMCS) enhances data quality and coverage by promoting teamwork in task sensing, with worker recruitment representing a complex multi-objective optimization problem. Existing strategies mainly focus on the characteristics of workers themselves, neglecting the asymmetric trust relationships between them, which affects the rationality of task utility evaluation. To address this, this paper first employs the Mini-Batch K-Means clustering algorithm and deploys edge servers to enable efficient distributed worker recruitment. Historical data and task requirements are utilized to obtain workers' ability types and distances. A trust-directed graph in the worker's social network is input into the Graph Convolutional Network (GCN) framework for training, capturing asymmetric trustworthiness between worker pairs. Privacy leakage is prevented in CMCS scenarios through high trust values between workers. Ultimately, an undirected recruitment graph is constructed us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#26032;&#26679;&#26412;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#32463;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#22810;&#31181;&#20219;&#21153;&#23454;&#39564;&#39564;&#35777;&#65292;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14521</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Eliminating Spurious Correlations from Pre-trained Models via Data Mixing. (arXiv:2305.14521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#26032;&#26679;&#26412;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#32463;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#22810;&#31181;&#20219;&#21153;&#23454;&#39564;&#39564;&#35777;&#65292;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25910;&#25947;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#21033;&#29992;&#20102;&#26576;&#20123;&#23646;&#24615;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#22312;&#29305;&#23450;&#31867;&#21035;&#30340;&#22823;&#22810;&#25968;&#31034;&#20363;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#24182;&#19981;&#36275;&#20197;&#39044;&#27979;&#36825;&#20123;&#31867;&#21035;&#12290;&#23398;&#21040;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21487;&#33021;&#20250;&#22312;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21518;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#20250;&#38477;&#20302;&#27169;&#22411;&#23545;&#19981;&#23637;&#29616;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#31034;&#20363;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#19968;&#23567;&#32452;&#24102;&#26377;&#34394;&#20551;&#23646;&#24615;&#30340;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#24179;&#34913;&#25152;&#26377;&#31867;&#21035;&#20013;&#30340;&#34394;&#20551;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#65292;&#21253;&#25324;&#28040;&#38500;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models pre-trained on large datasets have achieved remarkable convergence and robustness properties. However, these models often exploit spurious correlations between certain attributes and labels, which are prevalent in the majority of examples within specific categories but are not predictive of these categories in general. The learned spurious correlations may persist even after fine-tuning on new data, which degrades models' performance on examples that do not exhibit the spurious correlation. In this work, we propose a simple and highly effective method to eliminate spurious correlations from pre-trained models. The key idea of our method is to leverage a small set of examples with spurious attributes, and balance the spurious attributes across all classes via data mixing. We theoretically confirm the effectiveness of our method, and empirically demonstrate its state-of-the-art performance on various vision and NLP tasks, including eliminating spurious correlation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#25193;&#23637;&#20197;&#36866;&#24212;&#22823;&#25968;&#25454;&#21644;&#39640;&#32500;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.05080</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#26102;&#20195;&#30340;&#22320;&#29699;&#31227;&#21160;&#32773;: &#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Earth Movers in The Big Data Era: A Review of Optimal Transport in Machine Learning. (arXiv:2305.05080v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#25193;&#23637;&#20197;&#36866;&#24212;&#22823;&#25968;&#25454;&#21644;&#39640;&#32500;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;(OT)&#26159;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;,&#39318;&#27425;&#20986;&#29616;&#20110;18&#19990;&#32426;,&#24182;&#24341;&#21457;&#20986;&#22823;&#37327;&#26041;&#27861;&#26469;&#22238;&#31572;&#35768;&#22810;&#29702;&#35770;&#21644;&#24212;&#29992;&#38382;&#39064;&#12290;&#36807;&#21435;&#30340;&#21313;&#24180;&#35265;&#35777;&#20102;&#36825;&#20010;&#32463;&#20856;&#20248;&#21270;&#38382;&#39064;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#26174;&#30528;&#36129;&#29486;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20351;&#29992;&#26041;&#24335;&#21450;&#20854;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;&#22312;&#19987;&#39064;&#19982;&#32972;&#26223;&#30340;&#20801;&#35768;&#19979;,&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#20840;&#38754;&#35843;&#26597;,&#24182;&#30830;&#20445;&#20854;&#21576;&#29616;&#20855;&#26377;&#21487;&#35775;&#38382;&#24615;&#12290;&#39318;&#20808;,&#25105;&#20204;&#35299;&#37322;&#20102;&#26368;&#20248;&#36755;&#36816;&#30340;&#32972;&#26223;,&#24182;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#31867;&#22411;&#12289;&#29305;&#24615;&#21644;&#26174;&#33879;&#24212;&#29992;&#12290;&#28982;&#21518;,&#25105;&#20204;&#30528;&#37325;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#26368;&#20248;&#36755;&#36816;&#25193;&#23637;&#20197;&#24212;&#23545;&#24403;&#21069;&#22823;&#25968;&#25454;&#21644;&#39640;&#32500;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23545;&#29992;&#20110;&#25193;&#23637;OT&#30340;&#25991;&#29486;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;,&#24182;&#20197;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#21576;&#29616;&#32467;&#26524;&#20197;&#20419;&#36827;&#29702;&#35299;&#12290;&#26368;&#21518;,&#25105;&#20204;&#25506;&#35752;&#20102;&#21487;&#25193;&#23637;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#20123;&#26368;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport (OT) is a mathematical framework that first emerged in the eighteenth century and has led to a plethora of methods for answering many theoretical and applied questions. The last decade is a witness of the remarkable contributions of this classical optimization problem to machine learning. This paper is about where and how optimal transport is used in machine learning with a focus on the question of salable optimal transport. We provide a comprehensive survey of optimal transport while ensuring an accessible presentation as permitted by the nature of the topic and the context. First, we explain optimal transport background and introduce different flavors (i.e. mathematical formulations), properties, and notable applications. We then address the fundamental question of how to scale optimal transport to cope with the current demands of big and high dimensional data. We conduct a systematic analysis of the methods used in the literature for scaling OT and present the find
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#36890;&#36807;&#27169;&#22359;&#21270;&#23398;&#20064;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#65292;&#36171;&#20104;LLMs&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#22312;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.14178</link><description>&lt;p&gt;
mPLUG-Owl: &#27169;&#22359;&#21270;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality. (arXiv:2304.14178v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#36890;&#36807;&#27169;&#22359;&#21270;&#23398;&#20064;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#65292;&#36171;&#20104;LLMs&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#22312;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#22312;&#21508;&#31181;&#24320;&#25918;&#24335;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23558;LLMs&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;mPLUG-Owl&#65292;&#36890;&#36807;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#30340;&#27169;&#22359;&#21270;&#23398;&#20064;&#65292;&#20351;LLMs&#20855;&#22791;&#20102;&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#27169;&#24577;&#65292;&#24182;&#36890;&#36807;&#27169;&#24577;&#21327;&#20316;&#20419;&#36827;&#20102;&#22810;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#21253;&#25324;&#29992;&#20110;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLM&#30340;&#36741;&#21161;&#23398;&#20064;&#35270;&#35273;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#29978;&#33267;&#25913;&#36827;&#20102;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#20351;&#29992;&#20923;&#32467;&#30340;LLM&#27169;&#22359;&#23545;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#25277;&#35937;&#22120;&#27169;&#22359;&#36827;&#34892;&#35757;&#32451;&#20197;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#20351;&#29992;&#20165;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30417;&#30563;&#25968;&#25454;&#38598;&#20849;&#21516;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23545;&#20110;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#32791;&#24863;&#30693;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#34920;&#26684;&#22522;&#20934; EC-NAS&#65292;&#35813;&#22522;&#20934;&#36890;&#36807;&#28155;&#21152;&#33021;&#32791;&#21644;&#30899;&#36275;&#36857;&#20449;&#24687;&#65292;&#25903;&#25345;&#35774;&#35745;&#33021;&#25928;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#38477;&#20302;&#24635;&#33021;&#32791;&#12290;</title><link>http://arxiv.org/abs/2210.06015</link><description>&lt;p&gt;
EC-NAS: &#38754;&#21521;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#33021;&#32791;&#24863;&#30693;&#34920;&#26684;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural Architecture Search. (arXiv:2210.06015v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06015
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#32791;&#24863;&#30693;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#34920;&#26684;&#22522;&#20934; EC-NAS&#65292;&#35813;&#22522;&#20934;&#36890;&#36807;&#28155;&#21152;&#33021;&#32791;&#21644;&#30899;&#36275;&#36857;&#20449;&#24687;&#65292;&#25903;&#25345;&#35774;&#35745;&#33021;&#25928;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#38477;&#20302;&#24635;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36873;&#25321;&#12289;&#35757;&#32451;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#33021;&#37327;&#28040;&#32791;&#19981;&#26029;&#22686;&#21152;&#12290;&#26412;&#25991;&#26088;&#22312;&#25903;&#25345;&#35774;&#35745;&#33021;&#25928;&#39640;&#12289;&#35757;&#32451;&#36164;&#28304;&#28040;&#32791;&#36739;&#20302;&#12289;&#36866;&#29992;&#20110;&#23454;&#38469;&#36793;&#32536;/&#31227;&#21160;&#35745;&#31639;&#29615;&#22659;&#24182;&#20855;&#26377;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#33021;&#25928;&#20316;&#20026;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034; (NAS) &#30340;&#19968;&#39033;&#39069;&#22806;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#19981;&#21516;&#26550;&#26500;&#30340;&#33021;&#32791;&#21644;&#30899;&#36275;&#36857;&#20449;&#24687;&#65292;&#25552;&#20379;&#26356;&#26032;&#30340;&#34920;&#26684;&#22522;&#20934; EC-NAS &#20197;&#22312;&#36739;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#35780;&#20272; NAS &#31574;&#30053;&#12290;EC-NAS &#36824;&#21253;&#25324;&#29992;&#20110;&#39044;&#27979;&#33021;&#32791;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#26377;&#21161;&#20110;&#38477;&#20302;&#24635;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy consumption from selecting, training and deploying deep learning models has continued to increase over the past few years. Our goal in this work is to support the design of energy-efficient deep learning models that are easier to train with lower compute resources, practical to deploy in real-world edge/mobile computing settings and environmentally sustainable. Tabular benchmarks for neural architecture search (NAS) allow the evaluation of NAS strategies at lower computational cost by providing pre-computed performance statistics. In this work, we suggest including energy efficiency as an additional performance criterion to NAS and present an updated tabular benchmark by including information on energy consumption and carbon footprint for different architectures. The benchmark called EC-NAS is made available open-source to support energy consumption-aware NAS research. EC-NAS also includes a surrogate model for predicting energy consumption, and helps us reduce the overall energ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#8220;Merlin-Arthur&#8221;&#21327;&#35758;&#30340;&#21551;&#21457;&#65292;&#22312;&#19981;&#20551;&#35774;&#26368;&#20248;&#26234;&#33021;&#20307;&#25110;&#29305;&#24449;&#29420;&#31435;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#30456;&#23545;&#24378;&#24230;&#21644;&#8220;&#38750;&#23545;&#31216;&#29305;&#24449;&#30456;&#20851;&#24615;&#8221;&#27010;&#24565;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#31934;&#30830;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#21487;&#35299;&#37322;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2206.00759</link><description>&lt;p&gt;
Merlin-Arthur&#20998;&#31867;&#22120;&#30340;&#24418;&#24335;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Formal Interpretability with Merlin-Arthur Classifiers. (arXiv:2206.00759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00759
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#8220;Merlin-Arthur&#8221;&#21327;&#35758;&#30340;&#21551;&#21457;&#65292;&#22312;&#19981;&#20551;&#35774;&#26368;&#20248;&#26234;&#33021;&#20307;&#25110;&#29305;&#24449;&#29420;&#31435;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#30456;&#23545;&#24378;&#24230;&#21644;&#8220;&#38750;&#23545;&#31216;&#29305;&#24449;&#30456;&#20851;&#24615;&#8221;&#27010;&#24565;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#31934;&#30830;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#21487;&#35299;&#37322;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#20998;&#31867;&#22120;&#65292;&#21363;&#20351;&#26159;&#20687;&#31070;&#32463;&#32593;&#32476;&#36825;&#26679;&#30340;&#22797;&#26434;&#26234;&#33021;&#20307;&#20063;&#33021;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#21487;&#35299;&#37322;&#24615;&#20445;&#35777;&#12290;&#36825;&#20123;&#20445;&#35777;&#21253;&#25324;&#23545;&#27492;&#20998;&#31867;&#22120;&#36873;&#25321;&#30340;&#29305;&#24449;&#20043;&#38388;&#20114;&#20449;&#24687;&#30340;&#19978;&#19979;&#30028;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21463;&#20132;&#20114;&#24335;&#35777;&#26126;&#31995;&#32479;&#20013; Merlin-Arthur &#21327;&#35758;&#30340;&#21551;&#21457;&#65292;&#24182;&#20197;&#21487;&#27979;&#37327;&#30340;&#25351;&#26631;&#65288;&#22914;&#22768;&#38899;&#21644;&#23436;&#25972;&#24615;&#65289;&#34920;&#36798;&#20102;&#36825;&#20123;&#32422;&#26463;&#12290;&#19982;&#29616;&#26377;&#30340;&#20132;&#20114;&#24335;&#35774;&#32622;&#30456;&#27604;&#65292;&#25105;&#20204;&#19981;&#20381;&#36182;&#20110;&#26368;&#20248;&#26234;&#33021;&#20307;&#25110;&#29305;&#24449;&#29420;&#31435;&#20998;&#24067;&#30340;&#20551;&#35774;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#26234;&#33021;&#20307;&#30340;&#30456;&#23545;&#24378;&#24230;&#20197;&#21450;&#26032;&#30340;&#8220;&#38750;&#23545;&#31216;&#29305;&#24449;&#30456;&#20851;&#24615;&#8221;&#27010;&#24565;&#26469;&#25429;&#25417;&#20351;&#21487;&#35299;&#37322;&#24615;&#20445;&#35777;&#22256;&#38590;&#30340;&#31934;&#30830;&#30456;&#20851;&#24615;&#31867;&#22411;&#12290; &#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#23454;&#39564;&#26469;&#27979;&#35797;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#23454;&#39564;&#21487;&#39564;&#35777;&#39640;&#20114;&#20449;&#24687;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new type of multi-agent interactive classifier that provides provable interpretability guarantees even for complex agents such as neural networks. These guarantees consist of bounds on the mutual information of the features selected by this classifier. Our results are inspired by the Merlin-Arthur protocol from Interactive Proof Systems and express these bounds in terms of measurable metrics such as soundness and completeness. Compared to existing interactive setups we do not rely on optimal agents or on the assumption that features are distributed independently. Instead, we use the relative strength of the agents as well as the new concept of Asymmetric Feature Correlation which captures the precise kind of correlations that make interpretability guarantees difficult. %relates the information carried by sets of features to one of the individual features. We test our results through numerical experiments on two small-scale datasets where high mutual information can be veri
&lt;/p&gt;</description></item></channel></rss>