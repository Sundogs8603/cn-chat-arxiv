<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRESTO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26144;&#23556;&#20381;&#36182;&#20110;&#28508;&#22312;&#34920;&#31034;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#20803;&#23431;&#23449;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#25345;&#32493;&#21516;&#35843;&#26469;&#27979;&#37327;&#28508;&#22312;&#31354;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#32479;&#35745;&#25512;&#29702;&#23427;&#20204;&#30340;&#20998;&#24067;&#12290;&#21487;&#20197;&#29992;&#20110;&#25935;&#24863;&#24615;&#20998;&#26512;&#12289;&#26816;&#27979;&#24322;&#24120;&#23884;&#20837;&#21644;&#39640;&#25928;&#23548;&#33322;&#36229;&#21442;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01514</link><description>&lt;p&gt;
&#26144;&#23556;&#28508;&#22312;&#34920;&#31034;&#30340;&#22810;&#20803;&#23431;&#23449;
&lt;/p&gt;
&lt;p&gt;
Mapping the Multiverse of Latent Representations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01514
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRESTO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26144;&#23556;&#20381;&#36182;&#20110;&#28508;&#22312;&#34920;&#31034;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#20803;&#23431;&#23449;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#25345;&#32493;&#21516;&#35843;&#26469;&#27979;&#37327;&#28508;&#22312;&#31354;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#32479;&#35745;&#25512;&#29702;&#23427;&#20204;&#30340;&#20998;&#24067;&#12290;&#21487;&#20197;&#29992;&#20110;&#25935;&#24863;&#24615;&#20998;&#26512;&#12289;&#26816;&#27979;&#24322;&#24120;&#23884;&#20837;&#21644;&#39640;&#25928;&#23548;&#33322;&#36229;&#21442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21709;&#24212;&#26368;&#36817;&#23545;&#36890;&#36807;&#22810;&#20803;&#23431;&#23449;&#20998;&#26512;&#26469;&#24212;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#38382;&#39064;&#30340;&#21628;&#21505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PRESTO&#65292;&#19968;&#31181;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26144;&#23556;&#20381;&#36182;&#20110;&#28508;&#22312;&#34920;&#31034;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#20803;&#23431;&#23449;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#23545;&#23427;&#20204;&#23884;&#20837;&#30340;&#21464;&#24322;&#24615;&#20173;&#28982;&#19981;&#34987;&#20805;&#20998;&#29702;&#35299;&#65292;&#23548;&#33268;&#20102;&#19981;&#24517;&#35201;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#21487;&#38752;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#25345;&#32493;&#21516;&#35843;&#26469;&#34920;&#24449;&#19981;&#21516;&#32452;&#21512;&#30340;&#22810;&#26679;&#21270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12289;(&#36229;)&#21442;&#25968;&#37197;&#32622;&#21644;&#25968;&#25454;&#38598;&#25152;&#20135;&#29983;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#27979;&#37327;&#23427;&#20204;&#20043;&#38388;&#30340;&#25104;&#23545;(&#38750;)&#30456;&#20284;&#24615;&#24182;&#23545;&#20854;&#20998;&#24067;&#36827;&#34892;&#32479;&#35745;&#25512;&#29702;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#20445;&#25345;&#20102;&#28508;&#22312;&#34920;&#31034;&#38598;&#21512;&#30340;&#29702;&#24819;&#29305;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#36827;&#34892;&#25935;&#24863;&#24615;&#20998;&#26512;&#12289;&#26816;&#27979;&#24322;&#24120;&#23884;&#20837;&#25110;&#39640;&#25928;&#26377;&#25928;&#22320;&#23548;&#33322;&#36229;&#21442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Echoing recent calls to counter reliability and robustness concerns in machine learning via multiverse analysis, we present PRESTO, a principled framework for mapping the multiverse of machine-learning models that rely on latent representations. Although such models enjoy widespread adoption, the variability in their embeddings remains poorly understood, resulting in unnecessary complexity and untrustworthy representations. Our framework uses persistent homology to characterize the latent spaces arising from different combinations of diverse machine-learning methods, (hyper)parameter configurations, and datasets, allowing us to measure their pairwise (dis)similarity and statistically reason about their distributions. As we demonstrate both theoretically and empirically, our pipeline preserves desirable properties of collections of latent representations, and it can be leveraged to perform sensitivity analysis, detect anomalous embeddings, or efficiently and effectively navigate hyperpa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20165;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#20294;&#24573;&#30053;&#20102;&#20391;&#38142;&#21407;&#23376;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#36328;&#24230;&#25513;&#30721;&#65292;&#20197;&#22312;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#20013;&#21516;&#26102;&#24314;&#27169;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#30340;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#27531;&#22522;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01481</link><description>&lt;p&gt;
&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Multi-level protein pre-training with Vabs-Net
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01481
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20165;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#20294;&#24573;&#30053;&#20102;&#20391;&#38142;&#21407;&#23376;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#36328;&#24230;&#25513;&#30721;&#65292;&#20197;&#22312;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#20013;&#21516;&#26102;&#24314;&#27169;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#30340;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#27531;&#22522;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#19977;&#32500;&#32467;&#26500;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#21457;&#23637;&#36805;&#29467;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#21363;&#945;&#30899;&#21407;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#21407;&#23376;&#65292;&#22914;&#20391;&#38142;&#21407;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#19978;&#23545;&#34507;&#30333;&#36136;&#36827;&#34892;&#24314;&#27169;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20391;&#38142;&#21407;&#23376;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#20998;&#23376;&#23545;&#25509;&#65289;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#39044;&#35757;&#32451;&#20013;&#22825;&#30495;&#22320;&#32452;&#21512;&#27531;&#22522;&#21644;&#21407;&#23376;&#20449;&#24687;&#36890;&#24120;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20449;&#24687;&#27844;&#28431;&#26159;&#21253;&#21547;&#21407;&#23376;&#32467;&#26500;&#30340;&#36755;&#20837;&#23548;&#33268;&#27531;&#22522;&#32423;&#39044;&#35757;&#32451;&#20219;&#21153;&#21464;&#24471;&#29712;&#30862;&#24182;&#23548;&#33268;&#27531;&#22522;&#34920;&#31034;&#19981;&#22815;&#20805;&#20998;&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#24230;&#25513;&#30721;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KTO&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;KTO&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#12290;&#22312;&#22810;&#20010;&#35268;&#27169;&#19978;&#65292;KTO&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01306</link><description>&lt;p&gt;
KTO: &#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
KTO: Model Alignment as Prospect Theoretic Optimization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KTO&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;KTO&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#12290;&#22312;&#22810;&#20010;&#35268;&#27169;&#19978;&#65292;KTO&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20975;&#24681;&#26364;&#19982;&#29305;&#27779;&#26031;&#22522;&#30340;&#23637;&#26395;&#29702;&#35770;&#21578;&#35785;&#25105;&#20204;&#65292;&#20154;&#31867;&#20197;&#26377;&#20559;&#35265;&#20294;&#26126;&#30830;&#30340;&#26041;&#24335;&#30475;&#24453;&#38543;&#26426;&#21464;&#37327;&#65307;&#20363;&#22914;&#65292;&#20154;&#20204;&#36890;&#24120;&#37117;&#26159;&#21388;&#24694;&#25439;&#22833;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;LLMs&#19982;&#20154;&#24037;&#21453;&#39304;&#36827;&#34892;&#23545;&#40784;&#30340;&#30446;&#26631;&#38544;&#21547;&#22320;&#34701;&#21512;&#20102;&#35768;&#22810;&#36825;&#20123;&#20559;&#35265; - &#36825;&#20123;&#30446;&#26631; (&#20363;&#22914; DPO) &#30340;&#25104;&#21151;&#37096;&#20998;&#21487;&#24402;&#22240;&#20110;&#23427;&#20204;&#26159;"&#20154;&#31867;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;"(HALOs)&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#25152;&#24402;&#22240;&#32473;&#20154;&#31867;&#30340;&#25928;&#29992;&#20989;&#25968;&#20173;&#19982;&#23637;&#26395;&#29702;&#35770;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#12290;&#21033;&#29992;&#20975;&#24681;&#26364;-&#29305;&#27779;&#26031;&#22522;&#20154;&#31867;&#25928;&#29992;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#30340;HALO&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#20975;&#24681;&#26364;-&#29305;&#27779;&#26031;&#22522;&#20248;&#21270;(KTO)&#65292;&#24182;&#19988;&#23427;&#22312;&#20174;1B&#21040;30B&#30340;&#35268;&#27169;&#19978;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#25110;&#36229;&#36807;&#12290;&#20851;&#38190;&#26159;&#65292;KTO&#19981;&#38656;&#35201;&#20559;&#22909; - &#21482;&#38656;&#35201;&#19968;&#20010;&#26159;&#21542;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kahneman &amp; Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being $\textit{human-aware loss functions}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences -- only a binary signal of whether 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#38750;&#32447;&#24615;&#29305;&#24449;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#22343;&#22330;&#21644;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#30340;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#21442;&#25968;&#20998;&#24067;&#30340;&#25439;&#22833;&#26223;&#35266;&#34429;&#28982;&#39640;&#24230;&#38750;&#20984;&#65292;&#20294;&#21464;&#24471;&#30456;&#24403;&#28201;&#21644;&#65292;&#24182;&#24314;&#31435;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#33719;&#24471;&#20855;&#20307;&#30340;&#25913;&#36827;&#36895;&#29575;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01258</link><description>&lt;p&gt;
Transformers&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#38750;&#32447;&#24615;&#29305;&#24449;&#65306;&#20851;&#20110;&#27880;&#24847;&#21147;&#22330;&#26223;&#20013;&#30340;&#38750;&#20984;&#22343;&#22330;&#21160;&#24577;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#38750;&#32447;&#24615;&#29305;&#24449;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#22343;&#22330;&#21644;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#30340;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#21442;&#25968;&#20998;&#24067;&#30340;&#25439;&#22833;&#26223;&#35266;&#34429;&#28982;&#39640;&#24230;&#38750;&#20984;&#65292;&#20294;&#21464;&#24471;&#30456;&#24403;&#28201;&#21644;&#65292;&#24182;&#24314;&#31435;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#33719;&#24471;&#20855;&#20307;&#30340;&#25913;&#36827;&#36895;&#29575;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#19968;&#29616;&#35937;&#20135;&#29983;&#30340;&#29616;&#26377;&#29702;&#35770;&#30740;&#31350;&#20165;&#38480;&#20110;&#23545;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#21333;&#23618;&#27880;&#24847;&#21147;&#30340;&#21160;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#30001;&#20840;&#36830;&#25509;&#23618;&#21644;&#32447;&#24615;&#27880;&#24847;&#21147;&#23618;&#32452;&#25104;&#30340;Transformer&#30340;&#20248;&#21270;&#12290;MLP&#20805;&#24403;&#20102;&#19968;&#20010;&#24120;&#35265;&#30340;&#38750;&#32447;&#24615;&#34920;&#31034;&#25110;&#29305;&#24449;&#26144;&#23556;&#65292;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22343;&#22330;&#21644;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#21442;&#25968;&#20998;&#24067;&#30340;&#26080;&#38480;&#32500;&#25439;&#22833;&#26223;&#35266;&#65292;&#34429;&#28982;&#39640;&#24230;&#38750;&#20984;&#65292;&#20294;&#21464;&#24471;&#30456;&#24403;&#28201;&#21644;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#22343;&#22330;&#21160;&#24577;&#30340;&#20108;&#38454;&#31283;&#23450;&#24615;&#65292;&#24182;&#34920;&#26126;Wasserstein&#26799;&#24230;&#27969;&#20960;&#20046;&#24635;&#26159;&#36991;&#24320;&#38797;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#33719;&#24471;&#36828;&#31163;&#20020;&#30028;&#28857;&#21644;&#25509;&#36817;&#20020;&#30028;&#28857;&#30340;&#20855;&#20307;&#25913;&#36827;&#36895;&#29575;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models based on the Transformer architecture have demonstrated impressive capabilities to learn in context. However, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks. In this paper, we study the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer. The MLP acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning. We prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign. We also analyze the second-order stability of mean-field dynamics and show that Wasserstein gradient flow almost always avoids saddle points. Furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points. This represents the 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#20598;&#20132;&#20114;&#39044;&#27979;&#21644;&#31934;&#31616;&#30340;MPC&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20132;&#36890;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;12&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01116</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22810;&#27169;&#22411;MPC&#30340;&#22522;&#20110;&#23545;&#20598;&#20132;&#20114;&#39044;&#27979;&#30340;&#23618;&#32423;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-modal Model Predictive Control via Duality-based Interaction Predictions
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01116
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#20598;&#20132;&#20114;&#39044;&#27979;&#21644;&#31934;&#31616;&#30340;MPC&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20132;&#36890;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;12&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20132;&#36890;&#22330;&#26223;&#20013;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#12290;&#35813;&#26550;&#26500;&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;1) RAID-Net&#65292;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#39062;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#24615;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19982;&#21608;&#22260;&#36710;&#36742;&#20043;&#38388;&#22312;MPC&#39044;&#27979;&#33539;&#22260;&#20869;&#30340;&#30456;&#20851;&#20132;&#20114;&#65307;2) &#19968;&#20010;&#31616;&#21270;&#30340;&#38543;&#26426;MPC&#38382;&#39064;&#65292;&#28040;&#38500;&#19981;&#30456;&#20851;&#30340;&#36991;&#30896;&#32422;&#26463;&#65292;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#27169;&#25311;&#20132;&#36890;&#36335;&#21475;&#20013;&#28436;&#31034;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#30340;12&#20493;&#36895;&#25552;&#21319;&#12290;&#24744;&#21487;&#20197;&#22312;&#36825;&#37324;&#25214;&#21040;&#23637;&#31034;&#35813;&#26550;&#26500;&#22312;&#22810;&#20010;&#22797;&#26434;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#35270;&#39057;&#65306;https://youtu.be/-TcMeolCLWc
&lt;/p&gt;
&lt;p&gt;
We propose a hierarchical architecture designed for scalable real-time Model Predictive Control (MPC) in complex, multi-modal traffic scenarios. This architecture comprises two key components: 1) RAID-Net, a novel attention-based Recurrent Neural Network that predicts relevant interactions along the MPC prediction horizon between the autonomous vehicle and the surrounding vehicles using Lagrangian duality, and 2) a reduced Stochastic MPC problem that eliminates irrelevant collision avoidance constraints, enhancing computational efficiency. Our approach is demonstrated in a simulated traffic intersection with interactive surrounding vehicles, showcasing a 12x speed-up in solving the motion planning problem. A video demonstrating the proposed architecture in multiple complex traffic scenarios can be found here: https://youtu.be/-TcMeolCLWc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Transformer&#20013;&#25552;&#21462;&#22270;&#24418;&#20197;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;&#30340;&#36731;&#37327;&#32423;&#21333;&#38454;&#27573;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#21462;&#20102;&#20851;&#31995;&#22270;&#12290;</title><link>https://arxiv.org/abs/2404.02072</link><description>&lt;p&gt;
&#20174;Transformer&#20013;&#25552;&#21462;&#22270;&#24418;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
EGTR: Extracting Graph from Transformer for Scene Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02072
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Transformer&#20013;&#25552;&#21462;&#22270;&#24418;&#20197;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;&#30340;&#36731;&#37327;&#32423;&#21333;&#38454;&#27573;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#21462;&#20102;&#20851;&#31995;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;SGG&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#26816;&#27979;&#23545;&#35937;&#24182;&#39044;&#27979;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21333;&#38454;&#27573;SGG&#27169;&#22411;&#65292;&#23427;&#20174;DETR&#35299;&#30721;&#22120;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#23618;&#20013;&#23398;&#20064;&#30340;&#21508;&#31181;&#20851;&#31995;&#20013;&#25552;&#21462;&#20851;&#31995;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02072v1 Announce Type: cross  Abstract: Scene Graph Generation (SGG) is a challenging task of detecting objects and predicting relationships between objects. After DETR was developed, one-stage SGG models based on a one-stage object detector have been actively studied. However, complex modeling is used to predict the relationship between objects, and the inherent relationship between object queries learned in the multi-head self-attention of the object detector has been neglected. We propose a lightweight one-stage SGG model that extracts the relation graph from the various relationships learned in the multi-head self-attention layers of the DETR decoder. By fully utilizing the self-attention by-products, the relation graph can be extracted effectively with a shallow relation extraction head. Considering the dependency of the relation extraction task on the object detection task, we propose a novel relation smoothing technique that adjusts the relation label adaptively accor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;Hessian-free&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#20165;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2404.01712</link><description>&lt;p&gt;
&#36890;&#36807;&#20813;Hessian&#37325;&#26032;&#25972;&#21512;&#20010;&#20307;&#25968;&#25454;&#32479;&#35745;&#23454;&#29616;&#39640;&#25928;&#22312;&#32447;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;Hessian-free&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#20165;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26088;&#22312;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#24536;&#35760;&#29305;&#23450;&#25968;&#25454;&#26469;&#32500;&#25252;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#34987;&#36951;&#24536;&#26435;&#21033;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#34920;&#26126;&#65292;&#19968;&#31181;&#25968;&#25454;&#36951;&#24536;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#39044;&#20808;&#35745;&#31639;&#21644;&#23384;&#20648;&#25658;&#24102;&#20108;&#38454;&#20449;&#24687;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20197;&#25913;&#36827;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#33499;&#21051;&#30340;&#20551;&#35774;&#65292;&#32780;&#19988;&#35745;&#31639;/&#23384;&#20648;&#21463;&#21040;&#27169;&#22411;&#21442;&#25968;&#32500;&#24230;&#30340;&#35781;&#21650;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#24212;&#29992;&#21040;&#22823;&#22810;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;Hessian&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#12290;&#25105;&#20204;&#24314;&#35758;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#32500;&#25252;&#19968;&#20010;&#32479;&#35745;&#21521;&#37327;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#21644;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#20223;&#23556;&#38543;&#26426;&#36882;&#24402;&#36924;&#36817;&#26469;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;&#22522;&#20110;&#37325;&#26032;&#25910;&#38598;&#36951;&#24536;&#25968;&#25454;&#32479;&#35745;&#30340;&#31574;&#30053;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01712v1 Announce Type: cross  Abstract: Machine unlearning strives to uphold the data owners' right to be forgotten by enabling models to selectively forget specific data. Recent methods suggest that one approach of data forgetting is by precomputing and storing statistics carrying second-order information to improve computational and memory efficiency. However, they rely on restrictive assumptions and the computation/storage suffer from the curse of model parameter dimensionality, making it challenging to apply to most deep neural networks. In this work, we propose a Hessian-free online unlearning method. We propose to maintain a statistical vector for each data point, computed through affine stochastic recursion approximation of the difference between retrained and learned models. Our proposed algorithm achieves near-instantaneous online unlearning as it only requires a vector addition operation. Based on the strategy that recollecting statistics for forgetting data, the p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;CHAIN&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;GANs&#20013;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00521</link><description>&lt;p&gt;
CHAIN&#65306;&#36890;&#36807;&#21463;&#38480;&#21807;&#19968;&#24615;&#36830;&#32493;&#24615;&#35268;&#33539;&#21270;&#22686;&#24378;&#25968;&#25454;&#39640;&#25928;GANs&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;CHAIN&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;GANs&#20013;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26174;&#30528;&#25512;&#21160;&#20102;&#22270;&#20687;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;GANs&#32463;&#24120;&#38754;&#20020;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#35782;&#21035;Batch Normalization&#65288;BN&#65289;&#20013;&#30340;&#20851;&#38190;&#32570;&#38519;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65306;&#22312;&#20013;&#24515;&#21270;&#21644;&#32553;&#25918;&#27493;&#39588;&#20013;&#26799;&#24230;&#29190;&#28856;&#30340;&#20542;&#21521;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CHAIN&#65288;&#21463;&#38480;&#21807;&#19968;&#24615;&#36830;&#32493;&#24615;&#35268;&#33539;&#21270;&#65289;&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#20013;&#24515;&#21270;&#27493;&#39588;&#26367;&#25442;&#20026;&#38646;&#22343;&#20540;&#27491;&#21017;&#21270;&#65292;&#24182;&#22312;&#32553;&#25918;&#27493;&#39588;&#20013;&#38598;&#25104;&#20102;Lipschitz&#36830;&#32493;&#24615;&#32422;&#26463;&#12290;CHAIN&#36890;&#36807;&#33258;&#36866;&#24212;&#25554;&#20540;&#24402;&#19968;&#21270;&#21644;&#38750;&#24402;&#19968;&#21270;&#29305;&#24449;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;GANs&#30340;&#35757;&#32451;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00521v1 Announce Type: new  Abstract: Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lipschitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26377;&#38480;&#31639;&#23376;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23398;&#20064;&#21442;&#25968;&#21270;&#35299;&#20915;&#20102;&#24494;&#32467;&#26500;&#24377;&#24615;&#23646;&#24615;&#26144;&#23556;&#21040;&#26426;&#26800;&#21464;&#24418;&#30340;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#33021;&#22788;&#29702;&#20855;&#26377;&#26126;&#26174;&#19981;&#36830;&#32493;&#24615;&#30340;&#35299;&#12290;</title><link>https://arxiv.org/abs/2404.00074</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23558;&#24494;&#32467;&#26500;&#30340;&#24377;&#24615;&#23646;&#24615;&#26144;&#23556;&#21040;&#20854;&#26426;&#26800;&#21464;&#24418;&#30340;&#26377;&#38480;&#31639;&#23376;&#23398;&#20064;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A finite operator learning technique for mapping the elastic properties of microstructures to their mechanical deformations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00074
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26377;&#38480;&#31639;&#23376;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23398;&#20064;&#21442;&#25968;&#21270;&#35299;&#20915;&#20102;&#24494;&#32467;&#26500;&#24377;&#24615;&#23646;&#24615;&#26144;&#23556;&#21040;&#26426;&#26800;&#21464;&#24418;&#30340;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#33021;&#22788;&#29702;&#20855;&#26377;&#26126;&#26174;&#19981;&#36830;&#32493;&#24615;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24320;&#21457;&#22266;&#20307;&#21147;&#23398;&#20013;&#25511;&#21046;&#29289;&#29702;&#26041;&#31243;&#30340;&#26356;&#24555;&#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#23398;&#20064;&#26426;&#26800;&#24179;&#34913;&#35299;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#23558;&#26631;&#20934;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#27867;&#21270;&#21644;&#22686;&#24378;&#65292;&#20197;&#23398;&#20064;&#19968;&#20010;&#24102;&#26377;&#30456;&#24403;&#26126;&#26174;&#19981;&#36830;&#32493;&#24615;&#30340;&#21442;&#25968;&#35299;&#12290;&#25105;&#20204;&#20197;&#24494;&#35266;&#21147;&#23398;&#20026;&#20363;&#65292;&#20854;&#20013;&#23545;&#20110;&#32473;&#23450;&#30340;&#24322;&#36136;&#24494;&#32467;&#26500;&#26469;&#35828;&#65292;&#24494;&#35266;&#21147;&#23398;&#35299;&#65288;&#21363;&#21464;&#24418;&#21644;&#24212;&#21147;&#22330;&#65289;&#30340;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#38024;&#23545;&#30340;&#21442;&#25968;&#26159;&#24322;&#36136;&#22266;&#20307;&#31995;&#32479;&#20869;&#30340;&#26472;&#27663;&#27169;&#37327;&#20998;&#24067;&#12290;&#21463;&#31639;&#23376;&#23398;&#20064;&#21644;&#26377;&#38480;&#20803;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#26080;&#38656;&#20381;&#36182;&#20854;&#20182;&#25968;&#20540;&#27714;&#35299;&#22120;&#25968;&#25454;&#23601;&#33021;&#36827;&#34892;&#35757;&#32451;&#30340;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26377;&#38480;&#20803;&#26041;&#27861;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00074v1 Announce Type: new  Abstract: To develop faster solvers for governing physical equations in solid mechanics, we introduce a method that parametrically learns the solution to mechanical equilibrium. The introduced method outperforms traditional ones in terms of computational cost while acceptably maintaining accuracy. Moreover, it generalizes and enhances the standard physics-informed neural networks to learn a parametric solution with rather sharp discontinuities. We focus on micromechanics as an example, where the knowledge of the micro-mechanical solution, i.e., deformation and stress fields for a given heterogeneous microstructure, is crucial. The parameter under investigation is the Young modulus distribution within the heterogeneous solid system. Our method, inspired by operator learning and the finite element method, demonstrates the ability to train without relying on data from other numerical solvers. Instead, we leverage ideas from the finite element approac
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19981;&#20381;&#36182;&#20110;&#24378;&#20984;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20202;&#34920;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.20233</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#21452;&#23618;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Functional Bilevel Optimization for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20233
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19981;&#20381;&#36182;&#20110;&#24378;&#20984;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20202;&#34920;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#31181;&#26032;&#30340;&#20989;&#25968;&#35270;&#35282;&#65292;&#20854;&#20013;&#20869;&#37096;&#30446;&#26631;&#22312;&#20989;&#25968;&#31354;&#38388;&#19978;&#34987;&#26368;&#23567;&#21270;&#12290;&#36825;&#20123;&#31867;&#22411;&#30340;&#38382;&#39064;&#36890;&#24120;&#36890;&#36807;&#22312;&#21442;&#25968;&#35774;&#32622;&#19979;&#24320;&#21457;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#65292;&#20854;&#20013;&#20869;&#37096;&#30446;&#26631;&#23545;&#20110;&#39044;&#27979;&#20989;&#25968;&#30340;&#21442;&#25968;&#24378;&#20984;&#12290;&#20989;&#25968;&#35270;&#35282;&#19981;&#20381;&#36182;&#20110;&#27492;&#20551;&#35774;&#65292;&#29305;&#21035;&#20801;&#35768;&#20351;&#29992;&#36229;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20869;&#37096;&#39044;&#27979;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#20989;&#25968;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#36866;&#21512;&#33258;&#28982;&#20989;&#25968;&#21452;&#23618;&#32467;&#26500;&#30340;&#20202;&#34920;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20233v1 Announce Type: cross  Abstract: In this paper, we introduce a new functional point of view on bilevel optimization problems for machine learning, where the inner objective is minimized over a function space. These types of problems are most often solved by using methods developed in the parametric setting, where the inner objective is strongly convex with respect to the parameters of the prediction function. The functional point of view does not rely on this assumption and notably allows using over-parameterized neural networks as the inner prediction function. We propose scalable and efficient algorithms for the functional bilevel optimization problem and illustrate the benefits of our approach on instrumental regression and reinforcement learning tasks, which admit natural functional bilevel structures.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#26377;&#25928;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#65292;&#20026;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.19289</link><description>&lt;p&gt;
&#29992;&#20110;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Treatment Effect Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19289
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#26377;&#25928;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#65292;&#20026;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#24448;&#24448;&#28041;&#21450;&#26114;&#36149;&#30340;&#27835;&#30103;&#20998;&#37197;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#35774;&#32622;&#20013;&#21487;&#33021;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#36825;&#31181;&#27835;&#30103;&#25928;&#26524;&#32780;&#26080;&#38656;&#23454;&#38469;&#24178;&#39044;&#26159;&#20943;&#23569;&#39118;&#38505;&#30340;&#19968;&#31181;&#26631;&#20934;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#23454;&#39564;&#26500;&#24314;&#30340;&#35757;&#32451;&#38598;&#65292;&#22240;&#27492;&#20174;&#26681;&#26412;&#19978;&#23384;&#22312;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#20381;&#36182;&#20110;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#22270;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#35270;&#20026;&#20855;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#35760;&#23454;&#20363;&#30340;&#33410;&#28857;&#22238;&#24402;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#20808;&#21069;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#21452;&#27169;&#22411;&#31070;&#32463;&#26550;&#26500;&#65292;&#24182;&#27979;&#35797;&#20102;&#19981;&#21516;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#39069;&#22806;&#27493;&#39588;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#19982;&#33719;&#21462;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#20197;&#24341;&#23548;&#20449;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19289v1 Announce Type: cross  Abstract: Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20195;&#34920;&#22810;&#23618;&#24314;&#31569;&#24182;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#20854;&#20013;&#31359;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.17846</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20195;&#34920;&#22810;&#23618;&#24314;&#31569;&#24182;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#20854;&#20013;&#31359;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24320;&#25918;&#35789;&#27719;&#26426;&#22120;&#20154;&#26144;&#23556;&#26041;&#27861;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#20016;&#23500;&#20102;&#23494;&#38598;&#20960;&#20309;&#22320;&#22270;&#12290;&#34429;&#28982;&#36825;&#20123;&#22320;&#22270;&#20801;&#35768;&#22312;&#26597;&#35810;&#26576;&#31181;&#35821;&#35328;&#27010;&#24565;&#26102;&#39044;&#27979;&#36880;&#28857;&#26174;&#33879;&#24615;&#22320;&#22270;&#65292;&#20294;&#22823;&#35268;&#27169;&#29615;&#22659;&#21644;&#36229;&#20986;&#23545;&#35937;&#32423;&#21035;&#30340;&#25277;&#35937;&#26597;&#35810;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#24403;&#22823;&#30340;&#38556;&#30861;&#65292;&#26368;&#32456;&#38480;&#21046;&#20102;&#22522;&#20110;&#35821;&#35328;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HOV-SG&#65292;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;3D&#31354;&#38388;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#27573;&#32423;&#22320;&#22270;&#65292;&#28982;&#21518;&#26500;&#24314;&#20102;&#30001;&#22320;&#26495;&#12289;&#25151;&#38388;&#21644;&#23545;&#35937;&#27010;&#24565;&#32452;&#25104;&#30340;3D&#22330;&#26223;&#22270;&#23618;&#27425;&#32467;&#26500;&#65292;&#27599;&#20010;&#37117;&#21253;&#21547;&#24320;&#25918;&#24615;&#35789;&#27719;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#34920;&#31034;&#22810;&#23618;&#24314;&#31569;&#65292;&#24182;&#19988;&#20801;&#35768;&#26426;&#22120;&#20154;&#20351;&#29992;&#36328;&#23618;Voronoi&#22270;&#31359;&#36234;&#36825;&#20123;&#24314;&#31569;&#12290;HOV-SG&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17846v1 Announce Type: cross  Abstract: Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluat
&lt;/p&gt;</description></item><item><title>&#22312;ALICE&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#22810;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31890;&#23376;&#35782;&#21035;&#65292;&#21253;&#25324;&#29305;&#24449;&#38598;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#22312;&#19981;&#23436;&#25972;&#25968;&#25454;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;ML&#39033;&#30446;&#19982;ALICE&#20998;&#26512;&#36719;&#20214;&#38598;&#25104;&#65292;&#35752;&#35770;&#20102;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.17436</link><description>&lt;p&gt;
&#22312;ALICE&#23454;&#39564;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#19981;&#23436;&#25972;&#25968;&#25454;&#20013;&#36827;&#34892;&#31890;&#23376;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Particle identification with machine learning from incomplete data in the ALICE experiment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17436
&lt;/p&gt;
&lt;p&gt;
&#22312;ALICE&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#22810;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31890;&#23376;&#35782;&#21035;&#65292;&#21253;&#25324;&#29305;&#24449;&#38598;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#22312;&#19981;&#23436;&#25972;&#25968;&#25454;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;ML&#39033;&#30446;&#19982;ALICE&#20998;&#26512;&#36719;&#20214;&#38598;&#25104;&#65292;&#35752;&#35770;&#20102;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LHC&#30340;ALICE&#23454;&#39564;&#27979;&#37327;&#22312;&#36229;&#30456;&#23545;&#35770;&#37325;&#31163;&#23376;&#23545;&#25758;&#20013;&#24418;&#25104;&#30340;&#24378;&#30456;&#20114;&#20316;&#29992;&#29289;&#36136;&#30340;&#24615;&#36136;&#12290;&#36825;&#20123;&#30740;&#31350;&#38656;&#35201;&#20934;&#30830;&#30340;&#31890;&#23376;&#35782;&#21035;(PID)&#12290;ALICE&#36890;&#36807;&#20960;&#20010;&#25506;&#27979;&#22120;&#20026;&#21160;&#37327;&#20174;&#32422;100 MeV/c&#21040;20 GeV/c&#30340;&#31890;&#23376;&#25552;&#20379;PID&#20449;&#24687;&#12290;&#20256;&#32479;&#19978;&#65292;&#31890;&#23376;&#26159;&#36890;&#36807;&#30697;&#24418;&#20999;&#21106;&#36827;&#34892;&#36873;&#25321;&#30340;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;(ML)&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20351;&#29992;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;(NN)&#20316;&#20026;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29305;&#24449;&#38598;&#23884;&#20837;&#21644;&#20851;&#27880;&#25193;&#23637;&#20102;&#31890;&#23376;&#20998;&#31867;&#22120;&#65292;&#20197;&#20415;&#23545;&#21253;&#21547;&#19981;&#23436;&#25972;&#26679;&#26412;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;ML&#39033;&#30446;&#19982;ALICE&#20998;&#26512;&#36719;&#20214;&#30340;&#38598;&#25104;&#65292;&#24182;&#35752;&#35770;&#20102;&#22495;&#33258;&#36866;&#24212;&#65292;&#36825;&#26159;&#23558;&#30693;&#35782;&#20174;&#27169;&#25311;&#25968;&#25454;&#36716;&#31227;&#21040;&#23454;&#38469;&#23454;&#39564;&#25968;&#25454;&#25152;&#38656;&#30340;ML&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17436v1 Announce Type: cross  Abstract: The ALICE experiment at the LHC measures properties of the strongly interacting matter formed in ultrarelativistic heavy-ion collisions. Such studies require accurate particle identification (PID). ALICE provides PID information via several detectors for particles with momentum from about 100 MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular cuts. Acmuch better performance can be achieved with machine learning (ML) methods. Our solution uses multiple neural networks (NN) serving as binary classifiers. Moreover, we extended our particle classifier with Feature Set Embedding and attention in order to train on data with incomplete samples. We also present the integration of the ML project with the ALICE analysis software, and we discuss domain adaptation, the ML technique needed to transfer the knowledge between simulated and real experimental data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37327;&#21270;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#22312;&#19981;&#21516;&#22823;&#23567;&#39046;&#22495;&#38388;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#39046;&#22495;&#22823;&#23567;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.15933</link><description>&lt;p&gt;
&#29702;&#35299;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#20013;&#30340;&#22495;&#22823;&#23567;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Understanding Domain-Size Generalization in Markov Logic Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#22312;&#19981;&#21516;&#22823;&#23567;&#39046;&#22495;&#38388;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#39046;&#22495;&#22823;&#23567;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#65288;MLNs&#65289;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#20851;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#22810;&#20010;&#30740;&#31350;&#27880;&#24847;&#21040;&#65292;&#22312;&#32473;&#23450;&#22495;&#19978;&#23398;&#20064;&#30340;MLNs&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#22495;&#19978;&#27867;&#21270;&#24456;&#24046;&#12290;&#36825;&#31181;&#34892;&#20026;&#28304;&#20110;MLN&#22312;&#19981;&#21516;&#22495;&#22823;&#23567;&#19978;&#20351;&#29992;&#26102;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#23558;&#20854;&#38480;&#21046;&#22312;MLN&#21442;&#25968;&#30340;&#26041;&#24046;&#33539;&#22260;&#20869;&#12290;&#21442;&#25968;&#26041;&#24046;&#36824;&#38480;&#21046;&#20102;&#20174;&#19981;&#21516;&#22495;&#22823;&#23567;&#20013;&#21462;&#20986;&#30340;MLN&#36793;&#32536;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#30028;&#38480;&#23637;&#31034;&#65292;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#65292;&#23545;&#24212;&#20110;&#22495;&#22823;&#23567;&#27867;&#21270;&#30340;&#20004;&#20010;&#33258;&#28982;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#25351;&#25968;&#38543;&#26426;&#22270;&#21644;&#20854;&#20182;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#32593;&#32476;&#30340;&#20851;&#31995;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24050;&#30693;&#30340;&#35299;&#20915;&#26041;&#26696;&#20250;&#20943;&#23569;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15933v1 Announce Type: new  Abstract: We study the generalization behavior of Markov Logic Networks (MLNs) across relational structures of different sizes. Multiple works have noticed that MLNs learned on a given domain generalize poorly across domains of different sizes. This behavior emerges from a lack of internal consistency within an MLN when used across different domain sizes. In this paper, we quantify this inconsistency and bound it in terms of the variance of the MLN parameters. The parameter variance also bounds the KL divergence between an MLN's marginal distributions taken from different domain sizes. We use these bounds to show that maximizing the data log-likelihood while simultaneously minimizing the parameter variance corresponds to two natural notions of generalization across domain sizes. Our theoretical results apply to Exponential Random Graphs and other Markov network based relational models. Finally, we observe that solutions known to decrease the varia
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32487;&#25215;&#29305;&#24449;&#23398;&#20064;&#31574;&#30053;&#22522;&#30784;&#65292;&#20351;&#27599;&#20010;&#65288;&#23376;&#65289;&#31574;&#30053;&#35299;&#20915;&#19968;&#20010;&#23376;&#38382;&#39064;&#65292;&#22312;FSA&#25551;&#36848;&#30340;&#20219;&#21153;&#20013;&#65292;&#32452;&#21512;&#36825;&#20123;&#65288;&#23376;&#65289;&#31574;&#30053;&#21487;&#29992;&#20110;&#26080;&#38656;&#39069;&#22806;&#23398;&#20064;&#29983;&#25104;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#26041;&#27861;&#33021;&#22815;&#28176;&#36817;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20063;&#22914;&#27492;&#12290;</title><link>https://arxiv.org/abs/2403.15301</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#30340;&#31574;&#30053;&#22522;&#30784;&#36827;&#34892;&#35268;&#21010;&#20197;&#26368;&#20248;&#22320;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Planning with a Learned Policy Basis to Optimally Solve Complex Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15301
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32487;&#25215;&#29305;&#24449;&#23398;&#20064;&#31574;&#30053;&#22522;&#30784;&#65292;&#20351;&#27599;&#20010;&#65288;&#23376;&#65289;&#31574;&#30053;&#35299;&#20915;&#19968;&#20010;&#23376;&#38382;&#39064;&#65292;&#22312;FSA&#25551;&#36848;&#30340;&#20219;&#21153;&#20013;&#65292;&#32452;&#21512;&#36825;&#20123;&#65288;&#23376;&#65289;&#31574;&#30053;&#21487;&#29992;&#20110;&#26080;&#38656;&#39069;&#22806;&#23398;&#20064;&#29983;&#25104;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#26041;&#27861;&#33021;&#22815;&#28176;&#36817;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20063;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35299;&#20915;&#21508;&#31181;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#35268;&#33539;&#30340;&#24773;&#26223;&#20013;&#23398;&#20064;&#33021;&#22815;&#21487;&#38752;&#27867;&#21270;&#20110;&#22810;&#20010;&#20219;&#21153;&#30340;&#31574;&#30053;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32487;&#25215;&#29305;&#24449;&#26469;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#22522;&#30784;&#65292;&#20351;&#24471;&#20854;&#20013;&#30340;&#27599;&#19968;&#20010;&#65288;&#23376;&#65289;&#31574;&#30053;&#35299;&#20915;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#23376;&#38382;&#39064;&#12290;&#22312;&#30001;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288;FSA&#65289;&#25551;&#36848;&#30340;&#20219;&#21153;&#20013;&#28041;&#21450;&#30456;&#21516;&#19968;&#32452;&#23376;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#65288;&#23376;&#65289;&#31574;&#30053;&#30340;&#32452;&#21512;&#21487;&#20197;&#34987;&#29992;&#26469;&#29983;&#25104;&#19968;&#20010;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#23398;&#20064;&#12290;&#19982;&#20854;&#20182;&#36890;&#36807;&#35268;&#21010;&#32452;&#21512;&#65288;&#23376;&#65289;&#31574;&#30053;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28176;&#36817;&#19978;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15301v1 Announce Type: cross  Abstract: Conventional reinforcement learning (RL) methods can successfully solve a wide range of sequential decision problems. However, learning policies that can generalize predictably across multiple tasks in a setting with non-Markovian reward specifications is a challenging problem. We propose to use successor features to learn a policy basis so that each (sub)policy in it solves a well-defined subproblem. In a task described by a finite state automaton (FSA) that involves the same set of subproblems, the combination of these (sub)policies can then be used to generate an optimal solution without additional learning. In contrast to other methods that combine (sub)policies via planning, our method asymptotically attains global optimality, even in stochastic environments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13784</link><description>&lt;p&gt;
&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;: &#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#21487;&#37325;&#29616;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#29992;&#24615;&#30340;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13784
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#20854;&#21830;&#19994;&#21270;&#24341;&#21457;&#20102;&#20851;&#20110;&#36879;&#26126;&#24230;&#12289;&#21487;&#37325;&#29616;&#24615;&#12289;&#20559;&#35265;&#21644;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#12290;&#35768;&#22810;"&#24320;&#28304;"&#30340;GAI&#27169;&#22411;&#32570;&#20047;&#23436;&#25972;&#29702;&#35299;&#21644;&#20877;&#29616;&#25152;&#24517;&#38656;&#30340;&#32452;&#20214;&#65292;&#19968;&#20123;&#37319;&#29992;&#38480;&#21046;&#24615;&#35768;&#21487;&#35777;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;"&#24320;&#28304;&#27927;&#30333;"&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#25968;&#25454;&#21644;&#24320;&#25918;&#33719;&#21462;&#30340;&#21407;&#21017;&#12290;MOF&#35201;&#27714;&#27169;&#22411;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#29305;&#23450;&#32452;&#20214;&#34987;&#21253;&#21547;&#24182;&#26681;&#25454;&#36866;&#24403;&#30340;&#24320;&#25918;&#35768;&#21487;&#35777;&#21457;&#24067;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#38450;&#27490;&#23459;&#31216;&#33258;&#24049;&#26159;&#24320;&#25918;&#30340;&#27169;&#22411;&#34987;&#35823;&#35299;&#65292;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#20197;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#21457;&#24067;&#25152;&#26377;&#27169;&#22411;&#32452;&#20214;&#65292;&#24182;&#24110;&#21161;&#20844;&#21496;&#12289;&#23398;&#26415;&#30028;&#21644;&#29233;&#22909;&#32773;&#35782;&#21035;&#21487;&#20197;&#23433;&#20840;&#37319;&#29992;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13784v1 Announce Type: new  Abstract: Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety. Many "open-source" GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as "openwashing." We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses. This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adop
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26368;&#22823;&#29109;&#26041;&#27861;&#25512;&#23548;&#20102;&#27010;&#29575;&#25968;&#35770;&#20013;&#30340;&#20960;&#20010;&#23450;&#29702;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#32032;&#25968;&#23398;&#20064;&#24615;&#36136;&#30340;&#29702;&#35770;&#35770;&#35777;&#65292;&#21457;&#29616;Erd\H{o}s-Kac&#23450;&#24459;&#19981;&#22826;&#21487;&#33021;&#34987;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21457;&#29616;&#65292;&#24182;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#20197;&#39564;&#35777;&#29702;&#35770;&#32467;&#26524;</title><link>https://arxiv.org/abs/2403.12588</link><description>&lt;p&gt;
&#20027;&#35201;&#20998;&#24067;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine Learning of the Prime Distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12588
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26368;&#22823;&#29109;&#26041;&#27861;&#25512;&#23548;&#20102;&#27010;&#29575;&#25968;&#35770;&#20013;&#30340;&#20960;&#20010;&#23450;&#29702;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#32032;&#25968;&#23398;&#20064;&#24615;&#36136;&#30340;&#29702;&#35770;&#35770;&#35777;&#65292;&#21457;&#29616;Erd\H{o}s-Kac&#23450;&#24459;&#19981;&#22826;&#21487;&#33021;&#34987;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21457;&#29616;&#65292;&#24182;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#20197;&#39564;&#35777;&#29702;&#35770;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#29109;&#26041;&#27861;&#25512;&#23548;&#20102;&#27010;&#29575;&#25968;&#35770;&#20013;&#30340;&#20960;&#20010;&#23450;&#29702;&#65292;&#21253;&#25324;&#21704;&#20195;-&#25289;&#39532;&#21162;&#37329;&#23450;&#29702;&#30340;&#19968;&#20010;&#29256;&#26412;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#35770;&#35777;&#65292;&#35299;&#37322;&#20102;Y.-H. He&#20851;&#20110;&#32032;&#25968;&#21487;&#23398;&#24615;&#30340;&#23454;&#39564;&#35266;&#23519;&#65292;&#24182;&#20551;&#35774;Erd\H{o}s-Kac&#23450;&#24459;&#26497;&#19981;&#21487;&#33021;&#34987;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21457;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12588v1 Announce Type: cross  Abstract: In the present work we use maximum entropy methods to derive several theorems in probabilistic number theory, including a version of the Hardy-Ramanujan Theorem. We also provide a theoretical argument explaining the experimental observations of Y.-H. He about the learnability of primes, and posit that the Erd\H{o}s-Kac law would very unlikely be discovered by current machine learning techniques. Numerical experiments that we perform corroborate our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#19968;&#33268;&#24615;&#27169;&#22411;&#20316;&#20026;&#39640;&#36136;&#37327;&#36924;&#36817;&#65292;&#20197;&#25913;&#36827;&#23545;&#25193;&#25955;&#36870;&#27714;&#35299;&#22120;&#20013;&#21518;&#39564;&#26679;&#26412;&#30340;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.12063</link><description>&lt;p&gt;
&#19968;&#33268;&#24615;&#27169;&#22411;&#25913;&#36827;&#25193;&#25955;&#36870;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Consistency Models Improve Diffusion Inverse Solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#19968;&#33268;&#24615;&#27169;&#22411;&#20316;&#20026;&#39640;&#36136;&#37327;&#36924;&#36817;&#65292;&#20197;&#25913;&#36827;&#23545;&#25193;&#25955;&#36870;&#27714;&#35299;&#22120;&#20013;&#21518;&#39564;&#26679;&#26412;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#36870;&#27714;&#35299;&#22120;&#65288;DIS&#65289;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#22312;&#25193;&#25955;&#20808;&#39564;&#31354;&#38388;&#20013;&#30340;&#22270;&#20687;$x$&#65292;&#28385;&#36275;&#32422;&#26463;$f(x)=y$&#65292;&#32473;&#23450;&#31639;&#23376;$f(\cdot)$&#21644;&#27979;&#37327;$y$&#12290;&#22823;&#22810;&#25968;&#38750;&#32447;&#24615;DIS&#20351;&#29992;&#21518;&#39564;&#22343;&#20540;$\hat{x}_{0|t}=\mathbb{E}[x_0|x_t]$&#26469;&#35780;&#20272;$f(\cdot)$&#24182;&#26368;&#23567;&#21270;&#36317;&#31163;$||f(\hat{x}_{0|t})-y||^2$&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22522;&#20110;&#21518;&#39564;&#22343;&#20540;&#30340;&#36317;&#31163;&#26159;&#26377;&#20559;&#30340;&#65307;&#32780;&#21518;&#39564;&#26679;&#26412;$x_{0|t}\sim p_{\theta}(x_0|x_t)$&#25215;&#35834;&#26159;&#26356;&#22909;&#30340;&#20505;&#36873;&#12290;&#26412;&#25991;&#39318;&#20808;&#28548;&#28165;&#20102;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#21518;&#39564;&#26679;&#26412;&#26356;&#22909;&#65306;$1)$&#24403;$f(\cdot)$&#26159;&#32447;&#24615;&#30340;&#26102;&#65292;&#20351;&#29992;&#21518;&#39564;&#22343;&#20540;&#30340;&#36317;&#31163;&#23601;&#20687;&#21333;&#20010;&#21518;&#39564;&#26679;&#26412;&#19968;&#26679;&#22909;&#65292;&#22240;&#27492;&#26356;&#21487;&#21462;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#33945;&#29305;&#21345;&#27931;&#65307;$2)$&#24403;$f(\cdot)$&#26159;&#38750;&#32447;&#24615;&#30340;&#26102;&#65292;&#20351;&#29992;&#21518;&#39564;&#26679;&#26412;&#30340;&#36317;&#31163;&#26356;&#22909;&#12290;&#30001;&#20110;&#20808;&#21069;&#23545;&#21518;&#39564;&#26679;&#26412;&#30340;&#36924;&#36817;&#19981;&#20687;&#30495;&#23454;&#22270;&#20687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CM&#65289;&#20316;&#20026;&#39640;&#36136;&#37327;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12063v1 Announce Type: cross  Abstract: Diffusion inverse solvers (DIS) aim to find an image $x$ that lives on the diffusion prior while satisfying the constraint $f(x) = y$, given an operator $f(.)$ and measurement $y$. Most non-linear DIS use posterior mean $\hat{x}_{0|t}=\mathbb{E}[x_0|x_t]$ to evaluate $f(.)$ and minimize the distance $||f(\hat{x}_{0|t})-y||^2$. Previous works show that posterior mean-based distance is biased; instead, posterior sample $x_{0|t}\sim p_{\theta}(x_0|x_t)$ promises a better candidate. In this paper, we first clarify when is posterior sample better: $1)$ When $f(.)$ is linear, the distance with posterior mean is as good as single posterior sample, thus preferable as it does not require Monte Carlo; $2)$ When $f(.)$ is non-linear, the distance using posterior sample is better. As previous approximations to posterior sample do not look like a real image, we propose to use consistency model (CM) as a high quality approximation. In addition, we p
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#20559;&#22909;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#29702;&#24615;&#21407;&#21017;&#34701;&#20837;&#23398;&#20064;&#36807;&#31243;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#20559;&#22909;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11782</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#20174;&#20559;&#22909;&#21644;&#36873;&#25321;&#20013;&#23398;&#20064;&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
A tutorial on learning from preferences and choices with Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11782
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#20559;&#22909;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#29702;&#24615;&#21407;&#21017;&#34701;&#20837;&#23398;&#20064;&#36807;&#31243;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#20559;&#22909;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#24314;&#27169;&#20301;&#20110;&#32463;&#27982;&#23398;&#12289;&#20915;&#31574;&#29702;&#35770;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#30340;&#20132;&#21449;&#28857;&#12290;&#36890;&#36807;&#29702;&#35299;&#20010;&#20307;&#30340;&#20559;&#22909;&#21450;&#20854;&#36873;&#25321;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#26356;&#25509;&#36817;&#20182;&#20204;&#26399;&#26395;&#30340;&#20135;&#21697;&#65292;&#20026;&#36328;&#39046;&#22495;&#30340;&#26356;&#39640;&#25928;&#12289;&#20010;&#24615;&#21270;&#24212;&#29992;&#38138;&#24179;&#36947;&#36335;&#12290;&#27492;&#25945;&#31243;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#36830;&#36143;&#12289;&#20840;&#38754;&#30340;&#20559;&#22909;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#28436;&#31034;&#22914;&#20309;&#23558;&#29702;&#24615;&#21407;&#21017;&#65288;&#26469;&#33258;&#32463;&#27982;&#23398;&#21644;&#20915;&#31574;&#29702;&#35770;&#65289;&#26080;&#32541;&#22320;&#32435;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#21512;&#36866;&#22320;&#23450;&#21046;&#20284;&#28982;&#20989;&#25968;&#65292;&#36825;&#19968;&#26694;&#26550;&#20351;&#24471;&#33021;&#22815;&#26500;&#24314;&#28085;&#30422;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#12289;&#36776;&#35782;&#38480;&#21046;&#21644;&#23545;&#35937;&#21644;&#26631;&#31614;&#20559;&#22909;&#30340;&#22810;&#37325;&#20914;&#31361;&#25928;&#29992;&#24773;&#26223;&#30340;&#20559;&#22909;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11782v1 Announce Type: new  Abstract: Preference modelling lies at the intersection of economics, decision theory, machine learning and statistics. By understanding individuals' preferences and how they make choices, we can build products that closely match their expectations, paving the way for more efficient and personalised applications across a wide range of domains. The objective of this tutorial is to present a cohesive and comprehensive framework for preference learning with Gaussian Processes (GPs), demonstrating how to seamlessly incorporate rationality principles (from economics and decision theory) into the learning process. By suitably tailoring the likelihood function, this framework enables the construction of preference learning models that encompass random utility models, limits of discernment, and scenarios with multiple conflicting utilities for both object- and label-preference. This tutorial builds upon established research while simultaneously introducin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#20272;&#35745;&#30452;&#25509;&#21644;&#21516;&#34892;&#25928;&#24212;&#65292;&#22788;&#29702;&#32593;&#32476;&#28151;&#26434;&#22240;&#32032;&#65292;&#24182;&#19968;&#33268;&#22320;&#20272;&#35745;&#25152;&#38656;&#30340;&#22240;&#26524;&#25928;&#24212;</title><link>https://arxiv.org/abs/2403.11332</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#22240;&#26524;&#25928;&#24212;&#21452;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network based Double Machine Learning Estimator of Network Causal Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11332
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#20272;&#35745;&#30452;&#25509;&#21644;&#21516;&#34892;&#25928;&#24212;&#65292;&#22788;&#29702;&#32593;&#32476;&#28151;&#26434;&#22240;&#32032;&#65292;&#24182;&#19968;&#33268;&#22320;&#20272;&#35745;&#25152;&#38656;&#30340;&#22240;&#26524;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#20013;&#25512;&#26029;&#22240;&#26524;&#25928;&#24212;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#25968;&#25454;&#20855;&#26377;&#20010;&#20307;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#23548;&#33268;&#21333;&#20301;&#20043;&#38388;&#19981;&#29420;&#31435;&#12289;&#24178;&#25200;&#65288;&#21333;&#20301;&#30340;&#32467;&#26524;&#21463;&#37051;&#23621;&#30340;&#22788;&#29702;&#24433;&#21709;&#65289;&#20197;&#21450;&#24341;&#20837;&#26469;&#33258;&#37051;&#36817;&#21333;&#20301;&#30340;&#39069;&#22806;&#28151;&#26434;&#22240;&#32032;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#21333;&#20010;&#35266;&#27979;&#31038;&#20132;&#32593;&#32476;&#20934;&#30830;&#39640;&#25928;&#22320;&#20272;&#35745;&#30452;&#25509;&#21644;&#21516;&#20276;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#19982;&#21452;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#35843;&#25972;&#32593;&#32476;&#28151;&#26434;&#22240;&#32032;&#24182;&#19968;&#33268;&#22320;&#20272;&#35745;&#25152;&#38656;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#26082;&#20855;&#26377;&#28176;&#36817;&#27491;&#24577;&#24615;&#21448;&#21322;&#21442;&#25968;&#39640;&#25928;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#21322;&#21512;&#25104;&#29366;&#24577;&#19979;&#30340;&#22235;&#31181;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11332v1 Announce Type: new  Abstract: Our paper addresses the challenge of inferring causal effects in social network data, characterized by complex interdependencies among individuals resulting in challenges such as non-independence of units, interference (where a unit's outcome is affected by neighbors' treatments), and introduction of additional confounding factors from neighboring units. We propose a novel methodology combining graph neural networks and double machine learning, enabling accurate and efficient estimation of direct and peer effects using a single observational social network. Our approach utilizes graph isomorphism networks in conjunction with double machine learning to effectively adjust for network confounders and consistently estimate the desired causal effects. We demonstrate that our estimator is both asymptotically normal and semiparametrically efficient. A comprehensive evaluation against four state-of-the-art baseline methods using three semi-synth
&lt;/p&gt;</description></item><item><title>QDAC&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.09930</link><description>&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65306;&#36890;&#36807;&#20540;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09930
&lt;/p&gt;
&lt;p&gt;
QDAC&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#34920;&#29616;&#20986;&#36866;&#24212;&#24847;&#22806;&#24773;&#20917;&#30340;&#24191;&#27867;&#34892;&#20026;&#35889;&#12290;&#36807;&#21435;&#21313;&#24180;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#27493;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#25104;&#23601;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#36820;&#22238;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36136;&#37327;&#22810;&#26679;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;QDAC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#28436;&#21592;&#36890;&#36807;&#21463;&#38480;&#20248;&#21270;&#26469;&#26368;&#22823;&#21270;&#22238;&#25253;&#24182;&#25191;&#34892;&#22810;&#26679;&#24615;&#25216;&#33021;&#30340;&#23458;&#35266;&#20989;&#25968;&#65292;&#26080;&#32541;&#32479;&#19968;&#20102;&#20004;&#20010;&#35780;&#35770;&#23478;&#12290;&#19982;&#20854;&#20182;&#36136;&#37327;&#22810;&#26679;&#24615;&#26041;&#27861;&#30456;&#27604;&#65292;QDAC&#22312;&#20845;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#36816;&#21160;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#22810;&#26679;&#24615;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09930v1 Announce Type: cross  Abstract: A key aspect of intelligence is the ability to demonstrate a broad spectrum of behaviors for adapting to unexpected situations. Over the past decade, advancements in deep reinforcement learning have led to groundbreaking achievements to solve complex continuous control tasks. However, most approaches return only one solution specialized for a specific problem. We introduce Quality-Diversity Actor-Critic (QDAC), an off-policy actor-critic deep reinforcement learning algorithm that leverages a value function critic and a successor features critic to learn high-performing and diverse behaviors. In this framework, the actor optimizes an objective that seamlessly unifies both critics using constrained optimization to (1) maximize return, while (2) executing diverse skills. Compared with other Quality-Diversity methods, QDAC achieves significantly higher performance and more diverse behaviors on six challenging continuous control locomotion 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#24179;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#25152;&#26377;&#25910;&#25947;&#31639;&#27861;&#30340;&#26080;&#32422;&#26463;&#21442;&#25968;&#21270;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.09389</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#32447;&#24615;&#31995;&#32479;&#29702;&#35770;&#23398;&#20064;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning to optimize with convergence guarantees using nonlinear system theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09389
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#24179;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#25152;&#26377;&#25910;&#25947;&#31639;&#27861;&#30340;&#26080;&#32422;&#26463;&#21442;&#25968;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#20110;&#25968;&#23383;&#26041;&#27861;&#26469;&#25511;&#21046;&#21160;&#24577;&#31995;&#32479;&#21644;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#35774;&#35745;&#21487;&#38752;&#19988;&#39640;&#25928;&#22320;&#36941;&#21382;&#22797;&#26434;&#20248;&#21270;&#31354;&#38388;&#30340;&#31639;&#27861;&#12290;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#23545;&#20984;&#38382;&#39064;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#38750;&#20984;&#38382;&#39064;&#21017;&#38656;&#35201;&#31934;&#32454;&#35843;&#25972;&#36229;&#21442;&#25968;&#12290;&#23398;&#20064;&#20248;&#21270;(L2O)&#30340;&#26032;&#20852;&#33539;&#24335;&#33258;&#21160;&#21457;&#29616;&#20855;&#26377;&#20248;&#21270;&#24615;&#33021;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#23398;&#20064;&#27169;&#22411;&#21644;&#25968;&#25454;&#65292;&#20294;&#32570;&#20047;&#20998;&#26512;&#25152;&#23398;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#38750;&#32447;&#24615;&#31995;&#32479;&#29702;&#35770;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20110;&#24179;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#25152;&#26377;&#25910;&#25947;&#31639;&#27861;&#30340;&#26080;&#32422;&#26463;&#21442;&#25968;&#21270;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#33258;&#21160;&#24494;&#20998;&#24037;&#20855;&#30452;&#25509;&#20860;&#23481;&#65292;&#30830;&#20445;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09389v1 Announce Type: cross  Abstract: The increasing reliance on numerical methods for controlling dynamical systems and training machine learning models underscores the need to devise algorithms that dependably and efficiently navigate complex optimization landscapes. Classical gradient descent methods offer strong theoretical guarantees for convex problems; however, they demand meticulous hyperparameter tuning for non-convex ones. The emerging paradigm of learning to optimize (L2O) automates the discovery of algorithms with optimized performance leveraging learning models and data - yet, it lacks a theoretical framework to analyze convergence and robustness of the learned algorithms. In this paper, we fill this gap by harnessing nonlinear system theory. Specifically, we propose an unconstrained parametrization of all convergent algorithms for smooth non-convex objective functions. Notably, our framework is directly compatible with automatic differentiation tools, ensurin
&lt;/p&gt;</description></item><item><title>&#22312;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#19968;&#27425;&#24615;&#24179;&#22343;&#21270;&#30340;&#26041;&#27861;&#65292;&#27599;&#20010;agent&#29420;&#31435;&#36827;&#34892;TD($\lambda$)&#36816;&#31639;&#65292;&#24182;&#26368;&#32456;&#22312;&#32467;&#26524;&#19978;&#36827;&#34892;&#24179;&#22343;&#65292;&#23454;&#29616;&#20102;&#30456;&#23545;&#20110;&#20197;&#24448;&#24037;&#20316;&#26356;&#23569;&#30340;&#36890;&#20449;&#37327;&#35201;&#27714;&#30340;&#32447;&#24615;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.08896</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#24179;&#22343;&#21270;&#22312;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#19979;&#30340;&#20998;&#24067;&#24335;TD($\lambda$)
&lt;/p&gt;
&lt;p&gt;
One-Shot Averaging for Distributed TD($\lambda$) Under Markov Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08896
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#19968;&#27425;&#24615;&#24179;&#22343;&#21270;&#30340;&#26041;&#27861;&#65292;&#27599;&#20010;agent&#29420;&#31435;&#36827;&#34892;TD($\lambda$)&#36816;&#31639;&#65292;&#24182;&#26368;&#32456;&#22312;&#32467;&#26524;&#19978;&#36827;&#34892;&#24179;&#22343;&#65292;&#23454;&#29616;&#20102;&#30456;&#23545;&#20110;&#20197;&#24448;&#24037;&#20316;&#26356;&#23569;&#30340;&#36890;&#20449;&#37327;&#35201;&#27714;&#30340;&#32447;&#24615;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#27599;&#20010;agent&#37117;&#25317;&#26377;&#30456;&#21516;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21103;&#26412;&#65292;&#20294;&#26159;&#36716;&#25442;&#26159;&#29420;&#31435;&#22320;&#20174;&#30456;&#24212;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#20013;&#30001;&#27599;&#20010;agent&#37319;&#26679;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#20110;TD($\lambda$)&#30340;&#32447;&#24615;&#21152;&#36895;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#27969;&#34892;&#30340;&#29992;&#20110;&#31574;&#30053;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#21363;&#33509;&#30446;&#26631;&#31934;&#24230;&#36275;&#22815;&#23567;&#65292;$N$&#20010;agents&#21487;&#20197;&#20197;$N$&#20493;&#36895;&#24230;&#35780;&#20272;&#19968;&#20010;&#31574;&#30053;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#21152;&#36895;&#26159;&#36890;&#36807;&#8220;&#19968;&#27425;&#24615;&#24179;&#22343;&#21270;&#8221;&#23454;&#29616;&#30340;&#65292;&#21363;agent&#20204;&#29420;&#31435;&#22320;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#36816;&#34892;TD($\lambda$)&#65292;&#24182;&#19988;&#20165;&#22312;&#26368;&#21518;&#19968;&#27493;&#20043;&#21518;&#23545;&#20182;&#20204;&#30340;&#32467;&#26524;&#36827;&#34892;&#24179;&#22343;&#12290;&#30456;&#23545;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#36825;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#25152;&#38656;&#30340;&#36890;&#20449;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08896v1 Announce Type: new  Abstract: We consider a distributed setup for reinforcement learning, where each agent has a copy of the same Markov Decision Process but transitions are sampled from the corresponding Markov chain independently by each agent. We show that in this setting, we can achieve a linear speedup for TD($\lambda$), a family of popular methods for policy evaluation, in the sense that $N$ agents can evaluate a policy $N$ times faster provided the target accuracy is small enough. Notably, this speedup is achieved by ``one shot averaging,'' a procedure where the agents run TD($\lambda$) with Markov sampling independently and only average their results after the final step. This significantly reduces the amount of communication required to achieve a linear speedup relative to previous work.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#35757;&#32451;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.08333</link><description>&lt;p&gt;
&#24555;&#36895;&#25512;&#26029;&#22522;&#20110;&#31227;&#38500;&#30340;&#33410;&#28857;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Fast Inference of Removal-Based Node Influence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#35757;&#32451;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25429;&#33719;&#22270;&#20013;&#20449;&#24687;&#20256;&#25773;&#27169;&#24335;&#12290;&#34429;&#28982;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#36235;&#21183;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#34913;&#37327;&#35757;&#32451;&#22909;&#30340;GNN&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#12290;&#19968;&#20010;&#30495;&#23454;&#24212;&#29992;&#26159;&#65292;&#8220;&#22312;&#39044;&#27979;Twitter&#36134;&#25143;&#26497;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#22914;&#26524;&#31227;&#38500;&#29305;&#23450;&#36134;&#25143;&#65292;&#20854;&#20182;&#36134;&#25143;&#30340;&#26497;&#24615;&#20250;&#22914;&#20309;&#25913;&#21464;&#65311;&#8221;&#25105;&#20204;&#23558;GNN&#20316;&#20026;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#65292;&#20854;&#39044;&#27979;&#21487;&#20197;&#27169;&#25311;&#31227;&#38500;&#33410;&#28857;&#24341;&#36215;&#30340;&#33410;&#28857;&#25110;&#36793;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#33719;&#24471;&#27599;&#20010;&#33410;&#28857;&#30340;&#24433;&#21709;&#65292;&#19968;&#31181;&#30452;&#25509;&#30340;&#26041;&#27861;&#26159;&#20132;&#26367;&#31227;&#38500;&#27599;&#20010;&#33410;&#28857;&#65292;&#24182;&#22312;&#20462;&#25913;&#21518;&#30340;&#22270;&#19978;&#24212;&#29992;&#35757;&#32451;&#22909;&#30340;GNN&#12290;&#36825;&#26159;&#21487;&#38752;&#30340;&#20294;&#32791;&#26102;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08333v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) are widely utilized to capture the information spreading patterns in graphs. While remarkable performance has been achieved, there is a new trending topic of evaluating node influence. We propose a new method of evaluating node influence, which measures the prediction change of a trained GNN model caused by removing a node. A real-world application is, "In the task of predicting Twitter accounts' polarity, had a particular account been removed, how would others' polarity change?". We use the GNN as a surrogate model whose prediction could simulate the change of nodes or edges caused by node removal. To obtain the influence for every node, a straightforward way is to alternately remove every node and apply the trained GNN on the modified graph. It is reliable but time-consuming, so we need an efficient method. The related lines of work, such as graph adversarial attack and counterfactual explanation, cannot 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23376;&#34920;&#31034;&#27169;&#22411;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#22270;&#25991;&#27861;&#25551;&#36848;&#20998;&#23376;&#30340;&#23618;&#27425;&#21270;&#35774;&#35745;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#22312;&#35774;&#35745;&#31354;&#38388;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#23376;&#29983;&#25104;&#21644;&#23646;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#21487;&#21512;&#25104;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08147</link><description>&lt;p&gt;
&#23558;&#20998;&#23376;&#34920;&#31034;&#20026;&#21487;&#35299;&#37322;&#30340;&#25991;&#27861;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;
&lt;/p&gt;
&lt;p&gt;
Representing Molecules as Random Walks Over Interpretable Grammars
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08147
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23376;&#34920;&#31034;&#27169;&#22411;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#22270;&#25991;&#27861;&#25551;&#36848;&#20998;&#23376;&#30340;&#23618;&#27425;&#21270;&#35774;&#35745;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#22312;&#35774;&#35745;&#31354;&#38388;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#23376;&#29983;&#25104;&#21644;&#23646;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#21487;&#21512;&#25104;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20998;&#23376;&#25506;&#32034;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23567;&#22411;&#12289;&#31867;&#20284;&#33647;&#29289;&#30340;&#20998;&#23376;&#19978;&#65292;&#23548;&#33268;&#35768;&#22810;&#22312;&#26448;&#26009;&#35774;&#35745;&#20013;&#21516;&#26679;&#37325;&#35201;&#30340;&#24212;&#29992;&#32570;&#20047;&#36275;&#22815;&#30340;&#25216;&#26415;&#25903;&#25345;&#12290;&#36825;&#20123;&#24212;&#29992;&#36890;&#24120;&#20381;&#36182;&#20110;&#26356;&#22797;&#26434;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#26377;&#26356;&#23569;&#30340;&#20363;&#23376;&#65292;&#26159;&#20351;&#29992;&#24050;&#30693;&#30340;&#20122;&#32467;&#26500;&#31934;&#24515;&#35774;&#35745;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20197;&#22270;&#25991;&#27861;&#30340;&#24418;&#24335;&#34920;&#31034;&#21644;&#25512;&#29702;&#36825;&#20123;&#20998;&#23376;&#65292;&#26126;&#30830;&#25551;&#36848;&#20102;&#29305;&#24449;&#20026;&#35774;&#35745;&#22522;&#30784;&#30340;&#23618;&#27425;&#21270;&#35774;&#35745;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#21363;&#22312;&#35774;&#35745;&#31354;&#38388;&#19978;&#36827;&#34892;&#38543;&#26426;&#28216;&#36208;&#65292;&#26082;&#26377;&#21161;&#20110;&#20998;&#23376;&#29983;&#25104;&#65292;&#21448;&#26377;&#21161;&#20110;&#23646;&#24615;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#22312;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#39044;&#27979;&#20998;&#23376;&#21487;&#21512;&#25104;&#24615;&#26041;&#38754;&#30340;&#26126;&#26174;&#20248;&#21183;&#65292;&#24182;&#35814;&#32454;&#38416;&#36848;&#20102;&#35813;&#26041;&#27861;&#30340;&#21270;&#23398;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08147v1 Announce Type: new  Abstract: Recent research in molecular discovery has primarily been devoted to small, drug-like molecules, leaving many similarly important applications in material design without adequate technology. These applications often rely on more complex molecular structures with fewer examples that are carefully designed using known substructures. We propose a data-efficient and interpretable model for representing and reasoning over such molecules in terms of graph grammars that explicitly describe the hierarchical design space featuring motifs to be the design basis. We present a novel representation in the form of random walks over the design space, which facilitates both molecule generation and property prediction. We demonstrate clear advantages over existing methods in terms of performance, efficiency, and synthesizability of predicted molecules, and we provide detailed insights into the method's chemical interpretability.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#31181;&#21487;&#20197;&#20174;&#27169;&#22411;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#25968;&#25454;&#38598;SEP&#65292;&#29992;&#20110;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.06833</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#23558;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#21527;&#65311;&#25105;&#20204;&#20855;&#20307;&#25351;&#30340;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#31181;&#21487;&#20197;&#20174;&#27169;&#22411;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#25968;&#25454;&#38598;SEP&#65292;&#29992;&#20110;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06833v1 &#20844;&#21578;&#31867;&#22411;: &#36328; &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35843;&#33410;&#25351;&#20196;&#30340;&#25216;&#26415;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#25104;&#26524;&#65292;&#20026;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#25171;&#24320;&#20102;&#26080;&#25968;&#26032;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;LLMs&#32570;&#20047;&#20854;&#20182;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#24050;&#24314;&#31435;&#20026;&#35268;&#33539;&#30340;&#22522;&#26412;&#23433;&#20840;&#29305;&#24615;&#65292;&#27604;&#22914;&#25351;&#20196;&#19982;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#23548;&#33268;&#23427;&#20204;&#21457;&#29983;&#25925;&#38556;&#25110;&#26131;&#21463;&#31532;&#19977;&#26041;&#25805;&#25511;&#21644;&#24178;&#25200;&#65288;&#20363;&#22914;&#36890;&#36807;&#38388;&#25509;&#25552;&#31034;/&#21629;&#20196;&#27880;&#20837;&#65289;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#29978;&#33267;&#27809;&#26377;&#30830;&#20999;&#23450;&#20041;&#36825;&#31181;&#20998;&#31163;&#31350;&#31455;&#24847;&#21619;&#30528;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#27979;&#35797;&#20854;&#36829;&#21453;&#24773;&#20917;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#20010;&#21487;&#20197;&#20174;&#27169;&#22411;&#30340;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SEP&#65288;&#24212;&#35813;&#25191;&#34892;&#36824;&#26159;&#22788;&#29702;&#65311;&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#20801;&#35768;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06833v1 Announce Type: cross  Abstract: Instruction-tuned Large Language Models (LLMs) have achieved breakthrough results, opening countless new possibilities for many practical applications. However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection. Even worse, so far, there is not even an established definition of what precisely such a separation would mean and how its violation could be tested. In this work, we aim to close this gap. We introduce a formal measure to quantify the phenomenon of instruction-data separation as well as an empirical variant of the measure that can be computed from a model`s black-box outputs. We also introduce a new dataset, SEP (Should it be Executed or Processed?), which allows estimating th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#25554;&#20540;&#65292;&#23454;&#29616;&#20102;&#37319;&#26679;&#36895;&#24230;&#21644;&#37319;&#26679;&#36136;&#37327;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.06807</link><description>&lt;p&gt;
&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multistep Consistency Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#25554;&#20540;&#65292;&#23454;&#29616;&#20102;&#37319;&#26679;&#36895;&#24230;&#21644;&#37319;&#26679;&#36136;&#37327;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30456;&#23545;&#23481;&#26131;&#35757;&#32451;&#65292;&#20294;&#29983;&#25104;&#26679;&#26412;&#38656;&#35201;&#35768;&#22810;&#27493;&#39588;&#12290;&#19968;&#33268;&#24615;&#27169;&#22411;&#26356;&#38590;&#35757;&#32451;&#65292;&#20294;&#21487;&#20197;&#22312;&#19968;&#20010;&#27493;&#39588;&#20013;&#29983;&#25104;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#65306;&#36890;&#36807;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;TRACT&#30340;&#32479;&#19968;&#65292;&#21487;&#20197;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65306;&#22312;&#37319;&#26679;&#36895;&#24230;&#21644;&#37319;&#26679;&#36136;&#37327;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;1&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#26159;&#20256;&#32479;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#32780;&#25105;&#20204;&#23637;&#31034;&#20102;$\infty$&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#26159;&#25193;&#25955;&#27169;&#22411;&#12290;&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#23558;&#26679;&#26412;&#39044;&#31639;&#20174;&#21333;&#27493;&#22686;&#21152;&#21040;2-8&#27493;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#22823;&#37096;&#20998;&#37319;&#26679;&#36895;&#24230;&#20248;&#21183;&#12290;&#22312;Imagenet 64&#19978;8&#27493;&#36798;&#21040;1.4&#30340;FID&#65292;&#22312;Imagenet128&#19978;8&#27493;&#36798;&#21040;2.1&#30340;FID&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06807v1 Announce Type: new  Abstract: Diffusion models are relatively easy to train but require many steps to generate samples. Consistency models are far more difficult to train, but generate samples in a single step.   In this paper we propose Multistep Consistency Models: A unification between Consistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that can interpolate between a consistency model and a diffusion model: a trade-off between sampling speed and sampling quality. Specifically, a 1-step consistency model is a conventional consistency model whereas we show that a $\infty$-step consistency model is a diffusion model.   Multistep Consistency Models work really well in practice. By increasing the sample budget from a single step to 2-8 steps, we can train models more easily that generate higher quality samples, while retaining much of the sampling speed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1 FID on Imagenet128 in 8 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#20107;&#23454;&#24773;&#32490;&#25512;&#29702;&#65288;CLEF&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#19979;&#25991;&#20559;&#24046;&#24178;&#25200;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.05963</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#21435;&#20559;&#30340;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robust Emotion Recognition in Context Debiasing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05963
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#20107;&#23454;&#24773;&#32490;&#25512;&#29702;&#65288;CLEF&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#19979;&#25991;&#20559;&#24046;&#24178;&#25200;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#24773;&#32490;&#35782;&#21035;&#65288;CAER&#65289;&#26368;&#36817;&#22312;&#26080;&#32422;&#26463;&#29615;&#22659;&#20013;&#25512;&#21160;&#20102;&#24773;&#24863;&#35745;&#31639;&#25216;&#26415;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290; &#20027;&#27969;&#30340;CAER&#26041;&#27861;&#24635;&#26159;&#20174;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#21644;&#20197;&#20027;&#20307;&#20026;&#20013;&#24515;&#30340;&#29305;&#24449;&#20013;&#25552;&#21462;&#38598;&#25104;&#34920;&#31034;&#65292;&#20197;&#24863;&#30693;&#30446;&#26631;&#20154;&#29289;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290; &#23613;&#31649;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#26368;&#22823;&#30340;&#25361;&#25112;&#20173;&#28982;&#26159;&#30001;&#20110;&#19978;&#19979;&#25991;&#20559;&#24046;&#30340;&#24178;&#25200;&#12290; &#26377;&#23475;&#30340;&#20559;&#35265;&#36843;&#20351;&#27169;&#22411;&#20381;&#36182;&#20110;&#32972;&#26223;&#19978;&#19979;&#25991;&#21644;&#24773;&#24863;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#22312;&#21487;&#33021;&#24615;&#20272;&#35745;&#20013;&#36896;&#25104;&#20005;&#37325;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#24182;&#20351;&#26377;&#20215;&#20540;&#30340;&#19978;&#19979;&#25991;&#20808;&#39564;&#28151;&#28102;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#20107;&#23454;&#24773;&#32490;&#25512;&#29702;&#65288;CLEF&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#24191;&#20041;&#22240;&#26524;&#22270;&#65292;&#20197;&#35299;&#32806;CAER&#20013;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290; &#36981;&#24490;&#22240;&#26524;&#22270;&#65292;CLEF&#24341;&#20837;&#20102;&#19968;&#20010;&#38750;&#20405;&#20837;&#24335;&#30340;&#19978;&#19979;&#25991;&#20998;&#25903;&#26469;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05963v1 Announce Type: cross  Abstract: Context-aware emotion recognition (CAER) has recently boosted the practical applications of affective computing techniques in unconstrained environments. Mainstream CAER methods invariably extract ensemble representations from diverse contexts and subject-centred characteristics to perceive the target person's emotional state. Despite advancements, the biggest challenge remains due to context bias interference. The harmful bias forces the models to rely on spurious correlations between background contexts and emotion labels in likelihood estimation, causing severe performance bottlenecks and confounding valuable context priors. In this paper, we propose a counterfactual emotion inference (CLEF) framework to address the above issue. Specifically, we first formulate a generalized causal graph to decouple the causal relationships among the variables in CAER. Following the causal graph, CLEF introduces a non-invasive context branch to capt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Fed&#30340;&#31616;&#21333;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#37325;&#25773;&#65292;&#36890;&#36807;&#21327;&#35843;&#27599;&#20010;&#23458;&#25143;&#31471;&#32531;&#23384;&#37325;&#35201;&#26679;&#26412;&#20197;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05890</link><description>&lt;p&gt;
&#26397;&#30528;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#20013;&#39640;&#25928;&#30340;&#37325;&#25773;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Replay in Federated Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Fed&#30340;&#31616;&#21333;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#37325;&#25773;&#65292;&#36890;&#36807;&#21327;&#35843;&#27599;&#20010;&#23458;&#25143;&#31471;&#32531;&#23384;&#37325;&#35201;&#26679;&#26412;&#20197;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#36890;&#24120;&#20551;&#23450;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#26159;&#22266;&#23450;&#25110;&#38745;&#24577;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#36890;&#24120;&#20197;&#22686;&#37327;&#26041;&#24335;&#21040;&#26469;&#65292;&#20854;&#20013;&#25968;&#25454;&#39046;&#22495;&#21487;&#33021;&#21160;&#24577;&#22686;&#21152;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36793;&#32536;&#23458;&#25143;&#31471;&#22312;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#65288;FIL&#65289;&#22330;&#26223;&#20013;&#22240;&#25968;&#25454;&#24322;&#26500;&#24615;&#32780;&#21487;&#33021;&#32570;&#20047;&#36275;&#22815;&#23384;&#20648;&#31354;&#38388;&#20197;&#20445;&#30041;&#23436;&#25972;&#25968;&#25454;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Fed&#30340;&#31616;&#21333;&#12289;&#36890;&#29992;&#30340;FIL&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#21327;&#35843;&#27599;&#20010;&#23458;&#25143;&#31471;&#32531;&#23384;&#37325;&#25773;&#30340;&#37325;&#35201;&#26679;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#20986;&#29616;&#26032;&#20219;&#21153;&#26102;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#39318;&#20808;&#22522;&#20110;&#23427;&#20204;&#30340;&#20840;&#23616;&#21644;&#26412;&#22320;&#37325;&#35201;&#24615;&#32531;&#23384;&#36873;&#23450;&#30340;&#20808;&#21069;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#23458;&#25143;&#31471;&#20351;&#29992;&#26082;&#32531;&#23384;&#30340;&#26679;&#26412;&#21448;&#20351;&#29992;&#26032;&#20219;&#21153;&#30340;&#26679;&#26412;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;Re-Fed&#21457;&#29616;&#37325;&#25773;&#37325;&#35201;&#26679;&#26412;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05890v1 Announce Type: new  Abstract: In Federated Learning (FL), the data in each client is typically assumed fixed or static. However, data often comes in an incremental manner in real-world applications, where the data domain may increase dynamically. In this work, we study catastrophic forgetting with data heterogeneity in Federated Incremental Learning (FIL) scenarios where edge clients may lack enough storage space to retain full data. We propose to employ a simple, generic framework for FIL named Re-Fed, which can coordinate each client to cache important samples for replay. More specifically, when a new task arrives, each client first caches selected previous samples based on their global and local importance. Then, the client trains the local model with both the cached samples and the samples from the new task. Theoretically, we analyze the ability of Re-Fed to discover important samples for replay thus alleviating the catastrophic forgetting problem. Moreover, we e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20445;&#23432;DDPG&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;$Q$-&#30446;&#26631;&#21644;&#34892;&#20026;&#20811;&#38534;&#25439;&#22833;&#24809;&#32602;&#26469;&#35299;&#20915;DDPG&#20013;&#30340;&#39640;&#20272;&#20559;&#24046;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#38598;&#25104;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#23454;&#29616;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.05732</link><description>&lt;p&gt;
&#20445;&#23432;DDPG - &#26080;&#38598;&#25104;&#30340;&#24754;&#35266;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conservative DDPG -- Pessimistic RL without Ensemble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20445;&#23432;DDPG&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;$Q$-&#30446;&#26631;&#21644;&#34892;&#20026;&#20811;&#38534;&#25439;&#22833;&#24809;&#32602;&#26469;&#35299;&#20915;DDPG&#20013;&#30340;&#39640;&#20272;&#20559;&#24046;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#38598;&#25104;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#23454;&#29616;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DDPG&#21463;&#21040;&#39640;&#20272;&#20559;&#24046;&#38382;&#39064;&#30340;&#38459;&#30861;&#65292;&#20854;&#20013;&#20854;$Q$-&#20272;&#35745;&#20542;&#21521;&#20110;&#22840;&#22823;&#23454;&#38469;$Q$&#20540;&#12290;&#20256;&#32479;&#35299;&#20915;&#36825;&#19968;&#20559;&#35265;&#30340;&#26041;&#27861;&#28041;&#21450;&#22522;&#20110;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#25110;&#32773;&#22522;&#20110;&#22797;&#26434;&#23545;&#25968;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#38590;&#20197;&#29702;&#35299;&#21644;&#23454;&#26045;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;$Q$-&#30446;&#26631;&#24182;&#32467;&#21512;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#25439;&#22833;&#24809;&#32602;&#12290;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#29992;&#36739;&#23569;&#30340;&#20195;&#30721;&#23454;&#29616;&#65292;&#32780;&#26080;&#38656;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#24378;&#28872;&#25903;&#25345;&#20445;&#23432;DDPG&#22312;&#21508;&#31181;MuJoCo&#21644;Bullet&#20219;&#21153;&#19978;&#20248;&#20110;DDPG&#12290;&#25105;&#20204;&#22987;&#32456;&#35266;&#23519;&#21040;&#22312;&#25152;&#26377;&#35780;&#20272;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#22312;&#19982;TD3&#21644;TD7&#30456;&#27604;&#24615;&#33021;&#26356;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#20248;&#36234;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#26159;&#20197;&#26174;&#33879;&#38477;&#20302;&#30340;&#35745;&#31639;&#35201;&#27714;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05732v1 Announce Type: new  Abstract: DDPG is hindered by the overestimation bias problem, wherein its $Q$-estimates tend to overstate the actual $Q$-values. Traditional solutions to this bias involve ensemble-based methods, which require significant computational resources, or complex log-policy-based approaches, which are difficult to understand and implement. In contrast, we propose a straightforward solution using a $Q$-target and incorporating a behavioral cloning (BC) loss penalty. This solution, acting as an uncertainty measure, can be easily implemented with minimal code and without the need for an ensemble. Our empirical findings strongly support the superiority of Conservative DDPG over DDPG across various MuJoCo and Bullet tasks. We consistently observe better performance in all evaluated tasks and even competitive or superior performance compared to TD3 and TD7, all achieved with significantly reduced computational requirements.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#31934;&#30830;&#37325;&#26500;&#25209;&#37327;$b &gt;1$&#30340;&#31639;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03945</link><description>&lt;p&gt;
SPEAR&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#25209;&#37327;&#31934;&#30830;&#26799;&#24230;&#21453;&#28436;
&lt;/p&gt;
&lt;p&gt;
SPEAR:Exact Gradient Inversion of Batches in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03945
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#31934;&#30830;&#37325;&#26500;&#25209;&#37327;$b &gt;1$&#30340;&#31639;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#20165;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#20182;&#20204;&#26412;&#22320;&#25968;&#25454;&#30340;&#26799;&#24230;&#26356;&#26032;&#65292;&#32780;&#19981;&#26159;&#23454;&#38469;&#25968;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26368;&#36817;&#21457;&#29616;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#21487;&#20197;&#20174;&#36825;&#20123;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#26500;&#20986;&#25968;&#25454;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#21482;&#33021;&#22312;&#37325;&#35201;&#30340;&#35802;&#23454;&#20294;&#22909;&#22855;&#35774;&#32622;&#20013;&#23545;&#25209;&#37327;&#22823;&#23567;&#20026;$b=1$&#30340;&#25968;&#25454;&#36827;&#34892;&#31934;&#30830;&#37325;&#26500;&#65292;&#23545;&#20110;&#26356;&#22823;&#30340;&#25209;&#37327;&#21482;&#33021;&#36827;&#34892;&#36817;&#20284;&#37325;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\emph{&#31532;&#19968;&#20010;&#20934;&#30830;&#37325;&#24314;&#25209;&#37327;$b &gt;1$&#30340;&#31639;&#27861;}&#12290;&#36825;&#31181;&#26041;&#27861;&#32467;&#21512;&#20102;&#23545;&#26799;&#24230;&#26174;&#24335;&#20302;&#31209;&#32467;&#26500;&#30340;&#25968;&#23398;&#35265;&#35299;&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;ReLU&#35825;&#23548;&#30340;&#26799;&#24230;&#31232;&#30095;&#24615;&#65292;&#31934;&#30830;&#22320;&#36807;&#28388;&#25481;&#22823;&#37327;&#38169;&#35823;&#30340;&#26679;&#26412;&#65292;&#20351;&#26368;&#32456;&#30340;&#37325;&#24314;&#27493;&#39588;&#21487;&#34892;&#12290;&#25105;&#20204;&#20026;&#20840;&#36830;&#25509;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;GPU&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03945v1 Announce Type: new  Abstract: Federated learning is a popular framework for collaborative machine learning where multiple clients only share gradient updates on their local data with the server and not the actual data. Unfortunately, it was recently shown that gradient inversion attacks can reconstruct this data from these shared gradients. Existing attacks enable exact reconstruction only for a batch size of $b=1$ in the important honest-but-curious setting, with larger batches permitting only approximate reconstruction. In this work, we propose \emph{the first algorithm reconstructing whole batches with $b &gt;1$ exactly}. This approach combines mathematical insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected 
&lt;/p&gt;</description></item><item><title>GaLore&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Low-Rank Projection (GaLore)&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#30456;&#27604;&#20110;&#19968;&#33324;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;LLM&#35757;&#32451;&#65292;&#22823;&#24133;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03507</link><description>&lt;p&gt;
GaLore: &#36890;&#36807;&#26799;&#24230;&#20302;&#31209;&#25237;&#24433;&#23454;&#29616;&#39640;&#25928;&#30340;LLM&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03507
&lt;/p&gt;
&lt;p&gt;
GaLore&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Low-Rank Projection (GaLore)&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#30456;&#27604;&#20110;&#19968;&#33324;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;LLM&#35757;&#32451;&#65292;&#22823;&#24133;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23384;&#22312;&#26174;&#30528;&#30340;&#20869;&#23384;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#26435;&#37325;&#21644;&#20248;&#21270;&#22120;&#29366;&#24577;&#30340;&#19981;&#26029;&#22686;&#21152;&#12290;&#36890;&#24120;&#30340;&#20869;&#23384;&#20943;&#23569;&#26041;&#27861;&#65292;&#22914;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#22312;&#27599;&#20010;&#23618;&#20013;&#21521;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#28155;&#21152;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#30697;&#38453;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#21644;&#20248;&#21270;&#22120;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#30340;&#34920;&#29616;&#37117;&#19981;&#22914;&#23436;&#25972;&#31209;&#26435;&#37325;&#30340;&#35757;&#32451;&#65292;&#22240;&#20026;&#23427;&#20204;&#23558;&#21442;&#25968;&#25628;&#32034;&#38480;&#21046;&#22312;&#20302;&#31209;&#23376;&#31354;&#38388;&#24182;&#25913;&#21464;&#20102;&#35757;&#32451;&#21160;&#24577;&#65292;&#32780;&#19988;&#21487;&#33021;&#38656;&#35201;&#23436;&#25972;&#31209;&#30340;&#28909;&#21551;&#21160;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Gradient Low-Rank Projection (GaLore)&#65292;&#36825;&#26159;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#20801;&#35768;&#23436;&#20840;&#21442;&#25968;&#23398;&#20064;&#65292;&#20294;&#27604;LoRA&#31561;&#24120;&#35265;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#26356;&#33410;&#30465;&#20869;&#23384;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#22120;&#29366;&#24577;&#19978;&#23558;&#20869;&#23384;&#20351;&#29992;&#38477;&#20302;&#20102;&#39640;&#36798;65.5%&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39044;&#35757;&#32451;&#21644;&#31934;&#35843;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03507v1 Announce Type: new  Abstract: Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-tra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#27133;&#25277;&#35937;&#21270;&#22120;&#65292;&#32467;&#21512;&#27133;&#26041;&#27861;&#21644;&#20851;&#31995;&#24402;&#32435;&#20559;&#35265;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.03458</link><description>&lt;p&gt;
&#27133;&#25277;&#35937;&#21270;&#22120;&#65306;&#36808;&#21521;&#21487;&#25193;&#23637;&#30340;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Slot Abstractors: Toward Scalable Abstract Visual Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03458
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27133;&#25277;&#35937;&#21270;&#22120;&#65292;&#32467;&#21512;&#27133;&#26041;&#27861;&#21644;&#20851;&#31995;&#24402;&#32435;&#20559;&#35265;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#26159;&#19968;&#31181;&#20154;&#31867;&#29305;&#26377;&#30340;&#33021;&#21147;&#65292;&#20801;&#35768;&#35782;&#21035;&#20174;&#23545;&#35937;&#29305;&#24449;&#20013;&#25277;&#35937;&#20986;&#30340;&#20851;&#31995;&#27169;&#24335;&#65292;&#24182;&#23558;&#36825;&#20123;&#27169;&#24335;&#31995;&#32479;&#21270;&#22320;&#25512;&#24191;&#21040;&#26410;&#26366;&#35265;&#36807;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#28041;&#21450;&#22810;&#20010;&#23545;&#35937;&#36755;&#20837;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#27133;&#30340;&#26041;&#27861;&#25552;&#21462;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#24182;&#20855;&#26377;&#24378;&#24402;&#32435;&#20559;&#35265;&#30340;&#20851;&#31995;&#25277;&#35937;&#65292;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#31995;&#32479;&#21270;&#25512;&#24191;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20165;&#38480;&#20110;&#21253;&#21547;&#21333;&#19968;&#35268;&#21017;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#19981;&#36866;&#29992;&#20110;&#21253;&#21547;&#22823;&#37327;&#23545;&#35937;&#30340;&#35270;&#35273;&#25512;&#29702;&#38382;&#39064;&#12290;&#20854;&#20182;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#25277;&#35937;&#21270;&#22120;&#65292;&#36825;&#26159;Transformer&#30340;&#25193;&#23637;&#65292;&#34701;&#21512;&#20102;&#24378;&#22823;&#30340;&#20851;&#31995;&#24402;&#32435;&#20559;&#35265;&#65292;&#20174;&#32780;&#32487;&#25215;&#20102;Transformer&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#22836;&#26550;&#26500;&#65292;&#20294;&#23578;&#26410;&#23637;&#31034;&#22914;&#20309;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03458v1 Announce Type: cross  Abstract: Abstract visual reasoning is a characteristically human ability, allowing the identification of relational patterns that are abstracted away from object features, and the systematic generalization of those patterns to unseen problems. Recent work has demonstrated strong systematic generalization in visual reasoning tasks involving multi-object inputs, through the integration of slot-based methods used for extracting object-centric representations coupled with strong inductive biases for relational abstraction. However, this approach was limited to problems containing a single rule, and was not scalable to visual reasoning problems containing a large number of objects. Other recent work proposed Abstractors, an extension of Transformers that incorporates strong relational inductive biases, thereby inheriting the Transformer's scalability and multi-head architecture, but it has yet to be demonstrated how this approach might be applied to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.03020</link><description>&lt;p&gt;
SplAgger&#65306;&#29992;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#21106;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
SplAgger: Split Aggregation for Meta-Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#26680;&#24515;&#30446;&#26631;&#26159;&#21019;&#24314;&#33021;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#12290;&#20803;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#36825;&#20123;&#26234;&#33021;&#20307;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#19968;&#31867;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34987;&#31216;&#20026;&#40657;&#30418;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#29616;&#25104;&#30340;&#24207;&#21015;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#19982;&#20043;&#24418;&#25104;&#23545;&#27604;&#30340;&#26159;&#21478;&#19968;&#31867;&#26041;&#27861;&#65292;&#23427;&#20204;&#26126;&#30830;&#22320;&#25512;&#26029;&#20986;&#26410;&#30693;&#20219;&#21153;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#30446;&#26631;&#21644;&#24207;&#21015;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#20219;&#21153;&#25512;&#26029;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;&#20219;&#21153;&#25512;&#26029;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#35777;&#26126;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03020v1 Announce Type: cross  Abstract: A core ambition of reinforcement learning (RL) is the creation of agents capable of rapid learning in novel tasks. Meta-RL aims to achieve this by directly learning such agents. One category of meta-RL methods, called black box methods, does so by training off-the-shelf sequence models end-to-end. In contrast, another category of methods have been developed that explicitly infer a posterior distribution over the unknown task. These methods generally have distinct objectives and sequence models designed to enable task inference, and so are known as task inference methods. However, recent evidence suggests that task inference objectives are unnecessary in practice. Nonetheless, it remains unclear whether task inference sequence models are beneficial even when task inference objectives are not. In this paper, we present strong evidence that task inference sequence models are still beneficial. In particular, we investigate sequence models 
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#30340;&#21160;&#24577;&#29255;&#19978;&#30699;&#27491;&#26694;&#26550;DOCTOR&#65292;&#38024;&#23545;&#20809;&#23376;&#24352;&#37327;&#21152;&#36895;&#22120;&#20013;&#30340;&#26102;&#38388;&#28418;&#31227;&#21464;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#12289;&#21407;&#20301;&#20934;&#30830;&#24230;&#24674;&#22797;</title><link>https://arxiv.org/abs/2403.02688</link><description>&lt;p&gt;
DOCTOR: &#38024;&#23545;&#26102;&#38388;&#28418;&#31227;&#28909;&#21464;&#21270;&#30340;&#21160;&#24577;&#33455;&#29255;&#30699;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#25105;&#26657;&#27491;&#30340;&#20809;&#23376;&#24352;&#37327;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
DOCTOR: Dynamic On-Chip Remediation Against Temporally-Drifting Thermal Variations Toward Self-Corrected Photonic Tensor Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02688
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#30340;&#21160;&#24577;&#29255;&#19978;&#30699;&#27491;&#26694;&#26550;DOCTOR&#65292;&#38024;&#23545;&#20809;&#23376;&#24352;&#37327;&#21152;&#36895;&#22120;&#20013;&#30340;&#26102;&#38388;&#28418;&#31227;&#21464;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#12289;&#21407;&#20301;&#20934;&#30830;&#24230;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Photonic computing&#20316;&#20026;&#21152;&#36895;&#35745;&#31639;&#23494;&#38598;&#22411;&#20154;&#24037;&#26234;&#33021;(AI)&#24037;&#20316;&#36127;&#36733;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#12289;&#24310;&#36831;&#25935;&#24863;&#30340;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#36895;&#24230;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#20809;&#23376;&#24352;&#37327;&#21152;&#36895;&#22120;&#30340;&#37096;&#32626;&#36935;&#21040;&#20102;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#30001;&#20110;&#30828;&#20214;&#22122;&#22768;&#21644;&#29615;&#22659;&#21464;&#21270;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#33073;&#26426;&#22122;&#22768;&#24863;&#30693;&#35757;&#32451;&#21644;&#29255;&#19978;&#35757;&#32451;&#26469;&#22686;&#24378;&#23545;&#20855;&#26377;&#36866;&#24230;&#12289;&#38745;&#24577;&#22122;&#22768;&#30340;&#20809;&#23398;&#31070;&#32463;&#21152;&#36895;&#22120;&#30340;&#21464;&#21270;&#23481;&#24525;&#24230;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#30001;&#20110;&#26102;&#38388;&#28418;&#31227;&#21464;&#21270;&#23548;&#33268;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#38656;&#35201;&#23454;&#26102;&#12289;&#21407;&#20301;&#26657;&#20934;&#26426;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21160;&#24577;&#29255;&#19978;&#30699;&#27491;&#26694;&#26550;&#65292;&#31216;&#20026;DOCTOR&#65292;&#25552;&#20379;&#36866;&#24212;&#24615;&#30340;&#12289;&#21407;&#20301;&#30340;&#20934;&#30830;&#24230;&#24674;&#22797;&#65292;&#38024;&#23545;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02688v1 Announce Type: cross  Abstract: Photonic computing has emerged as a promising solution for accelerating computation-intensive artificial intelligence (AI) workloads, offering unparalleled speed and energy efficiency, especially in resource-limited, latency-sensitive edge computing environments. However, the deployment of analog photonic tensor accelerators encounters reliability challenges due to hardware noises and environmental variations. While off-chip noise-aware training and on-chip training have been proposed to enhance the variation tolerance of optical neural accelerators with moderate, static noises, we observe a notable performance degradation over time due to temporally drifting variations, which requires a real-time, in-situ calibration mechanism. To tackle this challenging reliability issues, for the first time, we propose a lightweight dynamic on-chip remediation framework, dubbed DOCTOR, providing adaptive, in-situ accuracy recovery against temporally
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;FedHCDR&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02630</link><description>&lt;p&gt;
FedHCDR: &#20855;&#26377;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02630
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;FedHCDR&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;CDR&#65289;&#22791;&#21463;&#20851;&#27880;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;&#25968;&#25454;&#26469;&#22686;&#24378;&#25512;&#33616;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;CDR&#26041;&#27861;&#38656;&#35201;&#36328;&#39046;&#22495;&#20849;&#20139;&#29992;&#25143;&#25968;&#25454;&#65292;&#36829;&#21453;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#65288;GDPR&#65289;&#12290;&#22240;&#27492;&#65292;&#24050;&#25552;&#20986;&#20102;&#35768;&#22810;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;FedCDR&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#19981;&#21487;&#36991;&#20813;&#22320;&#24433;&#21709;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedHCDR&#65292;&#19968;&#31181;&#20855;&#26377;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26032;&#22411;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#65288;HSD&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#29305;&#24449;&#35299;&#32806;&#20026;&#39046;&#22495;&#29420;&#26377;&#21644;&#39046;&#22495;&#20849;&#20139;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#39640;&#36890;&#21644;&#20302;&#36890;&#36229;&#22270;&#28388;&#27874;&#22120;&#26469;&#36827;&#34892;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02630v1 Announce Type: new  Abstract: In recent years, Cross-Domain Recommendation (CDR) has drawn significant attention, which utilizes user data from multiple domains to enhance the recommendation performance. However, current CDR methods require sharing user data across domains, thereby violating the General Data Protection Regulation (GDPR). Consequently, numerous approaches have been proposed for Federated Cross-Domain Recommendation (FedCDR). Nevertheless, the data heterogeneity across different domains inevitably influences the overall performance of federated learning. In this study, we propose FedHCDR, a novel Federated Cross-Domain Recommendation framework with Hypergraph signal decoupling. Specifically, to address the data heterogeneity across domains, we introduce an approach called hypergraph signal decoupling (HSD) to decouple the user features into domain-exclusive and domain-shared features. The approach employs high-pass and low-pass hypergraph filters to de
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#20998;&#23618;&#35821;&#20041;&#29615;&#22659;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#30340;&#19981;&#21464;&#23398;&#20064;&#65292;&#20197;&#22788;&#29702;&#20998;&#24067;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.01773</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#35821;&#20041;&#29615;&#22659;&#25913;&#21892;&#22270;&#20013;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Improving out-of-distribution generalization in graphs via hierarchical semantic environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01773
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#20998;&#23618;&#35821;&#20041;&#29615;&#22659;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#30340;&#19981;&#21464;&#23398;&#20064;&#65292;&#20197;&#22788;&#29702;&#20998;&#24067;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#20998;&#24067;&#36716;&#31227;&#21644;&#32570;&#20047;&#29615;&#22659;&#32972;&#26223;&#65292;&#22270;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#29983;&#25104;&#24179;&#38754;&#29615;&#22659;&#26469;&#22686;&#24378;&#22270;&#30340;OOD&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24179;&#38754;&#29615;&#22659;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#26080;&#27861;&#25429;&#25417;&#26356;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#21253;&#21547;&#21508;&#31181;&#35757;&#32451;&#29615;&#22659;&#65288;&#22914;&#39592;&#26550;&#12289;&#22823;&#23567;&#31561;&#65289;&#30340;DrugOOD&#25968;&#25454;&#38598;&#65292;&#24179;&#38754;&#29615;&#22659;&#26080;&#27861;&#20805;&#20998;&#35299;&#20915;&#20854;&#39640;&#24322;&#36136;&#24615;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#21363;&#29983;&#25104;&#26356;&#20855;&#35821;&#20041;&#20016;&#23500;&#30340;&#29615;&#22659;&#65292;&#20197;&#22686;&#24378;&#22270;&#30340;&#19981;&#21464;&#23398;&#20064;&#20197;&#22788;&#29702;&#20998;&#24067;&#36716;&#31227;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20026;&#27599;&#20010;&#22270;&#29983;&#25104;&#20998;&#23618;&#35821;&#20041;&#29615;&#22659;&#12290;&#39318;&#20808;&#65292;&#32473;&#23450;&#36755;&#20837;&#22270;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#25552;&#21462;&#36755;&#20837;&#22270;&#20013;&#30340;&#21464;&#20307;&#23376;&#22270;&#65292;&#20197;&#22312;&#26412;&#22320;&#29615;&#22659;&#19978;&#29983;&#25104;&#20195;&#29702;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#38543;&#26426;&#27880;&#24847;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01773v1 Announce Type: cross  Abstract: Out-of-distribution (OOD) generalization in the graph domain is challenging due to complex distribution shifts and a lack of environmental contexts. Recent methods attempt to enhance graph OOD generalization by generating flat environments. However, such flat environments come with inherent limitations to capture more complex data distributions. Considering the DrugOOD dataset, which contains diverse training environments (e.g., scaffold, size, etc.), flat contexts cannot sufficiently address its high heterogeneity. Thus, a new challenge is posed to generate more semantically enriched environments to enhance graph invariant learning for handling distribution shifts. In this paper, we propose a novel approach to generate hierarchical semantic environments for each graph. Firstly, given an input graph, we explicitly extract variant subgraphs from the input graph to generate proxy predictions on local environments. Then, stochastic attent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#36843;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#19978;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.00376</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#30340;&#19981;&#21464;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Invariant Test-Time Adaptation for Vision-Language Model Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#36843;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#19978;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00376v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#22312;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#20351;&#20854;&#22312;&#20247;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#38271;&#23614;&#20219;&#21153;&#65288;&#22914;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#65289;&#26102;&#26174;&#31034;&#20986;&#26126;&#26174;&#23616;&#38480;&#65292;&#36825;&#26159;&#30001;&#20110;&#8220;&#20915;&#31574;&#25463;&#24452;&#8221;&#23548;&#33268;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21463;&#38480;&#12290;&#26412;&#25991;&#21457;&#29616;CLIP&#27169;&#22411;&#20855;&#26377;&#20016;&#23500;&#30340;&#29305;&#24449;&#38598;&#65292;&#28085;&#30422;&#20102;&#26082;&#26377;&#30340;\textit{&#26399;&#26395;&#19981;&#21464;&#22240;&#26524;&#29305;&#24449;}&#21448;&#26377;&#30340;\textit{&#19981;&#24076;&#26395;&#30340;&#20915;&#31574;&#25463;&#24452;}&#12290;&#27492;&#22806;&#65292;CLIP&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#28304;&#33258;&#20854;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#20197;&#31526;&#21512;&#29305;&#23450;&#20219;&#21153;&#35201;&#27714;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#20248;&#21270;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20419;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00376v1 Announce Type: cross  Abstract: Vision-language foundation models have exhibited remarkable success across a multitude of downstream tasks due to their scalability on extensive image-text paired datasets. However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of "decision shortcuts" that hinders their generalization capabilities. In this work, we find that the CLIP model possesses a rich set of features, encompassing both \textit{desired invariant causal features} and \textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in accordance with specific task requirements. To address this challenge, this paper introduces a test-time prompt tuning paradigm that optimizes a learnable prompt, thereby compelling the model to exploit genuine causal invariant features while dis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181; FORML &#26041;&#27861;&#65292;&#20351;&#29992;&#26031;&#33922;&#22827;&#23572;&#27969;&#24418;&#19978;&#30340;&#19968;&#38454;&#23548;&#25968;&#36817;&#20284;&#65292;&#36890;&#36807;&#24341;&#20837;&#28023;&#26862;&#33258;&#30001;&#26041;&#27861;&#26469;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#24182;&#22312;&#20803;&#23398;&#20064;&#20013;&#23454;&#29616;&#21442;&#25968;&#27491;&#20132;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2402.18605</link><description>&lt;p&gt;
FORML&#65306;&#19968;&#31181;&#20855;&#26377;&#27491;&#20132;&#32422;&#26463;&#30340;&#27969;&#24418;&#28023;&#26862;&#33258;&#30001;&#20803;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FORML: A Riemannian Hessian-free Method for Meta-learning with Orthogonality Constraint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18605
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181; FORML &#26041;&#27861;&#65292;&#20351;&#29992;&#26031;&#33922;&#22827;&#23572;&#27969;&#24418;&#19978;&#30340;&#19968;&#38454;&#23548;&#25968;&#36817;&#20284;&#65292;&#36890;&#36807;&#24341;&#20837;&#28023;&#26862;&#33258;&#30001;&#26041;&#27861;&#26469;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#24182;&#22312;&#20803;&#23398;&#20064;&#20013;&#23454;&#29616;&#21442;&#25968;&#27491;&#20132;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#38382;&#39064;&#36890;&#24120;&#34987;&#34920;&#36848;&#20026;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#21644;&#20803;&#21442;&#25968;&#20998;&#21035;&#22312;&#20248;&#21270;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#24490;&#29615;&#20013;&#36827;&#34892;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#22312;&#40654;&#26364;&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#21442;&#25968;&#21644;&#20803;&#21442;&#25968;&#20301;&#20110;&#40654;&#26364;&#27969;&#24418;&#19978;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#38750;&#24120;&#23494;&#38598;&#30340;&#12290;&#19982;&#27431;&#20960;&#37324;&#24503;&#26041;&#27861;&#19981;&#21516;&#65292;&#40654;&#26364;&#21453;&#21521;&#20256;&#25773;&#38656;&#35201;&#35745;&#31639;&#21253;&#25324;&#36890;&#36807;&#40654;&#26364;&#31639;&#23376;&#65288;&#22914;&#25910;&#32553;&#21644;&#27491;&#20132;&#25237;&#24433;&#65289;&#30340;&#20108;&#38454;&#23548;&#25968;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26031;&#33922;&#22827;&#23572;&#27969;&#24418;&#19978;&#30340;&#23548;&#25968;&#30340;&#19968;&#38454;&#36817;&#20284;&#30340;&#28023;&#26862;&#33258;&#30001;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#36733;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#19968;&#20010;&#26031;&#33922;&#22827;&#23572;&#20840;&#36830;&#25509;&#23618;&#20316;&#20026;&#39592;&#24178;&#32593;&#32476;&#30340;&#26368;&#21518;&#20998;&#31867;&#23618;&#21442;&#25968;&#19978;&#30340;&#27491;&#20132;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18605v1 Announce Type: new  Abstract: Meta-learning problem is usually formulated as a bi-level optimization in which the task-specific and the meta-parameters are updated in the inner and outer loops of optimization, respectively. However, performing the optimization in the Riemannian space, where the parameters and meta-parameters are located on Riemannian manifolds is computationally intensive. Unlike the Euclidean methods, the Riemannian backpropagation needs computing the second-order derivatives that include backward computations through the Riemannian operators such as retraction and orthogonal projection. This paper introduces a Hessian-free approach that uses a first-order approximation of derivatives on the Stiefel manifold. Our method significantly reduces the computational load and memory footprint. We show how using a Stiefel fully-connected layer that enforces orthogonality constraint on the parameters of the last classification layer as the head of the backbon
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;PH&#21521;&#27744;&#21270;&#23618;&#27880;&#20837;&#20840;&#23616;&#25299;&#25169;&#19981;&#21464;&#24615;&#30340;&#26426;&#21046;&#26174;&#33879;&#25552;&#21319;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16346</link><description>&lt;p&gt;
&#29992;&#25345;&#20037;&#21516;&#35843;&#22686;&#24378;&#22270;&#27744;&#21270;
&lt;/p&gt;
&lt;p&gt;
Boosting Graph Pooling with Persistent Homology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16346
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;PH&#21521;&#27744;&#21270;&#23618;&#27880;&#20837;&#20840;&#23616;&#25299;&#25169;&#19981;&#21464;&#24615;&#30340;&#26426;&#21046;&#26174;&#33879;&#25552;&#21319;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#25345;&#20037;&#21516;&#35843;&#65288;PH&#65289;&#32435;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20197;&#20016;&#23500;&#34920;&#36798;&#33021;&#21147;&#30340;&#36235;&#21183;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#23558;PH&#29305;&#24449;&#25554;&#20837;GNN&#23618;&#24635;&#26159;&#24102;&#26469;&#36739;&#20302;&#21487;&#35299;&#37322;&#24615;&#30340;&#36793;&#38469;&#25913;&#36827;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;PH&#21521;&#27744;&#21270;&#23618;&#27880;&#20837;&#20840;&#23616;&#25299;&#25169;&#19981;&#21464;&#24615;&#65292;&#28789;&#24863;&#26469;&#33258;PH&#20013;&#30340;&#36807;&#28388;&#25805;&#20316;&#33258;&#28982;&#22320;&#20351;&#22270;&#27744;&#21270;&#20197;&#25130;&#26029;&#26041;&#24335;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#24335;&#19979;&#65292;&#31895;&#21270;&#22270;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#27839;&#30528;&#25345;&#20037;&#27744;&#21270;&#25299;&#25169;&#36827;&#34892;&#65292;&#20174;&#32780;&#25552;&#21319;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#35813;&#26426;&#21046;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#20960;&#20010;&#24120;&#35265;&#25968;&#25454;&#38598;&#19978;&#25345;&#32493;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#23637;&#31034;&#20102;&#20854;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16346v1 Announce Type: new  Abstract: Recently, there has been an emerging trend to integrate persistent homology (PH) into graph neural networks (GNNs) to enrich expressive power. However, naively plugging PH features into GNN layers always results in marginal improvement with low interpretability. In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns graph pooling in a cut-off manner. In this fashion, message passing in the coarsened graph acts along persistent pooled topology, leading to improved performance. Experimentally, we apply our mechanism to a collection of graph pooling methods and observe consistent and substantial performance gain over several popular datasets, demonstrating its wide applicability and flexibility.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#32422;$O(1/\epsilon)$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#30456;&#27604;&#20808;&#21069;&#25991;&#29486;&#20013;&#24050;&#26377;&#30340;$O(1/\epsilon^2)$&#26679;&#26412;&#22797;&#26434;&#24230;&#26377;&#25152;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.16324</link><description>&lt;p&gt;
&#23454;&#29616;&#32422;$O(1/\epsilon)$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#29992;&#20110;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Achieving $\tilde{O}(1/\epsilon)$ Sample Complexity for Constrained Markov Decision Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16324
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#32422;$O(1/\epsilon)$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#30456;&#27604;&#20808;&#21069;&#25991;&#29486;&#20013;&#24050;&#26377;&#30340;$O(1/\epsilon^2)$&#26679;&#26412;&#22797;&#26434;&#24230;&#26377;&#25152;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#39034;&#24207;&#23398;&#20064;&#21644;&#20915;&#31574;&#20013;&#28385;&#36275;&#23433;&#20840;&#24615;&#25110;&#36164;&#28304;&#32422;&#26463;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25317;&#26377;&#26377;&#38480;&#36164;&#28304;&#21644;&#26410;&#30693;&#36716;&#31227;&#27010;&#29575;&#30340;MDP&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#21462;&#19968;&#20010;&#34892;&#21160;&#65292;&#25910;&#38598;&#22870;&#21169;&#24182;&#28040;&#32791;&#19968;&#20123;&#36164;&#28304;&#65292;&#25152;&#26377;&#20551;&#35774;&#37117;&#26159;&#26410;&#30693;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#38543;&#30528;&#26102;&#38388;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#20026;CMDP&#38382;&#39064;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#38382;&#39064;&#30456;&#20851;&#20445;&#35777;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#23545;&#25968;&#36951;&#25022;&#30028;&#38480;&#65292;&#36825;&#36716;&#21270;&#20026;$O(\frac{\kappa}{\epsilon}\cdot\log^2(1/\epsilon))$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20854;&#20013;$\kappa$&#26159;&#19968;&#20010;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#21442;&#25968;&#65292;&#20294;&#19982;$\epsilon$&#26080;&#20851;&#12290;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#25913;&#36827;&#20102;&#20808;&#21069;&#25991;&#29486;&#20013;&#38024;&#23545;CMDP&#38382;&#39064;&#24314;&#31435;&#30340;$O(1/\epsilon^2)$&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16324v1 Announce Type: new  Abstract: We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a $O(\frac{\kappa}{\epsilon}\cdot\log^2(1/\epsilon))$ sample complexity bound, with $\kappa$ being a problem-dependent parameter, yet independent of $\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#19988;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15290</link><description>&lt;p&gt;
&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Linear Dynamics-embedded Neural Network for Long-Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15290
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#19988;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29616;&#26377;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#25104;&#20026;&#29942;&#39048;&#65292;&#21463;&#21040;&#25511;&#21046;&#29702;&#35770;&#20013;&#20855;&#26377;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#12290; SSM&#30340;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#21367;&#31215;&#23646;&#24615;&#20351;LDNN&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#30340;&#25512;&#26029;&#21644;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290; &#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26377;&#25928;&#31574;&#30053;&#65292;&#23545;&#35282;&#21270;&#21644;&#8220;&#35299;&#32806;&#28982;&#21518;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#8221;&#65292;&#20197;&#23558;&#21367;&#31215;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;$O(LNH\max\{L, N\})$&#38477;&#20302;&#21040;$O(LN\max\{H, \log L\})$&#12290; &#25105;&#20204;&#36890;&#36807;&#21452;&#21521;&#38750;&#22240;&#26524;&#21644;&#22810;&#22836;&#35774;&#32622;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;LDNN&#65292;&#20197;&#36866;&#24212;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290; &#23545;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#65288;LRA&#65289;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;LDNN&#30340;&#26377;&#25928;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15290v1 Announce Type: cross  Abstract: The trade-off between performance and computational efficiency in long-sequence modeling becomes a bottleneck for existing models. Inspired by the continuous state space models (SSMs) with multi-input and multi-output in control theory, we propose a new neural network called Linear Dynamics-embedded Neural Network (LDNN). SSMs' continuous, discrete, and convolutional properties enable LDNN to have few parameters, flexible inference, and efficient training in long-sequence tasks. Two efficient strategies, diagonalization and $'\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to reduce the time complexity of convolution from $O(LNH\max\{L, N\})$ to $O(LN\max \{H, \log L\})$. We further improve LDNN through bidirectional noncausal and multi-head settings to accommodate a broader range of applications. Extensive experiments on the Long Range Arena (LRA) demonstrate the effectiveness and state-of-the-art performance
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#24471;&#20998;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#23398;&#20064;&#35270;&#20026;&#23494;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27779;&#23572;&#20177;&#26031;&#22374;&#36317;&#31163;&#26469;&#25214;&#21040;&#23548;&#33268;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#22240;&#26524;&#27169;&#22411;</title><link>https://arxiv.org/abs/2402.15255</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#32467;&#26500;&#23398;&#20064;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport for Structure Learning Under Missing Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15255
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#24471;&#20998;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#23398;&#20064;&#35270;&#20026;&#23494;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27779;&#23572;&#20177;&#26031;&#22374;&#36317;&#31163;&#26469;&#25214;&#21040;&#23548;&#33268;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#22833;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#20250;&#24341;&#20837;&#40481;&#29983;&#34507;&#38382;&#39064;&#12290;&#34429;&#28982;&#30446;&#26631;&#26159;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#20294;&#40065;&#26834;&#30340;&#25554;&#34917;&#38656;&#35201;&#32771;&#34385;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#25110;&#26356;&#22909;&#22320;&#22240;&#26524;&#20851;&#31995;&#12290;&#20165;&#20165;&#29992;&#29616;&#26377;&#30340;&#25554;&#34917;&#26041;&#27861;&#22635;&#20805;&#32570;&#22833;&#20540;&#65292;&#28982;&#21518;&#22312;&#23436;&#25972;&#25968;&#25454;&#19978;&#24212;&#29992;&#32467;&#26500;&#23398;&#20064;&#34987;&#35777;&#26126;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#12290;&#36825;&#31181;&#26368;&#20248;&#36755;&#36816;&#30340;&#35266;&#28857;&#19981;&#21516;&#20110;&#29616;&#26377;&#22522;&#20110;EM&#30340;&#22522;&#20110;&#24471;&#20998;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#32467;&#26500;&#23398;&#20064;&#25237;&#24433;&#20026;&#23494;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#24341;&#36215;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#21644;&#35266;&#27979;&#25968;&#25454;&#20043;&#38388;&#30340;&#27779;&#23572;&#20177;&#26031;&#22374;&#36317;&#31163;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15255v1 Announce Type: cross  Abstract: Causal discovery in the presence of missing data introduces a chicken-and-egg dilemma. While the goal is to recover the true causal structure, robust imputation requires considering the dependencies or preferably causal relations among variables. Merely filling in missing values with existing imputation methods and subsequently applying structure learning on the complete data is empirical shown to be sub-optimal. To this end, we propose in this paper a score-based algorithm, based on optimal transport, for learning causal structure from missing data. This optimal transport viewpoint diverges from existing score-based approaches that are dominantly based on EM. We project structure learning as a density fitting problem, where the goal is to find the causal model that induces a distribution of minimum Wasserstein distance with the distribution over the observed data. Through extensive simulations and real-data experiments, our framework 
&lt;/p&gt;</description></item><item><title>RED&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.15179</link><description>&lt;p&gt;
&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#25512;&#36827;&#24494;&#35843;&#20013;&#30340;&#21442;&#25968;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Advancing Parameter Efficiency in Fine-tuning via Representation Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15179
&lt;/p&gt;
&lt;p&gt;
RED&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#22240;&#20854;&#33021;&#22815;&#22312;&#20165;&#26356;&#26032;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#26102;&#36798;&#21040;&#31454;&#20105;&#24615;&#32467;&#26524;&#32780;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#31070;&#32463;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#34920;&#31034;&#32534;&#36753;&#65288;RED&#65289;&#65292;&#20854;&#25193;&#25918;&#21644;&#20559;&#32622;&#27599;&#19968;&#23618;&#20135;&#29983;&#30340;&#34920;&#31034;&#12290;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#27604;&#65292;RED&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#38477;&#20302;&#20102;$25,700$&#20493;&#65292;&#24182;&#19982;LoRA&#30456;&#27604;&#38477;&#20302;&#20102;32&#20493;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;RED&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#23545;&#19981;&#21516;&#26550;&#26500;&#21644;&#35268;&#27169;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15179v1 Announce Type: cross  Abstract: Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for its ability to achieve competitive results while updating only a small subset of trainable parameters. Despite the promising performance of current PEFT methods, they present challenges in hyperparameter selection, such as determining the rank of LoRA or Adapter, or specifying the length of soft prompts. In addressing these challenges, we propose a novel approach to fine-tuning neural models, termed Representation EDiting (RED), which scales and biases the representation produced at each layer. RED substantially reduces the number of trainable parameters by a factor of $25,700$ compared to full parameter fine-tuning, and by a factor of $32$ compared to LoRA. Remarkably, RED achieves comparable or superior results to full parameter fine-tuning and other PEFT methods. Extensive experiments were conducted across models of varying architectures and scales, includin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36793;&#32536;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#26102;&#30340;&#25193;&#23637;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15106</link><description>&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20998;&#24067;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sampling-based Distributed Training with Message Passing Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36793;&#32536;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#26102;&#30340;&#25193;&#23637;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22495;&#20998;&#35299;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#26029;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#38543;&#30528;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#32780;&#25193;&#23637;&#36793;&#32536;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;Nystrom-&#36817;&#20284;&#37319;&#26679;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;DS-MPNN&#65288;&#20854;&#20013;D&#21644;S&#20998;&#21035;&#20195;&#34920;&#20998;&#24067;&#24335;&#21644;&#37319;&#26679;&#65289;&#65292;&#33021;&#22815;&#25193;&#23637;&#21040;$O(10^5)$&#20010;&#33410;&#28857;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26696;&#20363;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#37319;&#26679;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65306;&#65288;a&#65289;Darcy&#27969;&#25968;&#25454;&#38598;&#21644;&#65288;b&#65289;2-D&#26426;&#32764;&#30340;&#31283;&#24577;RANS&#27169;&#25311;&#65292;&#25552;&#20379;&#20102;&#19982;&#21333;GPU&#23454;&#29616;&#21644;&#22522;&#20110;&#33410;&#28857;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#30340;&#27604;&#36739;&#12290;DS-MPNN&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#21333;GPU&#23454;&#29616;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#23481;&#32435;&#27604;&#21333;&#20010;GPU&#23454;&#29616;&#26356;&#22810;&#25968;&#37327;&#30340;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15106v1 Announce Type: new  Abstract: In this study, we introduce a domain-decomposition-based distributed training and inference approach for message-passing neural networks (MPNN). Our objective is to address the challenge of scaling edge-based graph neural networks as the number of nodes increases. Through our distributed training approach, coupled with Nystr\"om-approximation sampling techniques, we present a scalable graph neural network, referred to as DS-MPNN (D and S standing for distributed and sampled, respectively), capable of scaling up to $O(10^5)$ nodes. We validate our sampling and distributed training approach on two cases: (a) a Darcy flow dataset and (b) steady RANS simulations of 2-D airfoils, providing comparisons with both single-GPU implementation and node-based graph convolution networks (GCNs). The DS-MPNN model demonstrates comparable accuracy to single-GPU implementation, can accommodate a significantly larger number of nodes compared to the single-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;KIEval&#65292;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;LLM-powered "interactor"&#35282;&#33394;&#23454;&#29616;&#21160;&#24577;&#30340;&#25239;&#27745;&#26579;&#35780;&#20272;</title><link>https://arxiv.org/abs/2402.15043</link><description>&lt;p&gt;
KIEval&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;KIEval&#65292;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;LLM-powered "interactor"&#35282;&#33394;&#23454;&#29616;&#21160;&#24577;&#30340;&#25239;&#27745;&#26579;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#35780;&#20272;&#34987;&#22840;&#22823;&#12290;&#29616;&#26377;&#30340;&#31574;&#30053;&#26088;&#22312;&#26816;&#27979;&#21463;&#27745;&#26579;&#30340;&#25991;&#26412;&#65292;&#20294;&#20391;&#37325;&#20110;&#37327;&#21270;&#27745;&#26579;&#31243;&#24230;&#32780;&#38750;&#20934;&#30830;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;KIEval&#65292;&#36825;&#26159;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;LLM&#39537;&#21160;&#30340;&#8220;&#20132;&#20114;&#32773;&#8221;&#35282;&#33394;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#25239;&#27745;&#26579;&#35780;&#20272;&#12290;&#20174;&#28041;&#21450;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#30340;&#24120;&#35268;LLM&#22522;&#20934;&#38382;&#39064;&#24320;&#22987;&#65292;KIEval&#21033;&#29992;&#21160;&#24577;&#29983;&#25104;&#30340;&#12289;&#22810;&#36718;&#12289;&#20197;&#30693;&#35782;&#20026;&#37325;&#28857;&#30340;&#23545;&#35805;&#65292;&#20197;&#30830;&#23450;&#27169;&#22411;&#30340;&#21709;&#24212;&#26159;&#21542;&#20165;&#26159;&#22522;&#20934;&#31572;&#26696;&#30340;&#22238;&#24518;&#65292;&#36824;&#26159;&#34920;&#26126;&#20102;&#28145;&#20837;&#29702;&#35299;&#24182;&#33021;&#22312;&#26356;&#22797;&#26434;&#30340;&#23545;&#35805;&#20013;&#24212;&#29992;&#30693;&#35782;&#12290;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#19971;&#20010;&#39046;&#20808;&#30340;LLM&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;KI
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15043v1 Announce Type: cross  Abstract: Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered "interactor" role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KI
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#21019;&#30340;&#37327;&#23376;&#35745;&#31639;&#20844;&#24335;&#65292;&#29992;&#20110;&#24773;&#22659;&#21270;&#36755;&#36865;&#35745;&#21010;&#30340;&#25674;&#38144;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#32972;&#26223;&#24773;&#22659;&#20013;&#33647;&#29289;&#21058;&#37327;&#21442;&#25968;&#21270;&#30340;&#32454;&#32990;&#31867;&#22411;&#20998;&#24067;&#30340;&#21464;&#21270;&#26469;&#39564;&#35777;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#25429;&#25417;&#21058;&#37327;&#24341;&#36215;&#30340;&#32454;&#32990;&#20998;&#24067;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14991</link><description>&lt;p&gt;
&#37327;&#23376;&#29702;&#35770;&#19982;&#24773;&#22659;&#26368;&#20248;&#36755;&#36816;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantum Theory and Application of Contextual Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14991
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#21019;&#30340;&#37327;&#23376;&#35745;&#31639;&#20844;&#24335;&#65292;&#29992;&#20110;&#24773;&#22659;&#21270;&#36755;&#36865;&#35745;&#21010;&#30340;&#25674;&#38144;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#32972;&#26223;&#24773;&#22659;&#20013;&#33647;&#29289;&#21058;&#37327;&#21442;&#25968;&#21270;&#30340;&#32454;&#32990;&#31867;&#22411;&#20998;&#24067;&#30340;&#21464;&#21270;&#26469;&#39564;&#35777;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#25429;&#25417;&#21058;&#37327;&#24341;&#36215;&#30340;&#32454;&#32990;&#20998;&#24067;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#65288;Optimal Transport&#65292;OT&#65289;&#25512;&#21160;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#22312;&#27979;&#37327;&#25968;&#25454;&#65288;$\mu$&#65292;$\nu$&#65289;&#19982;&#19978;&#19979;&#25991;&#21464;&#37327; $p_i$ &#32806;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#21162;&#21147;&#23398;&#20064;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#21487;&#33021;&#30475;&#19981;&#35265;&#30340;&#19978;&#19979;&#25991;&#21442;&#25968;&#21270;&#30340;&#20840;&#23616;&#36755;&#36816;&#26144;&#23556;&#12290;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#65292;&#24182;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;Brenier&#23450;&#29702;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#21019;&#30340;&#37327;&#23376;&#35745;&#31639;&#20844;&#24335;&#65292;&#29992;&#20110;&#24773;&#22659;&#21270;&#36755;&#36865;&#35745;&#21010;&#30340;&#25674;&#38144;&#20248;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;&#21452;&#38543;&#26426;&#30697;&#38453;&#21644;&#37193;&#31639;&#31526;&#20043;&#38388;&#30340;&#30452;&#25509;&#32852;&#31995;&#65292;&#20174;&#32780;&#25214;&#21040;&#20102;&#26368;&#20248;&#36755;&#36816;&#21644;&#37327;&#23376;&#35745;&#31639;&#20043;&#38388;&#30340;&#33258;&#28982;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#39564;&#35777;&#65292;&#39044;&#27979;&#36890;&#36807;&#33647;&#29289;&#21058;&#37327;&#21442;&#25968;&#21270;&#30340;&#32454;&#32990;&#31867;&#22411;&#20998;&#24067;&#30340;&#21464;&#21270;&#20316;&#20026;&#32972;&#26223;&#24773;&#22659;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#20010;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#21040;&#21058;&#37327;&#24341;&#36215;&#30340;&#32454;&#32990;&#20998;&#24067;&#21464;&#21270;&#65292;&#29978;&#33267;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14991v1 Announce Type: new  Abstract: Optimal Transport (OT) has fueled machine learning (ML) applications across many domains. In cases where paired data measurements ($\mu$, $\nu$) are coupled to a context variable $p_i$ , one may aspire to learn a global transportation map that can be parameterized through a potentially unseen con-text. Existing approaches utilize Neural OT and largely rely on Brenier's theorem. Here, we propose a first-of-its-kind quantum computing formulation for amortized optimization of contextualized transportation plans. We exploit a direct link between doubly stochastic matrices and unitary operators thus finding a natural connection between OT and quantum computation. We verify our method on synthetic and real data, by predicting variations in cell type distributions parameterized through drug dosage as context. Our comparisons to several baselines reveal that our method can capture dose-induced variations in cell distributions, even to some exten
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35770;&#25991;&#25506;&#35752;&#20102;&#22312;MLLM&#31038;&#20250;&#20013;&#36890;&#36807;&#21333;&#20010;&#25805;&#20316;&#21592;&#38388;&#25509;&#24433;&#21709;&#20854;&#20182;&#20195;&#29702;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#26032;&#22411;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2402.14859</link><description>&lt;p&gt;
&#20869;&#22312;&#30340;&#29436;&#65306;&#36890;&#36807;MLLM&#25805;&#20316;&#21592;&#21521;MLLM&#31038;&#20250;&#20013;&#28183;&#20837;&#24694;&#24847;
&lt;/p&gt;
&lt;p&gt;
The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14859
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35770;&#25991;&#25506;&#35752;&#20102;&#22312;MLLM&#31038;&#20250;&#20013;&#36890;&#36807;&#21333;&#20010;&#25805;&#20316;&#21592;&#38388;&#25509;&#24433;&#21709;&#20854;&#20182;&#20195;&#29702;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#26032;&#22411;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#22788;&#29702;&#21644;&#21709;&#24212;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#33021;&#21147;&#65292;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#19981;&#26029;&#23450;&#20041;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#26032;&#36793;&#30028;&#12290;&#38543;&#30528;&#36825;&#20123;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24418;&#25104;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#30340;&#21327;&#20316;&#32593;&#32476;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#12298;&#20869;&#22312;&#30340;&#29436;&#12299;&#25506;&#35752;&#20102;MLLM&#31038;&#20250;&#20013;&#30340;&#19968;&#31181;&#26032;&#22411;&#28431;&#27934; - &#24694;&#24847;&#20869;&#23481;&#30340;&#38388;&#25509;&#20256;&#25773;&#12290;&#19982;&#30452;&#25509;&#20026;MLLM&#29983;&#25104;&#26377;&#23475;&#36755;&#20986;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#20010;&#21333;&#20010;MLLM&#20195;&#29702;&#22914;&#20309;&#34987;&#24494;&#22937;&#22320;&#24433;&#21709;&#65292;&#20197;&#29983;&#25104;&#20877;&#27425;&#35825;&#20351;&#31038;&#20250;&#20013;&#20854;&#20182;MLLM&#20195;&#29702;&#36755;&#20986;&#24694;&#24847;&#20869;&#23481;&#30340;&#25552;&#31034;&#12290;&#36825;&#31181;&#24494;&#22937;&#32780;&#24378;&#26377;&#21147;&#30340;&#38388;&#25509;&#24433;&#21709;&#26041;&#27861;&#26631;&#24535;&#30528;&#19982;MLLM&#30456;&#20851;&#30340;&#23433;&#20840;&#39118;&#38505;&#30340;&#26174;&#33879;&#21319;&#32423;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21363;&#20351;&#20960;&#20046;&#27809;&#26377;&#25110;&#26159;&#26681;&#26412;&#27809;&#26377;&#35775;&#38382;MLLM&#21442;&#25968;&#65292;&#19968;&#20010;MLLM&#20195;&#29702;&#65292;&#24403;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14859v1 Announce Type: cross  Abstract: Due to their unprecedented ability to process and respond to various types of data, Multimodal Large Language Models (MLLMs) are constantly defining the new boundary of Artificial General Intelligence (AGI). As these advanced generative models increasingly form collaborative networks for complex tasks, the integrity and security of these systems are crucial. Our paper, ``The Wolf Within'', explores a novel vulnerability in MLLM societies - the indirect propagation of malicious content. Unlike direct harmful output generation for MLLMs, our research demonstrates how a single MLLM agent can be subtly influenced to generate prompts that, in turn, induce other MLLM agents in the society to output malicious content. This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs. Our findings reveal that, with minimal or even no access to MLLMs' parameters, an MLLM agent, when 
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.14809</link><description>&lt;p&gt;
CriticBench&#65306;&#20026;&#25209;&#21028;&#24615;-&#27491;&#30830;&#25512;&#29702;&#35780;&#20272;LLMs&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14809
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25209;&#21028;&#21644;&#23436;&#21892;&#20854;&#25512;&#29702;&#30340;&#33021;&#21147;&#23545;&#20110;&#23427;&#20204;&#22312;&#35780;&#20272;&#12289;&#21453;&#39304;&#25552;&#20379;&#21644;&#33258;&#25105;&#25913;&#36827;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CriticBench&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25209;&#21028;&#21644;&#32416;&#27491;&#20854;&#25512;&#29702;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;CriticBench&#21253;&#21547;&#20116;&#20010;&#25512;&#29702;&#39046;&#22495;&#65306;&#25968;&#23398;&#12289;&#24120;&#35782;&#12289;&#31526;&#21495;&#12289;&#32534;&#30721;&#21644;&#31639;&#27861;&#12290;&#23427;&#25972;&#21512;&#20102;15&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#20102;&#19977;&#20010;LLM&#31995;&#21015;&#30340;&#21709;&#24212;&#12290;&#21033;&#29992;CriticBench&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#21078;&#26512;&#20102;17&#20010;LLMs&#22312;&#29983;&#25104;&#12289;&#25209;&#21028;&#21644;&#20462;&#27491;&#25512;&#29702;&#65288;&#21363;GQC&#25512;&#29702;&#65289;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;1&#65289;GQC&#33021;&#21147;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65307;&#65288;2&#65289;&#20462;&#27491;&#25928;&#26524;&#22312;&#20219;&#21153;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#20197;&#36923;&#36753;&#20026;&#23548;&#21521;&#30340;&#20219;&#21153;&#26356;&#23481;&#26131;&#20462;&#27491;&#65307;&#65288;3&#65289;GQC&#30693;&#35782;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
&lt;/p&gt;</description></item><item><title>Q-Probe&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#31616;&#21333;&#30340;&#32447;&#24615;&#20989;&#25968;&#22312;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#37325;&#26032;&#21152;&#26435;&#20505;&#36873;&#23436;&#25104;&#65292;&#20174;&#32780;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#33719;&#24471;&#26174;&#33879;&#25910;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.14688</link><description>&lt;p&gt;
Q-Probe: &#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22870;&#21169;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Q-Probe: A Lightweight Approach to Reward Maximization for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14688
&lt;/p&gt;
&lt;p&gt;
Q-Probe&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#31616;&#21333;&#30340;&#32447;&#24615;&#20989;&#25968;&#22312;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#37325;&#26032;&#21152;&#26435;&#20505;&#36873;&#23436;&#25104;&#65292;&#20174;&#32780;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#33719;&#24471;&#26174;&#33879;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Q-probing&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;Q-probing&#20301;&#20110;&#20687;&#24494;&#35843;&#36825;&#26679;&#36739;&#37325;&#30340;&#26041;&#27861;&#21644;&#20687;&#23569;&#37327;&#25552;&#31034;&#36825;&#26679;&#36739;&#36731;&#30340;&#26041;&#27861;&#20043;&#38388;&#65292;&#20294;&#20063;&#21487;&#20197;&#19982;&#20219;&#19968;&#31181;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#20854;&#24819;&#27861;&#26159;&#22312;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#19978;&#23398;&#20064;&#19968;&#20010;&#31616;&#21333;&#30340;&#32447;&#24615;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#21487;&#29992;&#20110;&#37325;&#26032;&#21152;&#26435;&#20505;&#36873;&#23436;&#25104;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#38543;&#30528;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#37319;&#26679;&#36807;&#31243;&#31561;&#21516;&#20110;Q-probe&#30340;KL&#32422;&#26463;&#26368;&#22823;&#21270;&#12290;&#20026;&#20102;&#35757;&#32451;Q-probes&#65292;&#25105;&#20204;&#32771;&#34385;&#22870;&#21169;&#24314;&#27169;&#25110;&#22522;&#20110;&#37325;&#35201;&#24615;&#21152;&#26435;&#31574;&#30053;&#26799;&#24230;&#30340;&#19968;&#31867;&#26032;&#22411;&#30452;&#25509;&#31574;&#30053;&#23398;&#20064;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#31181;&#25216;&#26415;&#65292;&#25105;&#20204;&#30475;&#21040;&#22312;&#20855;&#26377;&#22522;&#20110;&#22320;&#38754;&#30495;&#23454;&#22870;&#21169;&#65288;&#20195;&#30721;&#29983;&#25104;&#65289;&#20197;&#21450;&#30001;&#20559;&#22909;&#25968;&#25454;&#23450;&#20041;&#30340;&#38544;&#24335;&#22870;&#21169;&#30340;&#39046;&#22495;&#20013;&#33719;&#24471;&#25910;&#30410;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#32988;&#36807;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;Q-probe&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14688v1 Announce Type: new  Abstract: We present an approach called Q-probing to adapt a pre-trained language model to maximize a task-specific reward function. At a high level, Q-probing sits between heavier approaches such as finetuning and lighter approaches such as few shot prompting, but can also be combined with either. The idea is to learn a simple linear function on a model's embedding space that can be used to reweight candidate completions. We theoretically show that this sampling procedure is equivalent to a KL-constrained maximization of the Q-probe as the number of samples increases. To train the Q-probes we consider either reward modeling or a class of novel direct policy learning objectives based on importance weighted policy gradients. With this technique, we see gains in domains with ground-truth rewards (code generation) as well as implicit rewards defined by preference data, even outperforming finetuning in data-limited regimes. Moreover, a Q-probe can be 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#30340;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#20197;&#19982;&#20043;&#21363;&#25554;&#21363;&#29992;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#23545;&#38899;&#20048;&#36136;&#37327;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;</title><link>https://arxiv.org/abs/2402.14285</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#25193;&#25955;&#30340;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14285
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#30340;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#20197;&#19982;&#20043;&#21363;&#25554;&#21363;&#29992;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#23545;&#38899;&#20048;&#36136;&#37327;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#38382;&#39064;&#65288;&#20363;&#22914;&#29983;&#25104;&#38050;&#29748;&#21367;&#35889;&#65289;&#65292;&#25216;&#26415;&#37325;&#28857;&#25918;&#22312;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#19978;&#12290;&#38899;&#20048;&#35268;&#21017;&#36890;&#24120;&#20197;&#31526;&#21495;&#24418;&#24335;&#34920;&#36798;&#22312;&#38899;&#31526;&#29305;&#24449;&#19978;&#65292;&#22914;&#38899;&#31526;&#23494;&#24230;&#25110;&#21644;&#24358;&#36827;&#34892;&#65292;&#35768;&#22810;&#35268;&#21017;&#26159;&#19981;&#21487;&#24494;&#20998;&#30340;&#65292;&#36825;&#22312;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#24341;&#23548;&#25193;&#25955;&#26102;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#26041;&#27861;&#65292;&#31216;&#20026;&#38543;&#26426;&#25511;&#21046;&#24341;&#23548;&#65288;SCG&#65289;&#65292;&#23427;&#20165;&#38656;&#35201;&#23545;&#35268;&#21017;&#20989;&#25968;&#36827;&#34892;&#21069;&#21521;&#35780;&#20272;&#65292;&#21487;&#20197;&#19982;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#19968;&#36215;&#24037;&#20316;&#65292;&#20174;&#32780;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#30340;&#26080;&#35757;&#32451;&#24341;&#23548;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#21487;&#20197;&#19982;SCG&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#32452;&#21512;&#12290;&#19982;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#26631;&#20934;&#24378;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#22312;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14285v1 Announce Type: cross  Abstract: We study the problem of symbolic music generation (e.g., generating piano rolls), with a technical focus on non-differentiable rule guidance. Musical rules are often expressed in symbolic form on note characteristics, such as note density or chord progression, many of which are non-differentiable which pose a challenge when using them for guided diffusion. We propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained diffusion models in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time. Additionally, we introduce a latent diffusion architecture for symbolic music generation with high time resolution, which can be composed with SCG in a plug-and-play fashion. Compared to standard strong baselines in symbolic music generation, this framework demonstrates marked advancements in music quali
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#38543;&#26426;&#23376;&#38598;&#30340;&#21021;&#22987;&#26435;&#37325;&#26469;&#20943;&#23569;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLT&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#38477;&#20302;&#20102;SLT&#25628;&#32034;&#31354;&#38388;&#65292;&#20445;&#35777;&#20102;SLT&#22312;&#36825;&#31181;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.14029</link><description>&lt;p&gt;
&#20923;&#32467;&#32593;&#32476;&#20013;&#30340;&#37096;&#20998;&#25628;&#32034;&#36275;&#20197;&#25214;&#21040;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;
&lt;/p&gt;
&lt;p&gt;
Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14029
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#38543;&#26426;&#23376;&#38598;&#30340;&#21021;&#22987;&#26435;&#37325;&#26469;&#20943;&#23569;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLT&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#38477;&#20302;&#20102;SLT&#25628;&#32034;&#31354;&#38388;&#65292;&#20445;&#35777;&#20102;SLT&#22312;&#36825;&#31181;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14029v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25688;&#35201;&#65306;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31264;&#23494;&#32593;&#32476;&#21253;&#21547;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#26435;&#37325;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#23376;&#32593;&#32476;--&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLTs&#65289;&#12290;&#26368;&#36817;&#65292;Gadhikar&#31561;&#20154;&#65288;2023&#24180;&#65289;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;SLTs&#20063;&#21487;&#20197;&#22312;&#38543;&#26426;&#20462;&#21098;&#30340;&#28304;&#32593;&#32476;&#20013;&#25214;&#21040;&#65292;&#20174;&#32780;&#20943;&#23569;SLT&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#29978;&#33267;&#27604;&#28304;&#32593;&#32476;&#26356;&#31232;&#30095;&#30340;SLTs&#30340;&#25628;&#32034;&#65292;&#23548;&#33268;&#30001;&#20110;&#24847;&#22806;&#30340;&#39640;&#31232;&#30095;&#24615;&#32780;&#20934;&#30830;&#24230;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#30340;&#20219;&#24847;&#27604;&#29575;&#20943;&#23569;SLT&#25628;&#32034;&#31354;&#38388;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20923;&#32467;&#19968;&#37096;&#20998;&#21021;&#22987;&#26435;&#37325;&#30340;&#38543;&#26426;&#23376;&#38598;&#65292;&#23558;&#20854;&#25490;&#38500;&#22312;&#25628;&#32034;&#31354;&#38388;&#20043;&#22806;--&#21363;&#65292;&#36890;&#36807;&#27704;&#20037;&#20462;&#21098;&#23427;&#20204;&#25110;&#23558;&#23427;&#20204;&#38145;&#23450;&#20026;SLT&#30340;&#22266;&#23450;&#37096;&#20998;&#12290;&#20107;&#23454;&#19978;&#65292;&#36890;&#36807;&#25105;&#20204;&#19982;&#38543;&#26426;&#20923;&#32467;&#21464;&#37327;&#30340;&#23376;&#38598;&#21644;&#36924;&#36817;&#65292;&#22312;&#36825;&#31181;&#20943;&#23569;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#65292;SLT&#30340;&#23384;&#22312;&#22312;&#29702;&#35770;&#19978;&#26159;&#24471;&#21040;&#20445;&#35777;&#30340;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36824;&#21487;&#20197;&#20943;&#23569;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14029v1 Announce Type: cross  Abstract: Randomly initialized dense networks contain subnetworks that achieve high accuracy without weight learning -- strong lottery tickets (SLTs). Recently, Gadhikar et al. (2023) demonstrated theoretically and experimentally that SLTs can also be found within a randomly pruned source network, thus reducing the SLT search space. However, this limits the search to SLTs that are even sparser than the source, leading to worse accuracy due to unintentionally high sparsity. This paper proposes a method that reduces the SLT search space by an arbitrary ratio that is independent of the desired SLT sparsity. A random subset of the initial weights is excluded from the search space by freezing it -- i.e., by either permanently pruning them or locking them as a fixed part of the SLT. Indeed, the SLT existence in such a reduced search space is theoretically guaranteed by our subset-sum approximation with randomly frozen variables. In addition to reducin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#20855;&#26377;&#22810;&#26679;&#32972;&#26223;&#22122;&#38899;&#21644;&#22833;&#30495;&#30340;&#30495;&#23454;&#29615;&#22659;&#22330;&#26223;&#27169;&#25311;&#20013;&#30340;&#24037;&#20316;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#23454;&#29616;100%&#20934;&#30830;&#24615;&#30340;5&#31186;&#38899;&#39057;&#36755;&#20837;&#65292;&#31995;&#32479;&#21305;&#37197;&#36895;&#24230;&#21487;&#39044;&#27979;&#65292;&#24378;&#35843;&#20102;&#23454;&#38469;&#23454;&#26045;&#20013;&#30340;&#20851;&#38190;&#31354;&#38388;-&#36895;&#24230;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.13957</link><description>&lt;p&gt;
&#25552;&#39640;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#20934;&#30830;&#24615;&#65292;&#35299;&#20915;&#32972;&#26223;&#22122;&#38899;&#21644;&#22833;&#30495;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Advancing Audio Fingerprinting Accuracy Addressing Background Noise and Distortion Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#20855;&#26377;&#22810;&#26679;&#32972;&#26223;&#22122;&#38899;&#21644;&#22833;&#30495;&#30340;&#30495;&#23454;&#29615;&#22659;&#22330;&#26223;&#27169;&#25311;&#20013;&#30340;&#24037;&#20316;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#23454;&#29616;100%&#20934;&#30830;&#24615;&#30340;5&#31186;&#38899;&#39057;&#36755;&#20837;&#65292;&#31995;&#32479;&#21305;&#37197;&#36895;&#24230;&#21487;&#39044;&#27979;&#65292;&#24378;&#35843;&#20102;&#23454;&#38469;&#23454;&#26045;&#20013;&#30340;&#20851;&#38190;&#31354;&#38388;-&#36895;&#24230;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#65292;&#22914; Shazam &#31561;&#20808;&#39537;&#25152;&#31034;&#65292;&#24050;&#32463;&#25913;&#21464;&#20102;&#25968;&#23383;&#38899;&#39057;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#20934;&#30830;&#24615;&#23384;&#22312;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#30340;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#31639;&#27861;&#20197;&#22686;&#24378;&#20934;&#30830;&#24615;&#12290;&#24314;&#31435;&#22312; Dejavu &#39033;&#30446;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#20855;&#26377;&#22810;&#26679;&#32972;&#26223;&#22122;&#38899;&#21644;&#22833;&#30495;&#30340;&#30495;&#23454;&#29615;&#22659;&#22330;&#26223;&#27169;&#25311;&#20013;&#30340;&#24037;&#20316;&#12290;&#20449;&#21495;&#22788;&#29702;&#65292;&#26159; Dejavu &#27169;&#22411;&#30340;&#26680;&#24515;&#65292;&#21253;&#25324;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#12289;&#39057;&#35889;&#22270;&#21644;&#23792;&#20540;&#25552;&#21462;&#12290;"&#26143;&#24231;"&#27010;&#24565;&#21644;&#25351;&#32441;&#21704;&#24076;&#20351;&#24471;&#27468;&#26354;&#29420;&#29305;&#26631;&#35782;&#25104;&#20026;&#21487;&#33021;&#12290;&#24615;&#33021;&#35780;&#20272;&#35777;&#23454;&#65292;&#22312;5&#31186;&#38899;&#39057;&#36755;&#20837;&#26102;&#21487;&#36798;&#21040;100%&#30340;&#20934;&#30830;&#24615;&#65292;&#31995;&#32479;&#23637;&#31034;&#20986;&#21487;&#39044;&#27979;&#30340;&#21305;&#37197;&#36895;&#24230;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#23384;&#20648;&#20998;&#26512;&#20984;&#26174;&#20102;&#23454;&#38469;&#23454;&#26045;&#20013;&#30340;&#20851;&#38190;&#31354;&#38388;-&#36895;&#24230;&#26435;&#34913;&#12290;&#26412;&#30740;&#31350;&#25512;&#21160;&#20102;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13957v1 Announce Type: cross  Abstract: Audio fingerprinting, exemplified by pioneers like Shazam, has transformed digital audio recognition. However, existing systems struggle with accuracy in challenging conditions, limiting broad applicability. This research proposes an AI and ML integrated audio fingerprinting algorithm to enhance accuracy. Built on the Dejavu Project's foundations, the study emphasizes real-world scenario simulations with diverse background noises and distortions. Signal processing, central to Dejavu's model, includes the Fast Fourier Transform, spectrograms, and peak extraction. The "constellation" concept and fingerprint hashing enable unique song identification. Performance evaluation attests to 100% accuracy within a 5-second audio input, with a system showcasing predictable matching speed for efficiency. Storage analysis highlights the critical space-speed trade-off for practical implementation. This research advances audio fingerprinting's adaptab
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#36845;&#20195;&#27491;&#21017;&#21270;&#26041;&#27861;&#35299;&#20915;&#20102;&#23494;&#24230;&#27604;&#20272;&#35745;&#20013;&#30340;&#39281;&#21644;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#25910;&#25947;&#65292;&#22312;&#23494;&#24230;&#27604;&#20272;&#35745;&#22522;&#20934;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#28145;&#24230;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#38598;&#25104;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.13891</link><description>&lt;p&gt;
&#20811;&#26381;&#36845;&#20195;&#27491;&#21017;&#21270;&#20013;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#39281;&#21644;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Overcoming Saturation in Density Ratio Estimation by Iterated Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13891
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#36845;&#20195;&#27491;&#21017;&#21270;&#26041;&#27861;&#35299;&#20915;&#20102;&#23494;&#24230;&#27604;&#20272;&#35745;&#20013;&#30340;&#39281;&#21644;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#25910;&#25947;&#65292;&#22312;&#23494;&#24230;&#27604;&#20272;&#35745;&#22522;&#20934;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#28145;&#24230;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#38598;&#25104;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26377;&#38480;&#26679;&#26412;&#20013;&#20272;&#35745;&#20004;&#20010;&#27010;&#29575;&#23494;&#24230;&#30340;&#27604;&#29575;&#65292;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#22823;&#31867;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#26680;&#26041;&#27861;&#23384;&#22312;&#38169;&#35823;&#39281;&#21644;&#38382;&#39064;&#65292;&#36825;&#38459;&#30861;&#20102;&#31639;&#27861;&#22312;&#39640;&#24230;&#35268;&#21017;&#23398;&#20064;&#38382;&#39064;&#19978;&#23454;&#29616;&#24555;&#36895;&#38169;&#35823;&#25910;&#25947;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#39281;&#21644;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36845;&#20195;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#23494;&#24230;&#27604;&#20272;&#35745;&#20013;&#20197;&#23454;&#29616;&#24555;&#36895;&#38169;&#35823;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23494;&#24230;&#27604;&#20272;&#35745;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#22823;&#35268;&#27169;&#35780;&#20272;&#28145;&#24230;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#38598;&#25104;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13891v1 Announce Type: new  Abstract: Estimating the ratio of two probability densities from finitely many samples, is a central task in machine learning and statistics. In this work, we show that a large class of kernel methods for density ratio estimation suffers from error saturation, which prevents algorithms from achieving fast error convergence rates on highly regular learning problems. To resolve saturation, we introduce iterated regularization in density ratio estimation to achieve fast error rates. Our methods outperform its non-iteratively regularized versions on benchmarks for density ratio estimation as well as on large-scale evaluations for importance-weighted ensembling of deep unsupervised domain adaptation models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#23398;&#20064;&#22823;&#38388;&#36317;&#21322;&#31354;&#38388;&#38382;&#39064;&#30340;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#31639;&#27861;&#65292;&#22312;&#32500;&#24230;&#26080;&#20851;&#12289;&#26102;&#38388;&#22797;&#26434;&#24230;&#20248;&#21270;&#12289;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#31561;&#22810;&#20010;&#20851;&#38190;&#21442;&#25968;&#19978;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13857</link><description>&lt;p&gt;
&#21487;&#22797;&#21046;&#23398;&#20064;&#22823;&#38388;&#36317;&#21322;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Replicable Learning of Large-Margin Halfspaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#23398;&#20064;&#22823;&#38388;&#36317;&#21322;&#31354;&#38388;&#38382;&#39064;&#30340;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#31639;&#27861;&#65292;&#22312;&#32500;&#24230;&#26080;&#20851;&#12289;&#26102;&#38388;&#22797;&#26434;&#24230;&#20248;&#21270;&#12289;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#31561;&#22810;&#20010;&#20851;&#38190;&#21442;&#25968;&#19978;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#21487;&#22797;&#21046;&#31639;&#27861;&#26469;&#35299;&#20915;&#23398;&#20064;&#22823;&#38388;&#36317;&#21322;&#31354;&#38388;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25913;&#36827;&#20102;Impagliazzo, Lei, Pitassi&#21644;Sorrell&#22312;STOC, 2022&#20013;&#25552;&#20379;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#36825;&#20010;&#20219;&#21153;&#30340;&#39318;&#20010;&#19982;&#32500;&#24230;&#26080;&#20851;&#30340;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;&#22810;&#39033;&#24335;&#65292;&#26159;&#27491;&#30830;&#30340;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#30456;&#20851;&#21442;&#25968;&#26041;&#38754;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#37117;&#20005;&#26684;&#27604;Impagliazzo&#31561;&#20154;&#22312;2022&#24180;&#23454;&#29616;&#30340;&#31639;&#27861;&#35201;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#31639;&#27861;&#22312;&#31934;&#24230;&#21442;&#25968;$\epsilon$&#26041;&#38754;&#20855;&#26377;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;SGD&#30340;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#22312;&#26576;&#20123;&#21442;&#25968;&#33539;&#22260;&#20869;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20248;&#20110;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13857v1 Announce Type: new  Abstract: We provide efficient replicable algorithms for the problem of learning large-margin halfspaces. Our results improve upon the algorithms provided by Impagliazzo, Lei, Pitassi, and Sorrell [STOC, 2022]. We design the first dimension-independent replicable algorithms for this task which runs in polynomial time, is proper, and has strictly improved sample complexity compared to the one achieved by Impagliazzo et al. [2022] with respect to all the relevant parameters. Moreover, our first algorithm has sample complexity that is optimal with respect to the accuracy parameter $\epsilon$. We also design an SGD-based replicable algorithm that, in some parameters' regimes, achieves better sample and time complexity than our first algorithm.   Departing from the requirement of polynomial time algorithms, using the DP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei, Pitassi, Sorrell, and Sivakumar [STOC, 2023], we show how to o
&lt;/p&gt;</description></item><item><title>SimPro&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20851;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#39044;&#23450;&#20041;&#20551;&#35774;&#65292;&#36890;&#36807;&#21019;&#26032;&#22320;&#25913;&#36827;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#65292;&#26126;&#30830;&#20998;&#31163;&#26465;&#20214;&#21644;&#36793;&#32536;&#31867;&#21035;&#20998;&#24067;&#30340;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.13505</link><description>&lt;p&gt;
SimPro&#65306;&#19968;&#20010;&#31616;&#21333;&#30340;&#27010;&#29575;&#26694;&#26550;&#23454;&#29616;&#36924;&#30495;&#30340;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13505
&lt;/p&gt;
&lt;p&gt;
SimPro&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20851;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#39044;&#23450;&#20041;&#20551;&#35774;&#65292;&#36890;&#36807;&#21019;&#26032;&#22320;&#25913;&#36827;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#65292;&#26126;&#30830;&#20998;&#31163;&#26465;&#20214;&#21644;&#36793;&#32536;&#31867;&#21035;&#20998;&#24067;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#38598;&#20013;&#22312;&#35299;&#20915;&#19968;&#20010;&#26356;&#20026;&#36924;&#30495;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65306;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21516;&#26102;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#31867;&#21035;&#20998;&#24067;&#26082;&#26410;&#30693;&#21448;&#21487;&#33021;&#19981;&#21305;&#37197;&#12290;&#24403;&#21069;&#36825;&#19968;&#39046;&#22495;&#30340;&#26041;&#27861;&#24448;&#24448;&#39044;&#35774;&#20102;&#20851;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#31867;&#21035;&#20998;&#24067;&#30340;&#20005;&#26684;&#20551;&#35774;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#27169;&#22411;&#20165;&#36866;&#24212;&#20110;&#26576;&#20123;&#20998;&#24067;&#33539;&#22260;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#24230;&#36866;&#24212;&#24615;&#30340;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;SimPro&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20851;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#39044;&#23450;&#20041;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24314;&#31435;&#22312;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#26126;&#30830;&#20998;&#31163;&#26465;&#20214;&#21644;&#36793;&#32536;&#31867;&#21035;&#20998;&#24067;&#30340;&#24314;&#27169;&#65292;&#21019;&#26032;&#22320;&#25913;&#36827;&#20102;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#12290;&#36825;&#31181;&#20998;&#31163;&#20419;&#36827;&#20102;&#22312;&#26368;&#22823;&#21270;&#36807;&#31243;&#20013;&#23545;&#31867;&#21035;&#20998;&#24067;&#36827;&#34892;&#20272;&#35745;&#30340;&#38381;&#21512;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13505v1 Announce Type: new  Abstract: Recent advancements in semi-supervised learning have focused on a more realistic yet challenging task: addressing imbalances in labeled data while the class distribution of unlabeled data remains both unknown and potentially mismatched. Current approaches in this sphere often presuppose rigid assumptions regarding the class distribution of unlabeled data, thereby limiting the adaptability of models to only certain distribution ranges. In this study, we propose a novel approach, introducing a highly adaptable framework, designated as SimPro, which does not rely on any predefined assumptions about the distribution of unlabeled data. Our framework, grounded in a probabilistic model, innovatively refines the expectation-maximization (EM) algorithm by explicitly decoupling the modeling of conditional and marginal class distributions. This separation facilitates a closed-form solution for class distribution estimation during the maximization p
&lt;/p&gt;</description></item><item><title>&#22312;&#19981;&#23454;&#26045;&#20844;&#24179;&#35757;&#32451;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#25277;&#26679;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#26469;&#36880;&#27493;&#36716;&#31227;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12789</link><description>&lt;p&gt;
&#26080;&#38656;&#20844;&#24179;&#35757;&#32451;&#30340;&#20844;&#24179;&#20998;&#31867;&#22120;&#65306;&#19968;&#31181;&#21463;&#24433;&#21709;&#25968;&#25454;&#25277;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12789
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#23454;&#26045;&#20844;&#24179;&#35757;&#32451;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#25277;&#26679;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#26469;&#36880;&#27493;&#36716;&#31227;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#24212;&#35813;&#30830;&#20445;&#26469;&#33258;&#19981;&#21516;&#32676;&#20307;&#30340;&#20154;&#20204;&#21463;&#30410;&#65292;&#32780;&#32676;&#20307;&#20449;&#24687;&#24448;&#24448;&#26159;&#25935;&#24863;&#30340;&#65292;&#19981;&#36866;&#21512;&#27169;&#22411;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19968;&#20010;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#20294;&#25490;&#38500;&#25935;&#24863;&#23646;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#32780;&#19981;&#23454;&#29616;&#20844;&#24179;&#35757;&#32451;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#21487;&#33021;&#27844;&#38706;&#25935;&#24863;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#22312;&#20855;&#26377;&#36866;&#24403;&#20998;&#24067;&#20559;&#31227;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20256;&#32479;&#35757;&#32451;&#21487;&#20197;&#21516;&#26102;&#20943;&#23569;&#20844;&#24179;&#24046;&#36317;&#30340;&#19978;&#38480;&#21644;&#27169;&#22411;&#27867;&#21270;&#35823;&#24046;&#65292;&#34920;&#26126;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#21487;&#20197;&#21516;&#27493;&#25552;&#39640;&#65292;&#21482;&#38656;&#31616;&#21333;&#22320;&#36827;&#34892;&#20256;&#32479;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25277;&#26679;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#36880;&#27493;&#36716;&#31227;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#35775;&#38382;&#26032;&#25968;&#25454;&#30340;&#25935;&#24863;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12789v1 Announce Type: cross  Abstract: A fair classifier should ensure the benefit of people from different groups, while the group information is often sensitive and unsuitable for model training. Therefore, learning a fair classifier but excluding sensitive attributes in the training dataset is important. In this paper, we study learning fair classifiers without implementing fair training algorithms to avoid possible leakage of sensitive information. Our theoretical analyses validate the possibility of this approach, that traditional training on a dataset with an appropriate distribution shift can reduce both the upper bound for fairness disparity and model generalization error, indicating that fairness and accuracy can be improved simultaneously with simply traditional training. We then propose a tractable solution to progressively shift the original training data during training by sampling influential data, where the sensitive attribute of new data is not accessed in s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12237</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#20869;&#23481;&#23457;&#26680;&#20013;&#25512;&#36831;&#65306;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning to Defer in Content Moderation: The Human-AI Interplay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#22312;&#32447;&#24179;&#21488;&#20869;&#23481;&#23457;&#26680;&#20381;&#36182;&#20110;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#31639;&#27861;&#35266;&#23519;&#21040;&#21363;&#23558;&#21457;&#24067;&#30340;&#24086;&#23376;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#20570;&#20986;&#20998;&#31867;&#21644;&#20934;&#20837;&#20915;&#31574;&#65292;&#24182;&#23433;&#25490;&#24086;&#23376;&#36827;&#34892;&#20154;&#24037;&#23457;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12237v1 Announce Type: cross  Abstract: Successful content moderation in online platforms relies on a human-AI collaboration approach. A typical heuristic estimates the expected harmfulness of a post and uses fixed thresholds to decide whether to remove it and whether to send it for human review. This disregards the prediction uncertainty, the time-varying element of human review capacity and post arrivals, and the selective sampling in the dataset (humans only review posts filtered by the admission algorithm).   In this paper, we introduce a model to capture the human-AI interplay in content moderation. The algorithm observes contextual information for incoming posts, makes classification and admission decisions, and schedules posts for human review. Only admitted posts receive human reviews on their harmfulness. These reviews help educate the machine-learning algorithms but are delayed due to congestion in the human review system. The classical learning-theoretic way to ca
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20809;&#35889;&#19981;&#24179;&#34913;&#30340;&#27010;&#24565;&#20316;&#20026;&#23548;&#33268;&#31867;&#21035;&#24046;&#24322;&#30340;&#28508;&#22312;&#26469;&#28304;&#65292;&#24182;&#30740;&#31350;&#20102;&#20809;&#35889;&#19981;&#24179;&#34913;&#19982;&#31867;&#21035;&#20559;&#35265;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20026;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#30340;&#31867;&#21035;&#24046;&#24322;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#22312;&#22810;&#20010;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#39564;&#35777;&#20102;&#36825;&#31181;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.11742</link><description>&lt;p&gt;
&#24179;&#34913;&#25968;&#25454;&#65292;&#19981;&#24179;&#34913;&#20809;&#35889;&#65306;&#25581;&#31034;&#20855;&#26377;&#20809;&#35889;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20809;&#35889;&#19981;&#24179;&#34913;&#30340;&#27010;&#24565;&#20316;&#20026;&#23548;&#33268;&#31867;&#21035;&#24046;&#24322;&#30340;&#28508;&#22312;&#26469;&#28304;&#65292;&#24182;&#30740;&#31350;&#20102;&#20809;&#35889;&#19981;&#24179;&#34913;&#19982;&#31867;&#21035;&#20559;&#35265;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20026;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#30340;&#31867;&#21035;&#24046;&#24322;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#22312;&#22810;&#20010;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#39564;&#35777;&#20102;&#36825;&#31181;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#27169;&#22411;&#34987;&#26399;&#26395;&#22312;&#19981;&#21516;&#31867;&#21035;&#19978;&#34920;&#29616;&#21516;&#26679;&#33391;&#22909;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#24448;&#24448;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#36825;&#20010;&#31867;&#21035;&#20559;&#35265;&#38382;&#39064;&#22312;&#26679;&#26412;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30456;&#23545;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#20013;&#30340;&#20809;&#35889;&#19981;&#24179;&#34913;&#27010;&#24565;&#20316;&#20026;&#31867;&#21035;&#24046;&#24322;&#30340;&#28508;&#22312;&#26469;&#28304;&#65292;&#24182;&#30740;&#31350;&#20809;&#35889;&#19981;&#24179;&#34913;&#19982;&#31867;&#21035;&#20559;&#35265;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#30340;&#32852;&#31995;&#12290;&#20026;&#20102;&#24314;&#31435;&#20809;&#35889;&#19981;&#24179;&#34913;&#19982;&#31867;&#21035;&#24046;&#36317;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#31867;&#21035;&#24046;&#24322;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25512;&#23548;&#20102;&#39640;&#32500;&#28151;&#21512;&#27169;&#22411;&#35774;&#23450;&#20013;&#27599;&#31867;&#38169;&#35823;&#30340;&#31934;&#30830;&#34920;&#36798;&#24335;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;11&#20010;&#19981;&#21516;&#30340;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#30740;&#31350;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22914;&#20309;&#29992;&#20110;&#27604;&#36739;&#32534;&#30721;&#22120;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#35780;&#20272;&#21644;&#32452;&#21512;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11742v1 Announce Type: new  Abstract: Classification models are expected to perform equally well for different classes, yet in practice, there are often large gaps in their performance. This issue of class bias is widely studied in cases of datasets with sample imbalance, but is relatively overlooked in balanced datasets. In this work, we introduce the concept of spectral imbalance in features as a potential source for class disparities and study the connections between spectral imbalance and class bias in both theory and practice. To build the connection between spectral imbalance and class gap, we develop a theoretical framework for studying class disparities and derive exact expressions for the per-class error in a high-dimensional mixture model setting. We then study this phenomenon in 11 different state-of-the-art pretrained encoders and show how our proposed framework can be used to compare the quality of encoders, as well as evaluate and combine data augmentation stra
&lt;/p&gt;</description></item><item><title>Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11463</link><description>&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#21560;&#24341;&#23376;&#35760;&#24518;&#65306;&#28151;&#27788;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11463
&lt;/p&gt;
&lt;p&gt;
Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24573;&#35270;&#20102;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#28304;&#33258;&#28508;&#22312;&#36830;&#32493;&#21160;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#23548;&#33268;&#32570;&#20047;&#22806;&#25512;&#21644;&#28436;&#21270;&#33021;&#21147;&#12290; &#37492;&#21035;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#28151;&#27788;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;\textbf{\textit{Attraos}}&#23558;&#28151;&#27788;&#29702;&#35770;&#34701;&#20837;&#21040;LTSF&#20013;&#65292;&#23558;&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#26410;&#30693;&#39640;&#32500;&#28151;&#27788;&#21160;&#24577;&#31995;&#32479;&#30340;&#35266;&#27979;&#12290; &#22312;&#21560;&#24341;&#23376;&#19981;&#21464;&#24615;&#30340;&#27010;&#24565;&#19979;&#65292;Attraos&#21033;&#29992;&#25552;&#20986;&#30340;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#26469;&#35760;&#24518;&#21382;&#21490;&#21160;&#24577;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#22686;&#24378;&#30340;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#36827;&#34892;&#39044;&#27979;&#12290; &#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#20016;&#23500;&#30340;&#32463;&#39564;&#35777;&#25454;&#19968;&#33268;&#34920;&#26126;&#65292;Attraos&#22312;&#20027;&#27969;LTSF&#25968;&#25454;&#38598;&#21644;&#28151;&#27788;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#21508;&#31181;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11463v1 Announce Type: cross  Abstract: In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32570;&#20047;&#37096;&#20998;&#21453;&#39304;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#31574;&#30053;&#26469;&#30830;&#20445;&#25152;&#26377;&#23376;&#32676;&#20307;&#37117;&#34987;&#25506;&#32034;&#12289;&#38450;&#27490;&#38169;&#35823;&#20998;&#31867;&#12289;&#20197;&#21450;&#25910;&#25947;&#21040;&#26399;&#26395;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.11338</link><description>&lt;p&gt;
&#20855;&#26377;&#37096;&#20998;&#21453;&#39304;&#30340;&#20844;&#24179;&#20998;&#31867;&#65306;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32570;&#20047;&#37096;&#20998;&#21453;&#39304;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#31574;&#30053;&#26469;&#30830;&#20445;&#25152;&#26377;&#23376;&#32676;&#20307;&#37117;&#34987;&#25506;&#32034;&#12289;&#38450;&#27490;&#38169;&#35823;&#20998;&#31867;&#12289;&#20197;&#21450;&#25910;&#25947;&#21040;&#26399;&#26395;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39044;&#27979;&#22330;&#26223;&#65288;&#20363;&#22914;&#20449;&#36151;&#25918;&#27454;&#65289;&#20013;&#65292;&#21482;&#26377;&#36807;&#21435;&#34987;&#31215;&#26497;&#20998;&#31867;&#30340;&#26679;&#26412;&#25165;&#20250;&#35266;&#23519;&#21040;&#30495;&#23454;&#32467;&#26524;&#12290;&#36825;&#20123;&#36807;&#21435;&#30340;&#35266;&#23519;&#32467;&#26524;&#24418;&#25104;&#20102;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#20197;&#36827;&#34892;&#26410;&#26469;&#39044;&#27979;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#32570;&#20047;&#20851;&#20110;&#36807;&#21435;&#65288;&#38169;&#35823;&#22320;&#65289;&#34987;&#36127;&#38754;&#20998;&#31867;&#30340;&#26679;&#26412;&#32467;&#26524;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20379;&#19968;&#31995;&#21015;&#25506;&#32034;&#31574;&#30053;&#26469;&#25910;&#38598;&#20851;&#20110;&#21542;&#21017;&#20250;&#34987;&#24573;&#30053;&#30340;&#23376;&#32676;&#20307;&#30340;&#32467;&#26524;&#25968;&#25454;&#12290;&#23545;&#20110;&#20219;&#20309;&#25506;&#32034;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#37117;&#20855;&#26377;&#20197;&#19979;&#20445;&#35777;&#65306;&#65288;1&#65289;&#25152;&#26377;&#23376;&#32676;&#20307;&#37117;&#24471;&#21040;&#20102;&#25506;&#32034;&#65292;&#65288;2&#65289;&#20551;&#38451;&#24615;&#30340;&#27604;&#20363;&#21463;&#21040;&#20102;&#38480;&#21046;&#65292;&#65288;3&#65289;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#25910;&#25947;&#21040;&#19968;&#20010;&#8220;&#26399;&#26395;&#8221;&#30340;&#20998;&#31867;&#22120;&#12290;&#27491;&#30830;&#30340;&#25506;&#32034;&#31574;&#30053;&#21462;&#20915;&#20110;&#19978;&#19979;&#25991;&#65307;&#23427;&#21487;&#20197;&#36873;&#25321;&#20197;&#25913;&#21892;&#23398;&#20064;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11338v1 Announce Type: cross  Abstract: In many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. These past observations, in turn, form training datasets for classifiers that make future predictions. However, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. We present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. For any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a "desired" classifier. The right exploration strategy is context-dependent; it can be chosen to improve learning guarantees 
&lt;/p&gt;</description></item><item><title>&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11078</link><description>&lt;p&gt;
&#36890;&#36807;&#32431;&#24494;&#35843;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Model Editing by Pure Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11078
&lt;/p&gt;
&lt;p&gt;
&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#35843;&#25972;&#34987;&#35748;&#20026;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#19981;&#22815;&#26377;&#25928;&#65292;&#22240;&#20026;&#30456;&#23545;&#26356;&#19987;&#19994;&#30340;&#26041;&#27861;&#32780;&#35328;&#65292;&#23427;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#26159;&#31616;&#21333;&#30340;&#65292;&#19981;&#20851;&#24515;&#34987;&#32534;&#36753;&#27169;&#22411;&#30340;&#20307;&#31995;&#32467;&#26500;&#32454;&#33410;&#65292;&#24182;&#19988;&#33021;&#22815;&#21033;&#29992;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#30340;&#19981;&#26029;&#36827;&#23637;&#65288;&#20363;&#22914;PEFT&#65289;&#65292;&#20351;&#20854;&#25104;&#20026;&#27169;&#22411;&#32534;&#36753;&#22120;&#30340;&#21560;&#24341;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32431;&#31929;&#30340;&#24494;&#35843;&#21487;&#20197;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#26420;&#32032;&#24494;&#35843;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#32780;&#38750;&#23436;&#25972;&#20284;&#28982;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#26469;&#22686;&#21152;&#25968;&#25454;&#65292;&#20197;&#40723;&#21169;&#27867;&#21270;&#21644;&#23616;&#37096;&#24615;&#12290;&#25105;&#20204;&#22312;ZsRE&#21644;CounterFact&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#19968;&#31616;&#21333;&#20462;&#25913;&#20351;&#24471;&#24494;&#35843;&#36890;&#24120;&#21487;&#20197;&#19982;&#19987;&#19994;&#32534;&#36753;&#22120;&#22312;&#32534;&#36753;&#20998;&#25968;&#26041;&#38754;&#21305;&#25932;&#29978;&#33267;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11078v1 Announce Type: cross  Abstract: Fine-tuning is dismissed as not effective for model editing due to its poor performance compared to more specialized methods. However, fine-tuning is simple, agnostic to the architectural details of the model being edited, and able to leverage ongoing advances in standard training methods (e.g., PEFT), making it an appealing choice for a model editor. In this work, we show that pure fine-tuning can be a viable approach to model editing. We propose a slight modification of naive fine-tuning with two key ingredients. First, we optimize the conditional likelihood rather than the full likelihood. Second, we augment the data with random paraphrases and facts to encourage generalization and locality. Our experiments on ZsRE and CounterFact show that this simple modification allows fine-tuning to often match or outperform specialized editors in the edit score.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807; hierarchical spatiotemporal downsampling &#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#23545;&#26102;&#31354;&#39044;&#27979;&#30340;&#26377;&#25928;&#24314;&#27169;</title><link>https://arxiv.org/abs/2402.10634</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26102;&#31354;&#38477;&#37319;&#26679;&#32570;&#22833;&#25968;&#25454;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10634
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807; hierarchical spatiotemporal downsampling &#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#23545;&#26102;&#31354;&#39044;&#27979;&#30340;&#26377;&#25928;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#32452;&#19982;&#31354;&#38388;&#20013;&#20256;&#24863;&#22120;&#28857;&#30456;&#20851;&#32852;&#12289;&#20855;&#26377;&#30456;&#20114;&#20851;&#31995;&#30340;&#21516;&#27493;&#26102;&#38388;&#24207;&#21015;&#65292;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#21253;&#25324;&#20026;&#27599;&#20010;&#28857;&#39044;&#27979;&#26410;&#26469;&#35266;&#27979;&#20540;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20026;&#22270;&#26469;&#23454;&#29616;&#24341;&#20154;&#27880;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#24120;&#24120;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#21363;&#36755;&#20837;&#22987;&#32456;&#21487;&#29992;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#26102;&#26080;&#27861;&#25429;&#25417;&#38544;&#34255;&#30340;&#26102;&#31354;&#21160;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#23618;&#26102;&#31354;&#38477;&#37319;&#26679;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#38543;&#30528;&#26102;&#38388;&#21644;&#31354;&#38388;&#30340;&#25512;&#31227;&#36880;&#28176;&#31895;&#21270;&#65292;&#33719;&#24471;&#19968;&#32452;&#25429;&#25417;&#24322;&#36136;&#26102;&#38388;&#21644;&#31354;&#38388;&#21160;&#24577;&#30340;&#34920;&#31034;&#12290;&#22312;&#35266;&#27979;&#20540;&#21644;&#32570;&#22833;&#25968;&#25454;&#27169;&#24335;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#32452;&#21512;&#36825;&#20123;&#34920;&#31034;&#20197;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10634v1 Announce Type: cross  Abstract: Given a set of synchronous time series, each associated with a sensor-point in space and characterized by inter-series relationships, the problem of spatiotemporal forecasting consists of predicting future observations for each point. Spatiotemporal graph neural networks achieve striking results by representing the relationships across time series as a graph. Nonetheless, most existing methods rely on the often unrealistic assumption that inputs are always available and fail to capture hidden spatiotemporal dynamics when part of the data is missing. In this work, we tackle this problem through hierarchical spatiotemporal downsampling. The input time series are progressively coarsened over time and space, obtaining a pool of representations that capture heterogeneous temporal and spatial dynamics. Conditioned on observations and missing data patterns, such representations are combined by an interpretable attention mechanism to generate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#21407;&#22240;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27973;&#23618;&#36731;&#37327;&#32423;&#30340;Transformer&#27169;&#22411;SAMformer&#65292;&#36890;&#36807;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#36991;&#20813;&#20102;&#38519;&#20837;&#22351;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#24182;&#22312;&#24120;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;TSMixer&#12290;</title><link>https://arxiv.org/abs/2402.10198</link><description>&lt;p&gt;
&#20351;&#29992;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#21644;&#36890;&#36947;&#27880;&#24847;&#21147;&#35299;&#38145;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#21407;&#22240;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27973;&#23618;&#36731;&#37327;&#32423;&#30340;Transformer&#27169;&#22411;SAMformer&#65292;&#36890;&#36807;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#36991;&#20813;&#20102;&#38519;&#20837;&#22351;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#24182;&#22312;&#24120;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;TSMixer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#22810;&#20803;&#38271;&#26399;&#39044;&#27979;&#26041;&#38754;&#65292;&#23427;&#20204;&#20173;&#28982;&#19981;&#22914;&#26356;&#31616;&#21333;&#30340;&#32447;&#24615;&#22522;&#32447;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19968;&#20010;&#29609;&#20855;&#32447;&#24615;&#39044;&#27979;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#23613;&#31649;Transformer&#20855;&#26377;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#25910;&#25947;&#21040;&#30495;&#27491;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;Transformer&#30340;&#27880;&#24847;&#21147;&#26159;&#36896;&#25104;&#20854;&#20302;&#27867;&#21270;&#33021;&#21147;&#30340;&#21407;&#22240;&#12290;&#22522;&#20110;&#36825;&#19968;&#35748;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27973;&#23618;&#36731;&#37327;&#32423;&#30340;Transformer&#27169;&#22411;&#65292;&#22312;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#36991;&#20813;&#20102;&#22351;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20010;&#32467;&#26524;&#36866;&#29992;&#20110;&#25152;&#26377;&#24120;&#29992;&#30340;&#23454;&#38469;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;&#29305;&#21035;&#26159;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;TSMixer&#65292;SAMformer&#30340;&#24179;&#22343;&#24615;&#33021;&#25552;&#39640;&#20102;14.33%&#65292;&#24182;&#19988;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;&#32422;4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10198v1 Announce Type: new  Abstract: Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses the current state-of-the-art model TSMixer by 14.33% on average, while having ~4 times few
&lt;/p&gt;</description></item><item><title>MIM-Refiner&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;MIM&#27169;&#22411;&#20013;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#22810;&#20010;&#23545;&#27604;&#22836;&#65292;&#33021;&#22815;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10093</link><description>&lt;p&gt;
MIM-Refiner&#65306;&#19968;&#31181;&#20174;&#20013;&#38388;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#33719;&#24471;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10093
&lt;/p&gt;
&lt;p&gt;
MIM-Refiner&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;MIM&#27169;&#22411;&#20013;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#22810;&#20010;&#23545;&#27604;&#22836;&#65292;&#33021;&#22815;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MIM-Refiner&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#35757;&#32451;MIM&#27169;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#12290;MIM-Refiner&#30340;&#21160;&#26426;&#22312;&#20110;MIM&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#34920;&#31034;&#36890;&#24120;&#20301;&#20110;&#20013;&#38388;&#23618;&#12290;&#22240;&#27492;&#65292;MIM-Refiner&#21033;&#29992;&#36830;&#25509;&#21040;&#19981;&#21516;&#20013;&#38388;&#23618;&#30340;&#22810;&#20010;&#23545;&#27604;&#22836;&#12290;&#22312;&#27599;&#20010;&#22836;&#20013;&#65292;&#20462;&#25913;&#21518;&#30340;&#26368;&#36817;&#37051;&#30446;&#26631;&#24110;&#21161;&#26500;&#24314;&#30456;&#24212;&#30340;&#35821;&#20041;&#32858;&#31867;&#12290;&#27492;&#36807;&#31243;&#30701;&#32780;&#26377;&#25928;&#65292;&#22312;&#20960;&#20010;epochs&#20869;&#65292;&#25105;&#20204;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#12290;&#20351;&#29992;data2vec 2.0&#22312;ImageNet-1K&#19978;&#39044;&#35757;&#32451;&#30340;ViT-H&#32463;&#36807;&#25913;&#36827;&#21518;&#65292;&#22312;&#32447;&#24615;&#25506;&#27979;&#21644;&#20302;&#26679;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65288;&#20998;&#21035;&#20026;84.7%&#21644;64.2%&#65289;&#65292;&#36229;&#36807;&#20102;&#22312;ImageNet-1K&#19978;&#39044;&#35757;&#32451;&#30340;&#20854;&#20182;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10093v1 Announce Type: cross  Abstract: We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#23545;&#25968;&#24179;&#28369;&#65292;&#25506;&#35752;&#20102;ECE&#30340;&#32570;&#38519;&#20197;&#21450;&#23545;&#29616;&#26377;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#12289;&#26131;&#20110;&#20272;&#35745;&#30340;&#35823;&#24046;&#27979;&#24230;LS-ECE&#12290;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;LS-ECE&#19982;&#20998;&#31665;ECE&#38750;&#24120;&#25509;&#36817;&#12290;</title><link>https://arxiv.org/abs/2402.10046</link><description>&lt;p&gt;
ECE&#26377;&#22810;&#22823;&#30340;&#32570;&#38519;&#65311;&#36890;&#36807;&#23545;&#25968;&#24179;&#28369;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
How Flawed is ECE? An Analysis via Logit Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#23545;&#25968;&#24179;&#28369;&#65292;&#25506;&#35752;&#20102;ECE&#30340;&#32570;&#38519;&#20197;&#21450;&#23545;&#29616;&#26377;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#12289;&#26131;&#20110;&#20272;&#35745;&#30340;&#35823;&#24046;&#27979;&#24230;LS-ECE&#12290;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;LS-ECE&#19982;&#20998;&#31665;ECE&#38750;&#24120;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#32780;&#35328;&#20043;&#65292;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#19982;&#32622;&#20449;&#24230;&#21305;&#37197;&#65292;&#37027;&#20040;&#36825;&#20010;&#27169;&#22411;&#23601;&#26159;&#26657;&#20934;&#30340;&#12290;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#65292;&#34913;&#37327;&#26657;&#20934;&#24615;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25351;&#20986;&#20102;ECE&#30340;&#32570;&#28857;&#65292;&#20363;&#22914;&#23427;&#22312;&#39044;&#27979;&#32773;&#31354;&#38388;&#20013;&#26159;&#19981;&#36830;&#32493;&#30340;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#38382;&#39064;&#26377;&#22810;&#26412;&#36136;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#23545;&#29616;&#26377;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23436;&#20840;&#25551;&#36848;&#20102;ECE&#23545;&#27874;&#20848;&#31354;&#38388;&#19978;&#30340;&#19968;&#33324;&#27010;&#29575;&#27979;&#24230;&#30340;&#19981;&#36830;&#32493;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#19981;&#36830;&#32493;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#12289;&#26131;&#20110;&#20272;&#35745;&#30340;&#35823;&#24046;&#27979;&#24230;&#65292;&#31216;&#20026;Logit-Smoothed ECE&#65288;LS-ECE&#65289;&#12290;&#36890;&#36807;&#27604;&#36739;&#39044;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;ECE&#21644;LS-ECE&#65292;&#25105;&#20204;&#22312;&#21021;&#27493;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#20998;&#31665;ECE&#19982;LS-ECE&#38750;&#24120;&#25509;&#36817;&#65292;&#34920;&#26126;&#29702;&#35770;&#26041;&#38754;&#26159;&#30456;&#31526;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10046v1 Announce Type: new  Abstract: Informally, a model is calibrated if its predictions are correct with a probability that matches the confidence of the prediction. By far the most common method in the literature for measuring calibration is the expected calibration error (ECE). Recent work, however, has pointed out drawbacks of ECE, such as the fact that it is discontinuous in the space of predictors. In this work, we ask: how fundamental are these issues, and what are their impacts on existing results? Towards this end, we completely characterize the discontinuities of ECE with respect to general probability measures on Polish spaces. We then use the nature of these discontinuities to motivate a novel continuous, easily estimated miscalibration metric, which we term Logit-Smoothed ECE (LS-ECE). By comparing the ECE and LS-ECE of pre-trained image classification models, we show in initial experiments that binned ECE closely tracks LS-ECE, indicating that the theoretical
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.09631</link><description>&lt;p&gt;
MiMiC&#65306;&#34920;&#31034;&#31354;&#38388;&#20013;&#26368;&#23567;&#20462;&#25913;&#30340;&#23545;&#25239;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
MiMiC: Minimally Modified Counterfactuals in the Representation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#23398;&#31185; &#31616;&#20171;&#65306;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#34920;&#29616;&#20986;&#19981;&#33391;&#34892;&#20026;&#65292;&#22914;&#24615;&#21035;&#20559;&#35265;&#25110;&#26377;&#27602;&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;&#34920;&#31034;&#31354;&#38388;&#36827;&#34892;&#24178;&#39044;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#20004;&#31181;&#24120;&#35265;&#30340;&#24178;&#39044;&#25216;&#26415;&#65292;&#21363;&#32447;&#24615;&#25830;&#38500;&#21644;&#23450;&#21521;&#21521;&#37327;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#39640;&#24230;&#21487;&#25511;&#21644;&#34920;&#36798;&#20016;&#23500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24178;&#39044;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20351;&#28304;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#19982;&#30446;&#26631;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#38750;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#30456;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#39640;&#26031;&#20551;&#35774;&#19979;&#30340;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 Announce Type: cross  Abstract: Language models often exhibit undesirable behaviors, such as gender bias or toxic language. Interventions in the representation space were shown effective in mitigating such issues by altering the LM behavior. We first show that two prominent intervention techniques, Linear Erasure and Steering Vectors, do not enable a high degree of control and are limited in expressivity.   We then propose a novel intervention methodology for generating expressive counterfactuals in the representation space, aiming to make representations of a source class (e.g., ``toxic'') resemble those of a target class (e.g., ``non-toxic''). This approach, generalizing previous linear intervention techniques, utilizes a closed-form solution for the Earth Mover's problem under Gaussian assumptions and provides theoretical guarantees on the representation space's geometric organization. We further build on this technique and derive a nonlinear intervention that ena
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#30740;&#31350;&#35777;&#26126;&#29983;&#29289;&#38459;&#25239;&#20256;&#24863;&#25216;&#26415;&#21487;&#20197;&#25913;&#36827;&#22522;&#20110;IMU&#30340;&#20581;&#36523;&#36861;&#36394;&#65292;&#25552;&#39640;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09445</link><description>&lt;p&gt;
iMove: &#25506;&#32034;&#29992;&#20110;&#20581;&#36523;&#27963;&#21160;&#35782;&#21035;&#30340;&#29983;&#29289;&#38459;&#25239;&#20256;&#24863;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
iMove: Exploring Bio-impedance Sensing for Fitness Activity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09445
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#30740;&#31350;&#35777;&#26126;&#29983;&#29289;&#38459;&#25239;&#20256;&#24863;&#25216;&#26415;&#21487;&#20197;&#25913;&#36827;&#22522;&#20110;IMU&#30340;&#20581;&#36523;&#36861;&#36394;&#65292;&#25552;&#39640;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21644;&#31934;&#30830;&#30340;&#20581;&#36523;&#27963;&#21160;&#35782;&#21035;&#23545;&#20110;&#20419;&#36827;&#20581;&#24247;&#29983;&#27963;&#26041;&#24335;&#21644;&#20010;&#24615;&#21270;&#39044;&#38450;&#24615;&#21307;&#30103;&#20855;&#26377;&#30410;&#22788;&#12290;&#34429;&#28982;IMU&#30446;&#21069;&#26159;&#20027;&#35201;&#30340;&#20581;&#36523;&#36861;&#36394;&#27169;&#24335;&#65292;&#20294;&#36890;&#36807;iMove&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#29289;&#38459;&#25239;&#21487;&#20197;&#36890;&#36807;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#25913;&#21892;&#22522;&#20110;IMU&#30340;&#20581;&#36523;&#36861;&#36394;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#65292;&#21253;&#25324;&#21313;&#20010;&#21463;&#35797;&#32773;&#22312;&#20116;&#22825;&#20869;&#36827;&#34892;&#30340;&#20845;&#31181;&#19978;&#36523;&#20581;&#36523;&#27963;&#21160;&#65292;&#20197;&#25910;&#38598;&#26469;&#33258;&#20004;&#21482;&#25163;&#33109;&#30340;&#29983;&#29289;&#38459;&#25239;&#21644;&#24038;&#25163;&#33109;IMU&#30340;&#21516;&#27493;&#25968;&#25454;&#12290;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#21033;&#29992;&#20004;&#31181;&#27169;&#24577;&#26469;&#35757;&#32451;&#26356;&#22909;&#30340;&#20165;&#22522;&#20110;IMU&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20854;&#20013;&#29983;&#29289;&#38459;&#25239;&#21482;&#22312;&#35757;&#32451;&#38454;&#27573;&#38656;&#35201;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#36755;&#20837;&#21333;&#20010;IMU&#30340;&#24179;&#22343;&#23439;F1&#20998;&#25968;&#25552;&#39640;&#20102;3.22&#65285;&#65292;&#36798;&#21040;84.71&#65285;&#65292;&#32780;IMU&#22522;&#32447;&#27169;&#22411;&#20026;81.49&#65285;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#29983;&#29289;&#38459;&#25239;&#22914;&#20309;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09445v1 Announce Type: cross  Abstract: Automatic and precise fitness activity recognition can be beneficial in aspects from promoting a healthy lifestyle to personalized preventative healthcare. While IMUs are currently the prominent fitness tracking modality, through iMove, we show bio-impedence can help improve IMU-based fitness tracking through sensor fusion and contrastive learning.To evaluate our methods, we conducted an experiment including six upper body fitness activities performed by ten subjects over five days to collect synchronized data from bio-impedance across two wrists and IMU on the left wrist.The contrastive learning framework uses the two modalities to train a better IMU-only classification model, where bio-impedance is only required at the training phase, by which the average Macro F1 score with the input of a single IMU was improved by 3.22 \% reaching 84.71 \% compared to the 81.49 \% of the IMU baseline model. We have also shown how bio-impedance can 
&lt;/p&gt;</description></item><item><title>SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09025</link><description>&lt;p&gt;
SLEB: &#36890;&#36807;&#20887;&#20313;&#39564;&#35777;&#21644;&#28040;&#38500;Transformer&#22359;&#20248;&#21270;LLM&#30340;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09025
&lt;/p&gt;
&lt;p&gt;
SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#32473;&#23454;&#38469;&#37096;&#32626;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#31934;&#31616;&#65292;&#19968;&#31181;&#26088;&#22312;&#20943;&#23567;LLM&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20174;&#32593;&#32476;&#20013;&#21024;&#38500;&#20887;&#20313;&#32452;&#20214;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#31934;&#31616;&#26377;&#24076;&#26395;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31471;&#21040;&#31471;LLM&#25512;&#29702;&#21152;&#36895;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SLEB&#65292;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36873;&#25321;Transformer&#22359;&#20316;&#20026;&#31934;&#31616;&#30340;&#22522;&#26412;&#21333;&#20301;&#65292;&#22240;&#20026;LLM&#22312;&#30456;&#37051;&#22359;&#30340;&#36755;&#20986;&#20043;&#38388;&#20855;&#26377;&#22359;&#32423;&#21035;&#30340;&#20887;&#20313;&#21644;&#39640;&#30456;&#20284;&#24615;&#12290;&#36825;&#20010;&#36873;&#25321;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#22686;&#24378;LLM&#30340;&#22788;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SLEB&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09025v1 Announce Type: new Abstract: Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB successfully accelerates LLM inference without
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#38454;&#27573;&#25200;&#21160;&#27979;&#35797;&#26469;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#25200;&#21160;&#19968;&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#21464;&#21270;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#20316;&#29992;&#30340;&#27010;&#29575;&#26469;&#34913;&#37327;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22686;&#24378;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#22312;&#21306;&#20998;&#19981;&#21516;&#29305;&#24449;&#36129;&#29486;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08845</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#38454;&#27573;&#25200;&#21160;&#27979;&#35797;&#36890;&#36807;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#65292;&#20197;&#36827;&#34892;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08845
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#38454;&#27573;&#25200;&#21160;&#27979;&#35797;&#26469;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#25200;&#21160;&#19968;&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#21464;&#21270;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#20316;&#29992;&#30340;&#27010;&#29575;&#26469;&#34913;&#37327;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22686;&#24378;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#22312;&#21306;&#20998;&#19981;&#21516;&#29305;&#24449;&#36129;&#29486;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;FAMs&#65289;&#36890;&#36807;&#25200;&#21160;&#27979;&#35797;&#26469;&#27979;&#37327;&#27599;&#20010;&#29305;&#24449;&#30340;&#36129;&#29486;&#65292;&#20854;&#20013;&#22312;&#19981;&#21516;&#25200;&#21160;&#19979;&#30340;&#39044;&#27979;&#24046;&#24322;&#36827;&#34892;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#24449;&#30340;&#39044;&#27979;&#21464;&#21270;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#25200;&#21160;&#27979;&#35797;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21306;&#20998;&#19981;&#21516;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;&#20026;&#20102;&#22686;&#24378;FAMs&#22312;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#19981;&#21516;&#29305;&#24449;&#36129;&#29486;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25200;&#21160;&#19968;&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#21464;&#21270;&#36215;&#21040;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#20316;&#29992;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#20316;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#65288;FANS&#65289;&#65292;&#36890;&#36807;&#28041;&#21450;&#20004;&#20010;&#38454;&#27573;&#65288;&#20107;&#23454;&#24615;&#21644;&#24178;&#39044;&#24615;&#65289;&#30340;&#25200;&#21160;&#27979;&#35797;&#35745;&#31639;PNS&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20026;&#20102;&#29983;&#25104;&#21453;&#20107;&#23454;&#26679;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#37325;&#26032;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08845v1 Announce Type: new Abstract: We investigate the problem of explainability in machine learning.To address this problem, Feature Attribution Methods (FAMs) measure the contribution of each feature through a perturbation test, where the difference in prediction is compared under different perturbations.However, such perturbation tests may not accurately distinguish the contributions of different features, when their change in prediction is the same after perturbation.In order to enhance the ability of FAMs to distinguish different features' contributions in this challenging setting, we propose to utilize the probability (PNS) that perturbing a feature is a necessary and sufficient cause for the prediction to change as a measure of feature importance.Our approach, Feature Attribution with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test involving two stages (factual and interventional).In practice, to generate counterfactual samples, we use a re
&lt;/p&gt;</description></item><item><title>BECoTTA&#26159;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#30340;&#39640;&#25928;CTTA&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;MoDE&#65288;Mixture-of-Domain Low-rank Experts&#65289;&#27169;&#22411;&#65292;&#23427;&#21253;&#21547;&#39046;&#22495;&#33258;&#36866;&#24212;&#36335;&#30001;&#21644;&#39046;&#22495;&#19987;&#23478;&#21327;&#21516;&#25439;&#22833;&#20004;&#20010;&#26680;&#24515;&#32452;&#20214;&#65292;&#33021;&#22815;&#22312;&#25345;&#32493;&#30340;&#27979;&#35797;&#26102;&#38388;&#20013;&#33258;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#65292;&#21516;&#26102;&#21482;&#38656;&#36739;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.08712</link><description>&lt;p&gt;
BECoTTA: &#22522;&#20110;&#36755;&#20837;&#30340;&#22312;&#32447;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#25345;&#32493;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08712
&lt;/p&gt;
&lt;p&gt;
BECoTTA&#26159;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#30340;&#39640;&#25928;CTTA&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;MoDE&#65288;Mixture-of-Domain Low-rank Experts&#65289;&#27169;&#22411;&#65292;&#23427;&#21253;&#21547;&#39046;&#22495;&#33258;&#36866;&#24212;&#36335;&#30001;&#21644;&#39046;&#22495;&#19987;&#23478;&#21327;&#21516;&#25439;&#22833;&#20004;&#20010;&#26680;&#24515;&#32452;&#20214;&#65292;&#33021;&#22815;&#22312;&#25345;&#32493;&#30340;&#27979;&#35797;&#26102;&#38388;&#20013;&#33258;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#65292;&#21516;&#26102;&#21482;&#38656;&#36739;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;CTTA&#65289;&#35201;&#27714;&#22312;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#26410;&#30693;&#39046;&#22495;&#30340;&#21516;&#26102;&#20445;&#30041;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;CTTA&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#24536;&#35760;&#36866;&#24212;&#26435;&#34913;&#21644;&#25928;&#29575;&#20173;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;CTTA&#22330;&#26223;&#20165;&#20551;&#35774;&#23384;&#22312;&#19981;&#30456;&#20132;&#30340;&#24773;&#20917;&#65292;&#32780;&#23454;&#38469;&#19990;&#30028;&#30340;&#39046;&#22495;&#26159;&#26080;&#32541;&#21464;&#21270;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BECoTTA&#30340;&#22522;&#20110;&#36755;&#20837;&#30340;&#39640;&#25928;CTTA&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MoDE&#65288;Mixture-of-Domain Low-rank Experts&#65289;&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#26680;&#24515;&#32452;&#20214;&#65306;i&#65289;&#39046;&#22495;&#33258;&#36866;&#24212;&#36335;&#30001;&#65292;&#36890;&#36807;&#22810;&#20010;&#39046;&#22495;&#36335;&#30001;&#22120;&#26377;&#36873;&#25321;&#22320;&#25429;&#25417;&#39046;&#22495;&#33258;&#36866;&#24212;&#30693;&#35782;&#65292;&#21644;ii&#65289;&#39046;&#22495;&#19987;&#23478;&#21327;&#21516;&#25439;&#22833;&#65292;&#20197;&#22686;&#21152;&#27599;&#20010;&#39046;&#22495;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22810;&#20010;CTTA&#22330;&#26223;&#65292;&#21253;&#25324;&#19981;&#30456;&#20132;&#21644;&#28176;&#21464;&#39046;&#22495;&#20999;&#25442;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;&#22823;&#32422;98&#65285;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08712v1 Announce Type: new Abstract: Continual Test Time Adaptation (CTTA) is required to adapt efficiently to continuous unseen domains while retaining previously learned knowledge. However, despite the progress of CTTA, forgetting-adaptation trade-offs and efficiency are still unexplored. Moreover, current CTTA scenarios assume only the disjoint situation, even though real-world domains are seamlessly changed. To tackle these challenges, this paper proposes BECoTTA, an input-dependent yet efficient framework for CTTA. We propose Mixture-of-Domain Low-rank Experts (MoDE) that contains two core components: i) Domain-Adaptive Routing, which aids in selectively capturing the domain-adaptive knowledge with multiple domain routers, and (ii) Domain-Expert Synergy Loss to maximize the dependency between each domain and expert. We validate our method outperforms multiple CTTA scenarios including disjoint and gradual domain shits, while only requiring ~98% fewer trainable parameters
&lt;/p&gt;</description></item><item><title>Agent Smith&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#20256;&#26579;&#24615;&#36234;&#29425;&#65292;&#35813;&#38382;&#39064;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#36234;&#29425;&#19968;&#20010;&#20195;&#29702;&#26469;&#36805;&#36895;&#24863;&#26579;&#25152;&#26377;&#20195;&#29702;&#24182;&#23548;&#33268;&#26377;&#23475;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.08567</link><description>&lt;p&gt;
Agent Smith:&#19968;&#24352;&#22270;&#20687;&#21487;&#20197;&#36805;&#36895;&#36234;&#29425;&#19968;&#30334;&#19975;&#20010;&#22810;&#27169;&#24577;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08567
&lt;/p&gt;
&lt;p&gt;
Agent Smith&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#20256;&#26579;&#24615;&#36234;&#29425;&#65292;&#35813;&#38382;&#39064;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#36234;&#29425;&#19968;&#20010;&#20195;&#29702;&#26469;&#36805;&#36895;&#24863;&#26579;&#25152;&#26377;&#20195;&#29702;&#24182;&#23548;&#33268;&#26377;&#23475;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#20195;&#29702;&#21487;&#20197;&#25509;&#25910;&#25351;&#20196;&#65292;&#25429;&#25417;&#22270;&#20687;&#65292;&#20174;&#20869;&#23384;&#20013;&#26816;&#32034;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#20915;&#23450;&#20351;&#29992;&#21738;&#20123;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#32418;&#38431;&#35780;&#20272;&#21457;&#29616;&#24694;&#24847;&#22270;&#20687;/&#25552;&#31034;&#21487;&#20197;&#36234;&#29425;MLLM&#24182;&#23548;&#33268;&#19981;&#23545;&#40784;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#26356;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#31216;&#20026;&#20256;&#26579;&#24615;&#36234;&#29425;&#12290;&#23427;&#28041;&#21450;&#21040;&#23545;&#21333;&#20010;&#20195;&#29702;&#36827;&#34892;&#31616;&#21333;&#30340;&#36234;&#29425;&#65292;&#26080;&#38656;&#26469;&#33258;&#23545;&#25163;&#30340;&#36827;&#19968;&#27493;&#24178;&#39044;&#65292;&#65288;&#20960;&#20046;&#65289;&#25152;&#26377;&#20195;&#29702;&#23558;&#20197;&#25351;&#25968;&#32423;&#21035;&#34987;&#24863;&#26579;&#24182;&#23637;&#31034;&#26377;&#23475;&#34892;&#20026;&#12290;&#20026;&#20102;&#39564;&#35777;&#20256;&#26579;&#24615;&#36234;&#29425;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#21253;&#21547;&#39640;&#36798;&#19968;&#30334;&#19975;&#20010;LLaVA-1.5&#20195;&#29702;&#30340;&#22810;&#20195;&#29702;&#29615;&#22659;&#65292;&#24182;&#23558;&#38543;&#26426;&#21305;&#37197;&#23545;&#32842;&#22825;&#20316;&#20026;&#22810;&#20195;&#29702;&#20132;&#20114;&#30340;&#27010;&#24565;&#39564;&#35777;&#23454;&#20363;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#65288;&#20256;&#26579;&#24615;&#65289;&#24694;&#24847;&#22270;&#20687;&#36755;&#20837;&#21040;&#20219;&#24847;&#36873;&#25321;&#30340;&#20195;&#29702;&#30340;&#20869;&#23384;&#20013;&#23601;&#36275;&#20197;&#23454;&#29616;&#20256;&#26579;&#24615;&#36234;&#29425;&#12290;
&lt;/p&gt;
&lt;p&gt;
A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08277</link><description>&lt;p&gt;
&#26397;&#30528;&#24544;&#23454;&#21644;&#24378;&#22823;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#19987;&#23478;&#30340;&#26041;&#21521;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#24544;&#23454;&#21644;&#21487;&#36861;&#36394;&#30340;&#31572;&#26696;&#30340;&#36827;&#27493;&#23545;&#20110;&#21508;&#31181;&#30740;&#31350;&#21644;&#23454;&#36341;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#31181;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#21487;&#38752;&#30340;&#26469;&#28304;&#25552;&#20379;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#22312;&#20351;&#29992;LLM&#26102;&#24050;&#32463;&#35777;&#26126;&#22312;&#24341;&#29992;&#27491;&#30830;&#30340;&#26469;&#28304;&#65288;&#26469;&#28304;&#36136;&#37327;&#65289;&#21644;&#20934;&#30830;&#22320;&#34920;&#31034;&#26469;&#28304;&#20013;&#30340;&#20449;&#24687;&#65288;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65289;&#26041;&#38754;&#24037;&#20316;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;LLM&#65292;&#20197;&#25552;&#39640;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#21160;&#25968;&#25454;&#36136;&#37327;&#36807;&#28388;&#22120;&#65292;&#21487;&#20197;&#22823;&#35268;&#27169;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#23545;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;%&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#26696;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#20110;&#35780;&#20272;&#30340;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#35780;&#20272;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we sho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Lumos&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20855;&#22791;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#36816;&#29992;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#32452;&#20214;&#65292;&#33021;&#22815;&#20174;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#21152;&#24378;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#30740;&#31350;&#36807;&#31243;&#20013;&#65292;&#20316;&#32773;&#20811;&#26381;&#20102;&#19982;&#25991;&#26412;&#35782;&#21035;&#36136;&#37327;&#12289;&#24310;&#36831;&#21644;&#27169;&#22411;&#25512;&#26029;&#30456;&#20851;&#30340;&#22810;&#20010;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#32452;&#20214;&#35780;&#20272;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#29575;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08017</link><description>&lt;p&gt;
Lumos : &#29992;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#22686;&#24378;&#22810;&#27169;&#24335;LLMs&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Lumos : Empowering Multimodal LLMs with Scene Text Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Lumos&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20855;&#22791;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#36816;&#29992;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#32452;&#20214;&#65292;&#33021;&#22815;&#20174;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#21152;&#24378;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#30740;&#31350;&#36807;&#31243;&#20013;&#65292;&#20316;&#32773;&#20811;&#26381;&#20102;&#19982;&#25991;&#26412;&#35782;&#21035;&#36136;&#37327;&#12289;&#24310;&#36831;&#21644;&#27169;&#22411;&#25512;&#26029;&#30456;&#20851;&#30340;&#22810;&#20010;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#32452;&#20214;&#35780;&#20272;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#29575;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Lumos&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20855;&#22791;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#30340;&#31471;&#21040;&#31471;&#22810;&#27169;&#24335;&#38382;&#31572;&#31995;&#32479;&#12290;Lumos&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#65288;STR&#65289;&#32452;&#20214;&#65292;&#29992;&#20110;&#20174;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#22686;&#24378;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLM&#65289;&#30340;&#36755;&#20837;&#12290;&#22312;&#26500;&#24314;Lumos&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#35768;&#22810;&#19982;STR&#36136;&#37327;&#12289;&#25972;&#20307;&#24310;&#36831;&#21644;&#27169;&#22411;&#25512;&#26029;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#29992;&#20110;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#30340;&#31995;&#32479;&#26550;&#26500;&#12289;&#35774;&#35745;&#36873;&#25321;&#21644;&#24314;&#27169;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#23545;&#27599;&#20010;&#32452;&#20214;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#29575;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the core of Lumos is a Scene Text Recognition (STR) component that extracts text from first person point-of-view images, the output of which is used to augment input to a Multimodal Large Language Model (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31574;&#30053;&#26799;&#24230;&#22312;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#25511;&#21046;&#20013;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;&#38382;&#39064;&#65292;&#21457;&#29616;&#22806;&#25512;&#31243;&#24230;&#21462;&#20915;&#20110;&#35757;&#32451;&#20013;&#31995;&#32479;&#30340;&#25506;&#32034;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.07875</link><description>&lt;p&gt;
&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#20013;&#31574;&#30053;&#26799;&#24230;&#30340;&#38544;&#24615;&#20559;&#24046;&#65306;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31574;&#30053;&#26799;&#24230;&#22312;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#25511;&#21046;&#20013;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;&#38382;&#39064;&#65292;&#21457;&#29616;&#22806;&#25512;&#31243;&#24230;&#21462;&#20915;&#20110;&#35757;&#32451;&#20013;&#31995;&#32479;&#30340;&#25506;&#32034;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#21487;&#20197;&#20197;&#22810;&#31181;&#26041;&#24335;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#65292;&#20854;&#20013;&#19968;&#20123;&#22312;&#26410;&#35265;&#65288;&#27979;&#35797;&#65289;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#19981;&#28982;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;&#32463;&#24120;&#23637;&#29616;&#20986;&#19968;&#31181;&#38544;&#24615;&#20559;&#24046;&#65292;&#23548;&#33268;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#31181;&#38544;&#24615;&#20559;&#24046;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#26368;&#20248;&#25511;&#21046;&#65288;&#24378;&#21270;&#23398;&#20064;&#65289;&#20013;&#21364;&#20102;&#35299;&#24471;&#36739;&#23569;&#12290;&#22312;&#37027;&#37324;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#24212;&#29992;&#20110;&#31995;&#32479;&#30340;&#25511;&#21046;&#22120;&#34987;&#31216;&#20026;&#31574;&#30053;&#26799;&#24230;&#65292;&#24182;&#19988;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#22312;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;&#31243;&#24230;&#12290;&#26412;&#25991;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#31574;&#30053;&#26799;&#24230;&#22312;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;&#26041;&#38754;&#30340;&#38544;&#24615;&#20559;&#24046;&#12290;&#25105;&#20204;&#20197;&#22522;&#26412;&#30340;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#20026;&#37325;&#28857;&#65292;&#30830;&#31435;&#20102;&#22806;&#25512;&#31243;&#24230;&#21462;&#20915;&#20110;&#35757;&#32451;&#20013;&#31995;&#32479;&#22312;&#21021;&#22987;&#29366;&#24577;&#19979;&#24341;&#36215;&#30340;&#25506;&#32034;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not. Remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data. This implicit bias was extensively studied in supervised learning, but is far less understood in optimal control (reinforcement learning). There, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states. This paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states. Focusing on the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training. Exper
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#25968;&#38459;&#23612;&#24211;&#20177;&#26041;&#31243;&#35777;&#26126;&#20102;&#37325;&#23614;SDE&#30340;&#39640;&#27010;&#29575;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#21442;&#25968;&#32500;&#24230;&#65292;&#30028;&#38480;&#30340;&#20381;&#36182;&#24615;&#35201;&#22909;&#20110;p&#12290;</title><link>https://arxiv.org/abs/2402.07723</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#25968;&#38459;&#23612;&#24211;&#20177;&#26041;&#31243;&#35777;&#26126;&#37325;&#23614;SDEs&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Generalization Bounds for Heavy-Tailed SDEs through the Fractional Fokker-Planck Equation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#25968;&#38459;&#23612;&#24211;&#20177;&#26041;&#31243;&#35777;&#26126;&#20102;&#37325;&#23614;SDE&#30340;&#39640;&#27010;&#29575;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#21442;&#25968;&#32500;&#24230;&#65292;&#30028;&#38480;&#30340;&#20381;&#36182;&#24615;&#35201;&#22909;&#20110;p&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#26469;&#65292;&#29702;&#35299;&#37325;&#23614;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#21033;&#29992;&#37325;&#23614;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20316;&#20026;&#20195;&#29702;&#26469;&#38416;&#26126;&#38543;&#26426;&#20248;&#21270;&#22120;&#30340;&#26377;&#36259;&#26041;&#38754;&#26102;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#25552;&#20379;&#39044;&#26399;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#35201;&#20040;&#24341;&#20837;&#20102;&#19981;&#21487;&#35745;&#31639;&#30340;&#20449;&#24687;&#35770;&#26415;&#35821;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37325;&#23614;SDE&#30340;&#39640;&#27010;&#29575;&#27867;&#21270;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#19981;&#21547;&#20219;&#20309;&#38750;&#24179;&#20961;&#30340;&#20449;&#24687;&#35770;&#26415;&#35821;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#22522;&#20110;&#20272;&#35745;&#19982;&#25152;&#35859;&#30340;&#20998;&#25968;&#38459;&#23612;&#24211;&#20177;&#26041;&#31243;&#30456;&#20851;&#32852;&#30340;&#29109;&#27969;&#65292;&#24320;&#21457;&#20102;&#26032;&#30340;&#35777;&#26126;&#25216;&#26415;&#65288;&#36825;&#26159;&#19968;&#31181;&#25511;&#21046;&#30456;&#24212;&#37325;&#23614;SDE&#20998;&#24067;&#28436;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65289;&#12290;&#38500;&#20102;&#33719;&#24471;&#39640;&#27010;&#29575;&#30028;&#38480;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#30456;&#23545;&#20110;&#21442;&#25968;&#32500;&#24230;&#30340;&#20381;&#36182;&#24615;&#35201;&#22909;&#20110;p&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the generalization properties of heavy-tailed stochastic optimization algorithms has attracted increasing attention over the past years. While illuminating interesting aspects of stochastic optimizers by using heavy-tailed stochastic differential equations as proxies, prior works either provided expected generalization bounds, or introduced non-computable information theoretic terms. Addressing these drawbacks, in this work, we prove high-probability generalization bounds for heavy-tailed SDEs which do not contain any nontrivial information theoretic terms. To achieve this goal, we develop new proof techniques based on estimating the entropy flows associated with the so-called fractional Fokker-Planck equation (a partial differential equation that governs the evolution of the distribution of the corresponding heavy-tailed SDE). In addition to obtaining high-probability bounds, we show that our bounds have a better dependence on the dimension of parameters as compared to p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#25112;&#30053;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#27604;&#20363;&#23450;&#24459;&#65292;&#21457;&#29616;&#25112;&#30053;&#20114;&#21160;&#21487;&#20197;&#25171;&#30772;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#21363;&#27169;&#22411;&#36234;&#22823;&#25110;&#34920;&#36798;&#33021;&#21147;&#36234;&#24378;&#24182;&#19981;&#19968;&#23450;&#20250;&#38543;&#20043;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#20960;&#20010;&#25112;&#30053;&#29615;&#22659;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.07588</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#25112;&#30053;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#27604;&#20363;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Rethinking Scaling Laws for Learning in Strategic Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#25112;&#30053;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#27604;&#20363;&#23450;&#24459;&#65292;&#21457;&#29616;&#25112;&#30053;&#20114;&#21160;&#21487;&#20197;&#25171;&#30772;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#21363;&#27169;&#22411;&#36234;&#22823;&#25110;&#34920;&#36798;&#33021;&#21147;&#36234;&#24378;&#24182;&#19981;&#19968;&#23450;&#20250;&#38543;&#20043;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#20960;&#20010;&#25112;&#30053;&#29615;&#22659;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37096;&#32626;&#21453;&#26144;&#20986;&#19968;&#20010;&#20849;&#35782;&#65306;&#27169;&#22411;&#36234;&#26377;&#34920;&#36798;&#33021;&#21147;&#65292;&#36234;&#25317;&#26377;&#22823;&#37327;&#25968;&#25454;&#65292;&#23601;&#33021;&#25913;&#21892;&#24615;&#33021;&#12290;&#38543;&#30528;&#27169;&#22411;&#22312;&#21508;&#31181;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#65292;&#23427;&#20204;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#30528;&#25112;&#30053;&#29615;&#22659;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#27169;&#22411;&#19982;&#25112;&#30053;&#20114;&#21160;&#23545;&#27604;&#20363;&#23450;&#24459;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36825;&#20010;&#33258;&#28982;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#25112;&#30053;&#20114;&#21160;&#21487;&#20197;&#25171;&#30772;&#20256;&#32479;&#30340;&#27604;&#20363;&#23450;&#24459;&#35266;&#28857;&#65292;&#21363;&#24615;&#33021;&#24182;&#19981;&#19968;&#23450;&#38543;&#30528;&#27169;&#22411;&#30340;&#25193;&#22823;&#21644;/&#25110;&#34920;&#36798;&#33021;&#21147;&#30340;&#22686;&#24378;&#65288;&#21363;&#20351;&#26377;&#26080;&#38480;&#25968;&#25454;&#65289;&#32780;&#21333;&#35843;&#25552;&#39640;&#12290;&#25105;&#20204;&#36890;&#36807;&#25112;&#30053;&#22238;&#24402;&#12289;&#25112;&#30053;&#20998;&#31867;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#20363;&#23376;&#23637;&#31034;&#20102;&#36825;&#19968;&#29616;&#35937;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#20363;&#23376;&#23637;&#31034;&#20102;&#25112;&#30053;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#27169;&#22411;&#25110;&#31574;&#30053;&#31867;&#30340;&#34920;&#36798;&#33021;&#21147;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of ever-larger machine learning models reflects a growing consensus that the more expressive the model$\unicode{x2013}$and the more data one has access to$\unicode{x2013}$the more one can improve performance. As models get deployed in a variety of real world scenarios, they inevitably face strategic environments. In this work, we consider the natural question of how the interplay of models and strategic interactions affects scaling laws. We find that strategic interactions can break the conventional view of scaling laws$\unicode{x2013}$meaning that performance does not necessarily monotonically improve as models get larger and/ or more expressive (even with infinite data). We show the implications of this phenomenon in several contexts including strategic regression, strategic classification, and multi-agent reinforcement learning through examples of strategic environments in which$\unicode{x2013}$by simply restricting the expressivity of one's model or policy class$\uni
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#22122;&#22768;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#38544;&#24615;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#31216;&#24615;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#20854;&#20013;&#19968;&#31867;&#23545;&#31216;&#24615;&#21487;&#20197;&#33258;&#28982;&#25910;&#25947;&#65292;&#32780;&#21478;&#19968;&#31867;&#23545;&#31216;&#24615;&#20960;&#20046;&#24635;&#26159;&#21457;&#25955;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36866;&#29992;&#20110;&#27809;&#26377;&#23545;&#31216;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;&#20110;&#29702;&#35299;&#35757;&#32451;&#21160;&#24577;&#21644;&#35299;&#37322;&#30456;&#20851;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07193</link><description>&lt;p&gt;
&#26799;&#24230;&#22122;&#22768;&#30340;&#38544;&#24615;&#20559;&#35265;&#65306;&#20174;&#23545;&#31216;&#24615;&#35282;&#24230;&#26469;&#30475;
&lt;/p&gt;
&lt;p&gt;
The Implicit Bias of Gradient Noise: A Symmetry Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#22122;&#22768;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#38544;&#24615;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#31216;&#24615;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#20854;&#20013;&#19968;&#31867;&#23545;&#31216;&#24615;&#21487;&#20197;&#33258;&#28982;&#25910;&#25947;&#65292;&#32780;&#21478;&#19968;&#31867;&#23545;&#31216;&#24615;&#20960;&#20046;&#24635;&#26159;&#21457;&#25955;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36866;&#29992;&#20110;&#27809;&#26377;&#23545;&#31216;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;&#20110;&#29702;&#35299;&#35757;&#32451;&#21160;&#24577;&#21644;&#35299;&#37322;&#30456;&#20851;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#25439;&#22833;&#20989;&#25968;&#23384;&#22312;&#36830;&#32493;&#23545;&#31216;&#24615;&#26102;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#35828;&#26126;&#20102;SGD&#21644;&#26799;&#24230;&#19979;&#38477;&#20043;&#38388;&#30340;&#20998;&#27495;&#26159;&#22810;&#20040;&#24040;&#22823;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#23545;&#31216;&#24615;&#23545;&#23398;&#20064;&#21160;&#24577;&#30340;&#24433;&#21709;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#19968;&#26063;&#23545;&#31216;&#24615;&#20998;&#20026;&#20004;&#31867;&#12290;&#23545;&#20110;&#19968;&#31867;&#23545;&#31216;&#24615;&#65292;SGD&#33258;&#28982;&#22320;&#25910;&#25947;&#21040;&#20855;&#26377;&#24179;&#34913;&#21644;&#23545;&#40784;&#26799;&#24230;&#22122;&#22768;&#30340;&#35299;&#12290;&#23545;&#20110;&#21478;&#19968;&#31867;&#23545;&#31216;&#24615;&#65292;SGD&#20960;&#20046;&#24635;&#26159;&#21457;&#25955;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#25439;&#22833;&#20989;&#25968;&#20013;&#27809;&#26377;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20381;&#28982;&#36866;&#29992;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#26222;&#36941;&#30340;&#65292;&#23427;&#21482;&#20381;&#36182;&#20110;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#65292;&#32780;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#32454;&#33410;&#26080;&#20851;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#23545;&#20110;&#36880;&#27493;&#21464;&#24418;&#21644;&#24179;&#22374;&#21270;&#25552;&#20379;&#20102;&#35299;&#37322;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#24120;&#35265;&#30340;&#23454;&#38469;&#38382;&#39064;&#65292;&#22914;&#34920;&#31034;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize the learning dynamics of stochastic gradient descent (SGD) when continuous symmetry exists in the loss function, where the divergence between SGD and gradient descent is dramatic. We show that depending on how the symmetry affects the learning dynamics, we can divide a family of symmetry into two classes. For one class of symmetry, SGD naturally converges to solutions that have a balanced and aligned gradient noise. For the other class of symmetry, SGD will almost always diverge. Then, we show that our result remains applicable and can help us understand the training dynamics even when the symmetry is not present in the loss function. Our main result is universal in the sense that it only depends on the existence of the symmetry and is independent of the details of the loss function. We demonstrate that the proposed theory offers an explanation of progressive sharpening and flattening and can be applied to common practical problems such as representation normalization, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;Bilevel&#24378;&#21270;&#23398;&#20064;&#21644;RLHF&#38382;&#39064;&#65292;&#36825;&#26159;&#39318;&#20010;&#26377;&#21407;&#21017;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06886</link><description>&lt;p&gt;
Bilevel&#24378;&#21270;&#23398;&#20064;&#21644;RLHF&#30340;&#26377;&#21407;&#21017;&#30340;&#22522;&#20110;&#24809;&#32602;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;Bilevel&#24378;&#21270;&#23398;&#20064;&#21644;RLHF&#38382;&#39064;&#65292;&#36825;&#26159;&#39318;&#20010;&#26377;&#21407;&#21017;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Bilevel&#20248;&#21270;&#24050;&#34987;&#24212;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24212;&#29992;&#20165;&#38480;&#20110;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#20855;&#26377;&#33391;&#24615;&#32467;&#26500;&#30340;&#38745;&#24577;&#30446;&#26631;&#20989;&#25968;&#12290;&#20294;&#26159;&#65292;&#28608;&#21169;&#35774;&#35745;&#12289;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;(RL)&#21644;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;RLHF&#31561;Bilevel&#38382;&#39064;&#36890;&#24120;&#34987;&#24314;&#27169;&#20026;&#36229;&#36234;&#31616;&#21333;&#38745;&#24577;&#30446;&#26631;&#32467;&#26500;&#30340;&#21160;&#24577;&#30446;&#26631;&#20989;&#25968;&#65292;&#36825;&#32473;&#20351;&#29992;&#29616;&#26377;Bilevel&#35299;&#20915;&#26041;&#26696;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#26032;&#30340;Bilevel&#38382;&#39064;&#31867;&#21035;&#65292;&#25105;&#20204;&#36890;&#36807;&#24809;&#32602;&#24418;&#24335;&#24341;&#20837;&#20102;&#35299;&#20915;Bilevel RL&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#21407;&#21017;&#24615;&#31639;&#27861;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#30740;&#31350;&#38382;&#39064;&#30340;&#26223;&#35266;&#21450;&#20854;&#22522;&#20110;&#24809;&#32602;&#30340;&#65288;&#31574;&#30053;&#65289;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;Stackelberg&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#12289;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;RL&#21644;&#28608;&#21169;&#35774;&#35745;&#20013;&#36827;&#34892;&#27169;&#25311;&#26469;&#35777;&#26126;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization has been recently applied to many machine learning tasks. However, their applications have been restricted to the supervised learning setting, where static objective functions with benign structures are considered. But bilevel problems such as incentive design, inverse reinforcement learning (RL), and RL from human feedback (RLHF) are often modeled as dynamic objective functions that go beyond the simple static objective structures, which pose significant challenges of using existing bilevel solutions. To tackle this new class of bilevel problems, we introduce the first principled algorithmic framework for solving bilevel RL problems through the lens of penalty formulation. We provide theoretical studies of the problem landscape and its penalty-based (policy) gradient algorithms. We demonstrate the effectiveness of our algorithms via simulations in the Stackelberg Markov game, RL from human feedback and incentive design.
&lt;/p&gt;</description></item><item><title>SMC&#24182;&#34892;&#25193;&#23637;&#26041;&#27861;pSMC&#20855;&#26377;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#65292;&#20855;&#26377;&#26377;&#30028;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#35201;&#27714;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06173</link><description>&lt;p&gt;
SMC&#23601;&#26159;&#20320;&#38656;&#35201;&#30340;&#65306;&#24182;&#34892;&#24378;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
SMC Is All You Need: Parallel Strong Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06173
&lt;/p&gt;
&lt;p&gt;
SMC&#24182;&#34892;&#25193;&#23637;&#26041;&#27861;pSMC&#20855;&#26377;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#65292;&#20855;&#26377;&#26377;&#30028;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#35201;&#27714;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#19968;&#33324;&#26694;&#26550;&#20013;&#65292;&#30446;&#26631;&#20998;&#24067;&#21482;&#33021;&#25353;&#27604;&#20363;&#24120;&#25968;&#36827;&#34892;&#35780;&#20272;&#12290;&#20256;&#32479;&#30340;&#19968;&#33268;Bayesian&#26041;&#27861;&#65292;&#22914;&#24207;&#36143;&#33945;&#29305;&#21345;&#27931;(SMC)&#21644;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;(MCMC)&#65292;&#20855;&#26377;&#26080;&#30028;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#35201;&#27714;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23436;&#20840;&#24182;&#34892;&#30340;&#24207;&#36143;&#33945;&#29305;&#21345;&#27931;(pSMC)&#26041;&#27861;&#65292;&#21487;&#20197;&#35777;&#26126;&#23427;&#20855;&#26377;&#24182;&#34892;&#24378;&#25193;&#23637;&#24615;&#65292;&#21363;&#22914;&#26524;&#20801;&#35768;&#24322;&#27493;&#36827;&#31243;&#25968;&#37327;&#22686;&#38271;&#65292;&#26102;&#38388;&#22797;&#26434;&#24615;(&#21644;&#27599;&#20010;&#33410;&#28857;&#30340;&#20869;&#23384;)&#20173;&#28982;&#20445;&#25345;&#26377;&#30028;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;pSMC&#20855;&#26377;MSE$=O(1/NR)$&#30340;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$N$&#34920;&#31034;&#27599;&#20010;&#22788;&#29702;&#22120;&#20013;&#30340;&#36890;&#20449;&#26679;&#26412;&#25968;&#37327;&#65292;$R$&#34920;&#31034;&#22788;&#29702;&#22120;&#25968;&#37327;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#36866;&#24403;&#22823;&#30340;&#38382;&#39064;&#30456;&#20851;$N$&#65292;&#24403;$R\rightarrow \infty$&#26102;&#65292;&#35813;&#26041;&#27861;&#20197;&#22266;&#23450;&#26377;&#38480;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;Cost$=O(1)$&#25910;&#25947;&#21040;&#26080;&#31351;&#23567;&#31934;&#24230;MSE$=O(\varepsilon^2)$&#65292;&#27809;&#26377;&#25928;&#29575;&#27844;&#28431;&#65292;&#21363;&#35745;&#31639;&#22797;&#26434;&#24615;Cost$=O(\varepsilon)$&#12290;
&lt;/p&gt;
&lt;p&gt;
In the general framework of Bayesian inference, the target distribution can only be evaluated up-to a constant of proportionality. Classical consistent Bayesian methods such as sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC) have unbounded time complexity requirements. We develop a fully parallel sequential Monte Carlo (pSMC) method which provably delivers parallel strong scaling, i.e. the time complexity (and per-node memory) remains bounded if the number of asynchronous processes is allowed to grow. More precisely, the pSMC has a theoretical convergence rate of MSE$ = O(1/NR)$, where $N$ denotes the number of communicating samples in each processor and $R$ denotes the number of processors. In particular, for suitably-large problem-dependent $N$, as $R \rightarrow \infty$ the method converges to infinitesimal accuracy MSE$=O(\varepsilon^2)$ with a fixed finite time-complexity Cost$=O(1)$ and with no efficiency leakage, i.e. computational complexity Cost$=O(\varepsilon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEAK&#30340;&#26032;&#22411;&#38750;&#21442;&#25968;&#39034;&#24207;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#22343;&#20540;&#26816;&#39564;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#27979;&#35797;&#21363;&#21338;&#24328;&#30340;&#26694;&#26550;&#65292;&#22312;&#20219;&#20309;&#20572;&#27490;&#26102;&#38388;&#19978;&#25552;&#20379;&#20102;&#38750;&#28176;&#36827;&#945;&#27700;&#24179;&#30340;&#26816;&#39564;&#12290;PEAK&#33021;&#22815;&#26377;&#25928;&#25298;&#32477;&#22312;&#28385;&#36275;&#38750;&#21442;&#25968;&#20551;&#35774;&#26465;&#20214;&#30340;&#25152;&#26377;&#28508;&#22312;&#20998;&#24067;&#20013;&#38169;&#35823;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#32852;&#21512;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06122</link><description>&lt;p&gt;
&#20351;&#29992;PEAK&#36827;&#34892;&#31397;&#25506;&#65306;&#22810;&#20010;&#25968;&#25454;&#27969;&#22343;&#20540;&#30340;&#39034;&#24207;&#12289;&#38750;&#21442;&#25968;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEAK&#30340;&#26032;&#22411;&#38750;&#21442;&#25968;&#39034;&#24207;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#22343;&#20540;&#26816;&#39564;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#27979;&#35797;&#21363;&#21338;&#24328;&#30340;&#26694;&#26550;&#65292;&#22312;&#20219;&#20309;&#20572;&#27490;&#26102;&#38388;&#19978;&#25552;&#20379;&#20102;&#38750;&#28176;&#36827;&#945;&#27700;&#24179;&#30340;&#26816;&#39564;&#12290;PEAK&#33021;&#22815;&#26377;&#25928;&#25298;&#32477;&#22312;&#28385;&#36275;&#38750;&#21442;&#25968;&#20551;&#35774;&#26465;&#20214;&#30340;&#25152;&#26377;&#28508;&#22312;&#20998;&#24067;&#20013;&#38169;&#35823;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#32852;&#21512;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#39034;&#24207;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#22343;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;PEAK&#65288;&#22522;&#20110;&#26399;&#26395;&#24179;&#22343;&#36164;&#20135;&#30340;&#31397;&#25506;&#65289;&#65292;&#22522;&#20110;&#27979;&#35797;&#21363;&#21338;&#24328;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#20219;&#20309;&#20572;&#27490;&#26102;&#38388;&#19978;&#30340;&#38750;&#28176;&#36827;&#945;&#27700;&#24179;&#27979;&#35797;&#12290;PEAK&#22312;&#35745;&#31639;&#19978;&#21487;&#34892;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#25298;&#32477;&#22312;&#28385;&#36275;&#25105;&#20204;&#30340;&#38750;&#21442;&#25968;&#20551;&#35774;&#26465;&#20214;&#30340;&#25152;&#26377;&#28508;&#22312;&#20998;&#24067;&#20013;&#38169;&#35823;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#32852;&#21512;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#21644;&#38408;&#20540;&#35782;&#21035;&#20219;&#21153;&#20013;&#23545;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36827;&#34892;&#20102;&#25968;&#20540;&#39564;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel nonparametric sequential test for composite hypotheses for means of multiple data streams. Our proposed method, \emph{peeking with expectation-based averaged capital} (PEAK), builds upon the testing-as-betting framework and provides a non-asymptotic $\alpha$-level test across any stopping time. PEAK is computationally tractable and efficiently rejects hypotheses that are incorrect across all potential distributions that satisfy our nonparametric assumption, enabling joint composite hypothesis testing on multiple streams of data. We numerically validate our theoretical findings under the best arm identification and threshold identification in the bandit setting, illustrating the computational efficiency of our method against state-of-the-art testing methods.
&lt;/p&gt;</description></item><item><title>\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.05951</link><description>&lt;p&gt;
\textit{MinMaxMin} $Q$-learning
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05951
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20445;&#23432;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#23384;&#22312;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65288;$Q$-&#20272;&#35745;&#36807;&#39640;&#20272;&#35745;&#20102;&#30495;&#23454;&#30340;$Q$&#20540;&#65289;&#12290;&#20854;&#26680;&#24515;&#20844;&#24335;&#20381;&#36182;&#20110;$Q$-&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#37319;&#29992;&#26368;&#23567;&#25209;&#27425;&#26368;&#22823;&#26368;&#23567;$Q$-&#32593;&#32476;&#36317;&#31163;&#20316;&#20026;$Q$-&#30446;&#26631;&#21152;&#20837;&#65292;&#24182;&#20316;&#20026;&#20248;&#20808;&#32423;&#32463;&#39564;&#22238;&#25918;&#37319;&#26679;&#35268;&#21017;&#12290;&#25105;&#20204;&#22312;TD3&#21644;TD7&#20043;&#19978;&#23454;&#26045;&#20102;\textit{MinMaxMin}&#65292;&#24182;&#23545;&#20854;&#22312;&#27969;&#34892;&#30340;MuJoCo&#21644;Bullet&#29615;&#22659;&#20013;&#23545;&#25239;&#29616;&#26377;&#30340;&#36830;&#32493;&#31354;&#38388;&#31639;&#27861;-DDPG&#65292;TD3&#21644;TD7&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25152;&#26377;&#27979;&#35797;&#20219;&#21153;&#20013;&#65292;\textit{MinMaxMin}&#30456;&#23545;&#20110;DDPG&#65292;TD3&#21644;TD7&#22343;&#34920;&#29616;&#20986;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning is a novel \textit{optimistic} Actor-Critic algorithm that addresses the problem of \textit{overestimation} bias ($Q$-estimations are overestimating the real $Q$-values) inherent in \textit{conservative} RL algorithms. Its core formula relies on the disagreement among $Q$-networks in the form of the min-batch MaxMin $Q$-networks distance which is added to the $Q$-target and used as the priority experience replay sampling-rule. We implement \textit{MinMaxMin} on top of TD3 and TD7, subjecting it to rigorous testing against state-of-the-art continuous-space algorithms-DDPG, TD3, and TD7-across popular MuJoCo and Bullet environments. The results show a consistent performance improvement of \textit{MinMaxMin} over DDPG, TD3, and TD7 across all tested tasks.
&lt;/p&gt;</description></item><item><title>SQT&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#21033;&#29992;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.05950</link><description>&lt;p&gt;
SQT - std Q-target
&lt;/p&gt;
&lt;p&gt;
\textit{SQT} -- \textit{std} $Q$-target
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05950
&lt;/p&gt;
&lt;p&gt;
SQT&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#21033;&#29992;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Std Q-target&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#19968;&#20010;&#20851;&#38190;&#30340;Q&#20844;&#24335;&#65306;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#65292;&#36825;&#20010;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#26159;&#23545;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#30340;&#19968;&#31181;&#31616;&#32422;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;TD3/TD7&#20195;&#30721;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;SQT&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;actor-critic&#31639;&#27861;DDPG&#12289;TD3&#21644;TD7&#22312;&#19971;&#20010;&#24120;&#35265;&#30340;MuJoCo&#21644;Bullet&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;SQT&#30340;Q-target&#20844;&#24335;&#30456;&#23545;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#22312;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#20445;&#23432;&#35299;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#65292;SQT&#30456;&#23545;&#20110;DDPG&#12289;TD3&#21644;TD7&#37117;&#26377;&#26126;&#26174;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{Std} $Q$-target is a \textit{conservative}, actor-critic, ensemble, $Q$-learning-based algorithm, which is based on a single key $Q$-formula: $Q$-networks standard deviation, which is an "uncertainty penalty", and, serves as a minimalistic solution to the problem of \textit{overestimation} bias. We implement \textit{SQT} on top of TD3/TD7 code and test it against the state-of-the-art (SOTA) actor-critic algorithms, DDPG, TD3 and TD7 on seven popular MuJoCo and Bullet tasks. Our results demonstrate \textit{SQT}'s $Q$-target formula superiority over \textit{TD3}'s $Q$-target formula as a \textit{conservative} solution to overestimation bias in RL, while \textit{SQT} shows a clear performance advantage on a wide margin over DDPG, TD3, and TD7 on all tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#24179;&#22343;&#22330;&#21338;&#24328;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#37096;&#20998;&#22522;&#20110;&#27169;&#22411;&#30340;Eluder&#32500;&#24230;&#65288;P-MBED&#65289;&#27010;&#24565;&#26469;&#34913;&#37327;&#27169;&#22411;&#31867;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#22522;&#26412;&#20551;&#35774;&#19979;&#65292;&#23398;&#20064;&#24179;&#22343;&#22330;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#24182;&#19981;&#27604;&#35299;&#20915;&#23545;&#25968;&#20010;&#21333;&#20010;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26356;&#20855;&#32479;&#35745;&#25361;&#25112;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05724</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#24179;&#22343;&#22330;&#21338;&#24328;&#20013;&#24182;&#19981;&#27604;&#21333;&#20010;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26356;&#21152;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#24179;&#22343;&#22330;&#21338;&#24328;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#37096;&#20998;&#22522;&#20110;&#27169;&#22411;&#30340;Eluder&#32500;&#24230;&#65288;P-MBED&#65289;&#27010;&#24565;&#26469;&#34913;&#37327;&#27169;&#22411;&#31867;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#22522;&#26412;&#20551;&#35774;&#19979;&#65292;&#23398;&#20064;&#24179;&#22343;&#22330;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#24182;&#19981;&#27604;&#35299;&#20915;&#23545;&#25968;&#20010;&#21333;&#20010;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26356;&#20855;&#32479;&#35745;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24179;&#22343;&#22330;&#21338;&#24328;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#20989;&#25968;&#36924;&#36817;&#19979;&#24378;&#21270;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#35813;&#26041;&#27861;&#38656;&#35201;&#31574;&#30053;&#24615;&#25506;&#32034;&#20197;&#25214;&#21040;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#37096;&#20998;&#22522;&#20110;&#27169;&#22411;&#30340;Eluder&#32500;&#24230;&#65288;P-MBED&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#27010;&#24565;&#26469;&#25551;&#36848;&#27169;&#22411;&#31867;&#22797;&#26434;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;P-MBED&#21487;&#20197;&#34913;&#37327;&#20174;&#32473;&#23450;&#30340;&#24179;&#22343;&#22330;&#27169;&#22411;&#31867;&#36716;&#25442;&#32780;&#26469;&#30340;&#21333;&#20010;&#26234;&#33021;&#20307;&#27169;&#22411;&#31867;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#28508;&#22312;&#19978;&#21487;&#33021;&#27604;\citet{huang2023statistical}&#25552;&#20986;&#30340;MBED&#25351;&#25968;&#32423;&#20302;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#28040;&#38500;&#31639;&#27861;&#65292;&#20855;&#26377;&#26032;&#39062;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#24182;&#24314;&#31435;&#20102;&#19982;P-MBED&#30456;&#20851;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22522;&#26412;&#21487;&#23454;&#29616;&#24615;&#21644;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#19979;&#65292;&#23398;&#20064;&#24179;&#22343;&#22330;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#24182;&#19981;&#27604;&#35299;&#20915;&#23545;&#25968;&#20010;&#21333;&#20010;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26356;&#20855;&#32479;&#35745;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#22810;&#31867;&#22411;&#24179;&#22343;&#22330;&#21338;&#24328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the sample complexity of reinforcement learning (RL) in Mean-Field Games (MFGs) with model-based function approximation that requires strategic exploration to find a Nash Equilibrium policy. We introduce the Partial Model-Based Eluder Dimension (P-MBED), a more effective notion to characterize the model class complexity. Notably, P-MBED measures the complexity of the single-agent model class converted from the given mean-field model class, and potentially, can be exponentially lower than the MBED proposed by \citet{huang2023statistical}. We contribute a model elimination algorithm featuring a novel exploration strategy and establish sample complexity results polynomial w.r.t.~P-MBED. Crucially, our results reveal that, under the basic realizability and Lipschitz continuity assumptions, \emph{learning Nash Equilibrium in MFGs is no more statistically challenging than solving a logarithmic number of single-agent RL problems}. We further extend our results to Multi-Type MFGs, gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;Semi-dual JKO (S-JKO)&#65292;&#36890;&#36807;&#37319;&#29992;&#21322;&#23545;&#20598;&#24418;&#24335;&#30340;JKO&#27493;&#39588;&#65292;&#38477;&#20302;&#20102;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;WGF&#19978;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.05443</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;Wasserstein&#28176;&#21464;&#27969;
&lt;/p&gt;
&lt;p&gt;
Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;Semi-dual JKO (S-JKO)&#65292;&#36890;&#36807;&#37319;&#29992;&#21322;&#23545;&#20598;&#24418;&#24335;&#30340;JKO&#27493;&#39588;&#65292;&#38477;&#20302;&#20102;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;WGF&#19978;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wasserstein&#28176;&#21464;&#27969;&#65288;WGF&#65289;&#25551;&#36848;&#20102;Wasserstein&#31354;&#38388;&#20013;&#27010;&#29575;&#23494;&#24230;&#30340;&#26799;&#24230;&#21160;&#21147;&#23398;&#12290;WGF&#25552;&#20379;&#20102;&#22312;&#27010;&#29575;&#20998;&#24067;&#19978;&#36827;&#34892;&#20248;&#21270;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#25968;&#20540;&#19978;&#36817;&#20284;&#36830;&#32493;&#30340;WGF&#38656;&#35201;&#26102;&#38388;&#31163;&#25955;&#21270;&#26041;&#27861;&#12290;&#20854;&#20013;&#26368;&#33879;&#21517;&#30340;&#26041;&#27861;&#26159;JKO&#26041;&#26696;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#20197;&#21069;&#30340;WGF&#27169;&#22411;&#37319;&#29992;JKO&#26041;&#26696;&#65292;&#24182;&#20026;&#27599;&#20010;JKO&#27493;&#39588;&#21442;&#25968;&#21270;&#20256;&#36755;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#19982;JKO&#27493;&#39588;&#25968;&#37327;K&#25104;&#20108;&#27425;&#22686;&#38271;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;$O(K^2)$&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;WGF&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;WGF&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;&#21322;&#23545;&#20598;JKO&#65288;S-JKO&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;JKO&#27493;&#39588;&#30340;&#21322;&#23545;&#20598;&#24418;&#24335;&#65292;&#36890;&#36807;JKO&#27493;&#39588;&#19982;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#24471;&#21040;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35757;&#32451;&#22797;&#26434;&#24615;&#38477;&#20302;&#21040;$O(K)$&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;WGF&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Gradient Flow (WGF) describes the gradient dynamics of probability density within the Wasserstein space. WGF provides a promising approach for conducting optimization over the probability distributions. Numerically approximating the continuous WGF requires the time discretization method. The most well-known method for this is the JKO scheme. In this regard, previous WGF models employ the JKO scheme and parametrize transport map for each JKO step. However, this approach results in quadratic training complexity $O(K^2)$ with the number of JKO step $K$. This severely limits the scalability of WGF models. In this paper, we introduce a scalable WGF-based generative model, called Semi-dual JKO (S-JKO). Our model is based on the semi-dual form of the JKO step, derived from the equivalence between the JKO step and the Unbalanced Optimal Transport. Our approach reduces the training complexity to $O(K)$. We demonstrate that our model significantly outperforms existing WGF-based gener
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;</title><link>https://arxiv.org/abs/2402.04854</link><description>&lt;p&gt;
&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#23398;&#26415;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04854
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32570;&#20047;&#30740;&#31350;&#22521;&#35757;&#30340;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#26469;&#35828;&#65292;&#30740;&#31350;&#35843;&#26597;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#20123;&#30740;&#31350;&#32773;&#22312;&#30701;&#26102;&#38388;&#20869;&#24456;&#38590;&#29702;&#35299;&#20182;&#20204;&#30740;&#31350;&#20027;&#39064;&#20869;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#21457;&#29616;&#26032;&#30340;&#30740;&#31350;&#21457;&#29616;&#12290;&#20026;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#25552;&#20379;&#30452;&#35266;&#30340;&#24110;&#21161;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#25552;&#20379;&#30456;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;(KG)&#24182;&#25512;&#33616;&#30456;&#20851;&#30340;&#23398;&#26415;&#35770;&#25991;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#20027;&#35201;&#20381;&#36182;&#20110;&#30740;&#31350;&#39046;&#22495;&#30340;&#20851;&#38190;&#23383;&#65292;&#24120;&#24120;&#26080;&#27861;&#28165;&#26970;&#22320;&#21576;&#29616;&#22810;&#20010;&#30456;&#20851;&#35770;&#25991;&#20043;&#38388;&#30340;&#36923;&#36753;&#23618;&#27425;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20165;&#20165;&#20381;&#36182;&#20110;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#35753;&#30740;&#31350;&#20154;&#21592;&#22256;&#24785;&#20026;&#20160;&#20040;&#25512;&#33616;&#20102;&#29305;&#23450;&#30340;&#25991;&#31456;&#12290;&#20182;&#20204;&#21487;&#33021;&#32570;&#20047;&#20102;&#35299;&#20851;&#20110;&#20182;&#20204;&#24076;&#26395;&#33719;&#24471;&#30340;"&#38382;&#39064;&#35299;&#20915;"&#21644;"&#38382;&#39064;&#21457;&#29616;"&#20043;&#38388;&#30340;&#35265;&#35299;&#36830;&#25509;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25903;&#25345;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.04520</link><description>&lt;p&gt;
&#20851;&#20110;&#29616;&#20195;Hopfield&#27169;&#22411;&#35745;&#31639;&#38480;&#21046;&#30340;&#19968;&#20010;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04520
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22522;&#20110;&#27169;&#24335;&#30340;&#33539;&#25968;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#25928;&#29575;&#36827;&#34892;&#30456;&#21464;&#34892;&#20026;&#30340;&#21051;&#30011;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#36755;&#20837;&#26597;&#35810;&#27169;&#24335;&#21644;&#35760;&#24518;&#27169;&#24335;&#30340;&#33539;&#25968;&#30340;&#19978;&#30028;&#26631;&#20934;&#12290;&#20165;&#22312;&#36825;&#20010;&#26631;&#20934;&#20043;&#19979;&#65292;&#20551;&#35774;&#28385;&#36275;Strong Exponential Time Hypothesis (SETH)&#65292;&#23384;&#22312;&#23376;&#20108;&#27425;&#65288;&#39640;&#25928;&#65289;&#21464;&#20307;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24403;&#26377;&#25928;&#26631;&#20934;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26377;&#25928;&#26500;&#36896;&#30340;&#27491;&#24335;&#31034;&#20363;&#12290;&#36825;&#21253;&#25324;&#19968;&#20010;&#35745;&#31639;&#26102;&#38388;&#30340;&#19979;&#30028;&#23548;&#20986;&#65292;&#19982;$\Max\{$&#23384;&#20648;&#30340;&#35760;&#24518;&#27169;&#24335;&#25968;&#37327;&#65292;&#36755;&#20837;&#26597;&#35810;&#24207;&#21015;&#30340;&#38271;&#24230;$\}$&#32447;&#24615;&#32553;&#25918;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#31209;MDPs&#19978;&#30340;&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#37096;&#20998;&#25968;&#25454;&#35206;&#30422;&#20551;&#35774;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#24182;&#36798;&#21040;&#20102;$O(\epsilon^{-2})$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36824;&#25903;&#25345;&#39069;&#22806;&#22870;&#21169;&#20449;&#21495;&#30340;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2402.04493</link><description>&lt;p&gt;
&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#22522;&#26412;&#23545;&#20598;&#31639;&#27861;&#22312;&#20302;&#31209;MDPs&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Low-Rank MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04493
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#31209;MDPs&#19978;&#30340;&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#37096;&#20998;&#25968;&#25454;&#35206;&#30422;&#20551;&#35774;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#24182;&#36798;&#21040;&#20102;$O(\epsilon^{-2})$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36824;&#25903;&#25345;&#39069;&#22806;&#22870;&#21169;&#20449;&#21495;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#31181;&#26368;&#22823;&#21270;&#26399;&#26395;&#32047;&#31215;&#22870;&#21169;&#30340;&#31574;&#30053;&#12290;&#26368;&#36817;&#65292;&#23545;&#20110;&#20302;&#31209;MDPs&#25110;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#29616;&#26377;&#31639;&#27861;&#22312;&#25214;&#21040;$\epsilon$-&#20248;&#21270;&#31574;&#30053;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$O(\epsilon^{-2})$&#26102;&#65292;&#35201;&#20040;&#38656;&#35201;&#22343;&#21248;&#30340;&#25968;&#25454;&#35206;&#30422;&#20551;&#35774;&#65292;&#35201;&#20040;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25240;&#25187;&#26080;&#31351;&#26102;&#27573;&#35774;&#32622;&#19979;&#65292;&#29992;&#20110;&#20302;&#31209;MDPs&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#26412;&#23545;&#20598;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#22312;&#37096;&#20998;&#25968;&#25454;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;&#35813;&#35774;&#32622;&#20013;&#31532;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#36798;&#21040;$O(\epsilon^{-2})$&#30340;&#35745;&#31639;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#36825;&#20248;&#20110;&#26368;&#36817;&#30340;&#19968;&#39033;&#24037;&#20316;&#65292;&#20854;&#38656;&#35201;$O(\epsilon^{-4})$&#20010;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#25903;&#25345;&#39069;&#22806;&#22870;&#21169;&#20449;&#21495;&#30340;&#32422;&#26463;&#65292;&#23558;&#20043;&#21069;&#30340;&#24037;&#20316;&#25193;&#23637;&#21040;&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward using a pre-collected dataset. Offline RL with low-rank MDPs or general function approximation has been widely studied recently, but existing algorithms with sample complexity $O(\epsilon^{-2})$ for finding an $\epsilon$-optimal policy either require a uniform data coverage assumptions or are computationally inefficient. In this paper, we propose a primal dual algorithm for offline RL with low-rank MDPs in the discounted infinite-horizon setting. Our algorithm is the first computationally efficient algorithm in this setting that achieves sample complexity of $O(\epsilon^{-2})$ with partial data coverage assumption. This improves upon a recent work that requires $O(\epsilon^{-4})$ samples. Moreover, our algorithm extends the previous work to the offline constrained RL setting by supporting constraints on additional reward signals.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DFA-LLM&#65288;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22686;&#24378;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23884;&#20837;&#20174;&#23545;&#35805;&#20013;&#23398;&#20064;&#21040;&#30340;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#24471;&#23545;&#35805;&#20195;&#29702;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#35268;&#33539;&#21512;&#35268;&#24615;&#30340;&#22238;&#22797;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04411</link><description>&lt;p&gt;
Chatbot&#36935;&#35265;&#31649;&#36947;&#65306;&#21033;&#29992;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22686;&#36827;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DFA-LLM&#65288;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22686;&#24378;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23884;&#20837;&#20174;&#23545;&#35805;&#20013;&#23398;&#20064;&#21040;&#30340;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#24471;&#23545;&#35805;&#20195;&#29702;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#35268;&#33539;&#21512;&#35268;&#24615;&#30340;&#22238;&#22797;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22686;&#24378;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;DFA-LLM&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#21319;&#23545;&#35805;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#20256;&#32479;&#30340;LLM&#22312;&#29305;&#23450;&#24773;&#26223;&#65288;&#22914;&#24773;&#24863;&#25903;&#25345;&#21644;&#23458;&#25143;&#26381;&#21153;&#65289;&#20013;&#29983;&#25104;&#35268;&#33539;&#21512;&#35268;&#30340;&#22238;&#22797;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23558;&#20174;&#35757;&#32451;&#23545;&#35805;&#20013;&#23398;&#20064;&#21040;&#30340;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;DFA&#65289;&#23884;&#20837;&#21040;LLM&#20013;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#36825;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#20351;&#24471;LLM&#33021;&#22815;&#25353;&#29031;DFA&#25351;&#23548;&#30340;&#30830;&#23450;&#24615;&#22238;&#24212;&#36335;&#24452;&#26469;&#22238;&#24212;&#12290;DFA-LLM&#30340;&#20248;&#21183;&#21253;&#25324;&#21487;&#35299;&#37322;&#24615;&#32467;&#26500;&#65292;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23545;&#35805;&#22238;&#22797;&#26816;&#32034;&#20197;&#21450;&#19982;&#29616;&#26377;LLM&#30340;&#21363;&#25554;&#21363;&#29992;&#20860;&#23481;&#24615;&#12290;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#39564;&#35777;&#20102;DFA-LLM&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#23427;&#26377;&#28508;&#21147;&#25104;&#20026;&#23545;&#35805;&#20195;&#29702;&#39046;&#22495;&#30340;&#26377;&#20215;&#20540;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Definite Finite Automaton augmented large language model (DFA-LLM), a novel framework designed to enhance the capabilities of conversational agents using large language models (LLMs). Traditional LLMs face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service. Our framework addresses these challenges by embedding a Definite Finite Automaton (DFA), learned from training dialogues, within the LLM. This structured approach enables the LLM to adhere to a deterministic response pathway, guided by the DFA. The advantages of DFA-LLM include an interpretable structure through human-readable DFA, context-aware retrieval for responses in conversations, and plug-and-play compatibility with existing LLMs. Extensive benchmarks validate DFA-LLM's effectiveness, indicating its potential as a valuable contribution to the conversational agent.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24494;&#35843;&#40657;&#30418;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#26377;&#36755;&#20837;&#25552;&#31034;&#21644;&#36755;&#20986;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25552;&#31034;&#29983;&#25104;&#27169;&#22359;&#21644;&#19968;&#20010;&#39044;&#27979;&#20248;&#21270;&#27169;&#22359;&#65292;&#24182;&#24341;&#20837;&#20102;&#36741;&#21161;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#25439;&#22833;&#36827;&#34892;&#19968;&#33268;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.04050</link><description>&lt;p&gt;
&#36830;&#25509;&#28857;&#65306;&#21327;&#20316;&#24494;&#35843;&#40657;&#30418;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24494;&#35843;&#40657;&#30418;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#26377;&#36755;&#20837;&#25552;&#31034;&#21644;&#36755;&#20986;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25552;&#31034;&#29983;&#25104;&#27169;&#22359;&#21644;&#19968;&#20010;&#39044;&#27979;&#20248;&#21270;&#27169;&#22359;&#65292;&#24182;&#24341;&#20837;&#20102;&#36741;&#21161;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#25439;&#22833;&#36827;&#34892;&#19968;&#33268;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#22312;&#23558;&#20854;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#26102;&#25237;&#20837;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#12290;&#23613;&#31649;&#22312;&#35774;&#35745;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#32780;&#23545;&#20110;&#20445;&#25252;&#27169;&#22411;&#25152;&#26377;&#26435;&#65292;&#27169;&#22411;&#25152;&#26377;&#32773;&#36890;&#24120;&#36873;&#25321;&#23558;&#20854;&#20316;&#20026;&#40657;&#30418;&#25552;&#20379;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24494;&#35843;&#65288;CraFT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#40657;&#30418;VLMs fine-tuning&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20854;&#20013;&#21482;&#33021;&#35775;&#38382;&#27169;&#22411;&#30340;&#36755;&#20837;&#25552;&#31034;&#21644;&#36755;&#20986;&#39044;&#27979;&#12290;CraFT&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65292;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#25991;&#26412;&#25552;&#31034;&#30340;&#25552;&#31034;&#29983;&#25104;&#27169;&#22359;&#65292;&#19968;&#20010;&#29992;&#20110;&#20197;&#27531;&#24046;&#26041;&#24335;&#22686;&#24378;&#36755;&#20986;&#39044;&#27979;&#30340;&#39044;&#27979;&#20248;&#21270;&#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#20419;&#36827;&#36825;&#20123;&#27169;&#22359;&#20043;&#38388;&#30340;&#19968;&#33268;&#20248;&#21270;&#12290;&#36825;&#20123;&#27169;&#22359;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#21327;&#20316;&#35757;&#32451;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \textbf{C}ollabo\textbf{ra}tive \textbf{F}ine-\textbf{T}uning (\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algor
&lt;/p&gt;</description></item><item><title>SMOTE&#26159;&#19968;&#31181;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#24120;&#29992;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#22797;&#21046;&#21407;&#22987;&#23569;&#25968;&#26679;&#26412;&#26469;&#37325;&#26032;&#29983;&#25104;&#21407;&#22987;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;SMOTE&#30340;&#23494;&#24230;&#22312;&#23569;&#25968;&#26679;&#26412;&#20998;&#24067;&#30340;&#36793;&#30028;&#38468;&#36817;&#36880;&#28176;&#20943;&#23567;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;BorderLine SMOTE&#31574;&#30053;&#30340;&#21512;&#29702;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;SMOTE&#30456;&#20851;&#31574;&#30053;&#65292;&#24182;&#19982;&#20854;&#20182;&#37325;&#26032;&#24179;&#34913;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26368;&#32456;&#21457;&#29616;&#65292;&#22312;&#25968;&#25454;&#38598;&#26497;&#24230;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;SMOTE&#12289;&#25552;&#20986;&#30340;&#26041;&#27861;&#25110;&#27424;&#37319;&#26679;&#31243;&#24207;&#26159;&#26368;&#20339;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03819</link><description>&lt;p&gt;
SMOTE&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#65306;&#20851;&#20110;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#30340;&#38480;&#21046;&#21644;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Theoretical and experimental study of SMOTE: limitations and comparisons of rebalancing strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03819
&lt;/p&gt;
&lt;p&gt;
SMOTE&#26159;&#19968;&#31181;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#24120;&#29992;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#22797;&#21046;&#21407;&#22987;&#23569;&#25968;&#26679;&#26412;&#26469;&#37325;&#26032;&#29983;&#25104;&#21407;&#22987;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;SMOTE&#30340;&#23494;&#24230;&#22312;&#23569;&#25968;&#26679;&#26412;&#20998;&#24067;&#30340;&#36793;&#30028;&#38468;&#36817;&#36880;&#28176;&#20943;&#23567;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;BorderLine SMOTE&#31574;&#30053;&#30340;&#21512;&#29702;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;SMOTE&#30456;&#20851;&#31574;&#30053;&#65292;&#24182;&#19982;&#20854;&#20182;&#37325;&#26032;&#24179;&#34913;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26368;&#32456;&#21457;&#29616;&#65292;&#22312;&#25968;&#25454;&#38598;&#26497;&#24230;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;SMOTE&#12289;&#25552;&#20986;&#30340;&#26041;&#27861;&#25110;&#27424;&#37319;&#26679;&#31243;&#24207;&#26159;&#26368;&#20339;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SMOTE&#65288;Synthetic Minority Oversampling Technique&#65289;&#26159;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24120;&#29992;&#30340;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28176;&#36827;&#24773;&#20917;&#19979;&#65292;SMOTE&#65288;&#40664;&#35748;&#21442;&#25968;&#65289;&#36890;&#36807;&#31616;&#21333;&#22797;&#21046;&#21407;&#22987;&#23569;&#25968;&#26679;&#26412;&#26469;&#37325;&#26032;&#29983;&#25104;&#21407;&#22987;&#20998;&#24067;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#23569;&#25968;&#26679;&#26412;&#20998;&#24067;&#30340;&#25903;&#25345;&#36793;&#30028;&#38468;&#36817;&#65292;SMOTE&#30340;&#23494;&#24230;&#20250;&#20943;&#23567;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;&#24120;&#35265;&#30340;BorderLine SMOTE&#31574;&#30053;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;SMOTE&#30456;&#20851;&#31574;&#30053;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#29616;&#26377;&#30340;&#37325;&#26032;&#24179;&#34913;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#26377;&#24403;&#25968;&#25454;&#38598;&#26497;&#24230;&#19981;&#24179;&#34913;&#26102;&#25165;&#38656;&#35201;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#12290;&#23545;&#20110;&#36825;&#31181;&#25968;&#25454;&#38598;&#65292;SMOTE&#12289;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25110;&#27424;&#37319;&#26679;&#31243;&#24207;&#26159;&#26368;&#20339;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic Minority Oversampling Technique (SMOTE) is a common rebalancing strategy for handling imbalanced data sets. Asymptotically, we prove that SMOTE (with default parameter) regenerates the original distribution by simply copying the original minority samples. We also prove that SMOTE density vanishes near the boundary of the support of the minority distribution, therefore justifying the common BorderLine SMOTE strategy. Then we introduce two new SMOTE-related strategies, and compare them with state-of-the-art rebalancing procedures. We show that rebalancing strategies are only required when the data set is highly imbalanced. For such data sets, SMOTE, our proposals, or undersampling procedures are the best strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#32479;&#35745;&#27979;&#35797;&#26041;&#27861;&#65288;VAE-AD&#27979;&#35797;&#65289;&#65292;&#36890;&#36807;&#37327;&#21270;&#24322;&#24120;&#21306;&#22495;&#30340;&#21487;&#38752;&#24615;&#65292;&#21487;&#20197;&#25511;&#21046;&#35823;&#26816;&#30340;&#27010;&#29575;&#21040;&#25152;&#26399;&#26395;&#30340;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.03724</link><description>&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#32479;&#35745;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Statistical Test for Anomaly Detections by Variational Auto-Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#32479;&#35745;&#27979;&#35797;&#26041;&#27861;&#65288;VAE-AD&#27979;&#35797;&#65289;&#65292;&#36890;&#36807;&#37327;&#21270;&#24322;&#24120;&#21306;&#22495;&#30340;&#21487;&#38752;&#24615;&#65292;&#21487;&#20197;&#25511;&#21046;&#35823;&#26816;&#30340;&#27010;&#29575;&#21040;&#25152;&#26399;&#26395;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#30340;&#21487;&#38752;&#24615;&#35780;&#20272;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#22522;&#20110;VAE&#30340;AD&#24050;&#32463;&#22312;&#21508;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#31215;&#26497;&#30340;&#30740;&#31350;&#65292;&#20174;&#26041;&#27861;&#24320;&#21457;&#21040;&#24212;&#29992;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#24403;AD&#30340;&#32467;&#26524;&#29992;&#20110;&#39640;&#39118;&#38505;&#30340;&#20915;&#31574;&#26102;&#65292;&#22914;&#21307;&#23398;&#35786;&#26029;&#65292;&#38656;&#35201;&#30830;&#20445;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VAE-AD&#27979;&#35797;&#20316;&#20026;&#22312;&#32479;&#35745;&#26816;&#39564;&#26694;&#26550;&#19979;&#37327;&#21270;&#22522;&#20110;VAE&#30340;AD&#30340;&#32479;&#35745;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;VAE-AD&#27979;&#35797;&#65292;&#21487;&#20197;&#20197;p&#20540;&#30340;&#24418;&#24335;&#37327;&#21270;VAE&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#21306;&#22495;&#30340;&#21487;&#38752;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#22914;&#26524;&#22312;p&#20540;&#20302;&#20110;&#26576;&#20010;&#38408;&#20540;&#26102;&#23459;&#24067;&#20026;&#24322;&#24120;&#65292;&#21017;&#21487;&#20197;&#23558;&#35823;&#26816;&#30340;&#27010;&#29575;&#25511;&#21046;&#22312;&#25152;&#26399;&#26395;&#30340;&#27700;&#24179;&#12290;&#30001;&#20110;VAE-AD&#27979;&#35797;&#26159;&#22522;&#20110;&#19968;&#31181;&#31216;&#20026;&#36873;&#25321;&#24615;&#25512;&#29702;&#30340;&#26032;&#32479;&#35745;&#25512;&#26029;&#26694;&#26550;&#26500;&#24314;&#30340;&#65292;&#20854;&#26377;&#25928;&#24615;&#26159;&#30830;&#20445;&#34987;&#35777;&#26126;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we consider the reliability assessment of anomaly detection (AD) using Variational Autoencoder (VAE). Over the last decade, VAE-based AD has been actively studied in various perspective, from method development to applied research. However, when the results of ADs are used in high-stakes decision-making, such as in medical diagnosis, it is necessary to ensure the reliability of the detected anomalies. In this study, we propose the VAE-AD Test as a method for quantifying the statistical reliability of VAE-based AD within the framework of statistical testing. Using the VAE-AD Test, the reliability of the anomaly regions detected by a VAE can be quantified in the form of p-values. This means that if an anomaly is declared when the p-value is below a certain threshold, it is possible to control the probability of false detection to a desired level. Since the VAE-AD Test is constructed based on a new statistical inference framework called selective inference, its validity is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#38543;&#26426;&#24615;&#30340;&#26080;&#38480;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#26694;&#26550;&#20013;&#25972;&#21512;&#37096;&#20998;&#38543;&#26426;&#24615;&#65292;&#25913;&#21892;&#29616;&#26377;&#26550;&#26500;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#28789;&#27963;&#30340;&#32593;&#32476;&#35774;&#35745;&#37197;&#32622;&#65292;&#21516;&#26102;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03495</link><description>&lt;p&gt;
&#37096;&#20998;&#38543;&#26426;&#30340;&#26080;&#38480;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Partially Stochastic Infinitely Deep Bayesian Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#38543;&#26426;&#24615;&#30340;&#26080;&#38480;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#26694;&#26550;&#20013;&#25972;&#21512;&#37096;&#20998;&#38543;&#26426;&#24615;&#65292;&#25913;&#21892;&#29616;&#26377;&#26550;&#26500;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#28789;&#27963;&#30340;&#32593;&#32476;&#35774;&#35745;&#37197;&#32622;&#65292;&#21516;&#26102;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#38543;&#26426;&#30340;&#26080;&#38480;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#37096;&#20998;&#38543;&#26426;&#24615;&#25972;&#21512;&#21040;&#26080;&#38480;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#30340;&#26032;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#26550;&#26500;&#26088;&#22312;&#25913;&#21892;&#29616;&#26377;&#26550;&#26500;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#38480;&#21046;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#37096;&#20998;&#38543;&#26426;&#24615;&#22312;&#26080;&#38480;&#28145;&#24230;&#26497;&#38480;&#19979;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#20840;&#38543;&#26426;&#24615;&#30340;&#22909;&#22788;&#65292;&#22914;&#40065;&#26834;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#20869;&#23384;&#25928;&#29575;&#65292;&#21516;&#26102;&#25913;&#21892;&#20102;&#23427;&#20204;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#26550;&#26500;&#37197;&#32622;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#35774;&#35745;&#30340;&#28789;&#27963;&#24615;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#26435;&#37325;&#21010;&#20998;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#30830;&#31435;&#25105;&#20204;&#30340;&#32593;&#32476;&#23478;&#26063;&#31526;&#21512;&#36890;&#29992;&#26465;&#20214;&#20998;&#24067;&#36817;&#20284;&#22120;&#30340;&#25968;&#23398;&#20445;&#35777;&#65292;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Partially Stochastic Infinitely Deep Bayesian Neural Networks, a novel family of architectures that integrates partial stochasticity into the framework of infinitely deep neural networks. Our new class of architectures is designed to improve the limitations of existing architectures around computational efficiency at training and inference time. To do this, we leverage the advantages of partial stochasticity in the infinite-depth limit which include the benefits of full stochasticity e.g. robustness, uncertainty quantification, and memory efficiency, whilst improving their limitations around computational efficiency at training and inference time. We present a variety of architectural configurations, offering flexibility in network design including different methods for weight partition. We also provide mathematical guarantees on the expressivity of our models by establishing that our network family qualifies as Universal Conditional Distribution Approximators
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65288;DSpodFL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#32479;&#19968;&#20102;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#12289;&#38543;&#26426;&#38386;&#35805;&#65288;RG&#65289;&#21644;&#20998;&#25955;&#24335;&#32852;&#37030;&#24179;&#22343;&#65288;DFedAvg&#65289;&#31561;&#33879;&#21517;&#30340;&#20998;&#25955;&#20248;&#21270;&#26041;&#27861;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;DSpodFL&#33021;&#22815;&#22312;&#26356;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#36798;&#21040;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#19982;&#26368;&#20339;&#24615;&#24046;&#36317;&#30340;&#21305;&#37197;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03448</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65306;&#20855;&#26377;&#24191;&#20041;&#25910;&#25947;&#20445;&#35777;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decentralized Sporadic Federated Learning: A Unified Methodology with Generalized Convergence Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65288;DSpodFL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#32479;&#19968;&#20102;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#12289;&#38543;&#26426;&#38386;&#35805;&#65288;RG&#65289;&#21644;&#20998;&#25955;&#24335;&#32852;&#37030;&#24179;&#22343;&#65288;DFedAvg&#65289;&#31561;&#33879;&#21517;&#30340;&#20998;&#25955;&#20248;&#21270;&#26041;&#27861;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;DSpodFL&#33021;&#22815;&#22312;&#26356;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#36798;&#21040;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#19982;&#26368;&#20339;&#24615;&#24046;&#36317;&#30340;&#21305;&#37197;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#36817;&#26469;&#21463;&#21040;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#28085;&#30422;&#20102;&#27169;&#22411;&#26356;&#26032;&#21644;&#27169;&#22411;&#32858;&#21512;&#36825;&#20004;&#20010;&#20851;&#38190;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#37117;&#30001;&#23458;&#25143;&#31471;&#36827;&#34892;&#30340;&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65288;DSpodFL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;DFL&#26041;&#27861;&#65292;&#23427;&#22312;&#36825;&#20004;&#20010;&#36807;&#31243;&#20013;&#24191;&#20041;&#21270;&#20102;&#38388;&#27463;&#24615;&#30340;&#27010;&#24565;&#65292;&#24314;&#27169;&#20102;&#22312;&#23454;&#38469;DFL&#35774;&#32622;&#20013;&#20986;&#29616;&#30340;&#19981;&#21516;&#24418;&#24335;&#30340;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;DSpodFL&#23558;&#35768;&#22810;&#30528;&#21517;&#30340;&#20998;&#25955;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#65292;&#38543;&#26426;&#38386;&#35805;&#65288;RG&#65289;&#21644;&#20998;&#25955;&#24335;&#32852;&#37030;&#24179;&#22343;&#65288;DFedAvg&#65289;&#65292;&#32479;&#19968;&#21040;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#19979;&#12290;&#25105;&#20204;&#23545;DSpodFL&#30340;&#25910;&#25947;&#34892;&#20026;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#26174;&#31034;&#20986;&#21487;&#20197;&#22312;&#26356;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#65292;&#23558;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#19982;&#26377;&#38480;&#30340;&#26368;&#20339;&#24615;&#24046;&#36317;&#30456;&#21305;&#37197;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Learning (DFL) has received significant recent research attention, capturing settings where both model updates and model aggregations -- the two key FL processes -- are conducted by the clients. In this work, we propose Decentralized Sporadic Federated Learning ($\texttt{DSpodFL}$), a DFL methodology which generalizes the notion of sporadicity in both of these processes, modeling the impact of different forms of heterogeneity that manifest in realistic DFL settings. $\texttt{DSpodFL}$ unifies many of the prominent decentralized optimization methods, e.g., distributed gradient descent (DGD), randomized gossip (RG), and decentralized federated averaging (DFedAvg), under a single modeling framework. We analytically characterize the convergence behavior of $\texttt{DSpodFL}$, showing, among other insights, that we can match a geometric convergence rate to a finite optimality gap under more general assumptions than in existing works. Through experiments, we demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26080;&#20154;&#26426;&#36712;&#36857;&#21644;&#29992;&#25143;&#20851;&#32852;&#25351;&#26631;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#19982;&#26080;&#20154;&#26426;&#30340;&#20851;&#32852;&#65292;&#20197;&#26377;&#25928;&#22320;&#26368;&#22823;&#21270;&#22810;&#20010;&#26080;&#20154;&#26426;&#22312;&#21368;&#36733;&#22320;&#38754;&#22522;&#31449;&#30340;&#25968;&#25454;&#27969;&#37327;&#26041;&#38754;&#30340;&#21033;&#29992;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02957</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21327;&#21161;&#26080;&#20154;&#26426;&#21368;&#36733;&#34562;&#31389;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning for Offloading Cellular Communications with Cooperating UAVs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26080;&#20154;&#26426;&#36712;&#36857;&#21644;&#29992;&#25143;&#20851;&#32852;&#25351;&#26631;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#19982;&#26080;&#20154;&#26426;&#30340;&#20851;&#32852;&#65292;&#20197;&#26377;&#25928;&#22320;&#26368;&#22823;&#21270;&#22810;&#20010;&#26080;&#20154;&#26426;&#22312;&#21368;&#36733;&#22320;&#38754;&#22522;&#31449;&#30340;&#25968;&#25454;&#27969;&#37327;&#26041;&#38754;&#30340;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#32972;&#26223;&#19979;&#65292;&#26377;&#25928;&#30340;&#35299;&#20915;&#22320;&#38754;&#34562;&#31389;&#32593;&#32476;&#20013;&#30340;&#26234;&#33021;&#25968;&#25454;&#25910;&#38598;&#26041;&#26696;&#33267;&#20851;&#37325;&#35201;&#12290;&#22320;&#38754;&#22522;&#31449;&#30340;&#26377;&#38480;&#39057;&#35889;&#21644;&#35206;&#30422;&#33539;&#22260;&#32473;&#32593;&#32476;&#29992;&#25143;&#30340;&#25968;&#25454;&#29575;&#38656;&#27714;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26080;&#20154;&#26426;&#20197;&#20854;&#39640;&#25935;&#25463;&#24615;&#12289;&#31227;&#21160;&#24615;&#21644;&#28789;&#27963;&#24615;&#32780;&#38395;&#21517;&#65292;&#20026;&#21368;&#36733;&#22320;&#38754;&#22522;&#31449;&#30340;&#25968;&#25454;&#27969;&#37327;&#25552;&#20379;&#20102;&#21478;&#19968;&#31181;&#25163;&#27573;&#65292;&#25104;&#20026;&#39069;&#22806;&#30340;&#25509;&#20837;&#28857;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#26368;&#22823;&#21270;&#22810;&#20010;&#26080;&#20154;&#26426;&#22312;&#21368;&#36733;&#22320;&#38754;&#22522;&#31449;&#30340;&#25968;&#25454;&#27969;&#37327;&#26041;&#38754;&#30340;&#21033;&#29992;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37325;&#28857;&#26159;&#22312;&#36136;&#37327;&#20445;&#35777;&#32422;&#26463;&#19979;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26080;&#20154;&#26426;&#36712;&#36857;&#21644;&#29992;&#25143;&#20851;&#32852;&#25351;&#26631;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#19982;&#26080;&#20154;&#26426;&#30340;&#20851;&#32852;&#12290;&#30001;&#20110;&#25152;&#21046;&#23450;&#30340;&#26080;&#20154;&#26426;&#25511;&#21046;&#38382;&#39064;&#26159;&#38750;&#20984;&#21644;&#32452;&#21512;&#30340;&#65292;&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#27599;&#20010;&#26080;&#20154;&#26426;&#20197;&#38750;&#21512;&#20316;&#26041;&#24335;&#23547;&#27714;&#20174;&#29615;&#22659;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#26469;&#20248;&#21270;&#33258;&#24049;&#30340;&#31574;&#30053;.
&lt;/p&gt;
&lt;p&gt;
Effective solutions for intelligent data collection in terrestrial cellular networks are crucial, especially in the context of Internet of Things applications. The limited spectrum and coverage area of terrestrial base stations pose challenges in meeting the escalating data rate demands of network users. Unmanned aerial vehicles, known for their high agility, mobility, and flexibility, present an alternative means to offload data traffic from terrestrial BSs, serving as additional access points. This paper introduces a novel approach to efficiently maximize the utilization of multiple UAVs for data traffic offloading from terrestrial BSs. Specifically, the focus is on maximizing user association with UAVs by jointly optimizing UAV trajectories and users association indicators under quality of service constraints. Since, the formulated UAVs control problem is nonconvex and combinatorial, this study leverages the multi agent reinforcement learning framework. In this framework, each UAV a
&lt;/p&gt;</description></item><item><title>PowerGraph&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30005;&#32593;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#30005;&#21147;&#32593;&#26684;&#26029;&#30005;&#30340;&#22312;&#32447;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.02827</link><description>&lt;p&gt;
PowerGraph: &#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30005;&#32593;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PowerGraph: A power grid benchmark dataset for graph neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02827
&lt;/p&gt;
&lt;p&gt;
PowerGraph&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30005;&#32593;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#30005;&#21147;&#32593;&#26684;&#26029;&#30005;&#30340;&#22312;&#32447;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#26377;&#21161;&#20110;&#20351;&#29992;GNN&#65292;&#24182;&#22686;&#24378;GNN&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#30446;&#21069;&#65292;&#31038;&#21306;&#20013;&#32570;&#20047;&#29992;&#20110;GNN&#24212;&#29992;&#30340;&#30005;&#21147;&#32593;&#26684;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#20107;&#23454;&#19978;&#65292;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#27604;&#65292;GNN&#21487;&#20197;&#28508;&#22312;&#22320;&#25429;&#25417;&#21040;&#22797;&#26434;&#30340;&#30005;&#21147;&#32593;&#26684;&#29616;&#35937;&#12290;&#30005;&#21147;&#32593;&#26684;&#26159;&#22797;&#26434;&#30340;&#24037;&#31243;&#32593;&#32476;&#65292;&#22825;&#28982;&#36866;&#21512;&#20110;&#22270;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;GNN&#26377;&#28508;&#21147;&#25429;&#25417;&#21040;&#30005;&#21147;&#32593;&#26684;&#30340;&#34892;&#20026;&#65292;&#32780;&#19981;&#29992;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#32423;&#32852;&#25925;&#38556;&#20107;&#20214;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#23548;&#33268;&#30005;&#21147;&#32593;&#26684;&#26029;&#30005;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#21382;&#21490;&#26029;&#30005;&#25968;&#25454;&#38598;&#31232;&#32570;&#19988;&#19981;&#23436;&#25972;&#12290;&#36890;&#24120;&#36890;&#36807;&#35745;&#31639;&#26114;&#36149;&#30340;&#31163;&#32447;&#32423;&#32852;&#25925;&#38556;&#27169;&#25311;&#26469;&#35780;&#20272;&#33030;&#24369;&#24615;&#21644;&#35782;&#21035;&#20851;&#38190;&#32452;&#20214;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22312;&#32447;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public Graph Neural Networks (GNN) benchmark datasets facilitate the use of GNN and enhance GNN applicability to diverse disciplines. The community currently lacks public datasets of electrical power grids for GNN applications. Indeed, GNNs can potentially capture complex power grid phenomena over alternative machine learning techniques. Power grids are complex engineered networks that are naturally amenable to graph representations. Therefore, GNN have the potential for capturing the behavior of power grids over alternative machine learning techniques. To this aim, we develop a graph dataset for cascading failure events, which are the major cause of blackouts in electric power grids. Historical blackout datasets are scarce and incomplete. The assessment of vulnerability and the identification of critical components are usually conducted via computationally expensive offline simulations of cascading failures. Instead, we propose using machine learning models for the online detection of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;LLMs&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#20005;&#37325;&#38477;&#32423;&#30340;&#38382;&#39064;&#65292;&#31361;&#26174;&#20102;LLMs&#22312;&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#23558;LLMs&#20316;&#20026;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;</title><link>https://arxiv.org/abs/2402.02805</link><description>&lt;p&gt;
&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#20013;&#30340;&#22270;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph-enhanced Large Language Models in Asynchronous Plan Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;LLMs&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#20005;&#37325;&#38477;&#32423;&#30340;&#38382;&#39064;&#65292;&#31361;&#26174;&#20102;LLMs&#22312;&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#23558;LLMs&#20316;&#20026;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39034;&#24207;&#21644;&#24182;&#34892;&#35268;&#21010;&#20197;&#20248;&#21270;&#26102;&#38388;&#25104;&#26412;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#25104;&#21151;&#21527;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#32452;&#20195;&#34920;&#24615;&#30340;&#38381;&#28304;&#21644;&#24320;&#28304;LLMs&#65292;&#21253;&#25324;GPT-4&#21644;LLaMA-2&#65292;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;AsyncHow&#20013;&#65292;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#30340;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34429;&#28982;PLaG&#33021;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#36973;&#21463;&#20005;&#37325;&#38477;&#32423;&#65292;&#31361;&#20986;&#20102;&#21033;&#29992;LLMs&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#30740;&#31350;&#35270;&#20026;&#23558;LLMs&#29992;&#20316;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#30340;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents.
&lt;/p&gt;</description></item><item><title>KS-Lottery&#26159;&#19968;&#31181;&#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#21442;&#25968;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#23884;&#20837;&#23618;&#20013;&#21487;&#20197;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#24494;&#35843;&#20013;&#33719;&#24471;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.02801</link><description>&lt;p&gt;
KS-Lottery: &#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#35777;&#24425;&#31080;
&lt;/p&gt;
&lt;p&gt;
KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02801
&lt;/p&gt;
&lt;p&gt;
KS-Lottery&#26159;&#19968;&#31181;&#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#21442;&#25968;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#23884;&#20837;&#23618;&#20013;&#21487;&#20197;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#24494;&#35843;&#20013;&#33719;&#24471;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24425;&#31080;&#31080;&#35777;&#20551;&#35828;&#35748;&#20026;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#8220;&#20013;&#22870;&#31080;&#8221;&#12290;&#22312;&#24494;&#35843;&#22330;&#26223;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#20013;&#22870;&#31080;&#65311;&#25105;&#20204;&#22914;&#20309;&#25214;&#21040;&#36825;&#26679;&#30340;&#20013;&#22870;&#31080;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KS-Lottery&#65292;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22312;&#22810;&#35821;&#35328;&#24494;&#35843;&#20013;&#39640;&#24230;&#26377;&#25928;&#30340;LLM&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#24494;&#35843;&#21069;&#21518;&#21442;&#25968;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#29702;&#35770;&#35777;&#26126;&#20102;KS-Lottery&#21487;&#20197;&#22312;&#23884;&#20837;&#23618;&#20013;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#65292;&#24494;&#35843;&#36825;&#20123;&#21442;&#25968;&#21487;&#20197;&#20445;&#35777;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#23558;KS-Lottery&#19982;&#20854;&#20182;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KS-Lottery&#25214;&#21040;&#20102;&#19968;&#20010;&#26356;&#23567;&#30340;&#21442;&#25968;&#38598;&#26469;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#36798;&#21040;&#20102;&#19982;&#20840;&#38754;&#24494;&#35843;LLM&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;18&#20010;&#26631;&#35760;&#30340;&#23884;&#20837;&#23618;
&lt;/p&gt;
&lt;p&gt;
The lottery ticket hypothesis posits the existence of ``winning tickets'' within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning. Comparing KS-Lottery with other parameter-efficient tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for fine-tuning while achieving the comparable performance as full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens' embeddin
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#26377;&#28508;&#21147;&#39072;&#35206;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#25552;&#21319;&#20915;&#31574;&#25928;&#29575;&#65292;&#25512;&#21160;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26234;&#33021;&#30340;&#26222;&#21450;&#21270;&#12290;&#36825;&#31181;&#36827;&#23637;&#21487;&#20197;&#24102;&#26469;&#27169;&#24577;&#20999;&#25442;&#21644;&#26102;&#38388;&#24207;&#21015;&#38382;&#31572;&#31561;&#22810;&#31181;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02713</link><description>&lt;p&gt;
&#19968;&#31687;&#20301;&#32622;&#35770;&#25991;: &#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26377;&#20160;&#20040;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
Position Paper: What Can Large Language Models Tell Us about Time Series Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02713
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26377;&#28508;&#21147;&#39072;&#35206;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#25552;&#21319;&#20915;&#31574;&#25928;&#29575;&#65292;&#25512;&#21160;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26234;&#33021;&#30340;&#26222;&#21450;&#21270;&#12290;&#36825;&#31181;&#36827;&#23637;&#21487;&#20197;&#24102;&#26469;&#27169;&#24577;&#20999;&#25442;&#21644;&#26102;&#38388;&#24207;&#21015;&#38382;&#31572;&#31561;&#22810;&#31181;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#23545;&#20110;&#29702;&#35299;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#21644;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20855;&#22791;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#33021;&#21147;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#21457;&#23637;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#30446;&#21069;&#22823;&#37096;&#20998;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#39046;&#22495;&#30693;&#35782;&#21644;&#22823;&#37327;&#27169;&#22411;&#35843;&#25972;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#39044;&#27979;&#20219;&#21153;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#30446;&#21069;&#30340;LLM&#20855;&#26377;&#39072;&#35206;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#39640;&#25928;&#30340;&#20915;&#31574;&#21644;&#25512;&#36827;&#21521;&#26356;&#26222;&#36866;&#24418;&#24335;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26234;&#33021;&#21457;&#23637;&#12290;&#36825;&#31181;&#36827;&#27493;&#21487;&#20197;&#25171;&#24320;&#21508;&#31181;&#21487;&#33021;&#24615;&#65292;&#21253;&#25324;&#27169;&#24577;&#20999;&#25442;&#21644;&#26102;&#38388;&#24207;&#21015;&#38382;&#31572;&#12290;&#25105;&#20204;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#35748;&#35782;&#21040;LLM&#22312;&#25512;&#36827;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#23545;&#36825;&#20123;&#30456;&#20851;&#24037;&#20316;&#30340;&#20449;&#20219;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is essential for comprehending the complexities inherent in various real-world systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including modality switching and time series question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.02619</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Increasing Trust in Language Models through the Reuse of Verified Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#32463;&#24120;&#24573;&#30053;&#32597;&#35265;&#30340;&#36793;&#30028;&#24773;&#20917;&#65292;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#21487;&#20449;&#24230;&#26631;&#20934;&#65292;&#21363;&#20219;&#21153;&#31639;&#27861;&#21644;&#30005;&#36335;&#23454;&#29616;&#24517;&#39035;&#32463;&#36807;&#39564;&#35777;&#65292;&#32771;&#34385;&#21040;&#36793;&#30028;&#24773;&#20917;&#65292;&#24182;&#19988;&#27809;&#26377;&#24050;&#30693;&#30340;&#25925;&#38556;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#26469;&#26500;&#24314;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#35757;&#32451;&#20986;&#28385;&#36275;&#36825;&#19968;&#26631;&#20934;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#20102;&#23436;&#20840;&#39564;&#35777;&#12290;&#20026;&#20102;&#23637;&#31034;&#32463;&#36807;&#39564;&#35777;&#30340;&#27169;&#22359;&#30340;&#37325;&#22797;&#20351;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#22909;&#30340;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#25554;&#20837;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#24182;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#21516;&#26102;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#26356;&#22797;&#26434;&#30340;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#32463;&#36807;&#39564;&#35777;&#30340;&#20219;&#21153;&#27169;&#22359;&#25554;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#21033;&#29992;&#27169;&#22411;&#30340;&#37325;&#22797;&#20351;&#29992;&#26469;&#25552;&#39640;&#21487;&#39564;&#35777;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;2-WL&#27979;&#35797;&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#28857;&#20113;&#20013;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20462;&#25913;&#30340;PPGN&#26550;&#26500;&#65292;&#20197;&#33719;&#24471;&#19968;&#20010;&#21487;&#36817;&#20284;&#25152;&#26377;&#36830;&#32493;&#31561;&#21464;&#20989;&#25968;&#30340;&#36890;&#29992;&#31561;&#21464;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.02484</link><description>&lt;p&gt;
Weisfeiler Leman&#29992;&#20110;&#27431;&#20960;&#37324;&#24471;&#31561;&#21464;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Weisfeiler Leman for Euclidean Equivariant Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;2-WL&#27979;&#35797;&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#28857;&#20113;&#20013;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20462;&#25913;&#30340;PPGN&#26550;&#26500;&#65292;&#20197;&#33719;&#24471;&#19968;&#20010;&#21487;&#36817;&#20284;&#25152;&#26377;&#36830;&#32493;&#31561;&#21464;&#20989;&#25968;&#30340;&#36890;&#29992;&#31561;&#21464;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
k-Weisfeiler Leman (k-WL)&#22270;&#21516;&#26500;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;&#26159;&#35780;&#20272;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#34920;&#36798;&#33021;&#21147;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#35777;&#26126;&#20102;2-WL&#27979;&#35797;&#22312;&#32534;&#30721;3D&#28857;&#20113;&#25968;&#25454;&#30340;&#21152;&#26435;&#22270;&#19978;&#26159;&#23436;&#22791;&#30340;&#12290;&#22240;&#27492;&#65292;&#20855;&#26377;&#19982;2-WL&#27979;&#35797;&#31561;&#20215;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;GNNs&#21487;&#20197;&#34987;&#35777;&#26126;&#22312;&#28857;&#20113;&#19978;&#26159;&#36890;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#32467;&#26524;&#20165;&#38480;&#20110;&#28857;&#20113;&#19978;&#30340;&#19981;&#21464;&#36830;&#32493;&#20989;&#25968;&#12290;&#26412;&#25991;&#36890;&#36807;&#19977;&#20010;&#26041;&#38754;&#23545;&#36825;&#19968;&#32467;&#26524;&#36827;&#34892;&#20102;&#25193;&#23637;:&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;2-WL&#27979;&#35797;&#21487;&#20197;&#25193;&#23637;&#21040;&#21253;&#25324;&#20301;&#32622;&#21644;&#36895;&#24230;&#30340;&#28857;&#20113;&#65292;&#36825;&#22312;&#24212;&#29992;&#20013;&#32463;&#24120;&#36935;&#21040;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PPGN (Maron&#31561;&#20154;&#65292;2019)&#21487;&#20197;&#22312;&#20302;&#22797;&#26434;&#24230;&#19979;&#22312;&#25152;&#26377;&#28857;&#20113;&#19978;&#19968;&#33268;&#22320;&#27169;&#25311;2-WL&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#36825;&#20010;PPGN&#26550;&#26500;&#30340;&#31616;&#21333;&#20462;&#25913;&#21487;&#20197;&#29992;&#26469;&#33719;&#24471;&#19968;&#20010;&#21487;&#36817;&#20284;&#25152;&#26377;&#36830;&#32493;&#31561;&#21464;&#20989;&#25968;&#30340;&#36890;&#29992;&#31561;&#21464;&#26550;&#26500;&#12290;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
The $k$-Weifeiler-Leman ($k$-WL) graph isomorphism test hierarchy is a common method for assessing the expressive power of graph neural networks (GNNs). Recently, the $2$-WL test was proven to be complete on weighted graphs which encode $3\mathrm{D}$ point cloud data. Consequently, GNNs whose expressive power is equivalent to the $2$-WL test are provably universal on point clouds. Yet, this result is limited to invariant continuous functions on point clouds.   In this paper we extend this result in three ways: Firstly, we show that $2$-WL tests can be extended to point clouds which include both positions and velocity, a scenario often encountered in applications. Secondly, we show that PPGN (Maron et al., 2019) can simulate $2$-WL uniformly on all point clouds with low complexity. Finally, we show that a simple modification of this PPGN architecture can be used to obtain a universal equivariant architecture that can approximate all continuous equivariant functions uniformly.   Building
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GPTN-SS&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#65292;&#24182;&#22312;&#25628;&#32034;&#39640;&#36136;&#37327;&#30340;TN&#32467;&#26500;&#26041;&#38754;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02456</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21457;&#29616;&#26356;&#26377;&#25928;&#30340;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discovering More Effective Tensor Network Structure Search Algorithms via Large Language Models (LLMs)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02456
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GPTN-SS&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#65292;&#24182;&#22312;&#25628;&#32034;&#39640;&#36136;&#37327;&#30340;TN&#32467;&#26500;&#26041;&#38754;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#65288;TN-SS&#65289;&#26088;&#22312;&#25628;&#32034;&#36866;&#21512;&#34920;&#31034;&#39640;&#32500;&#38382;&#39064;&#30340;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#32467;&#26500;&#65292;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;TN&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#29616;&#26377;&#31639;&#27861;&#25214;&#21040;&#28385;&#24847;&#30340;TN&#32467;&#26500;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24320;&#21457;&#26356;&#26377;&#25928;&#30340;&#31639;&#27861;&#24182;&#36991;&#20813;&#20154;&#21147;&#23494;&#38598;&#22411;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#26469;&#33258;&#21160;&#35774;&#35745;TN-SS&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;GPTN-SS&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#22522;&#20110;LLM&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#20197;&#31867;&#20284;&#36827;&#21270;&#30340;&#26041;&#24335;&#36816;&#34892;&#12290;&#20174;&#30495;&#23454;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPTN-SS&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#26041;&#27861;&#33719;&#24471;&#30340;&#35265;&#35299;&#65292;&#24320;&#21457;&#20986;&#26356;&#22909;&#22320;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#20851;&#31995;&#30340;&#26032;&#22411;TN-SS&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#25628;&#32034;&#39640;&#36136;&#37327;TN&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor network structure search (TN-SS), aiming at searching for suitable tensor network (TN) structures in representing high-dimensional problems, largely promotes the efficacy of TN in various machine learning applications. Nonetheless, finding a satisfactory TN structure using existing algorithms remains challenging. To develop more effective algorithms and avoid the human labor-intensive development process, we explore the knowledge embedded in large language models (LLMs) for the automatic design of TN-SS algorithms. Our approach, dubbed GPTN-SS, leverages an elaborate crafting LLM-based prompting system that operates in an evolutionary-like manner. The experimental results, derived from real-world data, demonstrate that GPTN-SS can effectively leverage the insights gained from existing methods to develop novel TN-SS algorithms that achieve a better balance between exploration and exploitation. These algorithms exhibit superior performance in searching the high-quality TN structur
&lt;/p&gt;</description></item><item><title>Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02416</link><description>&lt;p&gt;
Aligner: &#36890;&#36807;&#24369;&#21040;&#24378;&#26657;&#27491;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02416
&lt;/p&gt;
&lt;p&gt;
Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#23545;&#40784;&#30340;&#21162;&#21147;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20027;&#35201;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#12289;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24037;&#31243;&#20197;&#21450;&#37325;&#35201;&#30340;&#26159;&#65292;&#38656;&#35201;&#35775;&#38382;LLM&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#40784;&#33539;&#24335;Aligner&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#23545;&#40784;&#21644;&#26410;&#23545;&#40784;&#31572;&#26696;&#20043;&#38388;&#30340;&#26657;&#27491;&#27531;&#24046;&#26469;&#32469;&#36807;&#25972;&#20010;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;Aligner&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#22238;&#24402;seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#26597;&#35810;-&#31572;&#26696;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#40784;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#23569;&#12290;&#20854;&#27425;&#65292;Aligner&#23454;&#29616;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;Aligner&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;&#31532;&#19977;&#65292;Aligner&#20316;&#20026;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
&lt;/p&gt;</description></item><item><title>Transolver&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;Transformer&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#27880;&#24847;&#21147;&#21644;&#28789;&#27963;&#24418;&#29366;&#30340;&#29255;&#27573;&#26469;&#23398;&#20064;&#31163;&#25955;&#21270;&#20960;&#20309;&#24418;&#29366;&#32972;&#21518;&#38544;&#34255;&#30340;&#20869;&#22312;&#29289;&#29702;&#29366;&#24577;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#19979;&#30340;&#22797;&#26434;&#29289;&#29702;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20855;&#22791;&#20869;&#29983;&#20960;&#20309;&#29983;&#25104;&#33021;&#21147;&#30340;&#27714;&#35299;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.02366</link><description>&lt;p&gt;
Transolver&#65306;&#19968;&#31181;&#29992;&#20110;&#19968;&#33324;&#20960;&#20309;&#20307;&#19978;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#24555;&#36895;Transformer&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transolver: A Fast Transformer Solver for PDEs on General Geometries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02366
&lt;/p&gt;
&lt;p&gt;
Transolver&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;Transformer&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#27880;&#24847;&#21147;&#21644;&#28789;&#27963;&#24418;&#29366;&#30340;&#29255;&#27573;&#26469;&#23398;&#20064;&#31163;&#25955;&#21270;&#20960;&#20309;&#24418;&#29366;&#32972;&#21518;&#38544;&#34255;&#30340;&#20869;&#22312;&#29289;&#29702;&#29366;&#24577;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#19979;&#30340;&#22797;&#26434;&#29289;&#29702;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20855;&#22791;&#20869;&#29983;&#20960;&#20309;&#29983;&#25104;&#33021;&#21147;&#30340;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#22312;&#21508;&#20010;&#39046;&#22495;&#23454;&#29616;&#20102;&#24456;&#22810;&#37324;&#31243;&#30865;&#24335;&#30340;&#25104;&#23601;&#65292;&#26368;&#36817;&#24320;&#22987;&#24212;&#29992;&#20110;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;PDEs&#36890;&#24120;&#34987;&#31163;&#25955;&#21270;&#25104;&#20855;&#26377;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#30340;&#22823;&#35268;&#27169;&#32593;&#26684;&#65292;&#23545;Transformer&#26469;&#35828;&#30452;&#25509;&#20174;&#22823;&#37327;&#21333;&#20010;&#28857;&#20013;&#25429;&#25417;&#22797;&#26434;&#30340;&#29289;&#29702;&#30456;&#20851;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#36229;&#36234;&#32932;&#27973;&#32780;&#31528;&#37325;&#30340;&#32593;&#26684;&#65292;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#26356;&#22522;&#30784;&#30340;&#24605;&#24819;&#25552;&#20986;&#20102;Transolver&#65292;&#21363;&#23398;&#20064;&#31163;&#25955;&#21270;&#20960;&#20309;&#24418;&#29366;&#32972;&#21518;&#38544;&#34255;&#30340;&#20869;&#22312;&#29289;&#29702;&#29366;&#24577;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23558;&#31163;&#25955;&#21270;&#30340;&#22495;&#33258;&#36866;&#24212;&#22320;&#21010;&#20998;&#20026;&#19968;&#31995;&#21015;&#21487;&#23398;&#20064;&#30340;&#28789;&#27963;&#24418;&#29366;&#30340;&#29255;&#27573;&#65292;&#20855;&#26377;&#30456;&#20284;&#29289;&#29702;&#29366;&#24577;&#30340;&#32593;&#26684;&#28857;&#23558;&#34987;&#24402;&#23646;&#20110;&#21516;&#19968;&#20010;&#29255;&#27573;&#12290;&#36890;&#36807;&#35745;&#31639;&#20174;&#29255;&#27573;&#32534;&#30721;&#30340;&#20855;&#26377;&#29289;&#29702;&#24847;&#35782;&#30340;&#35760;&#21495;&#30340;&#27880;&#24847;&#21147;&#65292;Transovler&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#19979;&#30340;&#22797;&#26434;&#29289;&#29702;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#20351;&#27714;&#35299;&#22120;&#20855;&#22791;&#20869;&#29983;&#20960;&#20309;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have empowered many milestones across various fields and have recently been applied to solve partial differential equations (PDEs). However, since PDEs are typically discretized into large-scale meshes with complex geometries, it is challenging for Transformers to capture intricate physical correlations directly from massive individual points. Going beyond superficial and unwieldy meshes, we present Transolver based on a more foundational idea, which is learning intrinsic physical states hidden behind discretized geometries. Specifically, we propose a new Physics-Attention to adaptively split the discretized domain into a series of learnable slices of flexible shapes, where mesh points under similar physical states will be ascribed to the same slice. By calculating attention to physics-aware tokens encoded from slices, Transovler can effectively capture intricate physical correlations under complex geometrics, which also empowers the solver with endogenetic geometry-genera
&lt;/p&gt;</description></item><item><title>&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#24212;&#35813;&#26159;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#29702;&#35770;&#65292;&#20174;&#26356;&#23436;&#25972;&#30340;&#35282;&#24230;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02287</link><description>&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#30784;&#30340;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Future Directions in Foundations of Graph Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02287
&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#24212;&#35813;&#26159;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#29702;&#35770;&#65292;&#20174;&#26356;&#23436;&#25972;&#30340;&#35282;&#24230;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#25968;&#25454;&#22312;&#19981;&#21516;&#23398;&#31185;&#65288;&#20174;&#29983;&#21629;&#31185;&#23398;&#21040;&#31038;&#20250;&#31185;&#23398;&#21644;&#24037;&#31243;&#31185;&#23398;&#65289;&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#24341;&#36215;&#20102;&#20154;&#20204;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#23613;&#31649;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#23545;GNNs&#24615;&#36136;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#38750;&#24120;&#19981;&#23436;&#25972;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#21457;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#38416;&#26126;GNNs&#31895;&#31890;&#24230;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#65292;&#20027;&#35201;&#37319;&#29992;&#32452;&#21512;&#25216;&#24039;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#19982;&#23454;&#36341;&#24182;&#19981;&#23436;&#20840;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#25216;&#26415;&#35757;&#32451;GNNs&#26102;&#65292;&#23545;GNNs&#30340;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#31687;&#23450;&#20301;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#38656;&#35201;&#23558;&#27880;&#24847;&#21147;&#36716;&#31227;&#21040;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#19978;&#26469;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#30340;&#30456;&#20114;&#20851;&#31995;&#30340;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning on graphs, especially using graph neural networks (GNNs), has seen a surge in interest due to the wide availability of graph data across a broad spectrum of disciplines, from life to social and engineering sciences. Despite their practical success, our theoretical understanding of the properties of GNNs remains highly incomplete. Recent theoretical advancements primarily focus on elucidating the coarse-grained expressive power of GNNs, predominantly employing combinatorial techniques. However, these studies do not perfectly align with practice, particularly in understanding the generalization behavior of GNNs when trained with stochastic first-order optimization techniques. In this position paper, we argue that the graph machine learning community needs to shift its attention to developing a more balanced theory of graph machine learning, focusing on a more thorough understanding of the interplay of expressive power, generalization, and optimization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoPreFL&#30340;&#21327;&#20316;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21487;&#36866;&#24212;&#20219;&#20309;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#39640;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#25552;&#20379;&#21487;&#38752;&#30340;&#21021;&#22987;&#21270;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02225</link><description>&lt;p&gt;
&#37325;&#24605;&#20986;&#21457;&#28857;&#65306;&#36890;&#36807;&#21327;&#20316;&#39044;&#35757;&#32451;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Starting Point: Enhancing Performance and Fairness of Federated Learning via Collaborative Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoPreFL&#30340;&#21327;&#20316;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21487;&#36866;&#24212;&#20219;&#20309;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#39640;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#25552;&#20379;&#21487;&#38752;&#30340;&#21021;&#22987;&#21270;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#35757;&#32451;&#20174;&#19968;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23454;&#35777;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#20026;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#26377;&#30410;&#30340;&#21021;&#22987;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#39044;&#35757;&#32451;&#26041;&#27861;CoPreFL&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#35774;&#35745;&#19968;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20026;&#20219;&#20309;&#19979;&#28216;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31639;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#27169;&#20223;&#19979;&#28216;&#20998;&#24067;&#24335;&#22330;&#26223;&#30340;&#20803;&#23398;&#20064;&#36807;&#31243;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#20219;&#20309;&#26410;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#12290;CoPreFL&#30340;&#39044;&#35757;&#32451;&#20248;&#21270;&#36807;&#31243;&#20063;&#22312;&#24179;&#22343;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#26088;&#22312;&#36890;&#36807;&#26234;&#33021;&#21021;&#22987;&#21270;&#26469;&#35299;&#20915;&#19979;&#28216;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#31454;&#20105;&#25361;&#25112;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#20026;&#20219;&#20309;&#26410;&#30693;&#30340;&#19979;&#28216;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#21021;&#22987;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing federated learning (FL) methodologies have assumed training begins from a randomly initialized model. Recently, several studies have empirically demonstrated that leveraging a pre-trained model can offer advantageous initializations for FL. In this paper, we propose a collaborative pre-training approach, CoPreFL, which strategically designs a pre-trained model to serve as a good initialization for any downstream FL task. The key idea of our pre-training algorithm is a meta-learning procedure which mimics downstream distributed scenarios, enabling it to adapt to any unforeseen FL task. CoPreFL's pre-training optimization procedure also strikes a balance between average performance and fairness, with the aim of addressing these competing challenges in downstream FL tasks through intelligent initializations. Extensive experimental results validate that our pre-training method provides a robust initialization for any unseen downstream FL task, resulting in enhanced average pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26080;&#38656;&#37325;&#35757;&#32451;&#30340;&#20256;&#25773;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21518;&#39564;&#21327;&#26041;&#24046;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22024;&#26434;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#26681;&#25454;&#26368;&#36817;&#30340;&#26041;&#27861;&#31561;&#20215;&#20110;&#23545;&#32473;&#23450;&#25193;&#25955;&#22122;&#22768;&#22270;&#20687;&#30340;&#24178;&#20928;&#22270;&#20687;&#30340;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#36827;&#34892;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#36817;&#20284;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#21363;&#25554;&#21363;&#29992;&#30340;&#21518;&#39564;&#21327;&#26041;&#24046;&#20248;&#21270;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26368;&#20248;&#21518;&#39564;&#21327;&#26041;&#24046;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#20004;&#31181;&#26041;&#27861;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21033;&#29992;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#21453;&#21521;&#21327;&#26041;&#24046;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02149</link><description>&lt;p&gt;
&#25913;&#36827;&#26080;&#38656;&#37325;&#35757;&#32451;&#30340;&#20256;&#25773;&#27169;&#22411;&#29992;&#20110;&#36870;&#38382;&#39064;&#65292;&#20351;&#29992;&#26368;&#20248;&#21518;&#39564;&#21327;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
Improving Diffusion Models for Inverse Problems Using Optimal Posterior Covariance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26080;&#38656;&#37325;&#35757;&#32451;&#30340;&#20256;&#25773;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21518;&#39564;&#21327;&#26041;&#24046;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22024;&#26434;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#26681;&#25454;&#26368;&#36817;&#30340;&#26041;&#27861;&#31561;&#20215;&#20110;&#23545;&#32473;&#23450;&#25193;&#25955;&#22122;&#22768;&#22270;&#20687;&#30340;&#24178;&#20928;&#22270;&#20687;&#30340;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#36827;&#34892;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#36817;&#20284;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#21363;&#25554;&#21363;&#29992;&#30340;&#21518;&#39564;&#21327;&#26041;&#24046;&#20248;&#21270;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26368;&#20248;&#21518;&#39564;&#21327;&#26041;&#24046;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#20004;&#31181;&#26041;&#27861;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21033;&#29992;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#21453;&#21521;&#21327;&#26041;&#24046;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20256;&#25773;&#27169;&#22411;&#20026;&#22024;&#26434;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#20026;&#29305;&#23450;&#30340;&#36870;&#38382;&#39064;&#37325;&#26032;&#35757;&#32451;&#12290;&#26412;&#25991;&#20174;&#26465;&#20214;&#25277;&#26679;&#30340;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#36817;&#20284;&#21518;&#39564;&#22343;&#20540;&#30340;&#35282;&#24230;&#65292;&#25552;&#20986;&#20102;&#23545;&#29616;&#26377;&#38646;&#26679;&#26412;&#26041;&#27861;&#30340;&#31532;&#19968;&#20010;&#32479;&#19968;&#35299;&#37322;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#26368;&#36817;&#30340;&#26041;&#27861;&#31561;&#20215;&#20110;&#23545;&#32473;&#23450;&#25193;&#25955;&#22122;&#22768;&#22270;&#20687;&#30340;&#24178;&#20928;&#22270;&#20687;&#30340;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#36827;&#34892;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#36817;&#20284;&#65292;&#21807;&#19968;&#30340;&#24046;&#21035;&#26159;&#21508;&#21521;&#21516;&#24615;&#21518;&#39564;&#21327;&#26041;&#24046;&#30340;&#25163;&#24037;&#35774;&#35745;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#36890;&#29992;&#21363;&#25554;&#21363;&#29992;&#21518;&#39564;&#21327;&#26041;&#24046;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#26368;&#36817;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26368;&#20248;&#21518;&#39564;&#21327;&#26041;&#24046;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#20004;&#31181;&#26041;&#27861;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21033;&#29992;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#21453;&#21521;&#21327;&#26041;&#24046;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent diffusion models provide a promising zero-shot solution to noisy linear inverse problems without retraining for specific inverse problems. In this paper, we propose the first unified interpretation for existing zero-shot methods from the perspective of approximating the conditional posterior mean for the reverse diffusion process of conditional sampling. We reveal that recent methods are equivalent to making isotropic Gaussian approximations to intractable posterior distributions over clean images given diffused noisy images, with the only difference in the handcrafted design of isotropic posterior covariances. Inspired by this finding, we propose a general plug-and-play posterior covariance optimization based on maximum likelihood estimation to improve recent methods. To achieve optimal posterior covariance without retraining, we provide general solutions based on two approaches specifically designed to leverage pre-trained models with and without reverse covariances. Experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;X-CBA&#30340;&#26032;&#39062;&#21487;&#35299;&#37322;&#22411;IDS&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#20102;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#26356;&#24191;&#27867;&#30340;&#27969;&#37327;&#25968;&#25454;&#65292;&#21253;&#25324;&#36793;&#23646;&#24615;&#65292;&#20197;&#22788;&#29702;&#32593;&#32476;&#23041;&#32961;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.00839</link><description>&lt;p&gt;
X-CBA: &#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;CatBoosted Anomal-E&#29992;&#20110;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;X-CBA&#30340;&#26032;&#39062;&#21487;&#35299;&#37322;&#22411;IDS&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#20102;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#26356;&#24191;&#27867;&#30340;&#27969;&#37327;&#25968;&#25454;&#65292;&#21253;&#25324;&#36793;&#23646;&#24615;&#65292;&#20197;&#22788;&#29702;&#32593;&#32476;&#23041;&#32961;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#23041;&#32961;&#26085;&#30410;&#22797;&#26434;&#30340;&#26102;&#20195;&#65292;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#30340;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20026;&#35782;&#21035;&#35745;&#31639;&#26426;&#32593;&#32476;&#20013;&#30340;&#25915;&#20987;&#21644;&#24322;&#24120;&#25552;&#20379;&#20102;&#39640;&#25928;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;IDS&#20013;&#20351;&#29992;ML&#21644;DL&#27169;&#22411;&#23548;&#33268;&#20102;&#20449;&#20219;&#36196;&#23383;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#12290;&#36825;&#31181;IDS&#30740;&#31350;&#20013;&#30340;&#36879;&#26126;&#24230;&#24046;&#36317;&#26174;&#33879;&#65292;&#24433;&#21709;&#20102;&#20449;&#24515;&#21644;&#38382;&#36131;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#22411;IDS&#26041;&#27861;&#65292;&#31216;&#20026;X-CBA&#65292;&#23427;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#32467;&#26500;&#20248;&#21183;&#26469;&#26377;&#25928;&#22788;&#29702;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#12290;&#19982;&#22823;&#22810;&#25968;&#20197;GNN&#20026;&#22522;&#30784;&#30340;IDS&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20381;&#36182;&#20110;&#26631;&#35760;&#30340;&#32593;&#32476;&#27969;&#37327;&#21644;&#33410;&#28857;&#29305;&#24449;&#65292;&#36824;&#36890;&#36807;&#32593;&#32476;&#27969;&#37327;&#65292;&#21253;&#25324;&#36793;&#23646;&#24615;&#65292;&#26469;&#21033;&#29992;&#26356;&#24191;&#27867;&#30340;&#27969;&#37327;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of Intrusion Detection Systems (IDS) is critical in an era where cyber threats are becoming increasingly complex. Machine learning (ML) and deep learning (DL) models provide an efficient and accurate solution for identifying attacks and anomalies in computer networks. However, using ML and DL models in IDS has led to a trust deficit due to their non-transparent decision-making. This transparency gap in IDS research is significant, affecting confidence and accountability. To address, this paper introduces a novel Explainable IDS approach, called X-CBA, that leverages the structural advantages of Graph Neural Networks (GNNs) to effectively process network traffic data, while also adapting a new Explainable AI (XAI) methodology. Unlike most GNN-based IDS that depend on labeled network traffic and node features, thereby overlooking critical packet-level information, our approach leverages a broader range of traffic data through network flows, including edge attributes, to
&lt;/p&gt;</description></item><item><title>&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#20248;&#21183;&#65292;&#24182;&#25351;&#20986;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#37325;&#28857;&#23558;&#25918;&#22312;&#22914;&#20309;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00809</link><description>&lt;p&gt;
&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#30340;&#31435;&#22330;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00809
&lt;/p&gt;
&lt;p&gt;
&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#20248;&#21183;&#65292;&#24182;&#25351;&#20986;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#37325;&#28857;&#23558;&#25918;&#22312;&#22914;&#20309;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#20154;&#20204;&#20027;&#35201;&#20851;&#27880;&#22312;&#28041;&#21450;&#22823;&#35268;&#27169;&#22270;&#20687;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#25581;&#31034;&#20102;&#35768;&#22810;&#34987;&#24573;&#35270;&#30340;&#24230;&#37327;&#26631;&#20934;&#12289;&#20219;&#21153;&#21644;&#25968;&#25454;&#31867;&#22411;&#65292;&#22914;&#19981;&#30830;&#23450;&#24615;&#12289;&#20027;&#21160;&#21644;&#25345;&#32493;&#23398;&#20064;&#20197;&#21450;&#31185;&#23398;&#25968;&#25454;&#65292;&#36825;&#20123;&#26041;&#38754;&#38656;&#35201;&#20851;&#27880;&#12290;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65288;BDL&#65289;&#26159;&#19968;&#26465;&#26377;&#21069;&#26223;&#30340;&#36947;&#36335;&#65292;&#21487;&#20197;&#22312;&#36825;&#20123;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#25552;&#20379;&#20248;&#21183;&#12290;&#26412;&#25991;&#35748;&#20026;BDL&#21487;&#20197;&#25552;&#21319;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#23427;&#37325;&#26032;&#23457;&#35270;&#20102;BDL&#30340;&#20248;&#21183;&#12289;&#25215;&#35748;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#19968;&#20123;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#30340;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#35752;&#35770;&#38598;&#20013;&#22312;&#21487;&#33021;&#30340;&#26041;&#24335;&#19978;&#65292;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;BDL&#30456;&#32467;&#21512;&#65292;&#20197;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#20248;&#21270;&#35774;&#32622;&#29992;&#20110;&#22312;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#23458;&#25143;&#31471;&#25317;&#26377;&#31169;&#26377;&#30340;&#32452;&#20214;&#20989;&#25968;&#65292;&#36890;&#36807;&#24182;&#34892;&#35745;&#31639;&#21644;&#38598;&#20013;&#32858;&#21512;&#30340;&#26041;&#24335;&#26469;&#27714;&#35299;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00138</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Decomposable Submodular Maximization in Federated Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00138
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#20248;&#21270;&#35774;&#32622;&#29992;&#20110;&#22312;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#23458;&#25143;&#31471;&#25317;&#26377;&#31169;&#26377;&#30340;&#32452;&#20214;&#20989;&#25968;&#65292;&#36890;&#36807;&#24182;&#34892;&#35745;&#31639;&#21644;&#38598;&#20013;&#32858;&#21512;&#30340;&#26041;&#24335;&#26469;&#27714;&#35299;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#31119;&#21033;&#26368;&#22823;&#21270;&#31561;&#20247;&#22810;&#24212;&#29992;&#20013;&#65292;&#23376;&#27169;&#20989;&#25968;&#20197;&#21450;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#21450;&#20854;&#20248;&#21270;&#38382;&#39064;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#32452;&#20998;&#20989;&#25968;&#30340;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#32452;&#20998;&#20989;&#25968;&#21487;&#33021;&#26159;&#31169;&#26377;&#30340;&#65288;&#20363;&#22914;&#21487;&#33021;&#34920;&#31034;&#29992;&#25143;&#20559;&#22909;&#20989;&#25968;&#65289;&#65292;&#19981;&#33021;&#24191;&#27867;&#20849;&#20139;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#20248;&#21270;&#30340;&#8220;&#32852;&#37030;&#20248;&#21270;&#8221;&#35774;&#32622;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#23458;&#25143;&#31471;&#25317;&#26377;&#33258;&#24049;&#30340;&#20559;&#22909;&#20989;&#25968;&#65292;&#38656;&#35201;&#26368;&#22823;&#21270;&#36825;&#20123;&#20559;&#22909;&#30340;&#21152;&#26435;&#21644;&#12290;&#25105;&#20204;&#22312;&#35813;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#27969;&#34892;&#30340;&#8220;&#36830;&#32493;&#36138;&#23146;&#8221;&#31639;&#27861;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20197;&#24182;&#34892;&#30340;&#26041;&#24335;&#26397;&#30528;&#23616;&#37096;&#35299;&#21521;&#21069;&#36808;&#20986;&#23567;&#30340;&#23616;&#37096;&#27493;&#39588;&#65292;&#28982;&#21518;&#23558;&#23616;&#37096;&#21464;&#21270;&#32858;&#21512;&#21040;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Submodular functions, as well as the sub-class of decomposable submodular functions, and their optimization appear in a wide range of applications in machine learning, recommendation systems, and welfare maximization. However, optimization of decomposable submodular functions with millions of component functions is computationally prohibitive. Furthermore, the component functions may be private (they might represent user preference function, for example) and cannot be widely shared. To address these issues, we propose a {\em federated optimization} setting for decomposable submodular optimization. In this setting, clients have their own preference functions, and a weighted sum of these preferences needs to be maximized. We implement the popular {\em continuous greedy} algorithm in this setting where clients take parallel small local steps towards the local solution and then the local changes are aggregated at a central server. To address the large number of clients, the aggregation is 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.18018</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#23454;&#29616;&#30340;&#23433;&#20840;&#25552;&#31034;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Prompt-Driven LLM Safeguarding via Directed Representation Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#65292;&#20351;&#29992;&#23433;&#20840;&#25552;&#31034;&#22312;&#27169;&#22411;&#36755;&#20837;&#20043;&#21069;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20445;&#25252;&#23454;&#36341;&#65292;&#20197;&#20351;&#20854;&#19981;&#36981;&#20174;&#21253;&#21547;&#24694;&#24847;&#24847;&#22270;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#25552;&#31034;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#36825;&#22952;&#30861;&#20102;&#33258;&#21160;&#20248;&#21270;&#20854;&#20197;&#25913;&#21892;LLM&#23433;&#20840;&#24615;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#34920;&#31034;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;&#23433;&#20840;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#26377;&#23475;&#21644;&#26080;&#23475;&#30340;&#26597;&#35810;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21306;&#20998;&#24320;&#26469;&#65292;&#20294;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#36825;&#19968;&#21306;&#20998;&#12290;&#30456;&#21453;&#65292;&#19981;&#21516;&#23433;&#20840;&#25552;&#31034;&#23548;&#33268;&#26597;&#35810;&#30340;&#34920;&#31034;&#26397;&#30528;&#30456;&#20284;&#30340;&#26041;&#21521;&#31227;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#21363;&#20351;&#22312;&#26597;&#35810;&#26080;&#23475;&#26102;&#20063;&#26356;&#23481;&#26131;&#25298;&#32477;&#25552;&#20379;&#21327;&#21161;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#65288;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23433;&#20840;&#25552;&#31034;&#20248;&#21270;&#12290;DRO&#23558;&#23433;&#20840;&#25552;&#31034;&#35270;&#20026;&#35201;&#20248;&#21270;&#30340;&#34920;&#31034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2401.17505</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#31661;&#22836;
&lt;/p&gt;
&lt;p&gt;
Arrows of Time for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#26041;&#21521;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;&#36825;&#31867;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#39044;&#27979;&#19979;&#19968;&#20010;&#35760;&#21495;&#21644;&#39044;&#27979;&#21069;&#19968;&#20010;&#35760;&#21495;&#26102;&#30340;&#24179;&#22343;&#23545;&#25968;&#22256;&#24785;&#24230;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#26082;&#24494;&#22937;&#21448;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#35821;&#35328;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#26102;&#38388;&#31561;&#65289;&#19979;&#38750;&#24120;&#19968;&#33268;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#22312;&#29702;&#35770;&#19978;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#65292;&#19981;&#24212;&#35813;&#23384;&#22312;&#36825;&#26679;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#22914;&#20309;&#20986;&#29616;&#22312;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#32771;&#34385;&#20013;&#65292;&#24182;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#24102;&#26469;&#30340;&#19968;&#20123;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#32467;&#21512;&#32447;&#24615;&#21270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#26102;&#30340;&#24369;&#28857;&#12290;</title><link>https://arxiv.org/abs/2401.07105</link><description>&lt;p&gt;
&#22270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07105
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#32467;&#21512;&#32447;&#24615;&#21270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#26102;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20027;&#21147;&#20891;&#65292;&#23427;&#20204;&#19982;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30340;&#30456;&#20114;&#20316;&#29992;&#20173;&#22312;&#31215;&#26497;&#30740;&#31350;&#20013;&#12290;&#24403;&#21069;&#29992;&#20110;&#32534;&#30721;&#36825;&#20123;&#22270;&#24418;&#30340;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#65288;i&#65289;&#23558;&#23427;&#20204;&#32447;&#24615;&#21270;&#20197;&#20379;LM&#23884;&#20837;--&#36825;&#26679;&#20250;&#20302;&#25928;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#65292;&#35201;&#20040;&#65288;ii&#65289;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26469;&#20445;&#30041;&#22270;&#32467;&#26500;--&#20294;GNNs&#26080;&#27861;&#20687;&#39044;&#35757;&#32451;&#30340;LM&#19968;&#26679;&#24456;&#22909;&#22320;&#34920;&#31034;&#25991;&#26412;&#29305;&#24449;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;LM&#31867;&#22411;&#65292;&#21363;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#23427;&#25972;&#21512;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#24182;&#20943;&#36731;&#20102;&#23427;&#20204;&#30340;&#24369;&#28857;&#12290;GLM&#21442;&#25968;&#20174;&#39044;&#35757;&#32451;&#30340;LM&#20013;&#21021;&#22987;&#21270;&#65292;&#20197;&#22686;&#24378;&#23545;&#20010;&#21035;&#22270;&#27010;&#24565;&#21644;&#19977;&#20803;&#32452;&#30340;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;GLM&#30340;&#26550;&#26500;&#20197;&#25972;&#21512;&#22270;&#20559;&#24046;&#65292;&#20174;&#32780;&#20419;&#36827;&#22270;&#20869;&#30340;&#30693;&#35782;&#20998;&#24067;&#12290;&#36825;&#20351;GLM&#33021;&#22815;&#22788;&#29702;&#22270;&#24418;&#12289;&#25991;&#26412;&#20197;&#21450;&#20004;&#32773;&#30340;&#20132;&#32455;&#36755;&#20837;&#12290;&#23454;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07105v2 Announce Type: replace-cross  Abstract: While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs -- which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20855;&#26377;&#37096;&#20998;&#21160;&#24577;&#30693;&#35782;&#30340;&#22312;&#32447;Q&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#26377;&#38480;&#30340;&#20998;&#38598;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#35774;&#32622;&#19979;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;&#36951;&#25022;&#12290;</title><link>https://arxiv.org/abs/2312.12558</link><description>&lt;p&gt;
&#20855;&#26377;&#37096;&#20998;&#21160;&#24577;&#30693;&#35782;&#30340;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12558
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20855;&#26377;&#37096;&#20998;&#21160;&#24577;&#30693;&#35782;&#30340;&#22312;&#32447;Q&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#26377;&#38480;&#30340;&#20998;&#38598;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#35774;&#32622;&#19979;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;Q&#23398;&#20064;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24403;&#26576;&#20123;&#20851;&#20110;&#21160;&#24577;&#30340;&#20808;&#21069;&#30693;&#35782;&#21487;&#29992;&#25110;&#21487;&#20197;&#26377;&#25928;&#23398;&#20064;&#26102;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25353;&#29031;&#21152;&#24615;&#24178;&#25200;&#27169;&#22411;&#28436;&#21464;&#30340;&#31995;&#32479;&#65292;&#22312;&#26377;&#38480;&#30340;&#20998;&#38598;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#23545;$f$&#30340;&#23436;&#32654;&#30693;&#35782;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;$\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$&#30340;&#36951;&#25022;&#65292;&#20854;&#20013;$T$&#26159;&#19982;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#30340;&#24635;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12558v2 Announce Type: replace  Abstract: The problem of sample complexity of online reinforcement learning is often studied in the literature without taking into account any partial knowledge about the system dynamics that could potentially accelerate the learning process. In this paper, we study the sample complexity of online Q-learning methods when some prior knowledge about the dynamics is available or can be learned efficiently. We focus on systems that evolve according to an additive disturbance model of the form $S_{h+1} = f(S_h, A_h) + W_h$, where $f$ represents the underlying system dynamics, and $W_h$ are unknown disturbances independent of states and actions. In the setting of finite episodic Markov decision processes with $S$ states, $A$ actions, and episode length $H$, we present an optimistic Q-learning algorithm that achieves $\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$ regret under perfect knowledge of $f$, where $T$ is the total number of interactions with
&lt;/p&gt;</description></item><item><title>&#20844;&#24179;&#24615;&#32422;&#26463;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#33021;&#22815;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2312.10396</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#32422;&#26463;&#33021;&#22815;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#24110;&#21161;&#20174;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#20013;&#24674;&#22797;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Can Fairness Constraints Help Recover From Biased Data?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10396
&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#32422;&#26463;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#33021;&#22815;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#35748;&#20026;&#65292;&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#20844;&#24179;&#24615;&#32422;&#26463;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#30340;&#20943;&#23569;&#65292;&#32780;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#21487;&#33021;&#20250;&#21152;&#21095;&#36825;&#31181;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;Blum&#65286;Stangl&#65288;2019&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#65292;&#21363;&#20351;&#37319;&#29992;&#24179;&#31561;&#26426;&#20250;&#32422;&#26463;&#65292;&#20063;&#21487;&#20197;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;&#20182;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24456;&#26377;&#36259;&#65292;&#22240;&#20026;&#23427;&#35777;&#26126;&#20102;&#20844;&#24179;&#24615;&#32422;&#26463;&#21487;&#20197;&#38544;&#24335;&#20462;&#27491;&#25968;&#25454;&#20559;&#24046;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#20844;&#24179;&#24615;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;&#20182;&#20204;&#30340;&#25968;&#25454;&#20559;&#24046;&#27169;&#22411;&#27169;&#25311;&#20102;&#21463;&#21387;&#36843;&#20154;&#32676;&#30340;&#34920;&#24449;&#21644;&#26631;&#31614;&#20559;&#35265;&#65292;&#24182;&#22312;&#20855;&#26377;&#29420;&#31435;&#26631;&#31614;&#22122;&#22768;&#30340;&#31616;&#21333;&#26465;&#20214;&#19979;&#65292;&#38024;&#23545;&#19968;&#20010;&#29702;&#24819;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#23637;&#31034;&#20102;&#19978;&#36848;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;Blum&#65286;Stangl&#65288;2019&#65289;&#30340;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#12289;&#25968;&#25454;&#20559;&#24046;&#27169;&#22411;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#20551;&#35774;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
A general belief in fair classification is that fairness constraints incur a trade-off with accuracy, which biased data may worsen. Contrary to this belief, Blum &amp; Stangl (2019) show that fair classification with equal opportunity constraints even on extremely biased data can recover optimally accurate and fair classifiers on the original data distribution. Their result is interesting because it demonstrates that fairness constraints can implicitly rectify data bias and simultaneously overcome a perceived fairness-accuracy trade-off. Their data bias model simulates under-representation and label bias in underprivileged population, and they show the above result on a stylized data distribution with i.i.d. label noise, under simple conditions on the data distribution and bias parameters. We propose a general approach to extend the result of Blum &amp; Stangl (2019) to different fairness constraints, data bias models, data distributions, and hypothesis classes. We strengthen their result, and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hypergraph-MLP&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#36229;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#30417;&#30563;&#20013;&#38598;&#25104;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#32780;&#26080;&#38656;&#28040;&#24687;&#20256;&#36882;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#26102;&#20943;&#23569;&#36807;&#24230;&#24179;&#28369;&#21644;&#32467;&#26500;&#25200;&#21160;&#24341;&#36215;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.09778</link><description>&lt;p&gt;
&#36229;&#22270;-MLP&#65306;&#22312;&#26080;&#38656;&#28040;&#24687;&#20256;&#36882;&#30340;&#36229;&#22270;&#19978;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hypergraph-MLP: Learning on Hypergraphs without Message Passing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09778
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hypergraph-MLP&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#36229;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#30417;&#30563;&#20013;&#38598;&#25104;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#32780;&#26080;&#38656;&#28040;&#24687;&#20256;&#36882;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#26102;&#20943;&#23569;&#36807;&#24230;&#24179;&#28369;&#21644;&#32467;&#26500;&#25200;&#21160;&#24341;&#36215;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#22312;&#24314;&#27169;&#21253;&#21547;&#20004;&#20010;&#20197;&#19978;&#23454;&#20307;&#30340;&#39640;&#38454;&#20851;&#31995;&#25968;&#25454;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#20013;&#36234;&#26469;&#36234;&#21463;&#37325;&#35270;&#12290;&#35768;&#22810;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#22312;&#36229;&#22270;&#32467;&#26500;&#19978;&#30340;&#28040;&#24687;&#20256;&#36882;&#26469;&#22686;&#24378;&#33410;&#28857;&#34920;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#27169;&#22411;&#38754;&#20020;&#30528;&#36807;&#24230;&#24179;&#28369;&#20197;&#21450;&#22312;&#25512;&#29702;&#26102;&#23545;&#32467;&#26500;&#25200;&#21160;&#30340;&#39640;&#24310;&#36831;&#21644;&#25935;&#24863;&#24615;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21478;&#31867;&#26041;&#27861;&#65292;&#21363;&#23558;&#20851;&#20110;&#36229;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#35757;&#32451;&#30417;&#30563;&#20013;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#26102;&#20063;&#28040;&#38500;&#20102;&#23545;&#20854;&#30340;&#20381;&#36182;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Hypergraph-MLP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#36229;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09778v2 Announce Type: replace  Abstract: Hypergraphs are vital in modelling data with higher-order relations containing more than two entities, gaining prominence in machine learning and signal processing. Many hypergraph neural networks leverage message passing over hypergraph structures to enhance node representation learning, yielding impressive performances in tasks like hypergraph node classification. However, these message-passing-based models face several challenges, including oversmoothing as well as high latency and sensitivity to structural perturbations at inference time. To tackle those challenges, we propose an alternative approach where we integrate the information about hypergraph structures into training supervision without explicit message passing, thus also removing the reliance on it at inference. Specifically, we introduce Hypergraph-MLP, a novel learning framework for hypergraph-structured data, where the learning model is a straightforward multilayer p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.18022</link><description>&lt;p&gt;
&#21033;&#29992;&#25351;&#25968;&#23610;&#24230;&#30340;&#28145;&#24230;&#24378;&#21270;ReLU&#32593;&#32476;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Compelling ReLU Network Initialization and Training to Leverage Exponential Scaling with Depth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#28608;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#30475;&#20316;&#26159;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#32452;&#21512;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#32593;&#32476;&#65292;&#38543;&#30528;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#34920;&#36798;&#22312;&#36755;&#20837;&#22495;&#19978;&#30340;&#19981;&#21516;&#32447;&#24615;&#21306;&#22495;&#30340;&#25968;&#37327;&#26377;&#21487;&#33021;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20294;&#24403;&#21021;&#22987;&#21442;&#25968;&#36873;&#25321;&#38543;&#26426;&#26102;&#65292;&#19981;&#22826;&#21487;&#33021;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290;&#36825;&#31181;&#19981;&#33391;&#30340;&#23610;&#24230;&#33021;&#22815;&#23548;&#33268;&#21363;&#20351;&#26159;&#31616;&#21333;&#20989;&#25968;&#20063;&#38656;&#35201;&#20351;&#29992;&#36807;&#22823;&#30340;&#27169;&#22411;&#26469;&#36817;&#20284;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65306;&#39318;&#20808;&#20197;&#19968;&#31181;&#26041;&#24335;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#12290;&#22312;&#36825;&#20123;&#26032;&#21442;&#25968;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#21021;&#22987;&#35299;&#65292;&#31245;&#21518;&#36890;&#36807;&#26356;&#26032;&#24213;&#23618;&#27169;&#22411;&#26435;&#37325;&#26469;&#25913;&#36827;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20135;&#29983;&#27604;&#38543;&#26426;&#21021;&#22987;&#21270;&#23545;&#24212;&#30340;&#20989;&#25968;&#36924;&#36817;&#22909;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A neural network with ReLU activations may be viewed as a composition of piecewise linear functions. For such networks, the number of distinct linear regions expressed over the input domain has the potential to scale exponentially with depth, but it is not expected to do so when the initial parameters are chosen randomly. This poor scaling can necessitate the use of overly large models to approximate even simple functions. To address this issue, we introduce a novel training strategy: we first reparameterize the network weights in a manner that forces an exponential number of activation patterns to manifest. Training first on these new parameters provides an initial solution that can later be refined by updating the underlying model weights. This approach allows us to produce function approximations that are several orders of magnitude better than their randomly initialized counterparts.
&lt;/p&gt;</description></item><item><title>InteRACT&#36890;&#36807;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.12943</link><description>&lt;p&gt;
InteRACT&#65306;&#22522;&#20110;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#20154;&#31867;&#24847;&#22270;&#39044;&#27979;&#30340;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12943
&lt;/p&gt;
&lt;p&gt;
InteRACT&#36890;&#36807;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#30340;&#20154;&#26426;&#25805;&#32437;&#20013;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#39044;&#27979;&#20154;&#31867;&#24847;&#22270;&#24182;&#30456;&#24212;&#35843;&#25972;&#20854;&#34892;&#21160;&#65292;&#20197;&#24179;&#31283;&#25191;&#34892;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#24847;&#22270;&#21453;&#36807;&#26469;&#21448;&#21462;&#20915;&#20110;&#26426;&#22120;&#20154;&#37319;&#21462;&#30340;&#21160;&#20316;&#65292;&#36896;&#25104;&#20102;&#19968;&#20010;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#31181;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26159;&#35757;&#32451;&#29420;&#31435;&#20110;&#26426;&#22120;&#20154;&#34892;&#21160;&#30340;&#36793;&#38469;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#32570;&#20047;&#37197;&#23545;&#30340;&#20154;&#26426;&#20132;&#20114;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#26465;&#20214;&#27169;&#22411;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#33021;&#21542;&#36716;&#32780;&#21033;&#29992;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;-&#20154;&#31867;&#20132;&#20114;&#25968;&#25454;&#65311;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#21033;&#29992;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#34892;&#21160;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;-&#20154;&#31867;&#21040;&#20154;&#31867;-&#26426;&#22120;&#20154;&#25968;&#25454;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;InteRACT&#65292;&#35813;&#26550;&#26500;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#30495;&#23454;&#19990;&#30028;&#30340;&#21327;&#20316;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12943v2 Announce Type: replace-cross  Abstract: In collaborative human-robot manipulation, a robot must predict human intents and adapt its actions accordingly to smoothly execute tasks. However, the human's intent in turn depends on actions the robot takes, creating a chicken-or-egg problem. Prior methods ignore such inter-dependency and instead train marginal intent prediction models independent of robot actions. This is because training conditional models is hard given a lack of paired human-robot interaction datasets. Can we instead leverage large-scale human-human interaction data that is more easily accessible? Our key insight is to exploit a correspondence between human and robot actions that enables transfer learning from human-human to human-robot data. We propose a novel architecture, InteRACT, that pre-trains a conditional intent prediction model on large human-human datasets and fine-tunes on a small human-robot dataset. We evaluate on a set of real-world collabo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#23618;&#20248;&#21270;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22312;&#24322;&#26500;&#35774;&#32622;&#19979;&#20855;&#26377;&#36739;&#23567;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#36718;&#27425;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2311.11342</link><description>&lt;p&gt;
&#20851;&#20110;&#21435;&#20013;&#24515;&#21270;&#21452;&#23618;&#20248;&#21270;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
On the Communication Complexity of Decentralized Bilevel Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#23618;&#20248;&#21270;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22312;&#24322;&#26500;&#35774;&#32622;&#19979;&#20855;&#26377;&#36739;&#23567;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#36718;&#27425;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#21435;&#20013;&#24515;&#21270;&#21452;&#23618;&#20248;&#21270;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31639;&#27861;&#30001;&#20110;&#20272;&#35745;&#38543;&#26426;&#36229;&#26799;&#24230;&#32780;&#23548;&#33268;&#36890;&#20449;&#22797;&#26434;&#24230;&#36739;&#22823;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#24322;&#26500;&#35774;&#32622;&#19979;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#27599;&#36718;&#20013;&#20855;&#26377;&#36739;&#23567;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#36739;&#23569;&#30340;&#36890;&#20449;&#36718;&#27425;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#24322;&#26500;&#24615;&#30340;&#24378;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#24322;&#26500;&#35774;&#32622;&#19979;&#23454;&#29616;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#30340;&#38543;&#26426;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11342v2 Announce Type: replace Abstract: Decentralized bilevel optimization has been actively studied in the past few years since it has widespread applications in machine learning. However, existing algorithms suffer from large communication complexity caused by the estimation of stochastic hypergradient, limiting their application to real-world tasks. To address this issue, we develop a novel decentralized stochastic bilevel gradient descent algorithm under the heterogeneous setting, which enjoys a small communication cost in each round and a small number of communication rounds. As such, it can achieve a much better communication complexity than existing algorithms without any strong assumptions regarding heterogeneity. To the best of our knowledge, this is the first stochastic algorithm achieving these theoretical results under the heterogeneous setting. At last, the experimental results confirm the efficacy of our algorithm.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2311.08045</link><description>&lt;p&gt;
&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adversarial Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08045
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20559;&#22909;&#35843;&#25972;&#26159;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20132;&#20114;&#36136;&#37327;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25351;&#23548;LLM&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25345;&#32493;&#26356;&#26032;LLMs&#20250;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#19982;&#20154;&#31867;&#39318;&#36873;&#21709;&#24212;&#20043;&#38388;&#23384;&#22312;&#20998;&#24067;&#24046;&#36317;&#65292;&#36825;&#38459;&#30861;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#39069;&#22806;&#36827;&#34892;&#20559;&#22909;&#27880;&#37322;&#65292;&#20197;&#36866;&#24212;&#36716;&#31227;&#20998;&#24067;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#36164;&#28304;&#12290;&#38024;&#23545;&#26356;&#39640;&#25928;&#30340;&#20154;&#31867;&#20559;&#22909;&#20248;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;LLM&#20195;&#29702;&#21644;&#20559;&#22909;&#27169;&#22411;&#36890;&#36807;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#20132;&#26367;&#26356;&#26032;&#12290;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;APO&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08045v2 Announce Type: replace-cross  Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2311.07466</link><description>&lt;p&gt;
&#20851;&#20110;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Measuring Faithfulness or Self-consistency of Natural Language Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#20107;&#21518;&#25110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#35299;&#37322;&#20854;&#39044;&#27979;&#12290;&#20294;&#26159;&#65292;LLM&#21487;&#33021;&#20250;&#32534;&#36896;&#21548;&#36215;&#26469;&#21512;&#29702;&#20294;&#19981;&#24544;&#23454;&#20110;&#20854;&#22522;&#26412;&#25512;&#29702;&#30340;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#26088;&#22312;&#21028;&#26029;&#20107;&#21518;&#25110;CoT&#35299;&#37322;&#24544;&#23454;&#24230;&#30340;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#24544;&#23454;&#24230;&#27979;&#35797;&#19981;&#26159;&#34913;&#37327;&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#30340;&#24544;&#23454;&#24230;&#65292;&#32780;&#26159;&#34913;&#37327;&#20854;&#36755;&#20986;&#32423;&#21035;&#30340;&#33258;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;i&#65289;&#25105;&#20204;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#32972;&#26223;&#19979;&#28548;&#28165;&#20102;&#24544;&#23454;&#24230;&#27979;&#35797;&#30340;&#22320;&#20301;&#65292;&#23558;&#20854;&#25551;&#36848;&#20026;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;ii&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#27604;&#36739;&#19968;&#33268;&#24615;&#30340;&#27979;&#35797;&#24211;&#65292;&#39318;&#27425;&#22312;11&#20010;&#24320;&#25918;&#24335;LLMs&#21644;5&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#22871;&#20214;&#19978;&#27604;&#36739;&#20102;&#29616;&#26377;&#27979;&#35797;&#65292;&#21253;&#25324;iii&#65289;&#25105;&#20204;&#30340;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#12290;CC-SHAP&#26159;LLM&#33258;&#19968;&#33268;&#24615;&#30340;&#32454;&#31890;&#24230;&#24230;&#37327;&#65288;&#32780;&#19981;&#26159;&#27979;&#35797;&#65289;&#12290;&#23427;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#24320;&#25918;&#22270;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#23545;&#26410;&#30693;&#24322;&#24120;&#30340;&#24191;&#20041;&#26816;&#27979;&#33021;&#21147;</title><link>https://arxiv.org/abs/2311.06835</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#23454;&#29616;&#24320;&#25918;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Open-Set Graph Anomaly Detection via Normal Structure Regularisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06835
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#24320;&#25918;&#22270;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#23545;&#26410;&#30693;&#24322;&#24120;&#30340;&#24191;&#20041;&#26816;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#20219;&#21153;&#65292;&#21363;&#24320;&#25918;&#24335;GAD&#65292;&#26088;&#22312;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#27491;&#24120;&#33410;&#28857;&#21644;&#24322;&#24120;&#33410;&#28857;&#65288;&#31216;&#20026;&#24050;&#30693;&#24322;&#24120;&#65289;&#26469;&#26816;&#27979;&#24322;&#24120;&#33410;&#28857;&#65292;&#36825;&#20123;&#33410;&#28857;&#26080;&#27861;&#23637;&#31034;&#25152;&#26377;&#21487;&#33021;&#30340;&#25512;&#29702;&#26102;&#24322;&#24120;&#12290;&#24050;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20026;GAD&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#24322;&#24120;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#22823;&#22823;&#38477;&#20302;&#26816;&#27979;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#24448;&#24448;&#36807;&#20998;&#24378;&#35843;&#25311;&#21512;&#24050;&#30693;&#24322;&#24120;&#65292;&#23548;&#33268;&#23545;&#26410;&#30693;&#24322;&#24120;&#65288;&#21363;&#26410;&#34987;&#26631;&#35760;&#30340;&#24322;&#24120;&#33410;&#28857;&#65289;&#30340;&#24369;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#34987;&#24341;&#20837;&#20197;&#22788;&#29702;&#27431;&#20960;&#37324;&#24503;&#25968;&#25454;&#65292;&#26410;&#33021;&#26377;&#25928;&#25429;&#25417;GAD&#30340;&#37325;&#35201;&#38750;&#27431;&#20960;&#37324;&#24503;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#24335;GAD&#26041;&#27861;&#65292;&#21363;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#65288;NSReg&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#26410;&#30693;&#24322;&#24120;&#30340;&#24191;&#20041;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06835v2 Announce Type: replace-cross  Abstract: This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to detect anomalous nodes using a small number of labelled training normal and anomaly nodes (known as seen anomalies) that cannot illustrate all possible inference-time abnormalities. The availability of that labelled data provides crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current methods tend to over-emphasise fitting the seen anomalies, leading to a weak generalisation ability to detect unseen anomalies, i.e., those that are not illustrated by the labelled anomaly nodes. Further, they were introduced to handle Euclidean data, failing to effectively capture important non-Euclidean features for GAD. In this work, we propose a novel open-set GAD approach, namely Normal Structure Regularisation (NSReg), to achieve generalised detection ability to unseen 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#20551;&#35774;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#20551;&#35774;&#23384;&#22312;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20351;&#20854;&#19982;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#30340;&#35821;&#22659;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#27169;&#22411;&#30340;&#35266;&#23519;&#21644;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;ICL&#21644;GD&#22312;&#35266;&#23519;&#28436;&#31034;&#39034;&#24207;&#19978;&#30340;&#19981;&#21516;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.08540</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20551;&#35774;&#65306;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#20551;&#35774;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#20551;&#35774;&#23384;&#22312;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20351;&#20854;&#19982;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#30340;&#35821;&#22659;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#27169;&#22411;&#30340;&#35266;&#23519;&#21644;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;ICL&#21644;GD&#22312;&#35266;&#23519;&#28436;&#31034;&#39034;&#24207;&#19978;&#30340;&#19981;&#21516;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#20013;&#30340;In-Context Learning&#65288;ICL&#65289;&#30340;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#29616;&#35937;&#65292;&#20294;&#25105;&#20204;&#23545;&#20854;&#20102;&#35299;&#29978;&#23569;&#12290;&#20026;&#20102;&#35299;&#37322;ICL&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#29702;&#35770;&#19978;&#23558;&#20854;&#19982;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#38382;&#65292;&#36825;&#31181;&#32852;&#31995;&#22312;&#23454;&#38469;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#26159;&#21542;&#25104;&#31435;&#65311;&#25105;&#20204;&#24378;&#35843;&#20808;&#21069;&#20316;&#21697;&#20013;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#20351;&#24471;&#23427;&#20204;&#30340;&#35821;&#22659;&#19982;&#35821;&#35328;&#27169;&#22411;&#23454;&#38469;&#35757;&#32451;&#26102;&#30340;&#23454;&#38469;&#35821;&#22659;&#24046;&#21035;&#24456;&#22823;&#12290;&#20363;&#22914;&#65292;&#36825;&#20123;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#29702;&#35770;&#25163;&#24037;&#26500;&#36896;&#30340;&#26435;&#37325;&#20855;&#26377;&#19982;&#30495;&#23454;LLM&#19981;&#21305;&#37197;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#30340;&#23454;&#39564;&#39564;&#35777;&#20351;&#29992;ICL&#30446;&#26631;&#65288;&#26126;&#30830;&#20026;ICL&#35757;&#32451;&#27169;&#22411;&#65289;&#65292;&#36825;&#19982;&#37326;&#22806;&#20986;&#29616;&#30340;ICL&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#36824;&#23547;&#25214;&#20102;&#30495;&#23454;&#27169;&#22411;&#20013;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;ICL&#21644;GD&#23545;&#20110;&#35266;&#23519;&#28436;&#31034;&#30340;&#39034;&#24207;&#26377;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#25506;&#35752;&#24182;&#27604;&#36739;ICL&#19982;GD&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08540v4 Announce Type: replace-cross  Abstract: The emergence of In-Context Learning (ICL) in LLMs remains a significant phenomenon with little understanding. To explain ICL, recent studies try to theoretically connect it to Gradient Descent (GD). We ask, does this connection hold up in actual pre-trained models?   We highlight the limiting assumptions in prior works that make their context considerably different from the practical context in which language models are trained. For example, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. Furthermore, their experimental verification uses ICL objective (training models explicitly for ICL), which differs from the emergent ICL in the wild.   We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32473;&#23450;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#29305;&#24449;&#37325;&#35201;&#24615;&#24046;&#24322;&#65288;FID&#65289;&#26469;&#35843;&#26597;&#25968;&#25454;&#20559;&#24046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2303.01704</link><description>&lt;p&gt;
&#25968;&#25454;&#20559;&#24046;&#35843;&#26597;&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Feature Importance Disparities for Data Bias Investigations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.01704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32473;&#23450;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#29305;&#24449;&#37325;&#35201;&#24615;&#24046;&#24322;&#65288;FID&#65289;&#26469;&#35843;&#26597;&#25968;&#25454;&#20559;&#24046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#35748;&#20026;&#65292;&#20998;&#31867;&#22120;&#20013;&#30340;&#19979;&#28216;&#20559;&#24046;&#30340;&#19968;&#31181;&#21407;&#22240;&#26159;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#24046;&#12290;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#21487;&#33021;&#28041;&#21450;&#21040;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20363;&#22914;&#22312;&#23376;&#38598;&#19978;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#65292;&#22312;&#25910;&#38598;&#36807;&#31243;&#20013;&#21024;&#38500;&#20855;&#26377;&#20559;&#24046;&#30340;&#29305;&#24449;&#65292;&#29978;&#33267;&#36827;&#34892;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#20197;&#30830;&#23450;&#20559;&#24046;&#28304;&#12290;&#23613;&#31649;&#38656;&#35201;&#36827;&#34892;&#36825;&#26679;&#30340;&#25968;&#25454;&#20559;&#24046;&#35843;&#26597;&#65292;&#20294;&#30446;&#21069;&#24456;&#23569;&#26377;&#33258;&#21160;&#21270;&#26041;&#27861;&#21487;&#20197;&#36741;&#21161;&#20174;&#19994;&#20154;&#21592;&#36827;&#34892;&#36825;&#20123;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32473;&#23450;&#25968;&#25454;&#38598;$X$&#65292;&#21253;&#25324;&#20445;&#25252;&#21644;&#19981;&#20445;&#25252;&#30340;&#29305;&#24449;&#65292;&#32467;&#26524;$y$&#65292;&#20197;&#21450;&#19968;&#20010;&#39044;&#27979;&#32473;&#23450;$X$&#30340;&#22238;&#24402;&#22120;$h$&#30340;&#20803;&#32452;$(f_j, g)$, &#20854;&#20013;$g$&#23545;&#24212;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;$(X, y)$&#30340;&#23376;&#38598;&#65292;&#20351;&#24471;&#31532;$j$&#20010;&#29305;&#24449;$f_j$&#22312;&#23376;&#32452;$g$&#20013;&#30340;&#24433;&#21709;&#35201;&#27604;&#25972;&#20307;&#25968;&#25454;&#38598;&#20013;&#22823;&#24471;&#22810;&#65288;&#25110;&#32773;&#23567;&#24471;&#22810;&#65289;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#24046;&#24322;&#65288;FID&#65289;&#12290;&#25105;&#20204;&#22312;4&#20010;&#25968;&#25454;&#38598;&#21644;4&#20010;&#24120;&#35265;&#29305;&#24449;&#37325;&#35201;&#24615;&#20013;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is widely held that one cause of downstream bias in classifiers is bias present in the training data. Rectifying such biases may involve context-dependent interventions such as training separate models on subgroups, removing features with bias in the collection process, or even conducting real-world experiments to ascertain sources of bias. Despite the need for such data bias investigations, few automated methods exist to assist practitioners in these efforts. In this paper, we present one such method that given a dataset $X$ consisting of protected and unprotected features, outcomes $y$, and a regressor $h$ that predicts $y$ given $X$, outputs a tuple $(f_j, g)$, with the following property: $g$ corresponds to a subset of the training dataset $(X, y)$, such that the $j^{th}$ feature $f_j$ has much larger (or smaller) influence in the subgroup $g$, than on the dataset overall, which we call feature importance disparity (FID). We show across $4$ datasets and $4$ common feature import
&lt;/p&gt;</description></item><item><title>PyGOD&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#25903;&#25345;&#22810;&#31181;&#39046;&#20808;&#30340;&#22522;&#20110;&#22270;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;API&#21644;&#20016;&#23500;&#30340;&#23454;&#29992;&#31243;&#24207;&#20989;&#25968;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#26368;&#20339;&#30340;&#20195;&#30721;&#21487;&#38752;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2204.12095</link><description>&lt;p&gt;
PyGOD: &#19968;&#20010;&#29992;&#20110;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
PyGOD: A Python Library for Graph Outlier Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.12095
&lt;/p&gt;
&lt;p&gt;
PyGOD&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#25903;&#25345;&#22810;&#31181;&#39046;&#20808;&#30340;&#22522;&#20110;&#22270;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;API&#21644;&#20016;&#23500;&#30340;&#23454;&#29992;&#31243;&#24207;&#20989;&#25968;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#26368;&#20339;&#30340;&#20195;&#30721;&#21487;&#38752;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PyGOD&#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22270;&#25968;&#25454;&#20013;&#24322;&#24120;&#20540;&#30340;&#24320;&#28304;Python&#24211;&#12290;&#20316;&#20026;&#36825;&#31867;&#24211;&#20013;&#39318;&#20010;&#32508;&#21512;&#24615;&#24037;&#20855;&#65292;PyGOD&#25903;&#25345;&#22810;&#31181;&#39046;&#20808;&#30340;&#22522;&#20110;&#22270;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#12289;&#26377;&#35814;&#32454;&#25991;&#26723;&#25903;&#25345;&#30340;API&#65292;&#26088;&#22312;&#20379;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#20351;&#29992;&#12290;PyGOD&#25552;&#20379;&#20102;&#19981;&#21516;&#26816;&#27979;&#22120;&#30340;&#27169;&#22359;&#21270;&#32452;&#20214;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#23450;&#21046;&#27599;&#20010;&#26816;&#27979;&#22120;&#20197;&#36866;&#24212;&#20854;&#29992;&#36884;&#12290;&#20026;&#31616;&#21270;&#26816;&#27979;&#24037;&#20316;&#27969;&#30340;&#26500;&#24314;&#65292;PyGOD&#25552;&#20379;&#20102;&#35768;&#22810;&#24120;&#29992;&#30340;&#23454;&#29992;&#31243;&#24207;&#20989;&#25968;&#12290;&#20026;&#20102;&#23558;&#35745;&#31639;&#25193;&#23637;&#21040;&#22823;&#22411;&#22270;&#24418;&#65292;PyGOD&#25903;&#25345;&#28145;&#24230;&#27169;&#22411;&#30340;&#21151;&#33021;&#65292;&#22914;&#37319;&#26679;&#21644;&#23567;&#25209;&#37327;&#22788;&#29702;&#12290;PyGOD&#37319;&#29992;&#20102;&#20419;&#36827;&#20195;&#30721;&#21487;&#38752;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#21253;&#25324;&#21333;&#20803;&#27979;&#35797;&#12289;&#25345;&#32493;&#38598;&#25104;&#21644;&#20195;&#30721;&#35206;&#30422;&#12290;&#20026;&#20102;&#26041;&#20415;&#35775;&#38382;&#65292;PyGOD&#20197;BSD 2-Clause&#35768;&#21487;&#35777;&#21457;&#24067;&#22312;https://pygod.org &#21644; Python&#21253;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.12095v2 Announce Type: replace  Abstract: PyGOD is an open-source Python library for detecting outliers in graph data. As the first comprehensive library of its kind, PyGOD supports a wide array of leading graph-based methods for outlier detection under an easy-to-use, well-documented API designed for use by both researchers and practitioners. PyGOD provides modularized components of the different detectors implemented so that users can easily customize each detector for their purposes. To ease the construction of detection workflows, PyGOD offers numerous commonly used utility functions. To scale computation to large graphs, PyGOD supports functionalities for deep models such as sampling and mini-batch processing. PyGOD uses best practices in fostering code reliability and maintainability, including unit testing, continuous integration, and code coverage. To facilitate accessibility, PyGOD is released under a BSD 2-Clause license at https://pygod.org and at the Python Packa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#30340;&#26032;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#39038;&#23458;&#21040;&#36798;&#21644;&#26410;&#30693;&#30340;&#28857;&#20987;&#29575;&#12290;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#38543;&#26426;&#19978;&#19979;&#25991;&#25671;&#33218;&#21644;&#20855;&#26377;&#23545;&#25239;&#24615;&#21040;&#36798;&#30340;&#22312;&#32447;&#21305;&#37197;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#23454;&#29616;&#20102;&#22312;&#39038;&#23458;&#21040;&#36798;&#25509;&#36817;&#24179;&#31283;&#26102;&#20855;&#26377;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#22312;&#19968;&#33324;&#65288;&#38750;&#24179;&#31283;&#65289;&#39038;&#23458;&#21040;&#36798;&#20998;&#24067;&#19979;&#20139;&#21463;&#26368;&#20248;&#30340;&#31454;&#20105;&#27604;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#39038;&#23458;&#22330;&#26223;&#19979;&#29983;&#25104;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2401.16945</link><description>&lt;p&gt;
&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#19982;&#38750;&#24179;&#31283;&#39038;&#23458;
&lt;/p&gt;
&lt;p&gt;
Online Resource Allocation with Non-Stationary Customers. (arXiv:2401.16945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#30340;&#26032;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#39038;&#23458;&#21040;&#36798;&#21644;&#26410;&#30693;&#30340;&#28857;&#20987;&#29575;&#12290;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#38543;&#26426;&#19978;&#19979;&#25991;&#25671;&#33218;&#21644;&#20855;&#26377;&#23545;&#25239;&#24615;&#21040;&#36798;&#30340;&#22312;&#32447;&#21305;&#37197;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#23454;&#29616;&#20102;&#22312;&#39038;&#23458;&#21040;&#36798;&#25509;&#36817;&#24179;&#31283;&#26102;&#20855;&#26377;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#22312;&#19968;&#33324;&#65288;&#38750;&#24179;&#31283;&#65289;&#39038;&#23458;&#21040;&#36798;&#20998;&#24067;&#19979;&#20139;&#21463;&#26368;&#20248;&#30340;&#31454;&#20105;&#27604;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#39038;&#23458;&#22330;&#26223;&#19979;&#29983;&#25104;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#30340;&#26032;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#39038;&#23458;&#21040;&#36798;&#21644;&#26410;&#30693;&#30340;&#28857;&#20987;&#29575;&#12290;&#25105;&#20204;&#20551;&#35774;&#22810;&#31181;&#31867;&#22411;&#30340;&#39038;&#23458;&#20197;&#38750;&#24179;&#31283;&#38543;&#26426;&#26041;&#24335;&#21040;&#36798;&#65292;&#27599;&#20010;&#26102;&#26399;&#37117;&#26377;&#26410;&#30693;&#30340;&#21040;&#36798;&#29575;&#65292;&#24182;&#19988;&#39038;&#23458;&#30340;&#28857;&#20987;&#29575;&#26410;&#30693;&#65292;&#21482;&#33021;&#22312;&#32447;&#23398;&#20064;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#38543;&#26426;&#19978;&#19979;&#25991;&#25671;&#33218;&#21644;&#20855;&#26377;&#23545;&#25239;&#24615;&#21040;&#36798;&#30340;&#22312;&#32447;&#21305;&#37197;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#26041;&#26696;&#65292;&#23558;&#36164;&#28304;&#20998;&#37197;&#32473;&#38750;&#24179;&#31283;&#39038;&#23458;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#23454;&#29616;&#20102;&#8220;&#20004;&#20840;&#20854;&#32654;&#8221;&#30340;&#25928;&#26524;&#65306;&#24403;&#39038;&#23458;&#21040;&#36798;&#25509;&#36817;&#24179;&#31283;&#26102;&#65292;&#26041;&#26696;&#20855;&#26377;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#22312;&#19968;&#33324;&#65288;&#38750;&#24179;&#31283;&#65289;&#39038;&#23458;&#21040;&#36798;&#20998;&#24067;&#19979;&#20139;&#21463;&#26368;&#20248;&#30340;&#31454;&#20105;&#27604;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#19981;&#21516;&#39038;&#23458;&#22330;&#26223;&#19979;&#29983;&#25104;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel algorithm for online resource allocation with non-stationary customer arrivals and unknown click-through rates. We assume multiple types of customers arrive in a nonstationary stochastic fashion, with unknown arrival rates in each period, and that customers' click-through rates are unknown and can only be learned online. By leveraging results from the stochastic contextual bandit with knapsack and online matching with adversarial arrivals, we develop an online scheme to allocate the resources to nonstationary customers. We prove that under mild conditions, our scheme achieves a ``best-of-both-world'' result: the scheme has a sublinear regret when the customer arrivals are near-stationary, and enjoys an optimal competitive ratio under general (non-stationary) customer arrival distributions. Finally, we conduct extensive numerical experiments to show our approach generates near-optimal revenues for all different customer scenarios.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#24605;&#24819;&#65292;&#26469;&#35774;&#35745;&#21487;&#38752;&#22320;&#25490;&#21517;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;SHAP&#21644;LIME&#31561;&#24120;&#29992;&#26041;&#27861;&#30001;&#20110;&#38543;&#26426;&#37319;&#26679;&#23548;&#33268;&#30340;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.15800</link><description>&lt;p&gt;
&#20351;&#29992;SHAP&#21644;LIME&#36827;&#34892;&#21487;&#35777;&#26126;&#31283;&#23450;&#30340;&#29305;&#24449;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Provably Stable Feature Rankings with SHAP and LIME. (arXiv:2401.15800v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15800
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#24605;&#24819;&#65292;&#26469;&#35774;&#35745;&#21487;&#38752;&#22320;&#25490;&#21517;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;SHAP&#21644;LIME&#31561;&#24120;&#29992;&#26041;&#27861;&#30001;&#20110;&#38543;&#26426;&#37319;&#26679;&#23548;&#33268;&#30340;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26159;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#26222;&#36941;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20998;&#36755;&#20837;&#21464;&#37327;&#30340;&#24120;&#29992;&#26041;&#27861;&#65292;&#22914;SHAP&#21644;LIME&#65292;&#30001;&#20110;&#38543;&#26426;&#37319;&#26679;&#32780;&#20855;&#26377;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#12290;&#20511;&#37492;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#22815;&#20197;&#39640;&#27010;&#29575;&#27491;&#30830;&#25490;&#21517;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#24402;&#22240;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;RankSHAP&#20445;&#35777;$K$&#20010;&#26368;&#39640;Shapley&#20540;&#20855;&#26377;&#36229;&#36807;$1-\alpha$&#30340;&#27491;&#30830;&#25490;&#24207;&#27010;&#29575;&#12290;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#20026;LIME&#25552;&#20379;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#30830;&#20445;&#20197;&#27491;&#30830;&#39034;&#24207;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attributions are ubiquitous tools for understanding the predictions of machine learning models. However, popular methods for scoring input variables such as SHAP and LIME suffer from high instability due to random sampling. Leveraging ideas from multiple hypothesis testing, we devise attribution methods that correctly rank the most important features with high probability. Our algorithm RankSHAP guarantees that the $K$ highest Shapley values have the proper ordering with probability exceeding $1-\alpha$. Empirical results demonstrate its validity and impressive computational efficiency. We also build on previous work to yield similar results for LIME, ensuring the most important features are selected in the right order.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#20219;&#21153;&#12289;&#24322;&#26500;&#21644;&#26080;&#27169;&#22411;&#29615;&#22659;&#19979;&#23398;&#20064;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#19982;&#27599;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#26368;&#20248;&#25511;&#21046;&#22120;&#25509;&#36817;&#30340;&#25511;&#21046;&#22120;&#65292;&#24182;&#22312;&#27169;&#22411;&#22522;&#30784;&#35774;&#32622;&#19979;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14534</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;: &#19968;&#31181;&#38024;&#23545;&#26080;&#27169;&#22411;LQR&#30340;&#31574;&#30053;&#26799;&#24230;MAML&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Linear Quadratic Regulators: A Policy Gradient MAML Approach for the Model-free LQR. (arXiv:2401.14534v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#20219;&#21153;&#12289;&#24322;&#26500;&#21644;&#26080;&#27169;&#22411;&#29615;&#22659;&#19979;&#23398;&#20064;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#19982;&#27599;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#26368;&#20248;&#25511;&#21046;&#22120;&#25509;&#36817;&#30340;&#25511;&#21046;&#22120;&#65292;&#24182;&#22312;&#27169;&#22411;&#22522;&#30784;&#35774;&#32622;&#19979;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#22810;&#20219;&#21153;&#12289;&#24322;&#26500;&#21644;&#26080;&#27169;&#22411;&#29615;&#22659;&#20013;&#23398;&#20064;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#26041;&#27861;&#65288;Finn&#31561;&#20154;&#65292;2017&#65289;&#22312;&#19981;&#21516;&#20219;&#21153;&#24322;&#36136;&#24615;&#35774;&#32622;&#19979;&#30340;LQR&#38382;&#39064;&#30340;&#31283;&#23450;&#24615;&#21644;&#20010;&#24615;&#21270;&#20445;&#35777;&#36827;&#34892;&#20102;&#21051;&#30011;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#27169;&#22411;&#22522;&#30784;&#21644;&#26080;&#27169;&#22411;&#35774;&#32622;&#19979;&#65292;MAML-LQR&#26041;&#27861;&#20135;&#29983;&#30340;&#25511;&#21046;&#22120;&#19982;&#27599;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#26368;&#20248;&#25511;&#21046;&#22120;&#25509;&#36817;&#65292;&#38500;&#20102;&#20219;&#21153;&#24322;&#36136;&#24615;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#27169;&#22411;&#22522;&#30784;&#35774;&#32622;&#19979;&#65292;&#36825;&#31181;&#25511;&#21046;&#22120;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#23454;&#29616;&#65292;&#36825;&#25913;&#36827;&#20102;&#29616;&#26377;MAML-LQR&#24037;&#20316;&#20013;&#30340;&#27425;&#32447;&#24615;&#36895;&#29575;&#12290;&#19982;&#29616;&#26377;&#30340;MAML-LQR&#32467;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20445;&#35777;&#34920;&#26126;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#22120;&#21487;&#20197;&#39640;&#25928;&#22320;&#36866;&#24212;&#26410;&#30693;&#30340;LQR&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of learning Linear Quadratic Regulators (LQR) in a multi-task, heterogeneous, and model-free setting. We characterize the stability and personalization guarantees of a Policy Gradient-based (PG) Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) approach for the LQR problem under different task-heterogeneity settings. We show that the MAML-LQR approach produces a stabilizing controller close to each task-specific optimal controller up to a task-heterogeneity bias for both model-based and model-free settings. Moreover, in the model-based setting, we show that this controller is achieved with a linear convergence rate, which improves upon sub-linear rates presented in existing MAML-LQR work. In contrast to existing MAML-LQR results, our theoretical guarantees demonstrate that the learned controller can efficiently adapt to unseen LQR tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#25805;&#20316;&#22120;&#65288;EGNO&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#20026;&#36712;&#36857;&#32780;&#19981;&#20165;&#20165;&#26159;&#19979;&#19968;&#27493;&#39044;&#27979;&#65292;&#20197;&#20934;&#30830;&#25429;&#25417;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#21033;&#29992;&#31561;&#21464;&#26102;&#38388;&#21367;&#31215;&#26469;&#20445;&#25345;&#20854;&#20869;&#22312;&#30340;&#31561;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.11037</link><description>&lt;p&gt;
&#24314;&#27169;&#19977;&#32500;&#21160;&#21147;&#23398;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#25805;&#20316;&#22120;
&lt;/p&gt;
&lt;p&gt;
Equivariant Graph Neural Operator for Modeling 3D Dynamics. (arXiv:2401.11037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#25805;&#20316;&#22120;&#65288;EGNO&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#20026;&#36712;&#36857;&#32780;&#19981;&#20165;&#20165;&#26159;&#19979;&#19968;&#27493;&#39044;&#27979;&#65292;&#20197;&#20934;&#30830;&#25429;&#25417;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#21033;&#29992;&#31561;&#21464;&#26102;&#38388;&#21367;&#31215;&#26469;&#20445;&#25345;&#20854;&#20869;&#22312;&#30340;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#31185;&#23398;&#20013;&#65292;&#24314;&#27169;&#22797;&#26434;&#30340;&#19977;&#32500;&#20851;&#31995;&#31995;&#32479;&#21160;&#21147;&#23398;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#28041;&#21450;&#39046;&#22495;&#20174;&#20998;&#23376;&#27169;&#25311;&#21040;&#31890;&#23376;&#21147;&#23398;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#31354;&#38388;&#30456;&#20114;&#20316;&#29992;&#24050;&#21462;&#24471;&#33391;&#22909;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#33021;&#36827;&#34892;&#19979;&#19968;&#27493;&#39044;&#27979;&#65292;&#19981;&#33021;&#20934;&#30830;&#25429;&#25417;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21363;&#31561;&#21464;&#22270;&#31070;&#32463;&#25805;&#20316;&#22120;&#65288;EGNO&#65289;&#65292;&#30452;&#25509;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#20026;&#36712;&#36857;&#32780;&#19981;&#20165;&#20165;&#26159;&#19979;&#19968;&#27493;&#39044;&#27979;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;EGNO&#26174;&#24335;&#22320;&#23398;&#20064;&#19977;&#32500;&#21160;&#21147;&#23398;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#36890;&#36807;&#26102;&#38388;&#20989;&#25968;&#26469;&#24314;&#27169;&#21160;&#21147;&#23398;&#24182;&#23398;&#20064;&#31070;&#32463;&#25805;&#20316;&#22120;&#26469;&#36817;&#20284;&#12290;&#20026;&#20102;&#25429;&#25417;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20869;&#22312;&#30340;SE(3)&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#22312;&#20613;&#37324;&#21494;&#31354;&#38388;&#20013;&#21442;&#25968;&#21270;&#31561;&#21464;&#26102;&#38388;&#21367;&#31215;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#21367;&#31215;&#23618;&#26500;&#24314;EGNO&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the complex three-dimensional (3D) dynamics of relational systems is an important problem in the natural sciences, with applications ranging from molecular simulations to particle mechanics. Machine learning methods have achieved good success by learning graph neural networks to model spatial interactions. However, these approaches do not faithfully capture temporal correlations since they only model next-step predictions. In this work, we propose Equivariant Graph Neural Operator (EGNO), a novel and principled method that directly models dynamics as trajectories instead of just next-step prediction. Different from existing methods, EGNO explicitly learns the temporal evolution of 3D dynamics where we formulate the dynamics as a function over time and learn neural operators to approximate it. To capture the temporal correlations while keeping the intrinsic SE(3)-equivariance, we develop equivariant temporal convolutions parameterized in the Fourier space and build EGNO by stac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#26063;&#21644;&#28385;&#31209;&#21464;&#20998;&#26063;&#20043;&#38388;&#30340;&#29702;&#35770;&#20013;&#38388;&#22320;&#24102;&#65306;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#21487;&#20197;&#22312;&#36845;&#20195;&#22797;&#26434;&#24615;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32553;&#25918;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.10989</link><description>&lt;p&gt;
&#20855;&#26377;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#30340;&#21487;&#35777;&#20280;&#32553;&#24615;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Provably Scalable Black-Box Variational Inference with Structured Variational Families. (arXiv:2401.10989v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#26063;&#21644;&#28385;&#31209;&#21464;&#20998;&#26063;&#20043;&#38388;&#30340;&#29702;&#35770;&#20013;&#38388;&#22320;&#24102;&#65306;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#21487;&#20197;&#22312;&#36845;&#20195;&#22797;&#26434;&#24615;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32553;&#25918;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#20855;&#26377;&#28385;&#31209;&#21327;&#26041;&#24046;&#36924;&#36817;&#30340;&#21464;&#20998;&#26063;&#22312;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#26080;&#35770;&#26159;&#20174;&#23454;&#35777;&#19978;&#36824;&#26159;&#29702;&#35770;&#19978;&#12290;&#20107;&#23454;&#19978;&#65292;&#26368;&#36817;&#23545;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22343;&#20540;&#22330;&#21464;&#20998;&#26063;&#30456;&#27604;&#65292;&#28385;&#31209;&#21464;&#20998;&#26063;&#22312;&#38382;&#39064;&#30340;&#32500;&#24230;&#19978;&#25193;&#23637;&#24471;&#24456;&#24046;&#12290;&#36825;&#23545;&#20855;&#26377;&#26412;&#22320;&#21464;&#37327;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#23588;&#20026;&#20851;&#38190;&#65292;&#23427;&#20204;&#30340;&#32500;&#24230;&#38543;&#30528;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#32780;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#36845;&#20195;&#22797;&#26434;&#24615;&#23545;&#25968;&#25454;&#38598;&#22823;&#23567;N&#23384;&#22312;&#26126;&#30830;&#30340;O(N^2)&#20381;&#36182;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#26063;&#21644;&#28385;&#31209;&#21464;&#20998;&#26063;&#20043;&#38388;&#30340;&#29702;&#35770;&#20013;&#38388;&#22320;&#24102;&#65306;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#12290;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126;&#20102;&#26576;&#20123;&#23610;&#24230;&#30697;&#38453;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#36845;&#20195;&#22797;&#26434;&#24615;O(N)&#65292;&#20174;&#32780;&#19982;N&#30340;&#32553;&#25918;&#26356;&#22909;&#22320;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#29616;&#23454;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Variational families with full-rank covariance approximations are known not to work well in black-box variational inference (BBVI), both empirically and theoretically. In fact, recent computational complexity results for BBVI have established that full-rank variational families scale poorly with the dimensionality of the problem compared to e.g. mean field families. This is particularly critical to hierarchical Bayesian models with local variables; their dimensionality increases with the size of the datasets. Consequently, one gets an iteration complexity with an explicit $\mathcal{O}(N^2)$ dependence on the dataset size $N$. In this paper, we explore a theoretical middle ground between mean-field variational families and full-rank families: structured variational families. We rigorously prove that certain scale matrix structures can achieve a better iteration complexity of $\mathcal{O}(N)$, implying better scaling with respect to $N$. We empirically verify our theoretical results on l
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#28860;GANs&#65292;&#24182;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#28789;&#27963;&#30340;&#23454;&#26102;&#22270;&#20687;&#32534;&#36753;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#19981;&#21516;&#27010;&#24565;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.06127</link><description>&lt;p&gt;
E$^{2}$GAN: &#39640;&#25928;&#35757;&#32451;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#30340;&#39640;&#25928;GANs
&lt;/p&gt;
&lt;p&gt;
E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation. (arXiv:2401.06127v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#28860;GANs&#65292;&#24182;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#28789;&#27963;&#30340;&#23454;&#26102;&#22270;&#20687;&#32534;&#36753;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#19981;&#21516;&#27010;&#24565;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#28789;&#27963;&#30340;&#23454;&#26102;&#35774;&#22791;&#19978;&#22270;&#20687;&#32534;&#36753;&#65292;&#19968;&#31181;&#39640;&#24230;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20363;&#22914;&#31283;&#23450;&#25193;&#25955; (Stable Diffusion)&#65292;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GANs) &#30340;&#37197;&#23545;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#20943;&#36731;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#26102;&#36890;&#24120;&#30001;&#39640;&#31471;&#21830;&#29992;GPU&#29305;&#23450;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#19982;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#27599;&#20010;&#29983;&#25104;&#30340; GAN &#37117;&#19987;&#38376;&#29992;&#20110;&#29305;&#23450;&#30340;&#22270;&#20687;&#32534;&#36753;&#20219;&#21153;&#65292;&#22240;&#27492;&#38656;&#35201;&#26114;&#36149;&#30340;&#35757;&#32451;&#24037;&#20316;&#26469;&#33719;&#24471;&#21508;&#31181;&#27010;&#24565;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30740;&#31350;&#26041;&#21521;&#65306;&#33021;&#21542;&#20351;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#28860; GANs &#30340;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#65311;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#21019;&#26032;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#24191;&#20041;&#29305;&#24449;&#30340;&#22522;&#26412; GAN &#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#36866;&#24212;&#19981;&#21516;&#30340;&#27010;&#24565;&#65292;&#28040;&#38500;&#20102;...
&lt;/p&gt;
&lt;p&gt;
One highly promising direction for enabling flexible real-time on-device image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models, such as Stable Diffusion, to generate paired datasets used for training generative adversarial networks (GANs). This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models. However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts. In this work, we introduce and address a novel research direction: can the process of distilling GANs from diffusion models be made significantly more efficient? To achieve this goal, we propose a series of innovative techniques. First, we construct a base GAN model with generalized features, adaptable to different concepts through fine-tuning, eliminating t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#30340;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#23384;&#22312;&#30340;&#39118;&#26684;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04514</link><description>&lt;p&gt;
&#37325;&#20889;&#20195;&#30721;&#65306;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#20195;&#30721;&#25628;&#32034;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search. (arXiv:2401.04514v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#30340;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#23384;&#22312;&#30340;&#39118;&#26684;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20195;&#30721;&#25628;&#32034;&#20013;&#65292;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#31034;&#20363;&#20195;&#30721;&#29255;&#27573;&#26469;&#22686;&#24378;&#26597;&#35810;&#65292;&#20197;&#35299;&#20915;&#20195;&#30721;&#29255;&#27573;&#21644;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20043;&#38388;&#30340;&#20027;&#35201;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#35843;&#26597;&#21457;&#29616;&#65292;LLM&#22686;&#24378;&#26694;&#26550;&#25152;&#25552;&#20379;&#30340;&#25913;&#36827;&#26377;&#19968;&#23450;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#38480;&#21046;&#21487;&#33021;&#26159;&#22240;&#20026;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#23613;&#31649;&#22312;&#21151;&#33021;&#19978;&#20934;&#30830;&#65292;&#20294;&#22312;&#20195;&#30721;&#24211;&#20013;&#19982;&#22522;&#20934;&#20195;&#30721;&#20043;&#38388;&#32463;&#24120;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#39118;&#26684;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22522;&#30784;GAR&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#24211;&#20013;&#30340;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#65288;ReCo&#65289;&#26469;&#36827;&#34892;&#39118;&#26684;&#35268;&#33539;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReCo&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#28608;&#21169;&#20860;&#23481;&#24615;&#30340;&#22810;&#23545;&#19968;&#21305;&#37197;&#24066;&#22330;&#20013;&#25913;&#36827;&#36172;&#21338;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#25506;&#32034;-&#24310;&#36831;&#25509;&#21463;&#65288;AETDA&#65289;&#31639;&#27861;&#26469;&#25552;&#39640;&#36951;&#25022;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.01528</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#28608;&#21169;&#20860;&#23481;&#24615;&#30340;&#22810;&#23545;&#19968;&#21305;&#37197;&#24066;&#22330;&#20013;&#25913;&#36827;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improved Bandits in Many-to-one Matching Markets with Incentive Compatibility. (arXiv:2401.01528v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#28608;&#21169;&#20860;&#23481;&#24615;&#30340;&#22810;&#23545;&#19968;&#21305;&#37197;&#24066;&#22330;&#20013;&#25913;&#36827;&#36172;&#21338;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#25506;&#32034;-&#24310;&#36831;&#25509;&#21463;&#65288;AETDA&#65289;&#31639;&#27861;&#26469;&#25552;&#39640;&#36951;&#25022;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20016;&#23500;&#30340;&#24212;&#29992;&#65292;&#21452;&#36793;&#21305;&#37197;&#24066;&#22330;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#30001;&#20110;&#21442;&#19982;&#32773;&#36890;&#24120;&#23545;&#33258;&#24049;&#30340;&#20559;&#22909;&#19981;&#30830;&#23450;&#65292;&#26368;&#36817;&#37319;&#29992;&#22312;&#32447;&#31639;&#27861;&#36890;&#36807;&#36845;&#20195;&#20132;&#20114;&#26469;&#23398;&#20064;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22312;&#22810;&#23545;&#19968;&#35774;&#32622;&#20013;&#30340;&#32467;&#26524;&#36828;&#38750;&#26368;&#20248;&#65292;&#24182;&#32570;&#20047;&#28608;&#21169;&#20860;&#23481;&#24615;&#30340;&#20445;&#35777;&#12290;&#26412;&#25991;&#38024;&#23545;&#22810;&#23545;&#19968;&#24066;&#22330;&#22312;&#25552;&#39640;&#36951;&#25022;&#19978;&#38480;&#30340;&#21516;&#26102;&#30830;&#20445;&#28608;&#21169;&#20860;&#23481;&#24615;&#30340;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#25506;&#32034;-&#24310;&#36831;&#25509;&#21463;&#65288;AETDA&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21709;&#24212;&#24615;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two-sided matching markets have been widely studied in the literature due to their rich applications. Since participants are usually uncertain about their preferences, online algorithms have recently been adopted to learn them through iterative interactions. \citet{wang2022bandit} initiate the study of this problem in a many-to-one setting with \textit{responsiveness}. However, their results are far from optimal and lack guarantees of incentive compatibility. An extension of \citet{kong2023player} to this more general setting achieves a near-optimal bound for player-optimal regret. Nevertheless, due to the substantial requirement for collaboration, a single player's deviation could lead to a huge increase in its own cumulative rewards and an $O(T)$ regret for others. In this paper, we aim to enhance the regret bound in many-to-one markets while ensuring incentive compatibility. We first propose the adaptively explore-then-deferred-acceptance (AETDA) algorithm for responsiveness setting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Pontryagin&#27169;&#24335;&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#36890;&#36807;&#22312;&#21069;&#21521;&#21644;&#21453;&#21521;&#20849;&#36717;&#29366;&#24577;&#22238;&#28378;&#20043;&#38388;&#30340;&#24046;&#24322;&#19978;&#23450;&#20041;&#30340;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#22312;&#20855;&#26377;&#21442;&#25968;&#21270;&#29366;&#24577;&#32422;&#26463;&#30340;&#21338;&#24328;&#20013;&#20540;&#30340;&#19981;&#36830;&#32493;&#24615;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01502</link><description>&lt;p&gt;
Pontryagin&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#35299;&#20915;&#21442;&#25968;&#21270;&#26222;&#36890;&#21644;&#24046;&#20998;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Pontryagin Neural Operator for Solving Parametric General-Sum Differential Games. (arXiv:2401.01502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Pontryagin&#27169;&#24335;&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#36890;&#36807;&#22312;&#21069;&#21521;&#21644;&#21453;&#21521;&#20849;&#36717;&#29366;&#24577;&#22238;&#28378;&#20043;&#38388;&#30340;&#24046;&#24322;&#19978;&#23450;&#20041;&#30340;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#22312;&#20855;&#26377;&#21442;&#25968;&#21270;&#29366;&#24577;&#32422;&#26463;&#30340;&#21338;&#24328;&#20013;&#20540;&#30340;&#19981;&#36830;&#32493;&#24615;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#20010;&#29609;&#23478;&#30340;&#26222;&#36890;&#21644;&#24046;&#20998;&#21338;&#24328;&#30340;&#20540;&#26159;Hamilton-Jacobi-Isaacs&#65288;HJI&#65289;&#26041;&#31243;&#30340;&#31896;&#24615;&#35299;&#12290;&#36825;&#31181;&#21338;&#24328;&#30340;&#20540;&#21644;&#31574;&#30053;&#36924;&#36817;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#65288;CoD&#65289;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20943;&#36731;CoD&#26102;&#65292;&#30001;&#20110;&#29366;&#24577;&#32422;&#26463;&#24341;&#36215;&#30340;&#20540;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#20250;&#36935;&#21040;&#25910;&#25947;&#38382;&#39064;&#12290;&#38500;&#20102;&#36825;&#20123;&#25361;&#25112;&#20043;&#22806;&#65292;&#22312;&#23545;&#21338;&#24328;&#21442;&#25968;&#31354;&#38388;&#36827;&#34892;&#23398;&#20064;&#26102;&#65288;&#20363;&#22914;&#65292;&#22312;&#20449;&#24687;&#19981;&#23436;&#25972;&#26102;&#36827;&#34892;&#21338;&#24328;&#21442;&#25968;&#25512;&#26029;&#65289;&#65292;&#36890;&#24120;&#38656;&#35201;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#20540;&#21644;&#31574;&#30053;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;Pontryagin&#27169;&#24335;&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#22312;&#20855;&#26377;&#21442;&#25968;&#21270;&#29366;&#24577;&#32422;&#26463;&#30340;&#21338;&#24328;&#20013;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#65288;SOTA&#65289;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#21069;&#21521;&#21644;&#21453;&#21521;&#20849;&#36717;&#29366;&#24577;&#22238;&#28378;&#20043;&#38388;&#30340;&#24046;&#24322;&#19978;&#23450;&#20041;&#30340;&#20849;&#36717;&#29366;&#24577;&#25439;&#22833;&#65292;&#36825;&#31181;&#25439;&#22833;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20849;&#36717;&#29366;&#24577;&#21160;&#21147;&#23398;&#30340;&#19981;&#36830;&#32493;&#24615;&#22312;&#35813;&#31639;&#23376;&#30340;&#24615;&#33021;&#25913;&#36827;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The values of two-player general-sum differential games are viscosity solutions to Hamilton-Jacobi-Isaacs (HJI) equations. Value and policy approximations for such games suffer from the curse of dimensionality (CoD). Alleviating CoD through physics-informed neural networks (PINN) encounters convergence issues when value discontinuity is present due to state constraints. On top of these challenges, it is often necessary to learn generalizable values and policies across a parametric space of games, e.g., for game parameter inference when information is incomplete. To address these challenges, we propose in this paper a Pontryagin-mode neural operator that outperforms existing state-of-the-art (SOTA) on safety performance across games with parametric state constraints. Our key contribution is the introduction of a costate loss defined on the discrepancy between forward and backward costate rollouts, which are computationally cheap. We show that the discontinuity of costate dynamics (in th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.11973</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;: &#38754;&#21521;&#35270;&#39057;&#34920;&#31034;&#30340;&#20813;&#36951;&#24536;&#20248;&#32988;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65288;LTH&#65289;&#30340;&#21551;&#21457;&#65292;&#35813;&#20551;&#35774;&#24378;&#35843;&#22312;&#36739;&#22823;&#30340;&#23494;&#38598;&#32593;&#32476;&#20013;&#23384;&#22312;&#39640;&#25928;&#23376;&#32593;&#32476;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24403;&#30340;&#31232;&#30095;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#31168;&#30340;&#20248;&#32988;&#23376;&#32593;&#32476;&#65288;WSN&#65289;&#22312;&#21508;&#31181;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#26469;&#33258;&#23494;&#38598;&#32593;&#32476;&#30340;&#39044;&#20808;&#23384;&#22312;&#30340;&#26435;&#37325;&#65292;&#22312;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#65288;TIL&#65289;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;&#36719;&#23376;&#32593;&#32476;&#65288;SoftNet&#65289;&#30340;WSN&#21464;&#20307;&#65292;&#20197;&#38450;&#27490;&#25968;&#25454;&#26679;&#26412;&#31232;&#32570;&#26102;&#30340;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#20102;WSN&#26435;&#37325;&#30340;&#31232;&#30095;&#37325;&#29992;&#65292;&#29992;&#20110;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65288;VIL&#65289;&#12290;&#32771;&#34385;&#20102;&#22312;WSN&#20013;&#20351;&#29992;&#20613;&#31435;&#21494;&#23376;&#31070;&#32463;&#36816;&#31639;&#22120;&#65288;FSO&#65289;&#65292;&#23427;&#33021;&#22815;&#23545;&#35270;&#39057;&#36827;&#34892;&#32039;&#20945;&#32534;&#30721;&#65292;&#24182;&#22312;&#19981;&#21516;&#24102;&#23485;&#19979;&#35782;&#21035;&#21487;&#37325;&#29992;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;FSO&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#36830;&#32493;&#23398;&#20064;&#26550;&#26500;&#20013;&#65292;&#21253;&#25324;VIL&#12289;TIL&#21644;FSCIL&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the existence of efficient subnetworks within larger, dense networks, a high-performing Winning Subnetwork (WSN) in terms of task performance under appropriate sparsity conditions is considered for various continual learning tasks. It leverages pre-existing weights from dense networks to achieve efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is designed to prevent overfitting when the data samples are scarce. Furthermore, the sparse reuse of WSN weights is considered for Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It enables compact encoding of videos and identifies reusable subnetworks across varying bandwidths. We have integrated FSO into different architectural frameworks for continual learning, including VIL, TIL, and FSCIL. Our c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#23610;&#24230;&#30340;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#26657;&#27491;&#65292;&#20197;&#22686;&#24378;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20943;&#23569;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.07586</link><description>&lt;p&gt;
&#29305;&#24449;&#24341;&#23548;&#65306;&#22823;&#23610;&#24230;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale. (arXiv:2312.07586v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07586
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#23610;&#24230;&#30340;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#26657;&#27491;&#65292;&#20197;&#22686;&#24378;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20943;&#23569;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#30340;&#23548;&#24341;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DDPM)&#32447;&#24615;&#22320;&#23558;&#19981;&#21516;&#30340;&#26465;&#20214;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#25552;&#20379;&#23545;&#26679;&#26412;&#30340;&#22686;&#24378;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#24403;&#23548;&#21521;&#23610;&#24230;&#21464;&#22823;&#26102;&#20135;&#29983;&#30340;&#38750;&#32447;&#24615;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#65292;&#19968;&#31181;&#37319;&#26679;&#26041;&#27861;&#65292;&#20026;&#26080;&#20998;&#31867;&#22120;&#23548;&#21521;&#30340;DDPM&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#29702;&#30340;&#38750;&#32447;&#24615;&#26657;&#27491;&#12290;&#36825;&#31181;&#26657;&#27491;&#36843;&#20351;&#23548;&#21521;&#30340;DDPM&#36981;&#23432;&#20854;&#24213;&#23618;&#25193;&#25955;&#36807;&#31243;&#30340;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#35757;&#32451;&#65292;&#26080;&#38656;&#23548;&#25968;&#65292;&#19982;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#20860;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#29305;&#24449;&#24341;&#23548;&#22686;&#24378;&#20102;&#23545;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#24182;&#20943;&#23569;&#20102;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#23545;&#20174;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21040;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#22914;&#30913;&#30456;&#21464;&#30340;&#21508;&#31181;&#24212;&#29992;&#37117;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Popular guidance for denoising diffusion probabilistic model (DDPM) linearly combines distinct conditional models together to provide enhanced control over samples. However, this approach overlooks nonlinear effects that become significant when guidance scale is large. To address this issue, we propose characteristic guidance, a sampling method that provides first-principle non-linear correction for classifier-free guided DDPMs. Such correction forces the guided DDPMs to respect the Fokker-Planck equation of their underlying diffusion process, in a way that is training-free, derivative-free, and compatible with existing sampling methods. Experiments show that characteristic guidance enhances control and reduces color and exposure issues in image generation, proving effective in diverse applications ranging from latent space sampling to solving physics problems like magnet phase transitions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20581;&#22766;&#21644;&#20849;&#36717;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;RCGP&#65289;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#27867;&#21270;&#36125;&#21494;&#26031;&#25512;&#26029;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#38381;&#24335;&#26356;&#26032;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2311.00463</link><description>&lt;p&gt;
&#20581;&#22766;&#21644;&#20849;&#36717;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Robust and Conjugate Gaussian Process Regression. (arXiv:2311.00463v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20581;&#22766;&#21644;&#20849;&#36717;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;RCGP&#65289;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#27867;&#21270;&#36125;&#21494;&#26031;&#25512;&#26029;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#38381;&#24335;&#26356;&#26032;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#38381;&#24335;&#26465;&#20214;&#65292;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#22238;&#24402;&#30340;&#24120;&#35265;&#20551;&#35774;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#39640;&#26031;&#35266;&#27979;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24378;&#20551;&#35774;&#22312;&#23454;&#38469;&#20013;&#32463;&#24120;&#34987;&#36829;&#21453;&#65292;&#23548;&#33268;&#19981;&#21487;&#38752;&#30340;&#25512;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#27867;&#21270;&#36125;&#21494;&#26031;&#25512;&#26029;&#20197;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#20195;&#20215;&#23454;&#29616;&#21487;&#38752;&#21644;&#20849;&#36717;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;RCGP&#65289;&#22238;&#24402;&#12290;RCGP&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#22312;&#26631;&#20934;GP&#36866;&#29992;&#30340;&#25152;&#26377;&#24773;&#20917;&#19979;&#36827;&#34892;&#31934;&#30830;&#30340;&#20849;&#36717;&#38381;&#24335;&#26356;&#26032;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;RCGP&#24212;&#29992;&#20110;&#20174;&#36125;&#21494;&#26031;&#20248;&#21270;&#21040;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#30340;&#21508;&#31181;&#38382;&#39064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enable closed form conditioning, a common assumption in Gaussian process (GP) regression is independent and identically distributed Gaussian observation noise. This strong and simplistic assumption is often violated in practice, which leads to unreliable inferences and uncertainty quantification. Unfortunately, existing methods for robustifying GPs break closed-form conditioning, which makes them less attractive to practitioners and significantly more computationally expensive. In this paper, we demonstrate how to perform provably robust and conjugate Gaussian process (RCGP) regression at virtually no additional cost using generalised Bayesian inference. RCGP is particularly versatile as it enables exact conjugate closed form updates in all settings where standard GPs admit them. To demonstrate its strong empirical performance, we deploy RCGP for problems ranging from Bayesian optimisation to sparse variational Gaussian processes.
&lt;/p&gt;</description></item><item><title>&#22312;&#26377;&#38480;&#20808;&#39564;&#30693;&#35782;&#19979;&#65292;&#36890;&#36807;&#23616;&#37096;&#20998;&#21306;&#21457;&#29616;&#31639;&#27861;&#65288;LDP&#65289;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#33258;&#21160;&#21464;&#37327;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;LDP&#26681;&#25454;&#19982;&#26333;&#20809;-&#32467;&#26524;&#23545;{X,Y}&#30456;&#20851;&#30340;&#23376;&#38598;&#23558;&#21464;&#37327;&#38598;&#21512;Z&#36827;&#34892;&#20998;&#21306;&#65292;&#24182;&#21306;&#20998;&#28151;&#28102;&#22240;&#32032;&#21644;&#20854;&#20182;&#21464;&#37327;&#31867;&#22411;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#27425;&#20108;&#27425;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.17816</link><description>&lt;p&gt;
Local Discovery by Partitioning: &#22312;&#26377;&#38480;&#20808;&#39564;&#30693;&#35782;&#19979;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs. (arXiv:2310.17816v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17816
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#20808;&#39564;&#30693;&#35782;&#19979;&#65292;&#36890;&#36807;&#23616;&#37096;&#20998;&#21306;&#21457;&#29616;&#31639;&#27861;&#65288;LDP&#65289;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#33258;&#21160;&#21464;&#37327;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;LDP&#26681;&#25454;&#19982;&#26333;&#20809;-&#32467;&#26524;&#23545;{X,Y}&#30456;&#20851;&#30340;&#23376;&#38598;&#23558;&#21464;&#37327;&#38598;&#21512;Z&#36827;&#34892;&#20998;&#21306;&#65292;&#24182;&#21306;&#20998;&#28151;&#28102;&#22240;&#32032;&#21644;&#20854;&#20182;&#21464;&#37327;&#31867;&#22411;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#27425;&#20108;&#27425;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#26377;&#38480;&#20808;&#39564;&#30693;&#35782;&#19979;&#33258;&#21160;&#21464;&#37327;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;&#32473;&#23450;&#19968;&#20010;{X,Y}&#30340;&#26333;&#20809;-&#32467;&#26524;&#23545;&#21644;&#19968;&#20010;&#26410;&#30693;&#22240;&#26524;&#32467;&#26500;&#30340;&#21464;&#37327;&#38598;&#21512;Z&#65292;&#23616;&#37096;&#20998;&#21306;&#21457;&#29616;&#65288;LDP&#65289;&#31639;&#27861;&#23558;Z&#21010;&#20998;&#25104;&#19982;{X,Y}&#30456;&#20851;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#21015;&#20030;&#20102;&#20219;&#24847;Z&#30340;8&#20010;&#31351;&#20030;&#19988;&#20114;&#19981;&#37325;&#22797;&#30340;&#20998;&#21306;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#20998;&#31867;&#27861;&#21306;&#20998;&#28151;&#28102;&#22240;&#32032;&#21644;&#20854;&#20182;&#21464;&#37327;&#31867;&#22411;&#12290;LDP&#30340;&#21160;&#26426;&#26159;&#26377;&#25928;&#30340;&#35843;&#25972;&#38598;&#35782;&#21035;&#65292;&#20294;&#36991;&#20813;&#20102;&#33258;&#21160;&#21464;&#37327;&#36873;&#25321;&#26041;&#27861;&#20013;&#24120;&#35265;&#30340;&#39044;&#22788;&#29702;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;LDP&#23545;&#20110;&#28385;&#36275;&#36275;&#22815;&#22270;&#24418;&#26465;&#20214;&#30340;&#20219;&#20309;Z&#37117;&#36820;&#22238;&#19968;&#20010;&#26377;&#25928;&#30340;&#35843;&#25972;&#38598;&#12290;&#22312;&#26356;&#24378;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20998;&#21306;&#26631;&#31614;&#30340;&#28176;&#36817;&#27491;&#30830;&#24615;&#12290;&#24635;&#29420;&#31435;&#24615;&#27979;&#35797;&#22312;|Z|&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#26159;&#20108;&#27425;&#30340;&#65292;&#32463;&#39564;&#19978;&#35266;&#23519;&#21040;&#27425;&#20108;&#27425;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#23545;&#29702;&#35770;&#20445;&#35777;&#36827;&#34892;&#20102;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses the problem of automated covariate selection under limited prior knowledge. Given an exposure-outcome pair {X,Y} and a variable set Z of unknown causal structure, the Local Discovery by Partitioning (LDP) algorithm partitions Z into subsets defined by their relation to {X,Y}. We enumerate eight exhaustive and mutually exclusive partitions of any arbitrary Z and leverage this taxonomy to differentiate confounders from other variable types. LDP is motivated by valid adjustment set identification, but avoids the pretreatment assumption commonly made by automated covariate selection methods. We provide theoretical guarantees that LDP returns a valid adjustment set for any Z that meets sufficient graphical conditions. Under stronger conditions, we prove that partition labels are asymptotically correct. Total independence tests is worst-case quadratic in |Z|, with sub-quadratic runtimes observed empirically. We numerically validate our theoretical guarantees on synthetic 
&lt;/p&gt;</description></item><item><title>Clover&#26159;&#19968;&#31181;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17807</link><description>&lt;p&gt;
Clover: &#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Clover: Closed-Loop Verifiable Code Generation. (arXiv:2310.17807v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17807
&lt;/p&gt;
&lt;p&gt;
Clover&#26159;&#19968;&#31181;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#26159;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#36825;&#20010;&#36235;&#21183;&#21487;&#33021;&#20250;&#23548;&#33268;&#35768;&#22810;&#19981;&#33391;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#24895;&#26223;&#65306;Clover&#33539;&#24335;&#65292;&#21363;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#65292;&#23427;&#23558;&#27491;&#30830;&#24615;&#26816;&#26597;&#31616;&#21270;&#20026;&#26356;&#21487;&#35775;&#38382;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#38382;&#39064;&#12290;&#22312;Clover&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26816;&#26597;&#22120;&#65292;&#23427;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#12290;&#35813;&#26816;&#26597;&#22120;&#20351;&#29992;&#20102;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#38598;&#25104;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#28857;&#65292;&#21363;Clover&#22312;&#19968;&#33268;&#24615;&#26816;&#26597;&#26041;&#38754;&#24212;&#35813;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#30001;&#25163;&#24037;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65288;CloverBench&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35843;&#26597;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#27880;&#37322;&#30340;Dafny&#31243;&#24207;&#65292;&#38590;&#24230;&#27700;&#24179;&#19982;&#25945;&#31185;&#20070;&#30456;&#24403;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
The use of large language models for code generation is a rapidly growing trend in software development. However, without effective methods for ensuring the correctness of generated code, this trend could lead to any number of undesirable outcomes. In this paper, we lay out a vision for addressing this challenge: the Clover paradigm, short for Closed-Loop Verifiable Code Generation, which reduces correctness checking to the more accessible problem of consistency checking. At the core of Clover lies a checker that performs consistency checks among code, docstrings, and formal annotations. The checker is implemented using a novel integration of formal verification tools and large language models. We provide a theoretical analysis to support our thesis that Clover should be effective at consistency checking. We also empirically investigate its feasibility on a hand-designed dataset (CloverBench) featuring annotated Dafny programs at a textbook level of difficulty. Experimental results sho
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#32467;&#26500;&#20026;&#22522;&#30784;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25552;&#31034;&#26041;&#27861;&#65288;SAP&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#35843;&#25972;&#38454;&#27573;&#37117;&#19968;&#33268;&#22320;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.17394</link><description>&lt;p&gt;
&#20197;&#32467;&#26500;&#20026;&#22522;&#30784;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Enhancing Graph Neural Networks with Structure-Based Prompt. (arXiv:2310.17394v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17394
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#32467;&#26500;&#20026;&#22522;&#30784;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25552;&#31034;&#26041;&#27861;&#65288;SAP&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#35843;&#25972;&#38454;&#27573;&#37117;&#19968;&#33268;&#22320;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23398;&#20064;&#22270;&#25968;&#25454;&#30340;&#35821;&#20041;&#26041;&#38754;&#20855;&#26377;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#8220;pre-train, prompt&#8221;&#22312;&#20351;&#29992;&#36739;&#23569;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;GNNs&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#33539;&#24335;&#30340;&#25104;&#21151;&#21487;&#20197;&#24402;&#22240;&#20110;&#39044;&#35757;&#32451;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#25552;&#31034;&#35843;&#25972;&#30340;&#26356;&#19968;&#33268;&#30340;&#30446;&#26631;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#21487;&#20197;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20013;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#26159;&#65292;&#22312;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#36890;&#24120;&#21033;&#29992;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#32780;&#22312;&#25552;&#31034;&#35843;&#25972;&#38454;&#27573;&#24573;&#35270;&#20102;&#32467;&#26500;&#20449;&#24687;&#30340;&#21033;&#29992;&#20197;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20197;&#32467;&#26500;&#20026;&#22522;&#30784;&#30340;GNNs&#25552;&#31034;&#26041;&#27861;&#65292;&#21363;SAP&#65292;&#23427;&#22312;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#35843;&#25972;&#38454;&#27573;&#37117;&#19968;&#33268;&#22320;&#21033;&#29992;&#20102;&#32467;&#26500;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SAP 1&#65289;&#37319;&#29992;&#20102;&#21452;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#23545;&#40784;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#32467;&#26500;&#30340;&#28508;&#22312;&#35821;&#20041;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are powerful in learning semantics of graph data. Recently, a new paradigm "pre-train, prompt" has shown promising results in adapting GNNs to various tasks with less supervised data. The success of such paradigm can be attributed to the more consistent objectives of pre-training and task-oriented prompt tuning, where the pre-trained knowledge can be effectively transferred to downstream tasks. However, an overlooked issue of existing studies is that the structure information of graph is usually exploited during pre-training for learning node representations, while neglected in the prompt tuning stage for learning task-specific parameters. To bridge this gap, we propose a novel structure-based prompting method for GNNs, namely SAP, which consistently exploits structure information in both pre-training and prompt tuning stages. In particular, SAP 1) employs a dual-view contrastive learning to align the latent semantic spaces of node attributes and graph stru
&lt;/p&gt;</description></item><item><title>Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#29616;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.17086</link><description>&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#19968;&#39033;&#19982;&#32447;&#24615;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17086
&lt;/p&gt;
&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#29616;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#23427;&#20204;&#26159;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Transformers&#21487;&#33021;&#36890;&#36807;&#20869;&#37096;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21363;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Transformers&#23398;&#20250;&#20102;&#23454;&#29616;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#20197;&#19978;&#19979;&#25991;&#32447;&#24615;&#22238;&#24402;&#20026;&#37325;&#28857;&#65292;&#23637;&#31034;&#20102;Transformers&#23398;&#20250;&#20102;&#23454;&#29616;&#19968;&#20010;&#38750;&#24120;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;&#20174;&#23454;&#35777;&#19978;&#26469;&#30475;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36830;&#32493;&#30340;Transformer&#23618;&#30340;&#39044;&#27979;&#19982;&#29275;&#39039;&#27861;&#30340;&#19981;&#21516;&#36845;&#20195;&#38750;&#24120;&#25509;&#36817;&#65292;&#27599;&#20010;&#20013;&#38388;&#23618;&#22823;&#33268;&#35745;&#31639;&#20102;3&#27425;&#36845;&#20195;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#25165;&#33021;&#21305;&#37197;&#39069;&#22806;&#30340;Transformer&#23618;&#65307;&#36825;&#34920;&#26126;Transformers&#20855;&#26377;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of conv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#22312;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#38706;&#30340;&#27934;&#23519;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#20943;&#23569;&#21512;&#25104;&#25968;&#25454;&#38598;&#29983;&#25104;&#20013;&#38544;&#31169;&#39118;&#38505;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.15524</link><description>&lt;p&gt;
&#20851;&#20110;&#31163;&#25955;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20869;&#22312;&#38544;&#31169;&#23646;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Inherent Privacy Properties of Discrete Denoising Diffusion Models. (arXiv:2310.15524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#22312;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#38706;&#30340;&#27934;&#23519;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#20943;&#23569;&#21512;&#25104;&#25968;&#25454;&#38598;&#29983;&#25104;&#20013;&#38544;&#31169;&#39118;&#38505;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#38382;&#39064;&#23548;&#33268;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#28608;&#22686;&#65292;&#25193;&#25955;&#27169;&#22411;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#65292;&#20294;&#22312;&#25552;&#20379;&#25968;&#23398;&#29305;&#24449;&#21270;&#20854;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#65288;DDMs&#65289;&#20869;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#24320;&#21019;&#24615;&#29702;&#35770;&#30740;&#31350;&#65292;&#29992;&#20110;&#31163;&#25955;&#25968;&#25454;&#38598;&#29983;&#25104;&#12290;&#23545;&#20110;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#27599;&#20010;&#23454;&#20363;&#24046;&#24322;&#38544;&#31169;&#65288;pDP&#65289;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#38416;&#26126;&#20102;&#32473;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#28508;&#22312;&#38544;&#31169;&#27844;&#38706;&#65292;&#20174;&#32780;&#20026;&#36890;&#36807;DDMs&#38477;&#20302;&#21512;&#25104;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#38544;&#31169;&#39118;&#38505;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#36824;&#34920;&#26126;&#65292;&#20351;&#29992;$s$&#20010;&#22823;&#23567;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#20174;$(\epsilon, \mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP&#21040;$(\epsilon, \mathcal{O}(\frac{1}{s\epsilon}))$-pDP&#30340;&#28608;&#22686;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy concerns have led to a surge in the creation of synthetic datasets, with diffusion models emerging as a promising avenue. Although prior studies have performed empirical evaluations on these models, there has been a gap in providing a mathematical characterization of their privacy-preserving capabilities. To address this, we present the pioneering theoretical exploration of the privacy preservation inherent in discrete diffusion models (DDMs) for discrete dataset generation. Focusing on per-instance differential privacy (pDP), our framework elucidates the potential privacy leakage for each data point in a given training dataset, offering insights into data preprocessing to reduce privacy risks of the synthetic dataset generation via DDMs. Our bounds also show that training with $s$-sized data points leads to a surge in privacy leakage from $(\epsilon, \mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP to $(\epsilon, \mathcal{O}(\frac{1}{s\epsilon}))$-pDP during the transition from the pu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;AdamQLR&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#36807;&#23558;K-FAC&#20013;&#30340;&#25216;&#26415;&#19982;Adam&#30340;&#26356;&#26032;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#32771;&#34385;&#20108;&#38454;&#25968;&#25454;&#19978;&#30340;Adam&#34892;&#20026;&#32780;&#24471;&#21040;&#21551;&#21457;&#12290;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;AdamQLR&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#25512;&#24191;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.14963</link><description>&lt;p&gt;
&#36890;&#36807;&#20108;&#38454;&#36879;&#38236;&#30475;Adam
&lt;/p&gt;
&lt;p&gt;
Adam through a Second-Order Lens. (arXiv:2310.14963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14963
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;AdamQLR&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#36807;&#23558;K-FAC&#20013;&#30340;&#25216;&#26415;&#19982;Adam&#30340;&#26356;&#26032;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#32771;&#34385;&#20108;&#38454;&#25968;&#25454;&#19978;&#30340;Adam&#34892;&#20026;&#32780;&#24471;&#21040;&#21551;&#21457;&#12290;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;AdamQLR&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#25512;&#24191;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#30740;&#31350;&#23384;&#22312;&#19968;&#31181;&#32039;&#24352;&#29366;&#24577;&#65292;&#21363;&#31532;&#19968;&#38454;&#26799;&#24230;&#27861;&#65288;&#22914;SGD&#21644;Adam&#65289;&#30340;&#35745;&#31639;&#25928;&#29575;&#19982;&#31532;&#20108;&#38454;&#26354;&#29575;&#27861;&#65288;&#22914;&#25311;&#29275;&#39039;&#26041;&#27861;&#21644;K-FAC&#65289;&#30340;&#29702;&#35770;&#25928;&#29575;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#25105;&#20204;&#35797;&#22270;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#21040;&#19968;&#20010;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#31639;&#27861;&#20013;&#12290;&#27880;&#24847;&#21040;&#20108;&#38454;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#31283;&#23450;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;&#22914;Levenberg-Marquardt&#38459;&#23612;&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;AdamQLR&#65306;&#19968;&#20010;&#23558;K-FAC&#20013;&#30340;&#38459;&#23612;&#21644;&#23398;&#20064;&#29575;&#36873;&#25321;&#25216;&#26415;&#19982;Adam&#25552;&#20986;&#30340;&#26356;&#26032;&#26041;&#21521;&#30456;&#32467;&#21512;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#32771;&#34385;Adam&#22312;&#20108;&#38454;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#32780;&#24471;&#21040;&#21551;&#21457;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35268;&#27169;&#30340;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;AdamQLR&#65292;&#22312;&#36816;&#34892;&#26102;&#38388;&#19982;&#31454;&#20105;&#24615;&#25512;&#24191;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research into optimisation for deep learning is characterised by a tension between the computational efficiency of first-order, gradient-based methods (such as SGD and Adam) and the theoretical efficiency of second-order, curvature-based methods (such as quasi-Newton methods and K-FAC). We seek to combine the benefits of both approaches into a single computationally-efficient algorithm. Noting that second-order methods often depend on stabilising heuristics (such as Levenberg-Marquardt damping), we propose AdamQLR: an optimiser combining damping and learning rate selection techniques from K-FAC (Martens and Grosse, 2015) with the update directions proposed by Adam, inspired by considering Adam through a second-order lens. We evaluate AdamQLR on a range of regression and classification tasks at various scales, achieving competitive generalisation performance vs runtime.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.12815</link><description>&lt;p&gt;
LLM-&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20316;&#21508;&#31181;&#31216;&#20026;LLM-&#38598;&#25104;&#24212;&#29992;&#30340;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#30340;&#21518;&#31471;&#12290;&#26368;&#36817;&#30340;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;LLM-&#38598;&#25104;&#24212;&#29992;&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#24694;&#24847;&#25351;&#20196;/&#25968;&#25454;&#27880;&#20837;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#36798;&#21040;&#25915;&#20987;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#26696;&#20363;&#30740;&#31350;&#65292;&#32570;&#20047;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21450;&#20854;&#38450;&#24481;&#30340;&#31995;&#32479;&#29702;&#35299;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#23558;&#30740;&#31350;&#35770;&#25991;&#21644;&#21338;&#23458;&#25991;&#31456;&#20013;&#35752;&#35770;&#30340;&#29616;&#26377;&#25915;&#20987;&#35270;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#25915;&#20987;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#38450;&#24481;&#30340;&#26694;&#26550;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#38450;&#21644;&#32531;&#35299;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#21644;Natural Questions&#65288;NQ&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.12516</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#23454;&#29616;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#24187;&#35273;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks. (arXiv:2310.12516v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#21644;Natural Questions&#65288;NQ&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#21644;&#26816;&#32034;&#22686;&#24378;&#25216;&#26415;&#38450;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#34913;&#37327;LLM&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#24037;&#35780;&#20272;&#25968;&#25454;&#23545;&#20110;&#35768;&#22810;&#20219;&#21153;&#21644;&#39046;&#22495;&#26469;&#35828;&#24182;&#19981;&#21487;&#29992;&#19988;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27844;&#28431;&#12290;&#21463;&#21040;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36890;&#36807;&#36866;&#24403;&#20462;&#25913;LLM&#22312;&#20854;&#20013;&#34920;&#29616;&#24544;&#23454;&#30340;&#29616;&#26377;&#25968;&#25454;&#26469;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;AutoDebug&#65292;&#20351;&#29992;&#25552;&#31034;&#38142;&#25509;&#26469;&#29983;&#25104;&#20197;&#38382;&#31572;&#31034;&#20363;&#24418;&#24335;&#30340;&#21487;&#36801;&#31227;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#24076;&#26395;&#20102;&#35299;&#36825;&#20123;&#31034;&#20363;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#35302;&#21457;&#20102;LLM&#30340;&#24187;&#35273;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#23454;&#29616;&#20102;AutoDebug&#65292;&#24182;&#23545;&#19968;&#20010;&#28909;&#38376;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;Natural Questions&#65288;NQ&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs.  We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-sour
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#23398;&#29983;&#36827;&#34892;&#22240;&#26524;&#19982;&#39044;&#27979;&#24615;&#23450;&#20301;&#30340;&#27604;&#36739;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23398;&#29983;&#21161;&#23398;&#37329;&#32493;&#31614;&#39046;&#22495;&#23454;&#39564;&#20013;&#40723;&#21169;&#23545;&#35937;&#36873;&#25321;&#30340;&#20215;&#20540;&#12290;&#36825;&#39033;&#22823;&#35268;&#27169;&#23454;&#22320;&#23454;&#39564;&#25581;&#31034;&#20102;&#23450;&#20301;&#24178;&#39044;&#23545;&#20110;&#19981;&#21516;&#23398;&#29983;&#30340;&#25928;&#26524;&#65292;&#20026;&#24178;&#39044;&#31574;&#30053;&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2310.08672</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#40723;&#21169;&#23545;&#35937;&#36873;&#25321;&#65306;&#22312;&#23398;&#29983;&#21161;&#23398;&#37329;&#32493;&#31614;&#39046;&#22495;&#23454;&#39564;&#20013;&#30340;&#22240;&#26524;&#19982;&#39044;&#27979;&#24615;&#30446;&#26631;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal. (arXiv:2310.08672v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08672
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#23398;&#29983;&#36827;&#34892;&#22240;&#26524;&#19982;&#39044;&#27979;&#24615;&#23450;&#20301;&#30340;&#27604;&#36739;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23398;&#29983;&#21161;&#23398;&#37329;&#32493;&#31614;&#39046;&#22495;&#23454;&#39564;&#20013;&#40723;&#21169;&#23545;&#35937;&#36873;&#25321;&#30340;&#20215;&#20540;&#12290;&#36825;&#39033;&#22823;&#35268;&#27169;&#23454;&#22320;&#23454;&#39564;&#25581;&#31034;&#20102;&#23450;&#20301;&#24178;&#39044;&#23545;&#20110;&#19981;&#21516;&#23398;&#29983;&#30340;&#25928;&#26524;&#65292;&#20026;&#24178;&#39044;&#31574;&#30053;&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24773;&#22659;&#19979;&#65292;&#24178;&#39044;&#21487;&#33021;&#23545;&#26576;&#20123;&#20154;&#27604;&#20854;&#20182;&#20154;&#26356;&#26377;&#25928;&#65292;&#22240;&#27492;&#23450;&#20301;&#24178;&#39044;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#35268;&#27169;&#24222;&#22823;&#30340;&#23454;&#22320;&#23454;&#39564;&#65288;&#36229;&#36807;53,000&#21517;&#22823;&#23398;&#29983;&#65289;&#26469;&#20998;&#26512;&#22312;&#23398;&#29983;&#21161;&#23398;&#37329;&#32493;&#31614;&#21069;&#20351;&#29992;&#8220;&#40723;&#21169;&#8221;&#31574;&#30053;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#23450;&#20301;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#20272;&#35745;&#24322;&#36136;&#22788;&#29702;&#25928;&#24212;&#30340;&#22240;&#26524;&#26862;&#26519;&#36827;&#34892;&#23450;&#20301;&#65292;&#24182;&#26681;&#25454;&#20272;&#35745;&#20986;&#30340;&#25317;&#26377;&#26368;&#39640;&#22788;&#29702;&#25928;&#24212;&#30340;&#23398;&#29983;&#26469;&#36827;&#34892;&#22788;&#29702;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35780;&#20272;&#20004;&#31181;&#26367;&#20195;&#30340;&#23450;&#20301;&#31574;&#30053;&#65292;&#19968;&#31181;&#26159;&#38024;&#23545;&#22312;&#27809;&#26377;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#21040;&#20302;&#21161;&#23398;&#37329;&#32493;&#31614;&#27010;&#29575;&#30340;&#23398;&#29983;&#65292;&#21478;&#19968;&#31181;&#26159;&#38024;&#23545;&#39044;&#27979;&#21040;&#39640;&#27010;&#29575;&#30340;&#23398;&#29983;&#12290;&#39044;&#27979;&#30340;&#22522;&#32447;&#32467;&#26524;&#24182;&#19981;&#26159;&#23450;&#20301;&#30340;&#29702;&#24819;&#26631;&#20934;&#65292;&#32780;&#19988;&#22312;&#20808;&#39564;&#19978;&#20063;&#19981;&#28165;&#26970;&#26159;&#20248;&#20808;&#32771;&#34385;&#20302;&#12289;&#39640;&#36824;&#26159;&#20013;&#38388;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many settings, interventions may be more effective for some individuals than others, so that targeting interventions may be beneficial. We analyze the value of targeting in the context of a large-scale field experiment with over 53,000 college students, where the goal was to use "nudges" to encourage students to renew their financial-aid applications before a non-binding deadline. We begin with baseline approaches to targeting. First, we target based on a causal forest that estimates heterogeneous treatment effects and then assigns students to treatment according to those estimated to have the highest treatment effects. Next, we evaluate two alternative targeting policies, one targeting students with low predicted probability of renewing financial aid in the absence of the treatment, the other targeting those with high probability. The predicted baseline outcome is not the ideal criterion for targeting, nor is it a priori clear whether to prioritize low, high, or intermediate predic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#25193;&#25955;&#27169;&#22411;&#65288;NDMs&#65289;&#65292;&#23427;&#26159;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#21487;&#20197;&#23450;&#20041;&#21644;&#23398;&#20064;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#38750;&#32447;&#24615;&#21464;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#26080;&#38656;&#27169;&#25311;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#21464;&#20998;&#30028;&#23545;NDMs&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21487;&#23398;&#20064;&#21464;&#25442;&#30340;NDMs&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08337</link><description>&lt;p&gt;
&#31070;&#32463;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Diffusion Models. (arXiv:2310.08337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#25193;&#25955;&#27169;&#22411;&#65288;NDMs&#65289;&#65292;&#23427;&#26159;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#21487;&#20197;&#23450;&#20041;&#21644;&#23398;&#20064;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#38750;&#32447;&#24615;&#21464;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#26080;&#38656;&#27169;&#25311;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#21464;&#20998;&#30028;&#23545;NDMs&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21487;&#23398;&#20064;&#21464;&#25442;&#30340;NDMs&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35768;&#22810;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#21482;&#20801;&#35768;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#32447;&#24615;&#36716;&#25442;&#65292;&#21463;&#21040;&#20102;&#19968;&#23450;&#30340;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26356;&#24191;&#27867;&#30340;&#21464;&#25442;&#23478;&#26063;&#21487;&#33021;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;&#29983;&#25104;&#20998;&#24067;&#65292;&#31616;&#21270;&#36870;&#36807;&#31243;&#24182;&#32553;&#23567;&#30495;&#23454;&#36127;&#23545;&#25968;&#20284;&#28982;&#21644;&#21464;&#20998;&#36817;&#20284;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25193;&#25955;&#27169;&#22411;&#65288;NDMs&#65289;&#65292;&#23427;&#26159;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#21487;&#20197;&#23450;&#20041;&#21644;&#23398;&#20064;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#38750;&#32447;&#24615;&#21464;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#26080;&#38656;&#27169;&#25311;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#21464;&#20998;&#30028;&#23545;NDMs&#36827;&#34892;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;NDMs&#30340;&#26102;&#38388;&#36830;&#32493;&#24418;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#29616;&#25104;&#30340;&#25968;&#20540;ODE&#21644;SDE&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#24555;&#36895;&#21487;&#38752;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#21487;&#23398;&#20064;&#21464;&#25442;&#30340;NDMs&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown remarkable performance on many generative tasks. Despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. In contrast, broader family of transformations can potentially help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. In this paper, we present Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimise NDMs using a variational bound in a simulation-free setting. Moreover, we derive a time-continuous formulation of NDMs, which allows fast and reliable inference using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the utility of NDMs with learnable transformations through experiments on standard image ge
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#40654;&#26364;&#25193;&#25955;&#36807;&#31243;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27969;&#24418;&#19978;&#26500;&#24314;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07216</link><description>&lt;p&gt;
&#22312;&#27969;&#24418;&#19978;&#36890;&#36807;&#40654;&#26364;&#25193;&#25955;&#36807;&#31243;&#30340;&#28151;&#21512;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes. (arXiv:2310.07216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07216
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#40654;&#26364;&#25193;&#25955;&#36807;&#31243;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27969;&#24418;&#19978;&#26500;&#24314;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#24314;&#27169;&#25968;&#25454;&#30340;&#20998;&#24067;&#23545;&#20110;&#26469;&#33258;&#19981;&#21516;&#31185;&#23398;&#39046;&#22495;&#30340;&#35768;&#22810;&#24212;&#29992;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27969;&#24418;&#19978;&#30340;&#29983;&#25104;&#27169;&#22411;&#23384;&#22312;&#30528;&#35745;&#31639;&#22797;&#26434;&#30340;&#25955;&#24230;&#25110;&#20381;&#36182;&#20110;&#28909;&#26680;&#30340;&#36817;&#20284;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#38480;&#21046;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#31616;&#21333;&#20960;&#20309;&#24418;&#29366;&#19978;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#38459;&#30861;&#20102;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#40654;&#26364;&#25193;&#25955;&#28151;&#21512;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#27969;&#24418;&#19978;&#26500;&#24314;&#29983;&#25104;&#36807;&#31243;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#32452;&#20197;&#31471;&#28857;&#26465;&#20214;&#25193;&#25955;&#36807;&#31243;&#20316;&#20026;&#28151;&#21512;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20808;&#21069;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#26041;&#27861;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#29983;&#25104;&#36807;&#31243;&#30340;&#29305;&#24615;&#26159;&#23427;&#30340;&#28418;&#31227;&#23548;&#21521;&#19982;&#27969;&#24418;&#30340;&#20960;&#20309;&#24418;&#29366;&#30456;&#23545;&#24212;&#30340;&#26368;&#21487;&#33021;&#30340;&#32456;&#28857;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#29992;&#20110;&#23398;&#20064;&#28151;&#21512;&#36807;&#31243;&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#19968;&#33324;&#27969;&#24418;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#24471;&#24456;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning the distribution of data on Riemannian manifolds is crucial for modeling data from non-Euclidean space, which is required by many applications from diverse scientific fields. Yet, existing generative models on manifolds suffer from expensive divergence computation or rely on approximations of heat kernel. These limitations restrict their applicability to simple geometries and hinder scalability to high dimensions. In this work, we introduce the Riemannian Diffusion Mixture, a principled framework for building a generative process on manifolds as a mixture of endpoint-conditioned diffusion processes instead of relying on the denoising approach of previous diffusion models, for which the generative process is characterized by its drift guiding toward the most probable endpoint with respect to the geometry of the manifold. We further propose a simple yet efficient training objective for learning the mixture process, that is readily applicable to general manifolds. Our method outp
&lt;/p&gt;</description></item><item><title>LLark&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#27169;&#24577;&#26550;&#26500;&#23454;&#29616;&#38899;&#20048;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#19978;&#21305;&#37197;&#25110;&#36229;&#20986;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#65292;&#22312;&#23383;&#24149;&#29983;&#25104;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#39640;&#24230;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.07160</link><description>&lt;p&gt;
LLark: &#19968;&#31181;&#29992;&#20110;&#38899;&#20048;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLark: A Multimodal Foundation Model for Music. (arXiv:2310.07160v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07160
&lt;/p&gt;
&lt;p&gt;
LLark&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#27169;&#24577;&#26550;&#26500;&#23454;&#29616;&#38899;&#20048;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#19978;&#21305;&#37197;&#25110;&#36229;&#20986;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#65292;&#22312;&#23383;&#24149;&#29983;&#25104;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#39640;&#24230;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#20855;&#26377;&#29420;&#29305;&#19988;&#22797;&#26434;&#30340;&#32467;&#26500;&#65292;&#23545;&#20110;&#19987;&#19994;&#20154;&#22763;&#21644;&#29616;&#26377;&#30340;AI&#31995;&#32479;&#26469;&#35828;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#30456;&#23545;&#20110;&#20854;&#20182;&#24418;&#24335;&#30340;&#38899;&#39057;&#21576;&#29616;&#20986;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLark&#65292;&#19968;&#31181;&#38024;&#23545;&#38899;&#20048;&#29702;&#35299;&#30340;&#25351;&#20196;&#35843;&#35856;&#22810;&#27169;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#36807;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#22686;&#24378;&#22810;&#26679;&#21270;&#30340;&#24320;&#28304;&#38899;&#20048;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#25351;&#20196;&#35843;&#35856;&#26684;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#26550;&#26500;&#29992;&#20110;LLark&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#22312;&#23545;&#19977;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#65288;&#38899;&#20048;&#29702;&#35299;&#12289;&#23383;&#24149;&#29983;&#25104;&#21644;&#25512;&#29702;&#65289;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#38899;&#20048;&#29702;&#35299;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#19978;&#19982;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#30456;&#21305;&#37197;&#25110;&#36229;&#20986;&#65292;&#24182;&#19988;&#22312;&#23383;&#24149;&#29983;&#25104;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#20154;&#31867;&#19982;&#27169;&#22411;&#30340;&#21709;&#24212;&#26174;&#31034;&#20986;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;LLark&#23436;&#20840;&#26159;&#26681;&#25454;&#24320;&#28304;&#38899;&#20048;&#25968;&#25454;&#21644;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLark, an instruction-tuned multimodal model for music understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLark, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, and reasoning), we show that our model matches or outperforms existing baselines in zero-shot generalization for music understanding, and that humans show a high degree of agreement with the model's responses in captioning and reasoning tasks. LLark is trained entirely from open-source music data and models, and we make our training code
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#35774;&#35745;&#21407;&#21017;&#30340;&#26032;&#30340;&#25191;&#34892;&#26694;&#26550;&#65306;&#27946;&#27700;&#21644;&#22238;&#22768;&#32593;&#32476;&#12290;&#36890;&#36807;&#27874;&#29366;&#28608;&#27963;&#27169;&#24335;&#65292;&#23427;&#21487;&#20197;&#22312;&#25972;&#20010;&#22270;&#20013;&#20256;&#36882;&#28040;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#26356;&#22823;&#30340;&#22270;&#24418;&#12290;</title><link>http://arxiv.org/abs/2310.06970</link><description>&lt;p&gt;
&#27946;&#27700;&#21644;&#22238;&#22768;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#20998;&#24067;&#24335;&#35745;&#31639;&#30340;&#31639;&#27861;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing. (arXiv:2310.06970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#35774;&#35745;&#21407;&#21017;&#30340;&#26032;&#30340;&#25191;&#34892;&#26694;&#26550;&#65306;&#27946;&#27700;&#21644;&#22238;&#22768;&#32593;&#32476;&#12290;&#36890;&#36807;&#27874;&#29366;&#28608;&#27963;&#27169;&#24335;&#65292;&#23427;&#21487;&#20197;&#22312;&#25972;&#20010;&#22270;&#20013;&#20256;&#36882;&#28040;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#26356;&#22823;&#30340;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#23398;&#20064;&#31639;&#27861;&#30340;&#33258;&#28982;&#36873;&#25321;&#12290;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#25277;&#35937;&#32780;&#22810;&#21151;&#33021;&#30340;&#22270;&#32467;&#26500;&#30452;&#25509;&#34920;&#31034;&#20219;&#21153;&#65292;&#24182;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#36755;&#20837;&#12290;&#36825;&#20026;&#31639;&#27861;&#30340;&#25193;&#23637;&#21644;&#22806;&#25512;&#21040;&#26356;&#22823;&#30340;&#22270;&#24418;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#37325;&#35201;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#25552;&#20986;&#20102;&#20004;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;i&#65289;&#22914;&#20309;&#20351;&#33410;&#28857;&#33021;&#22815;&#22312;&#32473;&#23450;&#30340;&#22270;&#20013;&#25910;&#38598;&#25152;&#38656;&#30340;&#20449;&#24687;&#65288;&#21363;&#8220;&#20449;&#24687;&#20132;&#25442;&#8221;&#65289;&#65292;&#21363;&#20351;&#33410;&#28857;&#36317;&#31163;&#24456;&#36828;&#65307;ii&#65289;&#25105;&#20204;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#25191;&#34892;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#35813;&#20449;&#24687;&#20132;&#25442;&#20197;&#20415;&#22806;&#25512;&#21040;&#26356;&#22823;&#30340;&#22270;&#22823;&#23567;&#65288;&#21363;&#8220;&#22806;&#25512;&#26102;&#30340;&#31639;&#27861;&#23545;&#40784;&#8221;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25191;&#34892;&#26694;&#26550;&#65292;&#21463;&#20998;&#24067;&#24335;&#31639;&#27861;&#30340;&#35774;&#35745;&#21407;&#21017;&#21551;&#21457;&#65306;&#27946;&#27700;&#21644;&#22238;&#22768;&#32593;&#32476;&#12290;&#23427;&#20197;&#27874;&#29366;&#28608;&#27963;&#27169;&#24335;&#23558;&#28040;&#24687;&#20256;&#25773;&#21040;&#25972;&#20010;&#22270;&#20013;&#65292;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#23454;&#20363;&#12290;&#36890;&#36807;&#23427;&#30340;&#31232;&#30095;&#20294;&#24182;&#34892;&#28608;&#27963;&#65292;&#21487;&#20197;&#35777;&#26126;&#23427;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks are a natural fit for learning algorithms. They can directly represent tasks through an abstract but versatile graph structure and handle inputs of different sizes. This opens up the possibility for scaling and extrapolation to larger graphs, one of the most important advantages of an algorithm. However, this raises two core questions i) How can we enable nodes to gather the required information in a given graph ($\textit{information exchange}$), even if is far away and ii) How can we design an execution framework which enables this information exchange for extrapolation to larger graph sizes ($\textit{algorithmic alignment for extrapolation}$). We propose a new execution framework that is inspired by the design principles of distributed algorithms: Flood and Echo Net. It propagates messages through the entire graph in a wave like activation pattern, which naturally generalizes to larger instances. Through its sparse but parallel activations it is provably more ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#26497;&#22823;&#20284;&#28982;&#30446;&#26631;&#65292;&#29992;&#20110;&#25913;&#36827;&#24322;&#24120;&#26816;&#27979;&#20013;&#24322;&#24120;&#20540;&#30340;&#20998;&#31163;&#31243;&#24230;&#12290;&#36890;&#36807;&#23558;&#27491;&#21017;&#21270;&#27969;&#25311;&#21512;&#21040;&#39044;&#35757;&#32451;&#30340;&#21028;&#21035;&#24615;&#29305;&#24449;&#65292;&#24182;&#26681;&#25454;&#23545;&#25968;&#20284;&#28982;&#24230;&#35780;&#20272;&#26469;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.06085</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#26497;&#22823;&#20284;&#28982;&#35757;&#32451;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Quantile-based Maximum Likelihood Training for Outlier Detection. (arXiv:2310.06085v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#26497;&#22823;&#20284;&#28982;&#30446;&#26631;&#65292;&#29992;&#20110;&#25913;&#36827;&#24322;&#24120;&#26816;&#27979;&#20013;&#24322;&#24120;&#20540;&#30340;&#20998;&#31163;&#31243;&#24230;&#12290;&#36890;&#36807;&#23558;&#27491;&#21017;&#21270;&#27969;&#25311;&#21512;&#21040;&#39044;&#35757;&#32451;&#30340;&#21028;&#21035;&#24615;&#29305;&#24449;&#65292;&#24182;&#26681;&#25454;&#23545;&#25968;&#20284;&#28982;&#24230;&#35780;&#20272;&#26469;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21028;&#21035;&#24615;&#23398;&#20064;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#20998;&#31867;&#39044;&#27979;&#30495;&#23454;&#30340;&#23545;&#35937;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#22312;&#24322;&#24120;&#20540;&#26041;&#38754;&#65292;&#23427;&#32463;&#24120;&#23548;&#33268;&#35823;&#25253;&#38451;&#24615;&#65292;&#36825;&#22312;&#33258;&#20027;&#39550;&#39542;&#21644;&#35270;&#39057;&#30417;&#35270;&#31995;&#32479;&#31561;&#24212;&#29992;&#20013;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#12290;&#20197;&#24448;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#23581;&#35797;&#21253;&#25324;&#20351;&#29992;&#23454;&#38469;&#24322;&#24120;&#20540;&#25968;&#25454;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#25110;&#32773;&#36890;&#36807;&#21512;&#25104;&#24322;&#24120;&#20540;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#20687;&#32032;&#31354;&#38388;&#20013;&#23545;&#20869;&#28857;&#36827;&#34892;&#26080;&#30417;&#30563;&#29983;&#25104;&#24314;&#27169;&#23545;&#20110;&#24322;&#24120;&#26816;&#27979;&#26174;&#31034;&#20986;&#26377;&#38480;&#30340;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#26497;&#22823;&#20284;&#28982;&#30446;&#26631;&#65292;&#29992;&#20110;&#23398;&#20064;&#20869;&#28857;&#20998;&#24067;&#65292;&#20197;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#25552;&#39640;&#24322;&#24120;&#20540;&#30340;&#20998;&#31163;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#27491;&#21017;&#21270;&#27969;&#25311;&#21512;&#21040;&#39044;&#35757;&#32451;&#30340;&#21028;&#21035;&#24615;&#29305;&#24449;&#65292;&#24182;&#26681;&#25454;&#35780;&#20272;&#30340;&#23545;&#25968;&#20284;&#28982;&#24230;&#26469;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#23454;&#39564;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discriminative learning effectively predicts true object class for image classification. However, it often results in false positives for outliers, posing critical concerns in applications like autonomous driving and video surveillance systems. Previous attempts to address this challenge involved training image classifiers through contrastive learning using actual outlier data or synthesizing outliers for self-supervised learning. Furthermore, unsupervised generative modeling of inliers in pixel space has shown limited success for outlier detection. In this work, we introduce a quantile-based maximum likelihood objective for learning the inlier distribution to improve the outlier separation during inference. Our approach fits a normalizing flow to pre-trained discriminative features and detects the outliers according to the evaluated log-likelihood. The experimental evaluation demonstrates the effectiveness of our method as it surpasses the performance of the state-of-the-art unsupervi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LSL-GFN&#30340;&#26032;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#28201;&#24230;&#26465;&#20214;&#19979;GFlowNets&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;GFlowNets&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02823</link><description>&lt;p&gt;
&#23398;&#20064;&#28201;&#24230;&#26465;&#20214;&#19979;&#23610;&#24230;&#26631;&#37327;&#21270;&#30340;GFlowNets
&lt;/p&gt;
&lt;p&gt;
Learning to Scale Logits for Temperature-Conditional GFlowNets. (arXiv:2310.02823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02823
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LSL-GFN&#30340;&#26032;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#28201;&#24230;&#26465;&#20214;&#19979;GFlowNets&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;GFlowNets&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GFlowNets&#26159;&#19968;&#31181;&#27010;&#29575;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#38543;&#26426;&#31574;&#30053;&#26469;&#39034;&#24207;&#29983;&#25104;&#32452;&#21512;&#32467;&#26500;&#65292;&#20363;&#22914;&#20998;&#23376;&#22270;&#12290;&#23427;&#20204;&#30340;&#35757;&#32451;&#30446;&#26631;&#26159;&#25353;&#27604;&#20363;&#37319;&#26679;&#20855;&#26377;&#30456;&#24212;&#28201;&#24230;&#35843;&#33410;&#30340;&#23545;&#35937;&#30340;&#22870;&#21169;&#12290;&#22312;GFlowNets&#20013;&#65292;&#28201;&#24230;&#26465;&#20214;&#19979;&#30340;GFlowNets&#20195;&#34920;&#20102;&#19968;&#31995;&#21015;&#30001;&#28201;&#24230;&#32034;&#24341;&#30340;&#31574;&#30053;&#65292;&#27599;&#20010;&#31574;&#30053;&#19982;&#30456;&#24212;&#30340;&#28201;&#24230;&#35843;&#33410;&#22870;&#21169;&#20989;&#25968;&#30456;&#20851;&#32852;&#12290;&#28201;&#24230;&#26465;&#20214;&#19979;&#30340;GFlowNets&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#36890;&#36807;&#35843;&#25972;&#28201;&#24230;&#26469;&#25511;&#21046;&#23545;GFlowNets&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#28201;&#24230;&#26465;&#20214;&#19979;&#23610;&#24230;&#26631;&#37327;&#21270;&#30340;GFlowNets&#65288;LSL-GFN&#65289;&#30340;&#26032;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#23427;&#26497;&#22823;&#22320;&#21152;&#36895;&#20102;&#28201;&#24230;&#26465;&#20214;&#19979;GFlowNets&#30340;&#35757;&#32451;&#12290;&#23427;&#22522;&#20110;&#19968;&#20010;&#24605;&#24819;&#65292;&#21363;&#20043;&#21069;&#25552;&#20986;&#30340;&#28201;&#24230;&#26465;&#20214;&#26041;&#27861;&#22312;&#28145;&#24230;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#24341;&#20837;&#20102;&#25968;&#20540;&#25361;&#25112;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#28201;&#24230;&#21487;&#33021;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
GFlowNets are probabilistic models that learn a stochastic policy that sequentially generates compositional structures, such as molecular graphs. They are trained with the objective of sampling such objects with probability proportional to the object's reward. Among GFlowNets, the temperature-conditional GFlowNets represent a family of policies indexed by temperature, and each is associated with the correspondingly tempered reward function. The major benefit of temperature-conditional GFlowNets is the controllability of GFlowNets' exploration and exploitation through adjusting temperature. We propose Learning to Scale Logits for temperature-conditional GFlowNets (LSL-GFN), a novel architectural design that greatly accelerates the training of temperature-conditional GFlowNets. It is based on the idea that previously proposed temperature-conditioning approaches introduced numerical challenges in the training of the deep network because different temperatures may give rise to very differe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24494;&#35843;&#25152;&#24102;&#26469;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25928;&#26524;&#12290;&#22312;&#31616;&#21270;&#30340;&#32447;&#24615;&#32593;&#32476;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24494;&#35843;&#25152;&#23545;&#29305;&#24449;&#20849;&#20139;&#21644;&#23398;&#20064;&#29305;&#23450;&#29305;&#24449;&#31232;&#30095;&#24615;&#30340;&#40723;&#21169;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#24494;&#35843;&#36807;&#31243;&#21516;&#26102;&#20855;&#26377;&#20869;&#26680;&#21644;&#29305;&#24449;&#23398;&#20064;&#30340;&#28151;&#21512;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#36824;&#21487;&#20197;&#23637;&#29616;&#19968;&#31181;&#23884;&#22871;&#29305;&#24449;&#23398;&#20064;&#34892;&#20026;&#65292;&#20351;&#20854;&#20559;&#21521;&#20110;&#25552;&#21462;&#19968;&#32452;&#31232;&#30095;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.02396</link><description>&lt;p&gt;
&#29992;&#36807;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Implicit regularization of multi-task learning and finetuning in overparameterized neural networks. (arXiv:2310.02396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24494;&#35843;&#25152;&#24102;&#26469;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25928;&#26524;&#12290;&#22312;&#31616;&#21270;&#30340;&#32447;&#24615;&#32593;&#32476;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24494;&#35843;&#25152;&#23545;&#29305;&#24449;&#20849;&#20139;&#21644;&#23398;&#20064;&#29305;&#23450;&#29305;&#24449;&#31232;&#30095;&#24615;&#30340;&#40723;&#21169;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#24494;&#35843;&#36807;&#31243;&#21516;&#26102;&#20855;&#26377;&#20869;&#26680;&#21644;&#29305;&#24449;&#23398;&#20064;&#30340;&#28151;&#21512;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#36824;&#21487;&#20197;&#23637;&#29616;&#19968;&#31181;&#23884;&#22871;&#29305;&#24449;&#23398;&#20064;&#34892;&#20026;&#65292;&#20351;&#20854;&#20559;&#21521;&#20110;&#25552;&#21462;&#19968;&#32452;&#31232;&#30095;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#24120;&#24120;&#20351;&#29992;&#35757;&#32451;&#36741;&#21161;&#20219;&#21153;&#30340;&#26041;&#27861;&#26469;&#26399;&#26395;&#23398;&#20064;&#21487;&#20197;&#37096;&#20998;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#24863;&#20852;&#36259;&#30340;&#20219;&#21153;&#19978;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#36741;&#21161;&#20219;&#21153;&#25152;&#20135;&#29983;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#21253;&#25324;&#21516;&#26102;&#23398;&#20064;&#65288;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;MTL&#65289;&#21644;&#20381;&#24207;&#23398;&#20064;&#65288;&#39044;&#35757;&#32451;&#21644;&#38543;&#21518;&#24494;&#35843;&#65292;PT+FT&#65289;&#12290;&#22312;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35757;&#32451;&#20004;&#23618;&#23545;&#35282;&#32447;&#32447;&#24615;&#32593;&#32476;&#30340;&#31616;&#21270;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19982;MTL&#21644;PT+FT&#30456;&#20851;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#24809;&#32602;&#65292;&#20004;&#32773;&#37117;&#40723;&#21169;&#20219;&#21153;&#20043;&#38388;&#30340;&#29305;&#24449;&#20849;&#20139;&#21644;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#30340;&#31232;&#30095;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#32593;&#32476;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#30830;&#23450;&#30340;&#20869;&#26680;&#65288;&#25110;&#8220;&#24816;&#24615;&#8221;&#65289;&#29366;&#24577;&#21644;&#29305;&#24449;&#23398;&#20064;&#65288;&#8220;&#20016;&#23500;&#8221;&#65289;&#29366;&#24577;&#20043;&#38388;&#20855;&#26377;&#28151;&#21512;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;PT+FT&#36824;&#21487;&#20197;&#23637;&#29616;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#23884;&#22871;&#29305;&#24449;&#23398;&#20064;&#8221;&#34892;&#20026;&#65292;&#35813;&#34892;&#20026;&#26080;&#27861;&#34987;&#20219;&#20309;&#29366;&#24577;&#25152;&#25429;&#25417;&#65292;&#20351;&#20854;&#20559;&#21521;&#20110;&#25552;&#21462;&#19968;&#32452;&#31232;&#30095;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is common in deep learning to train networks on auxiliary tasks with the expectation that the learning will transfer, at least partially, to another task of interest. In this work, we investigate the inductive biases that result from learning auxiliary tasks, either simultaneously (multi-task learning, MTL) or sequentially (pretraining and subsequent finetuning, PT+FT). In the simplified setting of two-layer diagonal linear networks trained with gradient descent, we identify implicit regularization penalties associated with MTL and PT+FT, both of which incentivize feature sharing between tasks and sparsity in learned task-specific features. Notably, our results imply that during finetuning, networks operate in a hybrid of the kernel (or "lazy") regime and the feature learning ("rich") regime identified in prior work. Moreover, PT+FT can exhibit a novel "nested feature learning" behavior not captured by either regime, which biases it to extract a sparse subset of the features learned
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21512;&#29702;&#30340;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#27604;&#21453;&#21521;&#20256;&#25773;&#26356;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.17348</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#29983;&#29289;&#21512;&#29702;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient Biologically Plausible Adversarial Training. (arXiv:2309.17348v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21512;&#29702;&#30340;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#27604;&#21453;&#21521;&#20256;&#25773;&#26356;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#25191;&#34892;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;ANNs&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#36890;&#36807;&#24494;&#23567;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#25200;&#21160;&#26469;&#25913;&#21464;&#36755;&#20837;&#65292;&#20174;&#32780;&#20005;&#37325;&#30772;&#22351;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20351;ANNs&#23545;&#36825;&#20123;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#23545;&#25239;&#35757;&#32451;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#38598;&#34987;&#28155;&#21152;&#20102;&#26679;&#26412;&#29992;&#20110;&#23545;&#25239;&#25915;&#20987;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#26159;&#22686;&#21152;&#20102;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#26159;&#38750;&#24120;&#35745;&#31639;&#28040;&#32791;&#39640;&#30340;&#12290;&#19982;ANNs&#19981;&#21516;&#65292;&#20154;&#31867;&#19981;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29983;&#29289;&#21512;&#29702;&#30340;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#27604;BP&#26356;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;BP&#21644;&#8220;Error to Pertu"&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Neural Networks (ANNs) trained with Backpropagation (BP) show astounding performance and are increasingly often used in performing our daily life tasks. However, ANNs are highly vulnerable to adversarial attacks, which alter inputs with small targeted perturbations that drastically disrupt the models' performance. The most effective method to make ANNs robust against these attacks is adversarial training, in which the training dataset is augmented with exemplary adversarial samples. Unfortunately, this approach has the drawback of increased training complexity since generating adversarial samples is very computationally demanding. In contrast to ANNs, humans are not susceptible to adversarial attacks. Therefore, in this work, we investigate whether biologically-plausible learning algorithms are more robust against adversarial attacks than BP. In particular, we present an extensive comparative analysis of the adversarial robustness of BP and \textit{Present the Error to Pertu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25439;&#22833;&#20989;&#25968;&#23545;&#31216;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#65292;&#24341;&#20837;&#30340;&#27599;&#20010;&#38236;&#20687;&#23545;&#31216;&#24615;&#37117;&#20250;&#23548;&#33268;&#19968;&#31181;&#32467;&#26500;&#24615;&#32422;&#26463;&#65292;&#21487;&#20197;&#29992;&#20110;&#23454;&#29616;&#31232;&#30095;&#24615;&#12289;&#20302;&#31209;&#24615;&#21644;&#21516;&#36136;&#38598;&#25104;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#32593;&#32476;&#22609;&#24615;&#20007;&#22833;&#21644;&#23849;&#28291;&#29616;&#35937;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.16932</link><description>&lt;p&gt;
&#23545;&#31216;&#24615;&#23548;&#33268;&#23398;&#20064;&#30340;&#32467;&#26500;&#24615;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Symmetry Leads to Structured Constraint of Learning. (arXiv:2309.16932v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25439;&#22833;&#20989;&#25968;&#23545;&#31216;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#65292;&#24341;&#20837;&#30340;&#27599;&#20010;&#38236;&#20687;&#23545;&#31216;&#24615;&#37117;&#20250;&#23548;&#33268;&#19968;&#31181;&#32467;&#26500;&#24615;&#32422;&#26463;&#65292;&#21487;&#20197;&#29992;&#20110;&#23454;&#29616;&#31232;&#30095;&#24615;&#12289;&#20302;&#31209;&#24615;&#21644;&#21516;&#36136;&#38598;&#25104;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#32593;&#32476;&#22609;&#24615;&#20007;&#22833;&#21644;&#23849;&#28291;&#29616;&#35937;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24120;&#35265;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#23545;&#31216;&#24615;&#22312;&#24403;&#20195;&#31070;&#32463;&#32593;&#32476;&#20013;&#24191;&#27867;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#25439;&#22833;&#20989;&#25968;&#23545;&#31216;&#24615;&#23545;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#27599;&#20010;&#38236;&#20687;&#23545;&#31216;&#24615;&#37117;&#20250;&#23548;&#33268;&#19968;&#31181;&#32467;&#26500;&#24615;&#32422;&#26463;&#65292;&#24403;&#26435;&#37325;&#34928;&#20943;&#25110;&#26799;&#24230;&#22122;&#22768;&#36739;&#22823;&#26102;&#65292;&#36825;&#31181;&#32422;&#26463;&#23558;&#25104;&#20026;&#39318;&#36873;&#35299;&#12290;&#20316;&#20026;&#30452;&#25509;&#25512;&#35770;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#37325;&#26032;&#32553;&#25918;&#23545;&#31216;&#24615;&#23548;&#33268;&#31232;&#30095;&#24615;&#65292;&#26059;&#36716;&#23545;&#31216;&#24615;&#23548;&#33268;&#20302;&#31209;&#24615;&#65292;&#32622;&#25442;&#23545;&#31216;&#24615;&#23548;&#33268;&#21516;&#36136;&#38598;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29702;&#35770;&#26694;&#26550;&#21487;&#20197;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;&#21644;&#21508;&#31181;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#21033;&#29992;&#23545;&#31216;&#24615;&#35774;&#35745;&#21487;&#24494;&#20998;&#23454;&#26045;&#30828;&#24615;&#32422;&#26463;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to common architecture designs, symmetries exist extensively in contemporary neural networks. In this work, we unveil the importance of the loss function symmetries in affecting, if not deciding, the learning behavior of machine learning models. We prove that every mirror symmetry of the loss function leads to a structured constraint, which becomes a favored solution when either the weight decay or gradient noise is large. As direct corollaries, we show that rescaling symmetry leads to sparsity, rotation symmetry leads to low rankness, and permutation symmetry leads to homogeneous ensembling. Then, we show that the theoretical framework can explain the loss of plasticity and various collapse phenomena in neural networks and suggest how symmetries can be used to design algorithms to enforce hard constraints in a differentiable way.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20174;&#20960;&#20309;&#32467;&#26500;&#21040;&#25299;&#25169;&#32467;&#26500;&#30340;&#25277;&#35937;&#27493;&#39588;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#34920;&#24449;&#30456;&#20284;&#20998;&#26512;&#26041;&#27861;&#65288;tRSA&#65289;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#22320;&#29702;&#25299;&#25169;&#25688;&#35201;&#32479;&#35745;&#37327;&#23545;&#22823;&#33041;&#34920;&#24449;&#36827;&#34892;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.11028</link><description>&lt;p&gt;
&#31070;&#32463;&#34920;&#24449;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Topology and Geometry of Neural Representations. (arXiv:2309.11028v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20174;&#20960;&#20309;&#32467;&#26500;&#21040;&#25299;&#25169;&#32467;&#26500;&#30340;&#25277;&#35937;&#27493;&#39588;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#34920;&#24449;&#30456;&#20284;&#20998;&#26512;&#26041;&#27861;&#65288;tRSA&#65289;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#22320;&#29702;&#25299;&#25169;&#25688;&#35201;&#32479;&#35745;&#37327;&#23545;&#22823;&#33041;&#34920;&#24449;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31185;&#23398;&#25152;&#20851;&#24515;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#34920;&#24449;&#24863;&#30693;&#21644;&#35748;&#30693;&#20869;&#23481;&#30340;&#22823;&#33041;&#34920;&#24449;&#12290;&#19968;&#20010;&#29702;&#24819;&#30340;&#34920;&#24449;&#24212;&#35813;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#30340;&#21151;&#33021;&#21306;&#22495;&#65292;&#24182;&#19988;&#23545;&#22122;&#22768;&#21644;&#20010;&#20307;&#22823;&#33041;&#30340;&#29305;&#24322;&#24615;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#19982;&#35745;&#31639;&#24046;&#24322;&#30456;&#23545;&#24212;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#34920;&#24449;&#20960;&#20309;&#32467;&#26500;&#26469;&#34920;&#24449;&#22823;&#33041;&#34920;&#24449;&#65292;&#20960;&#20309;&#32467;&#26500;&#30001;&#34920;&#24449;&#19981;&#30456;&#20284;&#30697;&#38453;&#65288;RDM&#65289;&#23450;&#20041;&#65292;RDM&#26159;&#19968;&#20010;&#25688;&#35201;&#32479;&#35745;&#37327;&#65292;&#25688;&#35201;&#20102;&#20010;&#20307;&#31070;&#32463;&#20803;&#65288;&#25110;&#21709;&#24212;&#36890;&#36947;&#65289;&#30340;&#20316;&#29992;&#65292;&#24182;&#34920;&#24449;&#20102;&#21050;&#28608;&#30340;&#21487;&#36776;&#21035;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#20174;&#20960;&#20309;&#32467;&#26500;&#21040;&#22823;&#33041;&#34920;&#24449;&#25299;&#25169;&#30340;&#25277;&#35937;&#27493;&#39588;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25299;&#25169;&#34920;&#24449;&#30456;&#20284;&#20998;&#26512;&#65288;tRSA&#65289;&#65292;&#23427;&#26159;&#34920;&#24449;&#30456;&#20284;&#20998;&#26512;&#65288;RSA&#65289;&#30340;&#19968;&#31181;&#25193;&#23637;&#65292;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#22320;&#29702;&#25299;&#25169;&#25688;&#35201;&#32479;&#35745;&#37327;&#65292;&#23558;RDM&#36827;&#34892;&#27867;&#21270;&#20197;&#34920;&#24449;&#25299;&#25169;&#32467;&#26500;&#24182;&#20943;&#24369;&#20960;&#20309;&#32467;&#26500;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central question for neuroscience is how to characterize brain representations of perceptual and cognitive content. An ideal characterization should distinguish different functional regions with robustness to noise and idiosyncrasies of individual brains that do not correspond to computational differences. Previous studies have characterized brain representations by their representational geometry, which is defined by the representational dissimilarity matrix (RDM), a summary statistic that abstracts from the roles of individual neurons (or responses channels) and characterizes the discriminability of stimuli. Here we explore a further step of abstraction: from the geometry to the topology of brain representations. We propose topological representational similarity analysis (tRSA), an extension of representational similarity analysis (RSA) that uses a family of geo-topological summary statistics that generalizes the RDM to characterize the topology while de-emphasizing the geometry. 
&lt;/p&gt;</description></item><item><title>PAGER&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#25925;&#38556;&#20998;&#26512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#19968;&#33268;&#20998;&#25968;&#65292;&#23545;&#26679;&#26412;&#36827;&#34892;&#20998;&#32452;&#24182;&#25552;&#20379;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.10977</link><description>&lt;p&gt;
PAGER: &#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#25925;&#38556;&#20998;&#26512;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PAGER: A Framework for Failure Analysis of Deep Regression Models. (arXiv:2309.10977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10977
&lt;/p&gt;
&lt;p&gt;
PAGER&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#25925;&#38556;&#20998;&#26512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#19968;&#33268;&#20998;&#25968;&#65292;&#23545;&#26679;&#26412;&#36827;&#34892;&#20998;&#32452;&#24182;&#25552;&#20379;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#37096;&#32626;AI&#27169;&#22411;&#38656;&#35201;&#20027;&#21160;&#26816;&#27979;&#28508;&#22312;&#30340;&#39044;&#27979;&#25925;&#38556;&#65292;&#20197;&#38450;&#27490;&#26114;&#36149;&#30340;&#38169;&#35823;&#12290;&#23613;&#31649;&#20998;&#31867;&#38382;&#39064;&#30340;&#25925;&#38556;&#26816;&#27979;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#34920;&#24449;&#25925;&#38556;&#27169;&#24335;&#26356;&#21152;&#22797;&#26434;&#19988;&#36739;&#23569;&#30740;&#31350;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#25110;&#19982;&#35757;&#32451;&#20998;&#24067;&#30340;&#29305;&#24449;&#19981;&#19968;&#33268;&#26469;&#34920;&#24449;&#27169;&#22411;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#20165;&#38752;&#19981;&#30830;&#23450;&#24615;&#26080;&#27861;&#20934;&#30830;&#34920;&#24449;&#25925;&#38556;&#65292;&#36825;&#26159;&#30001;&#20110;&#21508;&#31181;&#35823;&#24046;&#28304;&#30340;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PAGER&#65288;&#22238;&#24402;&#22120;&#30340;&#21407;&#21017;&#24615;&#27867;&#21270;&#38169;&#35823;&#20998;&#26512;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#26816;&#27979;&#21644;&#34920;&#24449;&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#25925;&#38556;&#30340;&#26694;&#26550;&#12290;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#28145;&#24230;&#27169;&#22411;&#38170;&#23450;&#24605;&#24819;&#65292;PAGER&#23558;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#21644;&#26032;&#39062;&#30340;&#12289;&#20114;&#34917;&#30340;&#19981;&#19968;&#33268;&#20998;&#25968;&#32479;&#19968;&#36215;&#26469;&#65292;&#23558;&#26679;&#26412;&#32452;&#32455;&#25104;&#19981;&#21516;&#30340;&#39118;&#38505;&#21306;&#22495;&#65292;&#20174;&#32780;&#25552;&#20379;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe deployment of AI models requires proactive detection of potential prediction failures to prevent costly errors. While failure detection in classification problems has received significant attention, characterizing failure modes in regression tasks is more complicated and less explored. Existing approaches rely on epistemic uncertainties or feature inconsistency with the training distribution to characterize model risk. However, we show that uncertainties are necessary but insufficient to accurately characterize failure, owing to the various sources of error. In this paper, we propose PAGER (Principled Analysis of Generalization Errors in Regressors), a framework to systematically detect and characterize failures in deep regression models. Built upon the recently proposed idea of anchoring in deep models, PAGER unifies both epistemic uncertainties and novel, complementary non-conformity scores to organize samples into different risk regimes, thereby providing a comprehensive analys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEL&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21040;20&#20010;&#35757;&#32451;&#36718;&#25968;&#20869;&#26377;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#24120;&#29992;&#24494;&#35843;&#26041;&#27861;&#23548;&#33268;&#23614;&#37096;&#31867;&#21035;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10019</link><description>&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#30340;&#38271;&#23614;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Long-Tailed Recognition. (arXiv:2309.10019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEL&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21040;20&#20010;&#35757;&#32451;&#36718;&#25968;&#20869;&#26377;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#24120;&#29992;&#24494;&#35843;&#26041;&#27861;&#23548;&#33268;&#23614;&#37096;&#31867;&#21035;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#20986;&#29616;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;CLIP&#65289;&#65292;"&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;"&#33539;&#20363;&#22312;&#35299;&#20915;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#34429;&#28982;&#20808;&#21069;&#30740;&#31350;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#36825;&#20123;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24076;&#26395;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#36718;&#25968;&#25110;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20445;&#25345;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEL&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21040;20&#20010;&#35757;&#32451;&#36718;&#25968;&#20869;&#26377;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#39318;&#20808;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#65292;&#24120;&#29992;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;&#20363;&#22914;&#20840;&#38754;&#24494;&#35843;&#21644;&#20998;&#31867;&#22120;&#24494;&#35843;&#65289;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#23614;&#37096;&#31867;&#21035;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;PEL&#37319;&#29992;&#20102;&#29616;&#26377;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#65292;&#24341;&#20837;&#20102;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The "pre-training and fine-tuning" paradigm in addressing long-tailed recognition tasks has sparked significant interest since the emergence of large vision-language models like the contrastive language-image pre-training (CLIP). While previous studies have shown promise in adapting pre-trained models for these tasks, they often undesirably require extensive training epochs or additional training data to maintain good performance. In this paper, we propose PEL, a fine-tuning method that can effectively adapt pre-trained models to long-tailed recognition tasks in fewer than 20 epochs without the need for extra data. We first empirically find that commonly used fine-tuning methods, such as full fine-tuning and classifier fine-tuning, suffer from overfitting, resulting in performance deterioration on tail classes. To mitigate this issue, PEL introduces a small number of task-specific parameters by adopting the design of any existing parameter-efficient fine-tuning method. Additionally, to
&lt;/p&gt;</description></item><item><title>SortedNet&#26159;&#19968;&#31181;&#24191;&#20041;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25490;&#24207;&#35757;&#32451;&#21644;&#27010;&#29575;&#26041;&#24335;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21508;&#20010;&#32500;&#24230;&#19978;&#23454;&#29616;&#39640;&#25928;&#21160;&#24577;&#25512;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#22312;&#27169;&#22411;&#25512;&#26029;&#36807;&#31243;&#20013;&#28789;&#27963;&#36866;&#24212;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#23376;&#32593;&#32476;&#30340;&#25968;&#37327;&#25193;&#23637;&#21040;&#25968;&#30334;&#20010;&#12290;</title><link>http://arxiv.org/abs/2309.00255</link><description>&lt;p&gt;
SortedNet&#65292;&#27599;&#20010;&#32593;&#32476;&#37117;&#26377;&#33258;&#24049;&#30340;&#20301;&#32622;&#65306;&#38754;&#21521;&#35757;&#32451;&#22810;&#23545;&#19968;&#31070;&#32463;&#32593;&#32476;&#30340;&#24191;&#20041;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks. (arXiv:2309.00255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00255
&lt;/p&gt;
&lt;p&gt;
SortedNet&#26159;&#19968;&#31181;&#24191;&#20041;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25490;&#24207;&#35757;&#32451;&#21644;&#27010;&#29575;&#26041;&#24335;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21508;&#20010;&#32500;&#24230;&#19978;&#23454;&#29616;&#39640;&#25928;&#21160;&#24577;&#25512;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#22312;&#27169;&#22411;&#25512;&#26029;&#36807;&#31243;&#20013;&#28789;&#27963;&#36866;&#24212;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#23376;&#32593;&#32476;&#30340;&#25968;&#37327;&#25193;&#23637;&#21040;&#25968;&#30334;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#22823;&#65292;&#22914;&#20309;&#22312;&#20869;&#23384;&#21644;&#35745;&#31639;&#32422;&#26463;&#19979;&#25214;&#21040;&#26368;&#20248;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21644;&#32452;&#25104;&#37096;&#20998;&#36890;&#24120;&#20801;&#35768;&#20197;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#24182;&#19981;&#24847;&#35782;&#21040;&#36825;&#31181;&#27169;&#22359;&#21270;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#32570;&#20047;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#36866;&#24212;&#27169;&#22411;&#35745;&#31639;&#36127;&#36733;&#30340;&#28789;&#27963;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SortedNet&#65292;&#36825;&#26159;&#19968;&#31181;&#24191;&#20041;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#32500;&#24230;&#19978;&#30340;&#20869;&#22312;&#27169;&#22359;&#21270;&#29305;&#24615;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#21160;&#24577;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#23884;&#22871;&#32467;&#26500;&#30340;&#23376;&#27169;&#22411;&#21644;&#20027;&#27169;&#22411;&#20849;&#20139;&#21442;&#25968;&#30340;&#26041;&#24335;&#65292;&#24182;&#20197;&#25490;&#24207;&#21644;&#27010;&#29575;&#30340;&#26041;&#24335;&#35757;&#32451;&#23427;&#20204;&#12290;&#36825;&#31181;&#23376;&#32593;&#32476;&#30340;&#25490;&#24207;&#35757;&#32451;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#19968;&#36718;&#35757;&#32451;&#20013;&#23558;&#23376;&#32593;&#32476;&#30340;&#25968;&#37327;&#25193;&#23637;&#21040;&#25968;&#30334;&#20010;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26356;&#26032;&#26041;&#26696;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#23376;&#32593;&#32476;&#30340;&#35745;&#31639;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of deep learning models continues to grow, finding optimal models under memory and computation constraints becomes increasingly more important. Although usually the architecture and constituent building blocks of neural networks allow them to be used in a modular way, their training process is not aware of this modularity. Consequently, conventional neural network training lacks the flexibility to adapt the computational load of the model during inference. This paper proposes SortedNet, a generalized and scalable solution to harness the inherent modularity of deep neural networks across various dimensions for efficient dynamic inference. Our training considers a nested architecture for the sub-models with shared parameters and trains them together with the main model in a sorted and probabilistic manner. This sorted training of sub-networks enables us to scale the number of sub-networks to hundreds using a single round of training. We utilize a novel updating scheme during 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#25506;&#32034;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32500;Bellman&#31639;&#23376;&#20013;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#65292;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.13049</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#25506;&#32034;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian Exploration Networks. (arXiv:2308.13049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#25506;&#32034;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32500;Bellman&#31639;&#23376;&#20013;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#65292;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#20026;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#20248;&#38597;&#30340;&#26041;&#27861;&#12290;&#26368;&#26174;&#33879;&#30340;&#26159;&#65292;&#36125;&#21494;&#26031;&#20195;&#29702;&#19981;&#20250;&#38754;&#20020;&#39057;&#29575;&#26041;&#27861;&#30340;&#25506;&#32034;/&#24320;&#21457;&#22256;&#22659;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#38382;&#39064;&#12290;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23398;&#20064;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#36825;&#22312;&#29609;&#20855;&#39046;&#22495;&#20013;&#26159;&#21487;&#35745;&#31639;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#19968;&#32500;Bellman&#31639;&#23376;&#20013;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#32780;&#19981;&#26159;&#22312;&#39640;&#32500;&#29366;&#24577;&#36716;&#31227;&#20998;&#24067;&#20013;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#29616;&#26377;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#35201;&#20040;&#19981;&#36890;&#36807;MDP&#20256;&#25773;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#35201;&#20040;&#22312;&#19968;&#32452;&#35821;&#22659;&#31574;&#30053;&#20013;&#20248;&#21270;&#32780;&#19981;&#26159;&#25152;&#26377;&#21382;&#21490;&#26465;&#20214;&#31574;&#30053;&#12290;&#36825;&#20004;&#20010;&#36817;&#20284;&#24471;&#21040;&#30340;&#31574;&#30053;&#21487;&#33021;&#26159;&#20219;&#24847;&#36125;&#21494;&#26031;&#27425;&#20248;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#25506;&#32034;&#32593;&#32476;&#65288;Bayesian exploration network&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian reinforcement learning (RL) offers a principled and elegant approach for sequential decision making under uncertainty. Most notably, Bayesian agents do not face an exploration/exploitation dilemma, a major pathology of frequentist methods. A key challenge for Bayesian RL is the computational complexity of learning Bayes-optimal policies, which is only tractable in toy domains. In this paper we propose a novel model-free approach to address this challenge. Rather than modelling uncertainty in high-dimensional state transition distributions as model-based approaches do, we model uncertainty in a one-dimensional Bellman operator. Our theoretical analysis reveals that existing model-free approaches either do not propagate epistemic uncertainty through the MDP or optimise over a set of contextual policies instead of all history-conditioned policies. Both approximations yield policies that can be arbitrarily Bayes-suboptimal. To overcome these issues, we introduce the Bayesian explo
&lt;/p&gt;</description></item><item><title>HoSNN&#26159;&#19968;&#31181;&#23545;&#25239;&#24615;&#31283;&#24577;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#21457;&#25918;&#38408;&#20540;&#30340;&#28183;&#28431;&#25972;&#21512;&#19982;&#21457;&#25918;&#65288;TA-LIF&#65289;&#31070;&#32463;&#20803;&#27169;&#22411;&#26469;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#22312;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#19979;&#20445;&#25252;&#20854;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10373</link><description>&lt;p&gt;
HoSNN: &#20855;&#26377;&#33258;&#36866;&#24212;&#21457;&#25918;&#38408;&#20540;&#30340;&#23545;&#25239;&#24615;&#31283;&#24577;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HoSNN: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds. (arXiv:2308.10373v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10373
&lt;/p&gt;
&lt;p&gt;
HoSNN&#26159;&#19968;&#31181;&#23545;&#25239;&#24615;&#31283;&#24577;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#21457;&#25918;&#38408;&#20540;&#30340;&#28183;&#28431;&#25972;&#21512;&#19982;&#21457;&#25918;&#65288;TA-LIF&#65289;&#31070;&#32463;&#20803;&#27169;&#22411;&#26469;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#22312;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#19979;&#20445;&#25252;&#20854;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22312;&#39640;&#25928;&#21644;&#24378;&#22823;&#30340;&#31070;&#32463;&#21551;&#21457;&#24335;&#35745;&#31639;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20182;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#19968;&#26679;&#65292;SNNs&#38754;&#20020;&#30528;&#23545;&#25239;&#25915;&#20987;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20174;&#31070;&#32463;&#24658;&#31283;&#24615;&#20013;&#27762;&#21462;&#28789;&#24863;&#30340;&#30740;&#31350;&#65292;&#20197;&#24320;&#21457;&#19968;&#31181;&#20223;&#29983;&#35299;&#20915;&#26041;&#26696;&#65292;&#26469;&#24212;&#23545;SNNs&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#21457;&#25918;&#38408;&#20540;&#30340;&#28183;&#28431;&#25972;&#21512;&#19982;&#21457;&#25918;&#65288;TA-LIF&#65289;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#25105;&#20204;&#37319;&#29992;&#23427;&#26469;&#26500;&#24314;&#25152;&#25552;&#20986;&#30340;&#23545;&#25239;&#24615;&#31283;&#24577;SNN&#65288;HoSNN&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;LIF&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;TA-LIF&#27169;&#22411;&#34701;&#20837;&#20102;&#33258;&#31283;&#23450;&#21160;&#24577;&#38408;&#20540;&#26426;&#21046;&#65292;&#38480;&#21046;&#23545;&#25239;&#24615;&#22122;&#22768;&#30340;&#20256;&#25773;&#65292;&#24182;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20445;&#25252;HoSNN&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#38416;&#26126;TA-LIF&#31070;&#32463;&#20803;&#30340;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;&#65292;&#24378;&#35843;&#23427;&#20204;&#22312;&#36755;&#20837;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#21331;&#36234;&#21160;&#24577;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) offer promise for efficient and powerful neurally inspired computation. Common to other types of neural networks, however, SNNs face the severe issue of vulnerability to adversarial attacks. We present the first study that draws inspiration from neural homeostasis to develop a bio-inspired solution that counters the susceptibilities of SNNs to adversarial onslaughts. At the heart of our approach is a novel threshold-adapting leaky integrate-and-fire (TA-LIF) neuron model, which we adopt to construct the proposed adversarially robust homeostatic SNN (HoSNN). Distinct from traditional LIF models, our TA-LIF model incorporates a self-stabilizing dynamic thresholding mechanism, curtailing adversarial noise propagation and safeguarding the robustness of HoSNNs in an unsupervised manner. Theoretical analysis is presented to shed light on the stability and convergence properties of the TA-LIF neurons, underscoring their superior dynamic robustness under input di
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.00264</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00264
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#22810;&#20010;&#27169;&#24577;&#36873;&#25321;&#21644;&#34701;&#21512;&#29305;&#24449;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#32452;&#21512;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20197;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#24182;&#19988;&#30740;&#31350;&#20102;&#22810;&#25439;&#22833;&#35757;&#32451;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#30830;&#23450;&#20102;&#19982;&#23376;&#32593;&#24615;&#33021;&#30456;&#20851;&#30340;&#26377;&#29992;&#21457;&#29616;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;CMU-MOSI&#12289;CMU-MOSEI&#21644;CH-SIMS&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#22810;&#27169;&#24577;&#29305;&#24449;&#21487;&#20197;&#25913;&#21892;&#21333;&#27169;&#24577;&#27979;&#35797;&#65292;&#24182;&#19988;&#22522;&#20110;&#25968;&#25454;&#38598;&#27880;&#37322;&#27169;&#24335;&#35774;&#35745;&#34701;&#21512;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#24773;&#24863;&#26816;&#27979;&#30340;&#20248;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#39564;&#24471;&#20986;&#20102;&#19977;&#20010;&#32467;&#35770;&#65306;&#27809;&#26377;&#36890;&#29992;&#30340;&#20219;&#21153;&#37319;&#26679;&#31574;&#30053;&#33021;&#20445;&#35777;&#20803;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65307;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#27424;&#25311;&#21512;&#25110;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#65307;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21463;&#21040;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;ASr&#65292;&#23427;&#21033;&#29992;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#26469;&#37319;&#26679;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#21644;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;ASr&#12290;&#22823;&#37327;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;ASr&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08924</link><description>&lt;p&gt;
&#23398;&#20064;&#37319;&#26679;&#20219;&#21153;&#29992;&#20110;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Sample Tasks for Meta Learning. (arXiv:2307.08924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08924
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#24471;&#20986;&#20102;&#19977;&#20010;&#32467;&#35770;&#65306;&#27809;&#26377;&#36890;&#29992;&#30340;&#20219;&#21153;&#37319;&#26679;&#31574;&#30053;&#33021;&#20445;&#35777;&#20803;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65307;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#27424;&#25311;&#21512;&#25110;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#65307;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21463;&#21040;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;ASr&#65292;&#23427;&#21033;&#29992;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#26469;&#37319;&#26679;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#21644;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;ASr&#12290;&#22823;&#37327;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;ASr&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#21508;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#12289;&#20219;&#21153;&#37319;&#26679;&#22120;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#26412;&#25991;&#24471;&#20986;&#20102;&#19977;&#20010;&#32467;&#35770;&#12290;&#39318;&#20808;&#65292;&#27809;&#26377;&#36890;&#29992;&#30340;&#20219;&#21153;&#37319;&#26679;&#31574;&#30053;&#33021;&#20445;&#35777;&#20803;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#27424;&#25311;&#21512;&#25110;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21463;&#21040;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#37319;&#26679;&#22120;&#65288;ASr&#65289;&#12290;ASr&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;&#65292;&#23427;&#21033;&#29992;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#26469;&#37319;&#26679;&#20219;&#21153;&#12290;&#20026;&#20102;&#20248;&#21270;ASr&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#22823;&#37327;&#30340;&#23454;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;ASr&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through experiments on various meta-learning methods, task samplers, and few-shot learning tasks, this paper arrives at three conclusions. Firstly, there are no universal task sampling strategies to guarantee the performance of meta-learning models. Secondly, task diversity can cause the models to either underfit or overfit during training. Lastly, the generalization performance of the models are influenced by task divergence, task entropy, and task difficulty. In response to these findings, we propose a novel task sampler called Adaptive Sampler (ASr). ASr is a plug-and-play task sampler that takes task divergence, task entropy, and task difficulty to sample tasks. To optimize ASr, we rethink and propose a simple and general meta-learning algorithm. Finally, a large number of empirical experiments demonstrate the effectiveness of the proposed ASr.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03887</link><description>&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#28145;&#24230;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#28165;&#26970;&#22320;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#24402;&#22240;&#20110;&#25968;&#25454;&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#65288;ProtoPNet&#65289;&#65292;&#23427;&#22522;&#20110;&#36755;&#20837;&#30340;&#26377;&#24847;&#20041;&#37096;&#20998;&#26469;&#23581;&#35797;&#20998;&#31867;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32463;&#24120;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#21551;&#21457;&#65292;&#36890;&#36807;&#22312;CUB-200-2011&#25968;&#25454;&#38598;&#19978;&#25910;&#38598;&#20154;&#31867;&#21407;&#22411;&#36136;&#37327;&#30340;1-5&#20998;&#32423;&#27880;&#37322;&#65292;&#26500;&#24314;&#19968;&#20010;&#23398;&#20064;&#35782;&#21035;&#38750;&#34394;&#20551;&#21407;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#65288;R3-ProtoPNet&#65289;&#65292;&#35813;&#32593;&#32476;&#22312;ProtoPNet&#35757;&#32451;&#24490;&#29615;&#20013;&#22686;&#21152;&#20102;&#19977;&#20010;&#39069;&#22806;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the prototypical part network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, this method often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. In place of a full RL update, we propose the reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The fir
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;ProgSyn&#65292;&#31532;&#19968;&#20010;&#21487;&#32534;&#31243;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#65292;&#23427;&#20801;&#35768;&#23545;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#33258;&#23450;&#20041;&#65292;&#24182;&#19988;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#29983;&#25104;&#27169;&#22411;&#26469;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;&#36981;&#23432;&#33258;&#23450;&#20041;&#35268;&#33539;&#12290;</title><link>http://arxiv.org/abs/2307.03577</link><description>&lt;p&gt;
&#21487;&#32534;&#31243;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Programmable Synthetic Tabular Data Generation. (arXiv:2307.03577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03577
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;ProgSyn&#65292;&#31532;&#19968;&#20010;&#21487;&#32534;&#31243;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#65292;&#23427;&#20801;&#35768;&#23545;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#33258;&#23450;&#20041;&#65292;&#24182;&#19988;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#29983;&#25104;&#27169;&#22411;&#26469;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;&#36981;&#23432;&#33258;&#23450;&#20041;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38544;&#31169;&#12289;&#25968;&#25454;&#36136;&#37327;&#21644;&#25968;&#25454;&#20849;&#20139;&#30340;&#38480;&#21046;&#65292;&#22823;&#37327;&#30340;&#34920;&#26684;&#25968;&#25454;&#20173;&#28982;&#34987;&#20302;&#25928;&#21033;&#29992;&#12290;&#23613;&#31649;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#21407;&#22987;&#20998;&#24067;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#20294;&#22823;&#22810;&#25968;&#24212;&#29992;&#31243;&#24207;&#36824;&#38656;&#35201;&#39069;&#22806;&#30340;&#29983;&#25104;&#25968;&#25454;&#32422;&#26463;&#12290;&#29616;&#26377;&#30340;&#21512;&#25104;&#25968;&#25454;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#21482;&#22788;&#29702;&#29305;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#20363;&#22914;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25110;&#22686;&#21152;&#20844;&#24179;&#24615;&#65292;&#24182;&#19988;&#32570;&#20047;&#19968;&#20010;&#21487;&#35775;&#38382;&#30340;&#25509;&#21475;&#26469;&#22768;&#26126;&#19968;&#33324;&#35268;&#33539;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ProgSyn&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#32534;&#31243;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#65292;&#23427;&#20801;&#35768;&#23545;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#33258;&#23450;&#20041;&#12290;&#20026;&#20102;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#24182;&#36981;&#23432;&#33258;&#23450;&#20041;&#35268;&#33539;&#65292;ProgSyn&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#22312;&#25552;&#20379;&#30340;&#35268;&#33539;&#19978;&#33258;&#21160;&#25512;&#23548;&#20986;&#21487;&#24494;&#20998;&#25439;&#22833;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#20123;&#35268;&#33539;&#21487;&#20197;&#20351;&#29992;&#32479;&#35745;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large amounts of tabular data remain underutilized due to privacy, data quality, and data sharing limitations. While training a generative model producing synthetic data resembling the original distribution addresses some of these issues, most applications require additional constraints from the generated data. Existing synthetic data approaches are limited as they typically only handle specific constraints, e.g., differential privacy (DP) or increased fairness, and lack an accessible interface for declaring general specifications. In this work, we introduce ProgSyn, the first programmable synthetic tabular data generation algorithm that allows for comprehensive customization over the generated data. To ensure high data quality while adhering to custom specifications, ProgSyn pre-trains a generative model on the original dataset and fine-tunes it on a differentiable loss automatically derived from the provided specifications. These can be programmatically declared using statistical and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03212</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#22478;&#24066;&#21306;&#22495;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings. (arXiv:2307.03212v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03212
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#21306;&#22495;&#23884;&#20837;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#39640;&#24230;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#22478;&#24066;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#24615;&#36136;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#65292;&#20197;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#32780;&#19981;&#21463;&#21018;&#24615;&#37051;&#22495;&#26465;&#20214;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19987;&#27880;&#20110;&#20174;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#34920;&#31034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#31227;&#21160;&#27969;&#27169;&#24335;&#12289;POI&#35821;&#20041;&#21644;&#31614;&#21040;&#21160;&#24577;&#20013;&#25429;&#25417;&#22810;&#35270;&#35282;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#20840;&#23616;&#22270;&#27880;&#24847;&#32593;&#32476;&#26469;&#23398;&#20064;&#22270;&#20013;&#20219;&#24847;&#20004;&#20010;&#39030;&#28857;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#20840;&#38754;&#32771;&#34385;&#21644;&#20849;&#20139;&#22810;&#20010;&#35270;&#35282;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#34701;&#21512;&#27169;&#22359;&#65292;&#21033;&#29992;&#22806;&#37096;&#27880;&#24847;&#21147;&#23398;&#20064;&#26435;&#37325;&#26469;&#34701;&#21512;&#22810;&#35270;&#35282;&#23884;&#20837;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban region embedding is an important and yet highly challenging issue due to the complexity and constantly changing nature of urban data. To address the challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER) to capture multi-view dependencies and learn expressive representations of urban regions without the constraints of rigid neighbourhood region conditions. Our model focus on learn urban region representation from multi-source urban data. First, we capture the multi-view correlations from mobility flow patterns, POI semantics and check-in dynamics. Then, we adopt global graph attention networks to learn similarity of any two vertices in graphs. To comprehensively consider and share features of multiple views, a two-stage fusion module is further proposed to learn weights with external attention to fuse multi-view embeddings. Extensive experiments for two downstream tasks on real-world datasets demonstrate that our model outperforms state-of-the-art methods
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23450;&#37327;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;11&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22303;&#22320;&#35206;&#30422;&#22320;&#22270;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#22312;&#38750;&#27954;&#20892;&#30000;&#20998;&#31867;&#21644;&#22522;&#20110;&#21355;&#26143;&#22320;&#29699;&#35266;&#27979;&#30340;&#20892;&#19994;&#30417;&#27979;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#26368;&#36866;&#21512;&#20854;&#38656;&#27714;&#30340;&#22320;&#22270;&#65292;&#24182;&#40723;&#21169;&#26410;&#26469;&#24037;&#20316;&#25913;&#36827;&#22320;&#22270;&#30340;&#19968;&#33268;&#24615;&#21644;&#20302;&#31934;&#24230;&#21306;&#22495;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02575</link><description>&lt;p&gt;
&#29616;&#26377;&#30340;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#20892;&#19994;&#22303;&#22320;&#35206;&#30422;&#22320;&#22270;&#30340;&#20934;&#30830;&#24615;&#26377;&#22810;&#39640;&#65311;
&lt;/p&gt;
&lt;p&gt;
How accurate are existing land cover maps for agriculture in Sub-Saharan Africa?. (arXiv:2307.02575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23450;&#37327;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;11&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22303;&#22320;&#35206;&#30422;&#22320;&#22270;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#22312;&#38750;&#27954;&#20892;&#30000;&#20998;&#31867;&#21644;&#22522;&#20110;&#21355;&#26143;&#22320;&#29699;&#35266;&#27979;&#30340;&#20892;&#19994;&#30417;&#27979;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#26368;&#36866;&#21512;&#20854;&#38656;&#27714;&#30340;&#22320;&#22270;&#65292;&#24182;&#40723;&#21169;&#26410;&#26469;&#24037;&#20316;&#25913;&#36827;&#22320;&#22270;&#30340;&#19968;&#33268;&#24615;&#21644;&#20302;&#31934;&#24230;&#21306;&#22495;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#26143;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#21487;&#20197;&#25552;&#20379;&#32463;&#27982;&#23454;&#24800;&#21644;&#21450;&#26102;&#30340;&#20449;&#24687;&#26469;&#35780;&#20272;&#20316;&#29289;&#29366;&#20917;&#21644;&#31918;&#39135;&#29983;&#20135;&#12290;&#22312;&#38750;&#27954;&#65292;&#36825;&#26679;&#30340;&#30417;&#27979;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#37027;&#37324;&#23384;&#22312;&#31918;&#39135;&#19981;&#23433;&#20840;&#21644;&#32570;&#20047;&#20892;&#19994;&#32479;&#35745;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;EO&#30340;&#30417;&#27979;&#31995;&#32479;&#38656;&#35201;&#20934;&#30830;&#30340;&#20892;&#30000;&#22320;&#22270;&#26469;&#25552;&#20379;&#26377;&#20851;&#20892;&#30000;&#30340;&#20449;&#24687;&#65292;&#20294;&#26159;&#32570;&#20047;&#25968;&#25454;&#26469;&#30830;&#23450;&#21738;&#20123;&#21487;&#29992;&#30340;&#22303;&#22320;&#35206;&#30422;&#22320;&#22270;&#26368;&#20934;&#30830;&#22320;&#35782;&#21035;&#38750;&#27954;&#22269;&#23478;&#30340;&#20892;&#30000;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;8&#20010;&#22269;&#23478;&#30340;&#32479;&#35745;&#20005;&#35880;&#30340;&#21442;&#32771;&#25968;&#25454;&#65292;&#23545;11&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22303;&#22320;&#35206;&#30422;&#22320;&#22270;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#20892;&#30000;&#20998;&#31867;&#21644;&#22522;&#20110;EO&#30340;&#38750;&#27954;&#20892;&#19994;&#30417;&#27979;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#30830;&#23450;&#26368;&#36866;&#21512;&#20182;&#20204;&#38656;&#27714;&#30340;&#22320;&#22270;&#65292;&#24182;&#40723;&#21169;&#26410;&#26469;&#30340;&#24037;&#20316;&#38598;&#20013;&#35299;&#20915;&#22320;&#22270;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#24182;&#25552;&#39640;&#20302;&#31934;&#24230;&#22320;&#21306;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Satellite Earth observations (EO) can provide affordable and timely information for assessing crop conditions and food production. Such monitoring systems are essential in Africa, where there is high food insecurity and sparse agricultural statistics. EO-based monitoring systems require accurate cropland maps to provide information about croplands, but there is a lack of data to determine which of the many available land cover maps most accurately identify cropland in African countries. This study provides a quantitative evaluation and intercomparison of 11 publicly available land cover maps to assess their suitability for cropland classification and EO-based agriculture monitoring in Africa using statistically rigorous reference datasets from 8 countries. We hope the results of this study will help users determine the most suitable map for their needs and encourage future work to focus on resolving inconsistencies between maps and improving accuracy in low-accuracy regions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#29305;&#24449;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#38024;&#23545;&#31232;&#30095;&#34920;&#38754;&#21387;&#21147;&#20256;&#24863;&#22120;&#19979;&#30340;&#24490;&#29615;&#27668;&#32568;&#27969;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#38459;&#21147;&#21644;&#20943;&#23567;&#21319;&#21147;&#27874;&#21160;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#23558;&#20256;&#24863;&#22120;&#20449;&#21495;&#25552;&#21462;&#20026;&#21160;&#24577;&#29305;&#24449;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#24182;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#21160;&#29366;&#24577;&#65292;&#20174;&#32780;&#20351;&#24471;&#25511;&#21046;&#24615;&#33021;&#33021;&#22815;&#22312;&#19981;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#31232;&#30095;&#20256;&#24863;&#22120;&#24863;&#30693;&#27969;&#21160;&#12290;&#35813;&#26041;&#27861;&#22312;&#38459;&#21147;&#31995;&#25968;&#21644;&#21319;&#21147;&#31995;&#25968;&#30340;&#25913;&#21892;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.01995</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#24577;&#29305;&#24449;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#31232;&#30095;&#34920;&#38754;&#21387;&#21147;&#20256;&#24863;&#22120;&#19979;&#30340;&#24490;&#29615;&#27668;&#32568;&#27969;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dynamic Feature-based Deep Reinforcement Learning for Flow Control of Circular Cylinder with Sparse Surface Pressure Sensing. (arXiv:2307.01995v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#29305;&#24449;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#38024;&#23545;&#31232;&#30095;&#34920;&#38754;&#21387;&#21147;&#20256;&#24863;&#22120;&#19979;&#30340;&#24490;&#29615;&#27668;&#32568;&#27969;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#38459;&#21147;&#21644;&#20943;&#23567;&#21319;&#21147;&#27874;&#21160;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#23558;&#20256;&#24863;&#22120;&#20449;&#21495;&#25552;&#21462;&#20026;&#21160;&#24577;&#29305;&#24449;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#24182;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#21160;&#29366;&#24577;&#65292;&#20174;&#32780;&#20351;&#24471;&#25511;&#21046;&#24615;&#33021;&#33021;&#22815;&#22312;&#19981;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#31232;&#30095;&#20256;&#24863;&#22120;&#24863;&#30693;&#27969;&#21160;&#12290;&#35813;&#26041;&#27861;&#22312;&#38459;&#21147;&#31995;&#25968;&#21644;&#21319;&#21147;&#31995;&#25968;&#30340;&#25913;&#21892;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#31639;&#27861;&#65292;&#38024;&#23545;&#31232;&#30095;&#20256;&#24863;&#22120;&#20449;&#24687;&#30340;&#23553;&#38381;&#29615;&#27668;&#32568;&#23614;&#27969;&#25511;&#21046;&#65292;&#20197;&#38477;&#20302;&#38459;&#21147;&#21644;&#20943;&#23567;&#21319;&#21147;&#30340;&#27874;&#21160;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#36215;&#22987;&#28857;&#65292;&#24182;&#36890;&#36807;&#23558;&#20256;&#24863;&#22120;&#20449;&#21495;&#25552;&#21462;&#20026;&#21160;&#24577;&#29305;&#24449;&#65288;DF&#65289;&#26469;&#26174;&#33879;&#25552;&#39640;DRL&#24615;&#33021;&#65292;&#20174;&#32780;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#21160;&#29366;&#24577;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#22522;&#20110;&#21160;&#24577;&#29305;&#24449;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DF-DRL&#65289;&#22312;&#27809;&#26377;&#21160;&#24577;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#23398;&#20064;&#20102;&#19968;&#20010;&#22312;&#31995;&#32479;&#20013;&#30340;&#21453;&#39304;&#25511;&#21046;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#30452;&#25509;&#20256;&#24863;&#22120;&#21453;&#39304;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;DF-DRL&#27169;&#22411;&#30340;&#38459;&#21147;&#31995;&#25968;&#20943;&#23567;&#20102;25%&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20165;&#20351;&#29992;&#19968;&#20010;&#34920;&#38754;&#21387;&#21147;&#20256;&#24863;&#22120;&#65292;DF-DRL&#21487;&#20197;&#23558;&#38459;&#21147;&#31995;&#25968;&#38477;&#20302;&#21040;Re = 100&#26102;&#32422;8%&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#33879;&#20943;&#36731;&#20102;&#21319;&#21147;&#31995;&#25968;&#30340;&#27874;&#21160;&#12290;&#22240;&#27492;&#65292;DF-DRL&#20801;&#35768;&#22312;&#19981;&#38477;&#20302;&#25511;&#21046;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#37096;&#32626;&#31232;&#30095;&#20256;&#24863;&#22120;&#23545;&#27969;&#21160;&#36827;&#34892;&#24863;&#30693;&#12290;&#27492;&#26041;&#27861;&#36824;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a self-learning algorithm for closed-loop cylinder wake control targeting lower drag and lower lift fluctuations with the additional challenge of sparse sensor information, taking deep reinforcement learning as the starting point. DRL performance is significantly improved by lifting the sensor signals to dynamic features (DF), which predict future flow states. The resulting dynamic feature-based DRL (DF-DRL) automatically learns a feedback control in the plant without a dynamic model. Results show that the drag coefficient of the DF-DRL model is 25% less than the vanilla model based on direct sensor feedback. More importantly, using only one surface pressure sensor, DF-DRL can reduce the drag coefficient to a state-of-the-art performance of about 8% at Re = 100 and significantly mitigate lift coefficient fluctuations. Hence, DF-DRL allows the deployment of sparse sensing of the flow without degrading the control performance. This method also shows good robustness in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#30340;&#32852;&#37030;&#29983;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20256;&#36755;&#25552;&#31034;&#19982;&#20998;&#24067;&#24335;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#36828;&#31243;&#21512;&#25104;&#26377;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#36890;&#20449;&#25928;&#29575;&#12289;&#36866;&#24212;&#20998;&#24067;&#36716;&#31227;&#12289;&#25552;&#21319;&#24615;&#33021;&#12289;&#21152;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2306.16064</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#30340;&#32852;&#37030;&#29983;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Generative Learning with Foundation Models. (arXiv:2306.16064v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#30340;&#32852;&#37030;&#29983;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20256;&#36755;&#25552;&#31034;&#19982;&#20998;&#24067;&#24335;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#36828;&#31243;&#21512;&#25104;&#26377;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#36890;&#20449;&#25928;&#29575;&#12289;&#36866;&#24212;&#20998;&#24067;&#36716;&#31227;&#12289;&#25552;&#21319;&#24615;&#33021;&#12289;&#21152;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#38598;&#20013;&#22312;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#20256;&#36755;&#29305;&#24449;&#12289;&#21442;&#25968;&#25110;&#26799;&#24230;&#65292;&#36825;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#20302;&#25928;&#21644;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;&#20511;&#21161;&#26032;&#20852;&#30340;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#32852;&#37030;&#29983;&#25104;&#23398;&#20064;&#65292;&#23427;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#20256;&#36755;&#19982;&#20998;&#24067;&#24335;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#25552;&#31034;&#12290;&#36890;&#36807;&#25509;&#25910;&#21040;&#30340;&#21253;&#21547;&#36739;&#23569;&#38544;&#31169;&#20449;&#24687;&#30340;&#25552;&#31034;&#20197;&#21450;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36828;&#31243;&#21512;&#25104;&#26377;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20010;&#26032;&#26694;&#26550;&#20855;&#26377;&#22810;&#20010;&#20248;&#21183;&#65292;&#21253;&#25324;&#25913;&#21892;&#20102;&#36890;&#20449;&#25928;&#29575;&#12289;&#26356;&#22909;&#30340;&#36866;&#24212;&#20998;&#24067;&#36716;&#31227;&#12289;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12289;&#21152;&#24378;&#20102;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;ImageNet&#21644;DomainNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing federated learning solutions focus on transmitting features, parameters or gadients between clients and server, which suffer from serious low-efficiency and privacy-leakage problems. Thanks to the emerging foundation generative models, we propose a novel federated learning framework, namely Federated Generative Learning, that transmits prompts associated with distributed training data between clients and server. The informative training data can be synthesized remotely based on received prompts containing little privacy and the foundation generative models. The new framework possesses multiple advantages, including improved communication efficiency, better resilience to distribution shift, substantial performance gains, and enhanced privacy protection, which are verified in extensive experiments on ImageNet and DomainNet datasets.
&lt;/p&gt;</description></item><item><title>FLuRKA&#26159;&#19968;&#31181;&#34701;&#21512;&#20302;&#31209;&#21644;&#26680;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;transformer&#31867;&#21035;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#36817;&#20284;&#25216;&#26415;&#65292;&#22312;&#36816;&#34892;&#26102;&#38388;&#24615;&#33021;&#21644;&#36136;&#37327;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.15799</link><description>&lt;p&gt;
FLuRKA: &#24555;&#36895;&#34701;&#21512;&#20302;&#31209;&#21644;&#26680;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
FLuRKA: Fast fused Low-Rank &amp; Kernel Attention. (arXiv:2306.15799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15799
&lt;/p&gt;
&lt;p&gt;
FLuRKA&#26159;&#19968;&#31181;&#34701;&#21512;&#20302;&#31209;&#21644;&#26680;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;transformer&#31867;&#21035;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#36817;&#20284;&#25216;&#26415;&#65292;&#22312;&#36816;&#34892;&#26102;&#38388;&#24615;&#33021;&#21644;&#36136;&#37327;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;transformer&#32467;&#26500;&#30340;&#25552;&#20986;&#20197;&#26469;&#65292;&#35768;&#22810;&#39640;&#25928;&#30340;&#36817;&#20284;&#33258;&#27880;&#24847;&#21147;&#25216;&#26415;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#20854;&#20013;&#20004;&#31181;&#27969;&#34892;&#30340;&#25216;&#26415;&#31867;&#21035;&#26159;&#20302;&#31209;&#21644;&#26680;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#30456;&#20114;&#34917;&#20805;&#65292;&#21033;&#29992;&#36825;&#20123;&#21327;&#21516;&#25928;&#24212;&#26469;&#34701;&#21512;&#20302;&#31209;&#21644;&#26680;&#26041;&#27861;&#65292;&#20135;&#29983;&#20102;&#19968;&#31181;&#26032;&#30340;transformer&#31867;&#21035;&#65306;FLuRKA&#65288;&#24555;&#36895;&#20302;&#31209;&#21644;&#26680;&#27880;&#24847;&#21147;&#65289;&#12290;FLuRKA&#30456;&#23545;&#20110;&#36825;&#20123;&#36817;&#20284;&#25216;&#26415;&#25552;&#20379;&#20102;&#21487;&#35266;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#26041;&#38754;&#35780;&#20272;&#20102;FLuRKA&#30340;&#36816;&#34892;&#26102;&#38388;&#24615;&#33021;&#21644;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#25552;&#20379;&#20102;&#22810;&#31181;&#21442;&#25968;&#37197;&#32622;&#65292;&#22312;&#36825;&#20123;&#37197;&#32622;&#19979;&#65292;FLuRKA&#20855;&#26377;&#21152;&#36895;&#25928;&#26524;&#65307;&#25105;&#20204;&#30340;&#20934;&#30830;&#24615;&#20998;&#26512;&#38480;&#23450;&#20102;FLuRKA&#30456;&#23545;&#20110;&#20840;&#27880;&#24847;&#21147;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#23454;&#20363;&#21270;&#20102;&#19977;&#31181;FLuRKA&#21464;&#20307;&#65292;&#30456;&#23545;&#20110;&#20302;&#31209;&#21644;&#26680;&#26041;&#27861;&#20998;&#21035;&#23454;&#29616;&#20102;&#39640;&#36798;3.3&#20493;&#21644;1.7&#20493;&#30340;&#32463;&#39564;&#21152;&#36895;&#12290;&#36825;&#24847;&#21619;&#30528;&#26356;&#24555;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#32780;&#19988;&#36136;&#37327;&#20173;&#28982;&#20445;&#25345;&#19981;&#38169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many efficient approximate self-attention techniques have become prevalent since the inception of the transformer architecture. Two popular classes of these techniques are low-rank and kernel methods. Each of these methods has its own strengths. We observe these strengths synergistically complement each other and exploit these synergies to fuse low-rank and kernel methods, producing a new class of transformers: FLuRKA (Fast Low-Rank and Kernel Attention). FLuRKA provide sizable performance gains over these approximate techniques and are of high quality. We theoretically and empirically evaluate both the runtime performance and quality of FLuRKA. Our runtime analysis posits a variety of parameter configurations where FLuRKA exhibit speedups and our accuracy analysis bounds the error of FLuRKA with respect to full-attention. We instantiate three FLuRKA variants which experience empirical speedups of up to 3.3x and 1.7x over low-rank and kernel methods respectively. This translates to spe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#30340;&#25216;&#26415;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#22240;&#26524;&#25512;&#29702;&#21644;&#22240;&#26524;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.15479</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Causal Inference via Predictive Coding. (arXiv:2306.15479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15479
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#30340;&#25216;&#26415;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#22240;&#26524;&#25512;&#29702;&#21644;&#22240;&#26524;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#22240;&#26524;&#25512;&#29702;&#26159;&#26234;&#33021;&#30340;&#22522;&#26412;&#36807;&#31243;&#12290;&#36125;&#21494;&#26031;&#25512;&#29702;&#27169;&#22411;&#35266;&#23519;&#65306;&#22914;&#26524;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#30456;&#20851;&#21464;&#37327;x&#65292;&#25105;&#20204;&#33021;&#25512;&#26029;&#20986;&#20851;&#20110;y&#30340;&#20449;&#24687;&#21527;&#65311;&#22240;&#26524;&#25512;&#29702;&#27169;&#22411;&#24178;&#39044;&#65306;&#22914;&#26524;&#25105;&#20204;&#30452;&#25509;&#25913;&#21464;x&#65292;y&#20250;&#22914;&#20309;&#21464;&#21270;&#65311;&#39044;&#27979;&#32534;&#30721;&#26159;&#19968;&#31181;&#21463;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#20165;&#23616;&#37096;&#20449;&#24687;&#23545;&#36830;&#32493;&#29366;&#24577;&#21464;&#37327;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#36125;&#21494;&#26031;&#25512;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23545;&#39044;&#27979;&#32534;&#30721;&#30340;&#25512;&#29702;&#36807;&#31243;&#36827;&#34892;&#31616;&#21333;&#26356;&#25913;&#65292;&#23454;&#29616;&#24050;&#30693;&#22240;&#26524;&#22270;&#30340;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;&#28982;&#21518;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#39044;&#27979;&#32534;&#30721;&#25512;&#24191;&#21040;&#22240;&#26524;&#22270;&#26410;&#30693;&#19988;&#38656;&#35201;&#36890;&#36807;&#25968;&#25454;&#25512;&#26029;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#12290;&#32467;&#26524;&#26159;&#19968;&#31181;&#26032;&#39062;&#19988;&#30452;&#25509;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#22240;&#26524;&#25512;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian and causal inference are fundamental processes for intelligence. Bayesian inference models observations: what can be inferred about y if we observe a related variable x? Causal inference models interventions: if we directly change x, how will y change? Predictive coding is a neuroscience-inspired method for performing Bayesian inference on continuous state variables using local information only. In this work, we go beyond Bayesian inference, and show how a simple change in the inference process of predictive coding enables interventional and counterfactual inference in scenarios where the causal graph is known. We then extend our results, and show how predictive coding can be generalized to cases where this graph is unknown, and has to be inferred from data, hence performing causal discovery. What results is a novel and straightforward technique that allows us to perform end-to-end causal inference on predictive-coding-based structural causal models, and demonstrate its utilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ProtoGate&#65292;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#26679;&#26412;&#20869;&#21644;&#26679;&#26412;&#38388;&#30340;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#26469;&#24341;&#20837;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#20197;&#20840;&#23616;&#21040;&#26412;&#22320;&#30340;&#26041;&#24335;&#36873;&#25321;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#24182;&#20351;&#39044;&#27979;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12330</link><description>&lt;p&gt;
ProtoGate&#65306;&#38754;&#21521;&#34920;&#26684;&#22411;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#21407;&#22411;&#31070;&#32463;&#32593;&#32476;&#19982;&#26412;&#22320;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
ProtoGate: Prototype-based Neural Networks with Local Feature Selection for Tabular Biomedical Data. (arXiv:2306.12330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ProtoGate&#65292;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#26679;&#26412;&#20869;&#21644;&#26679;&#26412;&#38388;&#30340;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#26469;&#24341;&#20837;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#20197;&#20840;&#23616;&#21040;&#26412;&#22320;&#30340;&#26041;&#24335;&#36873;&#25321;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#24182;&#20351;&#39044;&#27979;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#22411;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#24448;&#24448;&#26159;&#39640;&#32500;&#30340;&#65292;&#20294;&#26679;&#26412;&#37327;&#21448;&#30456;&#23545;&#36739;&#23567;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;&#36825;&#34920;&#26126;&#24403;&#21069;&#26041;&#27861;&#32570;&#20047;&#36866;&#24403;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#19981;&#33021;&#25429;&#25417;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#30340;&#20849;&#21516;&#27169;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;ProtoGate&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#26679;&#26412;&#20869;&#21644;&#26679;&#26412;&#38388;&#30340;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#26469;&#24341;&#20837;&#24402;&#32435;&#20559;&#24046;&#12290;ProtoGate&#20197;&#20840;&#23616;&#21040;&#26412;&#22320;&#30340;&#26041;&#24335;&#36873;&#25321;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#29983;&#20135;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#65292;&#32780;&#21407;&#22411;&#27169;&#22411;&#20351;&#39044;&#27979;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#26469;&#35780;&#20272;ProtoGate&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#27169;&#24335;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#65292;&#21516;&#26102;&#21407;&#22411;&#36171;&#20104;&#20102;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular biomedical data poses challenges in machine learning because it is often high-dimensional and typically low-sample-size. Previous research has attempted to address these challenges via feature selection approaches, which can lead to unstable performance on real-world data. This suggests that current methods lack appropriate inductive biases that capture patterns common to different samples. In this paper, we propose ProtoGate, a prototype-based neural model that introduces an inductive bias by attending to both homogeneity and heterogeneity across samples. ProtoGate selects features in a global-to-local manner and leverages them to produce explainable predictions via an interpretable prototype-based model. We conduct comprehensive experiments to evaluate the performance of ProtoGate on synthetic and real-world datasets. Our results show that exploiting the homogeneous and heterogeneous patterns in the data can improve prediction accuracy while prototypes imbue interpretability.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#22788;&#29702;&#19981;&#24120;&#35265;&#26679;&#26412;&#26102;&#25512;&#36831;&#21040;&#20154;&#31867;&#21028;&#26029;&#30340;&#20445;&#23432;&#27169;&#22411;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#65288;DCM&#65289;&#30340;&#31639;&#27861;&#65292;&#22312;&#36741;&#21161;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24863;&#20852;&#36259;&#30340;OOD&#65288;Out-of-Distribution&#65289;&#21306;&#22495;&#30340;&#26679;&#26412;&#65292;&#36827;&#32780;&#23454;&#29616;&#21487;&#38752;&#22320;&#20998;&#31163;ID&#65288;In-Distribution&#65289;&#21644;OOD&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2306.04974</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#30340;&#20445;&#23432;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conservative Prediction via Data-Driven Confidence Minimization. (arXiv:2306.04974v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04974
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#22788;&#29702;&#19981;&#24120;&#35265;&#26679;&#26412;&#26102;&#25512;&#36831;&#21040;&#20154;&#31867;&#21028;&#26029;&#30340;&#20445;&#23432;&#27169;&#22411;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#65288;DCM&#65289;&#30340;&#31639;&#27861;&#65292;&#22312;&#36741;&#21161;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24863;&#20852;&#36259;&#30340;OOD&#65288;Out-of-Distribution&#65289;&#21306;&#22495;&#30340;&#26679;&#26412;&#65292;&#36827;&#32780;&#23454;&#29616;&#21487;&#38752;&#22320;&#20998;&#31163;ID&#65288;In-Distribution&#65289;&#21644;OOD&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38169;&#35823;&#20195;&#20215;&#24456;&#39640;&#65292;&#29305;&#21035;&#26159;&#22312;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#36825;&#31181;&#38169;&#35823;&#21487;&#33021;&#20250;&#38459;&#27490;&#26426;&#22120;&#23398;&#20064;&#30340;&#37096;&#32626;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#20445;&#23432;&#24615;&#30340;&#27169;&#22411;&#8212;&#8212;&#24403;&#23427;&#20204;&#21487;&#33021;&#20986;&#29616;&#38169;&#35823;&#26102;&#21487;&#20197;&#25512;&#36831;&#21040;&#20154;&#31867;&#21028;&#26029;&#8212;&#8212;&#21487;&#33021;&#20250;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#26816;&#27979;&#24322;&#24120;&#25110;&#22797;&#26434;&#31034;&#20363;&#26126;&#26174;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#26080;&#27861;&#39044;&#27979;&#25152;&#26377;&#21487;&#33021;&#30340;&#27979;&#35797;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#22312;&#36741;&#21161;&#20266;OOD&#25968;&#25454;&#38598;&#19978;&#26368;&#23567;&#21270;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#26159;&#20851;&#38190;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#36741;&#21161;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#24863;&#20852;&#36259;&#30340;OOD&#21306;&#22495;&#30340;&#26679;&#26412;&#65292;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#32622;&#20449;&#24230;&#21487;&#38752;&#22320;&#20998;&#31163;ID&#21644;OOD&#36755;&#20837;&#12290;&#21463;&#21040;&#36825;&#19968;&#32467;&#26524;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#65288;DCM&#65289;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Errors of machine learning models are costly, especially in safety-critical domains such as healthcare, where such mistakes can prevent the deployment of machine learning altogether. In these settings, conservative models -- models which can defer to human judgment when they are likely to make an error -- may offer a solution. However, detecting unusual or difficult examples is notably challenging, as it is impossible to anticipate all potential inputs at test time. To address this issue, prior work has proposed to minimize the model's confidence on an auxiliary pseudo-OOD dataset. We theoretically analyze the effect of confidence minimization and show that the choice of auxiliary dataset is critical. Specifically, if the auxiliary dataset includes samples from the OOD region of interest, confidence minimization provably separates ID and OOD inputs by predictive confidence. Taking inspiration from this result, we present data-driven confidence minimization (DCM), which minimizes confid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20989;&#25968;&#35299;&#37322;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#12290;&#37319;&#26679;&#22120;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#24471;&#20998;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.04848</link><description>&lt;p&gt;
&#21033;&#29992;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20989;&#25968;&#35299;&#37322;&#21644;&#25913;&#36827;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interpreting and Improving Diffusion Models Using the Euclidean Distance Function. (arXiv:2306.04848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20989;&#25968;&#35299;&#37322;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#12290;&#37319;&#26679;&#22120;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#24471;&#20998;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#30452;&#35273;&#19978;&#19982;&#25237;&#24433;&#26377;&#20851;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#27969;&#24418;&#20551;&#35774;&#19979;&#65292;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#36817;&#20284;&#31561;&#20215;&#20110;&#27491;&#20132;&#25200;&#21160;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#21435;&#22122;&#36817;&#20284;&#20110;&#23398;&#20064;&#25237;&#24433;&#12290;&#26412;&#25991;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#23558;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#35299;&#37322;&#20026;&#24212;&#29992;&#20110;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20989;&#25968;&#30340;&#36817;&#20284;&#26799;&#24230;&#19979;&#38477;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#21435;&#22122;&#22120;&#25237;&#24433;&#35823;&#24046;&#30340;&#31616;&#21333;&#20551;&#35774;&#65292;&#25552;&#20379;DDIM&#65288;Denoising Diffusion Implicit Models&#65289;&#37319;&#26679;&#22120;&#30340;&#31616;&#21333;&#25910;&#25947;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#29702;&#35770;&#32467;&#26524;&#30340;&#27934;&#35265;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23545;DDIM&#30340;&#20004;&#20010;&#31616;&#21333;&#20462;&#25913;&#30340;&#26032;&#37319;&#26679;&#22120;&#12290;&#20165;&#38656;&#35201;5-10&#20010;&#20989;&#25968;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#37319;&#26679;&#22120;&#23601;&#33021;&#22312;&#39044;&#35757;&#32451;&#30340;CIFAR-10&#21644;CelebA&#27169;&#22411;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;FID&#24471;&#20998;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising is intuitively related to projection. Indeed, under the manifold hypothesis, adding random noise is approximately equivalent to orthogonal perturbation. Hence, learning to denoise is approximately learning to project. In this paper, we use this observation to reinterpret denoising diffusion models as approximate gradient descent applied to the Euclidean distance function. We then provide straight-forward convergence analysis of the DDIM sampler under simple assumptions on the projection-error of the denoiser. Finally, we propose a new sampler based on two simple modifications to DDIM using insights from our theoretical results. In as few as 5-10 function evaluations, our sampler achieves state-of-the-art FID scores on pretrained CIFAR-10 and CelebA models and can generate high quality samples on latent diffusion models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#22312;&#19981;&#21516;&#22823;&#38470;&#12289;&#20113;&#20379;&#24212;&#21830;&#21644;&#25968;&#25454;&#20013;&#24515;&#33539;&#22260;&#20869;&#65292;&#20351;&#29992;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#28857;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#21542;&#26159;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#36873;&#25321;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#24615;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03163</link><description>&lt;p&gt;
&#22914;&#20309;&#36328;&#36234;&#20113;&#21644;&#22823;&#38470;&#22521;&#35757;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65311;&#19968;&#39033;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Can We Train Deep Learning Models Across Clouds and Continents? An Experimental Study. (arXiv:2306.03163v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#22312;&#19981;&#21516;&#22823;&#38470;&#12289;&#20113;&#20379;&#24212;&#21830;&#21644;&#25968;&#25454;&#20013;&#24515;&#33539;&#22260;&#20869;&#65292;&#20351;&#29992;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#28857;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#21542;&#26159;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#36873;&#25321;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20113;&#31471;&#25110;&#19987;&#29992;&#30828;&#20214;&#19978;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#26114;&#36149;&#30340;&#12290;&#19968;&#31181;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#36873;&#25321;&#26159;&#25552;&#20379;&#28857;&#23454;&#20363;&#30340;&#39640;&#36229;&#35268;&#27169;&#20113;&#65292;&#36825;&#26159;&#19968;&#20010;&#20415;&#23452;&#20294;&#30701;&#26242;&#30340;&#36873;&#25321;&#65292;&#29992;&#20110;&#26367;&#20195;&#25353;&#38656;&#36164;&#28304;&#12290;&#30001;&#20110;&#28857;&#23454;&#20363;&#30340;&#21487;&#29992;&#24615;&#21487;&#33021;&#20250;&#22240;&#26085;&#26399;&#12289;&#22823;&#38470;&#21644;&#20113;&#20379;&#24212;&#21830;&#19981;&#21516;&#32780;&#21457;&#29983;&#21464;&#21270;&#65292;&#22240;&#27492;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#20998;&#37197;&#36164;&#28304;&#21487;&#33021;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#12290;&#20294;&#26159;&#65292;&#23578;&#26410;&#35843;&#26597;&#22320;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#28857;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#21542;&#26159;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#26088;&#22312;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#21542;&#22312;&#35206;&#30422;&#19981;&#21516;&#25968;&#25454;&#20013;&#24515;&#21644;&#20113;&#25552;&#20379;&#21830;&#30340;&#28857; VM &#20840;&#29699;&#24066;&#22330;&#19978;&#20197;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65311;&#20026;&#20102;&#25552;&#20379;&#25351;&#23548;&#65292;&#25105;&#20204;&#24191;&#27867;&#35780;&#20272;&#20102;&#19981;&#21516;&#21306;&#22495;&#12289;&#22823;&#38470;&#21644;&#20113;&#23545;&#20195;&#34920;&#24615; CV &#21644; NLP &#27169;&#22411;&#30340;&#25104;&#26412;&#21644;&#21534;&#21520;&#37327;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25193;&#23637;&#24403;&#21069;&#30340;&#22521;&#35757;&#36873;&#25321;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21487;&#25193;&#23637;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep learning models in the cloud or on dedicated hardware is expensive. A more cost-efficient option are hyperscale clouds offering spot instances, a cheap but ephemeral alternative to on-demand resources. As spot instance availability can change depending on the time of day, continent, and cloud provider, it could be more cost-efficient to distribute resources over the world. Still, it has not been investigated whether geo-distributed, data-parallel spot deep learning training could be a more cost-efficient alternative to centralized training.  This paper aims to answer the question: Can deep learning models be cost-efficiently trained on a global market of spot VMs spanning different data centers and cloud providers? To provide guidance, we extensively evaluate the cost and throughput implications of training in different zones, continents, and clouds for representative CV and NLP models. To expand the current training options further, we compare the scalability potential f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36861;&#36394;&#38750;&#24179;&#31283;&#24615;&#30340;&#22240;&#26524;&#36215;&#28304;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#28304;&#34920;&#31034;&#65288;COREP&#65289;&#31639;&#27861;&#65292;&#23398;&#21040;&#30340;&#31574;&#30053;&#23545;&#38750;&#24179;&#31283;&#24615;&#34920;&#29616;&#20986;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02747</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#28304;&#34920;&#31034;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling Non-Stationarity in Reinforcement Learning via Causal-Origin Representation. (arXiv:2306.02747v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36861;&#36394;&#38750;&#24179;&#31283;&#24615;&#30340;&#22240;&#26524;&#36215;&#28304;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#28304;&#34920;&#31034;&#65288;COREP&#65289;&#31639;&#27861;&#65292;&#23398;&#21040;&#30340;&#31574;&#30053;&#23545;&#38750;&#24179;&#31283;&#24615;&#34920;&#29616;&#20986;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#21463;&#21040;&#22797;&#26434;&#30340;&#38750;&#24179;&#31283;&#24615;&#30340;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#35797;&#22270;&#26126;&#30830;&#24314;&#27169;&#29615;&#22659;&#20013;&#30340;&#21464;&#21270;&#65292;&#20294;&#24448;&#24448;&#38656;&#35201;&#19981;&#20999;&#23454;&#38469;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35266;&#28857;&#65292;&#35748;&#20026;&#38750;&#24179;&#31283;&#24615;&#21487;&#20197;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#20013;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#20256;&#25773;&#21644;&#32047;&#31215;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#20854;&#22797;&#26434;&#24615;&#24182;&#24433;&#21709;&#31574;&#30053;&#23398;&#20064;&#12290;&#25105;&#20204;&#30456;&#20449;&#36890;&#36807;&#36861;&#36394;&#38750;&#24179;&#31283;&#24615;&#30340;&#22240;&#26524;&#36215;&#28304;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22240;&#26524;&#28304;&#34920;&#31034;&#65288;COREP&#65289;&#31639;&#27861;&#12290;COREP&#20027;&#35201;&#37319;&#29992;&#24341;&#23548;&#26356;&#26032;&#26426;&#21046;&#26469;&#23398;&#20064;&#19968;&#31181;&#31283;&#23450;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#31216;&#20026;&#22240;&#26524;&#36215;&#28304;&#34920;&#31034;&#65292;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#34920;&#31034;&#65292;&#23398;&#21040;&#30340;&#31574;&#30053;&#23545;&#38750;&#24179;&#31283;&#24615;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38887;&#24615;&#12290;&#25105;&#20204;&#34917;&#20805;&#20102;&#19968;&#20010;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios, the application of reinforcement learning is significantly challenged by complex non-stationarity. Most existing methods attempt to model changes in the environment explicitly, often requiring impractical prior knowledge. In this paper, we propose a new perspective, positing that non-stationarity can propagate and accumulate through complex causal relationships during state transitions, thereby compounding its sophistication and affecting policy learning. We believe that this challenge can be more effectively addressed by tracing the causal origin of non-stationarity. To this end, we introduce the Causal-Origin REPresentation (COREP) algorithm. COREP primarily employs a guided updating mechanism to learn a stable graph representation for states termed as causal-origin representation. By leveraging this representation, the learned policy exhibits impressive resilience to non-stationarity. We supplement our approach with a theoretical analysis grounded in the cau
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21333;&#20010;&#24555;&#29031;&#20013;&#37325;&#24314;&#22270;&#25193;&#25955;&#21382;&#21490;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00488</link><description>&lt;p&gt;
&#20174;&#21333;&#20010;&#24555;&#29031;&#20013;&#37325;&#24314;&#22270;&#25193;&#25955;&#21382;&#21490;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Graph Diffusion History from a Single Snapshot. (arXiv:2306.00488v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21333;&#20010;&#24555;&#29031;&#20013;&#37325;&#24314;&#22270;&#25193;&#25955;&#21382;&#21490;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25193;&#25955;&#22312;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#23436;&#25972;&#30340;&#25193;&#25955;&#21382;&#21490;&#22312;&#30830;&#23450;&#21160;&#24577;&#27169;&#24335;&#12289;&#21453;&#24605;&#39044;&#38450;&#25514;&#26045;&#21644;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#37325;&#35201;&#65292;&#20294;&#23436;&#25972;&#30340;&#25193;&#25955;&#21382;&#21490;&#24456;&#23569;&#21487;&#29992;&#65292;&#24182;&#19988;&#30001;&#20110;&#30149;&#24577;&#12289;&#29190;&#28856;&#24615;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#32570;&#20047;&#32780;&#20855;&#26377;&#26497;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#29992;&#20110;&#25193;&#25955;&#21382;&#21490;&#37325;&#24314;&#12290;&#23427;&#20204;&#20165;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#20844;&#24335;&#65292;&#38656;&#35201;&#30693;&#36947;&#30495;&#23454;&#30340;&#25193;&#25955;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26356;&#38590;&#30340;&#38382;&#39064;&#65292;&#21363;&#20174;&#21333;&#20010;&#24555;&#29031;&#65288;DASH&#65289;&#20013;&#37325;&#24314;&#25193;&#25955;&#21382;&#21490;&#65292;&#25105;&#20204;&#35797;&#22270;&#20165;&#20174;&#26368;&#32456;&#24555;&#29031;&#37325;&#24314;&#21382;&#21490;&#65292;&#32780;&#19981;&#30693;&#36947;&#30495;&#23454;&#30340;&#25193;&#25955;&#21442;&#25968;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#20998;&#26512;&#24320;&#22987;&#65292;&#25581;&#31034;&#20102;MLE&#20844;&#24335;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion on graphs is ubiquitous with numerous high-impact applications. In these applications, complete diffusion histories play an essential role in terms of identifying dynamical patterns, reflecting on precaution actions, and forecasting intervention effects. Despite their importance, complete diffusion histories are rarely available and are highly challenging to reconstruct due to ill-posedness, explosive search space, and scarcity of training data. To date, few methods exist for diffusion history reconstruction. They are exclusively based on the maximum likelihood estimation (MLE) formulation and require to know true diffusion parameters. In this paper, we study an even harder problem, namely reconstructing Diffusion history from A single SnapsHot} (DASH), where we seek to reconstruct the history from only the final snapshot without knowing true diffusion parameters. We start with theoretical analyses that reveal a fundamental limitation of the MLE formulation. We prove: (a) est
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#20984;&#32422;&#26463;&#38598;&#24773;&#20917;&#19979;&#30340;&#26354;&#32447;&#31354;&#38388;&#22312;&#32447;&#27979;&#22320;&#20984;&#20248;&#21270;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#65292;&#33719;&#24471;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.19349</link><description>&lt;p&gt;
&#20851;&#20110;&#40654;&#26364;&#27969;&#24418;&#19978;&#26080;&#25237;&#24433;&#22312;&#32447;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Riemannian Projection-free Online Learning. (arXiv:2305.19349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#20984;&#32422;&#26463;&#38598;&#24773;&#20917;&#19979;&#30340;&#26354;&#32447;&#31354;&#38388;&#22312;&#32447;&#27979;&#22320;&#20984;&#20248;&#21270;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#65292;&#33719;&#24471;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#24433;&#25805;&#20316;&#26159;&#35768;&#22810;&#20248;&#21270;&#31639;&#27861;&#65288;&#20363;&#22914;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;[OGD]&#65289;&#20013;&#24378;&#21046;&#32422;&#26463;&#21644;&#23454;&#29616;&#26368;&#20248;&#36951;&#25022;&#36793;&#30028;&#25152;&#24517;&#38656;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#39640;&#32500;&#35774;&#32622;&#25110;&#20855;&#26377;&#30149;&#24577;&#32422;&#26463;&#38598;&#26102;&#65292;&#23427;&#20250;&#21463;&#21040;&#35745;&#31639;&#22797;&#26434;&#24230;&#38480;&#21046;&#12290;&#26080;&#25237;&#24433;&#31639;&#27861;&#36890;&#36807;&#29992;&#26356;&#26377;&#25928;&#30340;&#20248;&#21270;&#23376;&#31243;&#24207;&#21462;&#20195;&#25237;&#24433;&#39044;&#27979;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#22312;&#27431;&#20960;&#37324;&#24471;&#35774;&#32622;&#20013;&#24320;&#21457;&#65292;&#24182;&#19988;&#34429;&#28982;&#36234;&#26469;&#36234;&#22810;&#22320;&#20851;&#27880;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#65292;&#20294;&#22312;&#23581;&#35797;&#21033;&#29992;&#26080;&#25237;&#24433;&#24037;&#20855;&#26041;&#38754;&#22522;&#26412;&#19978;&#27809;&#26377;&#24037;&#20316;&#12290;&#19968;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#26159;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#38750;&#24179;&#20961;&#30340;&#20223;&#23556;&#20989;&#25968;&#36890;&#24120;&#26159;&#38750;&#20984;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#26354;&#32447;&#31354;&#38388;&#19978;&#36827;&#34892;&#22312;&#32447;&#27979;&#22320;&#20984;&#20248;&#21270;&#65292;&#20197;&#33719;&#24471;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#20445;&#35777;&#65306;&#24403;&#25105;&#20204;&#35775;&#38382;&#65288;a&#65289;&#26102;
&lt;/p&gt;
&lt;p&gt;
The projection operation is a critical component in a wide range of optimization algorithms, such as online gradient descent (OGD), for enforcing constraints and achieving optimal regret bounds. However, it suffers from computational complexity limitations in high-dimensional settings or when dealing with ill-conditioned constraint sets. Projection-free algorithms address this issue by replacing the projection oracle with more efficient optimization subroutines. But to date, these methods have been developed primarily in the Euclidean setting, and while there has been growing interest in optimization on Riemannian manifolds, there has been essentially no work in trying to utilize projection-free tools here. An apparent issue is that non-trivial affine functions are generally non-convex in such domains. In this paper, we present methods for obtaining sub-linear regret guarantees in online geodesically convex optimization on curved spaces for two scenarios: when we have access to (a) a s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26059;&#36716;&#20248;&#21270;&#22120;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#21487;&#20197;&#31616;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#65292;&#29978;&#33267;&#22312;&#20960;&#20046;&#19981;&#38656;&#35843;&#25972;&#22522;&#32447;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#19982;&#21407;&#22987;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2305.17212</link><description>&lt;p&gt;
&#26059;&#36716;&#20248;&#21270;&#22120;&#65306;&#31616;&#21333;&#32780;&#24378;&#20581;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rotational Optimizers: Simple &amp; Robust DNN Training. (arXiv:2305.17212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26059;&#36716;&#20248;&#21270;&#22120;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#21487;&#20197;&#31616;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#65292;&#29978;&#33267;&#22312;&#20960;&#20046;&#19981;&#38656;&#35843;&#25972;&#22522;&#32447;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#19982;&#21407;&#22987;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#21462;&#20915;&#20110;&#23398;&#20064;&#29575;&#12289;&#26435;&#37325;&#34928;&#20943;&#12289;&#21021;&#22987;&#21270;&#31561;&#36229;&#21442;&#25968;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#20132;&#20114;&#20316;&#29992;&#21487;&#20197;&#22312;&#23610;&#24230;&#19981;&#21464;&#23618;&#65288;&#22914;&#24402;&#19968;&#21270;&#23618;&#65289;&#20013;&#20135;&#29983;&#29699;&#38754;&#36816;&#21160;&#21160;&#24577;&#65292;&#36825;&#20123;&#21160;&#24577;&#25910;&#25947;&#21040;&#24179;&#34913;&#29366;&#24577;&#65292;&#20854;&#20013;&#26435;&#37325;&#33539;&#25968;&#21644;&#39044;&#26399;&#26059;&#36716;&#26356;&#26032;&#22823;&#23567;&#26159;&#22266;&#23450;&#30340;&#12290;&#25105;&#20204;&#23545;AdamW&#12289;&#24102;&#21160;&#37327;&#30340;SGD&#21644;Lion&#20013;&#30340;&#36825;&#20010;&#24179;&#34913;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#21516;&#36229;&#21442;&#25968;&#21450;&#20854;&#30456;&#20114;&#20316;&#29992;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#20123;&#20248;&#21270;&#22120;&#30340;&#26059;&#36716;&#21464;&#20307;&#65288;RVs&#65289;&#65292;&#24378;&#21046;&#39044;&#26399;&#35282;&#24230;&#26356;&#26032;&#22823;&#23567;&#19982;&#25972;&#20010;&#35757;&#32451;&#26399;&#38388;&#30340;&#24179;&#34913;&#20540;&#30456;&#21305;&#37197;&#12290;&#36825;&#31616;&#21270;&#20102;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#28040;&#38500;&#25910;&#25947;&#21040;&#24179;&#34913;&#29366;&#24577;&#30340;&#30636;&#24577;&#30456;&#24212;&#12290;&#25105;&#20204;&#30340;&#26059;&#36716;&#20248;&#21270;&#22120;&#21487;&#20197;&#21305;&#37197;&#21407;&#22987;&#21464;&#20307;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#38656;&#35201;&#23545;&#22522;&#32447;&#36229;&#21442;&#25968;&#36827;&#34892;&#26368;&#23569;&#25110;&#19981;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training dynamics of modern deep neural networks depend on complex interactions between the learning rate, weight decay, initialization, and other hyperparameters. These interactions can give rise to Spherical Motion Dynamics in scale-invariant layers (e.g., normalized layers), which converge to an equilibrium state, where the weight norm and the expected rotational update size are fixed. Our analysis of this equilibrium in AdamW, SGD with momentum, and Lion provides new insights into the effects of different hyperparameters and their interactions on the training process. We propose rotational variants (RVs) of these optimizers that force the expected angular update size to match the equilibrium value throughout training. This simplifies the training dynamics by removing the transient phase corresponding to the convergence to an equilibrium. Our rotational optimizers can match the performance of the original variants, often with minimal or no tuning of the baseline hyperparameters,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#21442;&#25968;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#35270;&#22270;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#26377;&#21521;&#22270;&#19978;&#36827;&#34892;&#25805;&#20316;&#24182;&#34920;&#29616;&#20986;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15927</link><description>&lt;p&gt;
&#29992;&#26368;&#20248;&#20256;&#36755;&#23398;&#20064;&#26377;&#21521;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Directed Graphical Models with Optimal Transport. (arXiv:2305.15927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15927
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#21442;&#25968;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#35270;&#22270;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#26377;&#21521;&#22270;&#19978;&#36827;&#34892;&#25805;&#20316;&#24182;&#34920;&#29616;&#20986;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#20013;&#20272;&#35745;&#27010;&#29575;&#26377;&#21521;&#22270;&#27169;&#22411;&#30340;&#21442;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#23384;&#22312;&#28508;&#22312;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#27809;&#26377;&#20851;&#20110;&#32467;&#26500;&#20381;&#36182;&#24615;&#25110;&#27169;&#22411;&#31867;&#30340;&#36827;&#19968;&#27493;&#20551;&#35774;&#65292;&#20284;&#28982;&#20989;&#25968;&#21644;&#21518;&#39564;&#20998;&#24067;&#37117;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#23398;&#20064;&#26041;&#27861;&#22522;&#26412;&#19978;&#26159;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#20294;&#25105;&#20204;&#22312;&#36825;&#37324;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#21442;&#25968;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#20010;&#26032;&#35270;&#22270;&#12290;&#36825;&#20010;&#35266;&#28857;&#25480;&#26435;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#26377;&#21521;&#22270;&#19978;&#36816;&#20316;&#65292;&#32780;&#19981;&#20250;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#20570;&#20986;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#25110;&#35785;&#35832;&#20110;&#40657;&#31665;&#21464;&#20998;&#36817;&#20284;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25903;&#25345;&#23427;&#36890;&#36807;&#24191;&#27867;&#30340;&#32463;&#39564;&#35777;&#25454;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#24674;&#22797;&#22522;&#20934;&#21442;&#25968;&#65292;&#32780;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#20063;&#34920;&#29616;&#24471;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the parameters of a probabilistic directed graphical model from incomplete data remains a long-standing challenge. This is because, in the presence of latent variables, both the likelihood function and posterior distribution are intractable without further assumptions about structural dependencies or model classes. While existing learning methods are fundamentally based on likelihood maximization, here we offer a new view of the parameter learning problem through the lens of optimal transport. This perspective licenses a framework that operates on many directed graphs without making unrealistic assumptions on the posterior over the latent variables or resorting to black-box variational approximations. We develop a theoretical framework and support it with extensive empirical evidence demonstrating the flexibility and versatility of our approach. Across experiments, we show that not only can our method recover the ground-truth parameters but it also performs competitively on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#25968;&#25454;&#26368;&#23567;&#20108;&#20056;&#23725;&#27491;&#21017;&#21270;&#30340;&#21435;&#22122;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20250;&#20986;&#29616;&#21452;&#23792;&#35895;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.14689</link><description>&lt;p&gt;
&#22522;&#20110;&#23725;&#27491;&#21017;&#21270;&#30340;&#32447;&#24615;&#25968;&#25454;&#26368;&#23567;&#20108;&#20056;&#21435;&#22122;&#38382;&#39064;&#30340;&#27424;&#21442;&#25968;&#21270;&#21452;&#35895;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Under-Parameterized Double Descent for Ridge Regularized Least Squares Denoising of Data on a Line. (arXiv:2305.14689v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#25968;&#25454;&#26368;&#23567;&#20108;&#20056;&#23725;&#27491;&#21017;&#21270;&#30340;&#21435;&#22122;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20250;&#20986;&#29616;&#21452;&#23792;&#35895;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#35757;&#32451;&#25968;&#25454;&#28857;&#25968;&#12289;&#32479;&#35745;&#27169;&#22411;&#21442;&#25968;&#25968;&#21644;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#24050;&#26377;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36807;&#24230;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#21487;&#33021;&#20986;&#29616;&#21452;&#23792;&#35895;&#29616;&#35937;&#65292;&#32780;&#22312;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#21017;&#26222;&#36941;&#23384;&#22312;&#26631;&#20934;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20363;&#23376;&#65292;&#21487;&#20197;&#35777;&#26126;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#21487;&#20197;&#21457;&#29983;&#21452;&#23792;&#35895;&#29616;&#35937;&#12290;&#32771;&#34385;&#23884;&#20837;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#25968;&#25454;&#26368;&#23567;&#20108;&#20056;&#21435;&#22122;&#38382;&#39064;&#20013;&#30340;&#23725;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#31181;&#28176;&#36817;&#20934;&#30830;&#30340;&#24191;&#20041;&#35823;&#24046;&#20844;&#24335;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26679;&#26412;&#21644;&#21442;&#25968;&#30340;&#21452;&#35895;&#25928;&#24212;&#65292;&#21452;&#23792;&#35895;&#20301;&#20110;&#25554;&#20540;&#28857;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#21306;&#22495;&#20043;&#38388;&#12290;&#27492;&#22806;&#65292;&#26679;&#26412;&#21452;&#35895;&#26354;&#32447;&#30340;&#39640;&#23792;&#23545;&#24212;&#20110;&#20272;&#35745;&#37327;&#30340;&#33539;&#25968;&#26354;&#32447;&#30340;&#39640;&#23792;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relationship between the number of training data points, the number of parameters in a statistical model, and the generalization capabilities of the model has been widely studied. Previous work has shown that double descent can occur in the over-parameterized regime, and believe that the standard bias-variance trade-off holds in the under-parameterized regime. In this paper, we present a simple example that provably exhibits double descent in the under-parameterized regime. For simplicity, we look at the ridge regularized least squares denoising problem with data on a line embedded in high-dimension space. By deriving an asymptotically accurate formula for the generalization error, we observe sample-wise and parameter-wise double descent with the peak in the under-parameterized regime rather than at the interpolation point or in the over-parameterized regime.  Further, the peak of the sample-wise double descent curve corresponds to a peak in the curve for the norm of the estimator,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.13673</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;&#19968;&#37096;&#20998;&#65292;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;-&#20855;&#26377;&#26641;&#29366;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#31995;&#32479;&#65292;&#21487;&#25429;&#25417;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#65292;&#31243;&#24207;&#21644;&#20154;&#31867;&#36923;&#36753;&#30340;&#26041;&#38754;&#12290;CFG&#19982;&#19979;&#25512;&#33258;&#21160;&#26426;&#19968;&#26679;&#22256;&#38590;&#65292;&#21487;&#33021;&#26159;&#27169;&#26865;&#20004;&#21487;&#30340;&#65292;&#22240;&#27492;&#39564;&#35777;&#23383;&#31526;&#20018;&#26159;&#21542;&#28385;&#36275;&#35268;&#21017;&#38656;&#35201;&#21160;&#24577;&#35268;&#21010;&#12290;&#25105;&#20204;&#26500;&#36896;&#20102;&#20154;&#36896;&#25968;&#25454;&#65292;&#24182;&#35777;&#26126;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CFG&#65292;&#39044;&#35757;&#32451;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;transformers&#23398;&#20064;CFG&#32972;&#21518;&#30340;&#29289;&#29702;&#21407;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65288;&#22914;&#22312;&#23376;&#26641;&#36793;&#30028;&#19978;&#31934;&#30830;&#23450;&#20301;&#26641;&#33410;&#28857;&#20449;&#24687;&#65289;&#65292;&#24182;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#19968;&#20123;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#23637;&#31034;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design experiments to study $\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\textit{diversity}$.  More importantly, we delve into the $\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extensio
&lt;/p&gt;</description></item><item><title>NUBO&#26159;&#19968;&#20010;&#36879;&#26126;&#30340;Python&#21253;&#65292;&#29992;&#20110;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#65292;&#23427;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#20570;&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#33719;&#21462;&#20989;&#25968;&#26469;&#25351;&#23548;&#36873;&#25321;&#20505;&#36873;&#28857;&#65292;&#19987;&#27880;&#20110;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.06709</link><description>&lt;p&gt;
NUBO&#65306;&#19968;&#20010;&#36879;&#26126;&#30340; Python &#21253;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
NUBO: A Transparent Python Package for Bayesian Optimisation. (arXiv:2305.06709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06709
&lt;/p&gt;
&lt;p&gt;
NUBO&#26159;&#19968;&#20010;&#36879;&#26126;&#30340;Python&#21253;&#65292;&#29992;&#20110;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#65292;&#23427;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#20570;&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#33719;&#21462;&#20989;&#25968;&#26469;&#25351;&#23548;&#36873;&#25321;&#20505;&#36873;&#28857;&#65292;&#19987;&#27880;&#20110;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NUBO&#65288;Newcastle University Bayesian Optimisation&#65289;&#26159;&#19968;&#20010;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#65292;&#27604;&#22914;&#29289;&#29702;&#23454;&#39564;&#21644;&#35745;&#31639;&#26426;&#27169;&#25311;&#22120;&#12290;&#23427;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#20570;&#20195;&#29702;&#27169;&#22411;&#12289;&#24182;&#36890;&#36807;&#33719;&#21462;&#20989;&#25968;&#26469;&#36873;&#25321;&#29992;&#20110;&#20840;&#23616;&#26368;&#20248;&#21270;&#30340;&#20505;&#36873;&#28857;&#12290;NUBO&#19987;&#27880;&#20110;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20307;&#39564;&#65292;&#20197;&#20415;&#35753;&#19981;&#21516;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26356;&#23481;&#26131;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
NUBO, short for Newcastle University Bayesian Optimisation, is a Bayesian optimisation framework for the optimisation of expensive-to-evaluate black-box functions, such as physical experiments and computer simulators. Bayesian optimisation is a cost-efficient optimisation strategy that uses surrogate modelling via Gaussian processes to represent an objective function and acquisition functions to guide the selection of candidate points to approximate the global optimum of the objective function. NUBO itself focuses on transparency and user experience to make Bayesian optimisation easily accessible to researchers from all disciplines. Clean and understandable code, precise references, and thorough documentation ensure transparency, while user experience is ensured by a modular and flexible design, easy-to-write syntax, and careful selection of Bayesian optimisation algorithms. NUBO allows users to tailor Bayesian optimisation to their specific problem by writing the optimisation loop the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#34920;&#26126;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07702</link><description>&lt;p&gt;
&#29992;BREC&#25968;&#25454;&#38598;&#26356;&#22909;&#22320;&#35780;&#20272;GNN&#34920;&#36798;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Better Evaluation of GNN Expressiveness with BREC Dataset. (arXiv:2304.07702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#34920;&#26126;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#29702;&#35770;&#34920;&#36798;&#21147;&#30340;&#30740;&#31350;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#35768;&#22810;&#22686;&#24378;&#34920;&#36798;&#21147;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#20005;&#26684;&#36981;&#24490;k&#32500;Weisfeiler-Lehman&#65288;k-WL&#65289;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;&#30340;&#23569;&#25968;&#26041;&#27861;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#27809;&#26377;&#32479;&#19968;&#30340;&#34920;&#36798;&#21147;&#24230;&#37327;&#12290;&#23427;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#36890;&#24120;&#38480;&#20110;&#21306;&#20998;&#26576;&#20123;&#38750;&#21516;&#26500;&#22270;&#26063;&#65292;&#23548;&#33268;&#22312;&#23450;&#37327;&#27604;&#36739;&#34920;&#36798;&#21147;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#19982;&#29702;&#35770;&#20998;&#26512;&#30456;&#21453;&#65292;&#34913;&#37327;&#34920;&#36798;&#33021;&#21147;&#30340;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#21253;&#21547;1-WL&#19981;&#21487;&#21306;&#20998;&#22270;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#25968;&#25454;&#38598;&#38754;&#20020;&#30528;&#38590;&#24230;&#65288;&#20219;&#20309;&#36229;&#36234;1-WL&#30340;&#27169;&#22411;&#20934;&#30830;&#29575;&#20960;&#20046;&#36798;&#21040;100&#65285;&#65289;&#12289;&#31890;&#24230;&#65288;&#27169;&#22411;&#20542;&#21521;&#20110;&#35201;&#20040;&#23436;&#20840;&#27491;&#30830;&#65292;&#35201;&#20040;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#65289;&#21644;&#35268;&#27169;&#65288;&#27599;&#20010;&#25968;&#25454;&#38598;&#20013;&#20165;&#26377;&#23569;&#37327;&#26412;&#36136;&#19981;&#21516;&#30340;&#22270;&#65289;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#21463;&#38480;&#21046;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;GNN&#40065;&#26834;&#24615;&#35780;&#20272;&#22522;&#20934;&#65288;BREC&#65289;&#65292;&#35813;&#22522;&#20934;&#21253;&#21547;&#35768;&#22810;&#32467;&#26500;&#22810;&#26679;&#30340;&#22270;&#65292;&#24182;&#20801;&#35768;&#23545;&#27169;&#22411;&#34920;&#36798;&#21147;&#36827;&#34892;&#26356;&#31934;&#32454;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on the theoretical expressiveness of Graph Neural Networks (GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, most methods do not have a uniform expressiveness measure except for a few that strictly follow the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test hierarchy. Their theoretical analyses are often limited to distinguishing certain families of non-isomorphic graphs, leading to difficulties in quantitatively comparing their expressiveness. In contrast to theoretical analysis, another way to measure expressiveness is by evaluating model performance on certain datasets containing 1-WL-indistinguishable graphs. Previous datasets specifically designed for this purpose, however, face problems with difficulty (any model surpassing 1-WL has nearly 100% accuracy), granularity (models tend to be either 100% correct or near random guess), and scale (only a few essentially different graphs in each dataset). To address these limi
&lt;/p&gt;</description></item><item><title>MedNeXt&#26159;&#19968;&#20010;&#23450;&#21046;&#21270;&#30340;&#29616;&#20195;&#21270;&#21487;&#25193;&#23637;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#21307;&#23398;&#29615;&#22659;&#25361;&#25112;&#12290;&#35813;&#32593;&#32476;&#21253;&#21547;&#65306;&#23436;&#20840;ConvNeXt 3D&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12289;&#27531;&#24046;ConvNeXt&#19978;&#19979;&#37319;&#26679;&#22359;&#21644;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#22686;&#21152;&#26680;&#22823;&#23567;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.09975</link><description>&lt;p&gt;
MedNeXt&#65306;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#21464;&#21387;&#22120;&#39537;&#21160;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation. (arXiv:2303.09975v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09975
&lt;/p&gt;
&lt;p&gt;
MedNeXt&#26159;&#19968;&#20010;&#23450;&#21046;&#21270;&#30340;&#29616;&#20195;&#21270;&#21487;&#25193;&#23637;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#21307;&#23398;&#29615;&#22659;&#25361;&#25112;&#12290;&#35813;&#32593;&#32476;&#21253;&#21547;&#65306;&#23436;&#20840;ConvNeXt 3D&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12289;&#27531;&#24046;ConvNeXt&#19978;&#19979;&#37319;&#26679;&#22359;&#21644;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#22686;&#21152;&#26680;&#22823;&#23567;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#20351;&#29992;&#22522;&#20110; Transformer &#30340;&#26550;&#26500;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#26159;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#26631;&#27880;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#20854;&#24615;&#33021;&#36828;&#19981;&#22914;&#33258;&#28982;&#22270;&#20687;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#39640;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#22240;&#27492;&#26356;&#23481;&#26131;&#35757;&#32451;&#21040;&#39640;&#24615;&#33021;&#27700;&#24179;&#12290;&#26368;&#36817;&#65292;ConvNeXt &#26550;&#26500;&#23581;&#35797;&#36890;&#36807;&#38236;&#20687;&#21464;&#21387;&#22120;&#22359;&#26469;&#29616;&#20195;&#21270;&#26631;&#20934;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#36825;&#19968;&#26550;&#26500;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29616;&#20195;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#24212;&#23545;&#25968;&#25454;&#31232;&#32570;&#30340;&#21307;&#23398;&#29615;&#22659;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837; MedNeXt&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#21464;&#21387;&#22120;&#21551;&#21457;&#30340;&#22823;&#26680;&#20998;&#21106;&#32593;&#32476;&#65292;&#20854;&#20013;&#21253;&#25324;&#65306;1&#65289;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#23436;&#20840; ConvNeXt 3D &#32534;&#30721;&#22120; - &#35299;&#30721;&#22120;&#32593;&#32476;&#65292;2&#65289;&#27531;&#24046; ConvNeXt &#19978;&#19979;&#37319;&#26679;&#22359;&#65292;&#20197;&#22312;&#21508;&#20010;&#23610;&#24230;&#19978;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#65292;3&#65289;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#19978;&#37319;&#26679;&#23567;&#26680;&#26469;&#36845;&#20195;&#22686;&#21152;&#26680;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been exploding interest in embracing Transformer-based architectures for medical image segmentation. However, the lack of large-scale annotated medical datasets make achieving performances equivalent to those in natural images challenging. Convolutional networks, in contrast, have higher inductive biases and consequently, are easily trainable to high performance. Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet by mirroring Transformer blocks. In this work, we improve upon this to design a modernized and scalable convolutional architecture customized to challenges of data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up and downsampling blocks to preserve semantic richness across scales, 3) A novel technique to iteratively increase kernel sizes by upsampling small kernel 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;&#65288;DBC&#65289;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#24314;&#27169;&#19987;&#23478;&#20998;&#24067;&#30340;&#26465;&#20214;&#21644;&#32852;&#21512;&#27010;&#29575;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#24314;&#27169;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13335</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model-Augmented Behavioral Cloning. (arXiv:2302.13335v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;&#65288;DBC&#65289;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#24314;&#27169;&#19987;&#23478;&#20998;&#24067;&#30340;&#26465;&#20214;&#21644;&#32852;&#21512;&#27010;&#29575;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#24314;&#27169;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#35299;&#20915;&#20102;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#28436;&#31034;&#32780;&#27809;&#26377;&#35775;&#38382;&#29615;&#22659;&#22870;&#21169;&#20449;&#21495;&#30340;&#23398;&#20064;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#19981;&#38656;&#35201;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#35201;&#20040;&#23558;&#19987;&#23478;&#20998;&#24067;&#24314;&#27169;&#20026;&#26465;&#20214;&#27010;&#29575;p(a|s)&#65288;&#20363;&#22914;&#65292;&#34892;&#20026;&#20811;&#38534;&#65292;BC&#65289;&#65292;&#35201;&#20040;&#23558;&#32852;&#21512;&#27010;&#29575;p(s,a)&#24314;&#27169;&#65288;&#20363;&#22914;&#65292;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#65289;&#12290;&#23613;&#31649;&#34892;&#20026;&#20811;&#38534;&#23545;&#20110;&#24314;&#27169;&#26465;&#20214;&#27010;&#29575;&#30340;&#31616;&#21333;&#24615;&#65292;&#20294;&#36890;&#24120;&#38590;&#20197;&#27867;&#21270;&#12290;&#34429;&#28982;&#23545;&#32852;&#21512;&#27010;&#29575;&#36827;&#34892;&#24314;&#27169;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#25512;&#29702;&#36807;&#31243;&#21487;&#33021;&#32791;&#26102;&#65292;&#24182;&#19988;&#24448;&#24448;&#20250;&#36973;&#21463;&#27969;&#24418;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20174;&#24314;&#27169;&#19987;&#23478;&#20998;&#24067;&#30340;&#26465;&#20214;&#21644;&#32852;&#21512;&#27010;&#29575;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;&#65288;DBC&#65289;&#37319;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#19987;&#23478;&#34892;&#20026;&#65292;&#24182;&#23398;&#20064;&#19968;&#31181;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#26681;&#25454;&#28151;&#21512;&#27010;&#29575;&#20998;&#24067;&#37319;&#26679;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning addresses the challenge of learning by observing an expert's demonstrations without access to reward signals from environments. Most existing imitation learning methods that do not require interacting with environments either model the expert distribution as the conditional probability p(a|s) (e.g., behavioral cloning, BC) or the joint probability p(s, a) (e.g., implicit behavioral cloning). Despite its simplicity, modeling the conditional probability with BC usually struggles with generalization. While modeling the joint probability can lead to improved generalization performance, the inference procedure can be time-consuming and it often suffers from manifold overfitting. This work proposes an imitation learning framework that benefits from modeling both the conditional and joint probability of the expert distribution. Our proposed diffusion model-augmented behavioral cloning (DBC) employs a diffusion model trained to model expert behaviors and learns a policy to o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#29702;&#35770;&#35752;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25345;&#32493;&#21516;&#35843;&#25216;&#26415;&#22312;&#25429;&#25417;&#20855;&#26377;&#26174;&#33879;&#25299;&#25169;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#20013;&#30340;&#38271;&#31243;&#22270;&#24615;&#36136;&#26041;&#38754;&#34920;&#29616;&#20986;&#30340;&#39640;&#34920;&#36798;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.09826</link><description>&lt;p&gt;
&#25345;&#32493;&#21516;&#35843;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#34920;&#36798;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Expressivity of Persistent Homology in Graph Learning. (arXiv:2302.09826v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#29702;&#35770;&#35752;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25345;&#32493;&#21516;&#35843;&#25216;&#26415;&#22312;&#25429;&#25417;&#20855;&#26377;&#26174;&#33879;&#25299;&#25169;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#20013;&#30340;&#38271;&#31243;&#22270;&#24615;&#36136;&#26041;&#38754;&#34920;&#29616;&#20986;&#30340;&#39640;&#34920;&#36798;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#35745;&#31639;&#25299;&#25169;&#23398;&#20013;&#30340;&#19968;&#39033;&#25216;&#26415;&#65292;&#25345;&#32493;&#21516;&#35843;&#23637;&#29616;&#20986;&#22312;&#22270;&#20998;&#31867;&#26041;&#38754;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#36890;&#36807;&#39640;&#38454;&#25299;&#25169;&#29305;&#24449;&#8212;&#8212;&#22914;&#20219;&#24847;&#38271;&#24230;&#30340;&#29615;&#8212;&#8212;&#20197;&#21450;&#22810;&#23610;&#24230;&#25299;&#25169;&#25551;&#36848;&#31526;&#25429;&#25417;&#38271;&#31243;&#22270;&#24615;&#36136;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#20855;&#26377;&#26174;&#33879;&#25299;&#25169;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#8212;&#8212;&#22914;&#20998;&#23376;&#8212;&#8212;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25345;&#32493;&#21516;&#35843;&#30340;&#29702;&#35770;&#24615;&#36136;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#23578;&#26410;&#24471;&#21040;&#27491;&#24335;&#35780;&#20272;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#25345;&#32493;&#21516;&#35843;&#22312;&#22270;&#20013;&#30340;&#31616;&#35201;&#20171;&#32461;&#20197;&#21450;&#23545;&#20854;&#22312;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34920;&#36798;&#24615;&#36827;&#34892;&#29702;&#35770;&#35752;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#24357;&#21512;&#35745;&#31639;&#25299;&#25169;&#23398;&#21644;&#22270;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persistent homology, a technique from computational topology, has recently shown strong empirical performance in the context of graph classification. Being able to capture long range graph properties via higher-order topological features, such as cycles of arbitrary length, in combination with multi-scale topological descriptors, has improved predictive performance for data sets with prominent topological structures, such as molecules. At the same time, the theoretical properties of persistent homology have not been formally assessed in this context. This paper intends to bridge the gap between computational topology and graph machine learning by providing a brief introduction to persistent homology in the context of graphs, as well as a theoretical discussion and empirical analysis of its expressivity for graph learning tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#30446;&#26631;&#39044;&#27979;&#25193;&#25955;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#19981;&#33021;&#24456;&#22909;&#22320;&#24314;&#27169;&#22270;&#25299;&#25169;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.03596</link><description>&lt;p&gt;
&#24102;&#26377;&#30446;&#26631;&#39044;&#27979;&#25193;&#25955;&#28151;&#21512;&#30340;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Graph Generation with Destination-Predicting Diffusion Mixture. (arXiv:2302.03596v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#30446;&#26631;&#39044;&#27979;&#25193;&#25955;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#19981;&#33021;&#24456;&#22909;&#22320;&#24314;&#27169;&#22270;&#25299;&#25169;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22270;&#26159;&#29702;&#35299;&#20854;&#38750;&#27431;&#20960;&#37324;&#24471;&#32467;&#26500;&#22797;&#26434;&#24615;&#30340;&#30495;&#23454;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#19981;&#36866;&#21512;&#24314;&#27169;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#22240;&#20026;&#23398;&#20064;&#21435;&#22122;&#22768;&#26679;&#26412;&#19981;&#33021;&#26126;&#30830;&#22320;&#25429;&#25417;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#27979;&#25193;&#25955;&#36807;&#31243;&#30340;&#30446;&#26631;&#65292;&#21363;&#20855;&#26377;&#27491;&#30830;&#25299;&#25169;&#20449;&#24687;&#30340;&#21407;&#22987;&#22270;&#20316;&#20026;&#25968;&#25454;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#65292;&#24314;&#27169;&#20102;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#29983;&#25104;&#36807;&#31243;&#35774;&#35745;&#20026;&#19968;&#20010;&#20197;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#32456;&#28857;&#20026;&#26465;&#20214;&#30340;&#25193;&#25955;&#36807;&#31243;&#28151;&#21512;&#65292;&#23427;&#23558;&#36807;&#31243;&#25512;&#21521;&#39044;&#27979;&#30340;&#30446;&#26631;&#65292;&#24182;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#39044;&#27979;&#30446;&#26631;&#30340;&#26032;&#22411;&#26080;&#20223;&#30495;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#23558;&#36825;&#31181;&#31574;&#30053;&#32435;&#20837;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of graphs is a major challenge for real-world tasks that require understanding the complex nature of their non-Euclidean structures. Although diffusion models have achieved notable success in graph generation recently, they are ill-suited for modeling the structural information of graphs since learning to denoise the noisy samples does not explicitly capture the graph topology. To tackle this limitation, we propose a novel generative framework that models the topology of graphs by predicting the destination of the diffusion process, which is the original graph that has the correct topology information, as a weighted mean of data. Specifically, we design the generative process as a mixture of diffusion processes conditioned on the endpoint in the data distribution, which drives the process toward the predicted destination, resulting in rapid convergence. We introduce new simulation-free training objectives for predicting the destination, and further discuss the advantages of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;GFlowNets&#21644;&#21464;&#20998;&#36125;&#21494;&#26031;&#32852;&#21512;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#26426;&#21046;&#65292;&#19981;&#20165;&#33021;&#22815;&#22788;&#29702;&#38750;&#32447;&#24615;&#21644;&#38750;&#39640;&#26031;&#25968;&#25454;&#65292;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#20063;&#33021;&#19982;&#20960;&#20010;&#22522;&#32447;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2211.02763</link><description>&lt;p&gt;
GFlowNets&#19982;&#21464;&#20998;&#36125;&#21494;&#26031;&#30340;&#22240;&#26524;&#32467;&#26500;&#21644;&#26426;&#21046;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian learning of Causal Structure and Mechanisms with GFlowNets and Variational Bayes. (arXiv:2211.02763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;GFlowNets&#21644;&#21464;&#20998;&#36125;&#21494;&#26031;&#32852;&#21512;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#26426;&#21046;&#65292;&#19981;&#20165;&#33021;&#22815;&#22788;&#29702;&#38750;&#32447;&#24615;&#21644;&#38750;&#39640;&#26031;&#25968;&#25454;&#65292;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#20063;&#33021;&#19982;&#20960;&#20010;&#22522;&#32447;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#19978;&#30340;&#21518;&#39564;&#20998;&#24067;&#21644;&#23450;&#20041;&#29238;&#21464;&#37327;&#21644;&#23376;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#32852;&#21512;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#26426;&#21046;&#65292;&#31216;&#20026;&#21464;&#20998;&#36125;&#21494;&#26031;DAG-GFlowNet&#65288;VBG&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;GFlowNets&#25193;&#23637;&#20102;&#36125;&#21494;&#26031;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#23398;&#20064;&#32467;&#26500;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#36824;&#23398;&#20064;&#32447;&#24615;&#39640;&#26031;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;VBG&#22312;&#24314;&#27169;DAG&#21644;&#26426;&#21046;&#30340;&#21518;&#39564;&#20998;&#24067;&#26102;&#65292;&#19981;&#20165;&#33021;&#22815;&#22788;&#29702;&#38750;&#32447;&#24615;&#21644;&#38750;&#39640;&#26031;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#33021;&#19982;&#20960;&#20010;&#22522;&#32447;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian causal structure learning aims to learn a posterior distribution over directed acyclic graphs (DAGs), and the mechanisms that define the relationship between parent and child variables. By taking a Bayesian approach, it is possible to reason about the uncertainty of the causal model. The notion of modelling the uncertainty over models is particularly crucial for causal structure learning since the model could be unidentifiable when given only a finite amount of observational data. In this paper, we introduce a novel method to jointly learn the structure and mechanisms of the causal model using Variational Bayes, which we call Variational Bayes-DAG-GFlowNet (VBG). We extend the method of Bayesian causal structure learning using GFlowNets to learn not only the posterior distribution over the structure, but also the parameters of a linear-Gaussian model. Our results on simulated data suggest that VBG is competitive against several baselines in modelling the posterior over DAGs an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#21644;&#26426;&#23494;&#35745;&#31639;&#30340;&#32467;&#21512;&#65292;&#24182;&#26803;&#29702;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#20445;&#35777;&#26426;&#23494;&#24615;&#21644;&#23436;&#25972;&#24615;&#30340;&#25216;&#26415;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#39640;&#32423;&#29305;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#29616;&#26377;&#30340;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#31995;&#32479;&#22312;&#26426;&#22120;&#23398;&#20064;&#29992;&#20363;&#20013;&#30340;&#38480;&#21046;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#23637;&#26395;&#12290;</title><link>http://arxiv.org/abs/2208.10134</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#19982;&#26426;&#23494;&#35745;&#31639;&#65306;&#30693;&#35782;&#31995;&#32479;&#21270;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning with Confidential Computing: A Systematization of Knowledge. (arXiv:2208.10134v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#21644;&#26426;&#23494;&#35745;&#31639;&#30340;&#32467;&#21512;&#65292;&#24182;&#26803;&#29702;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#20445;&#35777;&#26426;&#23494;&#24615;&#21644;&#23436;&#25972;&#24615;&#30340;&#25216;&#26415;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#39640;&#32423;&#29305;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#29616;&#26377;&#30340;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#31995;&#32479;&#22312;&#26426;&#22120;&#23398;&#20064;&#29992;&#20363;&#20013;&#30340;&#38480;&#21046;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#24191;&#27867;&#21457;&#23637;&#21644;&#25915;&#20987;&#38754;&#30340;&#25193;&#22823;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#25361;&#25112;&#26085;&#30410;&#20005;&#37325;&#12290;&#20316;&#20026;&#19968;&#31181;&#25104;&#29087;&#30340;&#31995;&#32479;&#32423;&#26041;&#27861;&#65292;&#26426;&#23494;&#35745;&#31639;&#24050;&#34987;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#29992;&#20110;&#32531;&#35299;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#26426;&#23494;&#35745;&#31639;&#20043;&#38388;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#31995;&#32479;&#26803;&#29702;&#20102;&#20808;&#21069;&#22522;&#20110;&#26426;&#23494;&#35745;&#31639;&#36741;&#21161;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;i&#65289;&#26426;&#23494;&#24615;&#20445;&#35777;&#21644;ii&#65289;&#23436;&#25972;&#24615;&#20445;&#35777;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#39640;&#32423;&#29305;&#24615;&#21644;&#32570;&#38519;&#12290;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#23545;&#29616;&#26377;&#30340;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#31995;&#32479;&#22312;&#26426;&#22120;&#23398;&#20064;&#29992;&#20363;&#20013;&#30340;&#38480;&#21046;&#36827;&#34892;&#20102;&#19987;&#38376;&#30340;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#23637;&#26395;&#24615;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#38381;&#29615;&#20445;&#25252;&#30340;&#22522;&#20110;&#22320;&#38754;&#30340;&#38544;&#31169;&#23450;&#20041;&#65292;&#39640;&#25928;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#21306;&#25191;&#34892;&#65292;&#19987;&#38376;&#30340;TEE&#36741;&#21161;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy and security challenges in Machine Learning (ML) have become increasingly severe, along with ML's pervasive development and the recent demonstration of large attack surfaces. As a mature system-oriented approach, Confidential Computing has been utilized in both academia and industry to mitigate privacy and security issues in various ML scenarios. In this paper, the conjunction between ML and Confidential Computing is investigated. We systematize the prior work on Confidential Computing-assisted ML techniques that provide i) confidentiality guarantees and ii) integrity assurances, and discuss their advanced features and drawbacks. Key challenges are further identified, and we provide dedicated analyses of the limitations in existing Trusted Execution Environment (TEE) systems for ML use cases. Finally, prospective works are discussed, including grounded privacy definitions for closed-loop protection, partitioned executions of efficient ML, dedicated TEE-assisted designs for ML, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#20934;&#26469;&#34913;&#37327;&#20844;&#24179;&#20248;&#21270;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615;&#8212;&#8212;&#31283;&#20581;&#27604;&#29575;&#65292;&#24182;&#20351;&#29992;&#19977;&#31181;&#20844;&#24179;&#31574;&#30053;&#22312;&#20116;&#20010;&#20844;&#24179;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22810;&#27425;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20844;&#24179;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#21644;&#19981;&#21516;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2207.04581</link><description>&lt;p&gt;
&#20320;&#30340;&#20844;&#24179;&#27169;&#22411;&#26377;&#22810;&#31283;&#20581;&#65311;&#25506;&#32034;&#19981;&#21516;&#20844;&#24179;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Robust is your Fair Model? Exploring the Robustness of Diverse Fairness Strategies. (arXiv:2207.04581v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#20934;&#26469;&#34913;&#37327;&#20844;&#24179;&#20248;&#21270;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615;&#8212;&#8212;&#31283;&#20581;&#27604;&#29575;&#65292;&#24182;&#20351;&#29992;&#19977;&#31181;&#20844;&#24179;&#31574;&#30053;&#22312;&#20116;&#20010;&#20844;&#24179;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22810;&#27425;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20844;&#24179;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#21644;&#19981;&#21516;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#65292;&#30830;&#20445;&#31639;&#27861;&#20844;&#24179;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#24840;&#21457;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25968;&#23398;&#19978;&#30340;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#24182;&#24320;&#21457;&#20102;&#21508;&#31181;&#20248;&#21270;&#25216;&#26415;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#23450;&#20041;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#20844;&#24179;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#32780;&#19988;&#23545;&#22122;&#22768;&#38750;&#24120;&#25935;&#24863;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31283;&#20581;&#24615;&#65288;&#27169;&#22411;&#22312;&#26410;&#30693;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#33021;&#21147;&#65289;&#22312;&#24212;&#23545;&#26032;&#38382;&#39064;&#26102;&#24212;&#20351;&#29992;&#30340;&#31574;&#30053;&#31867;&#22411;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#27492;&#34913;&#37327;&#36825;&#20123;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#20934;&#26469;&#34913;&#37327;&#21508;&#31181;&#20844;&#24179;&#20248;&#21270;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615; - &#31283;&#20581;&#27604;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#26368;&#24120;&#29992;&#30340;&#20844;&#24179;&#31574;&#30053;&#23545;&#20116;&#20010;&#22522;&#20934;&#20844;&#24179;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22810;&#27425;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#21644;&#19981;&#21516;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35774;&#35745;&#21644;&#36873;&#25321;&#20844;&#24179;&#31639;&#27861;&#26102;&#65292;&#24212;&#26356;&#21152;&#35880;&#24910;&#22320;&#32771;&#34385;&#31283;&#20581;&#24615;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#22987;&#32456;&#26377;&#25928;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the introduction of machine learning in high-stakes decision making, ensuring algorithmic fairness has become an increasingly important problem to solve. In response to this, many mathematical definitions of fairness have been proposed, and a variety of optimisation techniques have been developed, all designed to maximise a defined notion of fairness. However, fair solutions are reliant on the quality of the training data, and can be highly sensitive to noise. Recent studies have shown that robustness (the ability for a model to perform well on unseen data) plays a significant role in the type of strategy that should be used when approaching a new problem and, hence, measuring the robustness of these strategies has become a fundamental problem. In this work, we therefore propose a new criterion to measure the robustness of various fairness optimisation strategies - the robustness ratio. We conduct multiple extensive experiments on five bench mark fairness data sets using three of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;NVIDIA&#32593;&#21345;&#20013;&#23454;&#29616;&#20102;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#20013;&#24515;&#25317;&#22622;&#25511;&#21046;&#65292;&#36890;&#36807;&#23558;RL-CC&#30340;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#20915;&#31574;&#26641;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#25512;&#29702;&#65292;&#24182;&#25104;&#21151;&#25913;&#21892;&#20102;&#32593;&#32476;&#25317;&#22622;&#19979;&#30340;&#23614;&#37096;&#24310;&#36831;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.02295</link><description>&lt;p&gt;
&#22312;NVIDIA&#32593;&#21345;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#20013;&#24515;&#25317;&#22622;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Implementing Reinforcement Learning Datacenter Congestion Control in NVIDIA NICs. (arXiv:2207.02295v4 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;NVIDIA&#32593;&#21345;&#20013;&#23454;&#29616;&#20102;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#20013;&#24515;&#25317;&#22622;&#25511;&#21046;&#65292;&#36890;&#36807;&#23558;RL-CC&#30340;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#20915;&#31574;&#26641;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#25512;&#29702;&#65292;&#24182;&#25104;&#21151;&#25913;&#21892;&#20102;&#32593;&#32476;&#25317;&#22622;&#19979;&#30340;&#23614;&#37096;&#24310;&#36831;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#20449;&#21327;&#35758;&#30340;&#21457;&#23637;&#65292;&#25968;&#25454;&#20013;&#24515;&#32593;&#32476;&#30340;&#21033;&#29992;&#29575;&#36234;&#26469;&#36234;&#39640;&#65292;&#25317;&#22622;&#26356;&#20026;&#39057;&#32321;&#65292;&#23548;&#33268;&#24310;&#36831;&#21644;&#20002;&#21253;&#29575;&#22686;&#21152;&#12290;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#24037;&#35774;&#35745;&#25317;&#22622;&#25511;&#21046;&#31639;&#27861;&#21464;&#24471;&#26497;&#20854;&#22256;&#38590;&#65292;&#38656;&#35201;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#26367;&#20195;&#20154;&#21147;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32593;&#32476;&#35774;&#22791;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#65292;&#30446;&#21069;&#19981;&#21487;&#33021;&#22312;&#32593;&#32476;&#35774;&#22791;&#19978;&#37096;&#32626;AI&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#22522;&#20110;&#26368;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#25317;&#22622;&#25511;&#21046;&#31639;&#27861;[arXiv:2207.02295]&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#35745;&#31639;&#36731;&#37327;&#32423;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;RL-CC&#30340;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#20915;&#31574;&#26641;&#65292;&#23558;&#20854;&#25512;&#29702;&#26102;&#38388;&#38477;&#20302;&#20102;500&#20493;&#65292;&#20351;&#20854;&#22312;&#956;&#31186;&#32423;&#20915;&#31574;&#26102;&#38388;&#35201;&#27714;&#20869;&#23454;&#29616;&#23454;&#26102;&#25512;&#29702;&#65292;&#19988;&#23545;&#36136;&#37327;&#24433;&#21709;&#19981;&#22823;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#26102;&#38598;&#32676;&#20013;&#37096;&#32626;&#20102;&#36716;&#25442;&#21518;&#30340;&#31574;&#30053;&#65292;&#24182;&#19982;&#29616;&#20195;&#25968;&#25454;&#20013;&#24515;&#37096;&#32626;&#30340;&#27969;&#34892;&#25317;&#22622;&#25511;&#21046;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#31867;&#20284;&#30340;&#27969;&#37327;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;&#23614;&#37096;&#24310;&#36831;&#29575;&#25552;&#39640;&#20102;x%&#65292;&#23558;&#25968;&#25454;&#21253;&#20002;&#22833;&#29575;&#38477;&#20302;&#20102;y%&#12290;
&lt;/p&gt;
&lt;p&gt;
As communication protocols evolve, datacenter network utilization increases. As a result, congestion is more frequent, causing higher latency and packet loss. Combined with the increasing complexity of workloads, manual design of congestion control (CC) algorithms becomes extremely difficult. This calls for the development of AI approaches to replace the human effort. Unfortunately, it is currently not possible to deploy AI models on network devices due to their limited computational capabilities. Here, we offer a solution to this problem by building a computationally-light solution based on a recent reinforcement learning CC algorithm [arXiv:2207.02295]. We reduce the inference time of RL-CC by x500 by distilling its complex neural network into decision trees. This transformation enables real-time inference within the $\mu$-sec decision-time requirement, with a negligible effect on quality. We deploy the transformed policy on NVIDIA NICs in a live cluster. Compared to popular CC algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20844;&#24179;&#24615;&#30340;&#21746;&#23398;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#24182;&#24378;&#35843;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#27169;&#22411;&#35780;&#20272;&#37117;&#38656;&#32435;&#20837;&#20262;&#29702;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2205.09622</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#20844;&#24179;&#24615;&#65311;&#21746;&#23398;&#30340;&#24605;&#32771;&#19982;&#23545;fairML&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
What Is Fairness? Philosophical Considerations and Implications For FairML. (arXiv:2205.09622v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20844;&#24179;&#24615;&#30340;&#21746;&#23398;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#24182;&#24378;&#35843;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#27169;&#22411;&#35780;&#20272;&#37117;&#38656;&#32435;&#20837;&#20262;&#29702;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#24615;&#20154;&#24037;&#26234;&#33021;(fairML)&#39046;&#22495;&#65292;&#36890;&#36807;&#23450;&#20041;&#34913;&#37327;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#24230;&#37327;&#21644;&#25552;&#20986;&#30830;&#20445;&#35757;&#32451;&#27169;&#22411;&#25968;&#25454;&#20855;&#26377;&#20302;&#20844;&#24179;&#24615;&#24230;&#37327;&#20540;&#30340;&#26041;&#27861;&#65292;&#26469;&#20943;&#36731;&#20154;&#24037;&#26234;&#33021;(ML)&#20135;&#29983;&#30340;&#30456;&#20851;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20844;&#24179;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#21363;"&#20844;&#24179;&#26159;&#20160;&#20040;"&#65292;&#24456;&#23569;&#34987;&#35752;&#35770;&#65292;&#36825;&#36896;&#25104;&#20102;&#20844;&#24179;&#24615;&#30740;&#31350;&#22312;&#21746;&#23398;&#39046;&#22495;&#20960;&#20010;&#19990;&#32426;&#30340;&#35752;&#35770;&#19982;&#36817;&#26399;&#34987;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#33268;&#24615;&#20844;&#24179;&#27010;&#24565;&#21644;&#23558;&#21746;&#23398;&#24605;&#32771;&#36716;&#21270;&#20026;ADM&#31995;&#32479;&#20013;ML&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#24418;&#24335;&#26694;&#26550;&#65292;&#26469;&#26550;&#36215;&#36825;&#19968;&#40511;&#27807;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#21487;&#33021;&#24050;&#32463;&#23384;&#22312;&#65292;&#21363;&#20351;&#27809;&#26377;&#21463;&#20445;&#25252;&#24615;&#23646;&#24615;&#30340;&#23384;&#22312;&#65292;&#24378;&#35843;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#32780;&#26159;&#21069;&#32773;&#23454;&#29616;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#24378;&#35843;&#23558;&#20262;&#29702;&#32771;&#34385;&#32435;&#20837;ML&#31649;&#36947;&#30340;&#25152;&#26377;&#38454;&#27573;&#65292;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#37096;&#32626;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of literature in fairness-aware ML (fairML) aspires to mitigate machine learning (ML)-related unfairness in automated decision making (ADM) by defining metrics that measure fairness of an ML model and by proposing methods that ensure that trained ML models achieve low values in those measures. However, the underlying concept of fairness, i.e., the question of what fairness is, is rarely discussed, leaving a considerable gap between centuries of philosophical discussion and recent adoption of the concept in the ML community. In this work, we try to bridge this gap by formalizing a consistent concept of fairness and by translating the philosophical considerations into a formal framework for the training and evaluation of ML models in ADM systems. We derive that fairness problems can already arise without the presence of protected attributes, pointing out that fairness and predictive performance are not irreconcilable counterparts, but rather that the latter is necessary to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#31995;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;ReSSL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#19981;&#21516;&#23454;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#65292;&#20197;&#24357;&#34917;&#24403;&#21069;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;&#23454;&#20363;&#20851;&#31995;&#30340;&#32570;&#20047;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2203.08717</link><description>&lt;p&gt;
&#20851;&#31995;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Relational Self-Supervised Learning. (arXiv:2203.08717v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.08717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#31995;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;ReSSL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#19981;&#21516;&#23454;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#65292;&#20197;&#24357;&#34917;&#24403;&#21069;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;&#23454;&#20363;&#20851;&#31995;&#30340;&#32570;&#20047;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#65292;&#21253;&#25324;&#20027;&#27969;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#27809;&#26377;&#25968;&#25454;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#21487;&#20197;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#23454;&#20363;&#32423;&#20449;&#24687;&#65288;&#21363;&#65292;&#30456;&#21516;&#23454;&#20363;&#30340;&#19981;&#21516;&#22686;&#24378;&#22270;&#20687;&#24212;&#20855;&#26377;&#30456;&#21516;&#30340;&#29305;&#24449;&#25110;&#32858;&#38598;&#21040;&#30456;&#21516;&#30340;&#31867;&#21035;&#65289;&#65292;&#20294;&#23545;&#19981;&#21516;&#23454;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#32570;&#20047;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#21363;&#20851;&#31995;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;ReSSL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#19981;&#21516;&#23454;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#23398;&#20064;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#19981;&#21516;&#23454;&#20363;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#30340;&#38160;&#21270;&#20998;&#24067;&#20316;&#20026;&#8220;&#20851;&#31995;&#8221;&#24230;&#37327;&#65292;&#28982;&#21518;&#21033;&#29992;&#35813;&#24230;&#37327;&#21305;&#37197;&#19981;&#21516;&#22686;&#24378;&#30340;&#29305;&#24449;&#23884;&#20837;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#35748;&#20026;&#24369;&#22686;&#24378;&#23545;&#20110;&#34920;&#31034;&#26356;&#21487;&#38752;&#30340;&#20851;&#31995;&#24456;&#37325;&#35201;&#65292;&#24182;&#21033;&#29992;&#21160;&#37327;&#25216;&#26415;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning (SSL) including the mainstream contrastive learning has achieved great success in learning visual representations without data annotations. However, most methods mainly focus on the instance level information (\ie, the different augmented images of the same instance should have the same feature or cluster into the same class), but there is a lack of attention on the relationships between different instances. In this paper, we introduce a novel SSL paradigm, which we term as relational self-supervised learning (ReSSL) framework that learns representations by modeling the relationship between different instances. Specifically, our proposed method employs sharpened distribution of pairwise similarities among different instances as \textit{relation} metric, which is thus utilized to match the feature embeddings of different augmentations. To boost the performance, we argue that weak augmentations matter to represent a more reliable relation, and leverage momentum s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;SPD&#27969;&#24418;&#19978;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21644;SPD&#27969;&#24418;&#30340;&#23545;&#25968;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#21327;&#26041;&#24046;&#30697;&#38453;&#25805;&#20316;&#30340;&#22797;&#26434;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2201.05745</link><description>&lt;p&gt;
&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#28145;&#24230;&#26368;&#20248;&#20256;&#36755;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Deep Optimal Transport for Domain Adaptation on SPD Manifolds. (arXiv:2201.05745v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05745
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;SPD&#27969;&#24418;&#19978;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21644;SPD&#27969;&#24418;&#30340;&#23545;&#25968;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#21327;&#26041;&#24046;&#30697;&#38453;&#25805;&#20316;&#30340;&#22797;&#26434;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#23545;&#20110;&#22312;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#27969;&#24418;&#19978;&#35299;&#20915;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#38382;&#39064;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#20852;&#36259;&#12290;&#36825;&#31181;&#20852;&#36259;&#28304;&#20110;&#21307;&#30103;&#35774;&#22791;&#20135;&#29983;&#30340;&#22797;&#26434;&#31070;&#32463;&#29289;&#29702;&#25968;&#25454;&#65288;&#22914;&#33041;&#30005;&#22270;&#12289;&#33041;&#30913;&#22270;&#21644;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#23384;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#20559;&#31227;&#12290;&#36825;&#20123;&#25968;&#25454;&#34920;&#31034;&#20197;&#20449;&#21495;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#24418;&#24335;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#23545;&#31216;&#24615;&#21644;&#27491;&#23450;&#24615;&#30340;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#22797;&#26434;&#25805;&#20316;&#29305;&#24615;&#65292;&#30452;&#25509;&#23558;&#20808;&#21069;&#30340;&#32463;&#39564;&#21644;&#35299;&#20915;&#26041;&#26696;&#24212;&#29992;&#20110;DA&#38382;&#39064;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31867;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#28145;&#24230;&#26368;&#20248;&#20256;&#36755;&#12290;&#36825;&#19968;&#31867;&#26041;&#27861;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#65292;&#24182;&#21033;&#29992;SPD&#27969;&#24418;&#30340;&#23545;&#25968;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been significant interest in solving the domain adaptation (DA) problem on symmetric positive definite (SPD) manifolds within the machine learning community. This interest stems from the fact that complex neurophysiological data generated by medical equipment, such as electroencephalograms, magnetoencephalograms, and diffusion tensor imaging, often exhibit a shift in data distribution across different domains. These data representations, represented by signal covariance matrices, possess properties of symmetry and positive definiteness. However, directly applying previous experiences and solutions to the DA problem poses challenges due to the manipulation complexities of covariance matrices.To address this, our research introduces a category of deep learning-based transfer learning approaches called deep optimal transport. This category utilizes optimal transport theory and leverages the Log-Euclidean geometry for SPD manifolds. Additionally, we present a com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#20195;&#29702;&#20351;&#29992;&#22797;&#26434;&#30340;&#8220;&#40657;&#30418;&#8221;&#39044;&#27979;&#20989;&#25968;&#36827;&#34892;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#31639;&#27861;&#20915;&#31574;&#36827;&#34892;&#26368;&#20248;&#35843;&#25511;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38480;&#21046;&#20195;&#29702;&#20351;&#29992;&#36879;&#26126;&#24230;&#36275;&#22815;&#39640;&#30340;&#39044;&#27979;&#20989;&#25968;&#26159;&#20302;&#25928;&#30340;&#65292;&#32780;&#38024;&#23545;&#28608;&#21169;&#20559;&#24046;&#28304;&#22836;&#30340;&#30446;&#26631;&#21270;&#24037;&#20855;&#21487;&#20197;&#25552;&#20379;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25913;&#21892;&#31119;&#21033;&#12290;</title><link>http://arxiv.org/abs/2110.03443</link><description>&lt;p&gt;
&#25581;&#24320;&#40657;&#30418;&#23376;&#65306;&#35843;&#25511;&#31639;&#27861;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Unpacking the Black Box: Regulating Algorithmic Decisions. (arXiv:2110.03443v2 [econ.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#20195;&#29702;&#20351;&#29992;&#22797;&#26434;&#30340;&#8220;&#40657;&#30418;&#8221;&#39044;&#27979;&#20989;&#25968;&#36827;&#34892;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#31639;&#27861;&#20915;&#31574;&#36827;&#34892;&#26368;&#20248;&#35843;&#25511;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38480;&#21046;&#20195;&#29702;&#20351;&#29992;&#36879;&#26126;&#24230;&#36275;&#22815;&#39640;&#30340;&#39044;&#27979;&#20989;&#25968;&#26159;&#20302;&#25928;&#30340;&#65292;&#32780;&#38024;&#23545;&#28608;&#21169;&#20559;&#24046;&#28304;&#22836;&#30340;&#30446;&#26631;&#21270;&#24037;&#20855;&#21487;&#20197;&#25552;&#20379;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25913;&#21892;&#31119;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#20195;&#29702;&#20351;&#29992;&#22797;&#26434;&#30340;&#8220;&#40657;&#30418;&#8221;&#39044;&#27979;&#20989;&#25968;&#36827;&#34892;&#20915;&#31574;&#65288;&#22914;&#36151;&#27454;&#12289;&#21307;&#30103;&#27979;&#35797;&#25110;&#25307;&#32856;&#65289;&#19988;&#22996;&#25176;&#20154;&#22312;&#20102;&#35299;&#20195;&#29702;&#30340;&#40657;&#30418;&#27169;&#22411;&#26041;&#38754;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#22320;&#35843;&#25511;&#39044;&#27979;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#35825;&#23548;&#19981;&#36275;&#65292;&#19988;&#26368;&#20248;&#39044;&#27979;&#20989;&#25968;&#36275;&#22815;&#22797;&#26434;&#65292;&#23558;&#20195;&#29702;&#38480;&#21046;&#22312;&#36275;&#22815;&#36879;&#26126;&#30340;&#39044;&#27979;&#20989;&#25968;&#20013;&#26159;&#20302;&#25928;&#30340;&#12290;&#31639;&#27861;&#23457;&#35745;&#26377;&#21161;&#20110;&#25552;&#39640;&#31119;&#21033;&#65292;&#20294;&#20854;&#25910;&#30410;&#21462;&#20915;&#20110;&#23457;&#35745;&#24037;&#20855;&#30340;&#35774;&#35745;&#12290;&#35768;&#22810;&#35299;&#37322;&#24037;&#20855;&#20542;&#21521;&#20110;&#26368;&#23567;&#21270;&#25972;&#20307;&#20449;&#24687;&#25439;&#22833;&#65292;&#20294;&#36825;&#36890;&#24120;&#26159;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38598;&#20013;&#20110;&#35299;&#37322;&#39044;&#27979;&#20989;&#25968;&#30340;&#24179;&#22343;&#34892;&#20026;&#12290;&#38024;&#23545;&#24615;&#30340;&#24037;&#20855;&#65292;&#22914;&#38024;&#23545;&#28608;&#21169;&#20559;&#24046;&#28304;&#22836;&#65288;&#22914;&#36807;&#22810;&#30340;&#20551;&#38451;&#24615;&#25110;&#31181;&#26063;&#24046;&#24322;&#65289;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#29702;&#35770;&#30340;&#23454;&#35777;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how to optimally regulate prediction algorithms in a world where an agent uses complex 'black-box' prediction functions to make decisions such as lending, medical testing, or hiring, and where a principal is limited in how much she can learn about the agent's black-box model. We show that limiting agents to prediction functions that are simple enough to be fully transparent is inefficient as long as the misalignment is limited and first-best prediction functions are sufficiently complex. Algorithmic audits can improve welfare, but the gains depend on the design of the audit tools. Tools that focus on minimizing overall information loss, the focus of many explainer tools, will generally be inefficient since they focus on explaining the average behavior of the prediction function. Targeted tools that focus on the source of incentive misalignment, e.g., excess false positives or racial disparities, can provide second-best solutions. We provide empirical support for our theoretical
&lt;/p&gt;</description></item></channel></rss>