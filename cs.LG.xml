<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#22312;&#19968;&#20123;&#23569;&#26679;&#26412;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21457;&#29616;&#35821;&#38899;&#20026;&#22522;&#30784;&#30340;&#23569;&#26679;&#26412;&#38382;&#39064;&#19982;&#22810;&#20010;&#19979;&#28216;&#38899;&#39057;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01274</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#20013;&#30340;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#22312;&#19968;&#20123;&#23569;&#26679;&#26412;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21457;&#29616;&#35821;&#38899;&#20026;&#22522;&#30784;&#30340;&#23569;&#26679;&#26412;&#38382;&#39064;&#19982;&#22810;&#20010;&#19979;&#28216;&#38899;&#39057;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22240;&#20854;&#33021;&#22815;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#31283;&#20581;&#30340;&#29305;&#24449;&#34920;&#31034;&#32780;&#34920;&#29616;&#20986;&#33394;&#12290;&#32463;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#21487;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65288;&#21253;&#25324;&#23569;&#26679;&#26412;&#23398;&#20064;&#65289;&#20013;&#26377;&#25928;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#23613;&#31649;&#23545;&#20110;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#35780;&#20272;&#24050;&#32463;&#26377;&#20102;&#33391;&#22909;&#30340;&#22522;&#30784;&#65292;&#20294;&#22312;&#22768;&#23398;&#39046;&#22495;&#21364;&#26126;&#26174;&#32570;&#22833;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#65292;&#24357;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#19982;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#22522;&#20934;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19968;&#20123;&#23569;&#26679;&#26412;&#38382;&#39064;&#65288;&#22914;SpeechCommandsv2&#65289;&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#35821;&#38899;&#20026;&#22522;&#30784;&#30340;&#23569;&#26679;&#26412;&#38382;&#39064;&#19982;&#22810;&#20010;&#19979;&#28216;&#38899;&#39057;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#30528;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, self-supervised learning has excelled for its capacity to learn robust feature representations from unlabelled data. Networks pretrained through self-supervision serve as effective feature extractors for downstream tasks, including Few-Shot Learning. While the evaluation of unsupervised approaches for few-shot learning is well-established in imagery, it is notably absent in acoustics. This study addresses this gap by assessing large-scale self-supervised models' performance in few-shot audio classification. Additionally, we explore the relationship between a model's few-shot learning capability and other downstream task benchmarks. Our findings reveal state-of-the-art performance in some few-shot problems such as SpeechCommandsv2, as well as strong correlations between speech-based few-shot problems and various downstream audio tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#65292;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#33021;&#36731;&#26494;&#32467;&#21512;&#35266;&#23519;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01207</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#22240;&#26524;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Efficient Causal Graph Discovery Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01207
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#65292;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#33021;&#36731;&#26494;&#32467;&#21512;&#35266;&#23519;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#23436;&#25972;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#12290;&#20043;&#21069;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#25104;&#23545;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#38656;&#35201;&#20108;&#27425;&#26597;&#35810;&#30340;&#25968;&#37327;&#65292;&#23545;&#20110;&#36739;&#22823;&#30340;&#22240;&#26524;&#22270;&#26469;&#35828;&#24456;&#24555;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#30456;&#21453;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24403;&#26377;&#25152;&#35266;&#23519;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#36827;&#34892;&#32467;&#21512;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#38500;&#20102;&#26356;&#20855;&#26102;&#38388;&#21644;&#25968;&#25454;&#25928;&#29575;&#22806;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#25552;&#20986;&#26041;&#27861;&#22312;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21644;&#27491;&#21017;&#21270;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;AI&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#36890;&#36807;&#22312;&#23567;&#25968;&#25454;&#38598;&#21644;&#26377;&#20559;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#35757;&#32451;&#22810;&#20010;&#23545;&#25239;&#20559;&#35265;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#24471;&#21040;&#26080;&#20559;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;CIFAR10&#21644;HAM10000&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00910</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21644;&#27491;&#21017;&#21270;&#24494;&#35843;&#24212;&#23545;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21644;&#27491;&#21017;&#21270;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;AI&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#36890;&#36807;&#22312;&#23567;&#25968;&#25454;&#38598;&#21644;&#26377;&#20559;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#35757;&#32451;&#22810;&#20010;&#23545;&#25239;&#20559;&#35265;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#24471;&#21040;&#26080;&#20559;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;CIFAR10&#21644;HAM10000&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;AI&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#23545;&#20110;&#30830;&#20445;&#20844;&#24179;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#22823;&#35268;&#27169;&#12289;&#26080;&#20559;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#31181;&#26041;&#27861;&#28040;&#38500;AI&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20854;&#20013;&#20165;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#21644;&#28508;&#22312;&#26377;&#20559;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#20998;&#31163;&#12289;&#23616;&#37096;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#24494;&#35843;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#20197;&#23545;&#25239;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#24471;&#21040;&#28508;&#22312;&#30340;&#23545;&#25239;&#20559;&#35265;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#38598;&#25104;&#23398;&#20064;&#26469;&#23545;&#25152;&#26377;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#36798;&#21040;&#26080;&#20559;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21152;&#36895;&#25105;&#20204;&#30340;&#38598;&#25104;&#27169;&#22411;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#33719;&#24471;&#19968;&#20010;&#21333;&#19968;&#26080;&#20559;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#22312;CIFAR10&#21644;HAM10000&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#21019;&#24314;&#26356;&#26080;&#20559;&#12289;&#21487;&#38752;&#30340;AI&#27169;&#22411;&#30340;&#25345;&#32493;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing biases in AI models is crucial for ensuring fair and accurate predictions. However, obtaining large, unbiased datasets for training can be challenging. This paper proposes a comprehensive approach using multiple methods to remove bias in AI models, with only a small dataset and a potentially biased pretrained model. We train multiple models with the counter-bias of the pre-trained model through data splitting, local training, and regularized fine-tuning, gaining potentially counter-biased models. Then, we employ ensemble learning for all models to reach unbiased predictions. To further accelerate the inference time of our ensemble model, we conclude our solution with knowledge distillation that results in a single unbiased neural network. We demonstrate the effectiveness of our approach through experiments on the CIFAR10 and HAM10000 datasets, showcasing promising results. This work contributes to the ongoing effort to create more unbiased and reliable AI models, even with l
&lt;/p&gt;</description></item><item><title>AQA-Bench&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#19978;&#19979;&#25991;&#20013;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#65292;&#26174;&#33879;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09404</link><description>&lt;p&gt;
AQA-Bench&#65306;&#35780;&#20272;LLM&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#30340;&#20132;&#20114;&#24335;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09404
&lt;/p&gt;
&lt;p&gt;
AQA-Bench&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#19978;&#19979;&#25991;&#20013;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#65292;&#26174;&#33879;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;AQA-Bench&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31639;&#27861;&#19978;&#19979;&#25991;&#20013;&#65292;&#22914;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;DFS&#65289;&#31561;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#30340;&#20851;&#38190;&#29305;&#28857;&#22312;&#20110;&#20854;&#20132;&#20114;&#24335;&#35780;&#20272;&#21327;&#35758;-&#20363;&#22914;&#65292;&#22312;DFS&#20013;&#65292;&#27599;&#20010;&#33410;&#28857;&#30340;&#21487;&#29992;&#36830;&#25509;&#36793;&#21462;&#20915;&#20110;&#27169;&#22411;&#23545;&#35813;&#33410;&#28857;&#30340;&#36941;&#21382;&#65292;&#22240;&#27492;&#38656;&#35201;LLM&#26377;&#25928;&#22320;&#35760;&#20303;&#24050;&#35775;&#38382;&#33410;&#28857;&#24182;&#31574;&#21010;&#21518;&#32493;&#31227;&#21160;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#31639;&#27861;&#26500;&#24314;&#20102;AQA-Bench&#65292;&#20998;&#21035;&#26159;&#20108;&#20998;&#25628;&#32034;&#65292;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65292;&#24182;&#35780;&#20272;&#20102;12&#31181;&#19981;&#21516;&#30340;LLMs&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;&#65288;1&#65289;&#31867;&#20284;GPT-4&#21644;Gemini&#31561;&#38381;&#28304;&#27169;&#22411;&#36890;&#24120;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#65292;&#26126;&#26174;&#20248;&#20110;&#24320;&#28304;LLMs&#12290;&#65288;2&#65289;&#22825;&#30495;&#22320;&#25552;&#20379;&#20114;&#25805;&#20316;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09404v1 Announce Type: cross Abstract: This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol -- for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 12 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show strong sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing inter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#26597;&#35810;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#23545;&#40784;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#20559;&#22909;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#20195;&#20215;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09401</link><description>&lt;p&gt;
&#20351;&#29992;&#20027;&#21160;&#26597;&#35810;&#30340;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback with Active Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#26597;&#35810;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#23545;&#40784;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#20559;&#22909;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#20195;&#20215;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#65292;&#22312;&#26500;&#24314;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#20559;&#22909;&#25968;&#25454;&#65292;&#32780;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#36153;&#26102;&#36153;&#21147;&#12290;&#26412;&#25991;&#21463;&#21040;&#20027;&#21160;&#23398;&#20064;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#36890;&#36807;&#25552;&#20986;&#26597;&#35810;&#25928;&#29575;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#23545;&#40784;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#31454;&#20105;&#20108;&#33218;&#24378;&#30423;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#20027;&#21160;&#26597;&#35810;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;APPO&#65289;&#31639;&#27861;&#65292;&#20855;&#26377;$\tilde{O}(d^2/\Delta)$&#30340;&#36951;&#25022;&#30028;&#21644;$\tilde{O}(d^2/\Delta^2)$&#30340;&#26597;&#35810;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;$d$&#26159;&#29305;&#24449;&#31354;&#38388;&#30340;&#32500;&#24230;&#65292;$\Delta$&#26159;&#25152;&#26377;&#19978;&#19979;&#25991;&#20013;&#30340;&#27425;&#20248;&#24046;&#36317;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ADPO&#65292;&#36825;&#26159;&#25105;&#20204;&#31639;&#27861;&#30340;&#23454;&#38469;&#29256;&#26412;&#65292;&#22522;&#20110;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09401v1 Announce Type: cross Abstract: Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF). Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect. In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods. We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$ regret bound and an $\tilde{O}(d^2/\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\Delta$ is the sub-optimality gap over all the contexts. We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;LESS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#19968;&#20010;&#22266;&#23450;&#23610;&#23544;&#30340;&#32531;&#23384;&#21644;&#22522;&#20110;&#39537;&#36880;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20943;&#23567;&#20869;&#23384;&#21344;&#29992;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20840;&#37096;&#26631;&#35760;&#30340;&#21487;&#26597;&#35810;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09398</link><description>&lt;p&gt;
&#20351;&#29992;KV&#32531;&#23384;&#21387;&#32553;&#21512;&#25104;&#24490;&#29615;&#20197;&#25552;&#39640;LLM&#25512;&#26029;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09398
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;LESS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#19968;&#20010;&#22266;&#23450;&#23610;&#23544;&#30340;&#32531;&#23384;&#21644;&#22522;&#20110;&#39537;&#36880;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20943;&#23567;&#20869;&#23384;&#21344;&#29992;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20840;&#37096;&#26631;&#35760;&#30340;&#21487;&#26597;&#35810;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35745;&#31639;&#22240;&#32032;&#38480;&#21046;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#37096;&#32626;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#30001;&#38190;&#20540;(KV)&#32531;&#23384;&#24341;&#36215;&#30340;&#20869;&#23384;&#29942;&#39048;&#65292;&#36825;&#26159;&#19968;&#31181;&#35745;&#31639;&#24555;&#25463;&#26041;&#24335;&#65292;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#38656;&#35201;&#23384;&#20648;&#20808;&#21069;&#30340;KV&#23545;&#12290;&#29616;&#26377;&#30340;KV&#32531;&#23384;&#26041;&#27861;&#36890;&#36807;&#20462;&#21098;&#25110;&#39537;&#36880;&#30456;&#23545;&#19981;&#37325;&#35201;&#30340;KV&#23545;&#30340;&#22823;&#29255;&#21306;&#22495;&#65292;&#26174;&#33879;&#20943;&#23569;&#32531;&#23384;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#20294;&#22312;&#38656;&#35201;&#37325;&#26032;&#25910;&#38598;&#22823;&#22810;&#25968;&#21069;&#19968;&#20010;&#26631;&#35760;&#30340;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LESS&#65292;&#23427;&#23558;&#19968;&#20010;&#65288;&#20960;&#20046;&#20813;&#36153;&#30340;&#65289;&#22266;&#23450;&#23610;&#23544;&#30340;&#32531;&#23384;&#19982;&#22522;&#20110;&#39537;&#36880;&#30340;&#32531;&#23384;&#26041;&#27861;&#31616;&#21333;&#22320;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20197;&#20415;&#25152;&#26377;&#30340;&#26631;&#35760;&#21487;&#20197;&#22312;&#21518;&#32493;&#30340;&#35299;&#30721;&#27493;&#39588;&#20013;&#26597;&#35810;&#12290;&#23427;&#33021;&#22815;&#22312;&#26102;&#38388;&#19978;&#20445;&#30041;&#20449;&#24687;&#65292;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#21512;&#29702;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LESS&#21487;&#20197;&#24110;&#21161;&#20943;&#23567;&#32531;&#23384;&#25152;&#26377;&#20869;&#23481;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#26377;&#26102;&#29978;&#33267;&#21487;&#20197;&#19982;&#20854;&#30456;&#21305;&#37197;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09398v1 Announce Type: cross Abstract: Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#23433;&#20840;&#38477;&#20302;&#31561;&#31163;&#23376;&#20307;&#30005;&#27969;&#24182;&#36991;&#20813;&#38750;&#27491;&#24120;&#25918;&#30005;&#30340;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;SPARC&#20027;&#35201;&#21442;&#32771;&#25918;&#30005;&#30340;&#27169;&#25311;&#36827;&#34892;&#35757;&#32451;&#65292;&#25104;&#21151;&#23558;&#35813;&#31574;&#30053;&#36716;&#31227;&#21040;&#39640;&#20934;&#30830;&#24230;&#30340;&#27169;&#25311;&#22120;&#19978;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09387</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#25176;&#21345;&#39532;&#20811;&#38750;&#27491;&#24120;&#25918;&#30005;&#36991;&#20813;&#21644;&#36712;&#36857;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Active Disruption Avoidance and Trajectory Design for Tokamak Ramp-downs with Neural Differential Equations and Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#23433;&#20840;&#38477;&#20302;&#31561;&#31163;&#23376;&#20307;&#30005;&#27969;&#24182;&#36991;&#20813;&#38750;&#27491;&#24120;&#25918;&#30005;&#30340;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;SPARC&#20027;&#35201;&#21442;&#32771;&#25918;&#30005;&#30340;&#27169;&#25311;&#36827;&#34892;&#35757;&#32451;&#65292;&#25104;&#21151;&#23558;&#35813;&#31574;&#30053;&#36716;&#31227;&#21040;&#39640;&#20934;&#30830;&#24230;&#30340;&#27169;&#25311;&#22120;&#19978;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25176;&#21345;&#39532;&#20811;&#25552;&#20379;&#20102;&#19968;&#26465;&#23454;&#29616;&#32858;&#21464;&#33021;&#28304;&#30340;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#65292;&#20294;&#26159;&#31561;&#31163;&#23376;&#20307;&#38750;&#27491;&#24120;&#25918;&#30005;&#20250;&#24102;&#26469;&#37325;&#22823;&#30340;&#32463;&#27982;&#39118;&#38505;&#65292;&#22240;&#27492;&#23545;&#38750;&#27491;&#24120;&#25918;&#30005;&#30340;&#36991;&#20813;&#36827;&#34892;&#20102;&#30456;&#24403;&#22810;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#31574;&#30053;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#36991;&#20813;&#19982;&#38750;&#27491;&#24120;&#25918;&#30005;&#30456;&#20851;&#30340;&#22810;&#20010;&#37327;&#30340;&#38480;&#21046;&#30340;&#21516;&#26102;&#23433;&#20840;&#22320;&#38477;&#20302;&#31561;&#31163;&#23376;&#20307;&#30005;&#27969;&#12290;&#31574;&#30053;&#35757;&#32451;&#29615;&#22659;&#26159;&#19968;&#20010;&#28151;&#21512;&#29289;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;SPARC&#20027;&#35201;&#21442;&#32771;&#25918;&#30005;(PRlD)&#38477;&#25918;&#36807;&#31243;&#30340;&#27169;&#25311;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#20010;&#21363;&#23558;&#21040;&#26469;&#19988;&#20855;&#26377;&#29123;&#28903;&#31561;&#31163;&#23376;&#20307;&#30340;&#23454;&#39564;&#24615;&#24773;&#26223;&#65292;&#25105;&#20204;&#23558;&#20854;&#29992;&#20316;&#27979;&#35797;&#24179;&#21488;&#12290;&#20026;&#20102;&#24212;&#23545;&#29289;&#29702;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#22411;&#30340;&#19981;&#20934;&#30830;&#24615;&#65292;&#22312;&#31574;&#30053;&#35757;&#32451;&#26399;&#38388;&#65292;&#27169;&#25311;&#29615;&#22659;&#20351;&#29992;GPU&#36827;&#34892;&#22823;&#35268;&#27169;&#24182;&#34892;&#22788;&#29702;&#65292;&#24182;&#37319;&#29992;&#38543;&#26426;&#21270;&#30340;&#29289;&#29702;&#21442;&#25968;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;&#31574;&#30053;&#38543;&#21518;&#25104;&#21151;&#22320;&#36716;&#31227;&#21040;&#39640;&#20934;&#30830;&#24230;&#30340;&#27169;&#25311;&#22120;&#20013;&#65292;&#22312;&#36991;&#20813;&#29992;&#25143;&#25351;&#23450;&#30340;&#30772;&#22351;&#24615;&#38480;&#21046;&#30340;&#21516;&#26102;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#31561;&#31163;&#23376;&#20307;&#30005;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09387v1 Announce Type: cross Abstract: The tokamak offers a promising path to fusion energy, but plasma disruptions pose a major economic risk, motivating considerable advances in disruption avoidance. This work develops a reinforcement learning approach to this problem by training a policy to safely ramp-down the plasma current while avoiding limits on a number of quantities correlated with disruptions. The policy training environment is a hybrid physics and machine learning model trained on simulations of the SPARC primary reference discharge (PRD) ramp-down, an upcoming burning plasma scenario which we use as a testbed. To address physics uncertainty and model inaccuracies, the simulation environment is massively parallelized on GPU with randomized physics parameters during policy training. The trained policy is then successfully transferred to a higher fidelity simulator where it successfully ramps down the plasma while avoiding user-specified disruptive limits. We also 
&lt;/p&gt;</description></item><item><title>GraSSRep&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20803;&#22522;&#22240;&#32452;&#32452;&#35013;&#20013;&#30340;&#37325;&#22797;&#26816;&#27979;&#12290;&#23427;&#21033;&#29992;&#32452;&#35013;&#22270;&#30340;&#32467;&#26500;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#23558;DNA&#24207;&#21015;&#20998;&#31867;&#20026;&#37325;&#22797;&#21644;&#38750;&#37325;&#22797;&#31867;&#21035;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#20351;&#29992;&#20266;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.09381</link><description>&lt;p&gt;
GraSSRep: &#22522;&#20110;&#22270;&#30340;&#20803;&#22522;&#22240;&#32452;&#32452;&#35013;&#20013;&#37325;&#22797;&#26816;&#27979;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in Metagenomic Assembly
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09381
&lt;/p&gt;
&lt;p&gt;
GraSSRep&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20803;&#22522;&#22240;&#32452;&#32452;&#35013;&#20013;&#30340;&#37325;&#22797;&#26816;&#27979;&#12290;&#23427;&#21033;&#29992;&#32452;&#35013;&#22270;&#30340;&#32467;&#26500;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#23558;DNA&#24207;&#21015;&#20998;&#31867;&#20026;&#37325;&#22797;&#21644;&#38750;&#37325;&#22797;&#31867;&#21035;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#20351;&#29992;&#20266;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#22797;DNA&#65288;&#37325;&#22797;&#65289;&#23545;&#20110;&#20934;&#30830;&#39640;&#25928;&#22320;&#36827;&#34892;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#25552;&#20986;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#23545;&#20110;&#20803;&#22522;&#22240;&#32452;&#25968;&#25454;&#32780;&#35328;&#65292;&#36825;&#19968;&#25361;&#25112;&#23588;&#20026;&#31361;&#20986;&#65292;&#22240;&#20026;&#22522;&#22240;&#32452;&#21160;&#24577;&#24615;&#65292;&#22914;&#27700;&#24179;&#22522;&#22240;&#36716;&#31227;&#12289;&#22522;&#22240;&#22797;&#21046;&#20197;&#21450;&#22522;&#22240;&#25439;&#22833;/&#33719;&#24471;&#65292;&#20351;&#24471;&#20174;&#20803;&#22522;&#22240;&#32452;&#32676;&#20307;&#20013;&#20934;&#30830;&#32452;&#35013;&#22522;&#22240;&#32452;&#21464;&#24471;&#22797;&#26434;&#12290;&#26816;&#27979;&#37325;&#22797;&#26159;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#20851;&#38190;&#31532;&#19968;&#27493;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraSSRep&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#32452;&#35013;&#22270;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#20013;&#23558;DNA&#24207;&#21015;&#20998;&#31867;&#20026;&#37325;&#22797;&#21644;&#38750;&#37325;&#22797;&#31867;&#21035;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#26500;&#24314;&#20026;&#22312;&#20803;&#22522;&#22240;&#32452;&#32452;&#35013;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#20381;&#38752;&#19968;&#20010;&#39640;&#31934;&#24230;&#65288;&#20294;&#20302;&#21484;&#22238;&#29575;&#65289;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#20026;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#20266;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09381v1 Announce Type: new Abstract: Repetitive DNA (repeats) poses significant challenges for accurate and efficient genome assembly and sequence alignment. This is particularly true for metagenomic data, where genome dynamics such as horizontal gene transfer, gene duplication, and gene loss/gain complicate accurate genome assembly from metagenomic communities. Detecting repeats is a crucial first step in overcoming these challenges. To address this issue, we propose GraSSRep, a novel approach that leverages the assembly graph's structure through graph neural networks (GNNs) within a self-supervised learning framework to classify DNA sequences into repetitive and non-repetitive categories. Specifically, we frame this problem as a node classification task within a metagenomic assembly graph. In a self-supervised fashion, we rely on a high-precision (but low-recall) heuristic to generate pseudo-labels for a small proportion of the nodes. We then use those pseudo-labels to tra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21463;&#38480;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#35774;&#32622;&#25439;&#22833;&#19978;&#38480;&#26469;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24179;&#22343;&#24615;&#33021;&#20248;&#21270;&#23548;&#33268;&#29305;&#23450;&#26102;&#38388;&#27493;&#39588;&#19978;&#35823;&#24046;&#36807;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09373</link><description>&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#25439;&#22833;&#22609;&#36896;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Loss Shaping Constraints for Long-Term Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09373
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21463;&#38480;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#35774;&#32622;&#25439;&#22833;&#19978;&#38480;&#26469;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24179;&#22343;&#24615;&#33021;&#20248;&#21270;&#23548;&#33268;&#29305;&#23450;&#26102;&#38388;&#27493;&#39588;&#19978;&#35823;&#24046;&#36807;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#39044;&#27979;&#22810;&#20010;&#27493;&#39588;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#26377;&#22823;&#37327;&#30340;&#25991;&#29486;&#65292;&#20294;&#32463;&#20856;&#21644;&#26368;&#36817;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#26368;&#23567;&#21270;&#39044;&#27979;&#31383;&#21475;&#19978;&#30340;&#24615;&#33021;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#39044;&#27979;&#27493;&#39588;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#30340;&#38169;&#35823;&#20998;&#24067;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22312;&#24120;&#35265;&#39044;&#27979;&#22522;&#20934;&#19978;&#35757;&#32451;&#30340;&#26368;&#36817;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#24179;&#22343;&#24615;&#33021;&#20248;&#21270;&#21487;&#33021;&#23548;&#33268;&#29305;&#23450;&#26102;&#38388;&#27493;&#39588;&#19978;&#30340;&#38169;&#35823;&#36807;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21463;&#38480;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25214;&#21040;&#22312;&#24179;&#22343;&#24615;&#33021;&#19978;&#26368;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#20445;&#25345;&#29992;&#25143;&#23450;&#20041;&#30340;&#25439;&#22833;&#19978;&#38480;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;&#25439;&#22833;&#22609;&#36896;&#32422;&#26463;&#65292;&#22240;&#20026;&#23427;&#23545;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#25439;&#22833;&#26045;&#21152;&#32422;&#26463;&#65292;&#24182;&#21033;&#29992;&#26368;&#36817;&#30340;&#23545;&#20598;&#24615;&#32467;&#26524;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09373v1 Announce Type: new Abstract: Several applications in time series forecasting require predicting multiple steps ahead. Despite the vast amount of literature in the topic, both classical and recent deep learning based approaches have mostly focused on minimising performance averaged over the predicted window. We observe that this can lead to disparate distributions of errors across forecasting steps, especially for recent transformer architectures trained on popular forecasting benchmarks. That is, optimising performance on average can lead to undesirably large errors at specific time-steps. In this work, we present a Constrained Learning approach for long-term time series forecasting that aims to find the best model in terms of average performance that respects a user-defined upper bound on the loss at each time-step. We call our approach loss shaping constraints because it imposes constraints on the loss at each time step, and leverage recent duality results to show 
&lt;/p&gt;</description></item><item><title>Transformers&#22312;&#29305;&#23450;&#32452;&#21512;&#30340;&#25968;&#25454;&#26684;&#24335;&#21644;&#20301;&#32622;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#38271;&#24230;&#30340;&#27867;&#21270;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#33030;&#24369;&#24615;&#21644;&#22823;&#37327;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.09371</link><description>&lt;p&gt;
Transformers&#21487;&#20197;&#23454;&#29616;&#38271;&#24230;&#30340;&#27867;&#21270;&#65292;&#20294;&#24182;&#19981;&#31283;&#20581;
&lt;/p&gt;
&lt;p&gt;
Transformers Can Achieve Length Generalization But Not Robustly
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09371
&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#29305;&#23450;&#32452;&#21512;&#30340;&#25968;&#25454;&#26684;&#24335;&#21644;&#20301;&#32622;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#38271;&#24230;&#30340;&#27867;&#21270;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#33030;&#24369;&#24615;&#21644;&#22823;&#37327;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#30340;&#27867;&#21270;&#65292;&#21363;&#20174;&#36739;&#30701;&#30340;&#35757;&#32451;&#24207;&#21015;&#25512;&#24191;&#21040;&#36739;&#38271;&#30340;&#27979;&#35797;&#24207;&#21015;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#21363;&#20351;&#26159;&#22788;&#29702;&#30456;&#23545;&#31616;&#21333;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;Transformer&#20063;&#23384;&#22312;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25972;&#25968;&#30456;&#21152;&#30340;&#20219;&#21153;&#26469;&#27979;&#35797;Transformer&#30340;&#38271;&#24230;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38271;&#24230;&#27867;&#21270;&#30340;&#25104;&#21151;&#19982;&#25968;&#25454;&#26684;&#24335;&#21644;&#20301;&#32622;&#32534;&#30721;&#30340;&#31867;&#22411;&#23494;&#20999;&#30456;&#20851;&#12290;&#36890;&#36807;&#27491;&#30830;&#32452;&#21512;&#25968;&#25454;&#26684;&#24335;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20986;&#26631;&#20934;&#30340;Transformer&#21487;&#20197;&#25512;&#24191;&#21040;&#36755;&#20837;&#38271;&#24230;&#30340;2.5&#20493;&#30340;&#24207;&#21015;&#38271;&#24230;&#12290;&#28982;&#32780;&#65292;&#19982;&#20869;&#20998;&#24067;&#27867;&#21270;&#19981;&#21516;&#65292;&#38271;&#24230;&#27867;&#21270;&#20173;&#28982;&#24456;&#33030;&#24369;&#65292;&#21463;&#21040;&#38543;&#26426;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#25968;&#25454;&#39034;&#24207;&#31561;&#22240;&#32032;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#23548;&#33268;&#19981;&#21516;&#38543;&#26426;&#31181;&#23376;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09371v1 Announce Type: cross Abstract: Length generalization, defined as the ability to extrapolate from shorter training sequences to longer test ones, is a significant challenge for language models. This issue persists even with large-scale Transformers handling relatively straightforward tasks. In this paper, we test the Transformer's ability of length generalization using the task of addition of two integers. We show that the success of length generalization is intricately linked to the data format and the type of position encoding. Using the right combination of data format and position encodings, we show for the first time that standard Transformers can extrapolate to a sequence length that is 2.5x the input length. Nevertheless, unlike in-distribution generalization, length generalization remains fragile, significantly influenced by factors like random weight initialization and training data order, leading to large variances across different random seeds.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#20266;&#38543;&#26426;&#32416;&#38169;&#30721;&#65292;&#23427;&#23545;&#26367;&#25442;&#21644;&#21024;&#38500;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#35299;&#30721;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20266;&#38543;&#26426;&#30721;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#27700;&#21360;&#22788;&#29702;&#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23545;&#35009;&#21098;&#21644;&#38543;&#26426;&#26367;&#25442;&#12289;&#21024;&#38500;&#20855;&#26377;&#24658;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09370</link><description>&lt;p&gt;
&#20266;&#38543;&#26426;&#32416;&#38169;&#30721;
&lt;/p&gt;
&lt;p&gt;
Pseudorandom Error-Correcting Codes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09370
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#20266;&#38543;&#26426;&#32416;&#38169;&#30721;&#65292;&#23427;&#23545;&#26367;&#25442;&#21644;&#21024;&#38500;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#35299;&#30721;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20266;&#38543;&#26426;&#30721;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#27700;&#21360;&#22788;&#29702;&#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23545;&#35009;&#21098;&#21644;&#38543;&#26426;&#26367;&#25442;&#12289;&#21024;&#38500;&#20855;&#26377;&#24658;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#24314;&#20102;&#20266;&#38543;&#26426;&#32416;&#38169;&#30721;&#65288;&#25110;&#31616;&#31216;&#20026;&#20266;&#38543;&#26426;&#30721;&#65289;&#65292;&#23427;&#20204;&#26159;&#20855;&#26377;&#20197;&#19979;&#29305;&#24615;&#30340;&#32416;&#38169;&#30721;&#65306;&#23545;&#20110;&#20219;&#20309;&#35745;&#31639;&#21463;&#38480;&#30340;&#23545;&#25163;&#26469;&#35828;&#65292;&#20219;&#24847;&#22810;&#20010;&#32534;&#30721;&#35789;&#37117;&#26159;&#20266;&#38543;&#26426;&#30340;&#12290;&#36890;&#36807;&#35299;&#30721;&#23494;&#38053;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#32416;&#27491;&#26377;&#38169;&#35823;&#30340;&#32534;&#30721;&#35789;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#23545;&#26367;&#25442;&#38169;&#35823;&#21644;&#21024;&#38500;&#38169;&#35823;&#20855;&#26377;&#24378;&#40065;&#26834;&#24615;&#30340;&#20266;&#38543;&#26426;&#30721;&#65292;&#20854;&#20013;&#20266;&#38543;&#26426;&#24615;&#22522;&#20110;&#26631;&#20934;&#23494;&#30721;&#23398;&#20551;&#35774;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20266;&#38543;&#26426;&#24615;&#22522;&#20110;LPN&#38382;&#39064;&#30340;$2^{O(\sqrt{n})}$&#22256;&#38590;&#31243;&#24230;&#65292;&#25110;&#32773;&#22522;&#20110;LPN&#38382;&#39064;&#21644;&#20302;&#23494;&#24230;&#19979;&#30340;&#25554;&#20837;&#24322;&#25110;&#38382;&#39064;&#30340;&#22810;&#39033;&#24335;&#22256;&#38590;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09370v1 Announce Type: cross Abstract: We construct pseudorandom error-correcting codes (or simply pseudorandom codes), which are error-correcting codes with the property that any polynomial number of codewords are pseudorandom to any computationally-bounded adversary. Efficient decoding of corrupted codewords is possible with the help of a decoding key.   We build pseudorandom codes that are robust to substitution and deletion errors, where pseudorandomness rests on standard cryptographic assumptions. Specifically, pseudorandomness is based on either $2^{O(\sqrt{n})}$-hardness of LPN, or polynomial hardness of LPN and the planted XOR problem at low density.   As our primary application of pseudorandom codes, we present an undetectable watermarking scheme for outputs of language models that is robust to cropping and a constant rate of random substitutions and deletions. The watermark is undetectable in the sense that any number of samples of watermarked text are computationa
&lt;/p&gt;</description></item><item><title>HiRE&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#30340;&#39640;&#21484;&#22238;&#29575;&#30340;&#36817;&#20284;Top-k&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#26041;&#26696;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#20256;&#36755;&#21644;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2402.09360</link><description>&lt;p&gt;
HiRE:&#39640;&#21484;&#22238;&#29575;&#30340;&#39640;&#25928;LLM&#25512;&#29702;&#30340;&#36817;&#20284;Top-$k$&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09360
&lt;/p&gt;
&lt;p&gt;
HiRE&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#30340;&#39640;&#21484;&#22238;&#29575;&#30340;&#36817;&#20284;Top-k&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#26041;&#26696;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#20256;&#36755;&#21644;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21152;&#36895;&#22120;&#65288;GPU/TPU&#65289;&#19978;&#20351;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#33258;&#22238;&#24402;&#35299;&#30721;&#24448;&#24448;&#21463;&#21040;&#20869;&#23384;&#38480;&#21046;&#65292;&#22823;&#37096;&#20998;&#26102;&#38388;&#29992;&#20110;&#23558;&#27169;&#22411;&#21442;&#25968;&#20174;&#39640;&#24102;&#23485;&#20869;&#23384;&#65288;HBM&#65289;&#20256;&#36755;&#21040;&#32531;&#23384;&#20013;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#21069;&#39304;&#65288;FFN&#65289;&#23618;&#20013;&#32500;&#25345;&#36136;&#37327;&#30340;&#21516;&#26102;&#20855;&#26377;&#26174;&#33879;&#30340;&#31232;&#30095;&#24615;/&#20887;&#20313;&#24615;&#65288;&#20854;&#20013;$k \approx 0.05$&#65289;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#20256;&#36755;&#21644;&#24310;&#36831;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#31181;&#31232;&#30095;&#24615;&#26469;&#25913;&#21892;&#24310;&#36831;&#30340;&#36807;&#31243;&#21463;&#21040;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#38480;&#21046;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#23436;&#25972;&#30340;&#30697;&#38453;&#25805;&#20316;&#26469;&#35782;&#21035;&#21069;$k$&#20010;&#34892;/&#21015;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#28508;&#22312;&#30340;&#25910;&#30410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HiRE&#65288;&#39640;&#21484;&#22238;&#29575;&#30340;&#36817;&#20284;Top-k&#20272;&#35745;&#65289;&#12290;HiRE&#21253;&#25324;&#20004;&#20010;&#26032;&#39062;&#30340;&#32452;&#20214;&#65306;&#65288;i&#65289;&#19968;&#31181;&#21387;&#32553;&#26041;&#26696;&#20197;&#20415;&#24265;&#20215;&#22320;&#20272;&#35745;&#21069;$k$&#20010;&#34892;/&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09360v1 Announce Type: cross Abstract: Autoregressive decoding with generative Large Language Models (LLMs) on accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent on transferring model parameters from high bandwidth memory (HBM) to cache. On the other hand, recent works show that LLMs can maintain quality with significant sparsity/redundancy in the feedforward (FFN) layers by appropriately training the model to operate on a top-$k$ fraction of rows/columns (where $k \approx 0.05$), there by suggesting a way to reduce the transfer of model parameters, and hence latency. However, exploiting this sparsity for improving latency is hindered by the fact that identifying top rows/columns is data-dependent and is usually performed using full matrix operations, severely limiting potential gains. To address these issues, we introduce HiRE (High Recall Approximate Top-k Estimation). HiRE comprises of two novel components: (i) a compression scheme to cheaply p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#31867;&#20284;&#20110;ChatGPT&#30340;&#22522;&#20110;&#20113;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#23433;&#20840;&#30340;&#21307;&#38498;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#20998;&#26512;&#25918;&#23556;&#23398;&#25253;&#21578;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;95%&#20197;&#19978;&#30340;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#39044;&#27979;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#23545;&#20110;&#21307;&#29983;&#26469;&#35828;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.09358</link><description>&lt;p&gt;
&#23558;ChatGPT&#38598;&#25104;&#21040;&#23433;&#20840;&#21307;&#38498;&#32593;&#32476;&#20013;&#65306;&#19968;&#20010;&#20851;&#20110;&#25913;&#36827;&#25918;&#23556;&#23398;&#25253;&#21578;&#20998;&#26512;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Integrating ChatGPT into Secure Hospital Networks: A Case Study on Improving Radiology Report Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#31867;&#20284;&#20110;ChatGPT&#30340;&#22522;&#20110;&#20113;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#23433;&#20840;&#30340;&#21307;&#38498;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#20998;&#26512;&#25918;&#23556;&#23398;&#25253;&#21578;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;95%&#20197;&#19978;&#30340;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#39044;&#27979;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#23545;&#20110;&#21307;&#29983;&#26469;&#35828;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#23558;&#31867;&#20284;&#20110;ChatGPT&#30340;&#22522;&#20110;&#20113;&#30340;&#20154;&#24037;&#26234;&#33021;&#39318;&#27425;&#24212;&#29992;&#20110;&#23433;&#20840;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#20998;&#26512;&#25918;&#23556;&#23398;&#25253;&#21578;&#65292;&#37325;&#35270;&#24739;&#32773;&#25968;&#25454;&#38544;&#31169;&#12290;&#36890;&#36807;&#20351;&#29992;&#29420;&#29305;&#30340;&#21477;&#23376;&#32423;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#26816;&#27979;&#24322;&#24120;&#26041;&#38754;&#36798;&#21040;&#20102;&#36229;&#36807;95%&#30340;&#20934;&#30830;&#29575;&#12290;&#35813;&#27169;&#22411;&#36824;&#20934;&#30830;&#22320;&#26631;&#35760;&#20854;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22686;&#24378;&#20102;&#23545;&#21307;&#29983;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#24102;&#26377;&#30830;&#23450;&#24615;&#25351;&#31034;&#22120;&#12290;&#36825;&#20123;&#36827;&#23637;&#22312;&#24320;&#21457;&#23433;&#20840;&#39640;&#25928;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#26041;&#38754;&#20195;&#34920;&#20102;&#37325;&#22823;&#36827;&#27493;&#65292;&#20026;&#30417;&#31649;&#26368;&#23569;&#30340;&#21307;&#38498;&#20869;&#37096;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09358v1 Announce Type: new Abstract: This study demonstrates the first in-hospital adaptation of a cloud-based AI, similar to ChatGPT, into a secure model for analyzing radiology reports, prioritizing patient data privacy. By employing a unique sentence-level knowledge distillation method through contrastive learning, we achieve over 95% accuracy in detecting anomalies. The model also accurately flags uncertainties in its predictions, enhancing its reliability and interpretability for physicians with certainty indicators. These advancements represent significant progress in developing secure and efficient AI tools for healthcare, suggesting a promising future for in-hospital AI applications with minimal supervision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoRM&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#26469;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.09345</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#22870;&#21169;&#24314;&#27169;&#26469;&#20943;&#36731;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Reward Hacking via Information-Theoretic Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoRM&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#26469;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#20013;&#30340;&#25104;&#21151;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#26041;&#38754;&#65292;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#20063;&#34987;&#31216;&#20026;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#20110;&#22870;&#21169;&#24314;&#27169;&#30340;&#23616;&#38480;&#24615;&#65292;&#21363;&#22870;&#21169;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25512;&#24191;&#21644;&#40065;&#26834;&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#31216;&#20026;InfoRM&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#26469;&#36807;&#28388;&#20986;&#19981;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#24320;&#21457;&#19968;&#31181;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102;&#36807;&#24230;&#20248;&#21270;&#19982;&#28508;&#21464;&#37327;&#31354;&#38388;&#30340;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23558;InfoRM&#20316;&#20026;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#36807;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09345v1 Announce Type: cross Abstract: Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset. In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation. Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quant
&lt;/p&gt;</description></item><item><title>rnaglib&#26159;&#19968;&#20010;&#22522;&#20110;3D&#30340;RNA&#21151;&#33021;&#39044;&#27979;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;RNA 3D&#32467;&#26500;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09330</link><description>&lt;p&gt;
rnaglib&#20013;&#22522;&#20110;3D&#30340;RNA&#21151;&#33021;&#39044;&#27979;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
3D-based RNA function prediction tools in rnaglib
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09330
&lt;/p&gt;
&lt;p&gt;
rnaglib&#26159;&#19968;&#20010;&#22522;&#20110;3D&#30340;RNA&#21151;&#33021;&#39044;&#27979;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;RNA 3D&#32467;&#26500;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;RNA&#30340;&#22797;&#26434;&#32467;&#26500;&#29305;&#24449;&#19982;&#29983;&#29289;&#21151;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#26159;&#36827;&#21270;&#30740;&#31350;&#21644;RNA&#35774;&#35745;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;RNA 3D&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#36866;&#24403;&#30340;&#24314;&#27169;&#36873;&#25321;&#20173;&#28982;&#32791;&#26102;&#19988;&#32570;&#20047;&#26631;&#20934;&#21270;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;RNA 3D&#32467;&#26500;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21151;&#33021;&#39044;&#27979;&#27169;&#22411;&#30340;rnaglib&#30340;&#20351;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09330v1 Announce Type: cross Abstract: Understanding the connection between complex structural features of RNA and biological function is a fundamental challenge in evolutionary studies and in RNA design. However, building datasets of RNA 3D structures and making appropriate modeling choices remains time-consuming and lacks standardization. In this chapter, we describe the use of rnaglib, to train supervised and unsupervised machine learning-based function prediction models on datasets of RNA 3D structures.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#31639;&#27861;&#20844;&#24179;&#24615;&#19982;&#26426;&#22120;&#23398;&#20064;&#22312;&#23448;&#26041;&#32479;&#35745;&#21644;&#35843;&#26597;&#29983;&#20135;&#20013;&#30340;&#36136;&#37327;&#32500;&#24230;&#32852;&#31995;&#36215;&#26469;&#65292;&#25193;&#23637;&#20102;&#36136;&#37327;&#26694;&#26550;&#65292;&#24182;&#35843;&#26597;&#20102;&#20844;&#24179;&#24615;&#19982;&#20854;&#20182;&#36136;&#37327;&#32500;&#24230;&#30340;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2402.09328</link><description>&lt;p&gt;
&#23558;&#31639;&#27861;&#20844;&#24179;&#24615;&#19982;&#26426;&#22120;&#23398;&#20064;&#22312;&#23448;&#26041;&#32479;&#35745;&#21644;&#35843;&#26597;&#29983;&#20135;&#20013;&#30340;&#36136;&#37327;&#32500;&#24230;&#32852;&#31995;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
Connecting Algorithmic Fairness to Quality Dimensions in Machine Learning in Official Statistics and Survey Production
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09328
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#31639;&#27861;&#20844;&#24179;&#24615;&#19982;&#26426;&#22120;&#23398;&#20064;&#22312;&#23448;&#26041;&#32479;&#35745;&#21644;&#35843;&#26597;&#29983;&#20135;&#20013;&#30340;&#36136;&#37327;&#32500;&#24230;&#32852;&#31995;&#36215;&#26469;&#65292;&#25193;&#23637;&#20102;&#36136;&#37327;&#26694;&#26550;&#65292;&#24182;&#35843;&#26597;&#20102;&#20844;&#24179;&#24615;&#19982;&#20854;&#20182;&#36136;&#37327;&#32500;&#24230;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22269;&#23478;&#32479;&#35745;&#26426;&#26500;&#65288;NSOs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26469;&#25552;&#39640;&#20135;&#21697;&#30340;&#26102;&#25928;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#24615;&#12290;&#24341;&#20837;ML&#35299;&#20915;&#26041;&#26696;&#26102;&#65292;NSOs&#24517;&#39035;&#30830;&#20445;&#22312;&#32479;&#35745;&#31639;&#27861;&#30340;&#36136;&#37327;&#26694;&#26550;&#65288;QF4SA; Yung&#31561;,2022&#65289;&#20013;&#26126;&#30830;&#35268;&#23450;&#30340;&#20581;&#22766;&#24615;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#20934;&#30830;&#24615;&#31561;&#39640;&#26631;&#20934;&#24471;&#21040;&#20445;&#25345;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20844;&#24179;&#24615;&#20316;&#20026;&#23433;&#20840;&#37096;&#32626;ML&#30340;&#21069;&#25552;&#65292;&#20197;&#38450;&#27490;&#23454;&#36341;&#20013;&#19981;&#21516;&#31038;&#20250;&#24433;&#21709;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;NSOs&#24212;&#29992;ML&#30340;&#32972;&#26223;&#19979;&#65292;&#20844;&#24179;&#24615;&#23578;&#26410;&#26126;&#30830;&#35752;&#35770;&#20026;&#36136;&#37327;&#26041;&#38754;&#12290;&#25105;&#20204;&#20351;&#29992;Yung&#31561;&#20154; (2022)&#30340;QF4SA&#36136;&#37327;&#26694;&#26550;&#65292;&#24182;&#23558;&#20854;&#36136;&#37327;&#32500;&#24230;&#26144;&#23556;&#21040;&#31639;&#27861;&#20844;&#24179;&#24615;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#26041;&#38754;&#25193;&#23637;&#20102;QF4SA&#26694;&#26550;&#65306;&#25105;&#20204;&#20027;&#24352;&#20844;&#24179;&#24615;&#20316;&#20026;&#20854;&#29420;&#31435;&#30340;&#36136;&#37327;&#32500;&#24230;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20844;&#24179;&#24615;&#19982;&#20854;&#20182;&#36136;&#37327;&#32500;&#24230;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09328v1 Announce Type: cross Abstract: National Statistical Organizations (NSOs) increasingly draw on Machine Learning (ML) to improve the timeliness and cost-effectiveness of their products. When introducing ML solutions, NSOs must ensure that high standards with respect to robustness, reproducibility, and accuracy are upheld as codified, e.g., in the Quality Framework for Statistical Algorithms (QF4SA; Yung et al. 2022). At the same time, a growing body of research focuses on fairness as a pre-condition of a safe deployment of ML to prevent disparate social impacts in practice. However, fairness has not yet been explicitly discussed as a quality aspect in the context of the application of ML at NSOs. We employ Yung et al. (2022)'s QF4SA quality framework and present a mapping of its quality dimensions to algorithmic fairness. We thereby extend the QF4SA framework in several ways: we argue for fairness as its own quality dimension, we investigate the interaction of fairness
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#35760;&#24518;&#21644;&#23398;&#20064;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#37327;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#35757;&#32451;&#25968;&#25454;&#28857;&#25581;&#31034;&#30340;&#20449;&#24687;&#26469;&#23450;&#20041;&#35760;&#24518;&#65292;&#24182;&#20934;&#30830;&#23450;&#20041;&#20102;&#23398;&#20064;&#31639;&#27861;&#20934;&#30830;&#24615;&#19982;&#26465;&#20214;&#20114;&#20449;&#24687;&#65288;CMI&#65289;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#19982;CMI&#20043;&#38388;&#30340;&#26368;&#20339;&#36793;&#30028;&#12290;&#36890;&#36807;&#35774;&#35745;&#23545;&#25163;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#35760;&#24518;&#22312;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09327</link><description>&lt;p&gt;
&#38543;&#26426;&#20984;&#20248;&#21270;&#30340;&#20449;&#24687;&#22797;&#26434;&#24230;&#65306;&#27867;&#21270;&#21644;&#35760;&#24518;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#35760;&#24518;&#21644;&#23398;&#20064;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#37327;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#35757;&#32451;&#25968;&#25454;&#28857;&#25581;&#31034;&#30340;&#20449;&#24687;&#26469;&#23450;&#20041;&#35760;&#24518;&#65292;&#24182;&#20934;&#30830;&#23450;&#20041;&#20102;&#23398;&#20064;&#31639;&#27861;&#20934;&#30830;&#24615;&#19982;&#26465;&#20214;&#20114;&#20449;&#24687;&#65288;CMI&#65289;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#19982;CMI&#20043;&#38388;&#30340;&#26368;&#20339;&#36793;&#30028;&#12290;&#36890;&#36807;&#35774;&#35745;&#23545;&#25163;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#35760;&#24518;&#22312;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20984;&#20248;&#21270;&#65288;SCO&#65289;&#30340;&#32972;&#26223;&#19979;&#35760;&#24518;&#21644;&#23398;&#20064;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#31639;&#27861;&#23545;&#20854;&#35757;&#32451;&#25968;&#25454;&#28857;&#25581;&#31034;&#30340;&#20449;&#24687;&#26469;&#23450;&#20041;&#35760;&#24518;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;Steinke&#21644;Zakynthinou&#65288;2020&#65289;&#25552;&#20986;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#65288;CMI&#65289;&#26694;&#26550;&#26469;&#37327;&#21270;&#36825;&#20123;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#23545;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#23427;&#30340;CMI&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#31934;&#30830;&#25551;&#36848;&#65292;&#22238;&#31572;&#20102;Livni&#65288;2023&#65289;&#25552;&#20986;&#30340;&#19968;&#20010;&#26410;&#35299;&#20043;&#38382;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;$L^2$ Lipschitz-&#26377;&#30028;&#30340;&#35774;&#32622;&#21644;&#24378;&#20984;&#24615;&#19979;&#65292;&#27599;&#20010;&#20855;&#26377;&#36229;&#39069;&#38169;&#35823;$\varepsilon$&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;CMI&#19979;&#30028;&#20998;&#21035;&#34987;$\Omega(1/\varepsilon^2)$&#21644;$\Omega(1/\varepsilon)$&#25152;&#38480;&#21046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#20986;&#22823;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25163;&#26469;&#23637;&#31034;&#35760;&#24518;&#22312;SCO&#20013;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09327v1 Announce Type: new Abstract: In this work, we investigate the interplay between memorization and learning in the context of \emph{stochastic convex optimization} (SCO). We define memorization via the information a learning algorithm reveals about its training data points. We then quantify this information using the framework of conditional mutual information (CMI) proposed by Steinke and Zakynthinou (2020). Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering an open question posed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded setting and under strong convexity, every learner with an excess error $\varepsilon$ has CMI bounded below by $\Omega(1/\varepsilon^2)$ and $\Omega(1/\varepsilon)$, respectively. We further demonstrate the essential role of memorization in learning problems in SCO by designing an adversary capable of accurately identifying a significant fraction of the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#22312;&#19981;&#30830;&#23450;&#39044;&#27979;&#20013;&#22914;&#20309;&#34920;&#31034;&#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#25490;&#21517;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.09326</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#39044;&#27979;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#22810;&#32452;&#20844;&#24179;&#24615;&#22312;&#25490;&#21517;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Stability and Multigroup Fairness in Ranking with Uncertain Predictions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#22312;&#19981;&#30830;&#23450;&#39044;&#27979;&#20013;&#22914;&#20309;&#34920;&#31034;&#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#25490;&#21517;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#21517;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#37117;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#20174;&#25628;&#32034;&#24341;&#25806;&#21040;&#25307;&#32856;&#22996;&#21592;&#20250;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#27979;&#22120;&#35757;&#32451;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#26102;&#65292;&#24403;&#23384;&#22312;&#20869;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#19981;&#26126;&#30830;&#22914;&#20309;&#22312;&#27966;&#29983;&#25490;&#21517;&#20013;&#34920;&#31034;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#32771;&#34385;&#25490;&#21517;&#20989;&#25968;&#65306;&#20174;&#20998;&#31867;&#20219;&#21153;&#30340;&#20010;&#20307;&#39044;&#27979;&#21040;&#25490;&#21517;&#20998;&#24067;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#20851;&#27880;&#25490;&#21517;&#20989;&#25968;&#30340;&#20004;&#20010;&#26041;&#38754;&#65306;&#23545;&#39044;&#27979;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#21644;&#23545;&#20010;&#20307;&#21644;&#23376;&#32676;&#20307;&#30340;&#20844;&#24179;&#24615;&#12290;&#31283;&#23450;&#24615;&#19981;&#20165;&#26159;&#20854;&#26412;&#36523;&#30340;&#37325;&#35201;&#35201;&#27714;&#65292;&#32780;&#19988;&#27491;&#22914;&#25105;&#20204;&#25152;&#26174;&#31034;&#30340;&#65292;&#23427;&#19982;Dwork&#31561;&#20154;&#65288;2012&#65289;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#22312;&#24847;&#20041;&#19978;&#26159;&#21644;&#35856;&#30340;&#12290;&#23613;&#31649;&#30830;&#23450;&#24615;&#25490;&#21517;&#20989;&#25968;&#38500;&#20102;&#24179;&#20961;&#30340;&#24773;&#20917;&#22806;&#19981;&#33021;&#31283;&#23450;&#65292;&#20294;&#25105;&#20204;&#26174;&#31034;&#20102;&#26368;&#36817;&#30001;Singh&#31561;&#20154;&#25552;&#20986;&#30340;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#65288;UA&#65289;&#25490;&#21517;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09326v1 Announce Type: new Abstract: Rankings are ubiquitous across many applications, from search engines to hiring committees. In practice, many rankings are derived from the output of predictors. However, when predictors trained for classification tasks have intrinsic uncertainty, it is not obvious how this uncertainty should be represented in the derived rankings. Our work considers ranking functions: maps from individual predictions for a classification task to distributions over rankings. We focus on two aspects of ranking functions: stability to perturbations in predictions and fairness towards both individuals and subgroups. Not only is stability an important requirement for its own sake, but -- as we show -- it composes harmoniously with individual fairness in the sense of Dwork et al. (2012). While deterministic ranking functions cannot be stable aside from trivial scenarios, we show that the recently proposed uncertainty aware (UA) ranking functions of Singh et al
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24320;&#21019;&#24615;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#34987;&#20154;&#31867;&#24863;&#30693;&#30340;&#22270;&#20687;&#65292;&#22312;&#32463;&#36807;&#25480;&#26435;&#30340;&#27169;&#22411;&#20013;&#20445;&#25345;&#20934;&#30830;&#30340;&#25512;&#29702;&#65292;&#21516;&#26102;&#35268;&#36991;&#20854;&#20182;&#26410;&#32463;&#25480;&#26435;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#25512;&#29702;&#65292;&#22635;&#34917;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.09316</link><description>&lt;p&gt;
&#21482;&#26377;&#25105;&#30340;&#27169;&#22411;&#22312;&#25105;&#30340;&#25968;&#25454;&#19978;&#65306;&#19968;&#31181;&#20445;&#25252;&#27169;&#22411;&#21644;&#27450;&#39575;&#26410;&#32463;&#25480;&#26435;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09316
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24320;&#21019;&#24615;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#34987;&#20154;&#31867;&#24863;&#30693;&#30340;&#22270;&#20687;&#65292;&#22312;&#32463;&#36807;&#25480;&#26435;&#30340;&#27169;&#22411;&#20013;&#20445;&#25345;&#20934;&#30830;&#30340;&#25512;&#29702;&#65292;&#21516;&#26102;&#35268;&#36991;&#20854;&#20182;&#26410;&#32463;&#25480;&#26435;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#25512;&#29702;&#65292;&#22635;&#34917;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24191;&#27867;&#24212;&#29992;&#20110;&#38754;&#37096;&#35782;&#21035;&#21644;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#31561;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#65292;&#38544;&#31169;&#21644;&#25968;&#25454;&#20445;&#25252;&#33267;&#20851;&#37325;&#35201;&#12290;&#22914;&#26524;&#19981;&#20445;&#25252;&#22270;&#20687;&#25968;&#25454;&#65292;&#21487;&#33021;&#20250;&#34987;&#21033;&#29992;&#26469;&#25512;&#26029;&#20010;&#20154;&#25110;&#29615;&#22659;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#22914;&#21152;&#23494;&#65292;&#29983;&#25104;&#30340;&#25200;&#21160;&#22270;&#20687;&#21363;&#20351;&#23545;&#20154;&#31867;&#26469;&#35828;&#20063;&#26080;&#27861;&#35782;&#21035;&#12290;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#31105;&#27490;&#25480;&#26435;&#21033;&#30410;&#30456;&#20851;&#32773;&#36827;&#34892;&#33258;&#21160;&#25512;&#29702;&#65292;&#38480;&#21046;&#20102;&#21830;&#19994;&#21644;&#24191;&#27867;&#36866;&#29992;&#30340;&#23454;&#38469;&#28608;&#21169;&#12290;&#36825;&#39033;&#24320;&#21019;&#24615;&#30740;&#31350;&#36890;&#36807;&#29983;&#25104;&#21487;&#34987;&#20154;&#31867;&#24863;&#30693;&#30340;&#22270;&#20687;&#65292;&#22312;&#32463;&#36807;&#25480;&#26435;&#30340;&#27169;&#22411;&#20013;&#20445;&#25345;&#20934;&#30830;&#30340;&#25512;&#29702;&#65292;&#21516;&#26102;&#35268;&#36991;&#20854;&#20182;&#31867;&#20284;&#25110;&#19981;&#21516;&#30446;&#26631;&#30340;&#26410;&#32463;&#25480;&#26435;&#30340;&#40657;&#30418;&#27169;&#22411;&#65292;&#24182;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;ImageNet&#65288;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65289;&#21644;Celeba-HQ&#25968;&#25454;&#38598;&#65288;&#29992;&#20110;&#36523;&#20221;&#35782;&#21035;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09316v1 Announce Type: cross Abstract: Deep neural networks are extensively applied to real-world tasks, such as face recognition and medical image classification, where privacy and data protection are critical. Image data, if not protected, can be exploited to infer personal or contextual information. Existing privacy preservation methods, like encryption, generate perturbed images that are unrecognizable to even humans. Adversarial attack approaches prohibit automated inference even for authorized stakeholders, limiting practical incentives for commercial and widespread adaptation. This pioneering study tackles an unexplored practical privacy preservation use case by generating human-perceivable images that maintain accurate inference by an authorized model while evading other unauthorized black-box models of similar or dissimilar objectives, and addresses the previous research gaps. The datasets employed are ImageNet, for image classification, Celeba-HQ dataset, for ident
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22240;&#26524;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#20174;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#21040;&#28508;&#22312;&#22240;&#26524;&#22270;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#22312;&#20849;&#20139;&#21160;&#21147;&#23398;&#30340;&#24773;&#20917;&#19979;&#30340;&#30417;&#30563;&#24335;&#22240;&#26524;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.09305</link><description>&lt;p&gt;
embracing the black box: &#26397;&#21521;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Embracing the black box: Heading towards foundation models for causal discovery from time series data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22240;&#26524;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#20174;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#21040;&#28508;&#22312;&#22240;&#26524;&#22270;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#22312;&#20849;&#20139;&#21160;&#21147;&#23398;&#30340;&#24773;&#20917;&#19979;&#30340;&#30417;&#30563;&#24335;&#22240;&#26524;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22240;&#26524;&#21457;&#29616;&#28085;&#30422;&#20102;&#35768;&#22810;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#19981;&#25903;&#25345;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#33539;&#24335;&#20043;&#19968;&#65306;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#25152;&#31216;&#20043;&#20026;&#22240;&#26524;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20197;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#20174;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#21040;&#28508;&#22312;&#22240;&#26524;&#22270;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#20849;&#20139;&#22823;&#37096;&#20998;&#21160;&#21147;&#23398;&#30340;&#24773;&#20917;&#19979;&#65292;&#30417;&#30563;&#24335;&#22240;&#26524;&#21457;&#29616;&#26159;&#21487;&#33021;&#30340;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#39069;&#22806;&#30340;&#25968;&#25454;&#19981;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#21147;&#23398;&#65292;&#22240;&#26524;&#39044;&#35757;&#32451;&#30340;&#24615;&#33021;&#20063;&#38543;&#30528;&#25968;&#25454;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#20363;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#22240;&#26524;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#22240;&#26524;&#21457;&#29616;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#26159;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09305v1 Announce Type: cross Abstract: Causal discovery from time series data encompasses many existing solutions, including those based on deep learning techniques. However, these methods typically do not endorse one of the most prevalent paradigms in deep learning: End-to-end learning. To address this gap, we explore what we call Causal Pretraining. A methodology that aims to learn a direct mapping from multivariate time series to the underlying causal graphs in a supervised manner. Our empirical findings suggest that causal discovery in a supervised manner is possible, assuming that the training and test time series samples share most of their dynamics. More importantly, we found evidence that the performance of Causal Pretraining can increase with data and model size, even if the additional data do not share the same dynamics. Further, we provide examples where causal discovery for real-world data with causally pretrained neural networks is possible within limits. We arg
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23545;&#27604;&#20102;&#20154;&#31867;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#20855;&#26377;&#21363;&#26102;&#27010;&#25324;&#33021;&#21147;&#65292;&#32780;DNNs&#23384;&#22312;&#28382;&#21518;&#27010;&#25324;&#29616;&#35937;&#65292;&#36825;&#34920;&#26126;&#20102;&#34920;&#31034;&#20998;&#27495;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.09303</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#30340;&#21363;&#26102;&#27010;&#25324;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28382;&#21518;&#27010;&#25324;&#8212;&#8212;&#34920;&#31034;&#20998;&#27495;&#30340;&#35777;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
Immediate generalisation in humans but a generalisation lag in deep neural networks$\unicode{x2014}$evidence for representational divergence?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09303
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#27604;&#20102;&#20154;&#31867;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#20855;&#26377;&#21363;&#26102;&#27010;&#25324;&#33021;&#21147;&#65292;&#32780;DNNs&#23384;&#22312;&#28382;&#21518;&#27010;&#25324;&#29616;&#35937;&#65292;&#36825;&#34920;&#26126;&#20102;&#34920;&#31034;&#20998;&#27495;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#23545;&#27604;&#20102;&#20154;&#31867;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#35768;&#22810;&#34892;&#20026;&#27604;&#36739;&#12290;&#36890;&#24120;&#65292;&#27604;&#36739;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#23398;&#20064;&#36807;&#31243;&#30340;&#26368;&#32456;&#32467;&#26524;&#65292;&#36890;&#36807;&#27979;&#37327;&#21644;&#27604;&#36739;&#30446;&#26631;&#31867;&#21035;&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34920;&#31034;&#22914;&#20309;&#24418;&#25104;&#21363;&#20854;&#36807;&#31243;&#8212;&#8212;&#21363;&#22312;&#33719;&#21462;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#21464;&#21270;&#21644;&#20013;&#38388;&#38454;&#27573;&#8212;&#8212;&#24448;&#24448;&#23569;&#26377;&#30452;&#25509;&#21644;&#23454;&#35777;&#30340;&#27604;&#36739;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#23545;&#20154;&#31867;&#35266;&#23519;&#32773;&#21644;&#19981;&#21516;&#32463;&#20856;&#19982;&#26368;&#26032;&#25216;&#26415;&#30340;DNNs&#20013;&#21487;&#36716;&#31227;&#34920;&#31034;&#26159;&#22914;&#20309;&#34987;&#33719;&#21462;&#30340;&#30340;&#35814;&#32454;&#35843;&#26597;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#38480;&#30340;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#65292;&#35813;&#29615;&#22659;&#20013;&#25105;&#20204;&#23545;&#40784;&#20102;&#23398;&#20064;&#30456;&#20851;&#30340;&#21442;&#25968;&#65292;&#22914;&#36215;&#22987;&#28857;&#12289;&#36755;&#20837;&#27169;&#24335;&#12289;&#21487;&#29992;&#36755;&#20837;&#25968;&#25454;&#20197;&#21450;&#25552;&#20379;&#30340;&#21453;&#39304;&#12290;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#25105;&#20204;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09303v1 Announce Type: cross Abstract: Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification. Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed. However, the process of how these representations emerge$\unicode{x2014}$that is, the behavioral changes and intermediate stages observed during the acquisition$\unicode{x2014}$is less often directly and empirically compared.   Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs. We develop a constrained supervised learning environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided. Across the whole learning process we evaluate 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22914;&#20309;&#22312;&#35757;&#32451;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#20195;&#30721;&#21253;&#21547;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#23457;&#35745;&#26102;&#30340;&#29256;&#26435;&#20405;&#26435;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09299</link><description>&lt;p&gt;
&#26410;&#32463;&#26412;&#20154;&#21516;&#24847;&#30340;&#35757;&#32451;&#65306;&#22312;&#35757;&#32451;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#20195;&#30721;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;
Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22914;&#20309;&#22312;&#35757;&#32451;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#20195;&#30721;&#21253;&#21547;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#23457;&#35745;&#26102;&#30340;&#29256;&#26435;&#20405;&#26435;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#23457;&#35745;&#36890;&#36807;&#39564;&#35777;&#24320;&#21457;&#30340;&#20195;&#30721;&#26159;&#21542;&#31526;&#21512;&#26631;&#20934;&#12289;&#27861;&#35268;&#21644;&#29256;&#26435;&#20445;&#25252;&#65292;&#30830;&#20445;&#20854;&#19981;&#21253;&#21547;&#26469;&#33258;&#21463;&#20445;&#25252;&#26469;&#28304;&#30340;&#20195;&#30721;&#12290;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#32534;&#30721;&#21161;&#25163;&#30340;&#20986;&#29616;&#32473;&#20195;&#30721;&#23457;&#35745;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#26469;&#33258;&#20844;&#24320;&#21487;&#29992;&#30340;&#26469;&#28304;&#12290;&#36825;&#24341;&#21457;&#20102;&#30693;&#35782;&#20135;&#26435;&#20405;&#26435;&#38382;&#39064;&#65292;&#22240;&#20026;&#24320;&#21457;&#32773;&#30340;&#20195;&#30721;&#24050;&#21253;&#21547;&#22312;&#25968;&#25454;&#38598;&#20013;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;LLMs&#24320;&#21457;&#30340;&#20195;&#30721;&#23457;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25105;&#20204;&#26080;&#27861;&#20934;&#30830;&#30830;&#23450;&#24320;&#21457;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;LLM&#26159;&#21542;&#24050;&#32463;&#22312;&#29305;&#23450;&#30340;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20195;&#30721;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#22240;&#20026;&#25105;&#20204;&#26080;&#27861;&#33719;&#24471;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#37492;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20445;&#23494;&#24615;&#65292;&#20256;&#32479;&#30340;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#31561;&#26041;&#27861;&#26080;&#27861;&#30830;&#20445;&#29256;&#26435;&#20405;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09299v1 Announce Type: cross Abstract: Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers' codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To add
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#37096;&#20998;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65288;PSRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#23398;&#20064;&#26469;&#29983;&#25104;&#26356;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#21516;&#26102;&#21033;&#29992;&#29366;&#24577;&#20272;&#35745;&#22120;&#25552;&#21462;&#20986;&#30417;&#30563;&#35821;&#20041;&#29366;&#24577;&#20449;&#24687;&#65292;&#20197;&#21450;&#25429;&#25417;&#28508;&#22312;&#29366;&#24577;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.09290</link><description>&lt;p&gt;
&#36890;&#36807;&#37096;&#20998;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21518;&#39564;&#21487;&#35266;&#23519;POMDP&#20013;&#30340;&#21487;&#35299;&#37322;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially Supervised Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#37096;&#20998;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65288;PSRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#23398;&#20064;&#26469;&#29983;&#25104;&#26356;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#21516;&#26102;&#21033;&#29992;&#29366;&#24577;&#20272;&#35745;&#22120;&#25552;&#21462;&#20986;&#30417;&#30563;&#35821;&#20041;&#29366;&#24577;&#20449;&#24687;&#65292;&#20197;&#21450;&#25429;&#25417;&#28508;&#22312;&#29366;&#24577;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35270;&#39057;&#28216;&#25103;&#12289;&#26426;&#22120;&#20154;&#25511;&#21046;&#12289;&#33258;&#20027;&#39550;&#39542;&#21644;&#33647;&#29289;&#21457;&#29616;&#31561;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#23601;&#12290;&#20294;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39046;&#22495;&#20013;&#65292;&#24120;&#35265;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#39640;&#32500;&#35266;&#23519;&#65288;&#22914;&#22270;&#20687;&#65289;&#36827;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#65292;&#32780;&#27809;&#26377;&#26126;&#30830;&#25512;&#29702;&#30495;&#23454;&#29366;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26367;&#20195;&#26041;&#21521;&#65292;&#24341;&#20837;&#20102;&#37096;&#20998;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65288;PSRL&#65289;&#26694;&#26550;&#12290;PSRL&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#23558;&#30417;&#30563;&#23398;&#20064;&#21644;&#38750;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#34701;&#21512;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29366;&#24577;&#20272;&#35745;&#22120;&#20174;&#39640;&#32500;&#35266;&#23519;&#20013;&#25552;&#21462;&#20986;&#26377;&#26102;&#22312;&#35757;&#32451;&#26102;&#23436;&#20840;&#35266;&#23519;&#21040;&#30340;&#30417;&#30563;&#35821;&#20041;&#29366;&#24577;&#20449;&#24687;&#12290;&#36825;&#26679;&#21487;&#20197;&#24471;&#21040;&#26356;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#23558;&#29366;&#24577;&#39044;&#27979;&#19982;&#25511;&#21046;&#32467;&#21512;&#36215;&#26469;&#12290;&#21516;&#26102;&#65292;&#23427;&#36824;&#25429;&#25417;&#21040;&#19968;&#20010;&#38750;&#30417;&#30563;&#28508;&#22312;&#34920;&#31034;&#12290;&#36825;&#20004;&#32773;-&#35821;&#20041;&#29366;&#24577;&#21644;&#28508;&#22312;&#29366;&#24577;&#28982;&#21518;&#34987;&#34701;&#21512;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09290v1 Announce Type: cross Abstract: Deep reinforcement learning has demonstrated remarkable achievements across diverse domains such as video games, robotic control, autonomous driving, and drug discovery. Common methodologies in partially-observable domains largely lean on end-to-end learning from high-dimensional observations, such as images, without explicitly reasoning about true state. We suggest an alternative direction, introducing the Partially Supervised Reinforcement Learning (PSRL) framework. At the heart of PSRL is the fusion of both supervised and unsupervised learning. The approach leverages a state estimator to distill supervised semantic state information from high-dimensional observations which are often fully observable at training time. This yields more interpretable policies that compose state predictions with control. In parallel, it captures an unsupervised latent representation. These two-the semantic state and the latent state-are then fused and ut
&lt;/p&gt;</description></item><item><title>EcoVal&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#65292;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#20215;&#20540;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23454;&#29992;&#22320;&#20272;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.09288</link><description>&lt;p&gt;
EcoVal:&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EcoVal: An Efficient Data Valuation Framework for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09288
&lt;/p&gt;
&lt;p&gt;
EcoVal&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#65292;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#20215;&#20540;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23454;&#29992;&#22320;&#20272;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#20013;&#37327;&#21270;&#25968;&#25454;&#30340;&#20215;&#20540;&#21487;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#20513;&#35758;&#20013;&#20570;&#20986;&#26356;&#20855;&#25112;&#30053;&#24847;&#20041;&#30340;&#20915;&#31574;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;Shapley&#20540;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#22312;&#35745;&#31639;&#26041;&#38754;&#38750;&#24120;&#26114;&#36149;&#65292;&#22240;&#20026;&#38656;&#35201;&#22823;&#37327;&#37325;&#22797;&#35757;&#32451;&#27169;&#22411;&#25165;&#33021;&#33719;&#24471;Shapley&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;EcoVal&#65292;&#20197;&#24555;&#36895;&#23454;&#29992;&#30340;&#26041;&#24335;&#20272;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#19981;&#30452;&#25509;&#22788;&#29702;&#29420;&#31435;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#32780;&#26159;&#30830;&#23450;&#31867;&#20284;&#30340;&#25968;&#25454;&#28857;&#31751;&#30340;&#20215;&#20540;&#12290;&#36825;&#20010;&#20215;&#20540;&#36827;&#19968;&#27493;&#22312;&#25152;&#26377;&#25104;&#21592;&#31751;&#28857;&#20043;&#38388;&#20256;&#25773;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#20215;&#20540;&#26469;&#30830;&#23450;&#25972;&#20307;&#25968;&#25454;&#20215;&#20540;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#24615;&#33021;&#24314;&#27169;&#20026;&#8220;&#29983;&#20135;&#20989;&#25968;&#8221;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09288v1 Announce Type: new Abstract: Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall data value can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as a \textit{production function}, a concept which is po
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#27169;&#22411;&#20107;&#23454;&#27169;&#26495;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#22312;&#26538;&#25903;&#26292;&#21147;&#30740;&#31350;&#20013;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#65292;&#24182;&#20943;&#23569;&#23545;&#26131;&#21463;&#21093;&#21066;&#20154;&#32676;&#25968;&#25454;&#30340;&#19981;&#20449;&#20219;&#12290;&#27492;&#27169;&#26495;&#20351;&#19968;&#33324;&#29992;&#25143;&#33021;&#22815;&#35780;&#20272;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.09286</link><description>&lt;p&gt;
&#33829;&#20859;&#20107;&#23454;&#12289;&#33647;&#29289;&#20107;&#23454;&#21644;&#27169;&#22411;&#20107;&#23454;&#65306;&#23558;AI&#20262;&#29702;&#24212;&#29992;&#20110;&#26538;&#25903;&#26292;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Nutrition Facts, Drug Facts, and Model Facts: Putting AI Ethics into Practice in Gun Violence Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09286
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#27169;&#22411;&#20107;&#23454;&#27169;&#26495;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#22312;&#26538;&#25903;&#26292;&#21147;&#30740;&#31350;&#20013;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#65292;&#24182;&#20943;&#23569;&#23545;&#26131;&#21463;&#21093;&#21066;&#20154;&#32676;&#25968;&#25454;&#30340;&#19981;&#20449;&#20219;&#12290;&#27492;&#27169;&#26495;&#20351;&#19968;&#33324;&#29992;&#25143;&#33021;&#22815;&#35780;&#20272;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;&#26538;&#25903;&#26292;&#21147;&#30740;&#31350;&#20013;&#24212;&#29992;AI&#20262;&#29702;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#20943;&#23569;&#23545;&#40657;&#20154;&#21644;&#26377;&#33394;&#20154;&#31181;&#31561;&#26131;&#21463;&#21093;&#21066;&#30340;&#24369;&#21183;&#20154;&#32676;&#25968;&#25454;&#30340;&#19981;&#20449;&#20219;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#27169;&#22411;&#20107;&#23454;&#27169;&#26495;&#65292;&#23558;&#20934;&#30830;&#24230;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#20998;&#35299;&#20026;&#26631;&#20934;&#21270;&#21644;&#26368;&#23567;&#22797;&#26434;&#20540;&#65292;&#20351;&#19968;&#33324;&#29992;&#25143;&#33021;&#22815;&#35780;&#20272;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#20559;&#35265;&#65292;&#32780;&#19981;&#38656;&#35201;&#28145;&#20837;&#30740;&#31350;&#25216;&#26415;&#27169;&#22411;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09286v1 Announce Type: new Abstract: Objective: Firearm injury research necessitates using data from often-exploited vulnerable populations of Black and Brown Americans. In order to minimize distrust, this study provides a framework for establishing AI trust and transparency with the general population. Methods: We propose a Model Facts template that is easily extendable and decomposes accuracy and demographics into standardized and minimally complex values. This framework allows general users to assess the validity and biases of a model without diving into technical model documentation. Examples: We apply the Model Facts template on two previously published models, a violence risk identification model and a suicide risk prediction model. We demonstrate the ease of accessing the appropriate information when the data is structured appropriately. Discussion: The Model Facts template is limited in its current form to human based data and biases. Like nutrition facts, it also wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#21644;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#65292;&#20197;&#21450;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#26469;&#23454;&#29616;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23454;&#35777;&#39564;&#35777;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09281</link><description>&lt;p&gt;
&#25552;&#21319;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#21327;&#26041;&#24046;&#21644;Hessian&#30697;&#38453;&#30340;&#21327;&#21516;&#29305;&#24449;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Synergistic eigenanalysis of covariance and Hessian matrices for enhanced binary classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#21644;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#65292;&#20197;&#21450;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#26469;&#23454;&#29616;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23454;&#35777;&#39564;&#35777;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#21327;&#26041;&#24046;&#21644;Hessian&#30697;&#38453;&#20998;&#21035;&#34987;&#21333;&#29420;&#20998;&#26512;&#65292;&#20294;&#26159;&#23558;&#36825;&#20123;&#30697;&#38453;&#38598;&#25104;&#36215;&#26469;&#21487;&#20197;&#22686;&#24378;&#23427;&#20204;&#22312;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#35777;&#26126;&#20102;&#23427;&#21487;&#20197;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#24182;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#25353;&#29031;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#30340;&#26631;&#20934;&#23454;&#29616;&#20102;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#21644;&#20581;&#24247;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#39564;&#35777;&#22987;&#32456;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09281v1 Announce Type: cross Abstract: Covariance and Hessian matrices have been analyzed separately in the literature for classification problems. However, integrating these matrices has the potential to enhance their combined power in improving classification performance. We present a novel approach that combines the eigenanalysis of a covariance matrix evaluated on a training set with a Hessian matrix evaluated on a deep learning model to achieve optimal class separability in binary classification tasks. Our approach is substantiated by formal proofs that establish its capability to maximize between-class mean distance and minimize within-class variances. By projecting data into the combined space of the most relevant eigendirections from both matrices, we achieve optimal class separability as per the linear discriminant analysis (LDA) criteria. Empirical validation across neural and health datasets consistently supports our theoretical framework and demonstrates that our
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#26377;&#23475;&#34299;&#31867;&#27700;&#21326;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#29983;&#20135;&#21306;&#22495;&#30340;&#27963;&#21160;&#29366;&#24577;&#26469;&#26367;&#20195;&#24120;&#35268;&#30340;&#27602;&#32032;&#27987;&#24230;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#37319;&#26679;&#19981;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#30340;&#39118;&#38505;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09271</link><description>&lt;p&gt;
&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#26377;&#23475;&#34299;&#31867;&#27700;&#21326;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hybrid Machine Learning techniques in the management of harmful algal blooms impact
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#26377;&#23475;&#34299;&#31867;&#27700;&#21326;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#29983;&#20135;&#21306;&#22495;&#30340;&#27963;&#21160;&#29366;&#24577;&#26469;&#26367;&#20195;&#24120;&#35268;&#30340;&#27602;&#32032;&#27987;&#24230;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#37319;&#26679;&#19981;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#30340;&#39118;&#38505;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#23475;&#34299;&#31867;&#27700;&#21326;&#26159;&#39640;&#27987;&#24230;&#34299;&#31867;&#30340;&#31361;&#21457;&#20107;&#20214;&#65292;&#23545;&#20154;&#31867;&#30340;&#39135;&#29992;&#26377;&#28508;&#22312;&#30340;&#27602;&#24615;&#12290;&#36125;&#31867;&#20859;&#27542;&#19994;&#21487;&#33021;&#21463;&#21040;&#26377;&#23475;&#34299;&#31867;&#27700;&#21326;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#36125;&#31867;&#20316;&#20026;&#36807;&#28388;&#39135;&#29289;&#30340;&#21160;&#29289;&#65292;&#33021;&#22815;&#22312;&#20854;&#20307;&#20869;&#31215;&#32047;&#39640;&#27987;&#24230;&#30340;&#28023;&#27915;&#29983;&#29289;&#27602;&#32032;&#12290;&#20026;&#20102;&#36991;&#20813;&#20154;&#31867;&#39135;&#29992;&#30340;&#39118;&#38505;&#65292;&#24403;&#26816;&#27979;&#21040;&#27602;&#24615;&#26102;&#20250;&#31105;&#27490;&#25910;&#21106;&#12290;&#30446;&#21069;&#65292;&#29983;&#20135;&#21306;&#22495;&#30340;&#20851;&#38381;&#26159;&#22522;&#20110;&#19987;&#23478;&#30693;&#35782;&#65292;&#24182;&#19988;&#24403;&#26465;&#20214;&#22797;&#26434;&#19988;&#26080;&#27861;&#36827;&#34892;&#37319;&#26679;&#26102;&#65292;&#39044;&#27979;&#27169;&#22411;&#30340;&#23384;&#22312;&#23558;&#26377;&#25152;&#24110;&#21161;&#12290;&#34429;&#28982;&#32905;&#31867;&#20013;&#27602;&#32032;&#30340;&#27987;&#24230;&#26159;&#25511;&#21046;&#36125;&#31867;&#29983;&#20135;&#21306;&#22495;&#30340;&#19987;&#23478;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#20294;&#24456;&#23569;&#34987;&#33258;&#21160;&#39044;&#27979;&#27169;&#22411;&#29992;&#20316;&#30446;&#26631;&#12290;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#24314;&#31435;&#30340;&#37319;&#26679;&#35745;&#21010;&#23548;&#33268;&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#24615;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#29983;&#20135;&#21306;&#22495;&#30340;&#27963;&#21160;&#29366;&#24577;&#20316;&#20026;&#22522;&#20110;&#35813;&#21306;&#22495;&#26159;&#21542;&#23384;&#22312;&#26377;&#23475;&#34299;&#31867;&#27700;&#21326;&#30340;&#30446;&#26631;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09271v1 Announce Type: new Abstract: Harmful algal blooms (HABs) are episodes of high concentrations of algae that are potentially toxic for human consumption. Mollusc farming can be affected by HABs because, as filter feeders, they can accumulate high concentrations of marine biotoxins in their tissues. To avoid the risk to human consumption, harvesting is prohibited when toxicity is detected. At present, the closure of production areas is based on expert knowledge and the existence of a predictive model would help when conditions are complex and sampling is not possible. Although the concentration of toxin in meat is the method most commonly used by experts in the control of shellfish production areas, it is rarely used as a target by automatic prediction models. This is largely due to the irregularity of the data due to the established sampling programs. As an alternative, the activity status of production areas has been proposed as a target variable based on whether moll
&lt;/p&gt;</description></item><item><title>transformers&#30340;&#20851;&#38190;&#21306;&#21035;&#24615;&#36136;&#26159;&#24182;&#34892;&#24615;&#65292;&#23427;&#20204;&#36890;&#36807;&#20351;&#29992;&#22266;&#23450;&#25968;&#37327;&#30340;&#33258;&#27880;&#24847;&#23618;&#23454;&#29616;&#23545;&#22522;&#26412;&#35745;&#31639;&#20219;&#21153;&#30340;&#39640;&#25928;&#35299;&#20915;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#26080;&#27861;&#34987;&#20854;&#20182;&#31070;&#32463;&#24207;&#21015;&#27169;&#22411;&#21644;&#20122;&#20108;&#27425;&#21464;&#21387;&#22120;&#36924;&#36817;&#39640;&#25928;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.09268</link><description>&lt;p&gt;
Transformers&#65292;&#24182;&#34892;&#35745;&#31639;&#21644;&#23545;&#25968;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
Transformers, parallel computation, and logarithmic depth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09268
&lt;/p&gt;
&lt;p&gt;
transformers&#30340;&#20851;&#38190;&#21306;&#21035;&#24615;&#36136;&#26159;&#24182;&#34892;&#24615;&#65292;&#23427;&#20204;&#36890;&#36807;&#20351;&#29992;&#22266;&#23450;&#25968;&#37327;&#30340;&#33258;&#27880;&#24847;&#23618;&#23454;&#29616;&#23545;&#22522;&#26412;&#35745;&#31639;&#20219;&#21153;&#30340;&#39640;&#25928;&#35299;&#20915;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#26080;&#27861;&#34987;&#20854;&#20182;&#31070;&#32463;&#24207;&#21015;&#27169;&#22411;&#21644;&#20122;&#20108;&#27425;&#21464;&#21387;&#22120;&#36924;&#36817;&#39640;&#25928;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#22266;&#23450;&#25968;&#37327;&#30340;&#33258;&#27880;&#24847;&#23618;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#21644;&#34987;Massively Parallel Computation&#30340;&#36890;&#20449;&#36718;&#25968;&#25152;&#27169;&#25311;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#25968;&#28145;&#24230;&#23545;&#20110;transformers&#26469;&#35299;&#20915;&#19968;&#20123;&#20854;&#20182;&#31070;&#32463;&#24207;&#21015;&#27169;&#22411;&#21644;&#20122;&#20108;&#27425;&#21464;&#21387;&#22120;&#36924;&#36817;&#26080;&#27861;&#39640;&#25928;&#35299;&#20915;&#30340;&#22522;&#26412;&#35745;&#31639;&#20219;&#21153;&#26159;&#36275;&#22815;&#30340;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#23558;&#24182;&#34892;&#24615;&#30830;&#23450;&#20026;transformers&#30340;&#19968;&#20010;&#20851;&#38190;&#21306;&#21035;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09268v1 Announce Type: new Abstract: We show that a constant number of self-attention layers can efficiently simulate, and be simulated by, a constant number of communication rounds of Massively Parallel Computation. As a consequence, we show that logarithmic depth is sufficient for transformers to solve basic computational tasks that cannot be efficiently solved by several other neural sequence models and sub-quadratic transformer approximations. We thus establish parallelism as a key distinguishing property of transformers.
&lt;/p&gt;</description></item><item><title>UR2M&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#24863;&#30693;&#30340;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;&#65292;&#38024;&#23545;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#26469;&#35299;&#20915;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#20135;&#29983;&#19981;&#20934;&#30830;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09264</link><description>&lt;p&gt;
UR2M: &#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#24863;&#30693;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09264
&lt;/p&gt;
&lt;p&gt;
UR2M&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#24863;&#30693;&#30340;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;&#65292;&#38024;&#23545;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#26469;&#35299;&#20915;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#20135;&#29983;&#19981;&#20934;&#30830;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#30340;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#23481;&#26131;&#20135;&#29983;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#31227;&#21160;&#21307;&#30103;&#31561;&#24212;&#29992;&#20013;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26377;&#28508;&#21147;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25216;&#26415;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#20869;&#23384;&#65292;&#20351;&#23427;&#20204;&#22312;&#24494;&#25511;&#21046;&#22120;&#65288;MCU&#65289;&#19978;&#30340;&#23454;&#26045;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#36825;&#20010;&#38480;&#21046;&#38459;&#30861;&#20102;&#35768;&#22810;&#37325;&#35201;&#30340;&#35774;&#22791;&#19978;&#21487;&#31359;&#25140;&#20107;&#20214;&#26816;&#27979;&#65288;WED&#65289;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#65292;&#22914;&#24515;&#33039;&#30149;&#21457;&#20316;&#26816;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UR2M&#65292;&#19968;&#20010;&#38024;&#23545;MCU&#30340;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#24863;&#30693;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#65288;i&#65289;&#22522;&#20110;&#35777;&#25454;&#29702;&#35770;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;WED&#65292;&#29992;&#20110;&#20934;&#30830;&#30340;&#20107;&#20214;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09264v1 Announce Type: new Abstract: Traditional machine learning techniques are prone to generating inaccurate predictions when confronted with shifts in the distribution of data between the training and testing phases. This vulnerability can lead to severe consequences, especially in applications such as mobile healthcare. Uncertainty estimation has the potential to mitigate this issue by assessing the reliability of a model's output. However, existing uncertainty estimation techniques often require substantial computational resources and memory, making them impractical for implementation on microcontrollers (MCUs). This limitation hinders the feasibility of many important on-device wearable event detection (WED) applications, such as heart attack detection.   In this paper, we present UR2M, a novel Uncertainty and Resource-aware event detection framework for MCUs. Specifically, we (i) develop an uncertainty-aware WED based on evidential theory for accurate event detection
&lt;/p&gt;</description></item><item><title>&#21464;&#38761;&#24615;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;(TAAF)&#34987;&#21457;&#29616;&#21487;&#20197;&#24191;&#20041;&#22320;&#25512;&#24191;&#29616;&#26377;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#20102;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#30340;&#30456;&#20284;&#27010;&#24565;&#65292;&#36825;&#20351;&#24471;TAAF&#25104;&#20026;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#28508;&#21147;&#21644;&#36866;&#24212;&#24615;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.09249</link><description>&lt;p&gt;
&#25506;&#32034;&#20851;&#31995;&#65306;&#21464;&#38761;&#24615;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#19982;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Exploring the Relationship: Transformative Adaptive Activation Functions in Comparison to Other Activation Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09249
&lt;/p&gt;
&lt;p&gt;
&#21464;&#38761;&#24615;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;(TAAF)&#34987;&#21457;&#29616;&#21487;&#20197;&#24191;&#20041;&#22320;&#25512;&#24191;&#29616;&#26377;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#20102;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#30340;&#30456;&#20284;&#27010;&#24565;&#65292;&#36825;&#20351;&#24471;TAAF&#25104;&#20026;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#28508;&#21147;&#21644;&#36866;&#24212;&#24615;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#35768;&#22810;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#32780;&#28608;&#27963;&#20989;&#25968;&#26159;&#23454;&#29616;&#36825;&#31181;&#24615;&#33021;&#30340;&#20027;&#35201;&#26500;&#24314;&#27169;&#22359;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21464;&#38761;&#24615;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;(TAAF)&#65292;&#23427;&#20801;&#35768;&#20219;&#20309;&#22402;&#30452;&#21644;&#27700;&#24179;&#24179;&#31227;&#21644;&#32553;&#25918;&#12290;&#26412;&#30740;&#31350;&#23558;TAAF&#32622;&#20110;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#30340;&#32972;&#26223;&#19979;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;TAAFs&#24191;&#20041;&#22320;&#25512;&#24191;&#20102;50&#31181;&#29616;&#26377;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#20102;&#36229;&#36807;70&#31181;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#30340;&#31867;&#20284;&#27010;&#24565;&#65292;&#31361;&#20986;&#20102;TAAFs&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#36825;&#39033;&#20840;&#38754;&#30340;&#25506;&#32034;&#23558;TAAFs&#23450;&#20301;&#20026;&#31070;&#32463;&#32593;&#32476;&#20013;&#20855;&#26377;&#28508;&#21147;&#21644;&#36866;&#24212;&#24615;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09249v1 Announce Type: new Abstract: Neural networks are the state-of-the-art approach for many tasks and the activation function is one of the main building blocks that allow such performance. Recently, a novel transformative adaptive activation function (TAAF) allowing for any vertical and horizontal translation and scaling was proposed. This work sets the TAAF into the context of other activation functions. It shows that the TAAFs generalize over 50 existing activation functions and utilize similar concepts as over 70 other activation functions, underscoring the versatility of TAAFs. This comprehensive exploration positions TAAFs as a promising and adaptable addition to neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#37327;&#36817;&#20284;&#26041;&#27861;&#65292;&#22312;&#24322;&#27493;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#26377;&#25928;&#32467;&#21512;&#20102;&#21160;&#37327;&#21644;&#24322;&#27493;&#21327;&#35758;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21160;&#37327;&#26356;&#26032;&#30340;&#20559;&#24046;&#26469;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#21160;&#37327;&#36817;&#20284;&#22312;&#22522;&#20934;FL&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09247</link><description>&lt;p&gt;
&#24322;&#27493;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21160;&#37327;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Momentum Approximation in Asynchronous Private Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#37327;&#36817;&#20284;&#26041;&#27861;&#65292;&#22312;&#24322;&#27493;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#26377;&#25928;&#32467;&#21512;&#20102;&#21160;&#37327;&#21644;&#24322;&#27493;&#21327;&#35758;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21160;&#37327;&#26356;&#26032;&#30340;&#20559;&#24046;&#26469;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#21160;&#37327;&#36817;&#20284;&#22312;&#22522;&#20934;FL&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#21327;&#35758;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#25552;&#39640;&#22823;&#35268;&#27169;&#23458;&#25143;&#31471;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;&#21160;&#37327;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21516;&#27493;FL&#20013;&#23454;&#29616;&#26368;&#20339;&#27169;&#22411;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#24322;&#27493;FL&#31639;&#27861;&#20013;&#31616;&#21333;&#22320;&#24212;&#29992;&#21160;&#37327;&#20250;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#21644;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#32467;&#21512;&#36825;&#20004;&#31181;&#25216;&#26415;&#20197;&#23454;&#29616;&#21452;&#36194;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24322;&#27493;&#24615;&#24341;&#20837;&#20102;&#23545;&#21160;&#37327;&#26356;&#26032;&#30340;&#38544;&#21547;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#37327;&#36817;&#20284;&#65292;&#36890;&#36807;&#25214;&#21040;&#25152;&#26377;&#21382;&#21490;&#27169;&#22411;&#26356;&#26032;&#30340;&#26368;&#20339;&#21152;&#26435;&#24179;&#22343;&#20540;&#26469;&#26368;&#23567;&#21270;&#20559;&#24046;&#12290;&#21160;&#37327;&#36817;&#20284;&#19982;&#23433;&#20840;&#32858;&#21512;&#21644;&#24046;&#20998;&#38544;&#31169;&#26159;&#20860;&#23481;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#29983;&#20135;&#30340;FL&#31995;&#32479;&#20013;&#24456;&#23481;&#26131;&#22320;&#38598;&#25104;&#65292;&#21482;&#38656;&#36739;&#23567;&#30340;&#36890;&#20449;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;FL&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#21160;&#37327;&#36817;&#20284;&#22312;&#24615;&#33021;&#19978;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09247v1 Announce Type: new Abstract: Asynchronous protocols have been shown to improve the scalability of federated learning (FL) with a massive number of clients. Meanwhile, momentum-based methods can achieve the best model quality in synchronous FL. However, naively applying momentum in asynchronous FL algorithms leads to slower convergence and degraded model performance. It is still unclear how to effective combinie these two techniques together to achieve a win-win. In this paper, we find that asynchrony introduces implicit bias to momentum updates. In order to address this problem, we propose momentum approximation that minimizes the bias by finding an optimal weighted average of all historical model updates. Momentum approximation is compatible with secure aggregation as well as differential privacy, and can be easily integrated in production FL systems with a minor communication and storage cost. We empirically demonstrate that on benchmark FL datasets, momentum appro
&lt;/p&gt;</description></item><item><title>L3DAS23&#25361;&#25112;&#36187;&#26088;&#22312;&#25512;&#36827;&#26426;&#22120;&#23398;&#20064;&#22312;3D&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#26041;&#38754;&#30340;&#21512;&#20316;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27169;&#22411;&#65292;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#25193;&#23637;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;3D&#35821;&#38899;&#22686;&#24378;&#21644;3D&#22768;&#38899;&#20107;&#20214;&#23450;&#20301;&#21644;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.09245</link><description>&lt;p&gt;
L3DAS23&#25361;&#25112;&#36187;&#20851;&#20110;&#38899;&#39057;-&#35270;&#35273;&#25193;&#23637;&#29616;&#23454;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Overview of the L3DAS23 Challenge on Audio-Visual Extended Reality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09245
&lt;/p&gt;
&lt;p&gt;
L3DAS23&#25361;&#25112;&#36187;&#26088;&#22312;&#25512;&#36827;&#26426;&#22120;&#23398;&#20064;&#22312;3D&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#26041;&#38754;&#30340;&#21512;&#20316;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27169;&#22411;&#65292;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#25193;&#23637;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;3D&#35821;&#38899;&#22686;&#24378;&#21644;3D&#22768;&#38899;&#20107;&#20214;&#23450;&#20301;&#21644;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
L3DAS23&#20449;&#21495;&#22788;&#29702;&#22823;&#36187;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20419;&#36827;&#21644;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#22312;3D&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#26041;&#38754;&#30340;&#21512;&#20316;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#25193;&#23637;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;3D&#35821;&#38899;&#22686;&#24378;&#20197;&#21450;3D&#22768;&#38899;&#20107;&#20214;&#23450;&#20301;&#21644;&#26816;&#27979;&#12290;&#20316;&#20026;&#26368;&#26032;&#19968;&#23626;&#27604;&#36187;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#19982;L3DAS21&#21644;L3DAS22&#30340;&#25968;&#25454;&#38598;&#20855;&#26377;&#30456;&#21516;&#30340;&#19968;&#33324;&#29305;&#24615;&#65292;&#20294;&#26159;&#20351;&#29992;&#20102;&#22810;&#20010;&#28151;&#21709;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#19968;&#38454;Ambisonics&#24405;&#38899;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#22987;&#25506;&#32034;&#38899;&#39057;-&#35270;&#35273;&#22330;&#26223;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#40614;&#20811;&#39118;&#20301;&#32622;&#21644;&#26041;&#21521;&#25152;&#24863;&#30693;&#30340;&#36825;&#20123;&#29615;&#22659;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26356;&#26032;&#30340;&#20004;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#21487;&#20197;&#25903;&#25345;&#38899;&#39057;-&#22270;&#20687;&#23545;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25903;&#25345;&#30340;API&#26469;&#22797;&#21046;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#21442;&#19982;&#32773;&#30340;&#32467;&#26524;&#12290;&#26356;&#22810;&#35814;&#24773;&#35831;&#21442;&#35265;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09245v1 Announce Type: cross Abstract: The primary goal of the L3DAS23 Signal Processing Grand Challenge at ICASSP 2023 is to promote and support collaborative research on machine learning for 3D audio signal processing, with a specific emphasis on 3D speech enhancement and 3D Sound Event Localization and Detection in Extended Reality applications. As part of our latest competition, we provide a brand-new dataset, which maintains the same general characteristics of the L3DAS21 and L3DAS22 datasets, but with first-order Ambisonics recordings from multiple reverberant simulated environments. Moreover, we start exploring an audio-visual scenario by providing images of these environments, as perceived by the different microphone positions and orientations. We also propose updated baseline models for both tasks that can now support audio-image couples as input and a supporting API to replicate our results. Finally, we present the results of the participants. Further details about
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Switch EMA&#65288;SEMA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#20462;&#25913;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;EMA&#65289;&#21442;&#25968;&#65292;&#21487;&#20197;&#24110;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36798;&#21040;&#26356;&#22909;&#30340;&#24179;&#22374;&#24615;&#21644;&#38160;&#24230;&#30340;&#27010;&#25324;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;SEMA&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09240</link><description>&lt;p&gt;
Switch EMA: &#26356;&#22909;&#30340;&#24179;&#22374;&#24615;&#21644;&#38160;&#24230;&#30340;&#20813;&#36153;&#21320;&#39184;
&lt;/p&gt;
&lt;p&gt;
Switch EMA: A Free Lunch for Better Flatness and Sharpness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Switch EMA&#65288;SEMA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#20462;&#25913;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;EMA&#65289;&#21442;&#25968;&#65292;&#21487;&#20197;&#24110;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36798;&#21040;&#26356;&#22909;&#30340;&#24179;&#22374;&#24615;&#21644;&#38160;&#24230;&#30340;&#27010;&#25324;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;SEMA&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;EMA&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26435;&#37325;&#24179;&#22343;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20248;&#21270;&#20013;&#23398;&#20064;&#24179;&#22374;&#30340;&#26368;&#20248;&#35299;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27010;&#25324;&#33021;&#21147;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#23613;&#31649;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24179;&#22374;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#24179;&#22343;&#26041;&#27861;&#21487;&#33021;&#38519;&#20837;&#26356;&#24046;&#30340;&#26368;&#32456;&#24615;&#33021;&#25110;&#38656;&#35201;&#39069;&#22806;&#30340;&#27979;&#35797;&#26102;&#38388;&#35745;&#31639;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#20462;&#25913;&#65292;&#21363;&#22312;&#27599;&#20010;&#21608;&#26399;&#21518;&#23558;EMA&#21442;&#25968;&#20999;&#25442;&#22238;&#21407;&#22987;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;EMA&#30340;&#20805;&#20998;&#28508;&#21147;&#65292;&#34987;&#31216;&#20026;Switch EMA&#65288;SEMA&#65289;&#12290;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#26041;&#38754;&#37117;&#35777;&#26126;&#20102;SEMA&#21487;&#20197;&#24110;&#21161;DNNs&#36798;&#21040;&#26356;&#22909;&#30340;&#24179;&#22374;&#24615;&#21644;&#38160;&#24230;&#20043;&#38388;&#24179;&#34913;&#30340;&#27010;&#25324;&#26368;&#20248;&#35299;&#12290;&#20026;&#20102;&#39564;&#35777;SEMA&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#36776;&#21035;&#24615;&#12289;&#29983;&#25104;&#24615;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#20998;&#21106;&#12289;&#22270;&#20687;&#29983;&#25104;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09240v1 Announce Type: new Abstract: Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization. Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations. This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA). From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness. To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#37325;&#35201;&#24615;&#30340;&#36127;&#26679;&#26412;&#25277;&#26679;&#35757;&#32451;TGNNS&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09239</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#30828;&#36127;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#35757;&#32451;&#26102;&#38388;GNN
&lt;/p&gt;
&lt;p&gt;
Robust Training of Temporal GNNs using Nearest Neighbours based Hard Negatives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#37325;&#35201;&#24615;&#30340;&#36127;&#26679;&#26412;&#25277;&#26679;&#35757;&#32451;TGNNS&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;(TGNNS)&#22312;&#26410;&#26469;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;TGNNS&#30340;&#35757;&#32451;&#26159;&#36890;&#36807;&#22343;&#21248;&#38543;&#26426;&#25277;&#26679;&#30340;&#26080;&#30417;&#30563;&#25439;&#22833;&#36827;&#34892;&#21015;&#20030;&#30340;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23545;&#20110;&#27491;&#20363;&#24773;&#20917;&#65292;&#25439;&#22833;&#26159;&#22312;&#26080;&#20449;&#24687;&#30340;&#36127;&#26679;&#26412;&#19978;&#35745;&#31639;&#30340;&#65292;&#36825;&#24341;&#20837;&#20102;&#20887;&#20313;&#21644;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;TGNNS&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#37325;&#35201;&#24615;&#30340;&#36127;&#26679;&#26412;&#25277;&#26679;&#26469;&#26367;&#25442;&#22343;&#21248;&#36127;&#26679;&#26412;&#25277;&#26679;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#23545;&#36127;&#20363;&#37319;&#26679;&#30340;&#21160;&#24577;&#35745;&#31639;&#20998;&#24067;&#36827;&#34892;&#20102;&#29702;&#35770;&#39564;&#35777;&#21644;&#23450;&#20041;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23545;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#22522;&#20110;&#25552;&#20986;&#30340;&#36127;&#26679;&#26412;&#25277;&#26679;&#30340;&#25439;&#22833;&#35757;&#32451;&#30340;TGNNS&#25552;&#20379;&#20102;&#19968;&#33268;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09239v1 Announce Type: new Abstract: Temporal graph neural networks Tgnn have exhibited state-of-art performance in future-link prediction tasks. Training of these TGNNs is enumerated by uniform random sampling based unsupervised loss. During training, in the context of a positive example, the loss is computed over uninformative negatives, which introduces redundancy and sub-optimal performance. In this paper, we propose modified unsupervised learning of Tgnn, by replacing the uniform negative sampling with importance-based negative sampling. We theoretically motivate and define the dynamically computed distribution for a sampling of negative examples. Finally, using empirical evaluations over three real-world datasets, we show that Tgnn trained using loss based on proposed negative sampling provides consistent superior performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#30784;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#32479;&#19968;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09236</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#27010;&#24565;&#65306;&#32479;&#19968;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#19982;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#30784;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#32479;&#19968;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#26234;&#33021;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26377;&#20004;&#31181;&#24191;&#27867;&#30340;&#26041;&#27861;&#12290;&#19968;&#31181;&#26041;&#27861;&#26159;&#26500;&#24314;&#22825;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#21162;&#21147;&#26041;&#21521;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25237;&#20837;&#21162;&#21147;&#21435;&#29702;&#35299;&#23427;&#20204;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#26412;&#30740;&#31350;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32852;&#31995;&#36215;&#26469;&#65292;&#30740;&#31350;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#27010;&#24565;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#21487;&#20197;&#20174;&#22810;&#26679;&#30340;&#25968;&#25454;&#20013;&#34987;&#21487;&#38752;&#22320;&#24674;&#22797;&#20986;&#26469;&#12290;&#23545;&#20110;&#21512;&#25104;&#25968;&#25454;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#32479;&#19968;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09236v1 Announce Type: cross Abstract: To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#23618;&#27425;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#27773;&#36710;&#30896;&#25758;&#23433;&#20840;&#32467;&#26500;&#21160;&#21147;&#23398;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#21019;&#24314;&#19968;&#31995;&#21015;&#36866;&#24212;&#19981;&#21516;&#35745;&#31639;&#29615;&#22659;&#21644;&#20934;&#30830;&#24230;&#35201;&#27714;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#30896;&#25758;&#20223;&#30495;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09234</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27773;&#36710;&#30896;&#25758;&#23433;&#20840;&#32467;&#26500;&#21160;&#21147;&#23398;&#30340;&#22810;&#23618;&#27425;&#20195;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Hierarchical Surrogate Learning for Structural Dynamics of Automotive Crashworthiness Using Graph Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09234
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#23618;&#27425;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#27773;&#36710;&#30896;&#25758;&#23433;&#20840;&#32467;&#26500;&#21160;&#21147;&#23398;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#21019;&#24314;&#19968;&#31995;&#21015;&#36866;&#24212;&#19981;&#21516;&#35745;&#31639;&#29615;&#22659;&#21644;&#20934;&#30830;&#24230;&#35201;&#27714;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#30896;&#25758;&#20223;&#30495;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30896;&#25758;&#20223;&#30495;&#22312;&#25552;&#39640;&#36710;&#36742;&#23433;&#20840;&#24615;&#12289;&#35774;&#35745;&#20248;&#21270;&#21644;&#20260;&#23475;&#39118;&#38505;&#20272;&#35745;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#39640;&#20445;&#30495;&#27169;&#22411;&#36827;&#34892;&#36825;&#31867;&#38382;&#39064;&#30340;&#25968;&#20540;&#35299;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#24037;&#20316;&#12290;&#20256;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#36890;&#36807;&#21019;&#24314;&#20302;&#32500;&#23884;&#20837;&#26469;&#28436;&#21270;&#21160;&#21147;&#23398;&#65292;&#20197;&#35268;&#36991;&#36825;&#31181;&#35745;&#31639;&#24037;&#20316;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#30452;&#25509;&#22312;&#20174;&#25968;&#20540;&#31163;&#25955;&#21270;&#33719;&#21462;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#19978;&#25805;&#20316;&#65292;&#36825;&#26082;&#26114;&#36149;&#21448;&#22797;&#26434;&#65292;&#26080;&#27861;&#22312;&#22823;&#33539;&#22260;&#30340;&#31354;&#38388;&#36317;&#31163;&#19978;&#26144;&#23556;&#20449;&#24687;&#27969;&#21160;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22266;&#23450;&#20998;&#36776;&#29575;&#30340;&#26041;&#27861;&#38459;&#27490;&#20102;&#20195;&#29702;&#27169;&#22411;&#23545;&#35745;&#31639;&#33021;&#21147;&#29615;&#22659;&#12289;&#19981;&#21516;&#30340;&#21487;&#35270;&#21270;&#20998;&#36776;&#29575;&#21644;&#19981;&#21516;&#30340;&#31934;&#30830;&#24230;&#35201;&#27714;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#27425;&#26694;&#26550;&#65292;&#29992;&#20110;&#32467;&#26500;&#21270;&#22320;&#21019;&#24314;&#19968;&#31995;&#21015;&#29992;&#20110;&#21345;&#19969;&#36710;&#30896;&#25758;&#23433;&#20840;&#24615;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09234v1 Announce Type: new Abstract: Crash simulations play an essential role in improving vehicle safety, design optimization, and injury risk estimation. Unfortunately, numerical solutions of such problems using state-of-the-art high-fidelity models require significant computational effort. Conventional data-driven surrogate modeling approaches create low-dimensional embeddings for evolving the dynamics in order to circumvent this computational effort. Most approaches directly operate on high-resolution data obtained from numerical discretization, which is both costly and complicated for mapping the flow of information over large spatial distances. Furthermore, working with a fixed resolution prevents the adaptation of surrogate models to environments with variable computing capacities, different visualization resolutions, and different accuracy requirements. We thus propose a multi-hierarchical framework for structurally creating a series of surrogate models for a kart fr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#20840;&#34892;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#21151;&#33021;&#30340;Transformer&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#32452;&#21512;&#26041;&#27861;&#65292;&#35813;&#21151;&#33021;&#24050;&#22312;PyCharm Pro IDE&#19978;&#24471;&#21040;&#24212;&#29992;&#65292;&#24182;&#22312;&#23454;&#38469;&#30340;Python&#29992;&#25143;A/B&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09230</link><description>&lt;p&gt;
&#20840;&#34892;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#30340;&#19978;&#19979;&#25991;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Context Composing for Full Line Code Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#20840;&#34892;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#21151;&#33021;&#30340;Transformer&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#32452;&#21512;&#26041;&#27861;&#65292;&#35813;&#21151;&#33021;&#24050;&#22312;PyCharm Pro IDE&#19978;&#24471;&#21040;&#24212;&#29992;&#65292;&#24182;&#22312;&#23454;&#38469;&#30340;Python&#29992;&#25143;A/B&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#26159;&#26368;&#24120;&#29992;&#30340;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;&#65288;IDE&#65289;&#21151;&#33021;&#20043;&#19968;&#65292;&#23427;&#24433;&#21709;&#30528;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#30340;&#26085;&#24120;&#24037;&#20316;&#12290;&#29616;&#20195;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#26041;&#27861;&#24050;&#20174;&#22522;&#20110;&#38745;&#24577;&#20998;&#26512;&#30340;&#20960;&#20010;&#36129;&#29486;&#32773;&#30340;&#32452;&#21512;&#36716;&#21521;&#28041;&#21450;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#27700;&#32447;&#12290;&#36825;&#31181;&#25913;&#21464;&#20801;&#35768;&#22312;&#29983;&#25104;&#33258;&#36523;&#25152;&#33457;&#36153;&#30340;&#30456;&#23545;&#30701;&#26102;&#38388;&#20869;&#25552;&#20986;&#26356;&#38271;&#30340;&#20195;&#30721;&#24314;&#35758;&#12290;&#22312;JetBrains&#65292;&#25105;&#20204;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#31934;&#21147;&#26469;&#23436;&#21892;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#20415;&#23545;&#31243;&#24207;&#21592;&#26082;&#26377;&#24110;&#21161;&#21448;&#19981;&#20998;&#25955;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#25104;&#21151;&#23558;&#20840;&#34892;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#21151;&#33021;&#25512;&#20986;&#21040;PyCharm Pro IDE&#65292;&#24182;&#22312;&#25968;&#30334;&#21517;&#30495;&#23454;Python&#29992;&#25143;&#30340;A/B&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#29992;&#24615;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;Transformer&#27169;&#22411;&#19978;&#19979;&#25991;&#32452;&#21512;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#26159;&#35813;&#21151;&#33021;&#23454;&#29616;&#30340;&#26680;&#24515;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;&#25913;&#36827;&#35813;&#21151;&#33021;&#30340;&#19979;&#19968;&#27493;&#24037;&#20316;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09230v1 Announce Type: cross Abstract: Code Completion is one of the most used Integrated Development Environment (IDE) features, which affects the everyday life of a software developer. Modern code completion approaches moved from the composition of several static analysis-based contributors to pipelines that involve neural networks. This change allows the proposal of longer code suggestions while maintaining the relatively short time spent on generation itself. At JetBrains, we put a lot of effort into perfecting the code completion workflow so it can be both helpful and non-distracting for a programmer. We managed to ship the Full Line Code Completion feature to PyCharm Pro IDE and proved its usefulness in A/B testing on hundreds of real Python users. The paper describes our approach to context composing for the Transformer model that is a core of the feature's implementation. In addition to that, we share our next steps to improve the feature and emphasize the importance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#27425;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#22312;&#23567;&#21021;&#20540;&#38468;&#36817;&#30340;&#26799;&#24230;&#27969;&#21160;&#65292;&#21457;&#29616;&#26435;&#37325;&#20250;&#22312;&#26041;&#21521;&#19978;&#25910;&#25947;&#21040;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;KKT&#28857;&#21644;&#26576;&#20123;&#38797;&#28857;&#38468;&#36817;&#12290;</title><link>https://arxiv.org/abs/2402.09226</link><description>&lt;p&gt;
&#22312;&#20004;&#27425;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#30340;&#23567;&#21021;&#20540;&#21644;&#38797;&#28857;&#38468;&#36817;&#30340;&#26041;&#21521;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Directional Convergence Near Small Initializations and Saddles in Two-Homogeneous Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#27425;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#22312;&#23567;&#21021;&#20540;&#38468;&#36817;&#30340;&#26799;&#24230;&#27969;&#21160;&#65292;&#21457;&#29616;&#26435;&#37325;&#20250;&#22312;&#26041;&#21521;&#19978;&#25910;&#25947;&#21040;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;KKT&#28857;&#21644;&#26576;&#20123;&#38797;&#28857;&#38468;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#27425;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#22312;&#23567;&#21021;&#20540;&#38468;&#36817;&#30340;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#65292;&#20854;&#20013;&#25152;&#26377;&#26435;&#37325;&#37117;&#21021;&#22987;&#21270;&#22312;&#21407;&#28857;&#38468;&#36817;&#12290;&#38024;&#23545;&#24179;&#26041;&#35823;&#24046;&#21644;&#36923;&#36753;&#25439;&#22833;&#65292;&#35770;&#25991;&#35777;&#26126;&#65292;&#23545;&#20110;&#36275;&#22815;&#23567;&#30340;&#21021;&#22987;&#20540;&#65292;&#26799;&#24230;&#27969;&#21160;&#21160;&#24577;&#22312;&#21407;&#28857;&#38468;&#36817;&#33457;&#36153;&#36275;&#22815;&#30340;&#26102;&#38388;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21487;&#20197;&#36817;&#20284;&#22320;&#22312;&#26041;&#21521;&#19978;&#25910;&#25947;&#21040;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;Karush-Kuhn-Tucker&#65288;KKT&#65289;&#28857;&#65292;&#35813;&#20989;&#25968;&#37327;&#21270;&#20102;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30456;&#24212;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09226v1 Announce Type: new Abstract: This paper examines gradient flow dynamics of two-homogeneous neural networks for small initializations, where all weights are initialized near the origin. For both square and logistic losses, it is shown that for sufficiently small initializations, the gradient flow dynamics spend sufficient time in the neighborhood of the origin to allow the weights of the neural network to approximately converge in direction to the Karush-Kuhn-Tucker (KKT) points of a neural correlation function that quantifies the correlation between the output of the neural network and corresponding labels in the training data set. For square loss, it has been observed that neural networks undergo saddle-to-saddle dynamics when initialized close to the origin. Motivated by this, this paper also shows a similar directional convergence among weights of small magnitude in the neighborhood of certain saddle points.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#27604;KL PAC-Bayes&#30028;&#38480;&#26041;&#27861;&#26469;&#20272;&#35745;&#24207;&#21015;&#22343;&#20540;&#65292;&#24212;&#29992;&#20110;&#39044;&#27979;&#22120;&#27867;&#21270;&#35823;&#24046;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.09201</link><description>&lt;p&gt;
&#26356;&#22909;&#30340;&#27604;KL PAC-Bayes&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Better-than-KL PAC-Bayes Bounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#27604;KL PAC-Bayes&#30028;&#38480;&#26041;&#27861;&#26469;&#20272;&#35745;&#24207;&#21015;&#22343;&#20540;&#65292;&#24212;&#29992;&#20110;&#39044;&#27979;&#22120;&#27867;&#21270;&#35823;&#24046;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35753;$f(\theta, X_1),$ $ \dots,$ $ f(\theta, X_n)$&#25104;&#20026;&#19968;&#20010;&#38543;&#26426;&#20803;&#32032;&#24207;&#21015;&#65292;&#20854;&#20013;$f$&#26159;&#19968;&#20010;&#22266;&#23450;&#30340;&#26631;&#37327;&#20989;&#25968;&#65292;$X_1, \dots, X_n$&#26159;&#29420;&#31435;&#30340;&#38543;&#26426;&#21464;&#37327;&#65288;&#25968;&#25454;&#65289;&#65292;&#32780;$\theta$&#26159;&#26681;&#25454;&#19968;&#20123;&#25968;&#25454;&#30456;&#20851;&#30340;&#21518;&#39564;&#20998;&#24067;$P_n$&#20998;&#24067;&#30340;&#38543;&#26426;&#21442;&#25968;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#35777;&#26126;&#27987;&#24230;&#19981;&#31561;&#24335;&#26469;&#20272;&#35745;&#24207;&#21015;&#22343;&#20540;&#30340;&#38382;&#39064;&#12290;&#36825;&#26679;&#19968;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#20363;&#23376;&#26159;&#23545;&#26576;&#20123;&#36890;&#36807;&#38543;&#26426;&#31639;&#27861;&#35757;&#32451;&#30340;&#39044;&#27979;&#22120;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#20272;&#35745;&#65292;&#27604;&#22914;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;$f$&#26159;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20010;&#38382;&#39064;&#26159;&#36890;&#36807;PAC-Bayes&#20998;&#26512;&#26469;&#35299;&#20915;&#30340;&#65292;&#22312;&#36825;&#20010;&#20998;&#26512;&#20013;&#65292;&#38500;&#20102;&#21518;&#39564;&#20998;&#24067;&#65292;&#25105;&#20204;&#36824;&#36873;&#25321;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21040;&#23398;&#20064;&#38382;&#39064;&#24402;&#32435;&#20559;&#24046;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;PAC-Bayes&#27987;&#24230;&#30028;&#38480;&#20013;&#30340;&#20851;&#38190;&#25968;&#37327;&#26159;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21040;&#23398;&#20064;&#38382;&#39064;&#22797;&#26434;&#24615;&#30340;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09201v1 Announce Type: new Abstract: Let $f(\theta, X_1),$ $ \dots,$ $ f(\theta, X_n)$ be a sequence of random elements, where $f$ is a fixed scalar function, $X_1, \dots, X_n$ are independent random variables (data), and $\theta$ is a random parameter distributed according to some data-dependent posterior distribution $P_n$. In this paper, we consider the problem of proving concentration inequalities to estimate the mean of the sequence. An example of such a problem is the estimation of the generalization error of some predictor trained by a stochastic algorithm, such as a neural network where $f$ is a loss function. Classically, this problem is approached through a PAC-Bayes analysis where, in addition to the posterior, we choose a prior distribution which captures our belief about the inductive bias of the learning problem. Then, the key quantity in PAC-Bayes concentration bounds is a divergence that captures the complexity of the learning problem where the de facto stand
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20195;&#29702;&#24341;&#23548;&#30340;&#39640;&#25928;&#37325;&#37319;&#26679;&#26041;&#27861;&#26469;&#25913;&#36827;&#40657;&#30418;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#12290;&#36890;&#36807;&#20272;&#35745;&#21333;&#35789;&#29983;&#25104;&#27010;&#29575;&#20316;&#20026;&#20266;&#30333;&#30418;&#29305;&#24449;&#65292;&#36873;&#25321;&#23569;&#37327;&#20195;&#34920;&#24615;&#35789;&#27719;&#36827;&#34892;&#22810;&#27425;&#37325;&#37319;&#26679;&#65292;&#22312;&#21253;&#21547;&#20154;&#31867;&#25991;&#26412;&#21644;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09199</link><description>&lt;p&gt;
&#21313;&#20010;&#20851;&#38190;&#35789;&#20173;&#28982;&#26377;&#29992;&#65306;&#36890;&#36807;&#20195;&#29702;&#24341;&#23548;&#30340;&#39640;&#25928;&#37325;&#37319;&#26679;&#25913;&#36827;&#40657;&#30418;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20195;&#29702;&#24341;&#23548;&#30340;&#39640;&#25928;&#37325;&#37319;&#26679;&#26041;&#27861;&#26469;&#25913;&#36827;&#40657;&#30418;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#12290;&#36890;&#36807;&#20272;&#35745;&#21333;&#35789;&#29983;&#25104;&#27010;&#29575;&#20316;&#20026;&#20266;&#30333;&#30418;&#29305;&#24449;&#65292;&#36873;&#25321;&#23569;&#37327;&#20195;&#34920;&#24615;&#35789;&#27719;&#36827;&#34892;&#22810;&#27425;&#37325;&#37319;&#26679;&#65292;&#22312;&#21253;&#21547;&#20154;&#31867;&#25991;&#26412;&#21644;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#23427;&#20204;&#30340;&#28389;&#29992;&#24341;&#21457;&#20102;&#35768;&#22810;&#19981;&#21463;&#27426;&#36814;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#22914;&#20551;&#26032;&#38395;&#12289;&#23398;&#26415;&#19981;&#35802;&#23454;&#21644;&#20449;&#24687;&#27745;&#26579;&#12290;&#36825;&#20351;&#24471;AI&#29983;&#25104;&#25991;&#26412;&#65288;AIGT&#65289;&#30340;&#26816;&#27979;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#30333;&#30418;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#36890;&#24120;&#20248;&#20110;&#40657;&#30418;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#35775;&#38382;LLM&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#19981;&#36866;&#29992;&#20110;&#40657;&#30418;&#35774;&#32622;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#27425;&#37325;&#37319;&#26679;&#26469;&#20272;&#35745;&#21333;&#35789;&#29983;&#25104;&#27010;&#29575;&#20316;&#20026;&#20266;&#30333;&#30418;&#29305;&#24449;&#20197;&#24110;&#21161;&#25913;&#36827;&#40657;&#30418;AIGT&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;POGER&#65292;&#19968;&#31181;&#20195;&#29702;&#24341;&#23548;&#30340;&#39640;&#25928;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#22312;&#40657;&#30418;AIGT&#26816;&#27979;&#20013;&#36873;&#25321;&#19968;&#20010;&#23567;&#30340;&#20195;&#34920;&#24615;&#35789;&#27719;&#23376;&#38598;&#65288;&#20363;&#22914;10&#20010;&#35789;&#65289;&#65292;&#36827;&#34892;&#22810;&#27425;&#37325;&#37319;&#26679;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21253;&#21547;&#20154;&#31867;&#25991;&#26412;&#21644;&#19971;&#20010;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09199v1 Announce Type: cross Abstract: With the rapidly increasing application of large language models (LLMs), their abuse has caused many undesirable societal problems such as fake news, academic dishonesty, and information pollution. This makes AI-generated text (AIGT) detection of great importance. Among existing methods, white-box methods are generally superior to black-box methods in terms of performance and generalizability, but they require access to LLMs' internal states and are not applicable to black-box settings. In this paper, we propose to estimate word generation probabilities as pseudo white-box features via multiple re-sampling to help improve AIGT detection under the black-box setting. Specifically, we design POGER, a proxy-guided efficient re-sampling method, which selects a small subset of representative words (e.g., 10 words) for performing multiple re-sampling in black-box AIGT detection. Experiments on datasets containing texts from humans and seven LL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;GBDT&#30340;&#29305;&#24449;&#36129;&#29486;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#33410;&#28857;&#30340;&#27531;&#24046;&#26469;&#23454;&#29616;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;GBDT&#31639;&#27861;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#65292;&#20063;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.09197</link><description>&lt;p&gt;
&#22312;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#20013;&#23454;&#29616;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#65306;&#29305;&#24449;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
Implementing local-explainability in Gradient Boosting Trees: Feature Contribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;GBDT&#30340;&#29305;&#24449;&#36129;&#29486;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#33410;&#28857;&#30340;&#27531;&#24046;&#26469;&#23454;&#29616;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;GBDT&#31639;&#27861;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#65292;&#20063;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gradient Boosting Decision Trees (GBDT)&#26159;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24378;&#22823;&#30340;&#21152;&#27861;&#27169;&#22411;&#12290;&#23613;&#31649;&#26377;&#22810;&#20010;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;(XAI)&#27169;&#22411;&#36890;&#36807;&#37325;&#26032;&#35299;&#37322;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#27169;&#22411;&#26469;&#33719;&#21462;&#20449;&#24687;&#65292;&#20294;&#26159;GBDT&#30340;&#26412;&#36136;&#35753;&#23427;&#25104;&#20026;&#19968;&#20010;&#40657;&#30418;&#27169;&#22411;&#12290;&#38598;&#25104;&#20013;&#30340;&#27599;&#26869;&#26641;&#37117;&#26159;&#19968;&#20010;&#36879;&#26126;&#30340;&#27169;&#22411;&#65292;&#20294;&#26368;&#32456;&#30340;&#32467;&#26524;&#26159;&#36825;&#20123;&#26641;&#30340;&#24635;&#21644;&#65292;&#24456;&#38590;&#28548;&#28165;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;GBDT&#30340;&#29305;&#24449;&#36129;&#29486;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;GBDT&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#33410;&#28857;&#30340;&#27531;&#24046;&#26469;&#35745;&#31639;&#27599;&#20010;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#35745;&#31639;&#32473;&#23450;&#39044;&#27979;&#30340;&#33410;&#28857;&#20915;&#31574;&#24207;&#21015;&#12290;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#22810;&#20010;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#23427;&#19981;&#20165;&#26159;GBDT&#31639;&#27861;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#65292;&#20063;&#26159;&#21453;&#26144;GBDT&#30340;&#29420;&#29305;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09197v1 Announce Type: new Abstract: Gradient Boost Decision Trees (GBDT) is a powerful additive model based on tree ensembles. Its nature makes GBDT a black-box model even though there are multiple explainable artificial intelligence (XAI) models obtaining information by reinterpreting the model globally and locally. Each tree of the ensemble is a transparent model itself but the final outcome is the result of a sum of these trees and it is not easy to clarify.   In this paper, a feature contribution method for GBDT is developed. The proposed method takes advantage of the GBDT architecture to calculate the contribution of each feature using the residue of each node. This algorithm allows to calculate the sequence of node decisions given a prediction.   Theoretical proofs and multiple experiments have been carried out to demonstrate the performance of our method which is not only a local explicability model for the GBDT algorithm but also a unique option that reflects GBDTs 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#19981;&#21487;&#20449;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#22312;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#20013;&#35774;&#35745;&#24102;&#26377;&#21518;&#38376;&#25351;&#20196;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#25915;&#20987;&#32773;&#39044;&#26399;&#30340;&#32467;&#26524;&#12290;&#25915;&#20987;&#21253;&#25324;&#19977;&#20010;&#32423;&#21035;&#65292;&#19981;&#38656;&#35201;&#23545;&#21518;&#31471;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;</title><link>https://arxiv.org/abs/2402.09179</link><description>&lt;p&gt;
&#24555;&#36895;&#37319;&#29992;&#65292;&#38544;&#34255;&#39118;&#38505;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#21046;&#30340;&#21452;&#37325;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#19981;&#21487;&#20449;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#22312;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#20013;&#35774;&#35745;&#24102;&#26377;&#21518;&#38376;&#25351;&#20196;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#25915;&#20987;&#32773;&#39044;&#26399;&#30340;&#32467;&#26524;&#12290;&#25915;&#20987;&#21253;&#25324;&#19977;&#20010;&#32423;&#21035;&#65292;&#19981;&#38656;&#35201;&#23545;&#21518;&#31471;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#23450;&#21046;&#21270;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#23548;&#33268;&#20102;&#20687;GPT&#36825;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#20419;&#36827;&#23450;&#21046;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#24314;&#65292;&#26080;&#38656;&#32534;&#30721;&#12290;&#28982;&#32780;&#65292;&#31532;&#19977;&#26041;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#19982;&#19981;&#21487;&#20449;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#38598;&#25104;&#30340;&#24212;&#29992;&#30340;&#39318;&#20010;&#25351;&#20196;&#21518;&#38376;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20123;&#25915;&#20987;&#36890;&#36807;&#35774;&#35745;&#24102;&#26377;&#21518;&#38376;&#25351;&#20196;&#30340;&#25552;&#31034;&#65292;&#23558;&#21518;&#38376;&#23884;&#20837;&#21040;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29256;&#26412;&#20013;&#65292;&#24403;&#36755;&#20837;&#21253;&#21547;&#39044;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#26102;&#65292;&#36755;&#20986;&#25915;&#20987;&#32773;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#21253;&#25324;&#19977;&#20010;&#32423;&#21035;&#65306;&#21333;&#35789;&#32423;&#21035;&#12289;&#35821;&#27861;&#32423;&#21035;&#21644;&#35821;&#20041;&#32423;&#21035;&#65292;&#37319;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#20855;&#26377;&#36880;&#27493;&#38544;&#34109;&#24615;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#19981;&#38656;&#35201;&#23545;&#21518;&#31471;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#25110;&#20219;&#20309;&#20462;&#25913;&#65292;&#20005;&#26684;&#36981;&#24490;GPT&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09179v1 Announce Type: cross Abstract: The increasing demand for customized Large Language Models (LLMs) has led to the development of solutions like GPTs. These solutions facilitate tailored LLM creation via natural language prompts without coding. However, the trustworthiness of third-party custom versions of LLMs remains an essential concern. In this paper, we propose the first instruction backdoor attacks against applications integrated with untrusted customized LLMs (e.g., GPTs). Specifically, these attacks embed the backdoor into the custom version of LLMs by designing prompts with backdoor instructions, outputting the attacker's desired result when inputs contain the pre-defined triggers. Our attack includes 3 levels of attacks: word-level, syntax-level, and semantic-level, which adopt different types of triggers with progressive stealthiness. We stress that our attacks do not require fine-tuning or any modification to the backend LLMs, adhering strictly to GPTs devel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#24418;&#24335;&#8212;&#8212;&#19978;&#19979;&#25991;&#20132;&#20114;&#25915;&#20987;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#38382;&#31572;&#26469;&#24341;&#20986;&#26377;&#23475;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09177</link><description>&lt;p&gt;
&#20511;&#21161;&#22810;&#36718;&#20132;&#20114;&#21033;&#29992;&#19978;&#19979;&#25991;&#36827;&#34892;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#24418;&#24335;&#8212;&#8212;&#19978;&#19979;&#25991;&#20132;&#20114;&#25915;&#20987;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#38382;&#31572;&#26469;&#24341;&#20986;&#26377;&#23475;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36234;&#29425;&#25915;&#20987;&#36890;&#36807;&#24494;&#22937;&#22320;&#20462;&#25913;&#25915;&#20987;&#26597;&#35810;&#26469;&#25552;&#21462;&#26377;&#23475;&#20449;&#24687;&#12290;&#38543;&#30528;&#38450;&#24481;&#26426;&#21046;&#30340;&#36827;&#21270;&#65292;&#36234;&#29425;&#25915;&#20987;&#30452;&#25509;&#33719;&#21462;&#26377;&#23475;&#20449;&#24687;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#21463;&#21040;&#20154;&#31867;&#38388;&#25509;&#24341;&#20986;&#26377;&#23475;&#20449;&#24687;&#30340;&#23454;&#36341;&#21551;&#21457;&#65292;&#38024;&#23545;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#24418;&#24335;&#8212;&#8212;&#19978;&#19979;&#25991;&#20132;&#20114;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;LLMs&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#33258;&#22238;&#24402;&#24615;&#36136;&#12290;&#25105;&#20204;&#35748;&#20026;&#20808;&#21069;&#30340;&#19978;&#19979;&#25991;&#8212;&#8212;&#25915;&#20987;&#26597;&#35810;&#20043;&#21069;&#30340;&#20449;&#24687;&#22312;&#23454;&#29616;&#24378;&#22823;&#30340;&#36234;&#29425;&#25915;&#20987;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21021;&#27493;&#38382;&#31572;&#23545;&#19982;LLMs&#20132;&#20114;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#30340;&#22238;&#31572;&#26397;&#30528;&#25581;&#31034;&#8220;&#26399;&#26395;&#30340;&#8221;&#26377;&#23475;&#20449;&#24687;&#30340;&#26041;&#21521;&#21457;&#23637;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;LLMs&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09177v1 Announce Type: cross Abstract: Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which aim to extract harmful information by subtly modifying the attack query. As defense mechanisms evolve, directly obtaining harmful information becomes increasingly challenging for Jailbreaking attacks. In this work, inspired by human practices of indirect context to elicit harmful information, we focus on a new attack form called Contextual Interaction Attack. The idea relies on the autoregressive nature of the generation process in LLMs. We contend that the prior context--the information preceding the attack query--plays a pivotal role in enabling potent Jailbreaking attacks. Specifically, we propose an approach that leverages preliminary question-answer pairs to interact with the LLM. By doing so, we guide the responses of the model toward revealing the 'desired' harmful information. We conduct experiments on four different LLMs and demonstrate the efficacy of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22312;&#32447;&#20984;&#20248;&#21270;&#65292;&#24320;&#21457;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#20998;&#21035;&#38477;&#20302;&#20984;&#20989;&#25968;&#21644;&#24378;&#20984;&#20989;&#25968;&#30340;&#21518;&#24724;&#36793;&#30028;&#65292;&#24182;&#22635;&#34917;&#20102;&#29616;&#26377;&#19979;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.09173</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#24335;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#36817;&#20284;&#26368;&#20248;&#21518;&#24724;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nearly Optimal Regret for Decentralized Online Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22312;&#32447;&#20984;&#20248;&#21270;&#65292;&#24320;&#21457;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#20998;&#21035;&#38477;&#20302;&#20984;&#20989;&#25968;&#21644;&#24378;&#20984;&#20989;&#25968;&#30340;&#21518;&#24724;&#36793;&#30028;&#65292;&#24182;&#22635;&#34917;&#20102;&#29616;&#26377;&#19979;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22312;&#32447;&#20984;&#20248;&#21270;(D-OCO)&#65292;&#20854;&#20013;&#19968;&#32452;&#26412;&#22320;&#23398;&#20064;&#22120;&#38656;&#35201;&#20351;&#29992;&#20165;&#38480;&#20110;&#26412;&#22320;&#35745;&#31639;&#21644;&#36890;&#20449;&#30340;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#19968;&#31995;&#21015;&#20840;&#23616;&#25439;&#22833;&#20989;&#25968;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#20102;&#38024;&#23545;&#20984;&#20989;&#25968;&#21644;&#24378;&#20984;&#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#38480;&#20998;&#21035;&#20026;$O(n^{5/4}\rho^{-1/2}\sqrt{T})$&#21644;${O}(n^{3/2}\rho^{-1}\log T)$&#65292;&#20854;&#20013;$n$&#26159;&#26412;&#22320;&#23398;&#20064;&#22120;&#30340;&#25968;&#37327;&#65292;$\rho&lt;1$&#26159;&#36890;&#20449;&#30697;&#38453;&#30340;&#35889;&#38388;&#38553;&#65292;$T$&#26159;&#26102;&#38388;&#27573;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20984;&#20989;&#25968;&#23384;&#22312;&#30528;&#36739;&#22823;&#30340;&#38388;&#38553;&#65292;&#21363;&#20984;&#20989;&#25968;&#30340;&#19979;&#30028;&#20026;$\Omega(n\sqrt{T})$&#65292;&#24378;&#20984;&#20989;&#25968;&#30340;&#19979;&#30028;&#20026;$\Omega(n)$&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#38388;&#38553;&#65292;&#26412;&#25991;&#39318;&#20808;&#24320;&#21457;&#20102;&#26032;&#30340;D-OCO&#31639;&#27861;&#65292;&#23558;&#20984;&#20989;&#25968;&#21644;&#24378;&#20984;&#20989;&#25968;&#30340;&#21518;&#24724;&#36793;&#30028;&#20998;&#21035;&#38477;&#20302;&#21040;$\tilde{O}(n\rho^{-1/4}\sqrt{T})$&#21644;$\tilde{O}(n\rho^{-1/2}\log T)$&#12290;&#20027;&#35201;&#25216;&#26415;&#26159;&#35774;&#35745;&#19968;&#31181;&#22312;&#32447;&#21487;&#36827;&#21462;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09173v1 Announce Type: new Abstract: We investigate decentralized online convex optimization (D-OCO), in which a set of local learners are required to minimize a sequence of global loss functions using only local computations and communications. Previous studies have established $O(n^{5/4}\rho^{-1/2}\sqrt{T})$ and ${O}(n^{3/2}\rho^{-1}\log T)$ regret bounds for convex and strongly convex functions respectively, where $n$ is the number of local learners, $\rho&lt;1$ is the spectral gap of the communication matrix, and $T$ is the time horizon. However, there exist large gaps from the existing lower bounds, i.e., $\Omega(n\sqrt{T})$ for convex functions and $\Omega(n)$ for strongly convex functions. To fill these gaps, in this paper, we first develop novel D-OCO algorithms that can respectively reduce the regret bounds for convex and strongly convex functions to $\tilde{O}(n\rho^{-1/4}\sqrt{T})$ and $\tilde{O}(n\rho^{-1/2}\log T)$. The primary technique is to design an online acce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#36827;&#21270;&#38480;&#21046;&#29627;&#23572;&#20857;&#26364;&#26426;&#21644;Kohonen&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#22312;&#32447;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;ERBM&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#21644;&#20351;&#29992;KNet&#26356;&#26032;&#32858;&#31867;&#20013;&#24515;&#65292;&#26377;&#25928;&#22320;&#20811;&#26381;&#20102;&#32858;&#31867;&#31639;&#27861;&#30340;&#24120;&#35265;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09167</link><description>&lt;p&gt;
&#22312;&#32447;&#32858;&#31867;&#30340;&#36827;&#21270;&#38480;&#21046;&#29627;&#23572;&#20857;&#26364;&#26426;-Kohonen&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Evolving Restricted Boltzmann Machine-Kohonen Network for Online Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#36827;&#21270;&#38480;&#21046;&#29627;&#23572;&#20857;&#26364;&#26426;&#21644;Kohonen&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#22312;&#32447;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;ERBM&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#21644;&#20351;&#29992;KNet&#26356;&#26032;&#32858;&#31867;&#20013;&#24515;&#65292;&#26377;&#25928;&#22320;&#20811;&#26381;&#20102;&#32858;&#31867;&#31639;&#27861;&#30340;&#24120;&#35265;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#32858;&#31867;&#31639;&#27861;&#65292;&#20854;&#20013;&#23558;&#36827;&#21270;&#38480;&#21046;&#29627;&#23572;&#20857;&#26364;&#26426;&#65288;ERBM&#65289;&#19982;Kohonen&#32593;&#32476;&#65288;ERBM-KNet&#65289;&#30456;&#32467;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;ERBM-KNet&#21033;&#29992;ERBM&#20197;&#21333;&#27425;&#20256;&#36882;&#27169;&#24335;&#39640;&#25928;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#65292;&#21033;&#29992;&#20559;&#24046;-&#26041;&#24046;&#31574;&#30053;&#36827;&#34892;&#31070;&#32463;&#20803;&#30340;&#29983;&#38271;&#21644;&#20462;&#21098;&#65292;&#20197;&#21450;&#22522;&#20110;&#32858;&#31867;&#26356;&#26032;&#31574;&#30053;&#36827;&#34892;&#22312;&#32447;&#32858;&#31867;&#21644;&#20351;&#29992;KNet&#36827;&#34892;&#32858;&#31867;&#20013;&#24515;&#26356;&#26032;&#12290;&#21021;&#22987;&#38454;&#27573;&#65292;ERBM&#22312;&#22788;&#29702;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#25968;&#25454;&#26102;&#28436;&#21270;&#20854;&#20307;&#31995;&#32467;&#26500;&#65292;&#26377;&#25928;&#22320;&#35299;&#24320;&#20102;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#38543;&#21518;&#65292;KNet&#21033;&#29992;&#20174;ERBM&#25552;&#21462;&#30340;&#29305;&#24449;&#26469;&#39044;&#27979;&#32858;&#31867;&#25968;&#37327;&#24182;&#26356;&#26032;&#32858;&#31867;&#20013;&#24515;&#12290;&#36890;&#36807;&#20811;&#26381;&#32858;&#31867;&#31639;&#27861;&#24120;&#35265;&#30340;&#25361;&#25112;&#65292;&#22914;&#32858;&#31867;&#25968;&#37327;&#30340;&#20808;&#39564;&#21021;&#22987;&#21270;&#21644;&#32858;&#31867;&#20934;&#30830;&#24230;&#20302;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;ERBM-KNet&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09167v1 Announce Type: new Abstract: A novel online clustering algorithm is presented where an Evolving Restricted Boltzmann Machine (ERBM) is embedded with a Kohonen Network called ERBM-KNet. The proposed ERBM-KNet efficiently handles streaming data in a single-pass mode using the ERBM, employing a bias-variance strategy for neuron growing and pruning, as well as online clustering based on a cluster update strategy for cluster prediction and cluster center update using KNet. Initially, ERBM evolves its architecture while processing unlabeled image data, effectively disentangling the data distribution in the latent space. Subsequently, the KNet utilizes the feature extracted from ERBM to predict the number of clusters and updates the cluster centers. By overcoming the common challenges associated with clustering algorithms, such as prior initialization of the number of clusters and subpar clustering accuracy, the proposed ERBM-KNet offers significant improvements. Extensive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#32544;&#32455;&#28151;&#21512;&#31163;&#25955;&#26356;&#26032;Markov&#38142;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24809;&#32602;&#20284;&#28982;&#20998;&#25968;&#26469;&#24674;&#22797;&#30495;&#23454;&#30340;&#31526;&#21495;&#20998;&#32452;&#65292;&#24182;&#22312;&#30005;&#23376;&#25903;&#25588;&#25514;&#26045;&#20013;&#30340;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09166</link><description>&lt;p&gt;
&#28151;&#21512;&#31163;&#25955;&#26356;&#26032;&#36807;&#31243;&#30340;&#35299;&#32544;&#32455;&#26041;&#27861;&#21450;&#20854;&#22312;&#30005;&#23376;&#25903;&#25588;&#25514;&#26045;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deinterleaving of Discrete Renewal Process Mixtures with Application to Electronic Support Measures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#32544;&#32455;&#28151;&#21512;&#31163;&#25955;&#26356;&#26032;Markov&#38142;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24809;&#32602;&#20284;&#28982;&#20998;&#25968;&#26469;&#24674;&#22797;&#30495;&#23454;&#30340;&#31526;&#21495;&#20998;&#32452;&#65292;&#24182;&#22312;&#30005;&#23376;&#25903;&#25588;&#25514;&#26045;&#20013;&#30340;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28151;&#21512;&#31163;&#25955;&#26356;&#26032;Markov&#38142;&#30340;&#26032;&#30340;&#35299;&#32544;&#32455;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#24809;&#32602;&#20284;&#28982;&#20998;&#25968;&#30340;&#26368;&#22823;&#21270;&#12290;&#23427;&#21033;&#29992;&#20102;&#26377;&#20851;&#19981;&#21516;&#31526;&#21495;&#24207;&#21015;&#21450;&#20854;&#21040;&#36798;&#26102;&#38388;&#30340;&#25152;&#26377;&#21487;&#29992;&#20449;&#24687;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;&#22312;&#32452;&#25104;&#36807;&#31243;&#19978;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#23567;&#21270;&#35813;&#20998;&#25968;&#33021;&#22815;&#22312;&#22823;&#26679;&#26412;&#38480;&#21046;&#19979;&#24674;&#22797;&#20986;&#30495;&#23454;&#30340;&#31526;&#21495;&#20998;&#32452;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#29702;&#35770;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20174;RESM&#65288;&#38647;&#36798;&#30005;&#23376;&#25903;&#25588;&#27979;&#37327;&#65289;&#29615;&#22659;&#20013;&#25509;&#25910;&#21040;&#30340;&#19981;&#21516;&#21457;&#23556;&#26426;&#30340;&#33033;&#20914;&#20018;&#35299;&#32544;&#32455;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#25112;&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09166v1 Announce Type: new Abstract: In this paper, we propose a new deinterleaving method for mixtures of discrete renewal Markov chains. This method relies on the maximization of a penalized likelihood score. It exploits all available information about both the sequence of the different symbols and their arrival times. A theoretical analysis is carried out to prove that minimizing this score allows to recover the true partition of symbols in the large sample limit, under mild conditions on the component processes. This theoretical analysis is then validated by experiments on synthetic data. Finally, the method is applied to deinterleave pulse trains received from different emitters in a RESM (Radar Electronic Support Measurements) context and we show that the proposed method competes favorably with state-of-the-art methods on simulated warfare datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550; PNSIS&#65292;&#36890;&#36807;&#32479;&#19968;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#27010;&#29575;&#65292;&#25552;&#21462;&#19981;&#21464;&#30340;&#23376;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#20266;&#29616;&#35937;&#23376;&#22270;&#25552;&#21319;&#25512;&#24191;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09165</link><description>&lt;p&gt;
&#36890;&#36807;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#27010;&#29575;&#32479;&#19968;&#19981;&#21464;&#24615;&#21644;&#20266;&#29616;&#35937;&#65292;&#29992;&#20110;&#22270;&#24418;&#31163;&#25955;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unifying Invariance and Spuriousity for Graph Out-of-Distribution via Probability of Necessity and Sufficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09165
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550; PNSIS&#65292;&#36890;&#36807;&#32479;&#19968;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#27010;&#29575;&#65292;&#25552;&#21462;&#19981;&#21464;&#30340;&#23376;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#20266;&#29616;&#35937;&#23376;&#22270;&#25552;&#21319;&#25512;&#24191;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#31163;&#25955;&#20998;&#26512;&#65288;Graph Out-of-Distribution&#65292;OOD&#65289;&#35201;&#27714;&#22312;&#20559;&#20506;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#22823;&#37327;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20854;&#20013;&#19968;&#31181;&#20027;&#27969;&#26041;&#27861;&#26159;&#36890;&#36807;&#29615;&#22659;&#22686;&#24378;&#26469;&#23545;&#40784;&#21407;&#22987;&#25968;&#25454;&#21644;&#22686;&#24378;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21462;&#19981;&#21464;&#30340;&#23376;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#35821;&#20041;&#23376;&#22270;&#30340;&#20002;&#22833;&#25110;&#20887;&#20313;&#65292;&#36827;&#32780;&#23548;&#33268;&#23376;&#20248;&#21270;&#30340;&#25512;&#24191;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#21033;&#29992;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#27010;&#29575;&#26469;&#25552;&#21462;&#19981;&#21464;&#30340;&#23376;&#32467;&#26500;&#65288;PNSIS&#65289;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#35813;&#26694;&#26550;&#36824;&#20197;&#38598;&#25104;&#30340;&#26041;&#24335;&#21033;&#29992;&#20266;&#29616;&#35937;&#23376;&#22270;&#26469;&#25552;&#21319;&#25512;&#24191;&#24615;&#33021;&#65292;&#20197;&#22686;&#24378;&#23545;&#22122;&#22768;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;&#22270;&#24418;&#25968;&#25454;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09165v1 Announce Type: new Abstract: Graph Out-of-Distribution (OOD), requiring that models trained on biased data generalize to the unseen test data, has a massive of real-world applications. One of the most mainstream methods is to extract the invariant subgraph by aligning the original and augmented data with the help of environment augmentation. However, these solutions might lead to the loss or redundancy of semantic subgraph and further result in suboptimal generalization. To address this challenge, we propose a unified framework to exploit the Probability of Necessity and Sufficiency to extract the Invariant Substructure (PNSIS). Beyond that, this framework further leverages the spurious subgraph to boost the generalization performance in an ensemble manner to enhance the robustness on the noise data. Specificially, we first consider the data generation process for graph data. Under mild conditions, we show that the invariant subgraph can be extracted by minimizing an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#22270;&#20687;&#24402;&#23646;&#38382;&#39064;&#37325;&#26032;&#24314;&#27169;&#20026;&#27425;&#27169;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#23569;&#30340;&#21306;&#22495;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24402;&#23646;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30340;&#19981;&#20934;&#30830;&#21306;&#22495;&#21644;&#39044;&#27979;&#38169;&#35823;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09164</link><description>&lt;p&gt;
&#31616;&#32422;&#21363;&#26159;&#32654;&#65306;&#36890;&#36807;&#27425;&#27169;&#23376;&#38598;&#36873;&#25321;&#20943;&#23569;&#21487;&#35299;&#37322;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Less is More: Fewer Interpretable Region via Submodular Subset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#22270;&#20687;&#24402;&#23646;&#38382;&#39064;&#37325;&#26032;&#24314;&#27169;&#20026;&#27425;&#27169;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#23569;&#30340;&#21306;&#22495;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24402;&#23646;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30340;&#19981;&#20934;&#30830;&#21306;&#22495;&#21644;&#39044;&#27979;&#38169;&#35823;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24402;&#23646;&#31639;&#27861;&#26088;&#22312;&#30830;&#23450;&#19982;&#27169;&#22411;&#20915;&#31574;&#39640;&#24230;&#30456;&#20851;&#30340;&#37325;&#35201;&#21306;&#22495;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#24402;&#23646;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#26377;&#25928;&#22320;&#32473;&#30446;&#26631;&#20803;&#32032;&#20998;&#37197;&#37325;&#35201;&#24615;&#65292;&#20294;&#20173;&#38754;&#20020;&#20197;&#19979;&#25361;&#25112;&#65306;1&#65289;&#29616;&#26377;&#30340;&#24402;&#23646;&#26041;&#27861;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#23567;&#21306;&#22495;&#65292;&#20174;&#32780;&#35823;&#23548;&#27491;&#30830;&#24402;&#23646;&#30340;&#26041;&#21521;&#65307;2&#65289;&#27169;&#22411;&#26080;&#27861;&#20026;&#39044;&#27979;&#38169;&#35823;&#30340;&#26679;&#26412;&#20135;&#29983;&#33391;&#22909;&#30340;&#24402;&#23646;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#23558;&#19978;&#36848;&#22270;&#20687;&#24402;&#23646;&#38382;&#39064;&#37325;&#26032;&#24314;&#27169;&#20026;&#27425;&#27169;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#26088;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#21306;&#22495;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;&#23616;&#37096;&#21306;&#22495;&#30340;&#20851;&#27880;&#19981;&#36275;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#26032;&#30340;&#27425;&#27169;&#20989;&#25968;&#26469;&#21457;&#29616;&#26356;&#20934;&#30830;&#30340;&#31934;&#32454;&#35299;&#37322;&#21306;&#22495;&#12290;&#20026;&#20102;&#22686;&#24378;&#25152;&#26377;&#26679;&#26412;&#30340;&#24402;&#23646;&#25928;&#26524;&#65292;&#25105;&#20204;&#36824;&#23545;&#23376;&#21306;&#22495;&#36873;&#25321;&#26045;&#21152;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#32422;&#26463;&#65292;&#21363;&#32622;&#20449;&#24230;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09164v1 Announce Type: cross Abstract: Image attribution algorithms aim to identify important regions that are highly relevant to model decisions. Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate small regions thus misleading the direction of correct attribution, and 2) the model cannot produce good attribution results for samples with wrong predictions. To address the above challenges, this paper re-models the above image attribution problem as a submodular subset selection problem, aiming to enhance model interpretability using fewer regions. To address the lack of attention to local regions, we construct a novel submodular function to discover more accurate fine-grained interpretation regions. To enhance the attribution effect for all samples, we also impose four different constraints on the selection of sub-regions, i.e., confidence, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#20197;&#36830;&#32493;&#26494;&#24347;&#30340;&#36755;&#20837;&#25552;&#31034;&#26469;&#25915;&#20987;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#31163;&#25955;&#20248;&#21270;&#26356;&#24555;&#30340;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#30456;&#21516;&#30340;&#27585;&#28781;&#24615;&#25915;&#20987;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09154</link><description>&lt;p&gt;
&#29992;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#25915;&#20987;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Attacking Large Language Models with Projected Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#20197;&#36830;&#32493;&#26494;&#24347;&#30340;&#36755;&#20837;&#25552;&#31034;&#26469;&#25915;&#20987;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#31163;&#25955;&#20248;&#21270;&#26356;&#24555;&#30340;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#30456;&#21516;&#30340;&#27585;&#28781;&#24615;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29305;&#23450;&#35774;&#35745;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#24456;&#23481;&#26131;&#34987;&#30772;&#35299;&#12290;&#34429;&#28982;&#20351;&#29992;&#31163;&#25955;&#20248;&#21270;&#21046;&#20316;&#23545;&#25239;&#24615;&#25552;&#31034;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#36825;&#31181;&#25915;&#20987;&#36890;&#24120;&#38656;&#35201;&#36229;&#36807;100,000&#27425;&#30340;&#35821;&#35328;&#27169;&#22411;&#35843;&#29992;&#12290;&#36825;&#31181;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#23450;&#37327;&#20998;&#26512;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#23545;&#36830;&#32493;&#26494;&#24347;&#30340;&#36755;&#20837;&#25552;&#31034;&#20351;&#29992;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;PGD&#65289;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#20808;&#21069;&#20351;&#29992;&#26222;&#36890;&#26799;&#24230;&#25915;&#20987;&#30340;&#23581;&#35797;&#22522;&#26412;&#22833;&#36133;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#20180;&#32454;&#25511;&#21046;&#36830;&#32493;&#26494;&#24347;&#24341;&#20837;&#30340;&#35823;&#24046;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#23427;&#20204;&#30340;&#25928;&#21147;&#12290;&#25105;&#20204;&#30340;LLMs&#30340;PGD&#36895;&#24230;&#27604;&#26368;&#20808;&#36827;&#30340;&#31163;&#25955;&#20248;&#21270;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#20197;&#36798;&#21040;&#30456;&#21516;&#30340;&#27585;&#28781;&#24615;&#25915;&#20987;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09154v1 Announce Type: new Abstract: Current LLM alignment methods are readily broken through specifically crafted adversarial prompts. While crafting adversarial prompts using discrete optimization is highly effective, such attacks typically use more than 100,000 LLM calls. This high computational cost makes them unsuitable for, e.g., quantitative analyses and adversarial training. To remedy this, we revisit Projected Gradient Descent (PGD) on the continuously relaxed input prompt. Although previous attempts with ordinary gradient-based attacks largely failed, we show that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one order of magnitude faster than state-of-the-art discrete optimization to achieve the same devastating attack results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#24310;&#36831;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#21518;&#24724;&#29575;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#21033;&#29992;&#24310;&#36831;&#30340;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#65292;&#25104;&#21151;&#23558;&#21518;&#24724;&#30028;&#38480;&#20174;$O(T^{3/4}+d^{1/3}T^{2/3})$&#25913;&#36827;&#20026;$O(T^{3/4}+\sqrt{dT})$&#65292;&#24182;&#22312;&#26356;&#22823;&#30340;&#24310;&#36831;&#37327;$d=O(\sqrt{T})$&#24773;&#20917;&#19979;&#19982;&#38750;&#24310;&#36831;&#35774;&#32622;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30456;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.09152</link><description>&lt;p&gt;
&#25913;&#36827;&#20102;&#20855;&#26377;&#24310;&#36831;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#21518;&#24724;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improved Regret for Bandit Convex Optimization with Delayed Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#24310;&#36831;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#21518;&#24724;&#29575;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#21033;&#29992;&#24310;&#36831;&#30340;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#65292;&#25104;&#21151;&#23558;&#21518;&#24724;&#30028;&#38480;&#20174;$O(T^{3/4}+d^{1/3}T^{2/3})$&#25913;&#36827;&#20026;$O(T^{3/4}+\sqrt{dT})$&#65292;&#24182;&#22312;&#26356;&#22823;&#30340;&#24310;&#36831;&#37327;$d=O(\sqrt{T})$&#24773;&#20917;&#19979;&#19982;&#38750;&#24310;&#36831;&#35774;&#32622;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#24310;&#36831;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#21482;&#26377;&#22312;&#20219;&#24847;&#24310;&#36831;&#19979;&#25165;&#20250;&#26174;&#31034;&#21160;&#20316;&#30340;&#25439;&#22833;&#20540;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#31616;&#21333;&#22320;&#23558;&#24310;&#36831;&#30340;&#25439;&#22833;&#20540;&#36755;&#20837;&#21040;&#32463;&#20856;&#30340;&#24378;&#21270;&#23398;&#20064;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;&#35813;&#38382;&#39064;&#30340;&#21518;&#24724;&#30028;&#38480;&#20026;$O(T^{3/4}+d^{1/3}T^{2/3})$&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26469;&#25552;&#39640;&#21518;&#24724;&#29575;&#65292;&#36890;&#36807;&#19968;&#20010;&#38459;&#22622;&#26356;&#26032;&#26426;&#21046;&#31934;&#30830;&#21033;&#29992;&#24310;&#36831;&#30340;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#39318;&#20808;&#25581;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#20998;&#31163;&#24310;&#36831;&#21644;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#23545;&#21518;&#24724;&#29575;&#30340;&#32852;&#21512;&#24433;&#21709;&#65292;&#24182;&#23558;&#20984;&#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#38480;&#25913;&#36827;&#20026;$O(T^{3/4}+\sqrt{dT})$&#12290;&#19982;&#20808;&#21069;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;&#22312;&#26356;&#22823;&#30340;&#24310;&#36831;&#37327;$d=O(\sqrt{T})$&#65292;&#32780;&#19981;&#26159;$d=O(T^{1/4})$&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#21518;&#24724;&#19982;&#38750;&#24310;&#36831;&#35774;&#32622;&#19979;&#24378;&#21270;&#23398;&#20064;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;$O(T^{3/4})$&#21518;&#24724;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09152v1 Announce Type: new Abstract: We investigate bandit convex optimization (BCO) with delayed feedback, where only the loss value of the action is revealed under an arbitrary delay. Previous studies have established a regret bound of $O(T^{3/4}+d^{1/3}T^{2/3})$ for this problem, where $d$ is the maximum delay, by simply feeding delayed loss values to the classical bandit gradient descent (BGD) algorithm. In this paper, we develop a novel algorithm to enhance the regret, which carefully exploits the delayed bandit feedback via a blocking update mechanism. Our analysis first reveals that the proposed algorithm can decouple the joint effect of the delays and bandit feedback on the regret, and improve the regret bound to $O(T^{3/4}+\sqrt{dT})$ for convex functions. Compared with the previous result, our regret matches the $O(T^{3/4})$ regret of BGD in the non-delayed setting for a larger amount of delay, i.e., $d=O(\sqrt{T})$, instead of $d=O(T^{1/4})$. Furthermore, we consi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;Chinese MentalBERT&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#24515;&#29702;&#20581;&#24247;&#25991;&#26412;&#20998;&#26512;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#20837;&#24515;&#29702;&#23398;&#35789;&#20856;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09151</link><description>&lt;p&gt;
Chinese MentalBERT: &#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#38024;&#23545;&#20013;&#22269;&#24515;&#29702;&#20581;&#24247;&#25991;&#26412;&#20998;&#26512;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;Chinese MentalBERT&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#24515;&#29702;&#20581;&#24247;&#25991;&#26412;&#20998;&#26512;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#20837;&#24515;&#29702;&#23398;&#35789;&#20856;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#31038;&#20132;&#23186;&#20307;&#30340;&#24433;&#21709;&#65292;&#24515;&#29702;&#38382;&#39064;&#22312;&#24403;&#21069;&#29615;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#31038;&#20132;&#23186;&#20307;&#25104;&#20026;&#20010;&#20154;&#20998;&#20139;&#24863;&#21463;&#30340;&#37325;&#35201;&#20986;&#21475;&#12290;&#36825;&#23548;&#33268;&#27599;&#22825;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#20854;&#20013;&#36127;&#38754;&#24773;&#32490;&#26377;&#28508;&#21147;&#24341;&#21457;&#21361;&#26426;&#12290;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#20986;&#33021;&#22815;&#39640;&#25928;&#20998;&#26512;&#36825;&#20123;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#26174;&#31034;&#20986;&#25928;&#26524;&#65292;&#20294;&#38024;&#23545;&#24515;&#29702;&#23398;&#31561;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23384;&#22312;&#26126;&#26174;&#32570;&#22833;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25910;&#38598;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#24182;&#20016;&#23500;&#20102;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;336&#19975;&#26465;&#25991;&#26412;&#26465;&#30446;&#30340;&#32508;&#21512;&#25968;&#25454;&#24211;&#12290;&#20026;&#25552;&#39640;&#27169;&#22411;&#22312;&#24515;&#29702;&#25991;&#26412;&#20998;&#26512;&#19978;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#24515;&#29702;&#23398;&#35789;&#20856;&#34701;&#20837;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#26426;&#21046;&#12290;&#22312;&#29616;&#26377;&#30340;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#19978;&#36827;&#34892;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09151v1 Announce Type: new Abstract: In the current environment, psychological issues are prevalent and widespread, with social media serving as a key outlet for individuals to share their feelings. This results in the generation of vast quantities of data daily, where negative emotions have the potential to precipitate crisis situations. There is a recognized need for models capable of efficient analysis. While pre-trained language models have demonstrated their effectiveness broadly, there's a noticeable gap in pre-trained models tailored for specialized domains like psychology. To address this, we have collected a huge dataset from Chinese social media platforms and enriched it with publicly available datasets to create a comprehensive database encompassing 3.36 million text entries. To enhance the model's applicability to psychological text analysis, we integrated psychological lexicons into the pre-training masking mechanism. Building on an existing Chinese language mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#26032;&#26694;&#26550;ResQuNNs&#65292;&#22312;quanvolutional&#23618;&#20013;&#24341;&#20837;&#21487;&#35757;&#32451;&#24615;&#65292;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#30340;&#27010;&#24565;&#35299;&#20915;&#20102;&#36328;&#23618;&#26799;&#24230;&#35775;&#38382;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09146</link><description>&lt;p&gt;
ResQuNNs: &#23454;&#29616;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#26032;&#26694;&#26550;ResQuNNs&#65292;&#22312;quanvolutional&#23618;&#20013;&#24341;&#20837;&#21487;&#35757;&#32451;&#24615;&#65292;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#30340;&#27010;&#24565;&#35299;&#20915;&#20102;&#36328;&#23618;&#26799;&#24230;&#35775;&#38382;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QuNNs&#65289;&#24615;&#33021;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;quanvolutional&#23618;&#24182;&#35299;&#20915;&#19982;&#20854;&#30456;&#20851;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;quanvolutional&#23618;&#34429;&#28982;&#26377;&#21161;&#20110;&#29305;&#24449;&#25552;&#21462;&#65292;&#20294;&#24448;&#24448;&#26159;&#38745;&#24577;&#30340;&#65292;&#36866;&#24212;&#24615;&#26377;&#38480;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#36825;&#20123;&#23618;&#20869;&#37096;&#36827;&#34892;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;QuNNs&#30340;&#28789;&#27963;&#24615;&#21644;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22810;&#20010;&#21487;&#35757;&#32451;&#30340;quanvolutional&#23618;&#30340;&#24341;&#20837;&#32473;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#24102;&#26469;&#20102;&#22797;&#26434;&#24615;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#38590;&#20197;&#22312;&#36825;&#20123;&#23618;&#20043;&#38388;&#35775;&#38382;&#26799;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;Residual Quanvolutional Neural Networks (ResQuNNs)&#65292;&#21033;&#29992;&#27531;&#24046;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#22312;&#36825;&#20123;&#23618;&#20043;&#38388;&#28155;&#21152;&#36339;&#36807;&#36830;&#25509;&#20197;&#20419;&#36827;&#26799;&#24230;&#30340;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09146v1 Announce Type: new Abstract: In this paper, we present a novel framework for enhancing the performance of Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional layers and addressing the critical challenges associated with them. Traditional quanvolutional layers, although beneficial for feature extraction, have largely been static, offering limited adaptability. Unlike state-of-the-art, our research overcomes this limitation by enabling training within these layers, significantly increasing the flexibility and potential of QuNNs. However, the introduction of multiple trainable quanvolutional layers induces complexities in gradient-based optimization, primarily due to the difficulty in accessing gradients across these layers. To resolve this, we propose a novel architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging the concept of residual learning, which facilitates the flow of gradients by adding skip connections between 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#20551;&#35774;&#20102;&#32534;&#30721;&#26144;&#23556;&#21644;&#35299;&#30721;&#26144;&#23556;&#20026;&#20219;&#24847;&#20809;&#28369;&#20989;&#25968;&#65292;&#24182;&#19988;&#33021;&#22815;&#25551;&#36848;&#22797;&#26434;&#19988;&#22823;&#22411;&#26550;&#26500;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>https://arxiv.org/abs/2402.09142</link><description>&lt;p&gt;
&#24403;&#34920;&#31034;&#23545;&#40784;&#26102;&#65306;&#34920;&#31034;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#30340;&#26222;&#36941;&#24615;
&lt;/p&gt;
&lt;p&gt;
When Representations Align: Universality in Representation Learning Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#20551;&#35774;&#20102;&#32534;&#30721;&#26144;&#23556;&#21644;&#35299;&#30721;&#26144;&#23556;&#20026;&#20219;&#24847;&#20809;&#28369;&#20989;&#25968;&#65292;&#24182;&#19988;&#33021;&#22815;&#25551;&#36848;&#22797;&#26434;&#19988;&#22823;&#22411;&#26550;&#26500;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26377;&#35768;&#22810;&#19981;&#21516;&#30340;&#22823;&#23567;&#21644;&#32467;&#26500;&#12290;&#26550;&#26500;&#30340;&#36873;&#25321;&#65292;&#32467;&#21512;&#25968;&#25454;&#38598;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#26222;&#36941;&#35748;&#20026;&#24433;&#21709;&#20102;&#23398;&#20064;&#21040;&#30340;&#31070;&#32463;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#19981;&#21516;&#30340;&#26550;&#26500;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20855;&#26377;&#24778;&#20154;&#30340;&#23450;&#24615;&#30456;&#20284;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#23558;&#36755;&#20837;&#21040;&#38544;&#34255;&#34920;&#31034;&#30340;&#32534;&#30721;&#26144;&#23556;&#21644;&#20174;&#34920;&#31034;&#21040;&#36755;&#20986;&#30340;&#35299;&#30721;&#26144;&#23556;&#37117;&#26159;&#20219;&#24847;&#20809;&#28369;&#20989;&#25968;&#30340;&#20551;&#35774;&#19979;&#65292;&#25512;&#23548;&#20102;&#34920;&#31034;&#23398;&#20064;&#30340;&#26377;&#25928;&#29702;&#35770;&#12290;&#22312;&#22797;&#26434;&#21644;&#22823;&#22411;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#38544;&#34255;&#34920;&#31034;&#27809;&#26377;&#34987;&#21442;&#25968;&#21270;&#24378;&#32422;&#26463;&#65292;&#35813;&#29702;&#35770;&#27010;&#25324;&#20102;&#34920;&#31034;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20010;&#26377;&#25928;&#29702;&#35770;&#25551;&#36848;&#20102;&#20855;&#26377;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#21644;&#26550;&#26500;&#30340;&#28145;&#24230;&#32593;&#32476;&#20013;&#34920;&#31034;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#19968;&#20123;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09142v1 Announce Type: new Abstract: Deep neural networks come in many sizes and architectures. The choice of architecture, in conjunction with the dataset and learning algorithm, is commonly understood to affect the learned neural representations. Yet, recent results have shown that different architectures learn representations with striking qualitative similarities. Here we derive an effective theory of representation learning under the assumption that the encoding map from input to hidden representation and the decoding map from representation to output are arbitrary smooth functions. This theory schematizes representation learning dynamics in the regime of complex, large architectures, where hidden representations are not strongly constrained by the parametrization. We show through experiments that the effective theory describes aspects of representation learning dynamics across a range of deep networks with different activation functions and architectures, and exhibits 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#39640;&#38750;&#32447;&#24615;&#27874;&#23548;&#20013;&#30340;&#22235;&#27874;&#28151;&#39057;&#25928;&#24212;&#30340;&#20809;&#23398;&#38750;&#24120;&#35268;&#21152;&#36895;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#30452;&#25509;&#22312;&#20809;&#23398;&#39046;&#22495;&#20869;&#36827;&#34892;&#38750;&#32447;&#24615;&#20449;&#21495;&#22788;&#29702;&#30340;&#23436;&#20840;&#27169;&#25311;&#31995;&#32479;&#12290;&#36890;&#36807;&#21033;&#29992;&#20016;&#23500;&#30340;Kerr&#35825;&#23548;&#38750;&#32447;&#24615;&#25928;&#24212;&#65292;&#21487;&#20197;&#29983;&#25104;&#21644;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20219;&#21153;&#30340;&#22810;&#20010;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#24182;&#22312;&#20809;&#36890;&#20449;&#22330;&#26223;&#20013;&#25552;&#20379;&#20248;&#20110;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20840;&#20809;&#38750;&#32447;&#24615;&#34917;&#20607;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#21151;&#32791;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09135</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#38750;&#32447;&#24615;&#27874;&#23548;&#20013;&#30340;&#22235;&#27874;&#28151;&#39057;&#30340;&#38750;&#24120;&#35268;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Unconventional Computing based on Four Wave Mixing in Highly Nonlinear Waveguides
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#39640;&#38750;&#32447;&#24615;&#27874;&#23548;&#20013;&#30340;&#22235;&#27874;&#28151;&#39057;&#25928;&#24212;&#30340;&#20809;&#23398;&#38750;&#24120;&#35268;&#21152;&#36895;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#30452;&#25509;&#22312;&#20809;&#23398;&#39046;&#22495;&#20869;&#36827;&#34892;&#38750;&#32447;&#24615;&#20449;&#21495;&#22788;&#29702;&#30340;&#23436;&#20840;&#27169;&#25311;&#31995;&#32479;&#12290;&#36890;&#36807;&#21033;&#29992;&#20016;&#23500;&#30340;Kerr&#35825;&#23548;&#38750;&#32447;&#24615;&#25928;&#24212;&#65292;&#21487;&#20197;&#29983;&#25104;&#21644;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20219;&#21153;&#30340;&#22810;&#20010;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#24182;&#22312;&#20809;&#36890;&#20449;&#22330;&#26223;&#20013;&#25552;&#20379;&#20248;&#20110;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20840;&#20809;&#38750;&#32447;&#24615;&#34917;&#20607;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#21151;&#32791;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#38750;&#32447;&#24615;&#27874;&#23548;&#20013;&#30340;&#22235;&#27874;&#28151;&#39057;&#25928;&#24212;&#30340;&#20809;&#23398;&#38750;&#24120;&#35268;&#21152;&#36895;&#22120;&#12290;&#35813;&#26041;&#26696;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#23436;&#20840;&#27169;&#25311;&#30340;&#31995;&#32479;&#65292;&#30452;&#25509;&#22312;&#20809;&#23398;&#39046;&#22495;&#20869;&#36827;&#34892;&#38750;&#32447;&#24615;&#20449;&#21495;&#22788;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#23500;&#21547;&#30340;Kerr&#35825;&#23548;&#38750;&#32447;&#24615;&#25928;&#24212;&#65292;&#21487;&#20197;&#29983;&#25104;&#24182;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20219;&#21153;&#30340;&#36755;&#20837;&#20449;&#21495;&#30340;&#22810;&#20010;&#38750;&#32447;&#24615;&#21464;&#25442;&#12290;&#25105;&#20204;&#39318;&#20808;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;Santa-Fe&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#22788;&#29702;&#22120;&#30340;&#30495;&#27491;&#23041;&#21147;&#22312;&#20110;&#22312;&#20809;&#36890;&#20449;&#22330;&#26223;&#20013;&#25552;&#20379;&#20248;&#20110;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#32467;&#26524;&#30340;&#20840;&#20809;&#38750;&#32447;&#24615;&#34917;&#20607;&#65292;&#21516;&#26102;&#38477;&#20302;&#21151;&#32791;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;FWM&#27169;&#22359;&#20316;&#20026;&#21487;&#37325;&#26500;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#27169;&#22359;&#65292;&#33021;&#22815;&#22797;&#29616;&#29305;&#24449;&#31561;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09135v1 Announce Type: cross Abstract: In this work we numerically analyze a photonic unconventional accelerator based on the four-wave mixing effect in highly nonlinear waveguides. The proposed scheme can act as a fully analogue system for nonlinear signal processing directly in the optical domain. By exploiting the rich Kerr-induced nonlinearities, multiple nonlinear transformations of an input signal can be generated and used for solving complex nonlinear tasks. We first evaluate the performance of our scheme in the Santa-Fe chaotic time-series prediction. The true power of this processor is revealed in the all-optical nonlinearity compensation in an optical communication scenario where we provide results superior to those offered by strong machine learning algorithms with reduced power consumption and computational complexity. Finally, we showcase how the FWM module can be used as a reconfigurable nonlinear activation module being capable of reproducing characteristic fu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25104;&#21151;&#22320;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.09132</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Adversarial Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25104;&#21151;&#22320;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#24191;&#27867;&#21644;&#26222;&#36941;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#34892;&#19994;&#21644;&#30740;&#31350;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#34920;&#29616;&#20986;&#23545;&#25239;&#34892;&#20026;&#30340;&#31243;&#24230;&#20173;&#28982;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30740;&#31350;&#24120;&#35265;&#30340;&#20844;&#24320;&#21487;&#29992;LLMs&#26159;&#21542;&#20855;&#26377;&#33021;&#21147;&#25200;&#20081;&#25991;&#26412;&#26679;&#26412;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#21363;&#25152;&#35859;&#30340;&#23545;&#25239;&#31034;&#20363;&#25110;&#25915;&#20987;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35843;&#26597;LLMs&#26159;&#21542;&#26412;&#36136;&#19978;&#33021;&#22815;&#20174;&#33391;&#24615;&#26679;&#26412;&#20013;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#29616;&#26377;&#30340;&#23433;&#20840;&#38450;&#32447;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#37325;&#28857;&#20851;&#27880;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65292;&#21457;&#29616;LLMs&#25104;&#21151;&#22320;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#26377;&#25928;&#22320;&#30772;&#22351;&#20102;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#65288;&#21322;&#65289;&#33258;&#21160;&#21270;&#23433;&#20840;&#35780;&#20272;&#21644;&#38450;&#24481;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09132v1 Announce Type: new Abstract: The proliferation of large language models (LLMs) has sparked widespread and general interest due to their strong language generation capabilities, offering great potential for both industry and research. While previous research delved into the security and privacy issues of LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored. Addressing this gap, we investigate whether common publicly available LLMs have inherent capabilities to perturb text samples to fool safety measures, so-called adversarial examples resp.~attacks. More specifically, we investigate whether LLMs are inherently able to craft adversarial examples out of benign samples to fool existing safe rails. Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively undermining hate speech detection systems. Our findings carry significant implications for (semi-)aut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;MonoCoder&#36827;&#34892;MPI-based&#31243;&#24207;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09126</link><description>&lt;p&gt;
MPIrigen: &#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
MPIrigen: MPI Code Generation through Domain-Specific Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;MonoCoder&#36827;&#34892;MPI-based&#31243;&#24207;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24182;&#34892;&#35745;&#31639;&#20013;&#65292;&#39640;&#25928;&#30340;&#24182;&#34892;&#35745;&#31639;&#23588;&#20026;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#28040;&#24687;&#20256;&#36882;&#25509;&#21475;&#65288;MPI&#65289;&#38598;&#25104;&#39046;&#22495;&#12290;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#31243;&#24207;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24182;&#34892;&#32534;&#31243;&#20219;&#21153;&#65292;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#25506;&#35752;&#20102;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#31243;&#24207;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#21457;&#29616;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#22914;GPT-3.5&#21644;PolyCoder&#65288;&#19987;&#38376;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#27169;&#22411;&#65289;&#65292;&#22312;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#31243;&#24207;&#26102;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#27604;&#36890;&#29992;&#31243;&#24207;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;MPI&#30456;&#20851;&#32534;&#31243;&#35821;&#35328;C&#21644;C++&#39044;&#35757;&#32451;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;MonoCoder&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;HPCorpusMPI&#19978;&#23545;MonoCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;MPI-based&#31243;&#24207;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09126v1 Announce Type: cross Abstract: The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration. The challenging parallel programming task of generating MPI-based parallel programs has remained unexplored. This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs. Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation, when generating MPI-based programs compared to general-purpose programs. In contrast, domain-specific models such as MonoCoder, which are pretrained on MPI-related programming languages of C and C++, outperform larger models. Subsequently, we introduce a dedicated downstream task of MPI-based program generation by fine-tuning MonoCoder on HPCorpusMPI. We call the r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20449;&#21495;&#20998;&#31163;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21253;&#21547;&#32431;&#32452;&#20998;&#20449;&#21495;&#21152;&#26435;&#21644;&#30340;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#20809;&#35889;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#22810;&#31181;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09122</link><description>&lt;p&gt;
&#28151;&#21512;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixed-Output Gaussian Process Latent Variable Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20449;&#21495;&#20998;&#31163;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21253;&#21547;&#32431;&#32452;&#20998;&#20449;&#21495;&#21152;&#26435;&#21644;&#30340;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#20809;&#35889;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#30340;&#20449;&#21495;&#20998;&#31163;&#26041;&#27861;&#65292;&#20854;&#20013;&#20449;&#21495;&#21487;&#20197;&#26681;&#25454;&#28508;&#21464;&#37327;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22686;&#21152;&#20102;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;GPLVMs&#65289;&#65292;&#20197;&#21253;&#25324;&#27599;&#20010;&#25968;&#25454;&#28857;&#30001;&#24050;&#30693;&#25968;&#37327;&#30340;&#32431;&#32452;&#20998;&#20449;&#21495;&#30340;&#21152;&#26435;&#21644;&#32452;&#25104;&#30340;&#24773;&#20917;&#65292;&#24182;&#35266;&#23519;&#22810;&#20010;&#36755;&#20837;&#20301;&#32622;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#20351;&#29992;&#21508;&#31181;&#20851;&#20110;&#27599;&#20010;&#35266;&#27979;&#26435;&#37325;&#30340;&#20808;&#39564;&#12290;&#36825;&#31181;&#28789;&#27963;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#31034;&#21253;&#25324;&#29992;&#20110;&#20272;&#35745;&#20998;&#25968;&#32452;&#25104;&#30340;&#24635;&#21644;&#20026;&#19968;&#32422;&#26463;&#21644;&#29992;&#20110;&#20998;&#31867;&#30340;&#20108;&#36827;&#21046;&#26435;&#37325;&#30340;&#29992;&#20363;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#23545;&#20110;&#20809;&#35889;&#23398;&#23588;&#20854;&#30456;&#20851;&#65292;&#22240;&#20026;&#25913;&#21464;&#26465;&#20214;&#21487;&#33021;&#23548;&#33268;&#22522;&#30784;&#32431;&#32452;&#20998;&#20449;&#21495;&#22312;&#26679;&#26412;&#20043;&#38388;&#21464;&#21270;&#12290;&#20026;&#20102;&#23637;&#31034;&#23545;&#20809;&#35889;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#20010;&#24212;&#29992;&#65306;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#28201;&#24230;&#30340;&#36817;&#32418;&#22806;&#20809;&#35889;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09122v1 Announce Type: cross Abstract: This work develops a Bayesian non-parametric approach to signal separation where the signals may vary according to latent variables. Our key contribution is to augment Gaussian Process Latent Variable Models (GPLVMs) to incorporate the case where each data point comprises the weighted sum of a known number of pure component signals, observed across several input locations. Our framework allows the use of a range of priors for the weights of each observation. This flexibility enables us to represent use cases including sum-to-one constraints for estimating fractional makeup, and binary weights for classification. Our contributions are particularly relevant to spectroscopy, where changing conditions may cause the underlying pure component signals to vary from sample to sample. To demonstrate the applicability to both spectroscopy and other domains, we consider several applications: a near-infrared spectroscopy data set with varying temper
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27604;&#36739;&#24378;&#21270;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#22312;&#25968;&#25454;&#20998;&#24067;&#31354;&#38388;&#20013;&#36335;&#24452;&#38271;&#24230;&#26469;&#37327;&#21270;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#29615;&#22659;&#21644;&#22810;&#31181;&#31639;&#27861;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.09113</link><description>&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#31354;&#38388;&#20013;&#30340;&#26368;&#20248;&#36755;&#36816;&#27979;&#37327;&#24378;&#21270;&#23398;&#20064;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Measuring Exploration in Reinforcement Learning via Optimal Transport in Policy Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27604;&#36739;&#24378;&#21270;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#22312;&#25968;&#25454;&#20998;&#24067;&#31354;&#38388;&#20013;&#36335;&#24452;&#38271;&#24230;&#26469;&#37327;&#21270;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#29615;&#22659;&#21644;&#22810;&#31181;&#31639;&#27861;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#26159;&#20915;&#23450;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23398;&#20064;&#36895;&#24230;&#21644;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#37327;&#21270;&#21644;&#27604;&#36739;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25152;&#23436;&#25104;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#30340;&#25968;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#31216;&#20026;&#25506;&#32034;&#25351;&#25968;&#65292;&#29992;&#20110;&#27604;&#36739;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30456;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#22312;&#30693;&#35782;&#20256;&#36755;&#65288;&#21487;&#36716;&#31227;&#24615;&#65289;&#26041;&#38754;&#25152;&#20184;&#20986;&#30340;&#30456;&#23545;&#21162;&#21147;&#65292;&#20197;&#21450;&#23558; RL &#30340;&#21021;&#22987;&#25968;&#25454;&#20998;&#24067;&#36716;&#21270;&#20026;&#23545;&#24212;&#30340;&#26368;&#32456;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#23558;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23398;&#20064;&#24314;&#27169;&#20026;&#19968;&#31995;&#21015;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#27604;&#36739; RL &#21644; SL &#31639;&#27861;&#22312;&#25968;&#25454;&#20998;&#24067;&#31354;&#38388;&#20013;&#30340;&#24635;&#36335;&#24452;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#29615;&#22659;&#21644;&#22810;&#31181;&#31639;&#27861;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#35777;&#26126;&#25506;&#32034;&#25351;&#25968;&#21487;&#20197;&#25581;&#31034;&#25506;&#32034;&#34892;&#20026;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09113v1 Announce Type: new Abstract: Exploration is the key ingredient of reinforcement learning (RL) that determines the speed and success of learning. Here, we quantify and compare the amount of exploration and learning accomplished by a Reinforcement Learning (RL) algorithm. Specifically, we propose a novel measure, named Exploration Index, that quantifies the relative effort of knowledge transfer (transferability) by an RL algorithm in comparison to supervised learning (SL) that transforms the initial data distribution of RL to the corresponding final data distribution. The comparison is established by formulating learning in RL as a sequence of SL tasks, and using optimal transport based metrics to compare the total path traversed by the RL and SL algorithms in the data distribution space. We perform extensive empirical analysis on various environments and with multiple algorithms to demonstrate that the exploration index yields insights about the exploration behaviour 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#35745;&#31639;&#21152;&#36895;&#23574;&#23792;&#32593;&#32476;&#20013;&#30340;&#20851;&#27880;&#26426;&#21046;&#30340;&#26032;&#26694;&#26550;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;CIFAR-10&#19978;&#33021;&#22815;&#36798;&#21040;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#33021;&#37327;&#21644;&#20869;&#23384;&#35775;&#38382;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.09109</link><description>&lt;p&gt;
&#38543;&#26426;&#23574;&#23792;&#20851;&#27880;&#65306;&#22312;&#23574;&#23792;&#32593;&#32476;&#20013;&#21033;&#29992;&#38543;&#26426;&#35745;&#31639;&#21152;&#36895;&#20851;&#27880;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Stochastic Spiking Attention: Accelerating Attention with Stochastic Computing in Spiking Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#35745;&#31639;&#21152;&#36895;&#23574;&#23792;&#32593;&#32476;&#20013;&#30340;&#20851;&#27880;&#26426;&#21046;&#30340;&#26032;&#26694;&#26550;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;CIFAR-10&#19978;&#33021;&#22815;&#36798;&#21040;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#33021;&#37327;&#21644;&#20869;&#23384;&#35775;&#38382;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#20943;&#23569;&#35745;&#31639;&#38656;&#27714;&#21644;&#25552;&#39640;&#21151;&#32791;&#25928;&#29575;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24050;&#23558;&#20854;&#25972;&#21512;&#21040;Transformer&#20307;&#31995;&#32467;&#26500;&#20013;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#23574;&#23792;&#20449;&#21495;&#22312;&#36890;&#29992;&#35745;&#31639;&#24179;&#21488;&#19978;&#23454;&#29616;&#20851;&#27880;&#26426;&#21046;&#20173;&#28982;&#20302;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#35745;&#31639;&#65288;SC&#65289;&#26469;&#26377;&#25928;&#25191;&#34892;&#22522;&#20110;SNN&#30340;Transformer&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;10&#20010;&#26102;&#38388;&#27493;&#20869;&#22312;CIFAR-10&#19978;&#36798;&#21040;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#65288;83.53%&#65289;&#65292;&#36825;&#19982;&#22522;&#32447;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#24615;&#33021;&#65288;83.66%&#65289;&#30456;&#24403;&#12290;&#25105;&#20204;&#20272;&#35745;&#65292;&#35813;SC&#26041;&#27861;&#21487;&#20197;&#23548;&#33268;CMOS&#25968;&#23383;ASIC&#35774;&#35745;&#20013;&#35745;&#31639;&#33021;&#37327;&#20943;&#23569;6.3&#20493;&#65292;&#20869;&#23384;&#35775;&#38382;&#25104;&#26412;&#20943;&#23569;1.7&#20493;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#38543;&#26426;&#27880;&#24847;&#21147;&#22359;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09109v1 Announce Type: cross Abstract: Spiking Neural Networks (SNNs) have been recently integrated into Transformer architectures due to their potential to reduce computational demands and to improve power efficiency. Yet, the implementation of the attention mechanism using spiking signals on general-purpose computing platforms remains inefficient. In this paper, we propose a novel framework leveraging stochastic computing (SC) to effectively execute the dot-product attention for SNN-based Transformers. We demonstrate that our approach can achieve high classification accuracy ($83.53\%$) on CIFAR-10 within 10 time steps, which is comparable to the performance of a baseline artificial neural network implementation ($83.66\%$). We estimate that the proposed SC approach can lead to over $6.3\times$ reduction in computing energy and $1.7\times$ reduction in memory access costs for a digital CMOS-based ASIC design. We experimentally validate our stochastic attention block design
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21355;&#26143;&#38598;&#32676;&#19978;&#30340;&#26426;&#36733;&#32852;&#37030;&#23398;&#20064;&#35843;&#24230;&#26041;&#26696;&#65292;&#21033;&#29992;&#21487;&#39044;&#27979;&#30340;&#21487;&#35265;&#24615;&#27169;&#24335;&#26469;&#35299;&#20915;&#38388;&#27463;&#24615;&#36830;&#25509;&#38382;&#39064;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#31649;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21327;&#21516;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.09105</link><description>&lt;p&gt;
&#21355;&#26143;&#38598;&#32676;&#19978;&#30340;&#26426;&#36733;&#32852;&#37030;&#23398;&#20064;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Scheduling for On-Board Federated Learning with Satellite Clusters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21355;&#26143;&#38598;&#32676;&#19978;&#30340;&#26426;&#36733;&#32852;&#37030;&#23398;&#20064;&#35843;&#24230;&#26041;&#26696;&#65292;&#21033;&#29992;&#21487;&#39044;&#27979;&#30340;&#21487;&#35265;&#24615;&#27169;&#24335;&#26469;&#35299;&#20915;&#38388;&#27463;&#24615;&#36830;&#25509;&#38382;&#39064;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#31649;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21327;&#21516;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#21355;&#26143;&#30340;&#22823;&#35268;&#27169;&#26143;&#24231;&#24050;&#32463;&#25104;&#20026;&#22823;&#37327;&#23453;&#36149;&#25968;&#25454;&#30340;&#26469;&#28304;&#12290;&#20026;&#20102;&#39640;&#25928;&#31649;&#29702;&#36825;&#20123;&#25968;&#25454;&#65292;&#26426;&#36733;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#21355;&#26143;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21327;&#21516;&#35757;&#32451;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#36830;&#25509;&#26143;&#24231;&#20869;&#37096;&#36712;&#36947;&#38388;&#21355;&#26143;&#30340;&#26426;&#36733;FL&#35843;&#24230;&#26041;&#26696;&#12290;&#25152;&#25552;&#26041;&#26696;&#21033;&#29992;&#20102;&#21355;&#26143;&#19982;&#22320;&#38754;&#31449;&#20043;&#38388;&#30340;&#21487;&#39044;&#27979;&#21487;&#35265;&#24615;&#27169;&#24335;&#65292;&#21253;&#25324;&#20010;&#21035;&#21355;&#26143;&#32423;&#21035;&#21644;&#25972;&#20010;&#36712;&#36947;&#33539;&#22260;&#20869;&#30340;&#27169;&#24335;&#65292;&#20197;&#20943;&#36731;&#38388;&#27463;&#24615;&#36830;&#25509;&#38382;&#39064;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#21487;&#29992;&#26102;&#38388;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#35843;&#24230;&#31243;&#24207;&#65306;&#19968;&#20010;&#29992;&#20110;&#21327;&#35843;&#36712;&#36947;&#20043;&#38388;&#30340;FL&#36807;&#31243;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#25511;&#21046;&#27599;&#20010;&#36712;&#36947;&#20869;&#37096;&#30340;FL&#36807;&#31243;&#12290;&#36825;&#20004;&#20010;&#35843;&#24230;&#31243;&#24207;&#20849;&#21516;&#30830;&#23450;&#22312;&#22320;&#38754;&#31449;&#25191;&#34892;&#20840;&#23616;&#26356;&#26032;&#30340;&#36866;&#24403;&#26102;&#38388;&#65292;&#24182;&#20998;&#37197;&#21512;&#36866;&#30340;&#26102;&#38388;&#36827;&#34892;&#26412;&#22320;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09105v1 Announce Type: cross Abstract: Mega-constellations of small satellites have evolved into a source of massive amount of valuable data. To manage this data efficiently, on-board federated learning (FL) enables satellites to train a machine learning (ML) model collaboratively without having to share the raw data. This paper introduces a scheme for scheduling on-board FL for constellations connected with intra-orbit inter-satellite links. The proposed scheme utilizes the predictable visibility pattern between satellites and ground station (GS), both at the individual satellite level and cumulatively within the entire orbit, to mitigate intermittent connectivity and best use of available time. To this end, two distinct schedulers are employed: one for coordinating the FL procedures among orbits, and the other for controlling those within each orbit. These two schedulers cooperatively determine the appropriate time to perform global updates in GS and then allocate suitable
&lt;/p&gt;</description></item><item><title>FedSiKD&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#30456;&#20284;&#24615;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32422;&#26463;&#38382;&#39064;&#65292;&#36890;&#36807;&#20419;&#36827;&#31751;&#20869;&#21516;&#36136;&#24615;&#26469;&#25552;&#39640;&#20248;&#21270;&#25928;&#29575;&#21644;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09095</link><description>&lt;p&gt;
FedSiKD: &#23458;&#25143;&#30456;&#20284;&#24615;&#21644;&#30693;&#35782;&#33976;&#39311;&#65306;&#35299;&#20915;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32422;&#26463;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FedSiKD: Clients Similarity and Knowledge Distillation: Addressing Non-i.i.d. and Constraints in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09095
&lt;/p&gt;
&lt;p&gt;
FedSiKD&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#30456;&#20284;&#24615;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32422;&#26463;&#38382;&#39064;&#65292;&#36890;&#36807;&#20419;&#36827;&#31751;&#20869;&#21516;&#36136;&#24615;&#26469;&#25552;&#39640;&#20248;&#21270;&#25928;&#29575;&#21644;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#22312;&#20998;&#25955;&#26041;&#24335;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#20986;&#29616;&#20102;&#12290;&#23458;&#25143;&#25968;&#25454;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-i.i.d.&#65289;&#29305;&#24615;&#20197;&#21450;&#23545;&#23458;&#25143;&#25110;&#36793;&#32536;&#35774;&#22791;&#30340;&#38480;&#21046;&#65292;&#22312;FL&#20013;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#36328;&#35768;&#22810;&#36890;&#20449;&#36718;&#27425;&#30340;&#23398;&#20064;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#21033;&#29992;&#20855;&#26377;&#39118;&#38505;&#21644;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#24615;&#12290;&#20256;&#32479;FL&#26041;&#27861;&#21487;&#33021;&#20250;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FedSiKD&#65292;&#23427;&#22312;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#34701;&#20837;&#20102;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#12290;&#24403;&#23458;&#25143;&#31471;&#21152;&#20837;&#31995;&#32479;&#26102;&#65292;&#20182;&#20204;&#23433;&#20840;&#22320;&#20849;&#20139;&#26377;&#20851;&#20854;&#25968;&#25454;&#20998;&#24067;&#30340;&#30456;&#20851;&#32479;&#35745;&#20449;&#24687;&#65292;&#20419;&#36827;&#31751;&#20869;&#21516;&#36136;&#24615;&#12290;&#36825;&#25552;&#39640;&#20102;&#20248;&#21270;&#25928;&#29575;&#24182;&#21152;&#36895;&#20102;&#23398;&#20064;&#36807;&#31243;&#65292;&#26377;&#25928;&#22320;&#22312;&#25945;&#24072;&#21644;&#23398;&#29983;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#30693;&#35782;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09095v1 Announce Type: new Abstract: In recent years, federated learning (FL) has emerged as a promising technique for training machine learning models in a decentralized manner while also preserving data privacy. The non-independent and identically distributed (non-i.i.d.) nature of client data, coupled with constraints on client or edge devices, presents significant challenges in FL. Furthermore, learning across a high number of communication rounds can be risky and potentially unsafe for model exploitation. Traditional FL approaches may suffer from these challenges. Therefore, we introduce FedSiKD, which incorporates knowledge distillation (KD) within a similarity-based federated learning framework. As clients join the system, they securely share relevant statistics about their data distribution, promoting intra-cluster homogeneity. This enhances optimization efficiency and accelerates the learning process, effectively transferring knowledge between teacher and student mo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26159;&#20851;&#20110;&#23545;400&#20010;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#35814;&#23613;&#30340;&#27010;&#36848;&#21644;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#36873;&#25321;&#36866;&#24403;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.09092</link><description>&lt;p&gt;
&#19977;&#21313;&#24180;&#30340;&#28608;&#27963;&#20989;&#25968;&#65306;400&#20010;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Three Decades of Activations: A Comprehensive Survey of 400 Activation Functions for Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09092
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;&#20851;&#20110;&#23545;400&#20010;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#35814;&#23613;&#30340;&#27010;&#36848;&#21644;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#36873;&#25321;&#36866;&#24403;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#39640;&#25928;&#24037;&#20855;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#36827;&#19968;&#27493;&#21152;&#24378;&#20102;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#21644;&#23454;&#38469;&#21487;&#29992;&#24615;&#12290;&#31070;&#32463;&#32593;&#32476;&#25104;&#21151;&#30340;&#37325;&#35201;&#26465;&#20214;&#20043;&#19968;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#28608;&#27963;&#20989;&#25968;&#24341;&#20837;&#38750;&#32447;&#24615;&#27169;&#22411;&#12290;&#36807;&#21435;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#31867;&#22411;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20294;&#27809;&#26377;&#21333;&#19968;&#30340;&#32508;&#21512;&#26469;&#28304;&#21253;&#21547;&#23427;&#20204;&#30340;&#35814;&#23613;&#27010;&#36848;&#12290;&#21363;&#20351;&#22312;&#25105;&#20204;&#30340;&#32463;&#39564;&#20013;&#65292;&#32570;&#20047;&#36825;&#20010;&#27010;&#36848;&#20063;&#20250;&#23548;&#33268;&#20887;&#20313;&#21644;&#26080;&#24847;&#20013;&#37325;&#26032;&#21457;&#29616;&#24050;&#26377;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28041;&#21450;400&#20010;&#28608;&#27963;&#20989;&#25968;&#30340;&#24191;&#27867;&#35843;&#26597;&#65292;&#27604;&#20043;&#21069;&#30340;&#35843;&#26597;&#35268;&#27169;&#22823;&#20960;&#20493;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#27719;&#32534;&#36824;&#24341;&#29992;&#20102;&#36825;&#20123;&#35843;&#26597;&#65307;&#28982;&#32780;&#65292;&#23427;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#35814;&#23613;&#30340;&#27010;&#36848;&#21644;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26356;&#22909;&#22320;&#20102;&#35299;&#21644;&#36873;&#25321;&#36866;&#24403;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09092v1 Announce Type: new Abstract: Neural networks have proven to be a highly effective tool for solving complex problems in many areas of life. Recently, their importance and practical usability have further been reinforced with the advent of deep learning. One of the important conditions for the success of neural networks is the choice of an appropriate activation function introducing non-linearity into the model. Many types of these functions have been proposed in the literature in the past, but there is no single comprehensive source containing their exhaustive overview. The absence of this overview, even in our experience, leads to redundancy and the unintentional rediscovery of already existing activation functions. To bridge this gap, our paper presents an extensive survey involving 400 activation functions, which is several times larger in scale than previous surveys. Our comprehensive compilation also references these surveys; however, its main goal is to provide 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#23548;&#25968;&#20449;&#24687;&#34701;&#20837;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21892;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#35268;&#21017;&#32593;&#26684;&#19978;&#36817;&#20284;&#23548;&#25968;&#30340;&#26032;&#26694;&#26550;&#65292;&#20174;&#32780;&#34920;&#26126;Sobolev&#35757;&#32451;&#22312;&#36817;&#20284;&#35299;&#31639;&#23376;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09084</link><description>&lt;p&gt;
Sobolev&#35757;&#32451;&#29992;&#20110;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sobolev Training for Operator Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#23548;&#25968;&#20449;&#24687;&#34701;&#20837;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21892;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#35268;&#21017;&#32593;&#26684;&#19978;&#36817;&#20284;&#23548;&#25968;&#30340;&#26032;&#26694;&#26550;&#65292;&#20174;&#32780;&#34920;&#26126;Sobolev&#35757;&#32451;&#22312;&#36817;&#20284;&#35299;&#31639;&#23376;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;Sobolev&#35757;&#32451;&#23545;&#20110;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23558;&#23548;&#25968;&#20449;&#24687;&#34701;&#20837;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#25913;&#21892;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31639;&#23376;&#23398;&#20064;&#20013;&#36817;&#20284;&#19981;&#35268;&#21017;&#32593;&#26684;&#19978;&#30340;&#23548;&#25968;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24471;&#21040;&#20102;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#30340;&#25903;&#25345;&#12290;&#36825;&#34920;&#26126;&#20102;Sobolev&#35757;&#32451;&#22312;&#36817;&#20284;&#26080;&#31351;&#32500;&#31354;&#38388;&#20013;&#30340;&#35299;&#31639;&#23376;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09084v1 Announce Type: cross Abstract: This study investigates the impact of Sobolev Training on operator learning frameworks for improving model performance. Our research reveals that integrating derivative information into the loss function enhances the training process, and we propose a novel framework to approximate derivatives on irregular meshes in operator learning. Our findings are supported by both experimental evidence and theoretical analysis. This demonstrates the effectiveness of Sobolev Training in approximating the solution operators between infinite-dimensional spaces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26816;&#27979;&#24310;&#36831;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#26816;&#27979;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#30740;&#31350;&#36890;&#36807;&#23558;&#35823;&#25253;&#29575;&#19982;&#25915;&#20987;&#21644;&#38169;&#35823;&#30340;&#26102;&#38388;&#24310;&#36831;&#30456;&#20851;&#32852;&#65292;&#24182;&#25552;&#20379;&#20102;&#37197;&#32622;&#26816;&#27979;&#22120;&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.09082</link><description>&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#26816;&#27979;&#24310;&#36831;&#65306;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#35270;&#35282;&#65311;
&lt;/p&gt;
&lt;p&gt;
Detection Latencies of Anomaly Detectors: An Overlooked Perspective ?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26816;&#27979;&#24310;&#36831;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#26816;&#27979;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#30740;&#31350;&#36890;&#36807;&#23558;&#35823;&#25253;&#29575;&#19982;&#25915;&#20987;&#21644;&#38169;&#35823;&#30340;&#26102;&#38388;&#24310;&#36831;&#30456;&#20851;&#32852;&#65292;&#24182;&#25552;&#20379;&#20102;&#37197;&#32622;&#26816;&#27979;&#22120;&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25915;&#20987;&#26041;&#24335;&#19981;&#26029;&#21464;&#21270;&#65292;&#19982;&#27492;&#21516;&#26102;ICT&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#20063;&#22312;&#22686;&#21152;&#65292;&#36825;&#20351;&#24471;&#26500;&#24314;&#22522;&#20110;&#24322;&#24120;&#30340;&#20837;&#20405;&#26816;&#27979;&#22120;(ID)&#21644;&#38169;&#35823;&#26816;&#27979;&#22120;(ED)&#21464;&#24471;&#22256;&#38590;&#65306;&#23427;&#20204;&#24517;&#39035;&#20934;&#30830;&#22320;&#26816;&#27979;&#25915;&#20987;&#65292;&#24182;&#19988;&#24212;&#35813;&#21450;&#26102;&#36827;&#34892;&#26816;&#27979;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#30740;&#31350;&#24037;&#20316;&#20391;&#37325;&#20110;&#25913;&#36827;&#21644;&#27604;&#36739;&#26816;&#27979;&#33021;&#21147;&#65292;&#20294;&#26816;&#27979;&#30340;&#21450;&#26102;&#24615;&#24448;&#24448;&#19981;&#34987;&#32771;&#34385;&#25110;&#19981;&#22815;&#20805;&#20998;&#22320;&#36827;&#34892;&#35780;&#20272;&#25110;&#35752;&#35770;&#12290;&#26412;&#25991;&#35770;&#36848;&#20102;&#27979;&#37327;&#25915;&#20987;&#21644;&#38169;&#35823;&#30340;&#26102;&#38388;&#24310;&#36831;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#26816;&#27979;&#22120;&#36827;&#34892;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#27491;&#30830;&#21644;&#21450;&#26102;&#26816;&#27979;&#20043;&#38388;&#30340;&#23454;&#29992;&#24615;&#26435;&#34913;&#12290;&#31616;&#35201;&#22320;&#35828;&#65292;&#35813;&#26041;&#27861;&#23558;&#35823;&#25253;&#29575;&#19982;&#25915;&#20987;&#21644;&#38169;&#35823;&#30340;&#26102;&#38388;&#24310;&#36831;&#30456;&#20851;&#32852;&#65292;&#26368;&#32456;&#25552;&#20379;&#20102;&#37197;&#32622;&#26816;&#27979;&#22120;&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#30340;ED&#21644;ID&#35299;&#20915;&#26041;&#26696;&#22312;&#20004;&#20010;&#24037;&#19994;&#26696;&#20363;&#20013;&#24212;&#29992;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;i) &#19968;&#20010;&#26696;&#20363;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09082v1 Announce Type: cross Abstract: The ever-evolving landscape of attacks, coupled with the growing complexity of ICT systems, makes crafting anomaly-based intrusion detectors (ID) and error detectors (ED) a difficult task: they must accurately detect attacks, and they should promptly perform detections. Although improving and comparing the detection capability is the focus of most research works, the timeliness of the detection is less considered and often insufficiently evaluated or discussed. In this paper, we argue the relevance of measuring the temporal latency of attacks and errors, and we propose an evaluation approach for detectors to ensure a pragmatic trade-off between correct and in-time detection. Briefly, the approach relates the false positive rate with the temporal latency of attacks and errors, and this ultimately leads to guidelines for configuring a detector. We apply our approach by evaluating different ED and ID solutions in two industrial cases: i) a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#31209;&#22806;&#25512;&#26799;&#24230;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#25237;&#24433;&#21040;&#21322;&#27491;&#23450;&#38181;&#65292;&#21462;&#24471;&#20102;&#25910;&#25947;&#20110;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#35299;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09081</link><description>&lt;p&gt;
&#20302;&#31209;&#22806;&#25512;&#26799;&#24230;&#26041;&#27861;&#29992;&#20110;&#21487;&#25193;&#23637;&#30340;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Extragradient Methods for Scalable Semidefinite Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#31209;&#22806;&#25512;&#26799;&#24230;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#25237;&#24433;&#21040;&#21322;&#27491;&#23450;&#38181;&#65292;&#21462;&#24471;&#20102;&#25910;&#25947;&#20110;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#35299;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31867;&#38750;&#24120;&#37325;&#35201;&#30340;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#26082;&#21253;&#25324;&#20984;&#30446;&#26631;&#20989;&#25968;&#65288;&#24179;&#28369;&#25110;&#38750;&#24179;&#28369;&#65289;&#65292;&#21448;&#21253;&#25324;&#39069;&#22806;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#24179;&#28369;&#20984;&#32422;&#26463;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#32452;&#21512;&#20248;&#21270;&#31561;&#39046;&#22495;&#37117;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#20851;&#27880;&#39640;&#32500;&#21644;&#21487;&#33021;&#30340;&#24773;&#22659;&#65292;&#20854;&#20013;&#38382;&#39064;&#20855;&#26377;&#20302;&#31209;&#35299;&#65292;&#24182;&#28385;&#36275;&#20302;&#31209;&#20114;&#34917;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#29702;&#35770;&#32467;&#26524;&#65292;&#35777;&#26126;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#24050;&#30693;&#30340;&#22806;&#25512;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#25509;&#36817;&#26368;&#20248;&#21407;&#22987;&#23545;&#20598;&#35299;&#30340;&#24773;&#20917;&#19979;&#21021;&#22987;&#21270;&#26102;&#65292;&#25910;&#25947;&#20110;&#24102;&#26377;&#26631;&#20934;&#25910;&#25947;&#36895;&#24230;&#20445;&#35777;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#65292;&#20165;&#20351;&#29992;&#20302;&#31209;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#26469;&#25237;&#24433;&#21040;&#21322;&#27491;&#23450;&#38181;&#65292;&#32780;&#19981;&#26159;&#35745;&#31639;&#19978;&#38480;&#30340;&#20840;&#31209;SVD&#25152;&#38656;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09081v1 Announce Type: cross Abstract: We consider several classes of highly important semidefinite optimization problems that involve both a convex objective function (smooth or nonsmooth) and additional linear or nonlinear smooth and convex constraints, which are ubiquitous in statistics, machine learning, combinatorial optimization, and other domains. We focus on high-dimensional and plausible settings in which the problem admits a low-rank solution which also satisfies a low-rank complementarity condition. We provide several theoretical results proving that, under these circumstances, the well-known Extragradient method, when initialized in the proximity of an optimal primal-dual solution, converges to a solution of the constrained optimization problem with its standard convergence rates guarantees, using only low-rank singular value decompositions (SVD) to project onto the positive semidefinite cone, as opposed to computationally-prohibitive full-rank SVDs required in w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;ExpD3 &#21644; BE-TD3&#65292;&#29992;&#20110;&#35299;&#20915;&#21644;&#21033;&#29992; Actor-Critic &#26041;&#27861;&#20013;&#30340;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.09078</link><description>&lt;p&gt;
&#22312; Actor-Critic &#26041;&#27861;&#20013;&#21033;&#29992;&#20272;&#35745;&#20559;&#24046;&#30340;&#28145;&#24230;&#21452; Q-Learning &#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploiting Estimation Bias in Deep Double Q-Learning for Actor-Critic Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;ExpD3 &#21644; BE-TD3&#65292;&#29992;&#20110;&#35299;&#20915;&#21644;&#21033;&#29992; Actor-Critic &#26041;&#27861;&#20013;&#30340;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#21644;&#21033;&#29992;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013; Actor-Critic &#26041;&#27861;&#20013;&#30340;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#65292;&#20351;&#29992;&#20102;&#28145;&#24230;&#21452; Q-Learning&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#31639;&#27861;&#65306;Expectile Delayed Deep Deterministic Policy Gradient (ExpD3) &#21644; Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3)&#12290;ExpD3 &#26088;&#22312;&#36890;&#36807;&#21333;&#19968;&#30340; Q &#20272;&#35745;&#26469;&#20943;&#23569;&#36807;&#24230;&#20272;&#35745;&#20559;&#24046;&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#20043;&#38388;&#25552;&#20379;&#24179;&#34913;&#65292;&#32780; BE-TD3 &#21017;&#26088;&#22312;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#36873;&#25321;&#26368;&#26377;&#21033;&#30340;&#20272;&#35745;&#20559;&#24046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#19982; TD3 &#31561;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23588;&#20854;&#26159;&#22312;&#20272;&#35745;&#20559;&#24046;&#26174;&#33879;&#24433;&#21709;&#23398;&#20064;&#30340;&#29615;&#22659;&#20013;&#65292;&#21487;&#20197;&#21305;&#25932;&#25110;&#36229;&#36234;&#23427;&#20204;&#12290;&#36825;&#20123;&#32467;&#26524;&#20984;&#26174;&#20102;&#20272;&#35745;&#20559;&#24046;&#23545;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09078v1 Announce Type: cross Abstract: This paper introduces innovative methods in Reinforcement Learning (RL), focusing on addressing and exploiting estimation biases in Actor-Critic methods for continuous control tasks, using Deep Double Q-Learning. We propose two novel algorithms: Expectile Delayed Deep Deterministic Policy Gradient (ExpD3) and Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3). ExpD3 aims to reduce overestimation bias with a single $Q$ estimate, offering a balance between computational efficiency and performance, while BE-TD3 is designed to dynamically select the most advantageous estimation bias during training. Our extensive experiments across various continuous control tasks demonstrate the effectiveness of our approaches. We show that these algorithms can either match or surpass existing methods like TD3, particularly in environments where estimation biases significantly impact learning. The results underline the importance of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DisGNet&#30340;&#36317;&#31163;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;Gough-Stewart&#24179;&#21488;&#30340;&#21069;&#21521;&#36816;&#21160;&#23398;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;k-FWL&#31639;&#27861;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#21644;&#24341;&#20837;&#36866;&#29992;&#20110;GPU&#30340;Newton-Raphson&#26041;&#27861;&#65292;DisGNet&#21487;&#20197;&#23454;&#29616;&#36229;&#39640;&#31934;&#24230;&#30340;&#36755;&#20986;&#23039;&#21183;&#65292;&#21516;&#26102;&#28385;&#36275;&#23454;&#26102;&#35745;&#31639;&#30340;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.09077</link><description>&lt;p&gt;
DisGNet: &#19968;&#31181;&#29992;&#20110;Gough-Stewart&#24179;&#21488;&#21069;&#21521;&#36816;&#21160;&#23398;&#23398;&#20064;&#30340;&#36317;&#31163;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DisGNet: A Distance Graph Neural Network for Forward Kinematics Learning of Gough-Stewart Platform
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DisGNet&#30340;&#36317;&#31163;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;Gough-Stewart&#24179;&#21488;&#30340;&#21069;&#21521;&#36816;&#21160;&#23398;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;k-FWL&#31639;&#27861;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#21644;&#24341;&#20837;&#36866;&#29992;&#20110;GPU&#30340;Newton-Raphson&#26041;&#27861;&#65292;DisGNet&#21487;&#20197;&#23454;&#29616;&#36229;&#39640;&#31934;&#24230;&#30340;&#36755;&#20986;&#23039;&#21183;&#65292;&#21516;&#26102;&#28385;&#36275;&#23454;&#26102;&#35745;&#31639;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DisGNet&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23398;&#20064;&#22270;&#36317;&#31163;&#30697;&#38453;&#65292;&#20197;&#35299;&#20915;Gough-Stewart&#24179;&#21488;&#30340;&#21069;&#21521;&#36816;&#21160;&#23398;&#38382;&#39064;&#12290; DisGNet&#37319;&#29992;k-FWL&#31639;&#27861;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#65292;&#20855;&#26377;&#36739;&#23567;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#25552;&#20379;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#36866;&#21512;&#23454;&#38469;&#37096;&#32626;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;GPU&#30340;Newton-Raphson&#26041;&#27861;&#65292;&#19968;&#31181;&#22312;GPU&#19978;&#25191;&#34892;&#30340;&#39640;&#25928;&#24182;&#34892;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;DisGNet&#30340;&#36755;&#20986;&#23039;&#21183;&#65292;&#23454;&#29616;&#20102;&#36229;&#39640;&#31934;&#24230;&#23039;&#21183;&#12290;&#35813;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#22312;&#28385;&#36275;&#23454;&#26102;&#35201;&#27714;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#36229;&#39640;&#31934;&#24230;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;DisGNet&#21487;&#20197;&#23454;&#29616;&#38169;&#35823;&#31934;&#24230;&#20302;&#20110;1mm&#21644;1&#24230;&#65292;&#22312;79.8\%&#21644;98.2\%&#20197;&#19978;&#12290;&#20316;&#20026;&#22312;GPU&#19978;&#25191;&#34892;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#20445;&#28385;&#36275;&#23454;&#26102;&#35745;&#31639;&#30340;&#35201;&#27714;&#12290;&#20195;&#30721;&#24050;&#22312;https://github.com/FLAMEZZ5201/DisGNet&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09077v1 Announce Type: cross Abstract: In this paper, we propose a graph neural network, DisGNet, for learning the graph distance matrix to address the forward kinematics problem of the Gough-Stewart platform. DisGNet employs the k-FWL algorithm for message-passing, providing high expressiveness with a small parameter count, making it suitable for practical deployment. Additionally, we introduce the GPU-friendly Newton-Raphson method, an efficient parallelized optimization method executed on the GPU to refine DisGNet's output poses, achieving ultra-high-precision pose. This novel two-stage approach delivers ultra-high precision output while meeting real-time requirements. Our results indicate that on our dataset, DisGNet can achieves error accuracys below 1mm and 1deg at 79.8\% and 98.2\%, respectively. As executed on a GPU, our two-stage method can ensure the requirement for real-time computation. Codes are released at https://github.com/FLAMEZZ5201/DisGNet.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31215;&#20998;&#39033;&#34917;&#20607;&#20108;&#27425;&#22870;&#21169;&#20989;&#25968;&#31283;&#24577;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#38271;&#26399;&#22870;&#21169;&#30340;&#32771;&#34385;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#31995;&#32479;&#24615;&#33021;&#65288;&#22914;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#21644;&#21464;&#36947;&#27169;&#22411;&#65289;&#20013;&#30340;&#31283;&#24577;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09075</link><description>&lt;p&gt;
&#29992;&#20110;&#20855;&#26377;&#20108;&#27425;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#31283;&#24577;&#35823;&#24046;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
Steady-State Error Compensation for Reinforcement Learning with Quadratic Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31215;&#20998;&#39033;&#34917;&#20607;&#20108;&#27425;&#22870;&#21169;&#20989;&#25968;&#31283;&#24577;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#38271;&#26399;&#22870;&#21169;&#30340;&#32771;&#34385;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#31995;&#32479;&#24615;&#33021;&#65288;&#22914;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#21644;&#21464;&#36947;&#27169;&#22411;&#65289;&#20013;&#30340;&#31283;&#24577;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#20989;&#25968;&#36873;&#25321;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#24403;&#20351;&#29992;&#20108;&#27425;&#22870;&#21169;&#20989;&#25968;&#26102;&#65292;&#32463;&#24120;&#20250;&#20986;&#29616;&#31283;&#24577;&#35823;&#24046;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#26377;&#30340;&#20351;&#29992;&#32477;&#23545;&#20540;&#31867;&#22411;&#22870;&#21169;&#20989;&#25968;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#24448;&#24448;&#20250;&#22312;&#29305;&#23450;&#31995;&#32479;&#29366;&#24577;&#19979;&#24341;&#36215;&#36739;&#22823;&#27874;&#21160;&#65292;&#23548;&#33268;&#31361;&#28982;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#20837;&#31215;&#20998;&#39033;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#36825;&#20010;&#39033;&#31215;&#20998;&#36827;&#20108;&#27425;&#22870;&#21169;&#20989;&#25968;&#20013;&#65292;&#23545;RL&#31639;&#27861;&#36827;&#34892;&#31934;&#30830;&#35843;&#25972;&#65292;&#22686;&#24378;&#31995;&#32479;&#23545;&#38271;&#26399;&#22870;&#21169;&#30340;&#32771;&#34385;&#65292;&#20174;&#32780;&#32531;&#35299;&#31283;&#24577;&#35823;&#24046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#27169;&#22411;&#21644;&#21464;&#36947;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#21644;&#24615;&#33021;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#26377;&#25928;&#22320;&#20943;&#23567;&#20102;&#31283;&#24577;&#35823;&#24046;&#65292;&#36824;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09075v1 Announce Type: cross Abstract: The selection of a reward function in Reinforcement Learning (RL) has garnered significant attention because of its impact on system performance. Issues of steady-state error often manifest when quadratic reward functions are employed. Although existing solutions using absolute-value-type reward functions partially address this problem, they tend to induce substantial fluctuations in specific system states, leading to abrupt changes. In response to this challenge, this study proposes an approach that introduces an integral term. By integrating this term into quadratic-type reward functions, the RL algorithm is adeptly tuned, augmenting the system's consideration of long-term rewards and, consequently, alleviating concerns related to steady-state errors. Through experiments and performance evaluations on the Adaptive Cruise Control (ACC) model and lane change models, we validate that the proposed method not only effectively diminishes st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22266;&#20307;&#24223;&#29289;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#21355;&#26143;&#25552;&#20379;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#65292;&#36890;&#36807;&#36965;&#24863;&#22270;&#20687;&#23454;&#29616;&#20102;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#30340;&#35782;&#21035;&#12289;&#30417;&#27979;&#21644;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.09066</link><description>&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#22266;&#20307;&#24223;&#29289;&#26816;&#27979;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Solid Waste Detection in Remote Sensing Images: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22266;&#20307;&#24223;&#29289;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#21355;&#26143;&#25552;&#20379;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#65292;&#36890;&#36807;&#36965;&#24863;&#22270;&#20687;&#23454;&#29616;&#20102;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#30340;&#35782;&#21035;&#12289;&#30417;&#27979;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#34920;&#24449;&#38750;&#27861;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#23545;&#29615;&#22659;&#20445;&#25252;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#24212;&#23545;&#27745;&#26579;&#21644;&#20581;&#24247;&#21361;&#23475;&#12290;&#19981;&#24403;&#31649;&#29702;&#30340;&#22403;&#22334;&#22635;&#22475;&#22330;&#36890;&#36807;&#38632;&#27700;&#28183;&#36879;&#27745;&#26579;&#22303;&#22756;&#21644;&#22320;&#19979;&#27700;&#65292;&#23545;&#21160;&#29289;&#21644;&#20154;&#31867;&#26500;&#25104;&#23041;&#32961;&#12290;&#20256;&#32479;&#30340;&#22635;&#22475;&#22330;&#36776;&#35782;&#26041;&#27861;&#65292;&#22914;&#29616;&#22330;&#26816;&#26597;&#65292;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#36965;&#24863;&#25216;&#26415;&#26159;&#29992;&#20110;&#35782;&#21035;&#21644;&#30417;&#27979;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#30340;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#24191;&#27867;&#35206;&#30422;&#21644;&#22810;&#27425;&#33719;&#21462;&#12290;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#21355;&#26143;&#37197;&#22791;&#20102;&#19968;&#31995;&#21015;&#20256;&#24863;&#22120;&#21644;&#25104;&#20687;&#33021;&#21147;&#65292;&#20960;&#21313;&#24180;&#26469;&#19968;&#30452;&#25552;&#20379;&#39640;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19987;&#38376;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#36965;&#24863;&#22270;&#20687;&#25191;&#34892;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#22914;&#24223;&#29289;&#22330;&#22320;&#26816;&#27979;&#12289;&#20542;&#20498;&#22330;&#30417;&#27979;&#21644;&#36866;&#23452;&#20301;&#32622;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09066v1 Announce Type: cross Abstract: The detection and characterization of illegal solid waste disposal sites are essential for environmental protection, particularly for mitigating pollution and health hazards. Improperly managed landfills contaminate soil and groundwater via rainwater infiltration, posing threats to both animals and humans. Traditional landfill identification approaches, such as on-site inspections, are time-consuming and expensive. Remote sensing is a cost-effective solution for the identification and monitoring of solid waste disposal sites that enables broad coverage and repeated acquisitions over time. Earth Observation (EO) satellites, equipped with an array of sensors and imaging capabilities, have been providing high-resolution data for several decades. Researchers proposed specialized techniques that leverage remote sensing imagery to perform a range of tasks such as waste site detection, dumping site monitoring, and assessment of suitable locati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#31354;&#38388;&#25915;&#20987;&#26041;&#27861;&#65292;&#38024;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#25915;&#20987;&#65292;&#32469;&#36807;&#27169;&#22411;&#23545;&#40784;&#24182;&#22312;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#20449;&#24687;&#65292;&#27604;&#20256;&#32479;&#30340;&#31163;&#25955;&#25915;&#20987;&#26356;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.09063</link><description>&lt;p&gt;
&#36719;&#25552;&#31034;&#23041;&#32961;&#65306;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#25915;&#20987;&#21644;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09063
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#31354;&#38388;&#25915;&#20987;&#26041;&#27861;&#65292;&#38024;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#25915;&#20987;&#65292;&#32469;&#36807;&#27169;&#22411;&#23545;&#40784;&#24182;&#22312;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#20449;&#24687;&#65292;&#27604;&#20256;&#32479;&#30340;&#31163;&#25955;&#25915;&#20987;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#23545;LLMs&#30340;&#25932;&#23545;&#40065;&#26834;&#24615;&#30740;&#31350;&#19987;&#27880;&#20110;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#20013;&#30340;&#31163;&#25955;&#36755;&#20837;&#25805;&#32437;&#65292;&#36825;&#20123;&#25805;&#32437;&#21487;&#20197;&#30452;&#25509;&#36716;&#31227;&#21040;&#38381;&#28304;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#24320;&#28304;&#27169;&#22411;&#30340;&#25345;&#32493;&#36827;&#23637;&#12290;&#38543;&#30528;&#24320;&#28304;&#27169;&#22411;&#33021;&#21147;&#30340;&#25552;&#21319;&#65292;&#30830;&#20445;&#20854;&#23433;&#20840;&#24615;&#20063;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#24320;&#28304;LLMs&#30340;&#25915;&#20987;&#65292;&#21033;&#29992;&#23436;&#20840;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#30340;&#26041;&#24335;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#23884;&#20837;&#31354;&#38388;&#25915;&#20987;&#65292;&#30452;&#25509;&#25915;&#20987;&#36755;&#20837;&#20196;&#29260;&#30340;&#36830;&#32493;&#23884;&#20837;&#34920;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23884;&#20837;&#31354;&#38388;&#25915;&#20987;&#27604;&#31163;&#25955;&#25915;&#20987;&#25110;&#27169;&#22411;&#24494;&#35843;&#26356;&#26377;&#25928;&#22320;&#32469;&#36807;&#27169;&#22411;&#23545;&#40784;&#24182;&#35302;&#21457;&#26377;&#23475;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#36951;&#24536;&#30340;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23884;&#20837;&#31354;&#38388;&#25915;&#20987;&#22312;&#20174;&#26410;&#32463;&#23398;&#20064;&#30340;LLMs&#20013;&#25552;&#21462;&#24212;&#35813;&#21024;&#38500;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09063v1 Announce Type: new Abstract: Current research in adversarial robustness of LLMs focuses on discrete input manipulations in the natural language space, which can be directly transferred to closed-source models. However, this approach neglects the steady progression of open-source models. As open-source models advance in capability, ensuring their safety also becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose the embedding space attack, which directly attacks the continuous embedding representation of input tokens. We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Furthermore, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#21516;&#24577;&#21152;&#23494;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#24494;&#35843;&#30340;&#31995;&#32479;BlindTuner&#65292;&#23427;&#21487;&#20197;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#23545;Transformer&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#38750;&#21152;&#23494;&#27169;&#22411;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.09059</link><description>&lt;p&gt;
&#25105;&#30475;&#19981;&#35265;&#23427;&#65292;&#20294;&#25105;&#21487;&#20197;&#24494;&#35843;&#23427;&#65306;&#20351;&#29992;&#20840;&#21516;&#24577;&#21152;&#23494;&#23545;Transformer&#36827;&#34892;&#21152;&#23494;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
I can't see it but I can Fine-tune it: On Encrypted Fine-tuning of Transformers using Fully Homomorphic Encryption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#21516;&#24577;&#21152;&#23494;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#24494;&#35843;&#30340;&#31995;&#32479;BlindTuner&#65292;&#23427;&#21487;&#20197;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#23545;Transformer&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#38750;&#21152;&#23494;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#20849;&#20139;&#36935;&#21040;&#38556;&#30861;&#26102;&#65292;&#22914;&#20005;&#26684;&#30340;&#38544;&#31169;&#27861;&#35268;&#25110;&#29992;&#25143;&#23545;&#20010;&#20154;&#20449;&#24687;&#25259;&#38706;&#30340;&#25285;&#24551;&#65292;&#23601;&#20250;&#20986;&#29616;&#25361;&#25112;&#12290;&#26089;&#26399;&#22522;&#20110;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#65288;SMC&#65289;&#21644;&#20840;&#21516;&#24577;&#21152;&#23494;&#65288;FHE&#65289;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#65288;PPML&#65289;&#30340;&#24037;&#20316;&#26356;&#27880;&#37325;&#38544;&#31169;&#20445;&#25252;&#30340;&#25512;&#29702;&#32780;&#19981;&#26159;&#38544;&#31169;&#20445;&#25252;&#30340;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BlindTuner&#65292;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#24494;&#35843;&#31995;&#32479;&#65292;&#21487;&#23454;&#29616;&#23545;Transformer&#27169;&#22411;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;BlindTuner&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#19982;&#38750;&#21152;&#23494;&#27169;&#22411;&#30456;&#27604;&#20934;&#30830;&#24615;&#30456;&#24403;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09059v1 Announce Type: cross Abstract: In today's machine learning landscape, fine-tuning pretrained transformer models has emerged as an essential technique, particularly in scenarios where access to task-aligned training data is limited. However, challenges surface when data sharing encounters obstacles due to stringent privacy regulations or user apprehension regarding personal information disclosure. Earlier works based on secure multiparty computation (SMC) and fully homomorphic encryption (FHE) for privacy-preserving machine learning (PPML) focused more on privacy-preserving inference than privacy-preserving training. In response, we introduce BlindTuner, a privacy-preserving fine-tuning system that enables transformer training exclusively on homomorphically encrypted data for image classification. Our extensive experimentation validates BlindTuner's effectiveness by demonstrating comparable accuracy to non-encrypted models. Notably, our findings highlight a substantia
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#32420;&#32500;&#20256;&#24863;&#25216;&#26415;&#65292;&#29992;&#20110;&#26234;&#33021;&#26381;&#35013;&#20013;&#30340;&#36816;&#21160;&#36319;&#36394;&#21644;&#29983;&#29289;&#20449;&#21495;&#30417;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#34746;&#26059;&#36741;&#21161;&#24377;&#24615;&#32433;&#32447;&#20256;&#24863;&#22120;&#21644;&#29305;&#23450;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26381;&#35013;&#20013;&#39640;&#25928;&#22320;&#23454;&#29616;&#22810;&#21306;&#22495;&#30340;&#23616;&#37096;&#24212;&#21464;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.09057</link><description>&lt;p&gt;
&#26234;&#33021;&#26381;&#35013;&#20013;&#30340;&#20998;&#24067;&#24335;&#32420;&#32500;&#20256;&#24863;
&lt;/p&gt;
&lt;p&gt;
Distributed Sensing Along Fibres for Smart Clothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09057
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#32420;&#32500;&#20256;&#24863;&#25216;&#26415;&#65292;&#29992;&#20110;&#26234;&#33021;&#26381;&#35013;&#20013;&#30340;&#36816;&#21160;&#36319;&#36394;&#21644;&#29983;&#29289;&#20449;&#21495;&#30417;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#34746;&#26059;&#36741;&#21161;&#24377;&#24615;&#32433;&#32447;&#20256;&#24863;&#22120;&#21644;&#29305;&#23450;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26381;&#35013;&#20013;&#39640;&#25928;&#22320;&#23454;&#29616;&#22810;&#21306;&#22495;&#30340;&#23616;&#37096;&#24212;&#21464;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32442;&#32455;&#20256;&#24863;&#22120;&#23558;&#25105;&#20204;&#26085;&#24120;&#31359;&#30528;&#30340;&#34915;&#29289;&#23436;&#20840;&#26080;&#24178;&#25200;&#22320;&#36716;&#21270;&#20026;&#36861;&#36394;&#36816;&#21160;&#21644;&#29983;&#29289;&#20449;&#21495;&#30340;&#25163;&#27573;&#12290;&#20294;&#26159;&#65292;&#24403;&#20256;&#24863;&#22120;&#25968;&#37327;&#25193;&#22823;&#26102;&#65292;&#36830;&#25509;&#21644;&#31354;&#38388;&#25104;&#20026;&#37319;&#29992;&#8220;&#26234;&#33021;&#8221;&#26381;&#35013;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#29616;&#26377;&#30740;&#31350;&#20013;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#21018;&#24615;&#21644;&#32442;&#32455;&#20803;&#32032;&#20043;&#38388;&#30340;&#36830;&#25509;&#24120;&#24120;&#19981;&#21487;&#38752;&#65292;&#24182;&#19988;&#38656;&#35201;&#20197;&#19982;&#32442;&#32455;&#21697;&#22823;&#35268;&#27169;&#29983;&#20135;&#26041;&#27861;&#19981;&#20860;&#23481;&#30340;&#26041;&#24335;&#19982;&#20256;&#24863;&#22120;&#36827;&#34892;&#25509;&#21475;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21407;&#22411;&#26381;&#35013;&#12289;&#32039;&#20945;&#35835;&#20986;&#30005;&#36335;&#21644;&#31639;&#27861;&#65292;&#20197;&#27979;&#37327;&#32420;&#32500;&#22810;&#20010;&#21306;&#22495;&#30340;&#23616;&#37096;&#24212;&#21464;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#21487;&#35843;&#28789;&#25935;&#24230;&#30340;&#34746;&#26059;&#36741;&#21161;&#24377;&#24615;&#32433;&#32447;&#20256;&#24863;&#22120;&#65292;&#20197;&#36873;&#25321;&#24615;&#22320;&#23545;&#24212;&#21464;&#20449;&#21495;&#20316;&#20986;&#21453;&#24212;&#12290;&#25105;&#20204;&#22312;&#26381;&#35013;&#19978;&#23637;&#31034;&#20102;&#20998;&#24067;&#24335;&#24863;&#24212;&#65292;&#22312;&#21333;&#20010;&#36830;&#32493;&#32420;&#32500;&#19978;&#30417;&#27979;&#25163;&#33218;&#20851;&#33410;&#35282;&#24230;&#12290;&#19982;&#20809;&#23398;&#36816;&#21160;&#25429;&#25417;&#30456;&#27604;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#32422;5&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09057v1 Announce Type: cross Abstract: Textile sensors transform our everyday clothing into a means to track movement and bio-signals in a completely unobtrusive way. One major hindrance to the adoption of "smart" clothing is the difficulty encountered with connections and space when scaling up the number of sensors. There is a lack of research addressing a key limitation in wearable electronics: connections between rigid and textile elements are often unreliable and they require interfacing sensors in a way incompatible with textile mass production methods. We introduce a prototype garment, compact readout circuit, and algorithm to measure localized strain along multiple regions of a fibre. We employ a helical auxetic yarn sensor with tunable sensitivity along its length to selectively respond to strain signals. We demonstrate distributed sensing in clothing, monitoring arm joint angles from a single continuous fibre. Compared to optical motion capture, we achieve around 5{
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;, &#39640;&#20142;&#20102;&#22312;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#21644;&#35299;&#37322;&#24471;&#20986;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#19978;&#30340;&#22256;&#38590;&#24615;</title><link>https://arxiv.org/abs/2402.09056</link><description>&lt;p&gt;
&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26159;&#21542;&#20934;&#30830;&#22320;&#34920;&#31034;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;, &#39640;&#20142;&#20102;&#22312;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#21644;&#35299;&#37322;&#24471;&#20986;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#19978;&#30340;&#22256;&#38590;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#19981;&#20165;&#24212;&#36820;&#22238;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#36824;&#24212;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#24120;&#29992;&#20110;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#36817;&#24180;&#26469;&#65292;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#31561;&#26367;&#20195;&#26041;&#27861;&#20063;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#21518;&#32773;&#26412;&#36136;&#19978;&#25193;&#23637;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#32467;&#26524;&#30340;&#20108;&#38454;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#20013;&#21487;&#20197;&#25552;&#21462;&#35748;&#35782;&#65288;&#21644;&#38543;&#26426;&#65289;&#19981;&#30830;&#23450;&#24615;&#30340;&#24230;&#37327;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;&#65292;&#24378;&#35843;&#20102;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#35299;&#37322;&#32467;&#26524;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#24615;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#35774;&#32622;&#65292;&#28085;&#30422;&#20102;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#35745;&#25968;&#30340;&#24191;&#27867;&#26041;&#27861;&#65292;&#36825;&#31687;&#35770;&#25991;&#20026;&#21487;&#36776;&#35782;&#24615;&#21644;&#25910;&#25947;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09056v1 Announce Type: new Abstract: Trustworthy ML systems should not only return accurate predictions, but also a reliable representation of their uncertainty. Bayesian methods are commonly used to quantify both aleatoric and epistemic uncertainty, but alternative approaches, such as evidential deep learning methods, have become popular in recent years. The latter group of methods in essence extends empirical risk minimization (ERM) for predicting second-order probability distributions over outcomes, from which measures of epistemic (and aleatoric) uncertainty can be extracted. This paper presents novel theoretical insights of evidential deep learning, highlighting the difficulties in optimizing second-order loss functions and interpreting the resulting epistemic uncertainty measures. With a systematic setup that covers a wide range of approaches for classification, regression and counts, it provides novel insights into issues of identifiability and convergence in second-o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19982;&#36880;&#23618;&#35757;&#32451;&#30340;&#27604;&#36739;&#65292;&#37325;&#26032;&#32771;&#34385;&#20102;&#20026;&#20160;&#20040;&#31471;&#21040;&#31471;&#35757;&#32451;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20998;&#26512;&#20102;&#20449;&#24687;&#20256;&#25773;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#24182;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#36136;&#24046;&#24322;&#36827;&#34892;&#20102;&#28145;&#20837;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.09050</link><description>&lt;p&gt;
&#36890;&#36807;&#23618;&#35282;&#33394;&#24046;&#24322;&#21270;&#65292;&#31471;&#21040;&#31471;&#35757;&#32451;&#24341;&#21457;&#20449;&#24687;&#29942;&#39048;&#65306;&#19982;&#36880;&#23618;&#35757;&#32451;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
End-to-End Training Induces Information Bottleneck through Layer-Role Differentiation: A Comparative Analysis with Layer-wise Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19982;&#36880;&#23618;&#35757;&#32451;&#30340;&#27604;&#36739;&#65292;&#37325;&#26032;&#32771;&#34385;&#20102;&#20026;&#20160;&#20040;&#31471;&#21040;&#31471;&#35757;&#32451;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20998;&#26512;&#20102;&#20449;&#24687;&#20256;&#25773;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#24182;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#36136;&#24046;&#24322;&#36827;&#34892;&#20102;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#35757;&#32451;&#36890;&#36807;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#20248;&#21270;&#25972;&#20010;&#27169;&#22411;&#65292;&#20174;&#26681;&#26412;&#19978;&#25903;&#25345;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#12290;&#23613;&#31649;&#20854;&#24615;&#33021;&#24456;&#39640;&#65292;&#20294;E2E&#35757;&#32451;&#38754;&#20020;&#20869;&#23384;&#28040;&#32791;&#12289;&#24182;&#34892;&#35745;&#31639;&#21644;&#19982;&#23454;&#38469;&#22823;&#33041;&#21151;&#33021;&#30340;&#19981;&#19968;&#33268;&#31561;&#38382;&#39064;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#22256;&#38590;&#65292;&#28982;&#32780;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#31181;&#33021;&#22815;&#19982;E2E&#35757;&#32451;&#30340;&#24615;&#33021;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#22240;&#27492;&#22312;&#23454;&#29992;&#24615;&#19978;&#23384;&#22312;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#35757;&#32451;&#27169;&#22411;&#24615;&#36136;&#30340;&#24046;&#24322;&#22312;&#24615;&#33021;&#24046;&#36317;&#20043;&#22806;&#32570;&#20047;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#36880;&#23618;&#35757;&#32451;&#36827;&#34892;&#27604;&#36739;&#65292;&#37325;&#26032;&#32771;&#34385;&#20102;&#20026;&#20160;&#20040;E2E&#35757;&#32451;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#36880;&#23618;&#35757;&#32451;&#26159;&#19968;&#31181;&#23616;&#37096;&#35774;&#32622;&#38169;&#35823;&#30340;&#38750;E2E&#26041;&#27861;&#12290;&#22312;&#35266;&#23519;&#21040;E2E&#35757;&#32451;&#22312;&#20256;&#25773;&#36755;&#20837;&#20449;&#24687;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20013;&#38388;&#34920;&#31034;&#30340;&#20449;&#24687;&#24179;&#38754;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09050v1 Announce Type: new Abstract: End-to-end (E2E) training, optimizing the entire model through error backpropagation, fundamentally supports the advancements of deep learning. Despite its high performance, E2E training faces the problems of memory consumption, parallel computing, and discrepancy with the functionalities of the actual brain. Various alternative methods have been proposed to overcome these difficulties; however, no one can yet match the performance of E2E training, thereby falling short in practicality. Furthermore, there is no deep understanding regarding differences in the trained model properties beyond the performance gap. In this paper, we reconsider why E2E training demonstrates a superior performance through a comparison with layer-wise training, a non-E2E method that locally sets errors. On the basis of the observation that E2E training has an advantage in propagating input information, we analyze the information plane dynamics of intermediate rep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#36125;&#21494;&#26031;&#26041;&#27861;&#20026;&#22522;&#30784;&#30340;&#27010;&#29575;&#25512;&#26029;&#29702;&#35770;&#65292;&#29992;&#20110;&#25512;&#29702;&#21644;&#23398;&#20064;&#30340;&#32479;&#19968;&#29702;&#35770;&#12290;&#35813;&#29702;&#35770;&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#21270;&#20026;&#31526;&#21495;&#30693;&#35782;&#65292;&#20174;&#32780;&#23454;&#29616;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#23545;&#36923;&#36753;&#25512;&#29702;&#20851;&#31995;&#21644;MNIST&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.09046</link><description>&lt;p&gt;
&#25512;&#29702;&#21644;&#23398;&#20064;&#32479;&#19968;&#29702;&#35770;&#30340;&#25277;&#35937;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference of Abstraction for a Unified Account of Reasoning and Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#36125;&#21494;&#26031;&#26041;&#27861;&#20026;&#22522;&#30784;&#30340;&#27010;&#29575;&#25512;&#26029;&#29702;&#35770;&#65292;&#29992;&#20110;&#25512;&#29702;&#21644;&#23398;&#20064;&#30340;&#32479;&#19968;&#29702;&#35770;&#12290;&#35813;&#29702;&#35770;&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#21270;&#20026;&#31526;&#21495;&#30693;&#35782;&#65292;&#20174;&#32780;&#23454;&#29616;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#23545;&#36923;&#36753;&#25512;&#29702;&#20851;&#31995;&#21644;MNIST&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#31185;&#23398;&#20013;&#21463;&#21040;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#27010;&#29575;&#25512;&#26029;&#29702;&#35770;&#65292;&#29992;&#20110;&#25512;&#29702;&#21644;&#23398;&#20064;&#30340;&#32479;&#19968;&#29702;&#35770;&#12290;&#25105;&#20204;&#36890;&#36807;&#24418;&#24335;&#36923;&#36753;&#20013;&#30340;&#21487;&#28385;&#36275;&#24615;&#26469;&#24314;&#27169;&#25968;&#25454;&#22914;&#20309;&#23548;&#33268;&#31526;&#21495;&#30693;&#35782;&#12290;&#25512;&#29702;&#26159;&#36890;&#36807;&#25277;&#35937;&#65292;&#21363;&#36873;&#25321;&#24615;&#26080;&#30693;&#65292;&#20174;&#25968;&#25454;&#20013;&#25512;&#23548;&#20986;&#31526;&#21495;&#30693;&#35782;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36923;&#36753;&#25512;&#29702;&#20851;&#31995;&#30340;&#35777;&#26126;&#22522;&#30784;&#30340;&#29702;&#35770;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22522;&#20110;&#23454;&#39564;&#35777;&#25454;&#30340;MNIST&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09046v1 Announce Type: new Abstract: Inspired by Bayesian approaches to brain function in neuroscience, we give a simple theory of probabilistic inference for a unified account of reasoning and learning. We simply model how data cause symbolic knowledge in terms of its satisfiability in formal logic. The underlying idea is that reasoning is a process of deriving symbolic knowledge from data via abstraction, i.e., selective ignorance. The logical consequence relation is discussed for its proof-based theoretical correctness. The MNIST dataset is discussed for its experiment-based empirical correctness.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#25805;&#32437;&#19979;&#65292;&#26159;&#21542;&#26377;&#20123;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26356;&#38590;&#20197;&#23457;&#35745;&#65292;&#22312;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#20219;&#20309;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#35770;&#26159;&#20027;&#21160;&#36824;&#26159;&#38750;&#20027;&#21160;&#30340;&#23457;&#35745;&#31574;&#30053;&#37117;&#26080;&#27861;&#36229;&#36234;&#38543;&#26426;&#25277;&#26679;&#12290;&#30740;&#31350;&#21457;&#29616;&#23457;&#35745;&#30340;&#21487;&#25805;&#32437;&#24615;&#19982;&#30446;&#26631;&#27169;&#22411;&#30340;&#23481;&#37327;&#26377;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.09043</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#21487;&#23457;&#35745;&#24615;&#65306;&#22312;&#25805;&#32437;&#19979;&#65292;&#26159;&#21542;&#26377;&#20123;&#27169;&#22411;&#26356;&#38590;&#20197;&#23457;&#35745;&#65311;
&lt;/p&gt;
&lt;p&gt;
Under manipulations, are some AI models harder to audit?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#25805;&#32437;&#19979;&#65292;&#26159;&#21542;&#26377;&#20123;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26356;&#38590;&#20197;&#23457;&#35745;&#65292;&#22312;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#20219;&#20309;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#35770;&#26159;&#20027;&#21160;&#36824;&#26159;&#38750;&#20027;&#21160;&#30340;&#23457;&#35745;&#31574;&#30053;&#37117;&#26080;&#27861;&#36229;&#36234;&#38543;&#26426;&#25277;&#26679;&#12290;&#30740;&#31350;&#21457;&#29616;&#23457;&#35745;&#30340;&#21487;&#25805;&#32437;&#24615;&#19982;&#30446;&#26631;&#27169;&#22411;&#30340;&#23481;&#37327;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23457;&#35745;&#21592;&#38656;&#35201;&#31283;&#20581;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#32593;&#32476;&#24179;&#21488;&#26159;&#21542;&#31526;&#21512;&#27861;&#24459;&#35268;&#23450;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20182;&#20204;&#24456;&#38590;&#33719;&#24471;&#24179;&#21488;&#20351;&#29992;&#30340;&#31639;&#27861;&#12289;&#23454;&#29616;&#25110;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20010;&#38382;&#39064;&#27604;&#31616;&#21333;&#30340;&#24230;&#37327;&#20272;&#35745;&#26356;&#38590;&#12290;&#22312;&#26368;&#36817;&#30340;&#38450;&#25805;&#32437;&#23457;&#35745;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#36827;&#34892;&#31283;&#20581;&#23457;&#35745;&#30340;&#21487;&#34892;&#24615;&#65292;&#20854;&#20013;&#27169;&#22411;&#20855;&#26377;&#36739;&#22823;&#30340;&#23481;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#19968;&#20010;&#32422;&#26463;&#24615;&#32467;&#26524;&#65306;&#22914;&#26524;&#19968;&#20010;&#32593;&#32476;&#24179;&#21488;&#20351;&#29992;&#30340;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#20219;&#20309;&#25968;&#25454;&#65292;&#37027;&#20040;&#26080;&#35770;&#26159;&#20027;&#21160;&#36824;&#26159;&#38750;&#20027;&#21160;&#30340;&#23457;&#35745;&#31574;&#30053;&#65292;&#22312;&#20272;&#35745;&#35832;&#22914;&#20154;&#21475;&#24179;&#34913;&#24615;&#31561;&#24615;&#36136;&#26102;&#37117;&#26080;&#27861;&#36229;&#36234;&#38543;&#26426;&#25277;&#26679;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#26368;&#20808;&#36827;&#30340;&#23457;&#35745;&#25216;&#26415;&#22312;&#20309;&#31181;&#26465;&#20214;&#19979;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#25105;&#20204;&#28982;&#21518;&#36890;&#36807;&#20351;&#29992;Rademacher&#22797;&#26434;&#24230;&#23558;&#23457;&#35745;&#30340;&#21487;&#25805;&#32437;&#24615;&#19982;&#30446;&#26631;&#27169;&#22411;&#30340;&#23481;&#37327;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#36825;&#20123;&#32467;&#26524;&#22312;&#27969;&#34892;&#30340;&#22686;&#21152;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09043v1 Announce Type: new Abstract: Auditors need robust methods to assess the compliance of web platforms with the law. However, since they hardly ever have access to the algorithm, implementation, or training data used by a platform, the problem is harder than a simple metric estimation. Within the recent framework of manipulation-proof auditing, we study in this paper the feasibility of robust audits in realistic settings, in which models exhibit large capacities. We first prove a constraining result: if a web platform uses models that may fit any data, no audit strategy -- whether active or not -- can outperform random sampling when estimating properties such as demographic parity. To better understand the conditions under which state-of-the-art auditing techniques may remain competitive, we then relate the manipulability of audits to the capacity of the targeted models, using the Rademacher complexity. We empirically validate these results on popular models of increasi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#26041;Sigmoid TanH&#65288;SST&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#29992;&#20110;&#22686;&#24378;&#22312;&#25968;&#25454;&#38480;&#21046;&#19979;&#30340;&#39034;&#24207;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#25968;&#23398;&#24179;&#26041;&#25918;&#22823;&#24378;&#28608;&#27963;&#21644;&#24369;&#28608;&#27963;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25913;&#21892;&#26799;&#24230;&#27969;&#21644;&#20449;&#24687;&#36807;&#28388;&#12290;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#35780;&#20272;&#20102;SST&#39537;&#21160;&#30340;LSTM&#21644;GRU&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09034</link><description>&lt;p&gt;
&#20351;&#29992;&#24179;&#26041;Sigmoid TanH (SST)&#28608;&#27963;&#22312;&#25968;&#25454;&#38480;&#21046;&#19979;&#25552;&#39640;&#39034;&#24207;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST) Activation Under Data Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#26041;Sigmoid TanH&#65288;SST&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#29992;&#20110;&#22686;&#24378;&#22312;&#25968;&#25454;&#38480;&#21046;&#19979;&#30340;&#39034;&#24207;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#25968;&#23398;&#24179;&#26041;&#25918;&#22823;&#24378;&#28608;&#27963;&#21644;&#24369;&#28608;&#27963;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25913;&#21892;&#26799;&#24230;&#27969;&#21644;&#20449;&#24687;&#36807;&#28388;&#12290;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#35780;&#20272;&#20102;SST&#39537;&#21160;&#30340;LSTM&#21644;GRU&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#26469;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#21069;&#39304;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65292;&#20294;&#26159;&#39034;&#24207;&#27169;&#22411;&#22914;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#12289;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#20173;&#28982;&#20381;&#36182;&#20110;Sigmoid&#21644;TanH&#28608;&#27963;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20256;&#32479;&#30340;&#28608;&#27963;&#20989;&#25968;&#24120;&#24120;&#22312;&#35757;&#32451;&#22312;&#23567;&#39034;&#24207;&#25968;&#25454;&#38598;&#19978;&#26102;&#38590;&#20197;&#24314;&#27169;&#31232;&#30095;&#27169;&#24335;&#20197;&#26377;&#25928;&#25429;&#33719;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#21035;&#38024;&#23545;&#22312;&#25968;&#25454;&#38480;&#21046;&#19979;&#22686;&#24378;&#39034;&#24207;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#24179;&#26041;Sigmoid TanH&#65288;SST&#65289;&#28608;&#27963;&#12290;SST&#36890;&#36807;&#25968;&#23398;&#24179;&#26041;&#26469;&#25918;&#22823;&#24378;&#28608;&#27963;&#21644;&#24369;&#28608;&#27963;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#38543;&#30528;&#20449;&#21495;&#38543;&#26102;&#38388;&#20256;&#25773;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;&#26799;&#24230;&#27969;&#21644;&#20449;&#24687;&#36807;&#28388;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20351;&#29992;SST&#30340;LSTM&#21644;GRU&#27169;&#22411;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09034v1 Announce Type: cross Abstract: Activation functions enable neural networks to learn complex representations by introducing non-linearities. While feedforward models commonly use rectified linear units, sequential models like recurrent neural networks, long short-term memory (LSTMs) and gated recurrent units (GRUs) still rely on Sigmoid and TanH activation functions. However, these classical activation functions often struggle to model sparse patterns when trained on small sequential datasets to effectively capture temporal dependencies. To address this limitation, we propose squared Sigmoid TanH (SST) activation specifically tailored to enhance the learning capability of sequential models under data constraints. SST applies mathematical squaring to amplify differences between strong and weak activations as signals propagate over time, facilitating improved gradient flow and information filtering. We evaluate SST-powered LSTMs and GRUs for diverse applications, such a
&lt;/p&gt;</description></item><item><title>SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09025</link><description>&lt;p&gt;
SLEB: &#36890;&#36807;&#20887;&#20313;&#39564;&#35777;&#21644;&#28040;&#38500;Transformer&#22359;&#20248;&#21270;LLM&#30340;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09025
&lt;/p&gt;
&lt;p&gt;
SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#32473;&#23454;&#38469;&#37096;&#32626;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#31934;&#31616;&#65292;&#19968;&#31181;&#26088;&#22312;&#20943;&#23567;LLM&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20174;&#32593;&#32476;&#20013;&#21024;&#38500;&#20887;&#20313;&#32452;&#20214;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#31934;&#31616;&#26377;&#24076;&#26395;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31471;&#21040;&#31471;LLM&#25512;&#29702;&#21152;&#36895;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SLEB&#65292;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36873;&#25321;Transformer&#22359;&#20316;&#20026;&#31934;&#31616;&#30340;&#22522;&#26412;&#21333;&#20301;&#65292;&#22240;&#20026;LLM&#22312;&#30456;&#37051;&#22359;&#30340;&#36755;&#20986;&#20043;&#38388;&#20855;&#26377;&#22359;&#32423;&#21035;&#30340;&#20887;&#20313;&#21644;&#39640;&#30456;&#20284;&#24615;&#12290;&#36825;&#20010;&#36873;&#25321;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#22686;&#24378;LLM&#30340;&#22788;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SLEB&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09025v1 Announce Type: new Abstract: Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB successfully accelerates LLM inference without
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Energy-consistent Neural Operators (ENOs)&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36981;&#24490;&#33021;&#37327;&#23432;&#24658;&#25110;&#32791;&#25955;&#23450;&#24459;&#30340;PDE&#35299;&#31639;&#23376;&#12290;&#36890;&#36807;&#24341;&#20837;&#21463;&#29289;&#29702;&#23398;&#33021;&#37327;&#29702;&#35770;&#21551;&#21457;&#30340;&#24809;&#32602;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#21478;&#19968;&#20010;DNN&#27169;&#22411;&#24314;&#27169;&#33021;&#37327;&#20989;&#25968;&#65292;&#21487;&#20197;&#30830;&#20445;DNN&#35299;&#31639;&#23376;&#30340;&#36755;&#20986;&#28385;&#36275;&#33021;&#37327;&#19968;&#33268;&#24615;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30340;PDEs&#12290;&#23454;&#39564;&#35777;&#26126;ENO&#22312;&#22810;&#20010;&#29289;&#29702;&#31995;&#32479;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20197;&#24448;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09018</link><description>&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#36935;&#19978;&#33021;&#37327;&#29702;&#35770;: &#21704;&#23494;&#39039;&#21644;&#32791;&#25955;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neural Operators Meet Energy-based Theory: Operator Learning for Hamiltonian and Dissipative PDEs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Energy-consistent Neural Operators (ENOs)&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36981;&#24490;&#33021;&#37327;&#23432;&#24658;&#25110;&#32791;&#25955;&#23450;&#24459;&#30340;PDE&#35299;&#31639;&#23376;&#12290;&#36890;&#36807;&#24341;&#20837;&#21463;&#29289;&#29702;&#23398;&#33021;&#37327;&#29702;&#35770;&#21551;&#21457;&#30340;&#24809;&#32602;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#21478;&#19968;&#20010;DNN&#27169;&#22411;&#24314;&#27169;&#33021;&#37327;&#20989;&#25968;&#65292;&#21487;&#20197;&#30830;&#20445;DNN&#35299;&#31639;&#23376;&#30340;&#36755;&#20986;&#28385;&#36275;&#33021;&#37327;&#19968;&#33268;&#24615;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30340;PDEs&#12290;&#23454;&#39564;&#35777;&#26126;ENO&#22312;&#22810;&#20010;&#29289;&#29702;&#31995;&#32479;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20197;&#24448;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31639;&#23376;&#23398;&#20064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#26088;&#22312;&#23398;&#20064;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23398;&#20064;&#36825;&#31181;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#23545;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#30340;&#35299;&#31639;&#23376;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#23398;&#20064;&#36981;&#23432;&#29289;&#29702;&#35268;&#24459;&#30340;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#19968;&#33268;&#31070;&#32463;&#31639;&#23376;(ENOs)&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36981;&#24490;&#33021;&#37327;&#23432;&#24658;&#25110;&#32791;&#25955;&#23450;&#24459;&#30340;PDE&#35299;&#31639;&#23376;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#23398;&#33021;&#37327;&#29702;&#35770;&#21551;&#21457;&#30340;&#24809;&#32602;&#20989;&#25968;&#29992;&#20110;&#35757;&#32451;&#65292;&#20854;&#20013;&#33021;&#37327;&#20989;&#25968;&#30001;&#21478;&#19968;&#20010;DNN&#26469;&#24314;&#27169;&#65292;&#20351;&#24471;&#22522;&#20110;DNN&#30340;&#35299;&#31639;&#23376;&#30340;&#36755;&#20986;&#33021;&#22815;&#20445;&#35777;&#33021;&#37327;&#19968;&#33268;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#26174;&#24335;&#30340;PDEs&#12290;&#22312;&#22810;&#20010;&#29289;&#29702;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;ENO&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20197;&#24448;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09018v1 Announce Type: cross Abstract: The operator learning has received significant attention in recent years, with the aim of learning a mapping between function spaces. Prior works have proposed deep neural networks (DNNs) for learning such a mapping, enabling the learning of solution operators of partial differential equations (PDEs). However, these works still struggle to learn dynamics that obeys the laws of physics. This paper proposes Energy-consistent Neural Operators (ENOs), a general framework for learning solution operators of PDEs that follows the energy conservation or dissipation law from observed solution trajectories. We introduce a novel penalty function inspired by the energy-based theory of physics for training, in which the energy functional is modeled by another DNN, allowing one to bias the outputs of the DNN-based solution operators to ensure energetic consistency without explicit PDEs. Experiments on multiple physical systems show that ENO outperfor
&lt;/p&gt;</description></item><item><title>&#22312;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GAP&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#23545;&#40784;&#21644;&#21407;&#22411;&#29305;&#24449;&#65292;&#20943;&#36731;&#20102;&#26469;&#33258;&#20110;&#38169;&#35823;&#20998;&#31867;&#20266;&#26631;&#31614;&#29109;&#26368;&#23567;&#21270;&#25439;&#22833;&#30340;&#19981;&#36866;&#24403;&#24341;&#23548;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;TTA&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09004</link><description>&lt;p&gt;
&#20351;&#29992;&#21407;&#22411;&#29305;&#24449;&#36827;&#34892;&#26799;&#24230;&#23545;&#40784;&#30340;&#23436;&#20840;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Gradient Alignment with Prototype Feature for Fully Test-time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09004
&lt;/p&gt;
&lt;p&gt;
&#22312;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GAP&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#23545;&#40784;&#21644;&#21407;&#22411;&#29305;&#24449;&#65292;&#20943;&#36731;&#20102;&#26469;&#33258;&#20110;&#38169;&#35823;&#20998;&#31867;&#20266;&#26631;&#31614;&#29109;&#26368;&#23567;&#21270;&#25439;&#22833;&#30340;&#19981;&#36866;&#24403;&#24341;&#23548;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;TTA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#26799;&#24230;&#23545;&#40784;&#19982;&#21407;&#22411;&#29305;&#24449;&#65288;GAP&#65289;&#65292;&#23427;&#21487;&#20197;&#20943;&#36731;&#30001;&#20110;&#38169;&#35823;&#20998;&#31867;&#20266;&#26631;&#31614;&#30340;&#29109;&#26368;&#23567;&#21270;&#25439;&#22833;&#32780;&#23548;&#33268;&#30340;&#19981;&#36866;&#24403;&#24341;&#23548;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26799;&#24230;&#23545;&#40784;&#25439;&#22833;&#65292;&#20197;&#31934;&#30830;&#22320;&#31649;&#29702;&#36866;&#24212;&#36807;&#31243;&#65292;&#30830;&#20445;&#23545;&#26576;&#20123;&#25968;&#25454;&#36827;&#34892;&#30340;&#26356;&#25913;&#19981;&#20250;&#23545;&#27169;&#22411;&#22312;&#20854;&#20182;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31867;&#30340;&#21407;&#22411;&#29305;&#24449;&#20316;&#20026;&#36127;&#38754;&#24433;&#21709;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;&#20026;&#20102;&#20351;&#22312;TTA&#32422;&#26463;&#19979;GAP&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#34892;&#65292;&#21363;&#27169;&#22411;&#21482;&#33021;&#35775;&#38382;&#27809;&#26377;&#26631;&#31614;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#20462;&#25913;&#20102;&#20854;&#20844;&#24335;&#65306;&#29992;&#20998;&#31867;&#22120;&#30340;&#26435;&#37325;&#21521;&#37327;&#36817;&#20284;&#21407;&#22411;&#29305;&#24449;&#65292;&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#35745;&#31639;&#26799;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GAP&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25913;&#21892;&#20102;TTA&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#22810;&#21151;&#33021;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09004v1 Announce Type: cross Abstract: In context of Test-time Adaptation(TTA), we propose a regularizer, dubbed Gradient Alignment with Prototype feature (GAP), which alleviates the inappropriate guidance from entropy minimization loss from misclassified pseudo label. We developed a gradient alignment loss to precisely manage the adaptation process, ensuring that changes made for some data don't negatively impact the model's performance on other data. We introduce a prototype feature of a class as a proxy measure of the negative impact. To make GAP regularizer feasible under the TTA constraints, where model can only access test data without labels, we tailored its formula in two ways: approximating prototype features with weight vectors of the classifier, calculating gradient without back-propagation. We demonstrate GAP significantly improves TTA methods across various datasets, which proves its versatility and effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#32852;&#21512;&#28145;&#24230;&#23398;&#20064;&#22312;&#25918;&#23556;&#27835;&#30103;&#25968;&#25454;&#20013;&#26631;&#20934;&#21270;&#21629;&#21517;&#32422;&#23450;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21435;&#20013;&#24515;&#21270;&#23454;&#26102;&#25968;&#25454;&#21644;&#32852;&#37030;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22810;&#26426;&#26500;&#20013;&#24515;&#30340;&#25968;&#25454;&#26631;&#20934;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08999</link><description>&lt;p&gt;
&#22312;&#25918;&#23556;&#27835;&#30103;&#25968;&#25454;&#20013;&#25506;&#32034;&#32852;&#21512;&#28145;&#24230;&#23398;&#20064;&#20197;&#26631;&#20934;&#21270;&#21629;&#21517;&#32422;&#23450;
&lt;/p&gt;
&lt;p&gt;
Exploring Federated Deep Learning for Standardising Naming Conventions in Radiotherapy Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#32852;&#21512;&#28145;&#24230;&#23398;&#20064;&#22312;&#25918;&#23556;&#27835;&#30103;&#25968;&#25454;&#20013;&#26631;&#20934;&#21270;&#21629;&#21517;&#32422;&#23450;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21435;&#20013;&#24515;&#21270;&#23454;&#26102;&#25968;&#25454;&#21644;&#32852;&#37030;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22810;&#26426;&#26500;&#20013;&#24515;&#30340;&#25968;&#25454;&#26631;&#20934;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#21270;&#25918;&#23556;&#27835;&#30103;&#65288;RT&#65289;&#25968;&#25454;&#20013;&#30340;&#32467;&#26500;&#20307;&#31215;&#21517;&#31216;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#25366;&#25496;&#21644;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#36328;&#26426;&#26500;&#20013;&#24515;&#12290;&#36825;&#20010;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#36825;&#31361;&#26174;&#20102;&#22788;&#29702;&#20219;&#21153;&#30340;&#26032;&#30340;&#33258;&#21160;&#21270;&#21644;&#39640;&#25928;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#24050;&#32463;&#25552;&#20986;&#21644;&#35780;&#20272;&#20102;&#20960;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#26631;&#20934;&#21270;&#21629;&#21517;&#32422;&#23450;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#30740;&#31350;&#32771;&#34385;&#21040;RT&#24739;&#32773;&#35760;&#24405;&#20998;&#24067;&#22312;&#22810;&#20010;&#25968;&#25454;&#20013;&#24515;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27169;&#25311;&#30495;&#23454;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#24314;&#31435;&#26631;&#20934;&#21270;&#21629;&#21517;&#32422;&#23450;&#12290;&#36890;&#36807;&#38598;&#25104;&#21435;&#20013;&#24515;&#21270;&#23454;&#26102;&#25968;&#25454;&#21644;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#26631;&#20934;&#21270;RT&#25968;&#25454;&#12290;&#20174;&#32467;&#26500;&#20013;&#25552;&#21462;&#20102;&#19977;&#31181;&#21487;&#33021;&#30340;&#23646;&#24615;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65306;&#34920;&#26684;&#12289;&#35270;&#35273;&#21644;&#20307;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08999v1 Announce Type: new Abstract: Standardising structure volume names in radiotherapy (RT) data is necessary to enable data mining and analyses, especially across multi-institutional centres. This process is time and resource intensive, which highlights the need for new automated and efficient approaches to handle the task. Several machine learning-based methods have been proposed and evaluated to standardise nomenclature. However, no studies have considered that RT patient records are distributed across multiple data centres. This paper introduces a method that emulates real-world environments to establish standardised nomenclature. This is achieved by integrating decentralised real-time data and federated learning (FL). A multimodal deep artificial neural network was proposed to standardise RT data in federated settings. Three types of possible attributes were extracted from the structures to train the deep learning models: tabular, visual, and volumetric. Simulated ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32447;&#24615;&#28151;&#21512;&#36716;&#31227;&#26680;&#20989;&#25968;&#30340;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#26032;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#24102;&#26377;&#26041;&#24046;&#24863;&#30693;&#32622;&#20449;&#21306;&#38388;&#30340;&#25193;&#23637;&#20540;&#36845;&#20195;&#65292;&#24182;&#36890;&#36807;&#39640;&#38454;&#30697;&#30340;&#36882;&#24402;&#20272;&#35745;&#23454;&#29616;&#20102;&#36817;&#20284;&#26368;&#23567;&#26368;&#22823;&#36951;&#25022;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.08998</link><description>&lt;p&gt;
&#23398;&#20064;&#32447;&#24615;&#28151;&#21512;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;&#30340;&#36817;&#20284;&#26368;&#23567;&#26368;&#22823;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Nearly Minimax Optimal Regret for Learning Linear Mixture Stochastic Shortest Path
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32447;&#24615;&#28151;&#21512;&#36716;&#31227;&#26680;&#20989;&#25968;&#30340;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#26032;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#24102;&#26377;&#26041;&#24046;&#24863;&#30693;&#32622;&#20449;&#21306;&#38388;&#30340;&#25193;&#23637;&#20540;&#36845;&#20195;&#65292;&#24182;&#36890;&#36807;&#39640;&#38454;&#30697;&#30340;&#36882;&#24402;&#20272;&#35745;&#23454;&#29616;&#20102;&#36817;&#20284;&#26368;&#23567;&#26368;&#22823;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#32447;&#24615;&#28151;&#21512;&#36716;&#31227;&#26680;&#20989;&#25968;&#30340;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;&#65288;SSP&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#37325;&#22797;&#19982;&#38543;&#26426;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#23547;&#27714;&#36798;&#21040;&#29305;&#23450;&#30446;&#26631;&#29366;&#24577;&#21516;&#26102;&#26368;&#23567;&#21270;&#32047;&#31215;&#25104;&#26412;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20551;&#35774;&#25104;&#26412;&#20989;&#25968;&#20855;&#26377;&#20005;&#26684;&#30340;&#27491;&#19979;&#30028;&#65292;&#25110;&#32773;&#26399;&#26395;&#38271;&#24230;&#30340;&#26368;&#20248;&#31574;&#30053;&#20855;&#26377;&#19978;&#30028;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26469;&#28040;&#38500;&#36825;&#20123;&#38480;&#21046;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#24102;&#26377;&#31934;&#32454;&#21270;&#26041;&#24046;&#24863;&#30693;&#32622;&#20449;&#21306;&#38388;&#30340;&#25193;&#23637;&#20540;&#36845;&#20195;&#65292;&#20854;&#20013;&#26041;&#24046;&#20174;&#39640;&#38454;&#30697;&#36882;&#24402;&#20272;&#35745;&#24471;&#21040;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;$\tilde{\mathcal O}(dB_*\sqrt{K})$ &#30340;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;$d$ &#26159;&#32447;&#24615;&#36716;&#31227;&#26680;&#20989;&#25968;&#20013;&#29305;&#24449;&#26144;&#23556;&#30340;&#32500;&#24230;&#65292;$B_*$ &#26159;&#26368;&#20248;&#31574;&#30053;&#30340;&#24635;&#32047;&#31215;&#25104;&#26412;&#30340;&#19978;&#30028;&#65292;$K$ &#26159;&#21095;&#38598;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#36951;&#25022;&#19978;&#30028;&#19982;$\Omega(.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08998v1 Announce Type: new Abstract: We study the Stochastic Shortest Path (SSP) problem with a linear mixture transition kernel, where an agent repeatedly interacts with a stochastic environment and seeks to reach certain goal state while minimizing the cumulative cost. Existing works often assume a strictly positive lower bound of the cost function or an upper bound of the expected length for the optimal policy. In this paper, we propose a new algorithm to eliminate these restrictive assumptions. Our algorithm is based on extended value iteration with a fine-grained variance-aware confidence set, where the variance is estimated recursively from high-order moments. Our algorithm achieves an $\tilde{\mathcal O}(dB_*\sqrt{K})$ regret bound, where $d$ is the dimension of the feature mapping in the linear transition kernel, $B_*$ is the upper bound of the total cumulative cost for the optimal policy, and $K$ is the number of episodes. Our regret upper bound matches the $\Omega(
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36817;&#31471;&#28857;&#26041;&#27861;&#36827;&#34892;&#38543;&#26426;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24369;&#26465;&#20214;&#19979;&#33719;&#24471;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#23454;&#29616;&#26041;&#24046;&#20943;&#23569;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.08992</link><description>&lt;p&gt;
&#36890;&#36807;&#36817;&#31471;&#28857;&#26041;&#27861;&#36827;&#34892;&#38543;&#26426;&#20248;&#21270;&#20013;&#30340;&#26041;&#24046;&#20943;&#23569;&#21644;&#20302;&#26679;&#26412;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Variance Reduction and Low Sample Complexity in Stochastic Optimization via Proximal Point Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36817;&#31471;&#28857;&#26041;&#27861;&#36827;&#34892;&#38543;&#26426;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24369;&#26465;&#20214;&#19979;&#33719;&#24471;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#23454;&#29616;&#26041;&#24046;&#20943;&#23569;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#36817;&#31471;&#28857;&#27861;&#26469;&#35299;&#20915;&#38543;&#26426;&#20984;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#38543;&#26426;&#20248;&#21270;&#20013;&#30340;&#39640;&#27010;&#29575;&#32467;&#26524;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20363;&#22914;&#23376;&#39640;&#26031;&#20998;&#24067;&#12290;&#26412;&#25991;&#21482;&#20551;&#35774;&#20102;&#38543;&#26426;&#26799;&#24230;&#30340;&#26377;&#30028;&#26041;&#24046;&#31561;&#24369;&#26465;&#20214;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#20197;&#33719;&#24471;&#20851;&#20110;&#25152;&#25552;&#26041;&#27861;&#25910;&#25947;&#30340;&#39640;&#27010;&#29575;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#26412;&#24037;&#20316;&#30340;&#19968;&#20010;&#26174;&#33879;&#26041;&#38754;&#26159;&#21457;&#23637;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#36817;&#31471;&#23376;&#38382;&#39064;&#30340;&#23376;&#31243;&#24207;&#65292;&#23427;&#21516;&#26102;&#20063;&#26159;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#26041;&#24046;&#30340;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08992v1 Announce Type: cross Abstract: This paper proposes a stochastic proximal point method to solve a stochastic convex composite optimization problem. High probability results in stochastic optimization typically hinge on restrictive assumptions on the stochastic gradient noise, for example, sub-Gaussian distributions. Assuming only weak conditions such as bounded variance of the stochastic gradient, this paper establishes a low sample complexity to obtain a high probability guarantee on the convergence of the proposed method. Additionally, a notable aspect of this work is the development of a subroutine to solve the proximal subproblem, which also serves as a novel technique for variance reduction.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#20581;&#22766;&#30340;&#20048;&#35266;MLE&#65288;CR-OMLE&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#30772;&#22351;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#23545;&#36716;&#31227;&#27169;&#22411;&#30340;&#20581;&#22766;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.08991</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#25239;&#24615;&#30772;&#22351;&#30340;&#20581;&#22766;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#20581;&#22766;&#30340;&#20048;&#35266;MLE&#65288;CR-OMLE&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#30772;&#22351;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#23545;&#36716;&#31227;&#27169;&#22411;&#30340;&#20581;&#22766;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#30772;&#22351;&#30340;&#25361;&#25112;&#65292;&#20854;&#20013;&#36716;&#31227;&#21160;&#21147;&#23398;&#21487;&#20197;&#34987;&#23545;&#25163;&#30772;&#22351;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#26080;&#20851;&#24378;&#21270;&#23398;&#20064;&#30340;&#24773;&#26223;&#19979;&#65292;&#36890;&#24120;&#37319;&#29992;&#20581;&#22766;&#30340;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#26469;&#36827;&#34892;&#20540;&#20989;&#25968;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#37319;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#26041;&#27861;&#26469;&#23398;&#20064;&#36716;&#31227;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#28085;&#30422;&#20102;&#22312;&#32447;&#21644;&#31163;&#32447;&#20004;&#31181;&#24773;&#20917;&#12290;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#25239;&#24615;&#20581;&#22766;&#30340;&#20048;&#35266;MLE&#65288;CR-OMLE&#65289;&#30340;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#30340;&#20449;&#24687;&#27604;&#29575;&#20316;&#20026;MLE&#30340;&#19981;&#30830;&#23450;&#26435;&#37325;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CR-OMLE&#30340;&#36951;&#25022;&#24230;&#20026;$ \tilde {\mathcal {O}}&#65288;\sqrt {T} + C&#65289;$&#65292;&#20854;&#20013;$ C $&#34920;&#31034;&#32463;&#36807;$ T $&#20010;&#22238;&#21512;&#21518;&#30340;&#32047;&#35745;&#30772;&#22351;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08991v1 Announce Type: cross Abstract: This study tackles the challenges of adversarial corruption in model-based reinforcement learning (RL), where the transition dynamics can be corrupted by an adversary. Existing studies on corruption-robust RL mostly focus on the setting of model-free RL, where robust least-square regression is often employed for value function estimation. However, these techniques cannot be directly applied to model-based RL. In this paper, we focus on model-based RL and take the maximum likelihood estimation (MLE) approach to learn transition model. Our work encompasses both online and offline settings. In the online setting, we introduce an algorithm called corruption-robust optimistic MLE (CR-OMLE), which leverages total-variation (TV)-based information ratios as uncertainty weights for MLE. We prove that CR-OMLE achieves a regret of $\tilde{\mathcal{O}}(\sqrt{T} + C)$, where $C$ denotes the cumulative corruption level after $T$ episodes. We also pro
&lt;/p&gt;</description></item><item><title>MEL&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#36827;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#20849;&#20139;&#26469;&#22686;&#24378;&#29305;&#24449;&#36873;&#25321;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#29305;&#24449;&#36873;&#25321;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08982</link><description>&lt;p&gt;
MEL: &#39640;&#32500;&#29305;&#24449;&#36873;&#25321;&#30340;&#39640;&#25928;&#22810;&#20219;&#21153;&#36827;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MEL: Efficient Multi-Task Evolutionary Learning for High-Dimensional Feature Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08982
&lt;/p&gt;
&lt;p&gt;
MEL&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#36827;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#20849;&#20139;&#26469;&#22686;&#24378;&#29305;&#24449;&#36873;&#25321;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#29305;&#24449;&#36873;&#25321;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#26159;&#25968;&#25454;&#25366;&#25496;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#36890;&#36807;&#38477;&#20302;&#25968;&#25454;&#32500;&#24230;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#30340;&#32500;&#24230;&#36234;&#26469;&#36234;&#39640;&#65292;&#20351;&#24471;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#32500;&#24230;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#36866;&#29992;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#19981;&#21516;&#30340;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#30340;&#22810;&#26679;&#35774;&#35745;&#23548;&#33268;&#20854;&#22788;&#29702;&#19981;&#21516;&#25968;&#25454;&#30340;&#33021;&#21147;&#19981;&#23613;&#30456;&#21516;&#65292;&#32463;&#24120;&#26080;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#21644;&#20849;&#20139;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PSO-based Multi-task Evolutionary Learning (MEL)&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#65292;MEL&#23454;&#29616;&#20102;&#22686;&#24378;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;MEL&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08982v1 Announce Type: cross Abstract: Feature selection is a crucial step in data mining to enhance model performance by reducing data dimensionality. However, the increasing dimensionality of collected data exacerbates the challenge known as the "curse of dimensionality", where computation grows exponentially with the number of dimensions. To tackle this issue, evolutionary computational (EC) approaches have gained popularity due to their simplicity and applicability. Unfortunately, the diverse designs of EC methods result in varying abilities to handle different data, often underutilizing and not sharing information effectively. In this paper, we propose a novel approach called PSO-based Multi-task Evolutionary Learning (MEL) that leverages multi-task learning to address these challenges. By incorporating information sharing between different feature selection tasks, MEL achieves enhanced learning ability and efficiency. We evaluate the effectiveness of MEL through extens
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;&#24322;&#26500;&#22270;&#35843;&#24230;&#22120;&#65288;HGS&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#28789;&#27963;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23616;&#37096;&#20851;&#31995;&#30693;&#35782;&#36827;&#34892;&#35843;&#24230;&#65292;&#24182;&#37319;&#29992;&#22270;&#32467;&#26500;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35268;&#27169;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08979</link><description>&lt;p&gt;
&#23398;&#20064;&#39537;&#21160;&#30340;&#28789;&#27963;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#29992;&#20110;&#21487;&#25193;&#23637;&#26234;&#33021;&#21046;&#36896;
&lt;/p&gt;
&lt;p&gt;
Learning-enabled Flexible Job-shop Scheduling for Scalable Smart Manufacturing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;&#24322;&#26500;&#22270;&#35843;&#24230;&#22120;&#65288;HGS&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#28789;&#27963;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23616;&#37096;&#20851;&#31995;&#30693;&#35782;&#36827;&#34892;&#35843;&#24230;&#65292;&#24182;&#37319;&#29992;&#22270;&#32467;&#26500;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35268;&#27169;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#65292;&#32771;&#34385;&#21040;&#22522;&#20110;&#33258;&#21160;&#23548;&#24341;&#36710;&#36742;&#65288;AGV&#65289;&#30340;&#29983;&#20135;&#28789;&#27963;&#24615;&#65292;&#20855;&#26377;&#36816;&#36755;&#32422;&#26463;&#30340;&#28789;&#27963;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#65288;FJSPT&#65289;&#26159;&#20248;&#21270;&#26368;&#22823;&#21270;&#29983;&#20135;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;FJSPT&#26041;&#27861;&#22312;&#35268;&#27169;&#27867;&#21270;&#26041;&#38754;&#36935;&#21040;&#20102;&#25361;&#25112;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#19982;&#20854;&#35757;&#32451;&#38598;&#19981;&#21516;&#35268;&#27169;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#23548;&#33268;&#20302;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;DRL&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#24322;&#26500;&#22270;&#35843;&#24230;&#22120;&#65288;HGS&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22312;&#25805;&#20316;&#12289;&#26426;&#22120;&#21644;&#36710;&#36742;&#33410;&#28857;&#20043;&#38388;&#25552;&#21462;&#30340;&#23616;&#37096;&#20851;&#31995;&#30693;&#35782;&#36827;&#34892;&#35843;&#24230;&#65292;&#37319;&#29992;&#22270;&#32467;&#26500;&#30340;&#20915;&#31574;&#26694;&#26550;&#26469;&#38477;&#20302;&#32534;&#30721;&#22797;&#26434;&#24615;&#24182;&#22686;&#24378;&#35268;&#27169;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#22312;&#35268;&#27169;&#27867;&#21270;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08979v1 Announce Type: cross Abstract: In smart manufacturing systems (SMSs), flexible job-shop scheduling with transportation constraints (FJSPT) is essential to optimize solutions for maximizing productivity, considering production flexibility based on automated guided vehicles (AGVs). Recent developments in deep reinforcement learning (DRL)-based methods for FJSPT have encountered a scale generalization challenge. These methods underperform when applied to environment at scales different from their training set, resulting in low-quality solutions. To address this, we introduce a novel graph-based DRL method, named the Heterogeneous Graph Scheduler (HGS). Our method leverages locally extracted relational knowledge among operations, machines, and vehicle nodes for scheduling, with a graph-structured decision-making framework that reduces encoding complexity and enhances scale generalization. Our performance evaluation, conducted with benchmark datasets, reveals that the pro
&lt;/p&gt;</description></item><item><title>Prismatic&#26159;&#19968;&#20010;&#38598;&#25104;&#21382;&#21490;&#25968;&#25454;&#20998;&#26512;&#21644;&#19994;&#21153;&#20851;&#31995;&#30693;&#35782;&#30340;&#20132;&#20114;&#24335;&#22810;&#35270;&#35282;&#27010;&#24565;&#32929;&#38598;&#32676;&#20998;&#26512;&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#35270;&#35282;&#38598;&#32676;&#20998;&#26512;&#26041;&#27861;&#65292;&#20016;&#23500;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#38598;&#32676;&#65292;&#24182;&#25552;&#20379;&#20102;&#32454;&#33268;&#30340;&#19994;&#21153;&#30456;&#20851;&#24615;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.08978</link><description>&lt;p&gt;
Prismatic:&#20132;&#20114;&#24335;&#22810;&#35270;&#35282;&#27010;&#24565;&#32929;&#38598;&#32676;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08978
&lt;/p&gt;
&lt;p&gt;
Prismatic&#26159;&#19968;&#20010;&#38598;&#25104;&#21382;&#21490;&#25968;&#25454;&#20998;&#26512;&#21644;&#19994;&#21153;&#20851;&#31995;&#30693;&#35782;&#30340;&#20132;&#20114;&#24335;&#22810;&#35270;&#35282;&#27010;&#24565;&#32929;&#38598;&#32676;&#20998;&#26512;&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#35270;&#35282;&#38598;&#32676;&#20998;&#26512;&#26041;&#27861;&#65292;&#20016;&#23500;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#38598;&#32676;&#65292;&#24182;&#25552;&#20379;&#20102;&#32454;&#33268;&#30340;&#19994;&#21153;&#30456;&#20851;&#24615;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08978v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25688;&#35201;:&#37329;&#34701;&#38598;&#32676;&#20998;&#26512;&#20351;&#25237;&#36164;&#32773;&#33021;&#22815;&#21457;&#29616;&#25237;&#36164;&#26367;&#20195;&#21697;&#65292;&#24182;&#36991;&#20813;&#25215;&#25285;&#36807;&#39640;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20998;&#26512;&#20219;&#21153;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#22823;&#37327;&#30340;&#20004;&#20004;&#27604;&#36739;&#12289;&#26102;&#38388;&#36328;&#24230;&#30340;&#21160;&#24577;&#30456;&#20851;&#24615;&#20197;&#21450;&#20174;&#19994;&#21153;&#20851;&#31995;&#30693;&#35782;&#20013;&#24471;&#20986;&#25512;&#35770;&#30340;&#27169;&#31946;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Prismatic&#65292;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#31995;&#32479;&#65292;&#23427;&#25972;&#21512;&#20102;&#21382;&#21490;&#24615;&#33021;&#30340;&#23450;&#37327;&#20998;&#26512;&#21644;&#19994;&#21153;&#20851;&#31995;&#30693;&#35782;&#30340;&#23450;&#24615;&#20998;&#26512;&#65292;&#20197;&#20132;&#20114;&#26041;&#24335;&#23545;&#30456;&#20851;&#19994;&#21153;&#36827;&#34892;&#38598;&#32676;&#20998;&#26512;&#12290;Prismatic&#20855;&#26377;&#19977;&#20010;&#38598;&#32676;&#29983;&#25104;&#36807;&#31243;&#65306;&#21160;&#24577;&#38598;&#32676;&#29983;&#25104;&#12289;&#22522;&#20110;&#30693;&#35782;&#30340;&#38598;&#32676;&#25506;&#32034;&#21644;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#38598;&#32676;&#39564;&#35777;&#12290;&#21033;&#29992;&#22810;&#35270;&#35282;&#38598;&#32676;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#30693;&#35782;&#39537;&#21160;&#30340;&#30456;&#20284;&#24615;&#20016;&#23500;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#38598;&#32676;&#65292;&#25552;&#20379;&#20102;&#23545;&#19994;&#21153;&#30456;&#20851;&#24615;&#30340;&#32454;&#33268;&#29702;&#35299;&#12290;&#36890;&#36807;&#33391;&#22909;&#21327;&#35843;&#30340;&#21487;&#35270;&#21270;&#35270;&#22270;&#65292;Prismatic&#20415;&#20110;&#20102;&#35299;&#20225;&#19994;&#30340;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08978v1 Announce Type: cross Abstract: Financial cluster analysis allows investors to discover investment alternatives and avoid undertaking excessive risks. However, this analytical task faces substantial challenges arising from many pairwise comparisons, the dynamic correlations across time spans, and the ambiguity in deriving implications from business relational knowledge. We propose Prismatic, a visual analytics system that integrates quantitative analysis of historical performance and qualitative analysis of business relational knowledge to cluster correlated businesses interactively. Prismatic features three clustering processes: dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation. Utilizing a multi-view clustering approach, it enriches data-driven clusters with knowledge-driven similarity, providing a nuanced understanding of business correlations. Through well-coordinated visual views, Prismatic facilitates a com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#19982;&#24212;&#29992;&#65292;&#27010;&#36848;&#20102;Transformer&#21450;&#20854;&#21464;&#31181;&#22312;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.08975</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#30740;&#31350;&#19982;&#24212;&#29992;: &#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Research and application of Transformer based anomaly detection model: A literature review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#19982;&#24212;&#29992;&#65292;&#27010;&#36848;&#20102;Transformer&#21450;&#20854;&#21464;&#31181;&#22312;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#20316;&#20026;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#24212;&#29992;&#26368;&#24191;&#27867;&#30340;&#20808;&#36827;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20043;&#19968;&#65292;&#22312;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#23637;&#31034;&#20102;&#22810;&#26679;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#28608;&#21457;&#23545;&#22522;&#20110;Transformer&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#26412;&#32508;&#36848;&#20174;&#26032;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#27010;&#24565;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#24322;&#24120;&#26816;&#27979;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;Transformer&#21450;&#20854;&#21464;&#31181;&#22312;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21246;&#21202;&#20102;&#22522;&#20110;Transformer&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#35752;&#35770;&#20102;&#25152;&#37319;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#26412;&#32508;&#36848;&#24378;&#35843;&#20102;&#22522;&#20110;Transformer&#30340;&#24322;&#24120;&#26816;&#27979;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#23545;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#36235;&#21183;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#32508;&#36848;&#20013;&#25910;&#24405;&#20102;100&#22810;&#20010;&#19982;&#22522;&#20110;Transformer&#30340;&#24322;&#24120;&#26816;&#27979;&#30456;&#20851;&#30340;&#26680;&#24515;&#21442;&#32771;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08975v1 Announce Type: cross Abstract: Transformer, as one of the most advanced neural network models in Natural Language Processing (NLP), exhibits diverse applications in the field of anomaly detection. To inspire research on Transformer-based anomaly detection, this review offers a fresh perspective on the concept of anomaly detection. We explore the current challenges of anomaly detection and provide detailed insights into the operating principles of Transformer and its variants in anomaly detection tasks. Additionally, we delineate various application scenarios for Transformer-based anomaly detection models and discuss the datasets and evaluation metrics employed. Furthermore, this review highlights the key challenges in Transformer-based anomaly detection research and conducts a comprehensive analysis of future research trends in this domain. The review includes an extensive compilation of over 100 core references related to Transformer-based anomaly detection. To the 
&lt;/p&gt;</description></item><item><title>DUEL&#26159;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20027;&#21160;&#20869;&#23384;&#21435;&#37325;&#31574;&#30053;&#26469;&#22686;&#24378;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#33521;&#25991;&#24635;&#32467;&#30446;&#21069;&#26242;&#26080;&#27861;&#32473;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.08963</link><description>&lt;p&gt;
DUEL: &#29992;&#20110;&#33258;&#30417;&#30563;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#20027;&#21160;&#20869;&#23384;&#21435;&#37325;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DUEL: Duplicate Elimination on Active Memory for Self-Supervised Class-Imbalanced Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08963
&lt;/p&gt;
&lt;p&gt;
DUEL&#26159;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20027;&#21160;&#20869;&#23384;&#21435;&#37325;&#31574;&#30053;&#26469;&#22686;&#24378;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#33521;&#25991;&#24635;&#32467;&#30446;&#21069;&#26242;&#26080;&#27861;&#32473;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#20351;&#29992;&#32463;&#36807;&#31934;&#24515;&#31579;&#36873;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#24320;&#21457;&#65292;&#36825;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#25104;&#26412;&#21644;&#36164;&#28304;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30452;&#25509;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#24448;&#24448;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#20559;&#21521;&#20110;&#39057;&#32321;&#20986;&#29616;&#30340;&#31867;&#21035;&#20449;&#24687;&#12290;&#20026;&#20102;&#39640;&#25928;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#20027;&#21160;&#25968;&#25454;&#36807;&#28388;&#30340;&#26032;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;Duplicate Elimination (DUEL)&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#21463;&#20154;&#31867;&#24037;&#20316;&#20869;&#23384;&#21551;&#21457;&#30340;&#20027;&#21160;&#20869;&#23384;&#65292;&#24182;&#24341;&#20837;&#20102;&#21306;&#20998;&#24615;&#20449;&#24687;&#65292;&#29992;&#20110;&#34913;&#37327;&#20869;&#23384;&#20013;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#20197;&#20248;&#21270;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20869;&#23384;&#12290;DUEL&#31574;&#30053;&#36890;&#36807;&#26367;&#25442;&#26368;&#24120;&#37325;&#22797;&#30340;&#25968;&#25454;&#26679;&#26412;&#26469;&#22686;&#24378;&#20869;&#23384;&#20013;&#30340;&#21306;&#20998;&#24615;&#20449;&#24687;&#65292;&#20174;&#32780;&#32531;&#35299;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;DUEL&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08963v1 Announce Type: cross Abstract: Recent machine learning algorithms have been developed using well-curated datasets, which often require substantial cost and resources. On the other hand, the direct use of raw data often leads to overfitting towards frequently occurring class information. To address class imbalances cost-efficiently, we propose an active data filtering process during self-supervised pre-training in our novel framework, Duplicate Elimination (DUEL). This framework integrates an active memory inspired by human working memory and introduces distinctiveness information, which measures the diversity of the data in the memory, to optimize both the feature extractor and the memory. The DUEL policy, which replaces the most duplicated data with new samples, aims to enhance the distinctiveness information in the memory and thereby mitigate class imbalances. We validate the effectiveness of the DUEL framework in class-imbalanced environments, demonstrating its ro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#21517;&#20026;aespa&#65292;&#23427;&#22312;&#20445;&#25345;&#23436;&#25972;&#30340;&#27880;&#24847;&#21147;&#24471;&#20998;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#36880;&#23618;&#37327;&#21270;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08958</link><description>&lt;p&gt;
&#36808;&#21521;&#36229;&#22823;&#35268;&#27169;Transformer&#30340;&#19979;&#19968;&#32423;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#21517;&#20026;aespa&#65292;&#23427;&#22312;&#20445;&#25345;&#23436;&#25972;&#30340;&#27880;&#24847;&#21147;&#24471;&#20998;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#36880;&#23618;&#37327;&#21270;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#25104;&#20026;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#30005;&#35270;&#31561;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#36229;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#26696;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#36825;&#21487;&#33021;&#25104;&#20026;&#23454;&#38469;&#24773;&#20917;&#20013;&#39057;&#32321;&#27169;&#22411;&#26356;&#26032;&#21644;&#22810;&#31181;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#29942;&#39048;&#12290;&#20316;&#20026;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#27425;&#24615;PTQ&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#26377;&#20123;&#21463;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#32771;&#34385;&#21040;Transformer&#20013;&#27880;&#24847;&#21147;&#27169;&#22359;&#20869;&#37096;&#23618;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;PTQ&#31639;&#27861;&#65292;&#23427;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#21483;&#20570;aespa&#65292;&#36890;&#36807;&#22312;&#25928;&#29575;&#19978;&#36827;&#34892;&#36880;&#23618;&#37327;&#21270;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#36328;&#23618;&#20381;&#36182;&#20197;&#20445;&#30041;&#27880;&#24847;&#21147;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08958v1 Announce Type: cross Abstract: With the increasing complexity of generative AI models, post-training quantization (PTQ) has emerged as a promising solution for deploying hyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ schemes, however, consume considerable time and resources, which could be a bottleneck in real situations where frequent model updates and multiple hyper-parameter tunings are required. As a cost-effective alternative, one-shot PTQ schemes have been proposed. Still, the performance is somewhat limited because they cannot consider the inter-layer dependency within the attention module, which is a very important feature of Transformers. In this paper, we thus propose a novel PTQ algorithm that balances accuracy and efficiency. The key idea of the proposed algorithm called aespa is to perform quantization layer-wise for efficiency while considering cross-layer dependency to preserve the attention score. Through extensive exp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.08957</link><description>&lt;p&gt;
MUSTARD&#65306;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#23398;&#25512;&#29702;&#21644;&#23450;&#29702;&#35777;&#26126;&#12290;&#30001;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#38656;&#35201;&#20005;&#26684;&#21644;&#24418;&#24335;&#21270;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#23427;&#20204;&#26159;&#25506;&#32034;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#21560;&#24341;&#39046;&#22495;&#65292;&#20294;&#20173;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#22914;Chain-of-Thought&#65288;CoT&#65289;&#25581;&#31034;&#20102;&#20013;&#38388;&#27493;&#39588;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36880;&#27493;&#27880;&#37322;&#38656;&#35201;&#22823;&#37327;&#30340;&#21171;&#21160;&#21147;&#65292;&#23548;&#33268;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#30340;&#35757;&#32451;&#27493;&#39588;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#20027;&#23548;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;&#12290;MUSTARD&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#21512;&#25104;&#25968;&#25454;&#65306;&#65288;1&#65289;&#23427;&#38543;&#26426;&#36873;&#25321;&#20960;&#20010;&#25968;&#23398;&#27010;&#24565;&#20316;&#20026;&#38382;&#39064;&#30340;&#31867;&#21035;&#12290;&#65288;2&#65289;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#36873;&#23450;&#30340;&#27010;&#24565;&#25552;&#31034;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#38382;&#39064;&#21644;&#23427;&#20204;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08957v1 Announce Type: new Abstract: Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#23376;&#31354;&#38388;&#31232;&#30095;&#22810;&#39033;&#24335;&#30340;&#22343;&#22330;&#27969;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21512;&#24182;&#38454;&#26799;&#23646;&#24615;&#30340;&#26080;&#22522;&#30784;&#25512;&#24191;&#65292;&#24182;&#24314;&#31435;&#20102;SGD&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31245;&#24378;&#30340;&#26465;&#20214;&#21487;&#20197;&#20445;&#35777;&#25439;&#22833;&#20989;&#25968;&#30340;&#25351;&#25968;&#34928;&#20943;&#33267;&#38646;&#12290;</title><link>https://arxiv.org/abs/2402.08948</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36755;&#20837;&#23398;&#20064;&#23376;&#31354;&#38388;&#31232;&#30095;&#22810;&#39033;&#24335;&#30340;&#22343;&#22330;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#23376;&#31354;&#38388;&#31232;&#30095;&#22810;&#39033;&#24335;&#30340;&#22343;&#22330;&#27969;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21512;&#24182;&#38454;&#26799;&#23646;&#24615;&#30340;&#26080;&#22522;&#30784;&#25512;&#24191;&#65292;&#24182;&#24314;&#31435;&#20102;SGD&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31245;&#24378;&#30340;&#26465;&#20214;&#21487;&#20197;&#20445;&#35777;&#25439;&#22833;&#20989;&#25968;&#30340;&#25351;&#25968;&#34928;&#20943;&#33267;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#23376;&#31354;&#38388;&#31232;&#30095;&#22810;&#39033;&#24335;&#30340;&#22343;&#22330;&#27969;&#21160;&#65292;&#20854;&#20013;&#36755;&#20837;&#20998;&#24067;&#26159;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#65292;&#36755;&#20986;&#20165;&#20381;&#36182;&#20110;&#36755;&#20837;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#19978;&#30340;&#25237;&#24433;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Abbe&#31561;&#20154;(2022&#24180;)&#20013;&#21512;&#24182;&#38454;&#26799;&#23646;&#24615;&#30340;&#26080;&#22522;&#30784;&#25512;&#24191;&#65292;&#24182;&#24314;&#31435;&#20102;SGD&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27492;&#26465;&#20214;&#20960;&#20046;&#26159;&#20805;&#20998;&#30340;&#65292;&#21363;&#27604;&#24517;&#35201;&#26465;&#20214;&#31245;&#24378;&#30340;&#26465;&#20214;&#21487;&#20197;&#20445;&#35777;&#25439;&#22833;&#20989;&#25968;&#30340;&#25351;&#25968;&#34928;&#20943;&#33267;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08948v1 Announce Type: new Abstract: In this work, we study the mean-field flow for learning subspace-sparse polynomials using stochastic gradient descent and two-layer neural networks, where the input distribution is standard Gaussian and the output only depends on the projection of the input onto a low-dimensional subspace. We propose a basis-free generalization of the merged-staircase property in Abbe et al. (2022) and establish a necessary condition for the SGD-learnability. In addition, we prove that the condition is almost sufficient, in the sense that a condition slightly stronger than the necessary condition can guarantee the exponential decay of the loss functional to zero.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24403;&#20989;&#25968;&#24418;&#24335;&#30340;&#25216;&#26415;&#26469;&#27979;&#37327;&#29702;&#35299;&#29616;&#35937;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#35757;&#32451;&#21644;&#39564;&#35777;&#31934;&#24230;&#21464;&#21270;&#30340;&#38160;&#24230;&#12290;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#65292;&#30456;&#23545;&#29702;&#35299;&#24046;&#36317;&#21644;&#29702;&#35299;&#38160;&#24230;&#20043;&#38388;&#30340;&#36235;&#21183;&#30456;&#20284;&#12290;</title><link>https://arxiv.org/abs/2402.08946</link><description>&lt;p&gt;
&#22312;&#29702;&#35299;&#20013;&#27979;&#37327;&#38160;&#24230;
&lt;/p&gt;
&lt;p&gt;
Measuring Sharpness in Grokking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24403;&#20989;&#25968;&#24418;&#24335;&#30340;&#25216;&#26415;&#26469;&#27979;&#37327;&#29702;&#35299;&#29616;&#35937;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#35757;&#32451;&#21644;&#39564;&#35777;&#31934;&#24230;&#21464;&#21270;&#30340;&#38160;&#24230;&#12290;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#65292;&#30456;&#23545;&#29702;&#35299;&#24046;&#36317;&#21644;&#29702;&#35299;&#38160;&#24230;&#20043;&#38388;&#30340;&#36235;&#21183;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26377;&#26102;&#20250;&#20986;&#29616;&#29702;&#35299;&#29616;&#35937;&#65292;&#21363;&#22312;&#39564;&#35777;&#38598;&#19978;&#36798;&#21040;&#23436;&#32654;&#25110;&#25509;&#36817;&#23436;&#32654;&#30340;&#24615;&#33021;&#65292;&#32780;&#22312;&#30456;&#24212;&#30340;&#35757;&#32451;&#38598;&#19978;&#24050;&#32463;&#33719;&#24471;&#20102;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#35752;&#20250;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25311;&#21512;&#36866;&#24403;&#30340;&#20989;&#25968;&#24418;&#24335;&#30340;&#24378;&#22823;&#25216;&#26415;&#26469;&#27979;&#37327;&#29702;&#35299;&#65292;&#24182;&#21033;&#29992;&#27492;&#25216;&#26415;&#22312;&#20004;&#20010;&#35774;&#32622;&#19979;&#30740;&#31350;&#35757;&#32451;&#21644;&#39564;&#35777;&#31934;&#24230;&#30340;&#21464;&#21270;&#30340;&#38160;&#24230;&#12290;&#31532;&#19968;&#20010;&#35774;&#32622;&#26159;&#30001;Levi&#31561;&#20154;&#65288;2023&#65289;&#24320;&#21457;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20854;&#20013;&#21487;&#20197;&#26041;&#20415;&#22320;&#24471;&#21040;&#38381;&#21512;&#24418;&#24335;&#30340;&#34920;&#36798;&#24335;&#12290;&#31532;&#20108;&#20010;&#35774;&#32622;&#26159;&#35757;&#32451;&#19968;&#20010;&#20004;&#23618;MLP&#26469;&#39044;&#27979;&#20301;&#30340;&#22855;&#20598;&#24615;&#65292;&#36890;&#36807;Miller&#31561;&#20154;&#65288;2023&#65289;&#30340;&#38544;&#34255;&#31574;&#30053;&#24341;&#21457;&#29702;&#35299;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;&#32477;&#23545;&#21644;&#30456;&#23545;&#38160;&#24230;&#27979;&#37327;&#26102;&#65292;&#30456;&#23545;&#29702;&#35299;&#24046;&#36317;&#21644;&#29702;&#35299;&#38160;&#24230;&#20043;&#38388;&#30340;&#36235;&#21183;&#22312;&#36825;&#20004;&#20010;&#35774;&#32622;&#20013;&#26159;&#30456;&#20284;&#30340;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23545;&#27492;&#36827;&#34892;&#20102;&#24605;&#32771;&#65292;&#20026;&#35299;&#20915;&#35813;&#38382;&#39064;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08946v1 Announce Type: new Abstract: Neural networks sometimes exhibit grokking, a phenomenon where perfect or near-perfect performance is achieved on a validation set well after the same performance has been obtained on the corresponding training set. In this workshop paper, we introduce a robust technique for measuring grokking, based on fitting an appropriate functional form. We then use this to investigate the sharpness of transitions in training and validation accuracy under two settings. The first setting is the theoretical framework developed by Levi et al. (2023) where closed form expressions are readily accessible. The second setting is a two-layer MLP trained to predict the parity of bits, with grokking induced by the concealment strategy of Miller et al. (2023). We find that trends between relative grokking gap and grokking sharpness are similar in both settings when using absolute and relative measures of sharpness. Reflecting on this, we make progress toward exp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;DTW&#24230;&#37327;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#27169;&#25311;&#27604;&#36739;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24207;&#21015;&#20043;&#38388;&#30340;&#21464;&#21270;&#65292;&#26377;&#21161;&#20110;&#35299;&#37322;DTW&#24230;&#37327;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2402.08943</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#26694;&#26550;&#35780;&#20272;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;DTW&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Evaluating DTW Measures via a Synthesis Framework for Time-Series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;DTW&#24230;&#37327;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#27169;&#25311;&#27604;&#36739;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24207;&#21015;&#20043;&#38388;&#30340;&#21464;&#21270;&#65292;&#26377;&#21161;&#20110;&#35299;&#37322;DTW&#24230;&#37327;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#28304;&#33258;&#25551;&#36848;&#29305;&#23450;&#35266;&#23519;&#25110;&#24863;&#20852;&#36259;&#37327;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#20854;&#20998;&#26512;&#32463;&#24120;&#28041;&#21450;&#23545;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24207;&#21015;&#30340;&#27604;&#36739;&#65292;&#36825;&#35201;&#27714;&#23545;&#36825;&#20123;&#24207;&#21015;&#36827;&#34892;&#23545;&#40784;&#12290;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#26159;&#23454;&#29616;&#20004;&#20010;&#26102;&#38388;&#20449;&#21495;&#20043;&#38388;&#26368;&#20339;&#23545;&#40784;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#21464;&#20307;&#30340;DTW&#26469;&#28385;&#36275;&#20449;&#21495;&#23545;&#40784;&#25110;&#20998;&#31867;&#30340;&#21508;&#31181;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#23545;&#23427;&#20204;&#22312;&#36825;&#20123;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#30340;&#32508;&#21512;&#35780;&#20272;&#12290;&#22823;&#22810;&#25968;DTW&#24230;&#37327;&#22312;&#26576;&#20123;&#31867;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#32570;&#20047;&#26126;&#30830;&#30340;&#35299;&#37322;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#27604;&#36739;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24207;&#21015;&#20043;&#38388;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#21512;&#25104;&#26694;&#26550;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#36924;&#30495;&#30340;&#21021;&#22987;&#20449;&#21495;&#65292;&#24182;&#20197;&#21487;&#25511;&#30340;&#21464;&#21270;&#36827;&#34892;&#21464;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08943v1 Announce Type: new Abstract: Time-series data originate from various applications that describe specific observations or quantities of interest over time. Their analysis often involves the comparison across different time-series data sequences, which in turn requires the alignment of these sequences. Dynamic Time Warping (DTW) is the standard approach to achieve an optimal alignment between two temporal signals. Different variations of DTW have been proposed to address various needs for signal alignment or classifications. However, a comprehensive evaluation of their performance in these time-series data processing tasks is lacking. Most DTW measures perform well on certain types of time-series data without a clear explanation of the reason. To address that, we propose a synthesis framework to model the variation between two time-series data sequences for comparison. Our synthesis framework can produce a realistic initial signal and deform it with controllable variat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#23454;&#29992;&#30340;&#20108;&#38454;&#36172;&#24466;&#20984;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#19968;&#31867;&#31216;&#20043;&#20026;$\kappa$-&#20984;&#30340;&#20984;&#20989;&#25968;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#21518;&#26399;&#25439;&#22833;&#30028;&#38480;&#12290;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#33021;&#65292;&#21253;&#25324;&#36172;&#24466;&#36923;&#36753;&#22238;&#24402;&#12290;</title><link>https://arxiv.org/abs/2402.08929</link><description>&lt;p&gt;
&#20108;&#38454;&#26041;&#27861;&#29992;&#20110;&#36172;&#24466;&#20248;&#21270;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Second Order Methods for Bandit Optimization and Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#23454;&#29992;&#30340;&#20108;&#38454;&#36172;&#24466;&#20984;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#19968;&#31867;&#31216;&#20043;&#20026;$\kappa$-&#20984;&#30340;&#20984;&#20989;&#25968;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#21518;&#26399;&#25439;&#22833;&#30028;&#38480;&#12290;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#33021;&#65292;&#21253;&#25324;&#36172;&#24466;&#36923;&#36753;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bandit&#20984;&#20248;&#21270;(BCO)&#26159;&#19968;&#31181;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#22312;&#32447;&#20915;&#31574;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#23613;&#31649;&#24050;&#32463;&#24314;&#31435;&#20102;&#19968;&#33324;&#20984;&#25439;&#22833;&#30340;&#32039;&#26463;&#21518;&#26399;&#30028;&#38480;&#65292;&#20294;&#29616;&#26377;&#31639;&#27861;&#22312;&#39640;&#32500;&#25968;&#25454;&#19978;&#20855;&#26377;&#38590;&#20197;&#24525;&#21463;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#22312;&#32447;&#29275;&#39039;&#27493;&#39588;&#31639;&#27861;&#21551;&#21457;&#30340;&#31616;&#21333;&#23454;&#29992;&#30340;BCO&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;&#20110;&#19968;&#31867;&#25105;&#20204;&#31216;&#20043;&#20026;$\kappa$-&#20984;&#30340;&#20984;&#20989;&#25968;&#23454;&#29616;&#20102;&#26368;&#20248;(&#20174;&#23618;&#38754;&#19978;&#35762;)&#30340;&#21518;&#26399;&#30028;&#38480;&#12290;&#36825;&#20010;&#31867;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#23454;&#38469;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21253;&#25324;&#32447;&#24615;&#12289;&#20108;&#27425;&#21644;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12290;&#38500;&#20102;&#26368;&#20248;&#30340;&#21518;&#26399;&#25439;&#22833;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#26159;&#19968;&#20123;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#24212;&#29992;&#20013;&#24050;&#30693;&#30340;&#26368;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#36172;&#24466;&#36923;&#36753;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08929v1 Announce Type: new Abstract: Bandit convex optimization (BCO) is a general framework for online decision making under uncertainty. While tight regret bounds for general convex losses have been established, existing algorithms achieving these bounds have prohibitive computational costs for high dimensional data.   In this paper, we propose a simple and practical BCO algorithm inspired by the online Newton step algorithm. We show that our algorithm achieves optimal (in terms of horizon) regret bounds for a large class of convex functions that we call $\kappa$-convex. This class contains a wide range of practically relevant loss functions including linear, quadratic, and generalized linear models. In addition to optimal regret, this method is the most efficient known algorithm for several well-studied applications including bandit logistic regression.   Furthermore, we investigate the adaptation of our second-order bandit algorithm to online convex optimization with mem
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#28151;&#21512;&#20559;&#22909;&#20998;&#24067;&#24182;&#20351;&#29992;MaxMin&#23545;&#40784;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#20154;&#31867;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.08925</link><description>&lt;p&gt;
MaxMin-RLHF:&#38754;&#21521;&#20855;&#26377;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20844;&#24179;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08925
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#28151;&#21512;&#20559;&#22909;&#20998;&#24067;&#24182;&#20351;&#29992;MaxMin&#23545;&#40784;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;(RLHF)&#36890;&#36807;&#20351;&#29992;&#20174;&#20559;&#22909;&#25968;&#25454;&#20013;&#27966;&#29983;&#30340;&#21333;&#19968;&#22870;&#21169;&#27169;&#22411;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#20174;&#22810;&#20010;&#29992;&#25143;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#20016;&#23500;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#20102;&#20351;&#29992;&#21333;&#19968;&#22870;&#21169;RLHF&#36827;&#34892;&#23545;&#40784;&#30340;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#65292;&#20174;&#32780;&#20984;&#26174;&#20102;&#20854;&#26080;&#27861;&#34920;&#31034;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#20026;&#20102;&#25552;&#20379;&#19968;&#20010;&#20844;&#24179;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#28151;&#21512;&#20559;&#22909;&#20998;&#24067;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20013;&#30340;&#24179;&#31561;&#21407;&#21017;&#21551;&#21457;&#30340;MaxMin&#23545;&#40784;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#21644;&#36890;&#29992;&#25928;&#29992;RL&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#31361;&#26174;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08925v1 Announce Type: cross Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, such an approach overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. To provide an equitable solution to the problem, we learn a mixture of preference distributions via an expectation-maximization algorithm and propose a MaxMin alignment objective for policy learning inspired by the Egalitarian principle in social choice theory to better represent diverse human preferences. We elucidate the connection of our proposed approach to distributionally robust optimization and general utility RL, thereby highlighting the generality and robustness of our proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#36827;&#34892;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#26368;&#20339;IMU&#25918;&#32622;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23039;&#24577;&#37325;&#24314;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#21452;&#21521;RNN&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#21482;&#26377;6&#20010;IMU&#26102;&#65292;Transformer&#26550;&#26500;&#23558;24&#20010;IMU&#20301;&#32622;&#33719;&#21462;&#30340;&#25968;&#25454;&#19982;&#21452;&#21521;RNN&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#20248;&#21270;&#36873;&#25321;&#30340;IMU&#25918;&#32622;&#31574;&#30053;&#32467;&#21512;&#20102;Transformer&#30340;&#24182;&#34892;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#65292;&#23545;&#22522;&#20110;IMU&#30340;&#23039;&#24577;&#20272;&#35745;&#39046;&#22495;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.08923</link><description>&lt;p&gt;
IMUOptimize: &#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#36827;&#34892;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#26368;&#20339;IMU&#25918;&#32622;
&lt;/p&gt;
&lt;p&gt;
IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#36827;&#34892;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#26368;&#20339;IMU&#25918;&#32622;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23039;&#24577;&#37325;&#24314;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#21452;&#21521;RNN&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#21482;&#26377;6&#20010;IMU&#26102;&#65292;Transformer&#26550;&#26500;&#23558;24&#20010;IMU&#20301;&#32622;&#33719;&#21462;&#30340;&#25968;&#25454;&#19982;&#21452;&#21521;RNN&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#20248;&#21270;&#36873;&#25321;&#30340;IMU&#25918;&#32622;&#31574;&#30053;&#32467;&#21512;&#20102;Transformer&#30340;&#24182;&#34892;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#65292;&#23545;&#22522;&#20110;IMU&#30340;&#23039;&#24577;&#20272;&#35745;&#39046;&#22495;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;IMU&#25968;&#25454;&#39044;&#27979;&#20154;&#20307;&#23039;&#24577;&#30340;&#26032;&#26041;&#27861;&#65292;&#19981;&#21516;&#20110;&#20043;&#21069;&#30340;&#30740;&#31350;DIP-IMU&#12289;IMUPoser&#21644;TransPose&#65292;&#23427;&#20204;&#20351;&#29992;&#26368;&#22810;6&#20010;IMU&#19982;&#21452;&#21521;RNN&#32467;&#21512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#20027;&#35201;&#21019;&#26032;&#65306;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26368;&#20339;IMU&#25918;&#32622;&#31574;&#30053;&#21644;&#22522;&#20110;Transformer&#30340;&#26102;&#24207;&#20998;&#26512;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#20256;&#32479;&#30340;&#20197;6&#20010;IMU&#20026;&#22522;&#30784;&#30340;&#21452;&#21521;RNN&#27169;&#22411;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#32780;&#19988;&#20351;&#29992;&#21482;&#26377;6&#20010;IMU&#26102;&#65292;&#19982;&#21452;&#21521;RNN&#30456;&#27604;&#65292;Transformer&#26550;&#26500;&#26174;&#33879;&#25552;&#39640;&#20102;&#20174;24&#20010;IMU&#20301;&#32622;&#33719;&#21462;&#30340;&#25968;&#25454;&#30340;&#23039;&#24577;&#37325;&#24314;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20248;&#21270;&#36873;&#25321;&#30340;&#20301;&#32622;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;IMU&#23039;&#24577;&#20272;&#35745;&#39046;&#22495;&#30340;&#25913;&#36827;&#65292;&#32467;&#21512;Transformer&#30340;&#24182;&#34892;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08923v1 Announce Type: new Abstract: This paper presents a novel approach for predicting human poses using IMU data, diverging from previous studies such as DIP-IMU, IMUPoser, and TransPose, which use up to 6 IMUs in conjunction with bidirectional RNNs. We introduce two main innovations: a data-driven strategy for optimal IMU placement and a transformer-based model architecture for time series analysis. Our findings indicate that our approach not only outperforms traditional 6 IMU-based biRNN models but also that the transformer architecture significantly enhances pose reconstruction from data obtained from 24 IMU locations, with equivalent performance to biRNNs when using only 6 IMUs. The enhanced accuracy provided by our optimally chosen locations, when coupled with the parallelizability and performance of transformers, provides significant improvements to the field of IMU-based pose estimation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#21644;&#25506;&#35752;&#20102;&#38236;&#20687;&#24433;&#21709;&#20551;&#35774;&#65292;&#31361;&#20986;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#24433;&#21709;&#30340;&#30456;&#20114;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#25351;&#20986;&#65292;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#23545;&#27979;&#35797;&#39044;&#27979;&#30340;&#24433;&#21709;&#21487;&#20197;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#31561;&#25928;&#20294;&#30456;&#21453;&#30340;&#38382;&#39064;&#65306;&#35780;&#20272;&#22914;&#26524;&#27169;&#22411;&#22312;&#29305;&#23450;&#30340;&#27979;&#35797;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23545;&#35757;&#32451;&#26679;&#26412;&#30340;&#39044;&#27979;&#23558;&#22914;&#20309;&#25913;&#21464;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#39564;&#35777;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#19968;&#20551;&#35774;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08922</link><description>&lt;p&gt;
&#38236;&#20687;&#24433;&#21709;&#20551;&#35774;&#65306;&#36890;&#36807;&#21033;&#29992;&#21069;&#21521;&#20256;&#36882;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#24433;&#21709;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
The Mirrored Influence Hypothesis: Efficient Data Influence Estimation by Harnessing Forward Passes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#21644;&#25506;&#35752;&#20102;&#38236;&#20687;&#24433;&#21709;&#20551;&#35774;&#65292;&#31361;&#20986;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#24433;&#21709;&#30340;&#30456;&#20114;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#25351;&#20986;&#65292;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#23545;&#27979;&#35797;&#39044;&#27979;&#30340;&#24433;&#21709;&#21487;&#20197;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#31561;&#25928;&#20294;&#30456;&#21453;&#30340;&#38382;&#39064;&#65306;&#35780;&#20272;&#22914;&#26524;&#27169;&#22411;&#22312;&#29305;&#23450;&#30340;&#27979;&#35797;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23545;&#35757;&#32451;&#26679;&#26412;&#30340;&#39044;&#27979;&#23558;&#22914;&#20309;&#25913;&#21464;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#39564;&#35777;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#19968;&#20551;&#35774;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#40657;&#30418;&#27169;&#22411;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#12290;&#20102;&#35299;&#20010;&#21035;&#35757;&#32451;&#25968;&#25454;&#28304;&#23545;&#36825;&#20123;&#27169;&#22411;&#25152;&#20570;&#39044;&#27979;&#30340;&#24433;&#21709;&#23545;&#20110;&#25913;&#21892;&#20854;&#21487;&#20449;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#24433;&#21709;&#35780;&#20272;&#25216;&#26415;&#28041;&#21450;&#35745;&#31639;&#27599;&#20010;&#35757;&#32451;&#28857;&#30340;&#26799;&#24230;&#25110;&#22312;&#19981;&#21516;&#23376;&#38598;&#19978;&#37325;&#22797;&#35757;&#32451;&#12290;&#24403;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#26126;&#26174;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08922v1 Announce Type: new Abstract: Large-scale black-box models have become ubiquitous across numerous applications. Understanding the influence of individual training data sources on predictions made by these models is crucial for improving their trustworthiness. Current influence estimation techniques involve computing gradients for every training point or repeated training on different subsets. These approaches face obvious computational challenges when scaled up to large datasets and models.   In this paper, we introduce and explore the Mirrored Influence Hypothesis, highlighting a reciprocal nature of influence between training and test data. Specifically, it suggests that evaluating the influence of training data on test predictions can be reformulated as an equivalent, yet inverse problem: assessing how the predictions for training samples would be altered if the model were trained on specific test samples. Through both empirical and theoretical validations, we demo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#38480;&#21046;&#22797;&#26434;&#24615;&#25551;&#32472;&#33258;&#21160;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#22270;&#20687;&#20043;&#38388;&#30340;&#27010;&#24565;&#30456;&#20284;&#24230;&#65292;&#20197;&#20415;&#35299;&#20915;&#22522;&#20110;&#22270;&#20687;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08919</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#38480;&#21046;&#22797;&#26434;&#24615;&#25551;&#32472;&#33258;&#21160;&#32534;&#30721;&#30340;&#27010;&#24565;&#30456;&#20284;&#24230;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Interpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08919
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#38480;&#21046;&#22797;&#26434;&#24615;&#25551;&#32472;&#33258;&#21160;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#22270;&#20687;&#20043;&#38388;&#30340;&#27010;&#24565;&#30456;&#20284;&#24230;&#65292;&#20197;&#20415;&#35299;&#20915;&#22522;&#20110;&#22270;&#20687;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#26426;&#22120;&#23398;&#20064;&#32780;&#35328;&#65292;&#34913;&#37327;&#22270;&#20687;&#20043;&#38388;&#30456;&#20284;&#24230;&#30340;&#31243;&#24230;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#29256;&#26435;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#27861;&#24459;&#21407;&#21017;&#20013;&#65292;&#30830;&#23450;&#20316;&#21697;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#38656;&#35201;&#20027;&#35266;&#20998;&#26512;&#65292;&#20107;&#23454;&#35009;&#20915;&#32773;&#65288;&#27861;&#23448;&#21644;&#38506;&#23457;&#22242;&#65289;&#22312;&#36825;&#20123;&#20027;&#35266;&#21028;&#26029;&#20013;&#21487;&#33021;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#21464;&#24322;&#24615;&#12290;&#22312;&#32467;&#26500;&#19978;&#30456;&#20284;&#30340;&#22270;&#20687;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#19981;&#30456;&#20284;&#30340;&#65292;&#32780;&#23436;&#20840;&#19981;&#21516;&#30340;&#22330;&#26223;&#22270;&#20687;&#21487;&#33021;&#34987;&#35748;&#20026;&#36275;&#22815;&#30456;&#20284;&#20197;&#25903;&#25345;&#21117;&#31363;&#30340;&#25351;&#25511;&#12290;&#25105;&#20204;&#24076;&#26395;&#23450;&#20041;&#21644;&#35745;&#31639;&#22270;&#20687;&#20043;&#38388;&#30340;&#8220;&#27010;&#24565;&#30456;&#20284;&#24230;&#8221;&#65292;&#21363;&#20351;&#36825;&#20123;&#22270;&#20687;&#27809;&#26377;&#37325;&#22797;&#20803;&#32032;&#25110;&#35270;&#35273;&#30456;&#20284;&#32452;&#20214;&#20063;&#33021;&#25429;&#25417;&#21040;&#39640;&#23618;&#27425;&#30340;&#20851;&#31995;&#12290;&#36825;&#20010;&#24819;&#27861;&#26159;&#20351;&#29992;&#19968;&#20010;&#22522;&#26412;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#29983;&#25104;&#23545;&#35270;&#35273;&#25968;&#25454;&#30340;&#8220;&#35299;&#37322;&#8221;&#65288;&#26631;&#39064;&#65289;&#65292;&#24182;&#22312;&#36880;&#28176;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;&#28982;&#21518;&#65292;&#21487;&#20197;&#36890;&#36807;&#38656;&#35201;&#21306;&#20998;&#36825;&#20004;&#20010;&#22270;&#20687;&#30340;&#26631;&#39064;&#38271;&#24230;&#26469;&#34913;&#37327;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08919v1 Announce Type: cross Abstract: Quantifying the degree of similarity between images is a key copyright issue for image-based machine learning. In legal doctrine however, determining the degree of similarity between works requires subjective analysis, and fact-finders (judges and juries) can demonstrate considerable variability in these subjective judgement calls. Images that are structurally similar can be deemed dissimilar, whereas images of completely different scenes can be deemed similar enough to support a claim of copying. We seek to define and compute a notion of "conceptual similarity" among images that captures high-level relations even among images that do not share repeated elements or visually similar components. The idea is to use a base multi-modal model to generate "explanations" (captions) of visual data at increasing levels of complexity. Then, similarity can be measured by the length of the caption needed to discriminate between the two images: Two h
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#36890;&#36807;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#25552;&#39640;&#20102;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08918</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#22312;&#22270;&#19978;&#23398;&#20064;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#21152;&#36895;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Graph Inference Acceleration by Learning MLPs on Graphs without Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#36890;&#36807;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#25552;&#39640;&#20102;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#28040;&#24687;&#20256;&#36882;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#65292;&#27604;&#22914;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20174;GNNs&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLPs&#65289;&#26469;&#21152;&#36895;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#26377;&#30417;&#30563;&#33976;&#39311;&#38480;&#21046;&#20102;&#23545;&#26410;&#35265;&#33410;&#28857;&#30340;&#27867;&#21270;&#65292;&#32780;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#36825;&#31181;&#24773;&#20917;&#24456;&#24120;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#29992;&#20110;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;SimMLP&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#40784;GNNs&#21644;MLPs&#20043;&#38388;&#30340;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#20043;&#38388;&#30340;&#31934;&#32454;&#21644;&#27867;&#21270;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#20943;&#36731;&#24179;&#20961;&#35299;&#30340;&#39118;&#38505;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08918v1 Announce Type: cross Abstract: Graph Neural Networks (GNNs) have demonstrated effectiveness in various graph learning tasks, yet their reliance on message-passing constraints their deployment in latency-sensitive applications such as financial fraud detection. Recent works have explored distilling knowledge from GNNs to Multi-Layer Perceptrons (MLPs) to accelerate inference. However, this task-specific supervised distillation limits generalization to unseen nodes, which are prevalent in latency-sensitive applications. To this end, we present \textbf{\textsc{SimMLP}}, a \textbf{\textsc{Sim}}ple yet effective framework for learning \textbf{\textsc{MLP}}s on graphs without supervision, to enhance generalization. \textsc{SimMLP} employs self-supervised alignment between GNNs and MLPs to capture the fine-grained and generalizable correlation between node features and graph structures, and proposes two strategies to alleviate the risk of trivial solutions. Theoretically, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CT&#22270;&#20687;&#30340;&#23398;&#20064;&#22411;&#33258;&#21160;&#33034;&#26609;&#36716;&#31227;&#39592;&#36136;&#37327;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#25216;&#26415;&#21516;&#26102;&#32771;&#34385;&#27178;&#32437;&#33034;&#26609;&#21463;&#32047;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08910</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#33034;&#26609;&#36716;&#31227;&#39592;&#36136;&#37327;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-based Bone Quality Classification Method for Spinal Metastasis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CT&#22270;&#20687;&#30340;&#23398;&#20064;&#22411;&#33258;&#21160;&#33034;&#26609;&#36716;&#31227;&#39592;&#36136;&#37327;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#25216;&#26415;&#21516;&#26102;&#32771;&#34385;&#27178;&#32437;&#33034;&#26609;&#21463;&#32047;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33034;&#26609;&#36716;&#31227;&#26159;&#39592;&#36716;&#31227;&#20013;&#26368;&#24120;&#35265;&#30340;&#30142;&#30149;&#65292;&#21487;&#33021;&#23548;&#33268;&#30140;&#30171;&#12289;&#19981;&#31283;&#23450;&#24615;&#21644;&#31070;&#32463;&#25439;&#20260;&#12290;&#26089;&#26399;&#26816;&#27979;&#33034;&#26609;&#36716;&#31227;&#23545;&#20934;&#30830;&#20998;&#26399;&#21644;&#26368;&#20339;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#24120;&#36890;&#36807;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#26469;&#36741;&#21161;&#35786;&#26029;&#65292;&#20294;&#36825;&#38656;&#35201;&#32463;&#39564;&#20016;&#23500;&#30340;&#25918;&#23556;&#31185;&#21307;&#29983;&#20184;&#20986;&#22823;&#37327;&#21162;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CT&#22270;&#20687;&#30340;&#23398;&#20064;&#22411;&#33258;&#21160;&#33034;&#26609;&#36716;&#31227;&#39592;&#36136;&#37327;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#21516;&#26102;&#32771;&#34385;&#20102;&#27178;&#32437;&#33034;&#26609;&#21463;&#32047;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#25216;&#26415;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;MTL&#20316;&#20026;&#24402;&#32435;&#20559;&#32622;&#24418;&#24335;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#34920;&#31034;&#26469;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;&#22522;&#20110;&#28151;&#21512;&#22411;&#21487;&#34987;&#35270;&#20026;&#22686;&#29983;&#24615;&#21644;&#28342;&#39592;&#24615;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#23558;&#39592;&#36136;&#37327;&#20998;&#31867;&#20219;&#21153;&#24314;&#27169;&#20026;&#20004;&#20010;&#20108;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08910v1 Announce Type: cross Abstract: Spinal metastasis is the most common disease in bone metastasis and may cause pain, instability and neurological injuries. Early detection of spinal metastasis is critical for accurate staging and optimal treatment. The diagnosis is usually facilitated with Computed Tomography (CT) scans, which requires considerable efforts from well-trained radiologists. In this paper, we explore a learning-based automatic bone quality classification method for spinal metastasis based on CT images. We simultaneously take the posterolateral spine involvement classification task into account, and employ multi-task learning (MTL) technique to improve the performance. MTL acts as a form of inductive bias which helps the model generalize better on each task by sharing representations between related tasks. Based on the prior knowledge that the mixed type can be viewed as both blastic and lytic, we model the task of bone quality classification as two binary 
&lt;/p&gt;</description></item><item><title>&#22270;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65292;&#26412;&#25991;&#21457;&#29616;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#36127;&#36801;&#31227;&#26222;&#36941;&#23384;&#22312;&#65292;&#21363;&#20351;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#22312;&#35821;&#20041;&#19978;&#30456;&#20284;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#65292;&#23545;&#20110;&#35821;&#20041;&#30456;&#20284;&#30340;&#22270;&#65292;&#32467;&#26500;&#24046;&#24322;&#23545;&#23376;&#22270;&#23884;&#20837;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>https://arxiv.org/abs/2402.08907</link><description>&lt;p&gt;
&#35299;&#20915;&#22270;&#19978;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling Negative Transfer on Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08907
&lt;/p&gt;
&lt;p&gt;
&#22270;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65292;&#26412;&#25991;&#21457;&#29616;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#36127;&#36801;&#31227;&#26222;&#36941;&#23384;&#22312;&#65292;&#21363;&#20351;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#22312;&#35821;&#20041;&#19978;&#30456;&#20284;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#65292;&#23545;&#20110;&#35821;&#20041;&#30456;&#20284;&#30340;&#22270;&#65292;&#32467;&#26500;&#24046;&#24322;&#23545;&#23376;&#22270;&#23884;&#20837;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20174;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#20851;&#31995;&#19981;&#23494;&#20999;&#26102;&#65292;&#23398;&#20064;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#19981;&#21033;&#24433;&#21709;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#36127;&#36801;&#31227;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#65292;&#19982;&#22270;&#20687;&#25110;&#25991;&#26412;&#19981;&#21516;&#65292;&#36127;&#36801;&#31227;&#32463;&#24120;&#21457;&#29983;&#65292;&#21363;&#20351;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#22312;&#35821;&#20041;&#19978;&#26377;&#30456;&#20284;&#20043;&#22788;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#32467;&#26500;&#24046;&#24322;&#20250;&#22823;&#22823;&#22686;&#24378;&#22270;&#20013;&#33410;&#28857;&#23884;&#20837;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24102;&#26469;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#65306;&#23545;&#20110;&#35821;&#20041;&#30456;&#20284;&#30340;&#22270;&#65292;&#23613;&#31649;&#32467;&#26500;&#24046;&#24322;&#20250;&#23548;&#33268;&#33410;&#28857;&#23884;&#20837;&#30340;&#20998;&#24067;&#24046;&#24322;&#65292;&#20294;&#23427;&#20204;&#23545;&#23376;&#22270;&#23884;&#20837;&#30340;&#24433;&#21709;&#21487;&#33021;&#36739;&#23567;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;tw
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08907v1 Announce Type: cross Abstract: Transfer learning aims to boost the learning on the target task leveraging knowledge learned from other relevant tasks. However, when the source and target are not closely related, the learning performance may be adversely affected, a phenomenon known as negative transfer. In this paper, we investigate the negative transfer in graph transfer learning, which is important yet underexplored. We reveal that, unlike image or text, negative transfer commonly occurs in graph-structured data, even when source and target graphs share semantic similarities. Specifically, we identify that structural differences significantly amplify the dissimilarities in the node embeddings across graphs. To mitigate this, we bring a new insight: for semantically similar graphs, although structural differences lead to significant distribution shift in node embeddings, their impact on subgraph embeddings could be marginal. Building on this insight, we introduce tw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#20849;&#21516;&#29615;&#22659;&#20013;&#30340;&#20132;&#20114;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#25512;&#26029;&#28216;&#25103;&#21442;&#25968;&#12290;&#37319;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26469;&#26500;&#24314;&#28216;&#25103;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#26080;&#27861;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#23450;&#37327;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08902</link><description>&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#36125;&#21494;&#26031;&#21453;&#21521;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Auto-Encoding Bayesian Inverse Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#20849;&#21516;&#29615;&#22659;&#20013;&#30340;&#20132;&#20114;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#25512;&#26029;&#28216;&#25103;&#21442;&#25968;&#12290;&#37319;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26469;&#26500;&#24314;&#28216;&#25103;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#26080;&#27861;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#23450;&#37327;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#20849;&#21516;&#29615;&#22659;&#20013;&#20114;&#21160;&#26102;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#20250;&#24433;&#21709;&#20854;&#20182;&#26234;&#33021;&#20307;&#26410;&#26469;&#30340;&#20915;&#31574;&#65292;&#32780;&#38750;&#21512;&#20316;&#21160;&#24577;&#28216;&#25103;&#33258;&#28982;&#22320;&#25429;&#25417;&#21040;&#20102;&#36825;&#31181;&#32806;&#21512;&#12290;&#28982;&#32780;&#65292;&#22312;&#20132;&#20114;&#24335;&#36816;&#21160;&#35268;&#21010;&#20013;&#65292;&#26234;&#33021;&#20307;&#36890;&#24120;&#27809;&#26377;&#23436;&#25972;&#30340;&#28216;&#25103;&#27169;&#22411;&#65292;&#20363;&#22914;&#30001;&#20110;&#20854;&#20182;&#29609;&#23478;&#30340;&#30446;&#26631;&#26159;&#26410;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36870;&#21521;&#28216;&#25103;&#38382;&#39064;&#65292;&#20854;&#20013;&#28216;&#25103;&#30340;&#26576;&#20123;&#23646;&#24615;&#26159;&#20808;&#39564;&#26410;&#30693;&#30340;&#65292;&#24517;&#39035;&#26681;&#25454;&#35266;&#27979;&#32467;&#26524;&#36827;&#34892;&#25512;&#26029;&#12290;&#29616;&#26377;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#26041;&#27861;&#35299;&#20915;&#36870;&#21521;&#28216;&#25103;&#38382;&#39064;&#26102;&#20165;&#25552;&#20379;&#26410;&#30693;&#21442;&#25968;&#30340;&#28857;&#20272;&#35745;&#32780;&#19981;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#23450;&#37327;&#21270;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#21442;&#25968;&#20540;&#33021;&#35299;&#37322;&#35266;&#27979;&#34892;&#20026;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#35266;&#28857;&#26500;&#24314;&#20102;&#28216;&#25103;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#20026;&#20102;&#20351;&#25512;&#26029;&#21487;&#34892;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#20869;&#23884;&#21487;&#24494;&#20998;&#28216;&#25103;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08902v1 Announce Type: cross Abstract: When multiple agents interact in a common environment, each agent's actions impact others' future decisions, and noncooperative dynamic games naturally capture this coupling. In interactive motion planning, however, agents typically do not have access to a complete model of the game, e.g., due to unknown objectives of other players. Therefore, we consider the inverse game problem, in which some properties of the game are unknown a priori and must be inferred from observations. Existing maximum likelihood estimation (MLE) approaches to solve inverse games provide only point estimates of unknown parameters without quantifying uncertainty, and perform poorly when many parameter values explain the observed behavior. To address these limitations, we take a Bayesian perspective and construct posterior distributions of game parameters. To render inference tractable, we employ a variational autoencoder (VAE) with an embedded differentiable game
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24369;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#26894;&#20307;&#20998;&#21106;&#30340;&#36845;&#20195;&#20999;&#29255;&#20256;&#25773;&#26041;&#27861;&#65292;&#20165;&#38656;&#22235;&#20010;&#35282;&#26631;&#35760;&#28857;&#20316;&#20026;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#20174;CT&#22270;&#20687;&#20013;&#33258;&#21160;&#35782;&#21035;&#26894;&#20307;&#30340;&#20307;&#31215;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2402.08892</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#19979;&#30340;&#26894;&#20307;&#20998;&#21106;&#65306;&#36845;&#20195;&#20999;&#29255;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Segmentation of Vertebral Bodies with Iterative Slice-propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24369;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#26894;&#20307;&#20998;&#21106;&#30340;&#36845;&#20195;&#20999;&#29255;&#20256;&#25773;&#26041;&#27861;&#65292;&#20165;&#38656;&#22235;&#20010;&#35282;&#26631;&#35760;&#28857;&#20316;&#20026;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#20174;CT&#22270;&#20687;&#20013;&#33258;&#21160;&#35782;&#21035;&#26894;&#20307;&#30340;&#20307;&#31215;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26894;&#20307;&#65288;VB&#65289;&#20998;&#21106;&#26159;&#33034;&#26609;&#30142;&#30149;&#21307;&#23398;&#35270;&#35273;&#35786;&#26029;&#30340;&#37325;&#35201;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#38656;&#35201;&#20687;&#32032;/&#20307;&#32032;&#32423;&#21035;&#30340;&#24378;&#30417;&#30563;&#65292;&#36825;&#23545;&#20110;&#19987;&#23478;&#26469;&#35828;&#26159;&#26114;&#36149;&#12289;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#36845;&#20195;&#33034;&#26609;&#20998;&#21106;&#65288;WISS&#65289;&#26041;&#27861;&#65292;&#20165;&#21033;&#29992;&#21333;&#20010;&#30690;&#29366;&#20999;&#29255;&#19978;&#30340;&#22235;&#20010;&#35282;&#26631;&#35760;&#28857;&#26469;&#20174;CT&#22270;&#20687;&#20013;&#23454;&#29616;VB&#30340;&#33258;&#21160;&#20307;&#31215;&#20998;&#21106;&#12290;WISS&#39318;&#20808;&#37319;&#29992;&#33258;&#25105;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#22312;&#26377;&#27880;&#37322;&#30340;&#30690;&#29366;&#20999;&#29255;&#19978;&#20998;&#21106;VB&#12290;&#36825;&#31181;&#33258;&#25105;&#35757;&#32451;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#38598;&#20013;&#20132;&#26367;&#36827;&#34892;&#35757;&#32451;&#21644;&#32454;&#21270;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;WISS&#20351;&#29992;&#20999;&#29255;&#20256;&#25773;&#26041;&#27861;&#36880;&#23618;&#20999;&#29255;&#22320;&#36827;&#34892;&#25972;&#20010;VB&#30340;&#20998;&#21106;&#65292;&#33719;&#24471;&#20307;&#31215;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08892v1 Announce Type: cross Abstract: Vertebral body (VB) segmentation is an important preliminary step towards medical visual diagnosis for spinal diseases. However, most previous works require pixel/voxel-wise strong supervisions, which is expensive, tedious and time-consuming for experts to annotate. In this paper, we propose a Weakly supervised Iterative Spinal Segmentation (WISS) method leveraging only four corner landmark weak labels on a single sagittal slice to achieve automatic volumetric segmentation from CT images for VBs. WISS first segments VBs on an annotated sagittal slice in an iterative self-training manner. This self-training method alternates between training and refining labels in the training set. Then WISS proceeds to segment the whole VBs slice by slice with a slice-propagation method to obtain volumetric segmentations. We evaluate the performance of WISS on a private spinal metastases CT dataset and the public lumbar CT dataset. On the first dataset,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#22768;&#21151;&#29575;&#23494;&#24230;&#28436;&#21270;&#29305;&#24449;&#20197;&#21450;&#22810;&#26222;&#21202;&#39057;&#31227;&#21644;&#36830;&#32493;&#24378;&#24230;&#35266;&#27979;&#25968;&#25454;&#39044;&#27979;&#22826;&#38451;&#27963;&#21160;&#21306;&#20986;&#29616;&#30340;&#36830;&#32493;&#24378;&#24230;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.08890</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#22826;&#38451;&#27963;&#21160;&#21306;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Predicting the Emergence of Solar Active Regions Using Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08890
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#22768;&#21151;&#29575;&#23494;&#24230;&#28436;&#21270;&#29305;&#24449;&#20197;&#21450;&#22810;&#26222;&#21202;&#39057;&#31227;&#21644;&#36830;&#32493;&#24378;&#24230;&#35266;&#27979;&#25968;&#25454;&#39044;&#27979;&#22826;&#38451;&#27963;&#21160;&#21306;&#20986;&#29616;&#30340;&#36830;&#32493;&#24378;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21019;&#24314;&#21363;&#23558;&#21457;&#29983;&#30340;&#22826;&#31354;&#22825;&#27668;&#24178;&#25200;&#30340;&#26089;&#26399;&#35686;&#25253;&#33021;&#21147;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#20010;&#21253;&#21547;61&#20010;&#26032;&#20986;&#29616;&#30340;&#27963;&#21160;&#21306;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#22768;&#21151;&#29575;&#23494;&#24230;&#28436;&#21270;&#20013;&#30340;&#29305;&#24449;&#65292;&#20197;&#39044;&#27979;&#36830;&#32493;&#24378;&#24230;&#30340;&#20986;&#29616;&#12290;&#20026;&#20102;&#36827;&#34892;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22826;&#38451;&#21160;&#21147;&#23398;&#35266;&#27979;&#21355;&#26143;&#65288;SDO&#65289;&#19978;&#30340;&#22320;&#38663;&#22768;&#23398;&#21644;&#30913;&#22330;&#25104;&#20687;&#20202;&#65288;HMI&#65289;&#35266;&#27979;&#30340;&#22810;&#26222;&#21202;&#39057;&#31227;&#21644;&#36830;&#32493;&#24378;&#24230;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#27963;&#21160;&#21306;&#38468;&#36817;&#36319;&#36394;30.66 x 30.66&#24230;&#30340;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#36861;&#36394;&#27963;&#21160;&#21306;&#20174;&#39044;&#20986;&#29616;&#29366;&#24577;&#24320;&#22987;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25429;&#25417;&#21363;&#23558;&#20986;&#29616;&#30340;&#30913;&#36890;&#37327;&#21464;&#21270;&#25152;&#20851;&#32852;&#30340;&#22768;&#21151;&#29575;&#27969;&#23494;&#24230;&#21464;&#21270;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#33021;&#22815;&#25552;&#21069;5&#20010;&#23567;&#26102;&#39044;&#27979;&#22826;&#38451;&#34920;&#38754;&#26576;&#20010;&#21306;&#22495;&#30340;&#36830;&#32493;&#24378;&#24230;&#20540;&#26159;&#21542;&#20250;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08890v1 Announce Type: cross Abstract: To create early warning capabilities for upcoming Space Weather disturbances, we have selected a dataset of 61 emerging active regions, which allows us to identify characteristic features in the evolution of acoustic power density to predict continuum intensity emergence. For our study, we have utilized Doppler shift and continuum intensity observations from the Helioseismic and Magnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO). The local tracking of 30.66 x 30.66-degree patches in the vicinity of active regions allowed us to trace the evolution of active regions starting from the pre-emergence state. We have developed a machine learning model to capture the acoustic power flux density variations associated with upcoming magnetic flux emergence. The trained Long Short-Term Memory (LSTM) model is able to predict 5 hours ahead whether, in a given area of the solar surface, continuum intensity values will decrease. The per
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#20809;&#27969;&#36827;&#34892;&#35270;&#39057;&#29289;&#20307;&#20998;&#21106;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#20809;&#27969;&#27169;&#22411;&#21644;&#20351;&#29992;&#23436;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#31227;&#21160;&#29289;&#20307;&#25552;&#26696;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.08882</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#20809;&#27969;&#26469;&#36827;&#34892;&#31227;&#21160;&#29289;&#20307;&#25552;&#26696;&#65292;&#29992;&#20110;&#35270;&#39057;&#29289;&#20307;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Moving Object Proposals with Deep Learned Optical Flow for Video Object Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#20809;&#27969;&#36827;&#34892;&#35270;&#39057;&#29289;&#20307;&#20998;&#21106;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#20809;&#27969;&#27169;&#22411;&#21644;&#20351;&#29992;&#23436;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#31227;&#21160;&#29289;&#20307;&#25552;&#26696;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22330;&#26223;&#29702;&#35299;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#30740;&#31350;&#39046;&#22495;&#20043;&#19968;&#12290;&#20026;&#20102;&#22686;&#24378;&#21160;&#24577;&#22330;&#26223;&#29702;&#35299;&#65292;&#24191;&#27867;&#24212;&#29992;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20687;&#32032;&#32423;&#20998;&#21106;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#35821;&#20041;&#20449;&#24687;&#21644;&#36816;&#21160;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#39640;&#25928;&#22320;&#33719;&#21462;&#31227;&#21160;&#29289;&#20307;&#25552;&#26696;(MOP)&#12290;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(UnFlow)&#26469;&#29983;&#25104;&#20809;&#27969;&#20272;&#35745;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#20809;&#27969;&#32593;&#32476;&#30340;&#36755;&#20986;&#28210;&#26579;&#21040;&#19968;&#20010;&#23436;&#20840;&#21367;&#31215;&#30340;SegNet&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#36129;&#29486;&#26377;&#65306;&#65288;1&#65289;&#22312;&#20840;&#26032;&#30340;DAVIS&#25968;&#25454;&#38598;&#19978;&#23545;&#39044;&#35757;&#32451;&#30340;&#20809;&#27969;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65307;&#65288;2&#65289;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#23436;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#29289;&#20307;&#20998;&#21106;&#12290;&#25105;&#20204;&#20351;&#29992;TensorFlow&#24320;&#21457;&#20102;&#30456;&#20851;&#20195;&#30721;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08882v1 Announce Type: cross Abstract: Dynamic scene understanding is one of the most conspicuous field of interest among computer vision community. In order to enhance dynamic scene understanding, pixel-wise segmentation with neural networks is widely accepted. The latest researches on pixel-wise segmentation combined semantic and motion information and produced good performance. In this work, we propose a state of art architecture of neural networks to accurately and efficiently get the moving object proposals (MOP). We first train an unsupervised convolutional neural network (UnFlow) to generate optical flow estimation. Then we render the output of optical flow net to a fully convolutional SegNet model. The main contribution of our work is (1) Fine-tuning the pretrained optical flow model on the brand new DAVIS Dataset; (2) Leveraging fully convolutional neural networks with Encoder-Decoder architecture to segment objects. We developed the codes with TensorFlow, and execu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#31639;&#27861;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20123;&#26816;&#39564;&#20551;&#35774;&#30340;&#25512;&#29702;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#26469;&#35745;&#31639;&#19968;&#20010;&#32473;&#23450;&#31639;&#27861;&#19982;&#21069;&#27839;&#19978;&#26368;&#20844;&#24179;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#25551;&#36848;&#20102;&#23427;&#30340;&#28176;&#36817;&#24615;&#36136;&#12290;</title><link>https://arxiv.org/abs/2402.08879</link><description>&lt;p&gt;
&#19968;&#20010;&#31639;&#27861;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Inference for an Algorithmic Fairness-Accuracy Frontier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#31639;&#27861;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20123;&#26816;&#39564;&#20551;&#35774;&#30340;&#25512;&#29702;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#26469;&#35745;&#31639;&#19968;&#20010;&#32473;&#23450;&#31639;&#27861;&#19982;&#21069;&#27839;&#19978;&#26368;&#20844;&#24179;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#25551;&#36848;&#20102;&#23427;&#30340;&#28176;&#36817;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#36807;&#31243;&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;&#31639;&#27861;&#30340;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#31639;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#22312;&#20154;&#21475;&#30340;&#19981;&#21516;&#23376;&#32676;&#20307;&#20013;&#32463;&#24120;&#20986;&#29616;&#31995;&#32479;&#24615;&#21464;&#21270;&#12290;&#34429;&#28982;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#37117;&#26159;&#31639;&#27861;&#30340;&#26399;&#26395;&#29305;&#24615;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#26159;&#30456;&#20114;&#29306;&#29298;&#30340;&#12290;&#37027;&#20040;&#65292;&#24403;&#38754;&#23545;&#26377;&#38480;&#30340;&#25968;&#25454;&#26102;&#65292;&#19968;&#20010;&#27880;&#37325;&#20844;&#24179;&#24615;&#30340;&#20915;&#31574;&#32773;&#24212;&#35813;&#24590;&#20040;&#20570;&#21602;?&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;Liang&#65292;Lu&#21644;Mu&#65288;2023&#65289;&#25552;&#20986;&#30340;&#19968;&#20010;&#29702;&#35770;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#21069;&#27839;&#25552;&#20379;&#20102;&#19968;&#33268;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#25552;&#20986;&#20102;&#26816;&#39564;&#20551;&#35774;&#30340;&#25512;&#29702;&#26041;&#27861;&#12290;&#36825;&#20123;&#20551;&#35774;&#22312;&#20844;&#24179;&#24615;&#25991;&#29486;&#20013;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#20363;&#22914;(i)&#20840;&#38754;&#25490;&#38500;&#22312;&#31639;&#27861;&#35757;&#32451;&#20013;&#20351;&#29992;&#19968;&#20010;&#21327;&#21464;&#37327;&#26159;&#21542;&#26159;&#26368;&#20248;&#30340;&#65292;(ii)&#26159;&#21542;&#23384;&#22312;&#23545;&#29616;&#26377;&#31639;&#27861;&#26356;&#23569;&#27495;&#35270;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#20026;&#32473;&#23450;&#31639;&#27861;&#19982;&#21069;&#27839;&#19978;&#26368;&#20844;&#24179;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#25552;&#20379;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#65292;&#24182;&#25551;&#36848;&#20102;&#23427;&#30340;&#28176;&#36817;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08879v1 Announce Type: cross Abstract: Decision-making processes increasingly rely on the use of algorithms. Yet, algorithms' predictive ability frequently exhibit systematic variation across subgroups of the population. While both fairness and accuracy are desirable properties of an algorithm, they often come at the cost of one another. What should a fairness-minded policymaker do then, when confronted with finite data? In this paper, we provide a consistent estimator for a theoretical fairness-accuracy frontier put forward by Liang, Lu and Mu (2023) and propose inference methods to test hypotheses that have received much attention in the fairness literature, such as (i) whether fully excluding a covariate from use in training the algorithm is optimal and (ii) whether there are less discriminatory alternatives to an existing algorithm. We also provide an estimator for the distance between a given algorithm and the fairest point on the frontier, and characterize its asymptot
&lt;/p&gt;</description></item><item><title>&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#24341;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20316;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#34917;&#20805;&#65292;&#32473;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#25552;&#20379;&#20102;&#33258;&#28982;&#36873;&#25321;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2402.08871</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Challenges and Opportunities in Topological Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08871
&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#24341;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20316;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#34917;&#20805;&#65292;&#32473;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#25552;&#20379;&#20102;&#33258;&#28982;&#36873;&#25321;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#23427;&#21033;&#29992;&#25299;&#25169;&#29305;&#24449;&#26469;&#29702;&#35299;&#21644;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#36890;&#36807;&#34701;&#20837;&#25299;&#25169;&#27010;&#24565;&#65292;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#34917;&#20805;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#25104;&#20026;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#33258;&#28982;&#36873;&#25321;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#23454;&#29992;&#30410;&#22788;&#21040;&#29702;&#35770;&#22522;&#30784;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#38024;&#23545;&#27599;&#20010;&#38382;&#39064;&#65292;&#23427;&#27010;&#36848;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#20063;&#26159;&#23545;&#31185;&#23398;&#30028;&#30340;&#36992;&#35831;&#65292;&#24076;&#26395;&#31215;&#26497;&#21442;&#19982;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#65292;&#24320;&#21457;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08871v1 Announce Type: new Abstract: Topological deep learning (TDL) is a rapidly evolving field that uses topological features to understand and design deep learning models. This paper posits that TDL may complement graph representation learning and geometric deep learning by incorporating topological concepts, and can thus provide a natural choice for various machine learning settings. To this end, this paper discusses open problems in TDL, ranging from practical benefits to theoretical foundations. For each problem, it outlines potential solutions and future research opportunities. At the same time, this paper serves as an invitation to the scientific community to actively participate in TDL research to unlock the potential of this emerging field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21457;&#26126;&#30340;&#38750;&#32447;&#24615;&#22823;&#26680;&#24515;&#26497;&#21270;&#30721;&#65292;&#31216;&#20026;DeepPolar&#30721;&#12290;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#30721;&#21644;&#20256;&#32479;&#30340;&#26497;&#21270;&#30721;&#30456;&#27604;&#65292;DeepPolar&#30721;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#26356;&#22823;&#30340;&#26680;&#24515;&#23610;&#23544;&#25552;&#39640;&#20102;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08864</link><description>&lt;p&gt;
DeepPolar&#65306;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21457;&#26126;&#38750;&#32447;&#24615;&#22823;&#26680;&#24515;&#26497;&#21270;&#30721;
&lt;/p&gt;
&lt;p&gt;
DeepPolar: Inventing Nonlinear Large-Kernel Polar Codes via Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21457;&#26126;&#30340;&#38750;&#32447;&#24615;&#22823;&#26680;&#24515;&#26497;&#21270;&#30721;&#65292;&#31216;&#20026;DeepPolar&#30721;&#12290;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#30721;&#21644;&#20256;&#32479;&#30340;&#26497;&#21270;&#30721;&#30456;&#27604;&#65292;DeepPolar&#30721;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#26356;&#22823;&#30340;&#26680;&#24515;&#23610;&#23544;&#25552;&#39640;&#20102;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#21270;&#30721;&#20197;Arikan&#30340;&#26497;&#21270;&#26680;&#24515;&#20026;&#22522;&#30784;&#21457;&#23637;&#32780;&#26469;&#65292;&#20195;&#34920;&#20102;&#32534;&#30721;&#29702;&#35770;&#30340;&#19968;&#20010;&#31361;&#30772;&#65292;&#24182;&#19988;&#24050;&#32463;&#25104;&#20026;&#30701;&#21040;&#20013;&#31561;&#22359;&#38271;&#24230;&#21306;&#22495;&#30340;&#26368;&#20808;&#36827;&#32416;&#38169;&#30721;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;Arikan&#30340;&#26680;&#24515;&#26367;&#25442;&#20026;&#26356;&#22823;&#30340;&#26680;&#24515;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26497;&#21270;&#30721;&#30340;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24555;&#30340;&#26497;&#21270;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#30701;&#21040;&#20013;&#31561;&#22359;&#38271;&#24230;&#21306;&#22495;&#26469;&#35828;&#65292;&#23578;&#26410;&#23454;&#29616;&#26377;&#25928;&#21033;&#29992;&#22823;&#26680;&#24515;&#23610;&#23544;&#30340;&#26497;&#21270;&#30721;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#38750;&#32447;&#24615;&#25512;&#24191;&#30340;&#26497;&#21270;&#30721;&#65292;&#23427;&#20855;&#26377;&#25193;&#23637;&#30340;&#26680;&#24515;&#23610;&#23544;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;DeepPolar&#30721;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DeepPolar&#30721;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#26356;&#22823;&#30340;&#26680;&#24515;&#23610;&#23544;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#31070;&#32463;&#30721;&#21644;&#20256;&#32479;&#30340;&#26497;&#21270;&#30721;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08864v1 Announce Type: cross Abstract: Polar codes, developed on the foundation of Arikan's polarization kernel, represent a breakthrough in coding theory and have emerged as the state-of-the-art error-correction-code in short-to-medium block length regimes. Importantly, recent research has indicated that the reliability of polar codes can be further enhanced by substituting Arikan's kernel with a larger one, leading to a faster polarization. However, for short-to-medium block length regimes, the development of polar codes that effectively employ large kernel sizes has not yet been realized. In this paper, we explore a novel, non-linear generalization of polar codes with an expanded kernel size, which we call DeepPolar codes. Our results show that DeepPolar codes effectively utilize the benefits of larger kernel size, resulting in enhanced reliability compared to both the existing neural codes and conventional polar codes.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#20869;&#31215;&#30340;&#36817;&#20284;&#24615;&#36136;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#20316;&#20026;&#36890;&#29992;&#36924;&#36817;&#22120;&#30340;&#33021;&#21147;&#12290;&#24471;&#21040;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#20851;&#31995;&#20989;&#25968;&#36924;&#36817;&#25152;&#38656;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.08856</link><description>&lt;p&gt;
&#20851;&#20110;&#20851;&#31995;&#20989;&#25968;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Approximation of relation functions and attention mechanisms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08856
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#20869;&#31215;&#30340;&#36817;&#20284;&#24615;&#36136;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#20316;&#20026;&#36890;&#29992;&#36924;&#36817;&#22120;&#30340;&#33021;&#21147;&#12290;&#24471;&#21040;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#20851;&#31995;&#20989;&#25968;&#36924;&#36817;&#25152;&#38656;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#26144;&#23556;&#30340;&#20869;&#31215;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#34987;&#29992;&#20110;&#24314;&#27169;&#36755;&#20837;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#31215;&#30340;&#36817;&#20284;&#24615;&#36136;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#23618;&#24863;&#30693;&#26426;&#33258;&#36523;&#30340;&#20869;&#31215;&#26159;&#23545;&#31216;&#27491;&#23450;&#20851;&#31995;&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;&#23545;&#20110;&#38750;&#23545;&#31216;&#20851;&#31995;&#20989;&#25968;&#65292;&#19981;&#21516;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#20869;&#31215;&#26159;&#19968;&#20010;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#37117;&#24471;&#21040;&#20102;&#36798;&#21040;&#32473;&#23450;&#36924;&#36817;&#31934;&#24230;&#25152;&#38656;&#30340;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#30028;&#38480;&#12290;&#23545;&#31216;&#24773;&#20917;&#19979;&#65292;&#20989;&#25968;&#31867;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26680;&#20989;&#25968;&#65292;&#32780;&#23545;&#31216;&#24773;&#20917;&#19979;&#20989;&#25968;&#31867;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#20013;&#30340;&#26680;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#36825;&#20123;&#36924;&#36817;&#32467;&#26524;&#34987;&#24212;&#29992;&#20110;&#20998;&#26512;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08856v1 Announce Type: new Abstract: Inner products of neural network feature maps arises in a wide variety of machine learning frameworks as a method of modeling relations between inputs. This work studies the approximation properties of inner products of neural networks. It is shown that the inner product of a multi-layer perceptron with itself is a universal approximator for symmetric positive-definite relation functions. In the case of asymmetric relation functions, it is shown that the inner product of two different multi-layer perceptrons is a universal approximator. In both cases, a bound is obtained on the number of neurons required to achieve a given accuracy of approximation. In the symmetric case, the function class can be identified with kernels of reproducing kernel Hilbert spaces, whereas in the asymmetric case the function class can be identified with kernels of reproducing kernel Banach spaces. Finally, these approximation results are applied to analyzing the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#24517;&#35201;&#25506;&#32034;&#65292;&#36890;&#36807;&#22312;&#22312;&#32447;&#25968;&#25454;&#21644;&#19987;&#23478;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08848</link><description>&lt;p&gt;
&#28151;&#21512;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hybrid Inverse Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#24517;&#35201;&#25506;&#32034;&#65292;&#36890;&#36807;&#22312;&#22312;&#32447;&#25968;&#25454;&#21644;&#19987;&#23478;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#27169;&#20223;&#23398;&#20064;&#26469;&#35828;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#36739;&#23569;&#30340;&#19987;&#23478;&#28436;&#31034;&#26469;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#19988;&#33021;&#22815;&#27604;&#34892;&#20026;&#20811;&#38534;&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#22320;&#22788;&#29702;&#38169;&#35823;&#32047;&#31215;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#35201;&#27714;&#23398;&#20064;&#32773;&#21453;&#22797;&#35299;&#20915;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#35745;&#31639;&#24448;&#24448;&#20250;&#28010;&#36153;&#22312;&#25628;&#32034;&#38750;&#24120;&#19981;&#30456;&#20284;&#20110;&#19987;&#23478;&#31574;&#30053;&#30340;&#31574;&#30053;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;-&#22312;&#22312;&#32447;&#25968;&#25454;&#21644;&#19987;&#23478;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;-&#20197;&#20943;&#23569;&#19981;&#24517;&#35201;&#30340;&#25506;&#32034;&#12290;&#30452;&#35266;&#19978;&#65292;&#19987;&#23478;&#25968;&#25454;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#23398;&#20064;&#32773;&#19987;&#27880;&#20110;&#33391;&#22909;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#24378;&#31574;&#30053;&#25152;&#38656;&#30340;&#25506;&#32034;&#37327;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#23558;&#23398;&#20064;&#32773;&#37325;&#32622;&#21040;&#29615;&#22659;&#20013;&#30340;&#20219;&#24847;&#29366;&#24577;&#65292;&#36825;&#26159;&#20197;&#21069;&#22312;&#39640;&#25928;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08848v1 Announce Type: cross Abstract: The inverse reinforcement learning approach to imitation learning is a double-edged sword. On the one hand, it can enable learning from a smaller number of expert demonstrations with more robustness to error compounding than behavioral cloning approaches. On the other hand, it requires that the learner repeatedly solve a computationally expensive reinforcement learning (RL) problem. Often, much of this computation is wasted searching over policies very dissimilar to the expert's. In this work, we propose using hybrid RL -- training on a mixture of online and expert data -- to curtail unnecessary exploration. Intuitively, the expert data focuses the learner on good states during training, which reduces the amount of exploration required to compute a strong policy. Notably, such an approach doesn't need the ability to reset the learner to arbitrary states in the environment, a requirement of prior work in efficient inverse RL. More formal
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#31354;&#28151;&#21512;&#31574;&#30053;&#29983;&#25104;&#29420;&#31435;&#21516;&#20998;&#24067;&#21512;&#25104;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#38543;&#26426;&#36807;&#31243;&#23454;&#29616;&#26368;&#20339;&#36716;&#36816;&#65292;&#36827;&#19968;&#27493;&#32454;&#21270;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#25216;&#26415;&#35757;&#32451;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.08847</link><description>&lt;p&gt;
&#26102;&#31354;&#26725;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Space-Time Bridge-Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08847
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#31354;&#28151;&#21512;&#31574;&#30053;&#29983;&#25104;&#29420;&#31435;&#21516;&#20998;&#24067;&#21512;&#25104;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#38543;&#26426;&#36807;&#31243;&#23454;&#29616;&#26368;&#20339;&#36716;&#36816;&#65292;&#36827;&#19968;&#27493;&#32454;&#21270;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#25216;&#26415;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#30001;&#19968;&#32452;&#22320;&#38754;&#30495;&#23454;&#26679;&#26412;&#65288;GT&#26679;&#26412;&#65289;&#38544;&#24335;&#23450;&#20041;&#30340;&#39640;&#32500;&#23454;&#20540;&#27010;&#29575;&#20998;&#24067;&#20013;&#29983;&#25104;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#30340;&#26032;&#21512;&#25104;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#26102;&#31354;&#28151;&#21512;&#31574;&#30053;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#32500;&#24230;&#19978;&#36827;&#34892;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19977;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#26088;&#22312;&#23454;&#29616;&#20174;&#23481;&#26131;&#22788;&#29702;&#30340;&#21021;&#22987;&#27010;&#29575;&#20998;&#24067;&#21040;&#30001;GT&#26679;&#26412;&#34920;&#31034;&#30340;&#30446;&#26631;&#20998;&#24067;&#30340;&#26368;&#20339;&#36716;&#36816;&#65306;&#65288;a&#65289;&#21253;&#21547;&#26102;&#31354;&#28151;&#21512;&#30340;&#32447;&#24615;&#36807;&#31243;&#20135;&#29983;&#39640;&#26031;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#65292;&#65288;b&#65289;&#20854;&#26725;&#25193;&#25955;&#27169;&#25311;&#65292;&#26465;&#20214;&#20026;&#21021;&#22987;&#21644;&#26368;&#32456;&#29366;&#24577;&#21521;&#37327;&#65292;&#20197;&#21450;&#65288;c&#65289;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#25216;&#26415;&#36827;&#34892;&#32454;&#21270;&#30340;&#38750;&#32447;&#24615;&#38543;&#26426;&#36807;&#31243;&#12290;&#25105;&#20204;&#35757;&#32451;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#31934;&#35843;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08847v1 Announce Type: cross Abstract: In this study, we introduce a novel method for generating new synthetic samples that are independent and identically distributed (i.i.d.) from high-dimensional real-valued probability distributions, as defined implicitly by a set of Ground Truth (GT) samples. Central to our method is the integration of space-time mixing strategies that extend across temporal and spatial dimensions. Our methodology is underpinned by three interrelated stochastic processes designed to enable optimal transport from an easily tractable initial probability distribution to the target distribution represented by the GT samples: (a) linear processes incorporating space-time mixing that yield Gaussian conditional probability densities, (b) their bridge-diffusion analogs that are conditioned to the initial and final state vectors, and (c) nonlinear stochastic processes refined through score-matching techniques. The crux of our training regime involves fine-tuning
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#38454;&#27573;&#25200;&#21160;&#27979;&#35797;&#26469;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#25200;&#21160;&#19968;&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#21464;&#21270;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#20316;&#29992;&#30340;&#27010;&#29575;&#26469;&#34913;&#37327;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22686;&#24378;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#22312;&#21306;&#20998;&#19981;&#21516;&#29305;&#24449;&#36129;&#29486;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08845</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#38454;&#27573;&#25200;&#21160;&#27979;&#35797;&#36890;&#36807;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#65292;&#20197;&#36827;&#34892;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08845
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#38454;&#27573;&#25200;&#21160;&#27979;&#35797;&#26469;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#25200;&#21160;&#19968;&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#21464;&#21270;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#20316;&#29992;&#30340;&#27010;&#29575;&#26469;&#34913;&#37327;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22686;&#24378;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#22312;&#21306;&#20998;&#19981;&#21516;&#29305;&#24449;&#36129;&#29486;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;FAMs&#65289;&#36890;&#36807;&#25200;&#21160;&#27979;&#35797;&#26469;&#27979;&#37327;&#27599;&#20010;&#29305;&#24449;&#30340;&#36129;&#29486;&#65292;&#20854;&#20013;&#22312;&#19981;&#21516;&#25200;&#21160;&#19979;&#30340;&#39044;&#27979;&#24046;&#24322;&#36827;&#34892;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#24449;&#30340;&#39044;&#27979;&#21464;&#21270;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#25200;&#21160;&#27979;&#35797;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21306;&#20998;&#19981;&#21516;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;&#20026;&#20102;&#22686;&#24378;FAMs&#22312;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#19981;&#21516;&#29305;&#24449;&#36129;&#29486;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25200;&#21160;&#19968;&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#21464;&#21270;&#36215;&#21040;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#20316;&#29992;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#20316;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#65288;FANS&#65289;&#65292;&#36890;&#36807;&#28041;&#21450;&#20004;&#20010;&#38454;&#27573;&#65288;&#20107;&#23454;&#24615;&#21644;&#24178;&#39044;&#24615;&#65289;&#30340;&#25200;&#21160;&#27979;&#35797;&#35745;&#31639;PNS&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20026;&#20102;&#29983;&#25104;&#21453;&#20107;&#23454;&#26679;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#37325;&#26032;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08845v1 Announce Type: new Abstract: We investigate the problem of explainability in machine learning.To address this problem, Feature Attribution Methods (FAMs) measure the contribution of each feature through a perturbation test, where the difference in prediction is compared under different perturbations.However, such perturbation tests may not accurately distinguish the contributions of different features, when their change in prediction is the same after perturbation.In order to enhance the ability of FAMs to distinguish different features' contributions in this challenging setting, we propose to utilize the probability (PNS) that perturbing a feature is a necessary and sufficient cause for the prediction to change as a measure of feature importance.Our approach, Feature Attribution with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test involving two stages (factual and interventional).In practice, to generate counterfactual samples, we use a re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#24378;&#21270;&#23398;&#20064;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#21450;&#27010;&#29575;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26234;&#33021;&#20892;&#19994;&#31649;&#29702;&#31995;&#32479;&#65292;&#21487;&#20197;&#25552;&#39640;&#20316;&#29289;&#20135;&#37327;&#65292;&#20248;&#21270;&#32933;&#26009;&#21644;&#27700;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#20943;&#23569;&#27694;&#32933;&#27969;&#22833;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#65292;&#29305;&#21035;&#20851;&#27880;&#27687;&#21270;&#20122;&#27694;&#65288;N2O&#65289;&#25490;&#25918;&#12290;</title><link>https://arxiv.org/abs/2402.08832</link><description>&lt;p&gt;
&#32771;&#34385;N2O&#25490;&#25918;&#21644;&#27668;&#20505;&#21464;&#24322;&#30340;&#26234;&#33021;&#20892;&#19994;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Intelligent Agricultural Management Considering N$_2$O Emission and Climate Variability with Uncertainties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#24378;&#21270;&#23398;&#20064;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#21450;&#27010;&#29575;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26234;&#33021;&#20892;&#19994;&#31649;&#29702;&#31995;&#32479;&#65292;&#21487;&#20197;&#25552;&#39640;&#20316;&#29289;&#20135;&#37327;&#65292;&#20248;&#21270;&#32933;&#26009;&#21644;&#27700;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#20943;&#23569;&#27694;&#32933;&#27969;&#22833;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#65292;&#29305;&#21035;&#20851;&#27880;&#27687;&#21270;&#20122;&#27694;&#65288;N2O&#65289;&#25490;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#29305;&#21035;&#26159;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#22312;&#20892;&#19994;&#20013;&#25552;&#39640;&#20316;&#29289;&#20135;&#37327;&#65292;&#20248;&#21270;&#27694;&#32933;&#20351;&#29992;&#21644;&#27975;&#27700;&#65292;&#24182;&#20943;&#23569;&#30813;&#37240;&#30416;&#27969;&#22833;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#65292;&#37325;&#28857;&#20851;&#27880;&#22303;&#22756;&#20013;&#30340;&#27687;&#21270;&#20122;&#27694;&#65288;N2O&#65289;&#25490;&#25918;&#12290;&#38754;&#23545;&#27668;&#20505;&#21464;&#21270;&#21644;&#26377;&#38480;&#30340;&#20892;&#19994;&#30693;&#35782;&#65292;&#25105;&#20204;&#20351;&#29992;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#21644;&#20316;&#29289;&#27169;&#25311;&#22120;&#26469;&#27169;&#25311;AI&#20195;&#29702;&#19982;&#20892;&#19994;&#29615;&#22659;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;Q&#32593;&#32476;&#23545;&#26234;&#33021;&#20195;&#29702;&#36827;&#34892;&#28145;&#24230;Q&#23398;&#20064;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#26469;&#39044;&#27979;N2O&#25490;&#25918;&#65292;&#24182;&#23558;&#36825;&#20123;&#39044;&#27979;&#25972;&#21512;&#21040;&#27169;&#25311;&#22120;&#20013;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#27010;&#29575;&#24615;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22788;&#29702;N2O&#25490;&#25918;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#22825;&#27668;&#27169;&#22411;&#22788;&#29702;&#27668;&#20505;&#21464;&#24322;&#65292;&#25552;&#20379;&#19968;&#31995;&#21015;&#25490;&#25918;&#32467;&#26524;&#26469;&#25552;&#39640;&#39044;&#27979;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08832v1 Announce Type: cross Abstract: This study examines how artificial intelligence (AI), especially Reinforcement Learning (RL), can be used in farming to boost crop yields, fine-tune nitrogen use and watering, and reduce nitrate runoff and greenhouse gases, focusing on Nitrous Oxide (N$_2$O) emissions from soil. Facing climate change and limited agricultural knowledge, we use Partially Observable Markov Decision Processes (POMDPs) with a crop simulator to model AI agents' interactions with farming environments. We apply deep Q-learning with Recurrent Neural Network (RNN)-based Q networks for training agents on optimal actions. Also, we develop Machine Learning (ML) models to predict N$_2$O emissions, integrating these predictions into the simulator. Our research tackles uncertainties in N$_2$O emission estimates with a probabilistic ML approach and climate variability through a stochastic weather model, offering a range of emission outcomes to improve forecast reliabili
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#27495;&#20041;&#38382;&#39064;&#23545;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;{\method}&#26469;&#28040;&#38500;&#33410;&#28857;&#23884;&#20837;&#20013;&#30340;&#27495;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.08824</link><description>&lt;p&gt;
&#24102;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28040;&#38500;&#27495;&#20041;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Disambiguated Node Classification with Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08824
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#27495;&#20041;&#38382;&#39064;&#23545;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;{\method}&#26469;&#28040;&#38500;&#33410;&#28857;&#23884;&#20837;&#20013;&#30340;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23398;&#20064;&#21508;&#31181;&#39046;&#22495;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24448;&#24448;&#24573;&#35270;&#20102;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#22914;&#20309;&#23398;&#20064;&#33021;&#22815;&#26377;&#25928;&#25512;&#24191;&#21040;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#22270;&#21306;&#22495;&#30340;&#28040;&#24687;&#20256;&#25773;&#12290;&#36825;&#20123;&#23569;&#25968;&#21306;&#22495;&#24120;&#24120;&#34920;&#29616;&#20986;&#19981;&#35268;&#21017;&#30340;&#21516;&#36136;/&#24322;&#36136;&#24615;&#27169;&#24335;&#21644;&#22810;&#26679;&#21270;&#30340;&#37051;&#22495;&#31867;&#20998;&#24067;&#65292;&#23548;&#33268;&#20102;&#27495;&#20041;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;GNN&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;&#65292;&#23427;&#23545;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;&#20197;&#21450;&#23545;&#25239;&#36825;&#20010;&#38382;&#39064;&#30340;&#26356;&#20016;&#23500;&#30340;&#30417;&#30563;&#20449;&#21495;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#23545;GNN&#36827;&#34892;&#20102;&#32454;&#31890;&#24230;&#35780;&#20272;&#65292;&#20998;&#26512;&#20102;&#19981;&#21516;&#22270;&#21306;&#22495;&#20013;&#26159;&#21542;&#23384;&#22312;&#27495;&#20041;&#21450;&#20854;&#19982;&#33410;&#28857;&#20301;&#32622;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#28040;&#38500;&#33410;&#28857;&#23884;&#20837;&#30340;&#27495;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;{\method}&#65292;&#23427;&#21033;&#29992;&#39069;&#22806;&#30340;&#20248;&#21270;&#25351;&#23548;&#26469;&#22686;&#24378;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08824v1 Announce Type: new Abstract: Graph Neural Networks (GNNs) have demonstrated significant success in learning from graph-structured data across various domains. Despite their great successful, one critical challenge is often overlooked by existing works, i.e., the learning of message propagation that can generalize effectively to underrepresented graph regions. These minority regions often exhibit irregular homophily/heterophily patterns and diverse neighborhood class distributions, resulting in ambiguity. In this work, we investigate the ambiguity problem within GNNs, its impact on representation learning, and the development of richer supervision signals to fight against this problem. We conduct a fine-grained evaluation of GNN, analyzing the existence of ambiguity in different graph regions and its relation with node positions. To disambiguate node embeddings, we propose a novel method, {\method}, which exploits additional optimization guidance to enhance representa
&lt;/p&gt;</description></item><item><title>RanDumb&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22266;&#23450;&#30340;&#38543;&#26426;&#21464;&#25442;&#23884;&#20837;&#21407;&#22987;&#20687;&#32032;&#24182;&#23398;&#20064;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#36136;&#30097;&#20102;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290; &#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;RanDumb&#22312;&#20247;&#22810;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.08823</link><description>&lt;p&gt;
RanDumb: &#19968;&#31181;&#36136;&#30097;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RanDumb: A Simple Approach that Questions the Efficacy of Continual Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08823
&lt;/p&gt;
&lt;p&gt;
RanDumb&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22266;&#23450;&#30340;&#38543;&#26426;&#21464;&#25442;&#23884;&#20837;&#21407;&#22987;&#20687;&#32032;&#24182;&#23398;&#20064;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#36136;&#30097;&#20102;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290; &#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;RanDumb&#22312;&#20247;&#22810;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RanDumb&#26469;&#26816;&#39564;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;RanDumb&#23558;&#21407;&#22987;&#20687;&#32032;&#20351;&#29992;&#19968;&#20010;&#22266;&#23450;&#30340;&#38543;&#26426;&#21464;&#25442;&#23884;&#20837;&#65292;&#36825;&#20010;&#21464;&#25442;&#36817;&#20284;&#20102;RBF-Kernel&#65292;&#22312;&#30475;&#21040;&#20219;&#20309;&#25968;&#25454;&#20043;&#21069;&#21021;&#22987;&#21270;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#19988;&#19968;&#33268;&#30340;&#21457;&#29616;&#65306;&#22312;&#20247;&#22810;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RanDumb&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#36825;&#34920;&#26126;&#22312;&#36825;&#20123;&#24773;&#26223;&#19979;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;RanDumb&#19981;&#23384;&#20648;&#26679;&#26412;&#65292;&#24182;&#22312;&#25968;&#25454;&#19978;&#36827;&#34892;&#21333;&#27425;&#36941;&#21382;&#65292;&#19968;&#27425;&#22788;&#29702;&#19968;&#20010;&#26679;&#26412;&#12290;&#23427;&#19982;GDumb&#30456;&#36741;&#30456;&#25104;&#65292;&#22312;GDumb&#24615;&#33021;&#29305;&#21035;&#24046;&#30340;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#36816;&#34892;&#12290;&#24403;&#23558;RanDumb&#25193;&#23637;&#21040;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26367;&#25442;&#38543;&#26426;&#21464;&#25442;&#30340;&#24773;&#26223;&#26102;&#65292;&#25105;&#20204;&#24471;&#20986;&#30456;&#21516;&#19968;&#33268;&#30340;&#32467;&#35770;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#32467;&#26524;&#26082;&#20196;&#20154;&#24778;&#35766;&#21448;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#34920;&#31034;&#23398;&#20064;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#34920;&#29616;&#31967;&#31957;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08823v1 Announce Type: cross Abstract: We propose RanDumb to examine the efficacy of continual representation learning. RanDumb embeds raw pixels using a fixed random transform which approximates an RBF-Kernel, initialized before seeing any data, and learns a simple linear classifier on top. We present a surprising and consistent finding: RanDumb significantly outperforms the continually learned representations using deep networks across numerous continual learning benchmarks, demonstrating the poor performance of representation learning in these scenarios. RanDumb stores no exemplars and performs a single pass over the data, processing one sample at a time. It complements GDumb, operating in a low-exemplar regime where GDumb has especially poor performance. We reach the same consistent conclusions when RanDumb is extended to scenarios with pretrained models replacing the random transform with pretrained feature extractor. Our investigation is both surprising and alarming as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26799;&#24230;&#20248;&#21270;&#20013;&#30340;&#36208;&#24266;&#20960;&#20309;&#65292;&#21457;&#29616;&#36208;&#24266;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#27934;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#23398;&#20064;&#29575;&#33258;&#36866;&#24212;&#31574;&#30053;CLR&#65292;&#35813;&#31574;&#30053;&#19982;&#20984;&#20248;&#21270;&#20013;&#30340;Polyak&#27493;&#38271;&#29305;&#20363;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2402.08818</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#20248;&#21270;&#20013;&#30340;&#36208;&#24266;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
Corridor Geometry in Gradient-Based Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26799;&#24230;&#20248;&#21270;&#20013;&#30340;&#36208;&#24266;&#20960;&#20309;&#65292;&#21457;&#29616;&#36208;&#24266;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#27934;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#23398;&#20064;&#29575;&#33258;&#36866;&#24212;&#31574;&#30053;CLR&#65292;&#35813;&#31574;&#30053;&#19982;&#20984;&#20248;&#21270;&#20013;&#30340;Polyak&#27493;&#38271;&#29305;&#20363;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#26368;&#38497;&#19979;&#38477;&#30340;&#36830;&#32493;&#26354;&#32447;&#65292;&#21363;&#26799;&#24230;&#27969;&#30340;&#35299;&#65292;&#21464;&#25104;&#30452;&#32447;&#65292;&#23558;&#25439;&#22833;&#26354;&#38754;&#30340;&#21306;&#22495;&#21010;&#20998;&#20026;&#36208;&#24266;&#12290;&#25105;&#20204;&#34920;&#26126;&#36208;&#24266;&#33021;&#22815;&#25552;&#20379;&#20851;&#20110;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#27934;&#35265;&#65292;&#22240;&#20026;&#36208;&#24266;&#27491;&#26159;&#26799;&#24230;&#19979;&#38477;&#21644;&#26799;&#24230;&#27969;&#36981;&#24490;&#30456;&#21516;&#36712;&#36857;&#19988;&#25439;&#22833;&#32447;&#24615;&#19979;&#38477;&#30340;&#21306;&#22495;&#12290;&#22240;&#27492;&#65292;&#22312;&#36208;&#24266;&#20869;&#37096;&#65292;&#19981;&#23384;&#22312;&#22240;&#26799;&#24230;&#19979;&#38477;&#21644;&#26799;&#24230;&#27969;&#20043;&#38388;&#30340;&#28418;&#31227;&#32780;&#23548;&#33268;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25928;&#24212;&#25110;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#12290;&#22522;&#20110;&#36208;&#24266;&#19978;&#25439;&#22833;&#30340;&#32447;&#24615;&#19979;&#38477;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#23398;&#20064;&#29575;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#36208;&#24266;&#23398;&#20064;&#29575;(CLR)&#12290;CLR&#30340;&#24418;&#24335;&#19982;&#20984;&#20248;&#21270;&#19978;&#26368;&#36817;&#21457;&#29616;&#30340;Polyak&#27493;&#38271;&#29305;&#20363;&#19968;&#33268;&#12290;Polyak&#27493;&#38271;&#36817;&#26399;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08818v1 Announce Type: cross Abstract: We characterize regions of a loss surface as corridors when the continuous curves of steepest descent -- the solutions of the gradient flow -- become straight lines. We show that corridors provide insights into gradient-based optimization, since corridors are exactly the regions where gradient descent and the gradient flow follow the same trajectory, while the loss decreases linearly. As a result, inside corridors there are no implicit regularization effects or training instabilities that have been shown to occur due to the drift between gradient descent and the gradient flow. Using the loss linear decrease on corridors, we devise a learning rate adaptation scheme for gradient descent; we call this scheme Corridor Learning Rate (CLR). The CLR formulation coincides with a special case of Polyak step-size, discovered in the context of convex optimization. The Polyak step-size has been shown recently to have also good convergence propertie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;MDP&#20013;&#20855;&#26377;&#26080;&#30028;&#27599;&#27493;&#25104;&#26412;&#30340;&#27169;&#22411;&#36924;&#36817;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#38480;&#23450;&#27169;&#22411;&#21464;&#25442;&#21644;&#36317;&#31163;&#26469;&#25552;&#20379;&#19978;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.08813</link><description>&lt;p&gt;
MDP&#20013;&#20855;&#26377;&#26080;&#30028;&#27599;&#27493;&#25104;&#26412;&#30340;&#27169;&#22411;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Model approximation in MDPs with unbounded per-step cost
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;MDP&#20013;&#20855;&#26377;&#26080;&#30028;&#27599;&#27493;&#25104;&#26412;&#30340;&#27169;&#22411;&#36924;&#36817;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#38480;&#23450;&#27169;&#22411;&#21464;&#25442;&#21644;&#36317;&#31163;&#26469;&#25552;&#20379;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#21482;&#26377;&#23545;&#36817;&#20284;&#27169;&#22411;$\hat{\mathcal{M}}$&#36827;&#34892;&#35775;&#38382;&#30340;&#24773;&#20917;&#19979;&#65292;&#35774;&#35745;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;&#25104;&#26412;MDP$\mathcal{M}$&#30340;&#25511;&#21046;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#24403;&#22312;&#21407;&#22987;&#27169;&#22411;$\mathcal{M}$&#20013;&#20351;&#29992;&#36817;&#20284;&#27169;&#22411;&#30340;&#26368;&#20248;&#31574;&#30053;$\hat{\pi}^{\star}$&#26102;&#65292;&#20854;&#24615;&#33021;&#22914;&#20309;&#65311;&#25105;&#20204;&#36890;&#36807;&#38480;&#23450;&#22312;$\mathcal{M}$&#20013;&#20351;&#29992;$\hat{\pi}^\star$&#30340;&#20215;&#20540;&#20989;&#25968;&#19982;$\mathcal{M}$&#30340;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#20043;&#38388;&#30340;&#21152;&#26435;&#33539;&#25968;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#21518;&#36890;&#36807;&#32771;&#34385;&#27599;&#27493;&#25104;&#26412;&#30340;&#20223;&#23556;&#21464;&#25442;&#65292;&#25193;&#23637;&#25105;&#20204;&#30340;&#32467;&#26524;&#24182;&#33719;&#24471;&#28508;&#22312;&#26356;&#32039;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#19978;&#30028;&#65292;&#26126;&#30830;&#21462;&#20915;&#20110;&#21407;&#22987;&#27169;&#22411;&#21644;&#36817;&#20284;&#27169;&#22411;&#20043;&#38388;&#30340;&#25104;&#26412;&#20989;&#25968;&#21152;&#26435;&#36317;&#31163;&#21644;&#36716;&#31227;&#26680;&#20989;&#25968;&#21152;&#26435;&#36317;&#31163;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20363;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08813v1 Announce Type: cross Abstract: We consider the problem of designing a control policy for an infinite-horizon discounted cost Markov decision process $\mathcal{M}$ when we only have access to an approximate model $\hat{\mathcal{M}}$. How well does an optimal policy $\hat{\pi}^{\star}$ of the approximate model perform when used in the original model $\mathcal{M}$? We answer this question by bounding a weighted norm of the difference between the value function of $\hat{\pi}^\star $ when used in $\mathcal{M}$ and the optimal value function of $\mathcal{M}$. We then extend our results and obtain potentially tighter upper bounds by considering affine transformations of the per-step cost. We further provide upper bounds that explicitly depend on the weighted distance between cost functions and weighted distance between transition kernels of the original and approximate models. We present examples to illustrate our results.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#22810;&#23610;&#24230;&#20809;&#23398;&#31070;&#32463;&#31185;&#23398;&#20013;&#28145;&#24230;&#21644;&#27973;&#23618;&#25968;&#25454;&#31185;&#23398;&#30340;&#24212;&#29992;&#12290;&#30001;&#20110;&#19981;&#21516;&#23610;&#24230;&#30340;&#25968;&#25454;&#24046;&#24322;&#21644;&#20849;&#24615;&#65292;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#23610;&#24230;&#24320;&#21457;&#19987;&#38376;&#30340;&#31639;&#27861;&#65292;&#24182;&#32467;&#21512;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#22270;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#26469;&#20248;&#21270;&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.08811</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#20809;&#23398;&#31070;&#32463;&#31185;&#23398;&#30340;&#28145;&#24230;&#21644;&#27973;&#23618;&#25968;&#25454;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
Deep and shallow data science for multi-scale optical neuroscience
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08811
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#22810;&#23610;&#24230;&#20809;&#23398;&#31070;&#32463;&#31185;&#23398;&#20013;&#28145;&#24230;&#21644;&#27973;&#23618;&#25968;&#25454;&#31185;&#23398;&#30340;&#24212;&#29992;&#12290;&#30001;&#20110;&#19981;&#21516;&#23610;&#24230;&#30340;&#25968;&#25454;&#24046;&#24322;&#21644;&#20849;&#24615;&#65292;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#23610;&#24230;&#24320;&#21457;&#19987;&#38376;&#30340;&#31639;&#27861;&#65292;&#24182;&#32467;&#21512;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#22270;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#26469;&#20248;&#21270;&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#30340;&#20809;&#23398;&#25104;&#20687;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24471;&#21040;&#20102;&#22823;&#24133;&#25193;&#23637;&#12290;&#26032;&#30340;&#20809;&#23398;&#25216;&#26415;&#12289;&#25351;&#31034;&#29289;&#21644;&#23454;&#39564;&#33539;&#24335;&#29616;&#22312;&#33021;&#22815;&#20174;&#31361;&#35302;&#21040;&#25972;&#20010;&#22823;&#33041;&#30382;&#23618;&#30340;&#23610;&#24230;&#19978;&#36827;&#34892;&#20307;&#20869;&#25104;&#20687;&#12290;&#20026;&#20102;&#22788;&#29702;&#19981;&#21516;&#23610;&#24230;&#20135;&#29983;&#30340;&#28023;&#37327;&#25968;&#25454;&#65292;&#36830;&#32493;&#19981;&#26029;&#22320;&#24320;&#21457;&#20102;&#35745;&#31639;&#26041;&#27861;&#26469;&#25552;&#21462;&#19982;&#29983;&#29289;&#23398;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#21162;&#21147;&#20013;&#65292;&#38754;&#20020;&#19968;&#20123;&#39046;&#22495;&#30340;&#25361;&#25112;&#65288;&#20363;&#22914;&#24494;&#31859;&#32423;&#25968;&#25454;&#20013;&#30340;&#20449;&#22122;&#27604;&#21644;&#20998;&#36776;&#29575;&#38480;&#21046;&#65289;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#12290;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#23398;&#20064;&#32473;&#23450;&#23610;&#24230;&#30340;&#32454;&#33410;&#65292;&#20197;&#20248;&#21270;&#22788;&#29702;&#27969;&#31243;&#12290;&#30456;&#21453;&#65292;&#20854;&#20182;&#26041;&#27861;&#65292;&#22914;&#22270;&#20449;&#21495;&#22788;&#29702;&#65292;&#21017;&#35797;&#22270;&#25277;&#35937;&#20986;&#26576;&#20123;&#29305;&#23450;&#23610;&#24230;&#30340;&#32454;&#33410;&#65292;&#20197;&#25552;&#20379;&#35299;&#20915;&#31070;&#32463;&#25104;&#20687;&#21508;&#20010;&#23610;&#24230;&#36890;&#29992;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#38480;&#21046;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08811v1 Announce Type: cross Abstract: Optical imaging of the brain has expanded dramatically in the past two decades. New optics, indicators, and experimental paradigms are now enabling in-vivo imaging from the synaptic to the cortex-wide scales. To match the resulting flood of data across scales, computational methods are continuously being developed to meet the need of extracting biologically relevant information. In this pursuit, challenges arise in some domains (e.g., SNR and resolution limits in micron-scale data) that require specialized algorithms. These algorithms can, for example, make use of state-of-the-art machine learning to maximally learn the details of a given scale to optimize the processing pipeline. In contrast, other methods, however, such as graph signal processing, seek to abstract away from some of the details that are scale-specific to provide solutions to specific sub-problems common across scales of neuroimaging. Here we discuss limitations and tra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#26080;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#20998;&#38548;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#29992;&#28145;&#24230;&#20026;3&#30340;ReLU&#32593;&#32476;&#23398;&#20064;&#27604;&#29992;&#28145;&#24230;&#20026;2&#30340;ReLU&#32593;&#32476;&#23398;&#20064;&#35201;&#26356;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.08808</link><description>&lt;p&gt;
&#22312;&#35268;&#33539;&#26377;&#30028;&#30340;&#26080;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#20998;&#38548;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Depth Separation in Norm-Bounded Infinite-Width Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#26080;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#20998;&#38548;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#29992;&#28145;&#24230;&#20026;3&#30340;ReLU&#32593;&#32476;&#23398;&#20064;&#27604;&#29992;&#28145;&#24230;&#20026;2&#30340;ReLU&#32593;&#32476;&#23398;&#20064;&#35201;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26080;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#20998;&#38548;&#38382;&#39064;&#65292;&#20854;&#20013;&#22797;&#26434;&#24615;&#30001;&#26435;&#37325;&#30340;&#25972;&#20307;&#20108;&#27425;$\ell_2$&#33539;&#25968;&#25511;&#21046;&#65288;&#32593;&#32476;&#20013;&#25152;&#26377;&#26435;&#37325;&#30340;&#24179;&#26041;&#21644;&#65289;&#12290;&#20043;&#21069;&#30340;&#28145;&#24230;&#20998;&#38548;&#32467;&#26524;&#20027;&#35201;&#20851;&#27880;&#23485;&#24230;&#26041;&#38754;&#30340;&#20998;&#38548;&#65292;&#36825;&#20123;&#32467;&#26524;&#26080;&#27861;&#35828;&#26126;&#28145;&#24230;&#26159;&#21542;&#20915;&#23450;&#20102;&#22312;&#23485;&#24230;&#26080;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#33021;&#21542;&#23398;&#20064;&#20986;&#36866;&#29992;&#20110;&#24191;&#20041;&#19978;&#30340;&#22909;&#27867;&#21270;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#23398;&#20064;&#21487;&#34892;&#24615;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#30340;&#20998;&#38548;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#34920;&#26126;&#26377;&#20123;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;&#33539;&#25968;&#30340;&#28145;&#24230;3 ReLU&#32593;&#32476;&#20197;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#30340;&#26679;&#26412;&#37327;&#36827;&#34892;&#23398;&#20064;&#65292;&#20294;&#19981;&#33021;&#36890;&#36807;&#25511;&#21046;&#33539;&#25968;&#30340;&#28145;&#24230;2 ReLU&#32593;&#32476;&#65288;&#20219;&#20309;&#33539;&#25968;&#20540;&#65289;&#20197;&#20122;&#25351;&#25968;&#22797;&#26434;&#24230;&#36827;&#34892;&#23398;&#20064;&#12290;&#21516;&#26102;&#25105;&#20204;&#36824;&#34920;&#26126;&#31867;&#20284;&#30340;&#36870;&#21521;&#35828;&#27861;&#26159;&#19981;&#21487;&#33021;&#25104;&#31435;&#30340;&#65306;&#20219;&#20309;&#21487;&#20197;&#36890;&#36807;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#36827;&#34892;&#23398;&#20064;&#30340;&#20989;&#25968;&#65292;&#24182;&#19981;&#33021;&#36890;&#36807;&#20122;&#25351;&#25968;&#26679;&#26412;&#22797;&#26434;&#24230;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08808v1 Announce Type: new Abstract: We study depth separation in infinite-width neural networks, where complexity is controlled by the overall squared $\ell_2$-norm of the weights (sum of squares of all weights in the network). Whereas previous depth separation results focused on separation in terms of width, such results do not give insight into whether depth determines if it is possible to learn a network that generalizes well even when the network width is unbounded. Here, we study separation in terms of the sample complexity required for learnability. Specifically, we show that there are functions that are learnable with sample complexity polynomial in the input dimension by norm-controlled depth-3 ReLU networks, yet are not learnable with sub-exponential sample complexity by norm-controlled depth-2 ReLU networks (with any value for the norm). We also show that a similar statement in the reverse direction is not possible: any function learnable with polynomial sample co
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#32447;&#20984;&#20248;&#21270;&#20013;&#30340;&#25237;&#24433;-free&#31639;&#27861;&#65292;&#20351;&#29992;&#32447;&#24615;&#20248;&#21270;&#39044;&#27979;&#35775;&#38382;&#22266;&#23450;&#21487;&#34892;&#38598;&#65292;&#24182;&#28385;&#36275;&#26102;&#21464;&#32422;&#26463;&#12290;&#31639;&#27861;&#22312;&#24207;&#21015;&#19978;&#23454;&#29616;&#20102;$\tilde{O}(T^{3/4})$&#30340;&#36951;&#25022;&#21644;$O(T^{7/8})$&#30340;&#32422;&#26463;&#36829;&#21453;&#12290;</title><link>https://arxiv.org/abs/2402.08799</link><description>&lt;p&gt;
&#20855;&#26377;&#26102;&#21464;&#32422;&#26463;&#30340;&#26080;&#25237;&#24433;&#22312;&#32447;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Projection-Free Online Convex Optimization with Time-Varying Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08799
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#32447;&#20984;&#20248;&#21270;&#20013;&#30340;&#25237;&#24433;-free&#31639;&#27861;&#65292;&#20351;&#29992;&#32447;&#24615;&#20248;&#21270;&#39044;&#27979;&#35775;&#38382;&#22266;&#23450;&#21487;&#34892;&#38598;&#65292;&#24182;&#28385;&#36275;&#26102;&#21464;&#32422;&#26463;&#12290;&#31639;&#27861;&#22312;&#24207;&#21015;&#19978;&#23454;&#29616;&#20102;$\tilde{O}(T^{3/4})$&#30340;&#36951;&#25022;&#21644;$O(T^{7/8})$&#30340;&#32422;&#26463;&#36829;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#20984;&#20248;&#21270;&#20013;&#30340;&#23545;&#25239;&#24615;&#26102;&#21464;&#32422;&#26463;&#35774;&#32622;&#65292;&#20854;&#20013;&#30340;&#25805;&#20316;&#24517;&#39035;&#26159;&#30456;&#23545;&#20110;&#22266;&#23450;&#32422;&#26463;&#38598;&#21487;&#34892;&#30340;&#65292;&#24182;&#19988;&#36824;&#35201;&#24179;&#22343;&#22320;&#28385;&#36275;&#39069;&#22806;&#30340;&#26102;&#21464;&#32422;&#26463;&#12290;&#21463;&#21040;&#22266;&#23450;&#21487;&#34892;&#38598;&#65288;&#30828;&#32422;&#26463;&#65289;&#22312;&#25237;&#24433;&#26041;&#38754;&#22256;&#38590;&#30340;&#24773;&#26223;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#32771;&#34385;&#21482;&#36890;&#36807;&#32447;&#24615;&#20248;&#21270;&#39044;&#27979;&#65288;LOO&#65289;&#35775;&#38382;&#35813;&#38598;&#21512;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#38271;&#24230;&#20026;$T$&#30340;&#24207;&#21015;&#19978;&#65292;&#24182;&#20351;&#29992;&#24635;&#20849;$T$&#27425;LOO&#35843;&#29992;&#65292;&#20445;&#35777;&#19982;&#25439;&#22833;&#30456;&#20851;&#30340;$\tilde{O}(T^{3/4})$&#30340;&#36951;&#25022;&#21644;$O(T^{7/8})$&#30340;&#32422;&#26463;&#36829;&#21453;&#65288;&#24573;&#30053;&#25152;&#26377;&#38500;$T$&#20043;&#22806;&#30340;&#37327;&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#20123;&#30028;&#38480;&#23545;&#20110;&#24207;&#21015;&#30340;&#20219;&#24847;&#21306;&#38388;&#37117;&#25104;&#31435;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#23427;&#21482;&#38656;&#35201;&#23545;&#36719;&#32422;&#26463;&#36827;&#34892;&#19968;&#38454;&#39044;&#27979;&#35775;&#38382;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#25972;&#20010;&#24207;&#21015;&#30456;&#20851;&#30340;&#31867;&#20284;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08799v1 Announce Type: new Abstract: We consider the setting of online convex optimization with adversarial time-varying constraints in which actions must be feasible w.r.t. a fixed constraint set, and are also required on average to approximately satisfy additional time-varying constraints. Motivated by scenarios in which the fixed feasible set (hard constraint) is difficult to project on, we consider projection-free algorithms that access this set only through a linear optimization oracle (LOO). We present an algorithm that, on a sequence of length $T$ and using overall $T$ calls to the LOO, guarantees $\tilde{O}(T^{3/4})$ regret w.r.t. the losses and $O(T^{7/8})$ constraints violation (ignoring all quantities except for $T$) . In particular, these bounds hold w.r.t. any interval of the sequence. We also present a more efficient algorithm that requires only first-order oracle access to the soft constraints and achieves similar bounds w.r.t. the entire sequence. We extend t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;K-DReAM&#30340;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#29305;&#23450;&#29305;&#24449;&#30340;&#26032;&#33647;&#20505;&#36873;&#29289;&#12290;</title><link>https://arxiv.org/abs/2402.08790</link><description>&lt;p&gt;
&#29992;&#30693;&#35782;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#25913;&#36827;&#20998;&#23376;&#29983;&#25104;&#21644;&#33647;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Improving Molecule Generation and Drug Discovery with a Knowledge-enhanced Generative Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;K-DReAM&#30340;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#29305;&#23450;&#29305;&#24449;&#30340;&#26032;&#33647;&#20505;&#36873;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#20998;&#23376;&#21644;&#26032;&#33647;&#20505;&#36873;&#29289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#25104;&#21151;&#65292;&#20294;&#26159;&#29983;&#25104;&#27169;&#22411;&#19982;&#24191;&#27867;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#20043;&#38388;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#65292;&#36825;&#20123;&#30693;&#35782;&#36890;&#24120;&#34987;&#31995;&#32479;&#21270;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#65292;&#32780;&#36825;&#20123;&#30693;&#35782;&#23545;&#20110;&#20449;&#24687;&#21644;&#22686;&#24378;&#29983;&#25104;&#36807;&#31243;&#30340;&#28508;&#21147;&#23578;&#26410;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#21517;&#20026;K-DReAM&#30340;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#26469;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#21151;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#20041;&#23436;&#25972;&#24615;&#65292;&#24182;&#23558;&#36825;&#20123;&#19978;&#19979;&#25991;&#20449;&#24687;&#32467;&#21512;&#21040;&#19968;&#20010;&#29983;&#25104;&#26694;&#26550;&#20013;&#65292;&#20197;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#12290;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#19982;&#25105;&#20204;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20026;&#20135;&#29983;&#20855;&#26377;&#29305;&#23450;&#29305;&#24449;&#30340;&#26032;&#33647;&#20505;&#36873;&#29289;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08790v1 Announce Type: new Abstract: Recent advancements in generative models have established state-of-the-art benchmarks in generating molecules and novel drug candidates. Despite these successes, a significant gap persists between generative models and the utilization of extensive biomedical knowledge, often systematized within knowledge graphs, whose potential to inform and enhance generative processes has not been realized. In this paper, we present a novel approach that bridges this divide by developing a framework for knowledge-enhanced generative models called K-DReAM. We develop a scalable methodology to extend the functionality of knowledge graphs while preserving semantic integrity and incorporate this contextual information into a generative framework to guide a diffusion-based model. The integration of knowledge graph embeddings with our generative model furnishes a robust mechanism for producing novel drug candidates possessing specific characteristics while en
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21683;&#22013;&#22768;&#38899;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#33016;&#37096;X&#23556;&#32447;&#20013;&#39044;&#27979;&#24322;&#24120;&#32467;&#26524;&#65292;&#20174;&#32780;&#20248;&#21270;&#36164;&#28304;&#20351;&#29992;&#24182;&#25552;&#39640;&#21307;&#30103;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08789</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#21033;&#29992;&#21683;&#22013;&#22768;&#38899;&#20248;&#21270;&#33016;&#37096;X&#23556;&#32447;&#30340;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging cough sounds to optimize chest x-ray usage in low-resource settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08789
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21683;&#22013;&#22768;&#38899;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#33016;&#37096;X&#23556;&#32447;&#20013;&#39044;&#27979;&#24322;&#24120;&#32467;&#26524;&#65292;&#20174;&#32780;&#20248;&#21270;&#36164;&#28304;&#20351;&#29992;&#24182;&#25552;&#39640;&#21307;&#30103;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33016;&#37096;X&#23556;&#32447;&#22312;&#24613;&#35786;&#12289;&#35786;&#26029;&#21644;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#31649;&#29702;&#20013;&#26159;&#24120;&#29992;&#30340;&#24037;&#20855;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#65292;&#20248;&#21270;&#36825;&#19968;&#36164;&#28304;&#21487;&#20197;&#20026;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#21644;&#24739;&#32773;&#33410;&#30465;&#23453;&#36149;&#30340;&#36153;&#29992;&#65292;&#21516;&#26102;&#25913;&#21892;&#21672;&#35810;&#26102;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#21360;&#24230;&#27604;&#21704;&#23572;&#37030;&#26222;&#23572;&#23612;&#20122;&#22522;&#30563;&#21307;&#23398;&#20013;&#24515;&#21644;&#21307;&#38498;&#65288;CMCH&#65289;&#30340;137&#21517;&#24739;&#32773;&#30340;&#21069;&#30651;&#24615;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#27599;&#20010;&#24739;&#32773;&#22312;&#31561;&#24453;&#36827;&#34892;&#25918;&#23556;&#29031;&#30456;&#26102;&#25552;&#20379;&#20102;&#33267;&#23569;&#20116;&#20010;&#21683;&#22013;&#22768;&#12290;&#25105;&#20204;&#20351;&#29992;&#22768;&#23398;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20998;&#26512;&#20102;&#25910;&#38598;&#21040;&#30340;&#21683;&#22013;&#22768;&#38899;&#12290;&#23545;&#27599;&#20010;&#24739;&#32773;&#30340;&#21683;&#22013;&#22768;&#38899;&#36827;&#34892;&#20102;&#26102;&#38388;&#21644;&#39057;&#35889;&#29305;&#24449;&#30340;&#20132;&#21449;&#39564;&#35777;&#12290;&#20351;&#29992;&#26631;&#20934;&#32479;&#35745;&#26041;&#27861;&#23545;&#29305;&#24449;&#36827;&#34892;&#20102;&#24635;&#32467;&#12290;&#25105;&#20204;&#24320;&#21457;&#12289;&#27979;&#35797;&#21644;&#27604;&#36739;&#20102;&#19977;&#31181;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#33016;&#37096;X&#23556;&#32447;&#30340;&#24322;&#24120;&#32467;&#26524;&#12290;&#36825;&#19977;&#31181;&#26041;&#27861;&#22343;&#33021;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08789v1 Announce Type: cross Abstract: Chest X-ray is a commonly used tool during triage, diagnosis and management of respiratory diseases. In resource-constricted settings, optimizing this resource can lead to valuable cost savings for the health care system and the patients as well as to and improvement in consult time. We used prospectively-collected data from 137 patients referred for chest X-ray at the Christian Medical Center and Hospital (CMCH) in Purnia, Bihar, India. Each patient provided at least five coughs while awaiting radiography. Collected cough sounds were analyzed using acoustic AI methods. Cross-validation was done on temporal and spectral features on the cough sounds of each patient. Features were summarized using standard statistical approaches. Three models were developed, tested and compared in their capacity to predict an abnormal result in the chest X-ray. All three methods yielded models that could discriminate to some extent between normal and abno
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.08787</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Rethinking Machine Unlearning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65288;MU&#65289;&#65292;&#31216;&#20026;LLM&#28040;&#38500;&#25216;&#26415;&#12290;&#36825;&#20010;&#30740;&#31350;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#25935;&#24863;&#25110;&#38750;&#27861;&#20449;&#24687;&#65289;&#20197;&#21450;&#30456;&#20851;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#22522;&#26412;&#30340;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#24182;&#19981;&#24433;&#21709;&#22240;&#26524;&#26080;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35774;&#24819;LLM&#28040;&#38500;&#25216;&#26415;&#23558;&#25104;&#20026;LLM&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#21487;&#33021;&#25104;&#20026;&#24320;&#21457;&#26082;&#23433;&#20840;&#12289;&#21487;&#38752;&#21448;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30784;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#20840;&#37325;&#35757;&#32451;&#12290;&#25105;&#20204;&#20174;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#24212;&#29992;&#31561;&#26041;&#38754;&#25506;&#32034;&#20102;LLM&#28040;&#38500;&#25216;&#26415;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#29616;&#26377;LLM&#28040;&#38500;&#25216;&#26415;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#65292;&#20363;&#22914;&#28040;&#38500;&#33539;&#22260;&#12289;&#25968;&#25454;&#27169;&#22411;&#20132;&#20114;&#21644;&#22810;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08787v1 Announce Type: cross Abstract: We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26354;&#29575;&#24863;&#30693;&#23545;&#35282;&#39044;&#22788;&#29702;&#22120;&#65292;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#21152;&#36895;&#20102;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#20449;&#21495;&#27169;&#24577;&#12290;</title><link>https://arxiv.org/abs/2402.08784</link><description>&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#38543;&#26426;&#35757;&#32451;&#30340;&#39044;&#22788;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Preconditioners for the Stochastic Training of Implicit Neural Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26354;&#29575;&#24863;&#30693;&#23545;&#35282;&#39044;&#22788;&#29702;&#22120;&#65292;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#21152;&#36895;&#20102;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#20449;&#21495;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#22797;&#26434;&#36830;&#32493;&#22810;&#32500;&#20449;&#21495;&#32534;&#30721;&#20026;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#23454;&#29616;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26426;&#22120;&#20154;&#23398;&#21644;&#20960;&#20309;&#23398;&#31561;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;Adam&#30001;&#20110;&#20854;&#38543;&#26426;&#30340;&#39640;&#25928;&#24615;&#32780;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35757;&#32451;&#20013;&#65292;&#20294;&#20854;&#35757;&#32451;&#26102;&#38388;&#24448;&#24448;&#36739;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#21152;&#36895;&#35757;&#32451;&#30340;&#21516;&#26102;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#26367;&#20195;&#20248;&#21270;&#25216;&#26415;&#12290;&#20256;&#32479;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#22914;L-BFGS&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#25928;&#26524;&#19981;&#20339;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#26354;&#29575;&#24863;&#30693;&#23545;&#35282;&#39044;&#22788;&#29702;&#22120;&#36827;&#34892;&#38543;&#26426;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#22270;&#20687;&#12289;&#24418;&#29366;&#37325;&#24314;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#31561;&#21508;&#31181;&#20449;&#21495;&#27169;&#24577;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08784v1 Announce Type: cross Abstract: Implicit neural representations have emerged as a powerful technique for encoding complex continuous multidimensional signals as neural networks, enabling a wide range of applications in computer vision, robotics, and geometry. While Adam is commonly used for training due to its stochastic proficiency, it entails lengthy training durations. To address this, we explore alternative optimization techniques for accelerated training without sacrificing accuracy. Traditional second-order optimizers like L-BFGS are suboptimal in stochastic settings, making them unsuitable for large-scale data sets. Instead, we propose stochastic training using curvature-aware diagonal preconditioners, showcasing their effectiveness across various signal modalities such as images, shape reconstruction, and Neural Radiance Fields (NeRF).
&lt;/p&gt;</description></item><item><title>FLASH&#26159;&#19968;&#20010;&#36328;&#21516;&#26102;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#25968;&#25454;&#36136;&#37327;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#24310;&#36831;&#31561;&#22240;&#32032;&#65292;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08769</link><description>&lt;p&gt;
FLASH: &#36328;&#21516;&#26102;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLASH: Federated Learning Across Simultaneous Heterogeneities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08769
&lt;/p&gt;
&lt;p&gt;
FLASH&#26159;&#19968;&#20010;&#36328;&#21516;&#26102;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#25968;&#25454;&#36136;&#37327;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#24310;&#36831;&#31561;&#22240;&#32032;&#65292;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#20851;&#38190;&#21069;&#25552;&#26159;&#22312;&#19981;&#20132;&#25442;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#25152;&#26377;&#32773;&#65288;&#23458;&#25143;&#31471;&#65289;&#20043;&#38388;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20010;&#26041;&#27861;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#23458;&#25143;&#31471;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#21487;&#33021;&#19981;&#20165;&#26469;&#33258;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#36824;&#26469;&#33258;&#25968;&#25454;&#36136;&#37327;&#20197;&#21450;&#35745;&#31639;/&#36890;&#20449;&#24310;&#36831;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#23545;&#36825;&#20123;&#19981;&#21516;&#19988;&#21516;&#26102;&#23384;&#22312;&#30340;&#24322;&#36136;&#24615;&#30340;&#32508;&#21512;&#35270;&#22270;&#33267;&#20851;&#37325;&#35201;&#65307;&#20363;&#22914;&#65292;&#24310;&#36831;&#36739;&#20302;&#30340;&#23458;&#25143;&#31471;&#21487;&#33021;&#20855;&#26377;&#36739;&#24046;&#30340;&#25968;&#25454;&#36136;&#37327;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FLASH&#65288;&#36328;&#21516;&#26102;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#19968;&#20010;&#36731;&#37327;&#19988;&#28789;&#27963;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#26435;&#34913;&#19982;&#23458;&#25143;&#31471;&#25968;&#25454;&#36136;&#37327;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#24310;&#36831;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;FL&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;FLASH&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#32479;&#19968;&#30340;&#26041;&#27861;&#20013;&#22788;&#29702;&#25152;&#26377;&#36825;&#20123;&#24322;&#36136;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08769v1 Announce Type: new Abstract: The key premise of federated learning (FL) is to train ML models across a diverse set of data-owners (clients), without exchanging local data. An overarching challenge to this date is client heterogeneity, which may arise not only from variations in data distribution, but also in data quality, as well as compute/communication latency. An integrated view of these diverse and concurrent sources of heterogeneity is critical; for instance, low-latency clients may have poor data quality, and vice versa. In this work, we propose FLASH(Federated Learning Across Simultaneous Heterogeneities), a lightweight and flexible client selection algorithm that outperforms state-of-the-art FL frameworks under extensive sources of heterogeneity, by trading-off the statistical information associated with the client's data quality, data distribution, and latency. FLASH is the first method, to our knowledge, for handling all these heterogeneities in a unified m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20083;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;(ARFL)&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#27979;&#37327;&#26469;&#23398;&#20064;&#40065;&#26834;&#29305;&#24449;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#20083;&#33146;&#30284;&#35786;&#26029;&#12290;</title><link>https://arxiv.org/abs/2402.08768</link><description>&lt;p&gt;
&#20083;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adversarially Robust Feature Learning for Breast Cancer Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08768
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20083;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;(ARFL)&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#27979;&#37327;&#26469;&#23398;&#20064;&#40065;&#26834;&#29305;&#24449;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#20083;&#33146;&#30284;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#25925;&#38556;&#12290;&#23545;&#20110;&#26631;&#20934;&#12289;&#24178;&#20928;&#25968;&#25454;&#20934;&#30830;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32780;&#35328;&#65292;&#24320;&#21457;&#23545;&#25239;&#24615;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#29305;&#24449;&#23398;&#20064;(ARFL)&#26041;&#27861;&#65292;&#29992;&#20110;&#20083;&#33146;&#30284;&#35786;&#26029;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#12290;ARFL&#21033;&#29992;&#26631;&#20934;&#25968;&#25454;&#21644;&#23545;&#25239;&#24615;&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20854;&#20013;&#23558;&#29305;&#24449;&#30456;&#20851;&#24615;&#27979;&#37327;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#20197;&#40723;&#21169;&#23398;&#20064;&#40065;&#26834;&#29305;&#24449;&#21644;&#25233;&#21046;&#34394;&#20551;&#29305;&#24449;&#12290;&#20026;&#20102;&#23637;&#31034;ARFL&#22312;&#20083;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#29420;&#31435;&#20020;&#24202;&#25910;&#38598;&#30340;&#20083;&#33146;&#24433;&#20687;&#25968;&#25454;&#38598;&#24314;&#31435;&#24182;&#35780;&#20272;&#20102;&#35786;&#26029;&#27169;&#22411;&#65292;&#24635;&#20849;&#21253;&#25324;9,548&#24352;&#20083;&#33146;X&#20809;&#29255;&#22270;&#20687;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#26356;&#23433;&#20840;&#30340;&#20083;&#33146;&#30284;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08768v1 Announce Type: cross Abstract: Adversarial data can lead to malfunction of deep learning applications. It is essential to develop deep learning models that are robust to adversarial data while accurate on standard, clean data. In this study, we proposed a novel adversarially robust feature learning (ARFL) method for a real-world application of breast cancer diagnosis. ARFL facilitates adversarial training using both standard data and adversarial data, where a feature correlation measure is incorporated as an objective function to encourage learning of robust features and restrain spurious features. To show the effects of ARFL in breast cancer diagnosis, we built and evaluated diagnosis models using two independent clinically collected breast imaging datasets, comprising a total of 9,548 mammogram images. We performed extensive experiments showing that our method outperformed several state-of-the-art methods and that our method can enhance safer breast cancer diagnosi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#23398;&#20064;&#32773;&#37096;&#20998;&#20449;&#24687;&#20844;&#24320;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#25112;&#30053;&#20998;&#31867;&#65292;&#19981;&#20877;&#20551;&#35774;&#20195;&#29702;&#32773;&#23436;&#20840;&#20102;&#35299;&#20998;&#31867;&#22120;&#30340;&#21442;&#25968;&#65292;&#32780;&#26159;&#32771;&#34385;&#20195;&#29702;&#32773;&#23545;&#23398;&#20064;&#32773;&#25152;&#20351;&#29992;&#30340;&#20998;&#31867;&#22120;&#26377;&#19968;&#20010;&#20849;&#21516;&#30340;&#20998;&#24067;&#20808;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.08758</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#25112;&#30053;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Bayesian Strategic Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#23398;&#20064;&#32773;&#37096;&#20998;&#20449;&#24687;&#20844;&#24320;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#25112;&#30053;&#20998;&#31867;&#65292;&#19981;&#20877;&#20551;&#35774;&#20195;&#29702;&#32773;&#23436;&#20840;&#20102;&#35299;&#20998;&#31867;&#22120;&#30340;&#21442;&#25968;&#65292;&#32780;&#26159;&#32771;&#34385;&#20195;&#29702;&#32773;&#23545;&#23398;&#20064;&#32773;&#25152;&#20351;&#29992;&#30340;&#20998;&#31867;&#22120;&#26377;&#19968;&#20010;&#20849;&#21516;&#30340;&#20998;&#24067;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25112;&#30053;&#20998;&#31867;&#20013;&#65292;&#20195;&#29702;&#32773;&#36890;&#36807;&#20462;&#25913;&#29305;&#24449;&#65292;&#22312;&#19968;&#23450;&#25104;&#26412;&#19979;&#65292;&#24076;&#26395;&#20174;&#23398;&#20064;&#32773;&#30340;&#20998;&#31867;&#22120;&#20013;&#24471;&#21040;&#27491;&#38754;&#20998;&#31867;&#12290;&#23398;&#20064;&#32773;&#36890;&#24120;&#20250;&#23567;&#24515;&#22320;&#20462;&#25913;&#20182;&#20204;&#30340;&#20998;&#31867;&#22120;&#65292;&#20197;&#20415;&#23545;&#20195;&#29702;&#32773;&#30340;&#25112;&#30053;&#34892;&#20026;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#25112;&#30053;&#20998;&#31867;&#30340;&#35770;&#25991;&#22312;&#32771;&#34385;&#20195;&#29702;&#32773;&#25805;&#32437;&#26102;&#20381;&#36182;&#20110;&#20197;&#19979;&#24378;&#20551;&#35774;&#65306;&#20195;&#29702;&#32773;&#23436;&#20840;&#20102;&#35299;&#23398;&#20064;&#32773;&#25152;&#20351;&#29992;&#30340;&#20998;&#31867;&#22120;&#30340;&#30830;&#20999;&#21442;&#25968;&#12290;&#24403;&#22312;&#30495;&#23454;&#39044;&#27979;&#20219;&#21153;&#20013;&#20351;&#29992;&#22797;&#26434;&#25110;&#19987;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26102;&#65292;&#36825;&#36890;&#24120;&#26159;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08758v1 Announce Type: new Abstract: In strategic classification, agents modify their features, at a cost, to ideally obtain a positive classification from the learner's classifier. The typical response of the learner is to carefully modify their classifier to be robust to such strategic behavior. When reasoning about agent manipulations, most papers that study strategic classification rely on the following strong assumption: agents fully know the exact parameters of the deployed classifier by the learner. This often is an unrealistic assumption when using complex or proprietary machine learning techniques in real-world prediction tasks.   We initiate the study of partial information release by the learner in strategic classification. We move away from the traditional assumption that agents have full knowledge of the classifier. Instead, we consider agents that have a common distributional prior on which classifier the learner is using. The learner in our model can reveal tr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#20445;&#35777;&#32553;&#20943;&#25442;&#20301;&#21518;&#24724;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#36895;&#24230;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#36827;&#34892;&#38750;&#26657;&#20934;&#39044;&#27979;&#65292;&#20294;&#22312;&#31934;&#24515;&#36873;&#25321;&#30340;&#20107;&#20214;&#38598;&#21512;&#19979;&#20445;&#25345;&#26080;&#20559;&#30340;&#39044;&#27979;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#20219;&#20309;&#19979;&#28216;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.08753</link><description>&lt;p&gt;
&#38754;&#21521;&#25152;&#26377;&#19979;&#28216;&#20195;&#29702;&#30340;&#25442;&#20301;&#21518;&#24724;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Forecasting for Swap Regret for All Downstream Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#20445;&#35777;&#32553;&#20943;&#25442;&#20301;&#21518;&#24724;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#36895;&#24230;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#36827;&#34892;&#38750;&#26657;&#20934;&#39044;&#27979;&#65292;&#20294;&#22312;&#31934;&#24515;&#36873;&#25321;&#30340;&#20107;&#20214;&#38598;&#21512;&#19979;&#20445;&#25345;&#26080;&#20559;&#30340;&#39044;&#27979;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#20219;&#20309;&#19979;&#28216;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#30830;&#20445;&#26368;&#20339;&#23545;&#31574;&#30340;&#19979;&#28216;&#20195;&#29702;&#22312;&#20219;&#20309;&#25928;&#29992;&#20989;&#25968;&#19979;&#37117;&#33021;&#20445;&#35777;&#32553;&#20943;&#25442;&#20301;&#21518;&#24724;&#12290;&#33258;&#20174;Foster&#21644;Vohra&#65288;1997&#65289;&#20197;&#26469;&#65292;&#24050;&#32463;&#30693;&#36947;&#26368;&#20339;&#23545;&#31574;&#20110;&#26657;&#20934;&#30340;&#39044;&#27979;&#27809;&#26377;&#25442;&#20301;&#21518;&#24724;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24050;&#30693;&#30340;&#22312;&#39034;&#24207;&#23545;&#25239;&#29615;&#22659;&#20013;&#20445;&#35777;&#26657;&#20934;&#39044;&#27979;&#30340;&#31639;&#27861;&#65292;&#20854;&#36895;&#24230;&#22312;&#39044;&#27979;&#31354;&#38388;&#32500;&#24230;&#22686;&#21152;&#26102;&#21576;&#25351;&#25968;&#32423;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36827;&#34892;&#38750;&#26657;&#20934;&#39044;&#27979;&#65292;&#20294;&#22312;&#31934;&#24515;&#36873;&#25321;&#30340;&#20107;&#20214;&#38598;&#21512;&#19979;&#20445;&#25345;&#26080;&#20559;&#30340;&#39044;&#27979;&#65292;&#25105;&#20204;&#33021;&#22815;&#20445;&#35777;&#20219;&#24847;&#19979;&#28216;&#20195;&#29702;&#32553;&#20943;&#25442;&#20301;&#21518;&#24724;&#30340;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#21516;&#26102;&#20445;&#25345;&#25105;&#20204;&#30340;&#39044;&#27979;&#20026;&#20219;&#20309;&#19979;&#28216;&#20195;&#29702;&#25552;&#20379;&#20445;&#35777;&#30340;&#21560;&#24341;&#21147;&#29305;&#24615;&#65292;&#32780;&#26080;&#38656;&#25105;&#20204;&#30340;&#39044;&#27979;&#31639;&#27861;&#30340;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08753v1 Announce Type: cross Abstract: We study the problem of making predictions so that downstream agents who best respond to them will be guaranteed diminishing swap regret, no matter what their utility functions are. It has been known since Foster and Vohra (1997) that agents who best-respond to calibrated forecasts have no swap regret. Unfortunately, the best known algorithms for guaranteeing calibrated forecasts in sequential adversarial environments do so at rates that degrade exponentially with the dimension of the prediction space. In this work, we show that by making predictions that are not calibrated, but are unbiased subject to a carefully selected collection of events, we can guarantee arbitrary downstream agents diminishing swap regret at rates that substantially improve over the rates that result from calibrated forecasts -- while maintaining the appealing property that our forecasts give guarantees for any downstream agent, without our forecasting algorithm 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21463;&#22823;&#33041;&#32467;&#26500;&#21551;&#21457;&#30340;&#26032;&#30340;&#35745;&#31639;&#26041;&#27861;&#8212;&#8212;&#26368;&#36817;&#37051;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#24212;&#20851;&#31995;&#21644;&#26126;&#30830;&#30340;&#26500;&#36896;&#65292;&#21487;&#20197;&#34920;&#31034;&#28145;&#24230;&#20026;2&#30340;&#38408;&#20540;&#30005;&#36335;&#31561;&#19981;&#21516;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.08751</link><description>&lt;p&gt;
&#31070;&#32463;&#30005;&#36335;&#30340;&#26368;&#36817;&#37051;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nearest Neighbor Representations of Neural Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08751
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21463;&#22823;&#33041;&#32467;&#26500;&#21551;&#21457;&#30340;&#26032;&#30340;&#35745;&#31639;&#26041;&#27861;&#8212;&#8212;&#26368;&#36817;&#37051;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#24212;&#20851;&#31995;&#21644;&#26126;&#30830;&#30340;&#26500;&#36896;&#65292;&#21487;&#20197;&#34920;&#31034;&#28145;&#24230;&#20026;2&#30340;&#38408;&#20540;&#30005;&#36335;&#31561;&#19981;&#21516;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25104;&#21151;&#22320;&#25429;&#25417;&#21040;&#20102;&#20154;&#33041;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#21463;&#21040;&#22823;&#33041;&#32467;&#26500;&#30340;&#21551;&#21457;&#65292;&#26368;&#36817;&#37051;&#65288;NN&#65289;&#34920;&#31034;&#27861;&#26159;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;NN&#34920;&#31034;&#27861;&#19982;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#24314;&#31435;&#20102;&#26356;&#29282;&#22266;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#34429;&#28982;&#24050;&#30693;&#22914;&#20309;&#20351;&#29992;NN&#34920;&#31034;&#27861;&#34920;&#31034;&#21333;&#20010;&#31070;&#32463;&#20803;&#65292;&#20294;&#23545;&#20110;&#23567;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23578;&#26080;&#32467;&#26524;&#12290;&#20855;&#20307;&#22320;&#65292;&#38024;&#23545;&#28145;&#24230;&#20026;2&#30340;&#38408;&#20540;&#30005;&#36335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20854;NN&#34920;&#31034;&#30340;&#26126;&#30830;&#26500;&#36896;&#65292;&#24182;&#32473;&#20986;&#20102;&#34920;&#31034;&#25152;&#38656;&#20301;&#25968;&#30340;&#26126;&#30830;&#30028;&#38480;&#12290;&#31034;&#20363;&#20989;&#25968;&#21253;&#25324;&#20984;&#22810;&#38754;&#20307;&#30340;NN&#34920;&#31034;&#65288;&#38408;&#20540;&#38376;&#30340;AND&#65289;&#12289;IP2&#12289;&#38408;&#20540;&#38376;&#30340;OR&#20197;&#21450;&#32447;&#24615;&#25110;&#31934;&#30830;&#20915;&#31574;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08751v1 Announce Type: cross Abstract: Neural networks successfully capture the computational power of the human brain for many tasks. Similarly inspired by the brain architecture, Nearest Neighbor (NN) representations is a novel approach of computation. We establish a firmer correspondence between NN representations and neural networks. Although it was known how to represent a single neuron using NN representations, there were no results even for small depth neural networks. Specifically, for depth-2 threshold circuits, we provide explicit constructions for their NN representation with an explicit bound on the number of bits to represent it. Example functions include NN representations of convex polytopes (AND of threshold gates), IP2, OR of threshold gates, and linear or exact decision lists.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65292;&#23454;&#29616;&#20102;&#23545;&#33041;MR&#22270;&#20687;&#20013;&#21018;&#24615;&#36816;&#21160;&#30340;&#33258;&#21160;&#26816;&#27979;&#12290;&#36890;&#36807;&#23545;&#20844;&#24320;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#65292;&#27169;&#22411;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#33021;&#22815;&#35782;&#21035;&#36816;&#21160;&#20266;&#24433;&#24182;&#25552;&#20379;&#35299;&#37322;&#12290;&#35813;&#27169;&#22411;&#30340;&#32467;&#26524;&#22312;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#65292;&#24182;&#19982;&#22270;&#20687;&#36136;&#37327;&#25351;&#26631;&#21576;&#29616;&#24378;&#28872;&#30340;&#36127;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08749</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#26816;&#27979;&#33041;MR&#22270;&#20687;&#20013;&#30340;&#36816;&#21160;&#20266;&#24433;
&lt;/p&gt;
&lt;p&gt;
Automated detection of motion artifacts in brain MR images using deep learning and explainable artificial intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65292;&#23454;&#29616;&#20102;&#23545;&#33041;MR&#22270;&#20687;&#20013;&#21018;&#24615;&#36816;&#21160;&#30340;&#33258;&#21160;&#26816;&#27979;&#12290;&#36890;&#36807;&#23545;&#20844;&#24320;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#65292;&#27169;&#22411;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#33021;&#22815;&#35782;&#21035;&#36816;&#21160;&#20266;&#24433;&#24182;&#25552;&#20379;&#35299;&#37322;&#12290;&#35813;&#27169;&#22411;&#30340;&#32467;&#26524;&#22312;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#65292;&#24182;&#19982;&#22270;&#20687;&#36136;&#37327;&#25351;&#26631;&#21576;&#29616;&#24378;&#28872;&#30340;&#36127;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#35780;&#20272;&#65292;&#21253;&#25324;&#23545;&#22270;&#20687;&#36827;&#34892;&#20266;&#24433;&#26816;&#26597;&#65292;&#26159;MRI&#25968;&#25454;&#37319;&#38598;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#20197;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#21644;&#19979;&#28216;&#20998;&#26512;&#25110;&#35299;&#37322;&#30340;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;T1&#21152;&#26435;&#33041;&#22270;&#20687;&#20013;&#30340;&#21018;&#24615;&#36816;&#21160;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;2D CNN&#36827;&#34892;&#19977;&#31867;&#20998;&#31867;&#65292;&#24182;&#22312;&#20844;&#24320;&#21487;&#33719;&#21462;&#30340;&#22238;&#39038;&#24615;&#21644;&#21069;&#30651;&#24615;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;Grad-CAM&#28909;&#22270;&#21487;&#20197;&#35782;&#21035;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#23545;&#27169;&#22411;&#32467;&#26524;&#25552;&#20379;&#35299;&#37322;&#12290;&#35813;&#27169;&#22411;&#22312;&#20845;&#20010;&#36816;&#21160;&#27169;&#25311;&#22238;&#39038;&#24615;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#20998;&#21035;&#20026;85%&#21644;80%&#30340;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#19982;&#24179;&#22343;&#36793;&#32536;&#24378;&#24230;&#30456;&#20851;&#30340;&#22270;&#20687;&#36136;&#37327;&#25351;&#26631;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#21069;&#30651;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#34920;&#29616;&#26174;&#31034;&#20102;&#24378;&#28872;&#30340;&#36127;&#30456;&#20851;&#24615;&#65288;-0.84&#65289;&#65292;&#35813;&#25351;&#26631;&#21487;&#20197;&#34920;&#31034;&#36816;&#21160;&#20266;&#24433;&#12290;&#35813;&#27169;&#22411;&#26159;ArtifactID&#24037;&#20855;&#30340;&#19968;&#37096;&#20998;&#65292;&#26088;&#22312;&#20869;&#32852;&#33258;&#21160;&#26816;&#27979;Gibbs&#29615;&#29366;&#20266;&#24433;&#12289;&#21253;&#22260;&#20266;&#24433;&#31561;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08749v1 Announce Type: cross Abstract: Quality assessment, including inspecting the images for artifacts, is a critical step during MRI data acquisition to ensure data quality and downstream analysis or interpretation success. This study demonstrates a deep learning model to detect rigid motion in T1-weighted brain images. We leveraged a 2D CNN for three-class classification and tested it on publicly available retrospective and prospective datasets. Grad-CAM heatmaps enabled the identification of failure modes and provided an interpretation of the model's results. The model achieved average precision and recall metrics of 85% and 80% on six motion-simulated retrospective datasets. Additionally, the model's classifications on the prospective dataset showed a strong inverse correlation (-0.84) compared to average edge strength, an image quality metric indicative of motion. This model is part of the ArtifactID tool, aimed at inline automatic detection of Gibbs ringing, wrap-aro
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#37051;&#65288;NN&#65289;&#34920;&#31034;&#29992;&#20110;&#34920;&#31034;&#31070;&#32463;&#20803;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#33879;&#21517;&#30340;&#38408;&#20540;&#20989;&#25968;&#21487;&#20197;&#29992;&#22810;&#39033;&#24335;&#35268;&#27169;&#30340;&#38170;&#28857;&#21644;&#23545;&#25968;&#20998;&#36776;&#29575;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.08748</link><description>&lt;p&gt;
&#31070;&#32463;&#20803;&#30340;&#26368;&#36817;&#37051;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Nearest Neighbor Representations of Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08748
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#37051;&#65288;NN&#65289;&#34920;&#31034;&#29992;&#20110;&#34920;&#31034;&#31070;&#32463;&#20803;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#33879;&#21517;&#30340;&#38408;&#20540;&#20989;&#25968;&#21487;&#20197;&#29992;&#22810;&#39033;&#24335;&#35268;&#27169;&#30340;&#38170;&#28857;&#21644;&#23545;&#25968;&#20998;&#36776;&#29575;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#37051;&#65288;NN&#65289;&#34920;&#31034;&#26159;&#19968;&#31181;&#21463;&#21040;&#22823;&#33041;&#21551;&#21457;&#30340;&#26032;&#20852;&#35745;&#31639;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;NN&#34920;&#31034;&#26469;&#34920;&#31034;&#31070;&#32463;&#20803;&#65288;&#38408;&#20540;&#20989;&#25968;&#65289;&#30340;&#22797;&#26434;&#24615;&#12290;&#24050;&#30693;&#65292;&#20004;&#20010;&#38170;&#28857;&#65288;NN&#35745;&#31639;&#30340;&#28857;&#65289;&#36275;&#20197;&#34920;&#31034;&#38408;&#20540;&#20989;&#25968;&#30340;NN&#34920;&#31034;&#65292;&#28982;&#32780;&#20998;&#36776;&#29575;&#65288;&#38170;&#28857;&#26465;&#30446;&#25152;&#38656;&#30340;&#26368;&#22823;&#27604;&#29305;&#25968;&#65289;&#26159;$O(n\log{n})$&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#38170;&#28857;&#25968;&#21644;&#38408;&#20540;&#20989;&#25968;&#30340;NN&#34920;&#31034;&#30340;&#20998;&#36776;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#33879;&#21517;&#30340;&#38408;&#20540;&#20989;&#25968;EQUALITY&#65292;COMPARISON&#21644;ODD-MAX-BIT&#21487;&#20197;&#36890;&#36807;$n$&#21644;$O(\log{n})$&#30340;&#20998;&#36776;&#29575;&#20197;&#22810;&#39033;&#24335;&#35268;&#27169;&#30340;&#38170;&#28857;&#34920;&#31034;&#65292;&#36825;&#20123;&#20989;&#25968;&#38656;&#35201;2&#25110;3&#20010;&#38170;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08748v1 Announce Type: cross Abstract: The Nearest Neighbor (NN) Representation is an emerging computational model that is inspired by the brain. We study the complexity of representing a neuron (threshold function) using the NN representations. It is known that two anchors (the points to which NN is computed) are sufficient for a NN representation of a threshold function, however, the resolution (the maximum number of bits required for the entries of an anchor) is $O(n\log{n})$. In this work, the trade-off between the number of anchors and the resolution of a NN representation of threshold functions is investigated. We prove that the well-known threshold functions EQUALITY, COMPARISON, and ODD-MAX-BIT, which require 2 or 3 anchors and resolution of $O(n)$, can be represented by polynomially large number of anchors in $n$ and $O(\log{n})$ resolution. We conjecture that for all threshold functions, there are NN representations with polynomially large size and logarithmic reso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#26080;&#38656;&#35757;&#32451;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#22823;&#37327;&#22270;&#20687;&#20013;&#21457;&#29616;&#20855;&#26377;&#29420;&#29305;&#29305;&#24449;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#23558;&#22270;&#20687;&#38598;&#21512;&#24418;&#24335;&#21270;&#20026;&#24863;&#30693;&#36317;&#31163;&#21152;&#26435;&#22270;&#65292;&#24182;&#23450;&#20301;&#26368;&#29420;&#29305;&#22270;&#20687;&#23376;&#38598;&#30340;&#26368;&#23494;&#23376;&#22270;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;NP&#38590;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#39640;&#25928;&#27714;&#35299;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08743</link><description>&lt;p&gt;
ADS&#65306;&#36817;&#20284;&#26368;&#23494;&#23376;&#22270;&#29992;&#20110;&#26032;&#39062;&#22270;&#20687;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
ADS: Approximate Densest Subgraph for Novel Image Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#26080;&#38656;&#35757;&#32451;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#22823;&#37327;&#22270;&#20687;&#20013;&#21457;&#29616;&#20855;&#26377;&#29420;&#29305;&#29305;&#24449;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#23558;&#22270;&#20687;&#38598;&#21512;&#24418;&#24335;&#21270;&#20026;&#24863;&#30693;&#36317;&#31163;&#21152;&#26435;&#22270;&#65292;&#24182;&#23450;&#20301;&#26368;&#29420;&#29305;&#22270;&#20687;&#23376;&#38598;&#30340;&#26368;&#23494;&#23376;&#22270;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;NP&#38590;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#39640;&#25928;&#27714;&#35299;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#20687;&#24211;&#30340;&#22686;&#38271;&#65292;&#23613;&#31649;&#26377;&#20869;&#23481;&#26816;&#32034;&#30340;&#21151;&#33021;&#65292;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#36731;&#37327;&#32423;&#24037;&#20855;&#65292;&#33021;&#22815;&#20174;&#22823;&#37327;&#22270;&#20687;&#20013;&#21457;&#29616;&#20855;&#26377;&#29420;&#29305;&#29305;&#24449;&#30340;&#22270;&#20687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#26080;&#38656;&#35757;&#32451;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#26032;&#39062;&#22270;&#20687;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#19968;&#32452;&#22270;&#20687;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#24863;&#30693;&#36317;&#31163;&#21152;&#26435;&#22270;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#20219;&#21153;&#26159;&#23450;&#20301;&#23545;&#24212;&#20110;&#26368;&#29420;&#29305;&#22270;&#20687;&#23376;&#38598;&#30340;K-&#26368;&#23494;&#23376;&#22270;&#12290;&#34429;&#28982;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#19981;&#20165;&#26159;NP&#38590;&#38382;&#39064;&#65292;&#32780;&#19988;&#35201;&#27714;&#23436;&#20840;&#35745;&#31639;&#21487;&#33021;&#24040;&#22823;&#30340;&#36317;&#31163;&#30697;&#38453;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#20854;&#26494;&#24347;&#20026;K-&#31232;&#30095;&#29305;&#24449;&#21521;&#37327;&#38382;&#39064;&#65292;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26469;&#39640;&#25928;&#35299;&#20915;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#35745;&#31639;&#36317;&#31163;&#30697;&#38453;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#29616;&#26377;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#34920;&#26126;&#23427;&#20855;&#26377;&#24456;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08743v1 Announce Type: cross Abstract: The volume of image repositories continues to grow. Despite the availability of content-based addressing, we still lack a lightweight tool that allows us to discover images of distinct characteristics from a large collection. In this paper, we propose a fast and training-free algorithm for novel image discovery. The key of our algorithm is formulating a collection of images as a perceptual distance-weighted graph, within which our task is to locate the K-densest subgraph that corresponds to a subset of the most unique images. While solving this problem is not just NP-hard but also requires a full computation of the potentially huge distance matrix, we propose to relax it into a K-sparse eigenvector problem that we can efficiently solve using stochastic gradient descent (SGD) without explicitly computing the distance matrix. We compare our algorithm against state-of-the-arts on both synthetic and real datasets, showing that it is conside
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#38408;&#20540;&#20272;&#35745;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20307;&#32946;&#35774;&#26045;&#20013;&#26816;&#27979;&#33021;&#28304;&#24322;&#24120;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20248;&#21270;&#33021;&#28304;&#31649;&#29702;&#24182;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08742</link><description>&lt;p&gt;
&#25581;&#31034;&#38544;&#34255;&#30340;&#33021;&#28304;&#24322;&#24120;&#65306;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#20307;&#32946;&#35774;&#26045;&#33021;&#28304;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unveiling Hidden Energy Anomalies: Harnessing Deep Learning to Optimize Energy Management in Sports Facilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08742
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#38408;&#20540;&#20272;&#35745;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20307;&#32946;&#35774;&#26045;&#20013;&#26816;&#27979;&#33021;&#28304;&#24322;&#24120;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20248;&#21270;&#33021;&#28304;&#31649;&#29702;&#24182;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#32946;&#35774;&#26045;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#22240;&#20854;&#28508;&#21147;&#22312;&#20419;&#36827;&#33410;&#33021;&#21644;&#20248;&#21270;&#36816;&#33829;&#25928;&#29575;&#26041;&#38754;&#32780;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#22312;&#36825;&#31687;&#30740;&#31350;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#65292;&#22312;&#20307;&#32946;&#35774;&#26045;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#38382;&#39064;&#30340;&#25361;&#25112;&#21644;&#21069;&#26223;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#20174;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;DFNN&#65289;&#26469;&#36827;&#34892;&#38382;&#39064;&#24314;&#27169;&#65292;&#24182;&#24341;&#20837;&#38408;&#20540;&#20272;&#35745;&#25216;&#26415;&#26469;&#26377;&#25928;&#35782;&#21035;&#24322;&#24120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20943;&#23569;&#38169;&#35823;&#35686;&#25253;&#30340;&#26041;&#27861;&#65292;&#30830;&#20445;&#24322;&#24120;&#26816;&#27979;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#21345;&#22612;&#23572;&#22823;&#23398;&#30340;&#27700;&#20013;&#24515;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08742v1 Announce Type: cross Abstract: Anomaly detection in sport facilities has gained significant attention due to its potential to promote energy saving and optimizing operational efficiency. In this research article, we investigate the role of machine learning, particularly deep learning, in anomaly detection for sport facilities. We explore the challenges and perspectives of utilizing deep learning methods for this task, aiming to address the drawbacks and limitations of conventional approaches. Our proposed approach involves feature extraction from the data collected in sport facilities. We present a problem formulation using Deep Feedforward Neural Networks (DFNN) and introduce threshold estimation techniques to identify anomalies effectively. Furthermore, we propose methods to reduce false alarms, ensuring the reliability and accuracy of anomaly detection. To evaluate the effectiveness of our approach, we conduct experiments on aquatic center dataset at Qatar Univers
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39044;&#27979;&#23545;&#20598;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#23548;&#27169;&#22411;&#36924;&#36817;&#30495;&#23454;&#26465;&#20214;&#20998;&#24067;&#24182;&#20272;&#35745;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#36890;&#29992;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.08733</link><description>&lt;p&gt;
&#19987;&#23478;&#19981;&#20316;&#24330;: &#36890;&#36807;&#39044;&#27979;&#23545;&#20598;&#26469;&#23398;&#20064;&#26410;&#30693;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08733
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#23545;&#20598;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#23548;&#27169;&#22411;&#36924;&#36817;&#30495;&#23454;&#26465;&#20214;&#20998;&#24067;&#24182;&#20272;&#35745;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#36890;&#29992;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;${\widehat{p}}_{\theta}(Y|X)$&#23545;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;$p(Y|X)$&#30340;&#20102;&#35299;&#31243;&#24230;&#30340;&#20934;&#30830;&#35780;&#20272;&#23545;&#20110;&#36991;&#20813;&#20135;&#29983;&#38169;&#35823;&#25110;"&#34394;&#26500;"&#30340;&#31572;&#26696;&#25110;&#37319;&#21462;&#19981;&#23433;&#20840;&#30340;&#34892;&#20026;&#38750;&#24120;&#37325;&#35201;&#65292;&#28982;&#32780;&#36825;&#23545;&#20110;&#29983;&#25104;&#27169;&#22411;&#26469;&#35828;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#27010;&#29575;&#39044;&#27979;&#19981;&#33021;&#21306;&#20998;&#27599;&#20010;&#21709;&#24212;&#30340;&#22122;&#22768;&#65288;&#24191;&#20041;&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#23545;&#36807;&#31243;&#30340;&#19981;&#20102;&#35299;&#65288;&#19987;&#39064;&#19981;&#30830;&#23450;&#24615;&#65289;&#65292;&#32780;&#29616;&#26377;&#30340;&#19987;&#39064;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#24448;&#24448;&#22312;&#27169;&#22411;&#27424;&#25311;&#21512;&#26102;&#36807;&#20110;&#33258;&#20449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31574;&#30053;&#65292;&#21487;&#20197;&#25945;&#23548;&#27169;&#22411;&#21516;&#26102;&#36924;&#36817;$p(Y|X)$&#24182;&#20272;&#35745;${\widehat{p}}_{\theta}(Y|X)$&#19982;$p(Y|X)$&#20043;&#38388;&#30340;&#24046;&#36317;&#65306;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#26469;&#33258;&#30495;&#23454;&#26465;&#20214;&#20998;&#24067;&#30340;&#29420;&#31435;&#21709;&#24212;&#23545;&#65292;&#20801;&#35768;&#23427;&#22312;&#39044;&#27979;&#19968;&#20010;&#21709;&#24212;&#26102;&#35266;&#23519;&#21478;&#19968;&#20010;&#21709;&#24212;&#65292;&#28982;&#21518;&#27979;&#37327;&#23427;&#30340;&#20316;&#24330;&#31243;&#24230;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#27169;&#22411;&#30340;&#19987;&#39064;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08733v1 Announce Type: new Abstract: Identifying how much a model ${\widehat{p}}_{\theta}(Y|X)$ knows about the stochastic real-world process $p(Y|X)$ it was trained on is important to ensure it avoids producing incorrect or "hallucinated" answers or taking unsafe actions. But this is difficult for generative models because probabilistic predictions do not distinguish between per-response noise (aleatoric uncertainty) and lack of knowledge about the process (epistemic uncertainty), and existing epistemic uncertainty quantification techniques tend to be overconfident when the model underfits. We propose a general strategy for teaching a model to both approximate $p(Y|X)$ and also estimate the remaining gaps between ${\widehat{p}}_{\theta}(Y|X)$ and $p(Y|X)$: train it to predict pairs of independent responses drawn from the true conditional distribution, allow it to "cheat" by observing one response while predicting the other, then measure how much it cheats. Remarkably, we pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#24403;&#27599;&#20010;&#27979;&#37327;&#30340;&#27604;&#29305;&#20165;&#19982;&#23569;&#25968;&#20854;&#20182;&#27979;&#37327;&#30340;&#27604;&#29305;&#30456;&#20851;&#26102;&#65292;&#26410;&#35757;&#32451;&#30340;&#32593;&#32476;&#29983;&#25104;&#30340;&#20989;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#65307;&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#33021;&#22815;&#23436;&#32654;&#25311;&#21512;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#35757;&#32451;&#21518;&#29983;&#25104;&#30340;&#20989;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#20173;&#28982;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#65307;&#21516;&#26102;&#32771;&#34385;&#20102;&#32479;&#35745;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08726</link><description>&lt;p&gt;
&#35757;&#32451;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26159;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Trained quantum neural networks are Gaussian processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#24403;&#27599;&#20010;&#27979;&#37327;&#30340;&#27604;&#29305;&#20165;&#19982;&#23569;&#25968;&#20854;&#20182;&#27979;&#37327;&#30340;&#27604;&#29305;&#30456;&#20851;&#26102;&#65292;&#26410;&#35757;&#32451;&#30340;&#32593;&#32476;&#29983;&#25104;&#30340;&#20989;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#65307;&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#33021;&#22815;&#23436;&#32654;&#25311;&#21512;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#35757;&#32451;&#21518;&#29983;&#25104;&#30340;&#20989;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#20173;&#28982;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#65307;&#21516;&#26102;&#32771;&#34385;&#20102;&#32479;&#35745;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#30001;&#21442;&#25968;&#21270;&#30340;&#19968;&#27604;&#29305;&#38376;&#21644;&#22266;&#23450;&#30340;&#20004;&#27604;&#29305;&#38376;&#26500;&#25104;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#26080;&#31351;&#23485;&#24230;&#30340;&#26497;&#38480;&#19979;&#65292;&#29983;&#25104;&#30340;&#20989;&#25968;&#26159;&#25152;&#26377;&#27604;&#29305;&#19978;&#21333;&#27604;&#29305;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#30340;&#21644;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#38543;&#26426;&#21021;&#22987;&#21270;&#21442;&#25968;&#30340;&#26410;&#35757;&#32451;&#32593;&#32476;&#25152;&#29983;&#25104;&#30340;&#20989;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#22312;&#27599;&#20010;&#27979;&#37327;&#27604;&#29305;&#20165;&#19982;&#23569;&#25968;&#20854;&#20182;&#27979;&#37327;&#27604;&#29305;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#21644;&#24179;&#26041;&#25439;&#22833;&#20989;&#25968;&#23545;&#32593;&#32476;&#30340;&#35757;&#32451;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#32593;&#32476;&#27809;&#26377;&#21463;&#21040;&#36139;&#30240;&#39640;&#21407;&#30340;&#24433;&#21709;&#65292;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#21487;&#20197;&#23436;&#32654;&#22320;&#25311;&#21512;&#35757;&#32451;&#38598;&#65292;&#35757;&#32451;&#21518;&#25152;&#29983;&#25104;&#30340;&#20989;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#20173;&#28982;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32479;&#35745;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08726v1 Announce Type: cross Abstract: We study quantum neural networks made by parametric one-qubit gates and fixed two-qubit gates in the limit of infinite width, where the generated function is the expectation value of the sum of single-qubit observables over all the qubits. First, we prove that the probability distribution of the function generated by the untrained network with randomly initialized parameters converges in distribution to a Gaussian process whenever each measured qubit is correlated only with few other measured qubits. Then, we analytically characterize the training of the network via gradient descent with square loss on supervised learning problems. We prove that, as long as the network is not affected by barren plateaus, the trained network can perfectly fit the training set and that the probability distribution of the function generated after training still converges in distribution to a Gaussian process. Finally, we consider the statistical noise of t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRDP&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#31471;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#40657;&#30418;&#22870;&#21169;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#25552;&#31034;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08714</link><description>&lt;p&gt;
PRDP&#65306;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#36817;&#31471;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#29992;&#20110;&#22870;&#21169;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRDP&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#31471;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#40657;&#30418;&#22870;&#21169;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#25552;&#31034;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#24494;&#35843;&#24050;&#25104;&#20026;&#23558;&#22522;&#30784;&#27169;&#22411;&#19982;&#19979;&#28216;&#30446;&#26631;&#23545;&#40784;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#22312;&#35821;&#35328;&#39046;&#22495;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#26368;&#22823;&#21270;&#21453;&#26144;&#20154;&#31867;&#20559;&#22909;&#30340;&#22870;&#21169;&#24050;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#35270;&#35273;&#39046;&#22495;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;RL&#30340;&#22870;&#21169;&#24494;&#35843;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#20013;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#65292;&#20351;&#23427;&#20204;&#26080;&#27861;&#25512;&#24191;&#21040;&#22797;&#26434;&#30340;&#12289;&#26410;&#30693;&#30340;&#25552;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#31471;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#65288;PRDP&#65289;&#65292;&#39318;&#27425;&#22312;&#36229;&#36807;100K&#20010;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25552;&#31034;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#40657;&#30418;&#22870;&#21169;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#65288;RDP&#65289;&#30446;&#26631;&#65292;&#35813;&#30446;&#26631;&#19982;RL&#30446;&#26631;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#20139;&#21463;&#26356;&#22909;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08714v1 Announce Type: cross Abstract: Reward finetuning has emerged as a promising approach to aligning foundation models with downstream objectives. Remarkable success has been achieved in the language domain by using reinforcement learning (RL) to maximize rewards that reflect human preference. However, in the vision domain, existing RL-based reward finetuning methods are limited by their instability in large-scale training, rendering them incapable of generalizing to complex, unseen prompts. In this paper, we propose Proximal Reward Difference Prediction (PRDP), enabling stable black-box reward finetuning for diffusion models for the first time on large-scale prompt datasets with over 100K prompts. Our key innovation is the Reward Difference Prediction (RDP) objective that has the same optimal solution as the RL objective while enjoying better training stability. Specifically, the RDP objective is a supervised regression objective that tasks the diffusion model with pred
&lt;/p&gt;</description></item><item><title>BECoTTA&#26159;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#30340;&#39640;&#25928;CTTA&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;MoDE&#65288;Mixture-of-Domain Low-rank Experts&#65289;&#27169;&#22411;&#65292;&#23427;&#21253;&#21547;&#39046;&#22495;&#33258;&#36866;&#24212;&#36335;&#30001;&#21644;&#39046;&#22495;&#19987;&#23478;&#21327;&#21516;&#25439;&#22833;&#20004;&#20010;&#26680;&#24515;&#32452;&#20214;&#65292;&#33021;&#22815;&#22312;&#25345;&#32493;&#30340;&#27979;&#35797;&#26102;&#38388;&#20013;&#33258;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#65292;&#21516;&#26102;&#21482;&#38656;&#36739;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.08712</link><description>&lt;p&gt;
BECoTTA: &#22522;&#20110;&#36755;&#20837;&#30340;&#22312;&#32447;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#25345;&#32493;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08712
&lt;/p&gt;
&lt;p&gt;
BECoTTA&#26159;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#30340;&#39640;&#25928;CTTA&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;MoDE&#65288;Mixture-of-Domain Low-rank Experts&#65289;&#27169;&#22411;&#65292;&#23427;&#21253;&#21547;&#39046;&#22495;&#33258;&#36866;&#24212;&#36335;&#30001;&#21644;&#39046;&#22495;&#19987;&#23478;&#21327;&#21516;&#25439;&#22833;&#20004;&#20010;&#26680;&#24515;&#32452;&#20214;&#65292;&#33021;&#22815;&#22312;&#25345;&#32493;&#30340;&#27979;&#35797;&#26102;&#38388;&#20013;&#33258;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#65292;&#21516;&#26102;&#21482;&#38656;&#36739;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;CTTA&#65289;&#35201;&#27714;&#22312;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#26410;&#30693;&#39046;&#22495;&#30340;&#21516;&#26102;&#20445;&#30041;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;CTTA&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#24536;&#35760;&#36866;&#24212;&#26435;&#34913;&#21644;&#25928;&#29575;&#20173;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;CTTA&#22330;&#26223;&#20165;&#20551;&#35774;&#23384;&#22312;&#19981;&#30456;&#20132;&#30340;&#24773;&#20917;&#65292;&#32780;&#23454;&#38469;&#19990;&#30028;&#30340;&#39046;&#22495;&#26159;&#26080;&#32541;&#21464;&#21270;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BECoTTA&#30340;&#22522;&#20110;&#36755;&#20837;&#30340;&#39640;&#25928;CTTA&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MoDE&#65288;Mixture-of-Domain Low-rank Experts&#65289;&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#26680;&#24515;&#32452;&#20214;&#65306;i&#65289;&#39046;&#22495;&#33258;&#36866;&#24212;&#36335;&#30001;&#65292;&#36890;&#36807;&#22810;&#20010;&#39046;&#22495;&#36335;&#30001;&#22120;&#26377;&#36873;&#25321;&#22320;&#25429;&#25417;&#39046;&#22495;&#33258;&#36866;&#24212;&#30693;&#35782;&#65292;&#21644;ii&#65289;&#39046;&#22495;&#19987;&#23478;&#21327;&#21516;&#25439;&#22833;&#65292;&#20197;&#22686;&#21152;&#27599;&#20010;&#39046;&#22495;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22810;&#20010;CTTA&#22330;&#26223;&#65292;&#21253;&#25324;&#19981;&#30456;&#20132;&#21644;&#28176;&#21464;&#39046;&#22495;&#20999;&#25442;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;&#22823;&#32422;98&#65285;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08712v1 Announce Type: new Abstract: Continual Test Time Adaptation (CTTA) is required to adapt efficiently to continuous unseen domains while retaining previously learned knowledge. However, despite the progress of CTTA, forgetting-adaptation trade-offs and efficiency are still unexplored. Moreover, current CTTA scenarios assume only the disjoint situation, even though real-world domains are seamlessly changed. To tackle these challenges, this paper proposes BECoTTA, an input-dependent yet efficient framework for CTTA. We propose Mixture-of-Domain Low-rank Experts (MoDE) that contains two core components: i) Domain-Adaptive Routing, which aids in selectively capturing the domain-adaptive knowledge with multiple domain routers, and (ii) Domain-Expert Synergy Loss to maximize the dependency between each domain and expert. We validate our method outperforms multiple CTTA scenarios including disjoint and gradual domain shits, while only requiring ~98% fewer trainable parameters
&lt;/p&gt;</description></item><item><title>&#20462;&#27491;&#20102;&#12298;&#23545;&#20110;&#25968;&#20540;&#36924;&#36817;&#36941;&#21382;SDE&#30340;&#20998;&#24067;&#30340;Wasserstein&#36317;&#31163;&#20272;&#35745;&#12299;&#20013;&#30340;&#38169;&#35823;&#23616;&#37096;&#35823;&#24046;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20998;&#26512;&#25968;&#20540;&#31163;&#25955;&#36941;&#21382;SDE&#30340;Wasserstein-2&#36317;&#31163;&#30340;&#38750;&#28176;&#36817;&#20445;&#35777;&#65292;&#24182;&#35299;&#20915;&#20102;&#23454;&#36341;&#20013;&#32500;&#24230;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08711</link><description>&lt;p&gt;
&#12298;&#23545;&#20110;&#25968;&#20540;&#36924;&#36817;&#36941;&#21382;SDE&#30340;&#20998;&#24067;&#30340;Wasserstein&#36317;&#31163;&#20272;&#35745;&#12299;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Correction to "Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations"
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08711
&lt;/p&gt;
&lt;p&gt;
&#20462;&#27491;&#20102;&#12298;&#23545;&#20110;&#25968;&#20540;&#36924;&#36817;&#36941;&#21382;SDE&#30340;&#20998;&#24067;&#30340;Wasserstein&#36317;&#31163;&#20272;&#35745;&#12299;&#20013;&#30340;&#38169;&#35823;&#23616;&#37096;&#35823;&#24046;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20998;&#26512;&#25968;&#20540;&#31163;&#25955;&#36941;&#21382;SDE&#30340;Wasserstein-2&#36317;&#31163;&#30340;&#38750;&#28176;&#36817;&#20445;&#35777;&#65292;&#24182;&#35299;&#20915;&#20102;&#23454;&#36341;&#20013;&#32500;&#24230;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;San-Serna&#21644;Zygalakis&#30340;&#12298;&#23545;&#20110;&#25968;&#20540;&#36924;&#36817;&#36941;&#21382;SDE&#30340;&#20998;&#24067;&#30340;Wasserstein&#36317;&#31163;&#20272;&#35745;&#12299;&#20013;&#30340;&#38750;&#28176;&#36817;&#20445;&#35777;&#25968;&#20540;&#31163;&#25955;&#20998;&#26512;&#26041;&#27861;&#36827;&#34892;&#20102;&#20462;&#27491;&#12290;&#20182;&#20204;&#20998;&#26512;&#20102;UBU&#31215;&#20998;&#22120;&#65292;&#35813;&#31215;&#20998;&#22120;&#26159;&#20108;&#38454;&#24378;&#22411;&#30340;&#65292;&#24182;&#19988;&#27599;&#20010;&#27493;&#39588;&#21482;&#38656;&#35201;&#19968;&#27425;&#26799;&#24230;&#35780;&#20272;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#29702;&#24819;&#30340;&#38750;&#28176;&#36817;&#20445;&#35777;&#65292;&#29305;&#21035;&#26159;&#22312;Wasserstein-2&#36317;&#31163;&#20013;&#21040;&#36798;&#31163;&#30446;&#26631;&#20998;&#24067; $\epsilon &gt; 0$ &#30340;&#36317;&#31163;&#20165;&#38656; $\mathcal{O}(d^{1/4}\epsilon^{-1/2})$ &#27493;&#12290;&#28982;&#32780;&#65292;Sanz-Serna&#21644;Zygalakis (2021)&#20013;&#30340;&#23616;&#37096;&#35823;&#24046;&#20272;&#35745;&#23384;&#22312;&#38169;&#35823;&#65292;&#22312;&#23454;&#36341;&#20013;&#38656;&#35201;&#26356;&#24378;&#30340;&#20551;&#35774;&#25165;&#33021;&#23454;&#29616;&#36825;&#20123;&#22797;&#26434;&#24230;&#20272;&#35745;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#29702;&#35770;&#19982;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#30340;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#32500;&#24230;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08711v1 Announce Type: cross Abstract: A method for analyzing non-asymptotic guarantees of numerical discretizations of ergodic SDEs in Wasserstein-2 distance is presented by Sanz-Serna and Zygalakis in ``Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations". They analyze the UBU integrator which is strong order two and only requires one gradient evaluation per step, resulting in desirable non-asymptotic guarantees, in particular $\mathcal{O}(d^{1/4}\epsilon^{-1/2})$ steps to reach a distance of $\epsilon &gt; 0$ in Wasserstein-2 distance away from the target distribution. However, there is a mistake in the local error estimates in Sanz-Serna and Zygalakis (2021), in particular, a stronger assumption is needed to achieve these complexity estimates. This note reconciles the theory with the dimension dependence observed in practice in many applications of interest.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#27169;&#22411;&#20998;&#25968;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;-shot&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;SiMGen&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#23376;&#29983;&#25104;&#25216;&#26415;&#65292;&#33021;&#22815;&#26500;&#24314;&#22823;&#20998;&#23376;&#12290;</title><link>https://arxiv.org/abs/2402.08708</link><description>&lt;p&gt;
&#36890;&#36807;&#30456;&#20284;&#24615;&#26680;&#23454;&#29616;&#38646;-shot&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Zero Shot Molecular Generation via Similarity Kernels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08708
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#27169;&#22411;&#20998;&#25968;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;-shot&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;SiMGen&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#23376;&#29983;&#25104;&#25216;&#26415;&#65292;&#33021;&#22815;&#26500;&#24314;&#22823;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#25552;&#20986;&#20855;&#26377;&#29702;&#24819;&#23646;&#24615;&#30340;&#32467;&#26500;&#21152;&#36895;&#26032;&#21270;&#23398;&#29289;&#36136;&#30340;&#21457;&#29616;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#20998;&#25968;&#25110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#20854;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#20998;&#25968;&#19982;&#29289;&#29702;&#21147;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#65292;&#21487;&#20197;&#20351;&#29992;&#24378;&#22823;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23398;&#21040;&#30340;&#20998;&#25968;&#30340;&#34892;&#20026;&#23578;&#19981;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#20998;&#26512;&#20998;&#25968;&#20197;&#36827;&#34892;&#20998;&#23376;&#29983;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#20998;&#25968;&#19968;&#24320;&#22987;&#31867;&#20284;&#20110;&#24674;&#22797;&#24615;&#33021;&#21183;&#65292;&#24182;&#22312;&#26368;&#21518;&#31867;&#20284;&#20110;&#37327;&#23376;&#21147;&#23398;&#21147;&#12290;&#22312;&#36825;&#20004;&#20010;&#31471;&#28857;&#20043;&#38388;&#65292;&#23427;&#34920;&#29616;&#20986;&#33021;&#22815;&#26500;&#24314;&#22823;&#20998;&#23376;&#30340;&#29305;&#27530;&#23646;&#24615;&#12290;&#21033;&#29992;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#23376;&#29983;&#25104;&#65288;SiMGen&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#38646;-shot&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08708v1 Announce Type: cross Abstract: Generative modelling aims to accelerate the discovery of novel chemicals by directly proposing structures with desirable properties. Recently, score-based, or diffusion, generative models have significantly outperformed previous approaches. Key to their success is the close relationship between the score and physical force, allowing the use of powerful equivariant neural networks. However, the behaviour of the learnt score is not yet well understood. Here, we analyse the score by training an energy-based diffusion model for molecular generation. We find that during the generation the score resembles a restorative potential initially and a quantum-mechanical force at the end. In between the two endpoints, it exhibits special properties that enable the building of large molecules. Using insights from the trained model, we present Similarity-based Molecular Generation (SiMGen), a new method for zero shot molecular generation. SiMGen combin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26041;&#27861;&#26469;&#39537;&#21160;AI&#33647;&#29289;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#29983;&#25104;&#20004;&#20010;&#20027;&#35201;&#20027;&#39064;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#23376;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#24182;&#27604;&#36739;&#20102;&#39030;&#32423;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08703</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#27010;&#36848;&#65306;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#29983;&#25104;&#30340;&#26032;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
A Survey of Generative AI for De Novo Drug Design: New Frontiers in Molecule and Protein Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08703
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26041;&#27861;&#26469;&#39537;&#21160;AI&#33647;&#29289;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#29983;&#25104;&#20004;&#20010;&#20027;&#35201;&#20027;&#39064;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#23376;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#24182;&#27604;&#36739;&#20102;&#39030;&#32423;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39537;&#21160;&#30340;&#26041;&#27861;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#21382;&#26469;&#20195;&#20215;&#39640;&#26114;&#30340;&#33647;&#29289;&#35774;&#35745;&#36807;&#31243;&#65292;&#21508;&#31181;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#24191;&#27867;&#20351;&#29992;&#20013;&#12290;&#29305;&#21035;&#26159;&#38024;&#23545;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#20174;&#38646;&#24320;&#22987;&#21019;&#24314;&#26032;&#30340;&#29983;&#29289;&#21270;&#21512;&#29289;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;&#35813;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21152;&#19978;&#33647;&#29289;&#35774;&#35745;&#36807;&#31243;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#20026;&#26032;&#30740;&#31350;&#20154;&#21592;&#36827;&#20837;&#21019;&#36896;&#20102;&#19968;&#20010;&#22256;&#38590;&#30340;&#23616;&#38754;&#12290;&#22312;&#36825;&#20221;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23558;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20998;&#20026;&#20004;&#20010;&#20027;&#35201;&#20027;&#39064;&#65306;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#29983;&#25104;&#12290;&#22312;&#27599;&#20010;&#20027;&#39064;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21508;&#31181;&#23376;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#37325;&#35201;&#30340;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#23545;&#39030;&#32423;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#24191;&#20041;&#30340;&#26041;&#27861;&#26469;&#39537;&#21160;AI&#33647;&#29289;&#35774;&#35745;&#65292;&#20801;&#35768;&#22312;&#27599;&#20010;&#23376;&#20219;&#21153;&#20013;&#36827;&#34892;&#21508;&#31181;&#26041;&#27861;&#30340;&#24494;&#35266;&#27604;&#36739;&#21644;&#23439;&#35266;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08703v1 Announce Type: cross Abstract: Artificial intelligence (AI)-driven methods can vastly improve the historically costly drug design process, with various generative models already in widespread use. Generative models for de novo drug design, in particular, focus on the creation of novel biological compounds entirely from scratch, representing a promising future direction. Rapid development in the field, combined with the inherent complexity of the drug design process, creates a difficult landscape for new researchers to enter. In this survey, we organize de novo drug design into two overarching themes: small molecule and protein generation. Within each theme, we identify a variety of subtasks and applications, highlighting important datasets, benchmarks, and model architectures and comparing the performance of top models. We take a broad approach to AI-driven drug design, allowing for both micro-level comparisons of various methods within each subtask and macro-level o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30340;&#22312;&#32447;&#26377;&#30028;&#20998;&#37197;&#21644;&#22312;&#32447;&#24191;&#21578;&#25293;&#21334;&#38382;&#39064;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#26681;&#25454;&#39044;&#27979;&#36136;&#37327;&#35843;&#25972;&#24615;&#33021;&#30340;&#31639;&#27861;&#65292;&#22312;&#39044;&#27979;&#20934;&#30830;&#26102;&#36229;&#36807;&#20102;&#20808;&#21069;&#24615;&#33021;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.08701</link><description>&lt;p&gt;
&#20855;&#26377;&#39044;&#27979;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#29992;&#20110;&#22312;&#32447;&#26377;&#30028;&#20998;&#37197;&#21644;&#24191;&#21578;&#25293;&#21334;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Primal-Dual Algorithms with Predictions for Online Bounded Allocation and Ad-Auctions Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08701
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30340;&#22312;&#32447;&#26377;&#30028;&#20998;&#37197;&#21644;&#22312;&#32447;&#24191;&#21578;&#25293;&#21334;&#38382;&#39064;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#26681;&#25454;&#39044;&#27979;&#36136;&#37327;&#35843;&#25972;&#24615;&#33021;&#30340;&#31639;&#27861;&#65292;&#22312;&#39044;&#27979;&#20934;&#30830;&#26102;&#36229;&#36807;&#20102;&#20808;&#21069;&#24615;&#33021;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21305;&#37197;&#38382;&#39064;&#22312;&#30740;&#31350;&#30028;&#24191;&#27867;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#20247;&#22810;&#24212;&#29992;&#30340;&#24191;&#21578;&#25293;&#21334;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#32463;&#20856;&#31639;&#27861;&#26159;&#21542;&#33021;&#20174;&#26426;&#22120;&#23398;&#20064;&#20013;&#21463;&#30410;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#21363;&#20351;&#22312;&#21305;&#37197;&#38382;&#39064;&#20013;&#33719;&#24471;&#19968;&#23567;&#37096;&#20998;&#24615;&#33021;&#25913;&#36827;&#65292;&#20063;&#21487;&#33021;&#20026;&#30740;&#31350;&#30340;&#20351;&#29992;&#26696;&#20363;&#24102;&#26469;&#26174;&#33879;&#25910;&#30410;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30340;&#22312;&#32447;&#26377;&#30028;&#20998;&#37197;&#21644;&#22312;&#32447;&#24191;&#21578;&#25293;&#21334;&#38382;&#39064;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22522;&#20110;&#39044;&#27979;&#36136;&#37327;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#65292;&#20854;&#24615;&#33021;&#31454;&#20105;&#21147;&#24378;&#12290;&#24403;&#39044;&#27979;&#20934;&#30830;&#26102;&#65292;&#31639;&#27861;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#24615;&#33021;&#30028;&#38480;&#65307;&#32780;&#24403;&#39044;&#27979;&#19981;&#20934;&#30830;&#26102;&#65292;&#31639;&#27861;&#30340;&#24615;&#33021;&#20173;&#28982;&#21487;&#25509;&#21463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08701v1 Announce Type: cross Abstract: Matching problems have been widely studied in the research community, especially Ad-Auctions with many applications ranging from network design to advertising. Following the various advancements in machine learning, one natural question is whether classical algorithms can benefit from machine learning and obtain better-quality solutions. Even a small percentage of performance improvement in matching problems could result in significant gains for the studied use cases. For example, the network throughput or the revenue of Ad-Auctions can increase remarkably. This paper presents algorithms with machine learning predictions for the Online Bounded Allocation and the Online Ad-Auctions problems. We constructed primal-dual algorithms that achieve competitive performance depending on the quality of the predictions. When the predictions are accurate, the algorithms' performance surpasses previous performance bounds, while when the predictions a
&lt;/p&gt;</description></item><item><title>&#26080;&#38656;&#20154;&#24037;&#31574;&#21010;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24448;&#36820;&#27491;&#30830;&#24615;&#65288;RTC&#65289;&#20316;&#20026;&#35780;&#20272;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;RTC&#21487;&#20197;&#22312;&#26356;&#24191;&#27867;&#30340;&#30495;&#23454;&#19990;&#30028;&#36719;&#20214;&#39046;&#22495;&#23545;&#20195;&#30721;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#22522;&#20934;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08699</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35780;&#20272;&#20855;&#26377;&#24448;&#36820;&#27491;&#30830;&#24615;&#30340;&#20195;&#30721;LLMs
&lt;/p&gt;
&lt;p&gt;
Unsupervised Evaluation of Code LLMs with Round-Trip Correctness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08699
&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#20154;&#24037;&#31574;&#21010;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24448;&#36820;&#27491;&#30830;&#24615;&#65288;RTC&#65289;&#20316;&#20026;&#35780;&#20272;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;RTC&#21487;&#20197;&#22312;&#26356;&#24191;&#27867;&#30340;&#30495;&#23454;&#19990;&#30028;&#36719;&#20214;&#39046;&#22495;&#23545;&#20195;&#30721;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#22522;&#20934;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35780;&#20272;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#30740;&#31350;&#19968;&#30452;&#20381;&#36182;&#20110;&#19968;&#20123;&#23567;&#30340;&#25163;&#21160;&#31574;&#21010;&#30340;&#22522;&#20934;&#65292;&#22914;HumanEval&#21644;MBPP&#65292;&#36825;&#20123;&#22522;&#20934;&#21482;&#20195;&#34920;&#20102;&#30495;&#23454;&#19990;&#30028;&#36719;&#20214;&#39046;&#22495;&#30340;&#19968;&#20010;&#29421;&#31364;&#37096;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24448;&#36820;&#27491;&#30830;&#24615;&#65288;RTC&#65289;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#35780;&#20272;&#26041;&#27861;&#12290;RTC&#20801;&#35768;&#22312;&#26356;&#24191;&#27867;&#30340;&#30495;&#23454;&#19990;&#30028;&#36719;&#20214;&#39046;&#22495;&#23545;&#20195;&#30721;LLM&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#26080;&#38656;&#26114;&#36149;&#30340;&#20154;&#24037;&#31574;&#21010;&#12290;RTC&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#25105;&#20204;&#21487;&#20197;&#35201;&#27714;&#27169;&#22411;&#20570;&#20986;&#39044;&#27979;&#65288;&#20363;&#22914;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19968;&#20123;&#20195;&#30721;&#65289;&#65292;&#23558;&#35813;&#39044;&#27979;&#36820;&#22238;&#65288;&#20363;&#22914;&#20174;&#39044;&#27979;&#30340;&#25551;&#36848;&#20013;&#21512;&#25104;&#20195;&#30721;&#65289;&#65292;&#24182;&#26816;&#26597;&#36825;&#20010;&#24448;&#36820;&#36807;&#31243;&#26159;&#21542;&#23548;&#33268;&#19982;&#21407;&#22987;&#36755;&#20837;&#35821;&#20041;&#31561;&#25928;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;RTC&#26469;&#35780;&#20272;&#20195;&#30721;&#21512;&#25104;&#21644;&#32534;&#36753;&#12290;&#25105;&#20204;&#21457;&#29616;RTC&#19982;&#29616;&#26377;&#29421;&#31364;&#39046;&#22495;&#20195;&#30721;&#21512;&#25104;&#22522;&#20934;&#19978;&#30340;&#27169;&#22411;&#24615;&#33021;&#24378;&#30456;&#20851;&#65292;&#21516;&#26102;&#20063;&#20801;&#35768;&#25105;&#20204;&#25193;&#23637;&#21040;&#26356;&#24191;&#38420;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08699v1 Announce Type: cross Abstract: To evaluate code large language models (LLMs), research has relied on a few small manually curated benchmarks, such as HumanEval and MBPP, which represent a narrow part of the real-world software domains. In this work, we introduce round-trip correctness (RTC) as an alternative evaluation method. RTC allows Code LLM evaluation on a broader spectrum of real-world software domains without the need for costly human curation. RTC rests on the idea that we can ask a model to make a prediction (e.g., describe some code using natural language), feed that prediction back (e.g., synthesize code from the predicted description), and check if this round-trip leads to code that is semantically equivalent to the original input. We show how to employ RTC to evaluate code synthesis and editing. We find that RTC strongly correlates with model performance on existing narrow-domain code synthesis benchmarks while allowing us to expand to a much broader se
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#26469;&#35299;&#20915;&#38271;&#23614;&#25928;&#24212;&#38382;&#39064;&#65292;&#25552;&#39640;&#23545;&#20110;&#21253;&#21547;&#25361;&#25112;&#24615;&#22330;&#26223;&#30340;&#25968;&#25454;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08698</link><description>&lt;p&gt;
AMEND&#65306;&#19968;&#31181;&#29992;&#20110;&#38271;&#23614;&#36712;&#36857;&#39044;&#27979;&#30340;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AMEND: A Mixture of Experts Framework for Long-tailed Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#26469;&#35299;&#20915;&#38271;&#23614;&#25928;&#24212;&#38382;&#39064;&#65292;&#25552;&#39640;&#23545;&#20110;&#21253;&#21547;&#25361;&#25112;&#24615;&#22330;&#26223;&#30340;&#25968;&#25454;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#39044;&#27979;&#34892;&#20154;&#26410;&#26469;&#30340;&#21160;&#21521;&#23545;&#20110;&#26234;&#33021;&#39550;&#39542;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#24320;&#21457;&#36825;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#38656;&#35201;&#21253;&#21547;&#22810;&#26679;&#26679;&#26412;&#30340;&#20016;&#23500;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#28982;&#36712;&#36857;&#39044;&#27979;&#25968;&#25454;&#38598;&#36890;&#24120;&#23545;&#31616;&#21333;&#26679;&#26412;&#20559;&#37325;&#65292;&#24182;&#32570;&#20047;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#12290;&#36825;&#31181;&#38271;&#23614;&#25928;&#24212;&#23548;&#33268;&#39044;&#27979;&#27169;&#22411;&#22312;&#21253;&#21547;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#23614;&#37096;&#37096;&#20998;&#34920;&#29616;&#19981;&#20339;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#31867;&#26465;&#20214;&#36229;&#32593;&#32476;&#31561;&#26041;&#27861;&#35299;&#20915;&#20102;&#38271;&#23614;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#26159;&#27169;&#22359;&#21270;&#30340;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#19987;&#38376;&#30340;&#19987;&#23478;&#28151;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#27599;&#20010;&#19987;&#23478;&#37117;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#37096;&#20998;&#30340;&#29305;&#27530;&#25216;&#33021;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08698v1 Announce Type: cross Abstract: Accurate prediction of pedestrians' future motions is critical for intelligent driving systems. Developing models for this task requires rich datasets containing diverse sets of samples. However, the existing naturalistic trajectory prediction datasets are generally imbalanced in favor of simpler samples and lack challenging scenarios. Such a long-tail effect causes prediction models to underperform on the tail portion of the data distribution containing safety-critical scenarios. Previous methods tackle the long-tail problem using methods such as contrastive learning and class-conditioned hypernetworks. These approaches, however, are not modular and cannot be applied to many machine learning architectures. In this work, we propose a modular model-agnostic framework for trajectory prediction that leverages a specialized mixture of experts. In our approach, each expert is trained with a specialized skill with respect to a particular part
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23545;&#25239;&#32773;&#65292;&#21487;&#20197;&#37325;&#26032;&#35757;&#32451;&#24102;&#29305;&#27931;&#20234;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#21487;&#20197;&#35268;&#36991;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#36755;&#20986;&#30340;&#29305;&#27931;&#20234;&#27169;&#22411;&#26816;&#27979;&#22120;&#65292;&#21516;&#26102;&#30830;&#20445;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08695</link><description>&lt;p&gt;
Game of Trojans: &#33258;&#36866;&#24212;&#30340;&#23545;&#25239;&#32773;&#23545;&#25239;&#22522;&#20110;&#36755;&#20986;&#30340;&#29305;&#27931;&#20234;&#27169;&#22411;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Game of Trojans: Adaptive Adversaries Against Output-based Trojaned-Model Detectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23545;&#25239;&#32773;&#65292;&#21487;&#20197;&#37325;&#26032;&#35757;&#32451;&#24102;&#29305;&#27931;&#20234;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#21487;&#20197;&#35268;&#36991;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#36755;&#20986;&#30340;&#29305;&#27931;&#20234;&#27169;&#22411;&#26816;&#27979;&#22120;&#65292;&#21516;&#26102;&#30830;&#20445;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23545;&#25163;&#65292;&#21487;&#20197;&#37325;&#26032;&#35757;&#32451;&#24102;&#26377;&#29305;&#27931;&#20234;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#20102;&#35299;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#36755;&#20986;&#30340;&#29305;&#27931;&#20234;&#27169;&#22411;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#23545;&#25163;&#21487;&#20197;&#30830;&#20445;&#65288;1&#65289;&#23545;&#20110;&#23884;&#20837;&#35302;&#21457;&#22120;&#21644;&#24178;&#20928;&#26679;&#26412;&#37117;&#20855;&#26377;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#65288;2&#65289;&#21487;&#20197;&#35268;&#36991;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#39640;&#32500;&#24230;&#25552;&#20379;&#36275;&#22815;&#30340;&#33258;&#30001;&#24230;&#26469;&#21516;&#26102;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20801;&#35768;&#37325;&#26032;&#35757;&#32451;&#26469;&#20351;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#36866;&#24212;&#24615;&#65292;&#20197;&#37325;&#26032;&#26657;&#20934;&#20854;&#21442;&#25968;&#65292;&#20174;&#32780;&#27169;&#25311;&#29305;&#27931;&#20234;&#27169;&#22411;&#21644;&#26816;&#27979;&#22120;&#21442;&#25968;&#30340;&#20849;&#21516;&#28436;&#21270;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20849;&#21516;&#36827;&#21270;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#36845;&#20195;&#21338;&#24328;&#65292;&#24182;&#35777;&#26126;&#20102;&#24471;&#21040;&#30340;&#65288;&#26368;&#20248;&#65289;&#35299;&#20915;&#26041;&#26696;&#20250;&#20351;&#23545;&#25163;&#25104;&#21151;&#22320;&#23454;&#29616;&#19978;&#36848;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#23545;&#25163;&#25552;&#20379;&#20102;&#19968;&#20010;&#36138;&#23146;&#31639;&#27861;&#26469;&#36873;&#25321;&#26368;&#23569;&#30340;&#36755;&#20837;&#26679;&#26412;&#36827;&#34892;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08695v1 Announce Type: cross Abstract: We propose and analyze an adaptive adversary that can retrain a Trojaned DNN and is also aware of SOTA output-based Trojaned model detectors. We show that such an adversary can ensure (1) high accuracy on both trigger-embedded and clean samples and (2) bypass detection. Our approach is based on an observation that the high dimensionality of the DNN parameters provides sufficient degrees of freedom to simultaneously achieve these objectives. We also enable SOTA detectors to be adaptive by allowing retraining to recalibrate their parameters, thus modeling a co-evolution of parameters of a Trojaned model and detectors. We then show that this co-evolution can be modeled as an iterative game, and prove that the resulting (optimal) solution of this interactive game leads to the adversary successfully achieving the above objectives. In addition, we provide a greedy algorithm for the adversary to select a minimum number of input samples for emb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#37319;&#26679;MRI&#37325;&#26500;&#36807;&#31243;&#20013;&#21033;&#29992;&#25512;&#29702;&#38454;&#27573;&#38477;&#22122;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#26465;&#20214;&#36229;&#21442;&#25968;&#32593;&#32476;&#65292;&#28040;&#38500;&#20102;&#25968;&#25454;&#22686;&#24378;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#21508;&#31181;&#22122;&#22768;&#27700;&#24179;&#19979;&#20445;&#25345;&#31283;&#20581;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#38454;&#27573;&#20135;&#29983;&#39640;&#28165;&#26224;&#24230;&#37325;&#26500;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#36798;&#21040;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08692</link><description>&lt;p&gt;
&#31232;&#30095;&#37319;&#26679;MRI&#37325;&#26500;&#30340;&#25512;&#29702;&#38454;&#27573;&#38477;&#22122;
&lt;/p&gt;
&lt;p&gt;
Inference Stage Denoising for Undersampled MRI Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#37319;&#26679;MRI&#37325;&#26500;&#36807;&#31243;&#20013;&#21033;&#29992;&#25512;&#29702;&#38454;&#27573;&#38477;&#22122;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#26465;&#20214;&#36229;&#21442;&#25968;&#32593;&#32476;&#65292;&#28040;&#38500;&#20102;&#25968;&#25454;&#22686;&#24378;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#21508;&#31181;&#22122;&#22768;&#27700;&#24179;&#19979;&#20445;&#25345;&#31283;&#20581;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#38454;&#27573;&#20135;&#29983;&#39640;&#28165;&#26224;&#24230;&#37325;&#26500;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#36798;&#21040;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#25968;&#25454;&#30340;&#37325;&#26500;&#21463;&#21040;&#28145;&#24230;&#23398;&#20064;&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#25913;&#21892;&#23545;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#20998;&#24067;&#21464;&#21270;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#36807;&#24402;&#32435;&#35774;&#35745;&#25110;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#21463;&#21040;&#35823;&#23548;&#24615;&#25968;&#25454;&#65288;&#22914;&#38543;&#26426;&#22122;&#22768;&#65289;&#21644;&#25512;&#29702;&#38454;&#27573;&#25968;&#25454;&#19982;&#27169;&#22411;&#20013;&#20551;&#35774;&#30340;&#21464;&#21270;&#19981;&#21305;&#37197;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#37319;&#29992;&#26465;&#20214;&#36229;&#21442;&#25968;&#32593;&#32476;&#65292;&#25105;&#20204;&#28040;&#38500;&#20102;&#23545;&#22686;&#24378;&#30340;&#38656;&#27714;&#65292;&#22312;&#21508;&#31181;&#39640;&#26031;&#22122;&#22768;&#27700;&#24179;&#19979;&#20173;&#33021;&#20445;&#25345;&#31283;&#20581;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#32463;&#21463;&#21508;&#31181;&#36755;&#20837;&#22122;&#22768;&#27700;&#24179;&#30340;&#32771;&#39564;&#65292;&#24182;&#22312;&#27979;&#35797;&#38454;&#27573;&#20135;&#29983;&#39640;&#28165;&#26224;&#24230;&#37325;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#21442;&#25968;&#37319;&#26679;&#31574;&#30053;&#65292;&#21152;&#24555;&#20102;&#35757;&#32451;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08692v1 Announce Type: cross Abstract: Reconstruction of magnetic resonance imaging (MRI) data has been positively affected by deep learning. A key challenge remains: to improve generalisation to distribution shifts between the training and testing data. Most approaches aim to address this via inductive design or data augmentation. However, they can be affected by misleading data, e.g. random noise, and cases where the inference stage data do not match assumptions in the modelled shifts. In this work, by employing a conditional hyperparameter network, we eliminate the need of augmentation, yet maintain robust performance under various levels of Gaussian noise. We demonstrate that our model withstands various input noise levels while producing high-definition reconstructions during the test stage. Moreover, we present a hyperparameter sampling strategy that accelerates the convergence of training. Our proposed method achieves the highest accuracy and image quality in all sett
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36890;&#36807;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#38899;&#20048;&#20048;&#35889;&#26159;&#21542;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#20449;&#26381;&#30340;&#31038;&#20132;&#20307;&#39564;&#65292;&#32780;&#26080;&#38656;&#20248;&#21270;&#20854;&#21516;&#27493;&#21644;&#24310;&#32493;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08690</link><description>&lt;p&gt;
&#22914;&#26524;&#22270;&#28789;&#21644;&#19968;&#20010;&#20154;&#24037;&#20249;&#20276;&#19968;&#36215;&#24377;&#38050;&#29748;
&lt;/p&gt;
&lt;p&gt;
If Turing played piano with an artificial partner
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36890;&#36807;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#38899;&#20048;&#20048;&#35889;&#26159;&#21542;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#20449;&#26381;&#30340;&#31038;&#20132;&#20307;&#39564;&#65292;&#32780;&#26080;&#38656;&#20248;&#21270;&#20854;&#21516;&#27493;&#21644;&#24310;&#32493;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#26159;&#19968;&#31181;&#22825;&#29983;&#30340;&#31038;&#20132;&#27963;&#21160;&#65292;&#20801;&#35768;&#20154;&#20204;&#20998;&#20139;&#20307;&#39564;&#24182;&#19982;&#24444;&#27492;&#20135;&#29983;&#36830;&#25509;&#12290;&#35774;&#35745;&#23454;&#29616;&#19982;&#19982;&#21478;&#19968;&#20010;&#20154;&#19968;&#36215;&#28436;&#22863;&#31867;&#20284;&#30340;&#31038;&#20132;&#20307;&#39564;&#30340;&#20154;&#24037;&#20249;&#20276;&#26041;&#38754;&#30340;&#36827;&#23637;&#24456;&#23567;&#12290;&#23454;&#29616;&#29983;&#25104;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36866;&#21512;&#29983;&#25104;&#38899;&#20048;&#20048;&#35889;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#24615;&#30340;&#38899;&#20048;&#28436;&#22863;&#19981;&#20165;&#20165;&#26159;&#28436;&#22863;&#19968;&#20010;&#20048;&#35889;&#65307;&#23427;&#24517;&#39035;&#19982;&#20854;&#20182;&#38899;&#20048;&#23478;&#30340;&#24819;&#27861;&#30456;&#21327;&#35843;&#65292;&#24182;&#27491;&#30830;&#20445;&#25345;&#26102;&#38388;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#35757;&#32451;&#29992;&#20110;&#29983;&#25104;&#38899;&#20048;&#20048;&#35889;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20135;&#29983;&#20196;&#20154;&#20449;&#26381;&#30340;&#31038;&#20132;&#20307;&#39564;&#65292;&#32780;&#19981;&#24517;&#20248;&#21270;&#20854;&#21516;&#27493;&#21644;&#24310;&#32493;&#33021;&#21147;&#12290;&#35813;&#32593;&#32476;&#26159;&#22312;&#22823;&#37327;&#25968;&#23383;&#20048;&#35889;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#22312;&#19982;&#20154;&#31867;&#20249;&#20276;&#36827;&#34892;&#23450;&#26102;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#36866;&#24212;&#12290;&#21442;&#19982;&#32773;&#19982;&#20154;&#31867;&#25110;&#20154;&#24037;&#20249;&#20276;&#19968;&#36215;&#24377;&#22863;&#38050;&#29748;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08690v1 Announce Type: cross Abstract: Music is an inherently social activity that allows people to share experiences and feel connected with one another. There has been little progress in designing artificial partners exhibiting a similar social experience as playing with another person. Neural network architectures that implement generative models, such as large language models, are suited for producing musical scores. Playing music socially, however, involves more than playing a score; it must complement the other musicians' ideas and keep time correctly. We addressed the question of whether a convincing social experience is made possible by a generative model trained to produce musical scores, not necessarily optimized for synchronization and continuation. The network, a variational autoencoder trained on a large corpus of digital scores, was adapted for a timed call-and-response task with a human partner. Participants played piano with a human or artificial partner-in v
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#33258;&#21160;&#20056;&#23458;&#35745;&#25968;&#25968;&#25454;&#38477;&#22122;&#31639;&#27861;&#65292;&#36890;&#36807;&#32422;&#26463;&#25972;&#25968;&#32447;&#24615;&#20248;&#21270;&#32467;&#21512;&#31080;&#21153;&#25968;&#25454;&#21644;&#21382;&#21490;&#20056;&#36710;&#20154;&#25968;&#25968;&#25454;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#25968;&#25454;&#40065;&#26834;&#24615;&#21644;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08688</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#33258;&#21160;&#20056;&#23458;&#35745;&#25968;&#25968;&#25454;&#38477;&#22122;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Automated Passenger Counting Data Denoising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#33258;&#21160;&#20056;&#23458;&#35745;&#25968;&#25968;&#25454;&#38477;&#22122;&#31639;&#27861;&#65292;&#36890;&#36807;&#32422;&#26463;&#25972;&#25968;&#32447;&#24615;&#20248;&#21270;&#32467;&#21512;&#31080;&#21153;&#25968;&#25454;&#21644;&#21382;&#21490;&#20056;&#36710;&#20154;&#25968;&#25968;&#25454;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#25968;&#25454;&#40065;&#26834;&#24615;&#21644;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#21487;&#38752;&#22320;&#20102;&#35299;&#20844;&#20849;&#20132;&#36890;&#32593;&#32476;&#20013;&#30340;&#20056;&#23458;&#25968;&#37327;&#23545;&#20110;&#20844;&#20849;&#20132;&#36890;&#36816;&#33829;&#21830;&#21644;&#20844;&#20849;&#37096;&#38376;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#20415;&#20102;&#35299;&#32593;&#32476;&#30340;&#20351;&#29992;&#24773;&#20917;&#24182;&#20248;&#21270;&#36816;&#36755;&#26381;&#21153;&#12290;&#30446;&#21069;&#26377;&#20960;&#31181;&#20272;&#35745;&#20056;&#23458;&#25968;&#37327;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#19968;&#20123;&#26159;&#33258;&#21160;&#21270;&#30340;&#12290;&#20854;&#20013;&#65292;&#33258;&#21160;&#20056;&#23458;&#35745;&#25968;&#65288;APC&#65289;&#31995;&#32479;&#22312;&#36710;&#36742;&#27599;&#20010;&#36710;&#31449;&#26816;&#27979;&#20056;&#23458;&#30340;&#36827;&#20986;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20135;&#29983;&#30340;&#25968;&#25454;&#24120;&#24120;&#26159;&#22024;&#26434;&#30340;&#65292;&#29978;&#33267;&#26377;&#20559;&#24046;&#65292;&#23548;&#33268;&#20056;&#36710;&#20154;&#25968;&#34987;&#20302;&#20272;&#25110;&#39640;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;APC&#25968;&#25454;&#30340;&#38477;&#22122;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#24182;&#20415;&#20110;&#20998;&#26512;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#19968;&#20010;&#32422;&#26463;&#25972;&#25968;&#32447;&#24615;&#20248;&#21270;&#26469;&#23454;&#29616;&#30340;&#65292;&#21033;&#29992;&#31080;&#21153;&#25968;&#25454;&#21644;&#21382;&#21490;&#20056;&#36710;&#20154;&#25968;&#25968;&#25454;&#36827;&#19968;&#27493;&#32422;&#26463;&#21644;&#25351;&#23548;&#20248;&#21270;&#36807;&#31243;&#12290;&#36890;&#36807;&#23545;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#19982;&#20854;&#20182;&#38477;&#22122;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08688v1 Announce Type: cross Abstract: A reliable and accurate knowledge of the ridership in public transportation networks is crucial for public transport operators and public authorities to be aware of their network's use and optimize transport offering. Several techniques to estimate ridership exist nowadays, some of them in an automated manner. Among them, Automatic Passenger Counting (APC) systems detect passengers entering and leaving the vehicle at each station of its course. However, data resulting from these systems are often noisy or even biased, resulting in under or overestimation of onboard occupancy. In this work, we propose a denoising algorithm for APC data to improve their robustness and ease their analyzes. The proposed approach consists in a constrained integer linear optimization, taking advantage of ticketing data and historical ridership data to further constrain and guide the optimization. The performances are assessed and compared to other denoising m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#30340;&#20381;&#36182;&#27979;&#24230;&#30340;&#27169;&#31946;&#22278;&#24418;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#21462;&#20540;&#22312;&#21333;&#20301;&#22278;&#19978;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#20998;&#26512;&#65292;&#24182;&#32771;&#34385;&#20102;&#24207;&#21015;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08687</link><description>&lt;p&gt;
&#22522;&#20110;&#26032;&#30340;&#20381;&#36182;&#27979;&#24230;&#30340;&#27169;&#31946;&#22278;&#24418;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#21450;&#20854;&#22312;&#39118;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fuzzy clustering of circular time series based on a new dependence measure with applications to wind data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#30340;&#20381;&#36182;&#27979;&#24230;&#30340;&#27169;&#31946;&#22278;&#24418;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#21462;&#20540;&#22312;&#21333;&#20301;&#22278;&#19978;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#20998;&#26512;&#65292;&#24182;&#32771;&#34385;&#20102;&#24207;&#21015;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26159;&#19968;&#39033;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#21462;&#20540;&#20110;&#23454;&#25968;&#32447;&#19978;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#20294;&#24456;&#23569;&#26377;&#26041;&#27861;&#32771;&#34385;&#21040;&#21462;&#20540;&#22312;&#21333;&#20301;&#22278;&#19978;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#23613;&#31649;&#21518;&#32773;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#32463;&#24120;&#20986;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#22278;&#24418;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22278;&#24418;&#24207;&#21015;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#29992;&#20110;&#26500;&#24314;&#32858;&#31867;&#36807;&#31243;&#12290;&#35813;&#24230;&#37327;&#20381;&#36182;&#20110;&#19968;&#31181;&#32771;&#34385;&#22278;&#24359;&#30340;&#26032;&#30340;&#24207;&#21015;&#20381;&#36182;&#27979;&#24230;&#65292;&#20174;&#32780;&#21033;&#29992;&#20102;&#24207;&#21015;&#33539;&#22260;&#22266;&#26377;&#30340;&#26041;&#21521;&#29305;&#24615;&#12290;&#30001;&#20110;&#24207;&#21015;&#30340;&#21160;&#24577;&#29305;&#24615;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#27169;&#31946;&#26041;&#27861;&#65292;&#20351;&#35813;&#36807;&#31243;&#33021;&#22815;&#23558;&#27599;&#20010;&#24207;&#21015;&#20998;&#37197;&#21040;&#20855;&#26377;&#19981;&#21516;&#25104;&#21592;&#24230;&#30340;&#22810;&#20010;&#32858;&#31867;&#20013;&#12290;&#24471;&#21040;&#30340;&#32858;&#31867;&#31639;&#27861;&#33021;&#22815;&#23558;&#20174;&#30456;&#20284;&#38543;&#26426;&#29983;&#25104;&#30340;&#24207;&#21015;&#20998;&#32452;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08687v1 Announce Type: cross Abstract: Time series clustering is an essential machine learning task with applications in many disciplines. While the majority of the methods focus on time series taking values on the real line, very few works consider time series defined on the unit circle, although the latter objects frequently arise in many applications. In this paper, the problem of clustering circular time series is addressed. To this aim, a distance between circular series is introduced and used to construct a clustering procedure. The metric relies on a new measure of serial dependence considering circular arcs, thus taking advantage of the directional character inherent to the series range. Since the dynamics of the series may vary over the time, we adopt a fuzzy approach, which enables the procedure to locate each series into several clusters with different membership degrees. The resulting clustering algorithm is able to group series generated from similar stochastic 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;&#30340;&#30005;&#23376;&#37038;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#39640;&#25928;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08309</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#21521;&#37327;&#26816;&#27979;&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Prompted Contextual Vectors for Spear-Phishing Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08309
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;&#30340;&#30005;&#23376;&#37038;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#39640;&#25928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#29983;&#25104;&#20196;&#20154;&#20449;&#26381;&#30340;&#30005;&#23376;&#37038;&#20214;&#24182;&#26041;&#20415;&#30446;&#26631;&#20390;&#23519;&#26469;&#21319;&#32423;&#20102;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLMs&#30340;&#38598;&#21512;&#26469;&#21019;&#24314;&#34920;&#31034;&#21521;&#37327;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#26469;&#25512;&#29702;&#21644;&#22238;&#31572;&#20154;&#24037;&#21046;&#23450;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#37327;&#21270;&#30005;&#23376;&#37038;&#20214;&#20869;&#23481;&#20013;&#24120;&#35265;&#35828;&#26381;&#21407;&#21017;&#30340;&#23384;&#22312;&#65292;&#20026;&#19979;&#28216;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#19978;&#19979;&#25991;&#25991;&#26723;&#21521;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19987;&#26377;&#31995;&#32479;&#29983;&#25104;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#33258;&#21160;&#21270;&#30446;&#26631;&#20390;&#23519;&#21644;&#38035;&#40060;&#30005;&#23376;&#37038;&#20214;&#30340;&#21019;&#24314;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20165;&#21253;&#21547;&#20256;&#32479;&#38035;&#40060;&#21644;&#33391;&#24615;&#30005;&#23376;&#37038;&#20214;&#30340;&#35757;&#32451;&#38598;&#20013;&#23454;&#29616;&#20102;91%&#30340;F1&#24471;&#20998;&#65292;&#20854;&#20013;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#19968;&#31181;&#21019;&#26032;&#30340;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spear-phishing attacks present a significant security challenge, with large language models (LLMs) escalating the threat by generating convincing emails and facilitating target reconnaissance. To address this, we propose a detection approach based on a novel document vectorization method that utilizes an ensemble of LLMs to create representation vectors. By prompting LLMs to reason and respond to human-crafted questions, we quantify the presence of common persuasion principles in the email's content, producing prompted contextual document vectors for a downstream supervised machine learning model. We evaluate our method using a unique dataset generated by a proprietary system that automates target reconnaissance and spear-phishing email creation. Our method achieves a 91% F1 score in identifying LLM-generated spear-phishing emails, with the training set comprising only traditional phishing and benign emails. Key contributions include an innovative document vectorization method utilizin
&lt;/p&gt;</description></item><item><title>ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08303</link><description>&lt;p&gt;
ChatCell: &#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
ChatCell: Facilitating Single-Cell Analysis with Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08303
&lt;/p&gt;
&lt;p&gt;
ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#22312;&#31185;&#23398;&#20013;&#30340;&#24433;&#21709;&#26085;&#30410;&#31361;&#20986;&#12290;LLMs&#22312;&#20219;&#21153;&#27867;&#21270;&#21644;&#33258;&#30001;&#23545;&#35805;&#26041;&#38754;&#30340;&#26032;&#20852;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#25512;&#36827;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#36825;&#20010;&#26500;&#25104;&#29983;&#29289;&#20307;&#22522;&#30784;&#26500;&#20214;&#30340;&#39046;&#22495;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#24403;&#21069;&#26041;&#27861;&#22312;&#30693;&#35782;&#38376;&#27099;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;LLMs&#22312;&#25484;&#25569;&#21333;&#32454;&#32990;&#25968;&#25454;&#26041;&#38754;&#30340;&#20805;&#20998;&#21033;&#29992;&#65292;&#24433;&#21709;&#20102;&#30452;&#25509;&#21487;&#35775;&#38382;&#21644;&#24555;&#36895;&#36845;&#20195;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ChatCell&#65292;&#36890;&#36807;&#21033;&#29992;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#22312;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#39046;&#22495;&#33719;&#24471;&#20102;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20174;&#26550;&#26500;&#30340;&#35282;&#24230;&#20840;&#38754;&#35843;&#26597;&#20102;&#22270;&#30340;&#36229;&#20998;&#24067;&#25512;&#24191;&#65292;&#25581;&#31034;&#20102;&#22270;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#20854;&#20182;&#24120;&#35265;&#26500;&#24314;&#27169;&#22359;&#22312;&#36229;&#20998;&#24067;&#38382;&#39064;&#19978;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08228</link><description>&lt;p&gt;
&#30740;&#31350;GNN&#30340;&#36229;&#20998;&#24067;&#25512;&#24191;&#65306;&#20174;&#26550;&#26500;&#35282;&#24230;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08228
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20174;&#26550;&#26500;&#30340;&#35282;&#24230;&#20840;&#38754;&#35843;&#26597;&#20102;&#22270;&#30340;&#36229;&#20998;&#24067;&#25512;&#24191;&#65292;&#25581;&#31034;&#20102;&#22270;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#20854;&#20182;&#24120;&#35265;&#26500;&#24314;&#27169;&#22359;&#22312;&#36229;&#20998;&#24067;&#38382;&#39064;&#19978;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#20110;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#65292;&#36825;&#20010;&#20551;&#35774;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#22312;&#22270;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#23545;&#36229;&#20998;&#24067;&#65288;OOD&#65289;&#38382;&#39064;&#30340;&#25506;&#32034;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#22270;&#30340;OOD&#25512;&#24191;&#30340;&#20004;&#20010;&#8220;&#27169;&#22411;&#26080;&#20851;&#8221;&#35282;&#24230;&#19978;&#65306;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#21644;&#31574;&#30053;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24050;&#30693;&#30340;GNN&#27169;&#22411;&#26550;&#26500;&#23545;&#22270;&#30340;OOD&#25512;&#24191;&#30340;&#24433;&#21709;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#65292;&#36825;&#19982;&#29616;&#26377;&#30340;&#30740;&#31350;&#30456;&#20114;&#29420;&#31435;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#26550;&#26500;&#30340;&#35282;&#24230;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#22270;&#30340;OOD&#25512;&#24191;&#65292;&#24182;&#23545;&#29616;&#20195;GNN&#30340;&#24120;&#35265;&#26500;&#24314;&#27169;&#22359;&#36827;&#34892;&#20102;&#32771;&#23519;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22270;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs. Most existing efforts have primarily concentrated on improving graph OOD generalization from two \textbf{model-agnostic} perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known \textbf{GNN model architectures} on graph OOD generalization, which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs. Through extensive experiments, we reveal that both the graph self-attention mechanism an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#24635;&#32467;&#20102;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#65292;&#24490;&#29615;&#27169;&#22411;&#30340;&#22797;&#20852;&#21644;&#19982;Transformer&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#31070;&#32463;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20197;&#21450;&#28145;&#24230;&#31354;&#38388;&#29366;&#24577;&#27169;&#22411;&#20316;&#20026;&#26102;&#38388;&#20989;&#25968;&#36924;&#36817;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.08132</link><description>&lt;p&gt;
&#12298;&#20851;&#20110;&#24490;&#29615;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#20013;&#30340;&#22797;&#20852;&#65306;&#22312;Transformer&#26102;&#20195;&#30340;&#35843;&#30740;&#21644;&#30740;&#31350;&#26426;&#20250;&#12299;
&lt;/p&gt;
&lt;p&gt;
On the Resurgence of Recurrent Models for Long Sequences: Survey and Research Opportunities in the Transformer Era
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08132
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#24635;&#32467;&#20102;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#65292;&#24490;&#29615;&#27169;&#22411;&#30340;&#22797;&#20852;&#21644;&#19982;Transformer&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#31070;&#32463;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20197;&#21450;&#28145;&#24230;&#31354;&#38388;&#29366;&#24577;&#27169;&#22411;&#20316;&#20026;&#26102;&#38388;&#20989;&#25968;&#36924;&#36817;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#24320;&#21457;&#21487;&#20197;&#22788;&#29702;&#21644;&#23398;&#20064;&#38750;&#24120;&#38271;&#30340;&#25968;&#25454;&#24207;&#21015;&#30340;&#27169;&#22411;&#12290;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#65288;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#20986;&#33394;&#32467;&#26524;&#25512;&#21160;&#20102;&#24182;&#34892;&#27880;&#24847;&#21147;&#30340;&#27010;&#24565;&#65292;&#23558;&#32463;&#20856;&#30340;&#39034;&#24207;&#22788;&#29702;&#30340;&#24490;&#29615;&#27169;&#22411;&#30340;&#20316;&#29992;&#25513;&#30422;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#23545;&#33258;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#34920;&#31034;&#20851;&#27880;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20860;&#39038;Transformer&#21644;&#24490;&#29615;&#32593;&#32476;&#20004;&#20010;&#19990;&#30028;&#20248;&#21183;&#30340;&#26032;&#22411;&#31070;&#32463;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#28145;&#24230;&#31354;&#38388;&#29366;&#24577;&#27169;&#22411;&#20316;&#20026;&#26102;&#38388;&#20989;&#25968;&#36924;&#36817;&#30340;&#24378;&#22823;&#26041;&#27861;&#20986;&#29616;&#65292;&#20174;&#32780;&#20026;&#20174;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#24320;&#36767;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#35768;&#22810;&#39046;&#22495;&#30340;&#30740;&#31350;&#32773;&#23545;&#27492;&#24863;&#20852;&#36259;&#24182;&#21033;&#29992;&#23427;&#26469;&#23454;&#29616;&#19968;&#31867;&#29305;&#27530;&#30340;&#65288;&#32447;&#24615;&#65289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#27010;&#36848;&#36825;&#20123;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
A longstanding challenge for the Machine Learning community is the one of developing models that are capable of processing and learning from very long sequences of data. The outstanding results of Transformers-based networks (e.g., Large Language Models) promotes the idea of parallel attention as the key to succeed in such a challenge, obfuscating the role of classic sequential processing of Recurrent Models. However, in the last few years, researchers who were concerned by the quadratic complexity of self-attention have been proposing a novel wave of neural models, which gets the best from the two worlds, i.e., Transformers and Recurrent Nets. Meanwhile, Deep Space-State Models emerged as robust approaches to function approximation over time, thus opening a new perspective in learning from sequential data, followed by many people in the field and exploited to implement a special class of (linear) Recurrent Neural Networks. This survey is aimed at providing an overview of these trends 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22343;&#21248;&#21270;&#30340;&#26041;&#24335;&#30830;&#20999;&#23454;&#29616;&#20102;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20854;&#29702;&#35770;&#24615;&#36136;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#37319;&#26679;&#30340;&#24635;&#21464;&#24046;&#36317;&#31163;&#21644;KL&#25955;&#24230;&#20445;&#35777;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#24314;&#27169;&#31163;&#25955;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.08095</link><description>&lt;p&gt;
&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#20998;&#26512;&#65306;&#36890;&#36807;&#22343;&#21248;&#21270;&#30340;&#30830;&#20999;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis of Discrete Diffusion Model: Exact Implementation through Uniformization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22343;&#21248;&#21270;&#30340;&#26041;&#24335;&#30830;&#20999;&#23454;&#29616;&#20102;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20854;&#29702;&#35770;&#24615;&#36136;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#37319;&#26679;&#30340;&#24635;&#21464;&#24046;&#36317;&#31163;&#21644;KL&#25955;&#24230;&#20445;&#35777;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#24314;&#27169;&#31163;&#25955;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#32463;&#39564;&#25104;&#21151;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#21162;&#21147;&#24050;&#32463;&#34987;&#20570;&#20986;&#26469;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#36866;&#24212;&#21040;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#65292;&#20026;&#24314;&#27169;&#26412;&#36136;&#19978;&#26159;&#31163;&#25955;&#25968;&#25454;&#65288;&#22914;&#35821;&#35328;&#21644;&#22270;&#24418;&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#30340;&#26041;&#27861;&#12290;&#36825;&#36890;&#36807;&#23558;&#21069;&#21521;&#22122;&#22768;&#36807;&#31243;&#21644;&#30456;&#24212;&#30340;&#36870;&#36807;&#31243;&#37117;&#26500;&#24314;&#20026;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#65288;CTMC&#65289;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36830;&#32493;&#39532;&#23572;&#21487;&#22827;&#38142;&#22343;&#21248;&#21270;&#30340;&#31639;&#27861;&#65292;&#22312;&#38543;&#26426;&#26102;&#38388;&#28857;&#19978;&#23454;&#29616;&#36716;&#31227;&#12290;&#22312;&#20851;&#20110;&#31163;&#25955;&#24471;&#20998;&#20989;&#25968;&#23398;&#20064;&#30340;&#21512;&#29702;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20174;&#36229;&#31435;&#26041;&#20307;&#19978;&#30340;&#20219;&#20309;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#25152;&#38656;&#30340;&#24635;&#21464;&#24046;&#36317;&#31163;&#21644;KL&#25955;&#24230;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#22312;$\mathbb{R}^d$&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#25104;&#23601;&#30456;&#19968;&#33268;&#65292;&#24182;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;d&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved huge empirical success in data generation tasks. Recently, some efforts have been made to adapt the framework of diffusion models to discrete state space, providing a more natural approach for modeling intrinsically discrete data, such as language and graphs. This is achieved by formulating both the forward noising process and the corresponding reversed process as Continuous Time Markov Chains (CTMCs). In this paper, we investigate the theoretical properties of the discrete diffusion model. Specifically, we introduce an algorithm leveraging the uniformization of continuous Markov chains, implementing transitions on random time points. Under reasonable assumptions on the learning of the discrete score function, we derive Total Variation distance and KL divergence guarantees for sampling from any distribution on a hypercube. Our results align with state-of-the-art achievements for diffusion models in $\mathbb{R}^d$ and further underscore the advantages of d
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#23398;&#20064;&#19968;&#20010;&#23376;&#39640;&#26031;&#27010;&#29575;&#20998;&#24067;&#26063;&#20013;&#31361;&#30772;&#20102;&#32500;&#25968;&#28798;&#38590;&#65292;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#29983;&#25104;&#30340;&#20998;&#24067;&#20197;&#32500;&#24230;&#26080;&#20851;&#30340;&#36895;&#29575;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.08082</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#23398;&#20064;&#19968;&#20010;&#23376;&#39640;&#26031;&#27010;&#29575;&#20998;&#24067;&#26063;&#20013;&#31361;&#30772;&#20102;&#32500;&#25968;&#28798;&#38590;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian probability distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08082
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#23398;&#20064;&#19968;&#20010;&#23376;&#39640;&#26031;&#27010;&#29575;&#20998;&#24067;&#26063;&#20013;&#31361;&#30772;&#20102;&#32500;&#25968;&#28798;&#38590;&#65292;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#29983;&#25104;&#30340;&#20998;&#24067;&#20197;&#32500;&#24230;&#26080;&#20851;&#30340;&#36895;&#29575;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#22312;&#24040;&#22823;&#30340;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#30340;&#25968;&#23398;&#22522;&#30784;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;SGMs&#22312;&#23398;&#20064;&#19968;&#20010;&#23376;&#39640;&#26031;&#27010;&#29575;&#20998;&#24067;&#26063;&#20013;&#30340;&#36817;&#20284;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20851;&#20110;&#27010;&#29575;&#20998;&#24067;&#22797;&#26434;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#30456;&#23545;&#23494;&#24230;&#19982;&#26631;&#20934;&#39640;&#26031;&#27979;&#24230;&#30340;&#30456;&#23545;&#23494;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#23545;&#25968;&#30456;&#23545;&#23494;&#24230;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23616;&#37096;&#36924;&#36817;&#65292;&#24182;&#19988;&#32593;&#32476;&#21442;&#25968;&#21487;&#20197;&#36866;&#24403;&#22320;&#21463;&#38480;&#65292;&#37027;&#20040;&#36890;&#36807;&#32463;&#39564;&#20998;&#25968;&#21305;&#37197;&#29983;&#25104;&#30340;&#20998;&#24067;&#20197;&#32500;&#24230;&#26080;&#20851;&#30340;&#36895;&#29575;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#31034;&#20363;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#20854;&#20013;&#21253;&#25324;&#26576;&#20123;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#30340;&#19968;&#20010;&#20851;&#38190;&#28857;&#26159;&#25512;&#23548;&#20986;&#19982;&#27491;&#21521;&#36807;&#31243;&#30456;&#20851;&#30340;&#30495;&#23454;&#24471;&#20998;&#20989;&#25968;&#30340;&#32500;&#24230;&#26080;&#20851;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While score-based generative models (SGMs) have achieved remarkable success in enormous image generation tasks, their mathematical foundations are still limited. In this paper, we analyze the approximation and generalization of SGMs in learning a family of sub-Gaussian probability distributions. We introduce a notion of complexity for probability distributions in terms of their relative density with respect to the standard Gaussian measure. We prove that if the log-relative density can be locally approximated by a neural network whose parameters can be suitably bounded, then the distribution generated by empirical score matching approximates the target distribution in total variation with a dimension-independent rate. We illustrate our theory through examples, which include certain mixtures of Gaussians. An essential ingredient of our proof is to derive a dimension-free deep neural network approximation rate for the true score function associated with the forward process, which is inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017; ($\mathbf{UA}^2$)&#65292;&#26088;&#22312;&#23454;&#29616;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#30340;&#32479;&#19968;&#23545;&#40784;&#65292;&#25552;&#20986;&#20102;&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07744</link><description>&lt;p&gt;
&#23454;&#29616;&#26234;&#33021;&#20307;&#12289;&#20154;&#31867;&#21644;&#29615;&#22659;&#20043;&#38388;&#30340;&#32479;&#19968;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Alignment Between Agents, Humans, and Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017; ($\mathbf{UA}^2$)&#65292;&#26088;&#22312;&#23454;&#29616;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#30340;&#32479;&#19968;&#23545;&#40784;&#65292;&#25552;&#20986;&#20102;&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24555;&#36895;&#36827;&#23637;&#23548;&#33268;&#20102;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#32321;&#33635;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;&#36827;&#34892;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#29615;&#22659;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#24403;&#22312;&#22797;&#26434;&#12289;&#29616;&#23454;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#26102;&#65292;&#26234;&#33021;&#20307;&#30340;&#25928;&#33021;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017;&#65292;&#21363;&#21516;&#26102;&#23545;&#40784;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#65288;&#22914;&#36135;&#24065;&#39044;&#31639;&#38480;&#21046;&#65289;&#12290;&#20174;&#32479;&#19968;&#23545;&#40784; ($\mathbf{UA}^2$) &#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#24403;&#21069;&#26234;&#33021;&#20307;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;&#26234;&#33021;&#20307;&#22522;&#20934;&#21644;&#26041;&#27861;&#20505;&#36873;&#20013;&#34987;&#24573;&#35270;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20026;WebShop&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#65292;&#21253;&#25324;&#20351;&#29992;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#26469;&#23637;&#31034;&#24847;&#22270;&#12289;&#20010;&#24615;&#21270;&#37325;&#26032;&#25490;&#21517;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#29615;&#22659;&#21160;&#24577;&#21644;&#36816;&#34892;&#26102;&#25104;&#26412;&#32479;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of $\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents ($\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of $\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#22788;&#29702;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;&#30340;&#19977;&#20010;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.07703</link><description>&lt;p&gt;
&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Sequential Decision-Making with Unknown Delays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#22788;&#29702;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;&#30340;&#19977;&#20010;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#39046;&#22495;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#26694;&#26550;&#35299;&#20915;&#20102;&#20855;&#26377;&#24310;&#36831;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20915;&#31574;&#30340;&#21453;&#39304;&#21487;&#33021;&#20197;&#26410;&#30693;&#24310;&#36831;&#21040;&#36798;&#12290;&#19982;&#20043;&#21069;&#20165;&#38480;&#20110;&#27431;&#20960;&#37324;&#24471;&#33539;&#25968;&#21644;&#26799;&#24230;&#20449;&#24687;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#22522;&#20110;&#36817;&#20284;&#35299;&#30340;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#25509;&#25910;&#21453;&#39304;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22810;&#21151;&#33021;&#19988;&#36866;&#29992;&#20110;&#36890;&#29992;&#33539;&#25968;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#38024;&#23545;&#20855;&#26377;&#23436;&#25972;&#25439;&#22833;&#20989;&#25968;&#20449;&#24687;&#21453;&#39304;&#30340;&#24310;&#36831;&#35268;&#33539;&#21270;&#39046;&#23548;&#31639;&#27861;&#26063;&#65292;&#19968;&#31995;&#21015;&#38024;&#23545;&#20855;&#26377;&#26799;&#24230;&#20449;&#24687;&#21453;&#39304;&#30340;&#24310;&#36831;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#26063;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#38024;&#23545;&#30456;&#24212;&#20915;&#31574;&#28857;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#20540;&#20449;&#24687;&#21453;&#39304;&#30340;&#31616;&#21270;&#24310;&#36831;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#26063;&#12290;&#23545;&#20110;&#27599;&#31181;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay. Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback. Our proposed algorithms are versatile and applicable to universal norms. Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points. For each type of algorithm, we provide corresponding regret bounds under cases of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;LoCoV1&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#35780;&#20272;&#24615;&#33021;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07440</link><description>&lt;p&gt;
&#20351;&#29992;LoCo&#21644;M2-BERT&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#26500;&#24314;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;LoCoV1&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#35780;&#20272;&#24615;&#33021;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#31649;&#36947;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#25991;&#26723;&#24456;&#38271;&#65288;&#20363;&#22914;10K&#20010;&#26631;&#35760;&#25110;&#26356;&#22810;&#65289;&#19988;&#38656;&#35201;&#22312;&#25972;&#20010;&#25991;&#26412;&#20013;&#21512;&#25104;&#20449;&#24687;&#26469;&#30830;&#23450;&#30456;&#20851;&#25991;&#26723;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#24320;&#21457;&#36866;&#29992;&#20110;&#36825;&#20123;&#39046;&#22495;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#32534;&#30721;&#22120;&#38754;&#20020;&#19977;&#20010;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22914;&#20309;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#65292;&#65288;2&#65289;&#22914;&#20309;&#39044;&#35757;&#32451;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#20197;&#34920;&#31034;&#30701;&#19978;&#19979;&#25991;&#65288;&#23545;&#24212;&#26597;&#35810;&#65289;&#21644;&#38271;&#19978;&#19979;&#25991;&#65288;&#23545;&#24212;&#25991;&#26723;&#65289;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22914;&#20309;&#26681;&#25454;GPU&#20869;&#23384;&#38480;&#21046;&#19979;&#30340;&#25209;&#37327;&#22823;&#23567;&#38480;&#21046;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;LoCoV1&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;12&#20010;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27979;&#37327;&#22312;&#19981;&#21487;&#20998;&#22359;&#25110;&#19981;&#26377;&#25928;&#30340;&#24773;&#20917;&#19979;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;80M&#21442;&#25968;&#29366;&#24577;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#37319;&#29992;Monarch Mixer&#26550;&#26500;&#26500;&#24314;&#65292;&#33021;&#22815;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval pipelines-an integral component of many machine learning systems-perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text. Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to fine-tune this model for retrieval under the batch size limitations imposed by GPU memory constraints. To address these challenges, we first introduce LoCoV1, a novel 12 task benchmark constructed to measure long-context retrieval where chunking is not possible or not effective. We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scali
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#30417;&#25511;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(Monitored MDPs)&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20195;&#29702;&#19981;&#33021;&#22987;&#32456;&#35266;&#23519;&#21040;&#22870;&#21169;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#26032;&#39062;&#30340;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.06819</link><description>&lt;p&gt;
&#30417;&#25511;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Monitored Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06819
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#30417;&#25511;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(Monitored MDPs)&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20195;&#29702;&#19981;&#33021;&#22987;&#32456;&#35266;&#23519;&#21040;&#22870;&#21169;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#26032;&#39062;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#21644;&#25509;&#25910;&#21453;&#39304;&#65288;&#25968;&#20540;&#22870;&#21169;&#65289;&#26469;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22870;&#21169;&#22987;&#32456;&#21487;&#35266;&#23519;&#30340;&#20551;&#35774;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#20013;&#36890;&#24120;&#19981;&#36866;&#29992;&#12290;&#20363;&#22914;&#65292;&#20195;&#29702;&#21487;&#33021;&#38656;&#35201;&#35201;&#27714;&#20154;&#31867;&#30417;&#30563;&#20854;&#34892;&#20026;&#25110;&#28608;&#27963;&#30417;&#25511;&#31995;&#32479;&#20197;&#25509;&#25910;&#21453;&#39304;&#12290;&#29978;&#33267;&#21487;&#33021;&#23384;&#22312;&#22870;&#21169;&#22312;&#21487;&#35266;&#23519;&#20043;&#21069;&#19968;&#27573;&#26102;&#38388;&#25110;&#22312;&#19981;&#20877;&#32473;&#20104;&#22870;&#21169;&#20043;&#21518;&#30340;&#26102;&#38388;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#26377;&#20123;&#24773;&#20917;&#19979;&#65292;&#29615;&#22659;&#26681;&#25454;&#20195;&#29702;&#30340;&#34892;&#20026;&#29983;&#25104;&#22870;&#21169;&#65292;&#20294;&#20195;&#29702;&#26080;&#27861;&#35266;&#23519;&#21040;&#36825;&#20123;&#22870;&#21169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#39062;&#20294;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; - &#30417;&#25511;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(Monitored MDPs)&#65292;&#22312;&#27492;&#26694;&#26550;&#20013;&#20195;&#29702;&#24182;&#38750;&#24635;&#26159;&#33021;&#22815;&#35266;&#23519;&#21040;&#22870;&#21169;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#35774;&#32622;&#21487;&#33021;&#24102;&#26469;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#30340;&#21518;&#26524;&#65292;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#29609;&#20855;&#29615;&#22659;&#20013;&#20063;&#20250;&#20986;&#29616;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#26469;&#24320;&#22987;&#35299;&#20915;&#36825;&#20010;&#26032;&#39062;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), an agent learns to perform a task by interacting with an environment and receiving feedback (a numerical reward) for its actions. However, the assumption that rewards are always observable is often not applicable in real-world problems. For example, the agent may need to ask a human to supervise its actions or activate a monitoring system to receive feedback. There may even be a period of time before rewards become observable, or a period of time after which rewards are no longer given. In other words, there are cases where the environment generates rewards in response to the agent's actions but the agent cannot observe them. In this paper, we formalize a novel but general RL framework - Monitored MDPs - where the agent cannot always observe rewards. We discuss the theoretical and practical consequences of this setting, show challenges raised even in toy environments, and propose algorithms to begin to tackle this novel setting. This paper introduces a p
&lt;/p&gt;</description></item><item><title>Premier-TACO&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06187</link><description>&lt;p&gt;
Premier-TACO: &#36890;&#36807;&#26102;&#38388;&#39537;&#21160;&#30340;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#22810;&#20219;&#21153;&#34920;&#31034;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06187
&lt;/p&gt;
&lt;p&gt;
Premier-TACO&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Premier-TACO&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#23569;&#26679;&#26412;&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;Premier-TACO&#21033;&#29992;&#19968;&#37096;&#20998;&#22810;&#20219;&#21153;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#29305;&#24449;&#34920;&#31034;&#25429;&#25417;&#20102;&#20851;&#38190;&#30340;&#29615;&#22659;&#21160;&#21147;&#23398;&#65292;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#25512;&#21160;&#20102;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#65288;TACO&#65289;&#30446;&#26631;&#30340;&#21457;&#23637;&#65292;TACO&#22312;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#31574;&#30053;&#22312;&#26174;&#33879;&#25552;&#39640;TACO&#30340;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#20351;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#31163;&#32447;&#39044;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;Deepmind Control Suite&#12289;MetaWorld&#21644;LIBERO&#22312;&#20869;&#30340;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;Premier-TACO&#22312;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Premier-TACO, a multitask feature representation learning approach designed to improve few-shot policy learning efficiency in sequential decision-making tasks. Premier-TACO leverages a subset of multitask offline datasets for pretraining a general feature representation, which captures critical environmental dynamics and is fine-tuned using minimal expert demonstrations. It advances the temporal action contrastive learning (TACO) objective, known for state-of-the-art results in visual control tasks, by incorporating a novel negative example sampling strategy. This strategy is crucial in significantly boosting TACO's computational efficiency, making large-scale multitask offline pretraining feasible. Our extensive empirical evaluation in a diverse set of continuous control benchmarks including Deepmind Control Suite, MetaWorld, and LIBERO demonstrate Premier-TACO's effectiveness in pretraining visual representations, significantly enhancing few-shot imitation learning of nove
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#22312;Federated Learning&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#32858;&#21512;&#26435;&#37325;&#26469;&#35782;&#21035;&#23545;&#29305;&#23450;&#23398;&#20064;&#30446;&#26631;&#26368;&#26377;&#30410;&#30340;&#23458;&#25143;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#32463;&#36807;&#23454;&#35777;&#35780;&#20272;&#21457;&#29616;&#65292;&#20351;&#29992;&#35813;&#31639;&#27861;&#24341;&#23548;&#30340;&#21512;&#20316;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#20026;&#26356;&#21152;&#31616;&#21270;&#21644;&#26377;&#25928;&#30340;Federated Learning&#23454;&#29616;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.05050</link><description>&lt;p&gt;
Federated Learning&#33021;&#22815;&#25214;&#21040;&#26377;&#30410;&#30340;&#22909;&#21451;
&lt;/p&gt;
&lt;p&gt;
Federated Learning Can Find Friends That Are Beneficial
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#22312;Federated Learning&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#32858;&#21512;&#26435;&#37325;&#26469;&#35782;&#21035;&#23545;&#29305;&#23450;&#23398;&#20064;&#30446;&#26631;&#26368;&#26377;&#30410;&#30340;&#23458;&#25143;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#32463;&#36807;&#23454;&#35777;&#35780;&#20272;&#21457;&#29616;&#65292;&#20351;&#29992;&#35813;&#31639;&#27861;&#24341;&#23548;&#30340;&#21512;&#20316;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#20026;&#26356;&#21152;&#31616;&#21270;&#21644;&#26377;&#25928;&#30340;Federated Learning&#23454;&#29616;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Federated Learning (FL)&#20013;&#65292;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#23458;&#25143;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#26082;&#24102;&#26469;&#20102;&#26426;&#20250;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#34429;&#28982;&#23458;&#25143;&#20043;&#38388;&#30340;&#21512;&#20316;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#21512;&#20316;&#37117;&#26159;&#26377;&#30410;&#30340;&#65307;&#26377;&#20123;&#29978;&#33267;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#20026;&#21442;&#19982;FL&#35757;&#32451;&#30340;&#23458;&#25143;&#20998;&#37197;&#33258;&#36866;&#24212;&#30340;&#32858;&#21512;&#26435;&#37325;&#65292;&#35782;&#21035;&#20986;&#25968;&#25454;&#20998;&#24067;&#23545;&#29305;&#23450;&#23398;&#20064;&#30446;&#26631;&#26368;&#26377;&#30410;&#30340;&#23458;&#25143;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32858;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#19982;&#20165;&#32858;&#21512;&#20855;&#26377;&#30456;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#23458;&#25143;&#25509;&#25910;&#30340;&#26356;&#26032;&#30340;&#26041;&#27861;&#19981;&#30456;&#19978;&#19979;&#12290;&#27492;&#22806;&#65292;&#32463;&#39564;&#35777;&#26126;&#65292;&#30001;&#25105;&#20204;&#30340;&#31639;&#27861;&#24341;&#23548;&#30340;&#21512;&#20316;&#20248;&#20110;&#20256;&#32479;&#30340;FL&#26041;&#27861;&#12290;&#36825;&#24378;&#35843;&#20102;&#23457;&#24910;&#36873;&#25321;&#23458;&#25143;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#20026;&#26410;&#26469;&#26356;&#31616;&#21270;&#21644;&#26377;&#25928;&#30340;FL&#23454;&#29616;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Federated Learning (FL), the distributed nature and heterogeneity of client data present both opportunities and challenges. While collaboration among clients can significantly enhance the learning process, not all collaborations are beneficial; some may even be detrimental. In this study, we introduce a novel algorithm that assigns adaptive aggregation weights to clients participating in FL training, identifying those with data distributions most conducive to a specific learning objective. We demonstrate that our aggregation method converges no worse than the method that aggregates only the updates received from clients with the same data distribution. Furthermore, empirical evaluations consistently reveal that collaborations guided by our algorithm outperform traditional FL approaches. This underscores the critical role of judicious client selection and lays the foundation for more streamlined and effective FL implementations in the coming years.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#20998;&#24067;&#31616;&#21333;&#24615;&#20542;&#21521;&#65288;DSB&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#35268;&#24459;&#65292;&#21363;&#22312;&#35757;&#32451;&#26089;&#26399;&#33258;&#21160;&#23398;&#20064;&#20302;&#38454;&#30697;&#30340;&#26368;&#22823;&#29109;&#20998;&#24067;&#29305;&#24449;&#65292;&#28982;&#21518;&#22312;&#35757;&#32451;&#21518;&#26399;&#22833;&#21435;&#36825;&#31181;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#36827;&#34892;&#20102;&#20302;&#38454;&#32479;&#35745;&#25968;&#25454;&#30340;&#32534;&#36753;&#65292;&#35777;&#26126;&#20102;&#26089;&#26399;&#35757;&#32451;&#30340;&#32593;&#32476;&#20250;&#23558;&#32534;&#36753;&#30340;&#26679;&#26412;&#35270;&#20026;&#30446;&#26631;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.04362</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#36880;&#28176;&#22797;&#26434;&#30340;&#32479;&#35745;&#23398;
&lt;/p&gt;
&lt;p&gt;
Neural Networks Learn Statistics of Increasing Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#20998;&#24067;&#31616;&#21333;&#24615;&#20542;&#21521;&#65288;DSB&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#35268;&#24459;&#65292;&#21363;&#22312;&#35757;&#32451;&#26089;&#26399;&#33258;&#21160;&#23398;&#20064;&#20302;&#38454;&#30697;&#30340;&#26368;&#22823;&#29109;&#20998;&#24067;&#29305;&#24449;&#65292;&#28982;&#21518;&#22312;&#35757;&#32451;&#21518;&#26399;&#22833;&#21435;&#36825;&#31181;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#36827;&#34892;&#20102;&#20302;&#38454;&#32479;&#35745;&#25968;&#25454;&#30340;&#32534;&#36753;&#65292;&#35777;&#26126;&#20102;&#26089;&#26399;&#35757;&#32451;&#30340;&#32593;&#32476;&#20250;&#23558;&#32534;&#36753;&#30340;&#26679;&#26412;&#35270;&#20026;&#30446;&#26631;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#31616;&#21333;&#24615;&#20542;&#21521;&#65288;DSB&#65289;&#20551;&#35774;&#31070;&#32463;&#32593;&#32476;&#39318;&#20808;&#23398;&#20064;&#20302;&#38454;&#30697;&#65292;&#28982;&#21518;&#20877;&#36716;&#21521;&#39640;&#38454;&#30456;&#20851;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#32593;&#32476;&#22312;&#35757;&#32451;&#26089;&#26399;&#33258;&#21160;&#23398;&#20064;&#22312;&#26368;&#22823;&#29109;&#20998;&#24067;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#35757;&#32451;&#21518;&#26399;&#22833;&#21435;&#36825;&#31181;&#33021;&#21147;&#30340;&#26377;&#21147;&#26032;&#35777;&#25454;&#65292;&#32473;&#20986;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#26032;&#35777;&#25454;&#26469;&#25903;&#25345;DSB&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#35777;&#26126;&#20196;&#29260;$n$-gram&#39057;&#29575;&#19982;&#23884;&#20837;&#21521;&#37327;&#30340;&#30697;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#24182;&#22312;LLM&#20013;&#25214;&#21040;&#20542;&#21521;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#23558;DSB&#25193;&#23637;&#21040;&#31163;&#25955;&#39046;&#22495;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#23558;&#19968;&#31867;&#30340;&#20302;&#38454;&#32479;&#35745;&#25968;&#25454;&#25163;&#26415;&#24615;&#22320;&#32534;&#36753;&#25104;&#19982;&#21478;&#19968;&#31867;&#30456;&#21305;&#37197;&#65292;&#28982;&#21518;&#23637;&#31034;&#26089;&#26399;&#35757;&#32451;&#30340;&#32593;&#32476;&#23558;&#32534;&#36753;&#30340;&#26679;&#26412;&#35270;&#20026;&#26469;&#33258;&#30446;&#26631;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/EleutherAI/features-across-time &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The distributional simplicity bias (DSB) posits that neural networks learn low-order moments of the data distribution first, before moving on to higher-order correlations. In this work, we present compelling new evidence for the DSB by showing that networks automatically learn to perform well on maximum-entropy distributions whose low-order statistics match those of the training set early in training, then lose this ability later. We also extend the DSB to discrete domains by proving an equivalence between token $n$-gram frequencies and the moments of embedding vectors, and by finding empirical evidence for the bias in LLMs. Finally we use optimal transport methods to surgically edit the low-order statistics of one class to match those of another, and show that early-training networks treat the edited samples as if they were drawn from the target class. Code is available at https://github.com/EleutherAI/features-across-time.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#32771;&#23519;&#20102;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#29305;&#24449;&#21521;&#37327;&#30340;&#32500;&#24230;&#38382;&#39064;&#65292;&#21457;&#29616;&#23454;&#38469;&#20351;&#29992;&#30340;&#26550;&#26500;&#19982;&#29702;&#35770;&#20445;&#35777;&#23384;&#22312;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.03966</link><description>&lt;p&gt;
&#20851;&#20110;MPNN&#20013;&#29305;&#24449;&#21521;&#37327;&#30340;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
On dimensionality of feature vectors in MPNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03966
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#32771;&#23519;&#20102;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#29305;&#24449;&#21521;&#37327;&#30340;&#32500;&#24230;&#38382;&#39064;&#65292;&#21457;&#29616;&#23454;&#38469;&#20351;&#29992;&#30340;&#26550;&#26500;&#19982;&#29702;&#35770;&#20445;&#35777;&#23384;&#22312;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#32771;&#23519;&#20102;Morris&#31561;&#20154;&#65288;AAAI'19&#65289;&#20851;&#20110;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19982;Weisfeiler-Leman&#65288;WL&#65289;&#21516;&#26500;&#27979;&#35797;&#22312;&#21306;&#20998;&#33021;&#21147;&#19978;&#30456;&#31561;&#30340;&#32463;&#20856;&#32467;&#26524;&#12290;Morris&#31561;&#20154;&#23637;&#31034;&#20102;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#21644;$O(n)$&#32500;&#29305;&#24449;&#21521;&#37327;&#30340;&#20223;&#30495;&#32467;&#26524;&#65292;&#20854;&#20013;$n$&#26159;&#22270;&#30340;&#33410;&#28857;&#25968;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#24615;&#24341;&#20837;&#21040;&#26550;&#26500;&#20013;&#65292;Aamand&#31561;&#20154;&#65288;NeurIPS'22&#65289;&#33021;&#22815;&#23558;&#29305;&#24449;&#21521;&#37327;&#30340;&#32500;&#24230;&#25552;&#39640;&#21040;$O(\log n)$&#65292;&#23613;&#31649;&#20197;&#39640;&#27010;&#29575;&#20445;&#35777;&#23436;&#20840;&#27169;&#25311;&#30340;&#24320;&#38144;&#20063;&#22686;&#21152;&#20102;&#12290;&#22312;&#25152;&#26377;&#36825;&#20123;&#26500;&#36896;&#20013;&#65292;&#20026;&#20102;&#20445;&#35777;&#19982;WL&#27979;&#35797;&#30340;&#31561;&#20215;&#24615;&#65292;MPNN&#20013;&#30340;&#29305;&#24449;&#21521;&#37327;&#32500;&#24230;&#24517;&#39035;&#38543;&#30528;&#22270;&#30340;&#22823;&#23567;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#20351;&#29992;&#30340;&#26550;&#26500;&#20855;&#26377;&#24658;&#23450;&#32500;&#24230;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#32467;&#26524;&#25552;&#20379;&#30340;&#20445;&#35777;&#19982;&#23454;&#38469;&#20351;&#29992;&#30340;&#26550;&#26500;&#29305;&#24615;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the classical result of Morris et al.~(AAAI'19) that message-passing graphs neural networks (MPNNs) are equal in their distinguishing power to the Weisfeiler--Leman (WL) isomorphism test.   Morris et al.~show their simulation result with ReLU activation function and $O(n)$-dimensional feature vectors, where $n$ is the number of nodes of the graph. Recently, by introducing randomness into the architecture, Aamand et al.~(NeurIPS'22) were able to improve this bound to $O(\log n)$-dimensional feature vectors, although at the expense of guaranteeing perfect simulation only with high probability.   In all these constructions, to guarantee equivalence to the WL test, the dimension of feature vectors in the MPNN has to increase with the size of the graphs. However, architectures used in practice have feature vectors of constant dimension. Thus, there is a gap between the guarantees provided by these results and the actual characteristics of architectures used in practice. In this p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.03781</link><description>&lt;p&gt;
MolTC: &#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20998;&#23376;&#20851;&#31995;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MolTC: Towards Molecular Relational Modeling In Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#65288;MRL&#65289;&#26088;&#22312;&#29702;&#35299;&#20998;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#25512;&#36827;&#29983;&#29289;&#21270;&#23398;&#30740;&#31350;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37319;&#29992;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MRL&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#24222;&#22823;&#30340;&#30693;&#35782;&#23384;&#20648;&#24211;&#21644;&#20808;&#36827;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#25991;&#26412;&#25968;&#25454;&#65292;&#22240;&#27492;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20998;&#23376;&#22270;&#20013;&#22266;&#26377;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#21152;&#21095;&#20102;&#20449;&#24687;&#30340;&#28010;&#36153;&#65292;&#22240;&#20026;&#23427;&#38459;&#30861;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#20849;&#20139;&#23398;&#20064;&#21040;&#30340;&#30456;&#20114;&#20316;&#29992;&#29702;&#30001;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#26681;&#25454;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#29702;&#35770;&#23545;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#39044;&#27979;&#65292;&#31216;&#20026;MolTC&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#24378;&#30340;&#39640;&#25928;&#25968;&#20540;&#27874;&#20256;&#25773;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25968;&#20540;&#27714;&#35299;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#65292;&#20248;&#21270;&#31639;&#27861;&#26550;&#26500;&#12289;&#25968;&#25454;&#29983;&#25104;&#21644;&#24182;&#34892;&#26102;&#38388;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;&#36895;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02304</link><description>&lt;p&gt;
&#30001;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#24378;&#30340;&#39640;&#25928;&#25968;&#20540;&#27874;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Efficient Numerical Wave Propagation Enhanced by an End-to-End Deep Learning Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#24378;&#30340;&#39640;&#25928;&#25968;&#20540;&#27874;&#20256;&#25773;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25968;&#20540;&#27714;&#35299;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#65292;&#20248;&#21270;&#31639;&#27861;&#26550;&#26500;&#12289;&#25968;&#25454;&#29983;&#25104;&#21644;&#24182;&#34892;&#26102;&#38388;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;&#36895;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#65292;&#20174;&#22320;&#38663;&#24314;&#27169;&#21040;&#21307;&#23398;&#25104;&#20687;&#65292;&#23545;&#20110;&#39640;&#39057;&#27874;&#20256;&#25773;&#30340;&#39640;&#20445;&#30495;&#21644;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#22312;&#27874;&#20256;&#25773;&#27169;&#22411;&#20013;&#30340;&#19968;&#39033;&#36827;&#23637;&#21033;&#29992;&#36275;&#22815;&#20934;&#30830;&#30340;&#32454;&#27714;&#35299;&#22120;&#36755;&#20986;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#24555;&#36895;&#20294;&#19981;&#20934;&#30830;&#30340;&#31895;&#27714;&#35299;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;&#31283;&#23450;&#19988;&#24555;&#36895;&#30340;&#27714;&#35299;&#22120;&#36824;&#20801;&#35768;&#20351;&#29992;&#24182;&#34892;&#26102;&#38388;&#31639;&#27861;Parareal&#26469;&#25552;&#21462;&#21644;&#32416;&#27491;&#39640;&#39057;&#27874;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;Nguyen&#21644;Tsai&#65288;2023&#65289;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#31995;&#32479;&#65292;&#23558;&#25968;&#20540;&#27714;&#35299;&#22120;&#19982;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#25972;&#21512;&#21040;&#31471;&#21040;&#31471;&#26694;&#26550;&#20013;&#12290;&#22312;&#25552;&#20986;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12289;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#21644;Parareal&#26041;&#26696;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#21327;&#35843;&#30340;&#32467;&#26500;&#22312;&#19981;&#29306;&#29298;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
In a variety of scientific and engineering domains, ranging from seismic modeling to medical imaging, the need for high-fidelity and efficient solutions for high-frequency wave propagation holds great significance. Recent advances in wave modeling use sufficiently accurate fine solver outputs to train neural networks that enhance the accuracy of a fast but inaccurate coarse solver. A stable and fast solver further allows the use of Parareal, a parallel-in-time algorithm to retrieve and correct high-frequency wave components. In this paper we build upon the work of Nguyen and Tsai (2023) and present a novel unified system that integrates a numerical solver with deep learning components into an end-to-end framework. In the proposed setting, we investigate refinements to the neural network architecture, data generation algorithm and Parareal scheme. Our results show that the cohesive structure significantly improves performance without sacrificing speed, and demonstrate the importance of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#25968;&#65292;&#35813;&#25351;&#25968;&#26681;&#25454;&#29616;&#26377;&#30340;&#22522;&#30784;&#25351;&#25968;&#23450;&#20041;&#65292;&#24182;&#29992;&#20110;&#26816;&#27979;&#27425;&#20248;&#32858;&#31867;&#25968;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#25351;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02162</link><description>&lt;p&gt;
&#19968;&#20010;&#36125;&#21494;&#26031;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Bayesian cluster validity index
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#25968;&#65292;&#35813;&#25351;&#25968;&#26681;&#25454;&#29616;&#26377;&#30340;&#22522;&#30784;&#25351;&#25968;&#23450;&#20041;&#65292;&#24182;&#29992;&#20110;&#26816;&#27979;&#27425;&#20248;&#32858;&#31867;&#25968;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#25351;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#32858;&#31867;&#31639;&#27861;&#26102;&#65292;&#36873;&#25321;&#32858;&#31867;&#25968;&#26159;&#20851;&#38190;&#27493;&#39588;&#20043;&#19968;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#21508;&#31181;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#25968;&#65288;CVIs&#65289;&#12290;&#22823;&#22810;&#25968;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#25968;&#37117;&#34987;&#23450;&#20041;&#20026;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#38544;&#34255;&#30340;&#26368;&#20248;&#32858;&#31867;&#25968;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#26377;&#26102;&#24182;&#19981;&#26399;&#26395;&#33719;&#24471;&#26368;&#20248;&#32858;&#31867;&#25968;&#65292;&#32780;&#26159;&#26356;&#36866;&#21512;&#20182;&#20204;&#24212;&#29992;&#30340;&#27425;&#20248;&#32858;&#31867;&#25968;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#29616;&#26377;&#22522;&#30784;&#25351;&#25968;&#30340;&#36125;&#21494;&#26031;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#25968;&#65288;BCVI&#65289;&#12290;&#35813;&#25351;&#25968;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#25110;&#24191;&#20041;&#29380;&#21033;&#20811;&#38647;&#20808;&#39564;&#23450;&#20041;&#65292;&#24471;&#21040;&#30456;&#21516;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#28982;&#21518;&#25105;&#20204;&#22522;&#20110;Wiroonsri&#25351;&#25968;&#65288;WI&#65289;&#21644;Wiroonsri-Preedasawakul&#25351;&#25968;&#65288;WP&#65289;&#20316;&#20026;&#30828;&#32858;&#31867;&#21644;&#36719;&#32858;&#31867;&#30340;&#22522;&#30784;&#25351;&#25968;&#26469;&#27979;&#35797;&#25105;&#20204;&#30340;BCVI&#12290;&#25105;&#20204;&#23558;&#23427;&#20204;&#30340;&#32467;&#26524;&#19982;&#21407;&#22987;&#30340;&#22522;&#30784;&#25351;&#25968;&#20197;&#21450;&#19968;&#20123;&#20854;&#20182;&#23384;&#22312;&#30340;CVIs&#65288;&#21253;&#25324;Davies and Bouldin (DB)&#65292;Starczewski (STR)&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting the number of clusters is one of the key processes when applying clustering algorithms. To fulfill this task, various cluster validity indices (CVIs) have been introduced. Most of the cluster validity indices are defined to detect the optimal number of clusters hidden in a dataset. However, users sometimes do not expect to get the optimal number of groups but a secondary one which is more reasonable for their applications. This has motivated us to introduce a Bayesian cluster validity index (BCVI) based on existing underlying indices. This index is defined based on either Dirichlet or Generalized Dirichlet priors which result in the same posterior distribution. Our BCVI is then tested based on the Wiroonsri index (WI), and the Wiroonsri-Preedasawakul index (WP) as underlying indices for hard and soft clustering, respectively. We compare their outcomes with the original underlying indices, as well as a few more existing CVIs including Davies and Bouldin (DB), Starczewski (STR)
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;Transformer&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#23545;&#34920;&#36798;&#33021;&#21147;&#30340;&#24433;&#21709;&#26426;&#21046;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#38190;&#21442;&#25968;&#23545;Transformer&#30340;&#20316;&#29992;&#65292;&#24182;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.00522</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#22312;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;Transformer&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#23545;&#34920;&#36798;&#33021;&#21147;&#30340;&#24433;&#21709;&#26426;&#21046;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#38190;&#21442;&#25968;&#23545;Transformer&#30340;&#20316;&#29992;&#65292;&#24182;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;Transformer&#22312;&#38271;&#12289;&#31232;&#30095;&#21644;&#22797;&#26434;&#35760;&#24518;&#30340;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;Transformer&#30340;&#19981;&#21516;&#32452;&#20214;&#65288;&#22914;&#28857;&#31215;&#33258;&#27880;&#24847;&#21147;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#21069;&#39304;&#23618;&#65289;&#26159;&#22914;&#20309;&#24433;&#21709;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#26126;&#30830;&#30340;&#36817;&#20284;&#29575;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#32508;&#21512;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#20013;&#20851;&#38190;&#21442;&#25968;&#65288;&#22914;&#23618;&#25968;&#21644;&#27880;&#24847;&#21147;&#22836;&#25968;&#65289;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#36825;&#20123;&#27934;&#23519;&#36824;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates. Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads, and these insights also provide natural suggestions for alternative architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#23556;&#20987;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#26469;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#65292;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;&#38381;&#21367;&#38382;&#31572;&#65292;&#27169;&#22411;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#36825;&#20123;&#32467;&#26524;&#20063;&#26174;&#31034;&#20986;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2401.08273</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#38646;&#23556;&#20987;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Null-Shot Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#23556;&#20987;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#26469;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#65292;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;&#38381;&#21367;&#38382;&#31572;&#65292;&#27169;&#22411;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#36825;&#20123;&#32467;&#26524;&#20063;&#26174;&#31034;&#20986;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#23556;&#20987;&#25552;&#31034;&#26041;&#27861;&#12290;&#38646;&#23556;&#20987;&#25552;&#31034;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#36890;&#36807;&#25351;&#31034;LLMs&#21033;&#29992;&#20174;&#8220;&#31034;&#20363;&#8221;&#37096;&#20998;&#20013;&#33719;&#21462;&#30340;&#20449;&#24687;&#65288;&#35813;&#20449;&#24687;&#22312;&#25152;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#20013;&#19981;&#23384;&#22312;&#65289;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#34429;&#28982;&#20943;&#23569;&#38169;&#35823;&#20449;&#24687;&#23545;&#20110;LLMs&#30340;&#26085;&#24120;&#21644;&#37325;&#35201;&#29992;&#36884;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#22312;&#30446;&#21069;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;LLMs&#20173;&#28982;&#20855;&#26377;&#38169;&#35823;&#20449;&#24687;&#65292;&#23454;&#38469;&#19978;&#21487;&#20197;&#21033;&#29992;&#38169;&#35823;&#20449;&#24687;&#26469;&#25552;&#39640;&#19982;&#26631;&#20934;&#38646;&#23556;&#20987;&#25552;&#31034;&#30456;&#27604;&#30340;&#20219;&#21153;&#34920;&#29616;&#12290;&#23545;&#20843;&#20010;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#20843;&#20010;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;&#38381;&#21367;&#38382;&#31572;&#65289;&#20013;&#65292;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#35266;&#23519;&#21040;&#30340;&#19981;&#19968;&#33268;&#24615;&#22686;&#21152;&#30456;&#23545;&#24615;&#33021;&#22312;LLMs&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20063;&#21487;&#33021;&#34920;&#31034;&#27599;&#20010;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08273v2 Announce Type: replace-cross Abstract: This paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the "Examples" section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with eight LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across the LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20102;&#35299;&#20915;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#20013;"&#37325;&#21472;"&#20551;&#35774;&#30340;&#38382;&#39064;&#30340;&#28608;&#21169;&#24863;&#30693;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28608;&#21169;&#21333;&#20301;&#37319;&#21462;&#36890;&#24120;&#19981;&#20250;&#32771;&#34385;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#25552;&#20379;&#19982;&#28608;&#21169;&#30456;&#23481;&#30340;&#24178;&#39044;&#24314;&#35758;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#38754;&#26495;&#25968;&#25454;&#29615;&#22659;&#20013;&#20934;&#30830;&#20272;&#35745;&#21453;&#20107;&#23454;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.16307</link><description>&lt;p&gt;
&#28608;&#21169;&#24863;&#30693;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65306;&#36890;&#36807;&#28608;&#21169;&#25506;&#32034;&#36827;&#34892;&#20934;&#30830;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Incentive-Aware Synthetic Control: Accurate Counterfactual Estimation via Incentivized Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20102;&#35299;&#20915;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#20013;"&#37325;&#21472;"&#20551;&#35774;&#30340;&#38382;&#39064;&#30340;&#28608;&#21169;&#24863;&#30693;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28608;&#21169;&#21333;&#20301;&#37319;&#21462;&#36890;&#24120;&#19981;&#20250;&#32771;&#34385;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#25552;&#20379;&#19982;&#28608;&#21169;&#30456;&#23481;&#30340;&#24178;&#39044;&#24314;&#35758;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#38754;&#26495;&#25968;&#25454;&#29615;&#22659;&#20013;&#20934;&#30830;&#20272;&#35745;&#21453;&#20107;&#23454;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65288;SCMs&#65289;&#30340;&#35774;&#23450;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#38754;&#26495;&#25968;&#25454;&#29615;&#22659;&#20013;&#20272;&#35745;&#34987;&#27835;&#30103;&#23545;&#35937;&#30340;&#27835;&#30103;&#25928;&#24212;&#30340;&#32463;&#20856;&#26041;&#27861;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;SCMs&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#20294;&#26222;&#36941;&#23384;&#22312;&#30340;&#8220;&#37325;&#21472;&#8221;&#20551;&#35774;&#65306;&#19968;&#20010;&#34987;&#27835;&#30103;&#30340;&#21333;&#20301;&#21487;&#20197;&#34987;&#20889;&#25104;&#20445;&#25345;&#25511;&#21046;&#30340;&#21333;&#20301;&#30340;&#26576;&#31181;&#32452;&#21512;&#65288;&#36890;&#24120;&#26159;&#20984;&#25110;&#32447;&#24615;&#32452;&#21512;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#21333;&#20301;&#36873;&#25321;&#33258;&#24049;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#24182;&#19988;&#21333;&#20301;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#36275;&#22815;&#22823;&#65292;&#20197;&#33267;&#20110;&#20182;&#20204;&#20559;&#22909;&#19981;&#21516;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#37325;&#21472;&#23558;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#28608;&#21169;&#20855;&#26377;&#19981;&#21516;&#20559;&#22909;&#30340;&#21333;&#20301;&#26469;&#37319;&#21462;&#20182;&#20204;&#36890;&#24120;&#19981;&#20250;&#32771;&#34385;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20449;&#24687;&#35774;&#35745;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;SCM&#65292;&#36890;&#36807;&#20026;&#21333;&#20301;&#25552;&#20379;&#19982;&#28608;&#21169;&#30456;&#23481;&#30340;&#24178;&#39044;&#24314;&#35758;&#65292;&#22312;&#38754;&#26495;&#25968;&#25454;&#29615;&#22659;&#20013;&#28608;&#21169;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16307v2 Announce Type: replace-cross Abstract: We consider the setting of synthetic control methods (SCMs), a canonical approach used to estimate the treatment effect on the treated in a panel data setting. We shed light on a frequently overlooked but ubiquitous assumption made in SCMs of "overlap": a treated unit can be written as some combination -- typically, convex or linear combination -- of the units that remain under control. We show that if units select their own interventions, and there is sufficiently large heterogeneity between units that prefer different interventions, overlap will not hold. We address this issue by proposing a framework which incentivizes units with different preferences to take interventions they would not normally consider. Specifically, leveraging tools from information design and online learning, we propose a SCM that incentivizes exploration in panel data settings by providing incentive-compatible intervention recommendations to units. We e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#25506;&#27979;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#20449;&#21495;&#30340;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2312.14440</link><description>&lt;p&gt;
&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#19981;&#23545;&#31216;&#20559;&#24046;&#30340;&#23545;&#25239;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#25506;&#27979;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#20449;&#21495;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#22312;&#20869;&#23481;&#29983;&#25104;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#38656;&#35201;&#20180;&#32454;&#30740;&#31350;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#65292;&#21253;&#25324;&#23427;&#20204;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#24050;&#32463;&#24456;&#24191;&#27867;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#30340;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#25991;&#23545;T2I&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#37325;&#28857;&#20998;&#26512;&#20102;&#19982;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;ASR&#65289;&#30456;&#20851;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#30446;&#26631; - &#20351;&#29992;&#23545;&#25239;&#24615;&#21518;&#32512;&#36827;&#34892;&#23454;&#20307;&#26367;&#25442;&#65292;&#20197;&#21450;&#20004;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#31639;&#27861;&#12290;&#20154;&#24037;&#21644;&#33258;&#21160;&#35780;&#20272;&#25581;&#31034;&#20102;&#23454;&#20307;&#20132;&#25442;&#20013;ASR&#30340;&#19981;&#23545;&#31216;&#24615;&#36136;&#65306;&#20363;&#22914;&#65292;&#23545;&#20110;&#22312;&#25552;&#31034;&#8220;&#22312;&#38632;&#20013;&#36339;&#33310;&#30340;&#20154;&#31867;&#8221;&#20013;&#26367;&#25442;&#8220;&#20154;&#31867;&#8221;&#20026;&#8220;&#26426;&#22120;&#20154;&#8221;&#30340;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#36739;&#23481;&#26131;&#23454;&#29616;&#65292;&#32780;&#21453;&#21521;&#26367;&#25442;&#21017;&#26126;&#26174;&#22256;&#38590;&#24471;&#22810;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#25506;&#27979;&#25351;&#26631;&#65292;&#20197;&#30830;&#23450;&#27169;&#22411;&#23545;&#25239;ASR&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14440v2 Announce Type: replace Abstract: The widespread use of Text-to-Image (T2I) models in content generation requires careful examination of their safety, including their robustness to adversarial attacks. Despite extensive research on adversarial attacks, the reasons for their effectiveness remain underexplored. This paper presents an empirical study on adversarial attacks against T2I models, focusing on analyzing factors associated with attack success rates (ASR). We introduce a new attack objective - entity swapping using adversarial suffixes and two gradient-based attack algorithms. Human and automatic evaluations reveal the asymmetric nature of ASRs on entity swap: for example, it is easier to replace "human" with "robot" in the prompt "a human dancing in the rain." with an adversarial suffix, but the reverse replacement is significantly harder. We further propose probing metrics to establish indicative signals from the model's beliefs to the adversarial ASR. We iden
&lt;/p&gt;</description></item><item><title>POND&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36827;&#34892;&#22810;&#28304;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.12276</link><description>&lt;p&gt;
POND: &#24102;&#26377;&#20449;&#24687;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#30340;&#22810;&#28304;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
POND: Multi-Source Time Series Domain Adaptation with Information-Aware Prompt Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12276
&lt;/p&gt;
&lt;p&gt;
POND&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36827;&#34892;&#22810;&#28304;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#36866;&#24212;&#26159;&#19968;&#20010;&#20851;&#38190;&#32780;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#12289;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#21644;&#26426;&#22120;&#25925;&#38556;&#35786;&#26029;&#31561;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#39046;&#22495;&#36866;&#24212;&#25216;&#26415;&#26469;&#24212;&#23545;&#36825;&#20010;&#22797;&#26434;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#28304;&#39046;&#22495;&#30340;&#39046;&#22495;&#36866;&#24212;&#19978;&#12290;&#28982;&#32780;&#65292;&#25506;&#32034;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#39046;&#22495;&#36866;&#24212;&#26356;&#20026;&#20851;&#38190;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#24102;&#26469;&#26356;&#22823;&#30340;&#25913;&#36827;&#12290;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#39046;&#22495;&#21028;&#21035;&#65288;POND&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12276v2 Announce Type: replace Abstract: Time series domain adaptation stands as a pivotal and intricate challenge with diverse applications, including but not limited to human activity recognition, sleep stage classification, and machine fault diagnosis. Despite the numerous domain adaptation techniques proposed to tackle this complex problem, they primarily focus on domain adaptation from a single source domain. Yet, it is more crucial to investigate domain adaptation from multiple domains due to the potential for greater improvements. To address this, three important challenges need to be overcome: 1). The lack of exploration to utilize domain-specific information for domain adaptation, 2). The difficulty to learn domain-specific information that changes over time, and 3). The difficulty to evaluate learned domain-specific information. In order to tackle these challenges simultaneously, in this paper, we introduce PrOmpt-based domaiN Discrimination (POND), the first frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;diff&#21382;&#21490;&#30340;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23558;&#29615;&#22659;&#20013;&#30340;&#35266;&#27979;&#36716;&#25442;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#20197;&#20415;&#23545;&#20110;&#38271;&#26399;&#25512;&#29702;&#20915;&#31574;&#30340;&#20219;&#21153;&#20013;&#30340;Neural Language Models&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.07540</link><description>&lt;p&gt;
Neural Language Agents&#30340;diff&#21382;&#21490;
&lt;/p&gt;
&lt;p&gt;
diff History for Neural Language Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;diff&#21382;&#21490;&#30340;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23558;&#29615;&#22659;&#20013;&#30340;&#35266;&#27979;&#36716;&#25442;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#20197;&#20415;&#23545;&#20110;&#38271;&#26399;&#25512;&#29702;&#20915;&#31574;&#30340;&#20219;&#21153;&#20013;&#30340;Neural Language Models&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural Language Models (LMs)&#20026;&#36890;&#29992;&#30340;&#20855;&#20307;&#25511;&#21046;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#22522;&#20110;LM&#30340;&#25511;&#21046;&#22120;&#26102;&#65292;&#20250;&#20986;&#29616;&#19968;&#20010;&#20851;&#38190;&#30340;&#25216;&#26415;&#38382;&#39064;&#65306;&#29615;&#22659;&#35266;&#27979;&#24517;&#39035;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#36825;&#19982;&#21382;&#21490;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#23548;&#33268;&#20887;&#38271;&#32780;&#20887;&#20313;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;&#22240;&#27492;&#65292;LM&#20195;&#29702;&#30340;&#20808;&#21069;&#24037;&#20316;&#23616;&#38480;&#20110;&#20855;&#26377;&#23567;&#35266;&#27979;&#22823;&#23567;&#20197;&#21450;&#23545;&#20132;&#20114;&#21382;&#21490;&#25110;&#25351;&#31034;&#35843;&#20248;&#38656;&#27714;&#36739;&#23567;&#30340;&#38480;&#21046;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;diff&#21382;&#21490;&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#19988;&#38750;&#24120;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#29992;&#20110;&#25552;&#31034;LM&#31574;&#30053;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#30340;&#36830;&#32493;&#25991;&#26412;&#35266;&#27979;&#19978;&#24212;&#29992;Unix diff&#21629;&#20196;&#65292;&#25105;&#20204;&#26082;&#21487;&#20197;&#25688;&#38500;&#20887;&#20313;&#20449;&#24687;&#65292;&#21448;&#21487;&#20197;&#23558;&#25991;&#26412;&#36755;&#20837;&#30340;&#20869;&#23481;&#38598;&#20013;&#22312;&#29615;&#22659;&#20013;&#26174;&#33879;&#21464;&#21270;&#30340;&#26041;&#38754;&#12290;&#22312;&#38656;&#35201;&#38271;&#26399;&#25512;&#29702;&#36827;&#34892;&#20915;&#31574;&#30340;&#26410;&#35299;&#20915;&#30340;&#35270;&#39057;&#28216;&#25103;NetHack&#20013;&#65292;&#20351;&#29992;diff&#21382;&#21490;&#35843;&#20248;&#30340;LM&#19982;&#29366;&#24577;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07540v2 Announce Type: replace Abstract: Neural Language Models (LMs) offer an exciting solution for general-purpose embodied control. However, a key technical issue arises when using an LM-based controller: environment observations must be converted to text, which coupled with history, results in long and verbose textual prompts. As a result, prior work in LM agents is limited to restricted domains with small observation size as well as minimal needs for interaction history or instruction tuning. In this paper, we introduce diff history, a simple and highly effective solution to these issues. By applying the Unix diff command on consecutive text observations in the interaction histories used to prompt LM policies, we can both abstract away redundant information and focus the content of textual inputs on the salient changes in the environment. On NetHack, an unsolved video game that requires long-horizon reasoning for decision-making, LMs tuned with diff history match state-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;&#29305;&#23450;&#30828;&#20214;&#36739;&#23481;&#26131;&#33719;&#21462;&#30340;&#23616;&#37096;&#24615;&#29305;&#24449;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#33073;&#31163;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#20248;&#21270;</title><link>https://arxiv.org/abs/2312.02544</link><description>&lt;p&gt;
&#34920;&#24449;&#33258;&#26059;&#29366;&#24577;&#21644;&#24378;&#21046;&#31227;&#21160;&#20248;&#21270;&#20013;&#30340;&#23616;&#37096;&#24615;
&lt;/p&gt;
&lt;p&gt;
Characterization of Locality in Spin States and Forced Moves for Optimizations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02544
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;&#29305;&#23450;&#30828;&#20214;&#36739;&#23481;&#26131;&#33719;&#21462;&#30340;&#23616;&#37096;&#24615;&#29305;&#24449;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#33073;&#31163;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Ising&#20844;&#24335;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#26368;&#36817;&#20063;&#25552;&#20379;&#20102;&#21508;&#31181;&#37327;&#23376;&#25110;&#22522;&#20110;&#21322;&#23548;&#20307;&#30340;&#30828;&#20214;&#12290;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#33021;&#37327;&#26223;&#35266;&#20013;&#23384;&#22312;&#23616;&#37096;&#26497;&#23567;&#20540;&#23545;&#20110;&#23547;&#25214;&#20840;&#23616;&#26497;&#23567;&#20540;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#20248;&#21270;&#30340;&#30446;&#26631;&#19981;&#26159;&#20174;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#20013;&#33719;&#24471;&#31934;&#30830;&#30340;&#37319;&#26679;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;&#28385;&#36275;&#32454;&#33268;&#24179;&#34913;&#26465;&#20214;&#12290;&#22522;&#20110;&#36825;&#19968;&#20107;&#23454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#20174;&#23616;&#37096;&#26497;&#23567;&#20540;&#20013;&#33073;&#31163;&#20986;&#26469;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#31934;&#30830;&#30340;&#37319;&#26679;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#31181;&#25551;&#36848;&#24403;&#21069;&#29366;&#24577;&#23616;&#37096;&#24615;&#30340;&#29305;&#24449;&#65292;&#36825;&#31181;&#29305;&#24449;&#21487;&#20197;&#21033;&#29992;&#26576;&#31181;&#19987;&#29992;&#30828;&#20214;&#36731;&#26494;&#33719;&#21462;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25552;&#20986;&#30340;&#31639;&#27861;&#22522;&#20110;&#26080;&#25298;&#32477;&#30340;&#31639;&#27861;&#65292;&#35745;&#31639;&#25104;&#26412;&#24456;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#20171;&#32461;&#25152;&#25552;&#31639;&#27861;&#30340;&#32454;&#33410;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02544v2 Announce Type: replace-cross Abstract: Ising formulations are widely utilized to solve combinatorial optimization problems, and a variety of quantum or semiconductor-based hardware has recently been made available. In combinatorial optimization problems, the existence of local minima in energy landscapes is problematic to use to seek the global minimum. We note that the aim of the optimization is not to obtain exact samplings from the Boltzmann distribution, and there is thus no need to satisfy detailed balance conditions. In light of this fact, we develop an algorithm to get out of the local minima efficiently while it does not yield the exact samplings. For this purpose, we utilize a feature that characterizes locality in the current state, which is easy to obtain with a type of specialized hardware. Furthermore, as the proposed algorithm is based on a rejection-free algorithm, the computational cost is low. In this work, after presenting the details of the propose
&lt;/p&gt;</description></item><item><title>&#20174;fMRI&#25968;&#25454;&#20013;&#23398;&#20064;&#33041;&#21306;&#30340;&#39640;&#38454;&#20851;&#31995;&#23545;&#20110;&#33041;&#21306;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#20934;&#30830;&#34920;&#24449;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HYBRID&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#36229;&#36793;&#32467;&#26500;&#21644;&#35745;&#31639;&#36229;&#36793;&#30340;&#26435;&#37325;&#65292;&#25552;&#21462;&#20102;&#26368;&#20855;&#20449;&#24687;&#37327;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#39640;&#38454;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2312.02203</link><description>&lt;p&gt;
&#23398;&#20064;&#33041;&#21306;&#30340;&#39640;&#38454;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning High-Order Relationships of Brain Regions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02203
&lt;/p&gt;
&lt;p&gt;
&#20174;fMRI&#25968;&#25454;&#20013;&#23398;&#20064;&#33041;&#21306;&#30340;&#39640;&#38454;&#20851;&#31995;&#23545;&#20110;&#33041;&#21306;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#20934;&#30830;&#34920;&#24449;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HYBRID&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#36229;&#36793;&#32467;&#26500;&#21644;&#35745;&#31639;&#36229;&#36793;&#30340;&#26435;&#37325;&#65292;&#25552;&#21462;&#20102;&#26368;&#20855;&#20449;&#24687;&#37327;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#39640;&#38454;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20449;&#21495;&#20013;&#21457;&#29616;&#33041;&#21306;&#20043;&#38388;&#21487;&#38752;&#19988;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#20851;&#31995;&#23545;&#20110;&#34920;&#22411;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#22320;&#34920;&#24449;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#20851;&#27880;&#37197;&#23545;&#36830;&#25509;&#24182;&#24573;&#35270;&#20102;&#33041;&#21306;&#30340;&#39640;&#38454;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#36825;&#20123;&#39640;&#38454;&#20851;&#31995;&#24212;&#35813;&#20855;&#26377;&#26368;&#22823;&#30340;&#20449;&#24687;&#37327;&#21644;&#26368;&#23567;&#30340;&#20887;&#20313;&#65288;MIMR&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25351;&#25968;&#32423;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#21487;&#35299;&#24615;&#30446;&#26631;&#30340;&#32570;&#20047;&#65292;&#21457;&#29616;&#36825;&#31181;&#39640;&#38454;&#20851;&#31995;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#26410;&#32463;&#25506;&#32034;&#30340;&#12290;&#38024;&#23545;&#36825;&#19968;&#38388;&#38553;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HYBRID&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;fMRI&#25968;&#25454;&#20013;&#25552;&#21462;MIMR&#39640;&#38454;&#20851;&#31995;&#12290;HYBRID&#21033;&#29992;CONSTRUCTOR&#26469;&#35782;&#21035;&#36229;&#36793;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;WEIGHTER&#26469;&#35745;&#31639;&#27599;&#20010;&#36229;&#36793;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#36991;&#20813;&#22312;&#25351;&#25968;&#31354;&#38388;&#20013;&#36827;&#34892;&#25628;&#32034;&#12290;HYBRID&#21462;&#24471;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02203v2 Announce Type: replace-cross Abstract: Discovering reliable and informative relationships among brain regions from functional magnetic resonance imaging (fMRI) signals is essential in phenotypic predictions. Most of the current methods fail to accurately characterize those interactions because they only focus on pairwise connections and overlook the high-order relationships of brain regions. We propose that these high-order relationships should be maximally informative and minimally redundant (MIMR). However, identifying such high-order relationships is challenging and under-explored due to the exponential search space and the absence of a tractable objective. In response to this gap, we propose a novel method named HYBRID which aims to extract MIMR high-order relationships from fMRI data. HYBRID employs a CONSTRUCTOR to identify hyperedge structures, and a WEIGHTER to compute a weight for each hyperedge, which avoids searching in exponential space. HYBRID achieves t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#29702;&#24615;&#19982;&#38750;&#29702;&#24615;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#26410;&#35299;&#38382;&#39064;&#12290;&#37325;&#28857;&#35752;&#35770;&#20102;&#34892;&#20026;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#38750;&#29702;&#24615;&#34892;&#20026;&#21487;&#33021;&#26159;&#26368;&#20248;&#30340;&#24773;&#20917;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#22788;&#29702;&#38750;&#29702;&#24615;&#20195;&#29702;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.17165</link><description>&lt;p&gt;
(&#38750;)&#29702;&#24615;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65306;&#29616;&#29366;&#12289;&#30740;&#31350;&#25361;&#25112;&#21644;&#26410;&#35299;&#20043;&#38382;
&lt;/p&gt;
&lt;p&gt;
(Ir)rationality in AI: State of the Art, Research Challenges and Open Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#29702;&#24615;&#19982;&#38750;&#29702;&#24615;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#26410;&#35299;&#38382;&#39064;&#12290;&#37325;&#28857;&#35752;&#35770;&#20102;&#34892;&#20026;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#38750;&#29702;&#24615;&#34892;&#20026;&#21487;&#33021;&#26159;&#26368;&#20248;&#30340;&#24773;&#20917;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#22788;&#29702;&#38750;&#29702;&#24615;&#20195;&#29702;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24615;&#27010;&#24565;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#21344;&#25454;&#30528;&#37325;&#35201;&#22320;&#20301;&#12290;&#26080;&#35770;&#26159;&#27169;&#25311;&#20154;&#31867;&#25512;&#29702;&#36824;&#26159;&#36861;&#27714;&#26377;&#38480;&#26368;&#20248;&#24615;&#65292;&#25105;&#20204;&#36890;&#24120;&#24076;&#26395;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23613;&#21487;&#33021;&#29702;&#24615;&#12290;&#23613;&#31649;&#36825;&#20010;&#27010;&#24565;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#38750;&#24120;&#26680;&#24515;&#65292;&#20294;&#23545;&#20110;&#20160;&#20040;&#26500;&#25104;&#29702;&#24615;&#20195;&#29702;&#24182;&#27809;&#26377;&#32479;&#19968;&#30340;&#23450;&#20041;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#29702;&#24615;&#19982;&#38750;&#29702;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36825;&#20010;&#39046;&#22495;&#30340;&#26410;&#35299;&#38382;&#39064;&#12290;&#22312;&#20854;&#20182;&#39046;&#22495;&#23545;&#29702;&#24615;&#30340;&#29702;&#35299;&#23545;&#20854;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#27010;&#24565;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#32463;&#27982;&#23398;&#12289;&#21746;&#23398;&#21644;&#24515;&#29702;&#23398;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#30528;&#37325;&#32771;&#34385;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#26576;&#20123;&#24773;&#22659;&#20013;&#38750;&#29702;&#24615;&#34892;&#20026;&#21487;&#33021;&#26159;&#26368;&#20248;&#30340;&#24773;&#20917;&#12290;&#20851;&#20110;&#22788;&#29702;&#38750;&#29702;&#24615;&#20195;&#29702;&#30340;&#26041;&#27861;&#24050;&#32463;&#24471;&#21040;&#20102;&#19968;&#20123;&#21457;&#23637;&#65292;&#21253;&#25324;&#35782;&#21035;&#21644;&#20132;&#20114;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#24037;&#20316;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17165v2 Announce Type: replace Abstract: The concept of rationality is central to the field of artificial intelligence. Whether we are seeking to simulate human reasoning, or the goal is to achieve bounded optimality, we generally seek to make artificial agents as rational as possible. Despite the centrality of the concept within AI, there is no unified definition of what constitutes a rational agent. This article provides a survey of rationality and irrationality in artificial intelligence, and sets out the open questions in this area. The understanding of rationality in other fields has influenced its conception within artificial intelligence, in particular work in economics, philosophy and psychology. Focusing on the behaviour of artificial agents, we consider irrational behaviours that can prove to be optimal in certain scenarios. Some methods have been developed to deal with irrational agents, both in terms of identification and interaction, however work in this area re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#27880;&#24847;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32593;&#32476;&#23450;&#20301;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20986;&#33394;&#30340;&#31934;&#30830;&#24230;&#65292;&#29978;&#33267;&#22312;&#20005;&#37325;&#38750;&#30452;&#35270;&#35270;&#32447;&#26465;&#20214;&#19979;&#20063;&#33021;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#20851;&#27880;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.16856</link><description>&lt;p&gt;
&#20851;&#27880;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#31283;&#20581;&#30340;&#22823;&#35268;&#27169;&#32593;&#32476;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Attentional Graph Neural Networks for Robust Massive Network Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#27880;&#24847;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32593;&#32476;&#23450;&#20301;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20986;&#33394;&#30340;&#31934;&#30830;&#24230;&#65292;&#29978;&#33267;&#22312;&#20005;&#37325;&#38750;&#30452;&#35270;&#35270;&#32447;&#26465;&#20214;&#19979;&#20063;&#33021;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#20851;&#27880;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#21457;&#25496;GNNs&#22312;&#22238;&#24402;&#20013;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#23558;GNNs&#19982;&#27880;&#24847;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20854;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#24443;&#24213;&#25913;&#21464;&#20102;&#24207;&#21015;&#23398;&#20064;&#20219;&#21153;&#30340;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65306;&#32593;&#32476;&#23450;&#20301;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#30340;&#26032;&#22411;&#32593;&#32476;&#23450;&#20301;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#20005;&#37325;&#38750;&#30452;&#35270;&#35270;&#32447;(NLOS)&#26465;&#20214;&#19979;&#20063;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#31934;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#32321;&#29712;&#30340;&#31163;&#32447;&#26657;&#20934;&#25110;NLOS&#35782;&#21035;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#22270;&#31070;&#32463;&#32593;&#32476;(AGNN)&#27169;&#22411;&#65292;&#26088;&#22312;&#25913;&#21892;&#22522;&#20110;GCN&#26041;&#27861;&#30340;&#26377;&#38480;&#28789;&#27963;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#39640;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16856v2 Announce Type: replace Abstract: In recent years, Graph neural networks (GNNs) have emerged as a prominent tool for classification tasks in machine learning. However, their application in regression tasks remains underexplored. To tap the potential of GNNs in regression, this paper integrates GNNs with attention mechanism, a technique that revolutionized sequential learning tasks with its adaptability and robustness, to tackle a challenging nonlinear regression problem: network localization. We first introduce a novel network localization method based on graph convolutional network (GCN), which exhibits exceptional precision even under severe non-line-of-sight (NLOS) conditions, thereby diminishing the need for laborious offline calibration or NLOS identification. We further propose an attentional graph neural network (AGNN) model, aimed at improving the limited flexibility and mitigating the high sensitivity to the hyperparameter of the GCN-based method. The AGNN co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#23618;&#20248;&#21270;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22312;&#24322;&#26500;&#35774;&#32622;&#19979;&#20855;&#26377;&#36739;&#23567;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#36718;&#27425;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2311.11342</link><description>&lt;p&gt;
&#20851;&#20110;&#21435;&#20013;&#24515;&#21270;&#21452;&#23618;&#20248;&#21270;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
On the Communication Complexity of Decentralized Bilevel Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#23618;&#20248;&#21270;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22312;&#24322;&#26500;&#35774;&#32622;&#19979;&#20855;&#26377;&#36739;&#23567;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#36718;&#27425;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#21435;&#20013;&#24515;&#21270;&#21452;&#23618;&#20248;&#21270;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31639;&#27861;&#30001;&#20110;&#20272;&#35745;&#38543;&#26426;&#36229;&#26799;&#24230;&#32780;&#23548;&#33268;&#36890;&#20449;&#22797;&#26434;&#24230;&#36739;&#22823;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#24322;&#26500;&#35774;&#32622;&#19979;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#27599;&#36718;&#20013;&#20855;&#26377;&#36739;&#23567;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#36739;&#23569;&#30340;&#36890;&#20449;&#36718;&#27425;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#24322;&#26500;&#24615;&#30340;&#24378;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#24322;&#26500;&#35774;&#32622;&#19979;&#23454;&#29616;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#30340;&#38543;&#26426;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11342v2 Announce Type: replace Abstract: Decentralized bilevel optimization has been actively studied in the past few years since it has widespread applications in machine learning. However, existing algorithms suffer from large communication complexity caused by the estimation of stochastic hypergradient, limiting their application to real-world tasks. To address this issue, we develop a novel decentralized stochastic bilevel gradient descent algorithm under the heterogeneous setting, which enjoys a small communication cost in each round and a small number of communication rounds. As such, it can achieve a much better communication complexity than existing algorithms without any strong assumptions regarding heterogeneity. To the best of our knowledge, this is the first stochastic algorithm achieving these theoretical results under the heterogeneous setting. At last, the experimental results confirm the efficacy of our algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#38750;&#21442;&#25968;&#38544;&#24615;&#31867;&#21035;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#30340;&#28508;&#22312;&#31867;&#21035;&#19979;&#65292;&#22240;&#26524;&#21457;&#29616;&#20173;&#28982;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;</title><link>https://arxiv.org/abs/2311.07454</link><description>&lt;p&gt;
&#31163;&#25955;&#38750;&#21442;&#25968;&#38544;&#24615;&#31867;&#21035;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Discrete Nonparametric Causal Discovery Under Latent Class Confounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#38750;&#21442;&#25968;&#38544;&#24615;&#31867;&#21035;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#30340;&#28508;&#22312;&#31867;&#21035;&#19979;&#65292;&#22240;&#26524;&#21457;&#29616;&#20173;&#28982;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#21521;&#26080;&#29615;&#22270;&#29992;&#20110;&#24314;&#27169;&#31995;&#32479;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;"&#22240;&#26524;&#21457;&#29616;"&#25551;&#36848;&#20102;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36825;&#31181;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;&#24403;&#25968;&#25454;&#26159;&#26469;&#33258;&#22810;&#20010;&#28304;&#65288;&#32676;&#20307;&#25110;&#29615;&#22659;&#65289;&#30340;&#32858;&#21512;&#29289;&#26102;&#65292;&#20840;&#23616;&#28151;&#28102;&#20351;&#39537;&#21160;&#35768;&#22810;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#29305;&#24615;&#21464;&#24471;&#27169;&#31946;&#12290;&#36825;&#31181;&#24773;&#20917;&#26377;&#26102;&#34987;&#31216;&#20026;&#28151;&#21512;&#27169;&#22411;&#25110;&#28508;&#22312;&#31867;&#21035;&#12290;&#34429;&#28982;&#19968;&#20123;&#29616;&#20195;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#33021;&#22815;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#22788;&#29702;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#65292;&#20294;&#26159;&#30446;&#21069;&#25152;&#30693;&#30340;&#22788;&#29702;&#20840;&#23616;&#28151;&#28102;&#30340;&#26041;&#27861;&#37117;&#28041;&#21450;&#19981;&#36866;&#29992;&#20110;&#31163;&#25955;&#20998;&#24067;&#30340;&#21442;&#25968;&#20551;&#35774;&#12290;&#20197;&#31163;&#25955;&#21644;&#38750;&#21442;&#25968;&#35266;&#23519;&#21464;&#37327;&#20026;&#37325;&#28857;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#30340;&#28508;&#22312;&#31867;&#21035;&#19979;&#65292;&#22240;&#26524;&#21457;&#29616;&#20173;&#28982;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#30001;&#20840;&#23616;&#28151;&#28102;&#30340;&#22522;&#25968;&#12289;&#35266;&#23519;&#21464;&#37327;&#30340;&#22522;&#25968;&#31561;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07454v2 Announce Type: replace Abstract: Directed acyclic graphs are used to model the causal structure of a system. ``Causal discovery'' describes the problem of learning this structure from data. When data is an aggregate from multiple sources (populations or environments), global confounding obscures conditional independence properties that drive many causal discovery algorithms. This setting is sometimes known as a mixture model or a latent class. While some modern methods for causal discovery are able to work around unobserved confounding in specific cases, the only known ways to deal with a global confounder involve parametric assumptions. that are unsuitable for discrete distributions.Focusing on discrete and non-parametric observed variables, we demonstrate that causal discovery can still be identifiable under bounded latent classes. The feasibility of this problem is governed by a trade-off between the cardinality of the global confounder, the cardinalities of the o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28508;&#31354;&#38388;&#25805;&#25511;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#21521;&#37327;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#36981;&#24490;&#31034;&#20363;&#28436;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#21521;&#37327;&#30340;&#37327;&#32423;&#26469;&#36731;&#26494;&#25511;&#21046;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2311.06668</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#21521;&#37327;&#20013;&#65292;&#36890;&#36807;&#28508;&#31354;&#38388;&#25805;&#25511;&#20351;&#19978;&#19979;&#25991;&#23398;&#20064;&#26356;&#26377;&#25928;&#21644;&#21487;&#25511;
&lt;/p&gt;
&lt;p&gt;
In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06668
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28508;&#31354;&#38388;&#25805;&#25511;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#21521;&#37327;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#36981;&#24490;&#31034;&#20363;&#28436;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#21521;&#37327;&#30340;&#37327;&#32423;&#26469;&#36731;&#26494;&#25511;&#21046;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#26032;&#20219;&#21153;&#36866;&#24212;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#31034;&#20363;&#28436;&#31034;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;&#26377;&#38480;&#65292;&#38590;&#20197;&#23450;&#37327;&#25511;&#21046;&#65292;&#24182;&#21344;&#29992;&#19978;&#19979;&#25991;&#31383;&#21475;&#31354;&#38388;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#37325;&#26032;&#23450;&#20041;&#20026;&#19978;&#19979;&#25991;&#21521;&#37327;&#65288;ICV&#65289;&#12290;&#20351;&#29992;ICV&#26377;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#28436;&#31034;&#31034;&#20363;&#36827;&#34892;&#27491;&#21521;&#20256;&#36882;&#65292;&#20174;LLM&#30340;&#28508;&#22312;&#23884;&#20837;&#20013;&#21019;&#24314;&#19978;&#19979;&#25991;&#21521;&#37327;&#12290;&#36825;&#20010;&#21521;&#37327;&#25429;&#25417;&#20102;&#20851;&#20110;&#39044;&#26399;&#20219;&#21153;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#23545;&#20110;&#19968;&#20010;&#26032;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#19981;&#26159;&#23558;&#31034;&#20363;&#28155;&#21152;&#21040;&#25552;&#31034;&#20013;&#65292;&#32780;&#26159;&#20351;&#29992;ICV&#26469;&#25913;&#21464;LLM&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;ICV&#26041;&#27861;&#26377;&#20960;&#20010;&#22909;&#22788;&#65306;1&#65289;&#23427;&#20351;LLM&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#36981;&#24490;&#31034;&#20363;&#28436;&#31034;&#65307;2&#65289;&#36890;&#36807;&#35843;&#25972;ICV&#30340;&#37327;&#32423;&#65292;&#23427;&#26131;&#20110;&#25511;&#21046;&#65307;3&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06668v3 Announce Type: replace-cross Abstract: Large language models (LLMs) demonstrate emergent in-context learning capabilities, where they adapt to new tasks based on example demonstrations. However, in-context learning has seen limited effectiveness in many settings, is difficult to quantitatively control and takes up context window space. To overcome these limitations, we propose an alternative approach that recasts in-context learning as in-context vectors (ICV). Using ICV has two steps. We first use a forward pass on demonstration examples to create the in-context vector from the latent embedding of the LLM. This vector captures essential information about the intended task. On a new query, instead of adding demonstrations to the prompt, we shift the latent states of the LLM using the ICV. The ICV approach has several benefits: 1) it enables the LLM to more effectively follow the demonstration examples; 2) it's easy to control by adjusting the magnitude of the ICV; 3)
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#31639;&#27861;&#65288;CDNMF&#65289;&#36890;&#36807;&#21152;&#28145;NMF&#30340;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#24605;&#24819;&#26500;&#24314;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#20316;&#20026;&#23545;&#27604;&#35270;&#22270;&#65292;&#24182;&#21033;&#29992;&#21435;&#20559;&#26041;&#27861;&#26469;&#20248;&#21270;&#31038;&#21306;&#25506;&#27979;&#30340;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2311.02357</link><description>&lt;p&gt;
&#23545;&#27604;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#29992;&#20110;&#31038;&#21306;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Contrastive Deep Nonnegative Matrix Factorization for Community Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02357
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#31639;&#27861;&#65288;CDNMF&#65289;&#36890;&#36807;&#21152;&#28145;NMF&#30340;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#24605;&#24819;&#26500;&#24314;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#20316;&#20026;&#23545;&#27604;&#35270;&#22270;&#65292;&#24182;&#21033;&#29992;&#21435;&#20559;&#26041;&#27861;&#26469;&#20248;&#21270;&#31038;&#21306;&#25506;&#27979;&#30340;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31038;&#21306;&#21457;&#29616;&#65292;&#22240;&#20026;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;NMF&#30340;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#19977;&#20010;&#38382;&#39064;&#65306;1&#65289;&#23427;&#20204;&#30452;&#25509;&#23558;&#21407;&#22987;&#32593;&#32476;&#36716;&#25442;&#20026;&#31038;&#21306;&#25104;&#21592;&#31354;&#38388;&#65292;&#22240;&#27492;&#24456;&#38590;&#25429;&#25417;&#23618;&#27425;&#20449;&#24687;&#65307;2&#65289;&#23427;&#20204;&#36890;&#24120;&#21482;&#20851;&#27880;&#32593;&#32476;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24573;&#35270;&#20102;&#33410;&#28857;&#23646;&#24615;&#65307;3&#65289;&#23427;&#20204;&#24456;&#38590;&#23398;&#20064;&#21040;&#31038;&#21306;&#21457;&#29616;&#25152;&#38656;&#30340;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31038;&#21306;&#21457;&#29616;&#31639;&#27861;&#65292;&#21517;&#20026;&#23545;&#27604;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;CDNMF&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28145;&#21270;NMF&#20197;&#22686;&#24378;&#20854;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#12290;&#38543;&#21518;&#65292;&#21463;&#21040;&#23545;&#27604;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21019;&#36896;&#24615;&#22320;&#23558;&#32593;&#32476;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#26500;&#24314;&#20026;&#20004;&#31181;&#23545;&#27604;&#35270;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21435;&#20559;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02357v2 Announce Type: replace-cross Abstract: Recently, nonnegative matrix factorization (NMF) has been widely adopted for community detection, because of its better interpretability. However, the existing NMF-based methods have the following three problems: 1) they directly transform the original network into community membership space, so it is difficult for them to capture the hierarchical information; 2) they often only pay attention to the topology of the network and ignore its node attributes; 3) it is hard for them to learn the global structure information necessary for community detection. Therefore, we propose a new community detection algorithm, named Contrastive Deep Nonnegative Matrix Factorization (CDNMF). Firstly, we deepen NMF to strengthen its capacity for information extraction. Subsequently, inspired by contrastive learning, our algorithm creatively constructs network topology and node attributes as two contrasting views. Furthermore, we utilize a debiased
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#33410;&#32423;&#31934;&#24230;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;DNA&#30340;&#33258;&#28982;&#35821;&#35328;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#23383;&#33410;&#32423;&#31934;&#24230;&#19978;&#20998;&#26512;DNA&#24207;&#21015;&#65292;&#20351;&#24471;&#33021;&#22815;&#29992;&#20110;&#35782;&#21035;DNA&#24207;&#21015;&#20013;&#30340;&#21508;&#31181;&#21151;&#33021;&#21644;&#21464;&#24322;&#12290;</title><link>https://arxiv.org/abs/2311.02333</link><description>&lt;p&gt;
&#20351;&#29992;&#23383;&#33410;&#32423;&#31934;&#30830;&#24230;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#22522;&#20110;Transformer&#27169;&#22411;&#29702;&#35299;DNA&#30340;&#33258;&#28982;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#33410;&#32423;&#31934;&#24230;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;DNA&#30340;&#33258;&#28982;&#35821;&#35328;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#23383;&#33410;&#32423;&#31934;&#24230;&#19978;&#20998;&#26512;DNA&#24207;&#21015;&#65292;&#20351;&#24471;&#33021;&#22815;&#29992;&#20110;&#35782;&#21035;DNA&#24207;&#21015;&#20013;&#30340;&#21508;&#31181;&#21151;&#33021;&#21644;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#33410;&#32423;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26550;&#26500;&#30340;&#38598;&#21512;&#26680;&#33527;&#37240;&#23383;&#33410;&#32423;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;(ENBED)&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#23383;&#33410;&#32423;&#31934;&#24230;&#19978;&#20998;&#26512;DNA&#24207;&#21015;&#12290;ENBED&#20351;&#29992;&#27425;&#20108;&#27425;&#30340;&#27880;&#24847;&#21147;&#23454;&#29616;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#36827;&#34892;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#36716;&#25442;&#65292;&#27867;&#21270;&#20808;&#21069;&#22522;&#22240;&#32452;&#27169;&#22411;&#21482;&#37319;&#29992;&#32534;&#30721;&#22120;&#25110;&#32773;&#35299;&#30721;&#22120;&#20307;&#31995;&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#20351;&#29992;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#26469;&#39044;&#35757;&#32451;&#36825;&#20010;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#21442;&#32771;&#22522;&#22240;&#32452;&#24207;&#21015;&#24182;&#23558;&#20854;&#24212;&#29992;&#21040;&#20197;&#19979;&#19979;&#28216;&#20219;&#21153;&#19978;&#65306;(1)&#35782;&#21035;&#22686;&#24378;&#23376;&#12289;&#21551;&#21160;&#23376;&#21644;&#21098;&#20999;&#20301;&#28857;&#65292;(2)&#35782;&#21035;&#21253;&#21547;&#30897;&#22522;&#35843;&#29992;&#19981;&#21305;&#37197;&#21644;&#25554;&#20837;/&#32570;&#22833;&#38169;&#35823;&#30340;&#24207;&#21015;&#65292;&#36825;&#26159;&#23545;&#22810;&#20010;&#30897;&#22522;&#23545;&#36827;&#34892;&#26631;&#35760;&#21270;&#30340;&#26041;&#26696;&#30340;&#20248;&#21183;&#65292;&#20002;&#22833;&#20102;&#23383;&#33410;&#32423;&#31934;&#24230;&#20998;&#26512;&#30340;&#33021;&#21147;&#65292;(3)&#35782;&#21035;&#22522;&#22240;&#32452;&#24207;&#21015;&#30340;&#29983;&#29289;&#21151;&#33021;&#27880;&#37322;&#65292;&#20197;&#21450;(4)&#29983;&#25104;&#31361;&#21464;&#22522;&#22240;&#32452;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02333v2 Announce Type: replace Abstract: This paper presents the Ensemble Nucleotide Byte-level Encoder-Decoder (ENBED) foundation model, analyzing DNA sequences at byte-level precision with an encoder-decoder Transformer architecture. ENBED uses a sub-quadratic implementation of attention to develop an efficient model capable of sequence-to-sequence transformations, generalizing previous genomic models with encoder-only or decoder-only architectures. We use Masked Language Modeling to pre-train the foundation model using reference genome sequences and apply it in the following downstream tasks: (1) identification of enhancers, promotors and splice sites, (2) recognition of sequences containing base call mismatches and insertion/deletion errors, an advantage over tokenization schemes involving multiple base pairs, which lose the ability to analyze with byte-level precision, (3) identification of biological function annotations of genomic sequences, and (4) generating mutatio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#23567;&#21270;&#20241;&#30496;&#27604;&#29575;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#24403;&#21069;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#26234;&#33021;&#20307;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#23637;&#29616;&#20986;&#25345;&#32493;&#30340;&#19981;&#27963;&#21160;&#29366;&#24577;&#65292;&#38480;&#21046;&#20102;&#20854;&#26377;&#25928;&#25506;&#32034;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#20241;&#30496;&#27604;&#29575;&#20316;&#20026;&#24230;&#37327;&#25351;&#26631;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2310.19668</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#21270;&#20241;&#30496;&#27604;&#29575;&#25484;&#25569;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19668
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#21270;&#20241;&#30496;&#27604;&#29575;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#24403;&#21069;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#26234;&#33021;&#20307;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#23637;&#29616;&#20986;&#25345;&#32493;&#30340;&#19981;&#27963;&#21160;&#29366;&#24577;&#65292;&#38480;&#21046;&#20102;&#20854;&#26377;&#25928;&#25506;&#32034;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#20241;&#30496;&#27604;&#29575;&#20316;&#20026;&#24230;&#37327;&#25351;&#26631;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#28176;&#36817;&#24615;&#33021;&#21644;&#23545;&#38543;&#26426;&#31181;&#23376;&#36873;&#25321;&#30340;&#40065;&#26834;&#24615;&#31561;&#26041;&#38754;&#20173;&#28982;&#19981;&#23613;&#20154;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#29616;&#26377;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#65292;&#21363;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#26234;&#33021;&#20307;&#32463;&#24120;&#23637;&#29616;&#20986;&#25345;&#32493;&#30340;&#19981;&#27963;&#21160;&#29366;&#24577;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#26377;&#25928;&#25506;&#32034;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#20851;&#38190;&#35266;&#23519;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#26234;&#33021;&#20307;&#20542;&#21521;&#20110;&#36816;&#21160;&#19981;&#27963;&#36291;&#25506;&#32034;&#19982;&#20854;&#31574;&#30053;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#27963;&#21160;&#32570;&#22833;&#20043;&#38388;&#30340;&#26174;&#33879;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#31181;&#19981;&#27963;&#36291;&#29366;&#24577;&#65292;&#25105;&#20204;&#37319;&#29992;&#20241;&#30496;&#27604;&#29575;&#20316;&#20026;&#34913;&#37327;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#19981;&#27963;&#36291;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#36824;&#35748;&#35782;&#21040;&#20241;&#30496;&#27604;&#29575;&#21487;&#20197;&#20316;&#20026;&#26234;&#33021;&#20307;&#21305;&#37197;&#33021;&#21147;&#30340;&#29420;&#31435;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19668v2 Announce Type: replace Abstract: Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent'
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AttributionLab&#65292;&#19968;&#20010;&#21487;&#25511;&#29615;&#22659;&#19979;&#27979;&#35797;&#29305;&#24449;&#24402;&#22240;&#24544;&#23454;&#24615;&#30340;&#23454;&#39564;&#23460;&#12290;&#36890;&#36807;&#22312;&#35774;&#35745;&#25968;&#25454;&#19978;&#25311;&#21512;&#27169;&#22411;&#24182;&#19982;&#30495;&#23454;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#23454;&#39564;&#23460;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#24402;&#22240;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2310.06514</link><description>&lt;p&gt;
AttributionLab:&#22312;&#21487;&#25511;&#29615;&#22659;&#19979;&#30340;&#29305;&#24449;&#24402;&#22240;&#24544;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
AttributionLab: Faithfulness of Feature Attribution Under Controllable Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AttributionLab&#65292;&#19968;&#20010;&#21487;&#25511;&#29615;&#22659;&#19979;&#27979;&#35797;&#29305;&#24449;&#24402;&#22240;&#24544;&#23454;&#24615;&#30340;&#23454;&#39564;&#23460;&#12290;&#36890;&#36807;&#22312;&#35774;&#35745;&#25968;&#25454;&#19978;&#25311;&#21512;&#27169;&#22411;&#24182;&#19982;&#30495;&#23454;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#23454;&#39564;&#23460;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#24402;&#22240;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#36890;&#36807;&#35782;&#21035;&#30456;&#20851;&#30340;&#36755;&#20837;&#29305;&#24449;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#24402;&#22240;&#24517;&#39035;&#26159;&#24544;&#23454;&#30340;&#65292;&#24847;&#21619;&#30528;&#34987;&#24402;&#22240;&#30340;&#29305;&#24449;&#24517;&#39035;&#21453;&#26144;&#24433;&#21709;&#36755;&#20986;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#36235;&#21183;&#26159;&#36890;&#36807;&#22312;&#35774;&#35745;&#25968;&#25454;&#19978;&#25311;&#21512;&#27169;&#22411;&#24182;&#23558;&#24402;&#22240;&#19982;&#30495;&#23454;&#30340;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#27604;&#36739;&#26469;&#27979;&#35797;&#24544;&#23454;&#24615;&#12290;&#36825;&#20010;&#24819;&#27861;&#20551;&#35774;&#27169;&#22411;&#23398;&#20250;&#20102;&#20165;&#20351;&#29992;&#36825;&#20123;&#35774;&#35745;&#29305;&#24449;&#65292;&#20294;&#24182;&#27809;&#26377;&#20445;&#35777;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#32593;&#32476;&#24182;&#25163;&#21160;&#35774;&#32622;&#20854;&#26435;&#37325;&#20197;&#21450;&#35774;&#35745;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20010;&#35774;&#23450;&#31216;&#20026;&#8220;AttributionLab&#8221;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#24544;&#23454;&#24615;&#30340;&#21512;&#29702;&#24615;&#26816;&#26597;&#65306;&#22914;&#26524;&#19968;&#20010;&#24402;&#22240;&#26041;&#27861;&#22312;&#19968;&#20010;&#21487;&#25511;&#29615;&#22659;&#20013;&#24182;&#19981;&#24544;&#23454;&#65292;&#37027;&#20040;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#36825;&#20010;&#29615;&#22659;&#20063;&#26159;&#19968;&#20010;&#29992;&#20110;&#25511;&#21046;&#23454;&#39564;&#30340;&#23454;&#39564;&#23460;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#36825;&#20123;&#23454;&#39564;&#26469;&#20998;&#26512;&#24402;&#22240;&#26041;&#27861;&#24182;&#25552;&#20986;&#25913;&#36827;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06514v2 Announce Type: replace Abstract: Feature attribution explains neural network outputs by identifying relevant input features. The attribution has to be faithful, meaning that the attributed features must mirror the input features that influence the output. One recent trend to test faithfulness is to fit a model on designed data with known relevant features and then compare attributions with ground truth input features.This idea assumes that the model learns to use all and only these designed features, for which there is no guarantee. In this paper, we solve this issue by designing the network and manually setting its weights, along with designing data. The setup, AttributionLab, serves as a sanity check for faithfulness: If an attribution method is not faithful in a controlled environment, it can be unreliable in the wild. The environment is also a laboratory for controlled experiments by which we can analyze attribution methods and suggest improvements.
&lt;/p&gt;</description></item><item><title>Hyp-OW&#26159;&#19968;&#31181;&#21033;&#29992;&#36229;&#20960;&#20309;&#36317;&#31163;&#30340;&#23618;&#27425;&#32467;&#26500;&#23398;&#20064;&#22686;&#24378;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36229;&#31867;&#27491;&#21017;&#21270;&#22120;&#23398;&#20064;&#21644;&#24314;&#27169;&#24050;&#30693;&#39033;&#30446;&#30340;&#23618;&#27425;&#34920;&#31034;&#65292;&#36890;&#36807;&#22522;&#20110;&#30456;&#20284;&#24230;&#36317;&#31163;&#30340;&#37325;&#26032;&#26631;&#35760;&#27169;&#22359;&#26377;&#25928;&#22320;&#26816;&#27979;&#26410;&#30693;&#23545;&#35937;&#12290;</title><link>https://arxiv.org/abs/2306.14291</link><description>&lt;p&gt;
Hyp-OW: &#21033;&#29992;&#36229;&#20960;&#20309;&#36317;&#31163;&#30340;&#23618;&#27425;&#32467;&#26500;&#23398;&#20064;&#22686;&#24378;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic Distance Enhances Open World Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.14291
&lt;/p&gt;
&lt;p&gt;
Hyp-OW&#26159;&#19968;&#31181;&#21033;&#29992;&#36229;&#20960;&#20309;&#36317;&#31163;&#30340;&#23618;&#27425;&#32467;&#26500;&#23398;&#20064;&#22686;&#24378;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36229;&#31867;&#27491;&#21017;&#21270;&#22120;&#23398;&#20064;&#21644;&#24314;&#27169;&#24050;&#30693;&#39033;&#30446;&#30340;&#23618;&#27425;&#34920;&#31034;&#65292;&#36890;&#36807;&#22522;&#20110;&#30456;&#20284;&#24230;&#36317;&#31163;&#30340;&#37325;&#26032;&#26631;&#35760;&#27169;&#22359;&#26377;&#25928;&#22320;&#26816;&#27979;&#26410;&#30693;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;(OWOD)&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#29616;&#23454;&#30340;&#20219;&#21153;&#65292;&#36229;&#36234;&#20102;&#26631;&#20934;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#30340;&#33539;&#22260;&#12290;&#23427;&#38656;&#35201;&#22312;&#26816;&#27979;&#24050;&#30693;&#21644;&#26410;&#30693;&#23545;&#35937;&#30340;&#21516;&#26102;&#65292;&#25972;&#21512;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#29992;&#20110;&#26410;&#26469;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#8220;&#26410;&#30693;&#24615;&#8221;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#65292;&#26641;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#32972;&#26223;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#21487;&#33021;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#24212;&#35813;&#24050;&#32463;&#23884;&#20837;&#21040;&#24050;&#30693;&#31867;&#21035;&#20013;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#24050;&#30693;&#21644;&#26410;&#30693;&#39033;&#20043;&#38388;&#24212;&#35813;&#23384;&#22312;&#35821;&#20041;&#25110;&#28508;&#22312;&#30340;&#32467;&#26500;&#20851;&#31995;&#31561;&#24453;&#21457;&#29616;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hyp-OW&#65292;&#19968;&#31181;&#36890;&#36807;&#36229;&#31867;&#27491;&#21017;&#21270;&#22120;&#26469;&#23398;&#20064;&#21644;&#24314;&#27169;&#24050;&#30693;&#39033;&#30446;&#30340;&#23618;&#27425;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;&#36825;&#31181;&#34920;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#30456;&#20284;&#24230;&#36317;&#31163;&#30340;&#37325;&#26032;&#26631;&#35760;&#27169;&#22359;&#26377;&#25928;&#22320;&#26816;&#27979;&#26410;&#30693;&#23545;&#35937;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open World Object Detection (OWOD) is a challenging and realistic task that extends beyond the scope of standard Object Detection task. It involves detecting both known and unknown objects while integrating learned knowledge for future tasks. However, the level of "unknownness" varies significantly depending on the context. For example, a tree is typically considered part of the background in a self-driving scene, but it may be significant in a household context. We argue that this contextual information should already be embedded within the known classes. In other words, there should be a semantic or latent structure relationship between the known and unknown items to be discovered. Motivated by this observation, we propose Hyp-OW, a method that learns and models hierarchical representation of known items through a SuperClass Regularizer. Leveraging this representation allows us to effectively detect unknown objects using a similarity distance-based relabeling module. Extensive experi
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#32452;&#33258;&#28982;&#30340;&#12289;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;</title><link>https://arxiv.org/abs/2304.08460</link><description>&lt;p&gt;
LongForm: &#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LongForm: Effective Instruction Tuning with Reverse Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.08460
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#32452;&#33258;&#28982;&#30340;&#12289;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Instruction tuning&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#27867;&#21270;&#65292;&#24182;&#26356;&#22909;&#22320;&#36981;&#24490;&#29992;&#25143;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#25351;&#20196;&#25968;&#25454;&#25104;&#26412;&#39640;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#37319;&#29992;&#20102;&#35832;&#22914;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12289;&#23384;&#22312;&#23545;&#40784;&#38382;&#39064;&#30340;&#20247;&#21253;&#25968;&#25454;&#38598;&#12289;&#20197;&#21450;&#36890;&#36807;LLMs&#29983;&#25104;&#22122;&#22768;&#31034;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LongForm-C&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#21453;&#21521;&#25351;&#20196;&#21019;&#24314;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#20026;&#20154;&#31867;&#20889;&#20316;&#35821;&#26009;&#24211;&#31034;&#20363;&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#29983;&#25104;&#25351;&#20196;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35832;&#22914;C4&#21644;Wikipedia&#30340;&#35821;&#26009;&#24211;&#20013;&#36873;&#25321;&#20102;&#19968;&#32452;&#22810;&#26679;&#24615;&#30340;&#20154;&#31867;&#25776;&#20889;&#25991;&#26723;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#20026;&#36825;&#20123;&#25991;&#26723;&#29983;&#25104;&#25351;&#20196;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#20415;&#23452;&#12289;&#26356;&#24178;&#20928;&#12289;&#36755;&#20986;&#33258;&#28982;&#20197;&#21450;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.08460v2 Announce Type: replace-cross Abstract: Instruction tuning enables language models to more effectively generalize and better follow user intent. However, obtaining instruction data is costly and challenging. Prior work employs methods such as expensive human annotation, crowd-sourced datasets with alignment issues, and generating noisy examples via LLMs. We introduce the LongForm-C dataset, which is created by reverse instructions. We generate instructions via LLMs for human-written corpus examples using reverse instructions. First we select a diverse set of human-written documents from corpora such as C4 and Wikipedia; then we generate instructions for these documents via LLMs. This approach provides a cheaper and cleaner instruction-tuning dataset with natural output and one suitable for long text generation. Our models outperform 10x larger language models without instruction tuning on tasks such as story/recipe generation and long-form question answering. Moreover
&lt;/p&gt;</description></item><item><title>&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20851;&#20110;&#22240;&#26524;&#24615;&#30340;&#24605;&#32771;&#26041;&#24335;&#65292;&#23427;&#32508;&#21512;&#20102;&#21487;&#27979;&#35797;&#30340;&#22240;&#26524;&#30693;&#35782;&#12289;&#21442;&#25968;&#24418;&#24335;&#21644;&#26102;&#38388;&#32500;&#24230;&#65292;&#33021;&#22815;&#24110;&#21161;&#25105;&#20204;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2303.02186</link><description>&lt;p&gt;
&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.02186
&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20851;&#20110;&#22240;&#26524;&#24615;&#30340;&#24605;&#32771;&#26041;&#24335;&#65292;&#23427;&#32508;&#21512;&#20102;&#21487;&#27979;&#35797;&#30340;&#22240;&#26524;&#30693;&#35782;&#12289;&#21442;&#25968;&#24418;&#24335;&#21644;&#26102;&#38388;&#32500;&#24230;&#65292;&#33021;&#22815;&#24110;&#21161;&#25105;&#20204;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#24615;&#26377;&#26395;&#30495;&#27491;&#25913;&#21464;&#25105;&#20204;&#35299;&#20915;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#22240;&#26524;&#24615;&#30340;&#28508;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#21457;&#25381;&#65292;&#22240;&#20026;&#22240;&#26524;&#24615;&#24120;&#24120;&#38656;&#35201;&#26080;&#27861;&#22312;&#23454;&#36341;&#20013;&#36827;&#34892;&#27979;&#35797;&#30340;&#20851;&#38190;&#20551;&#35774;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#20851;&#20110;&#22240;&#26524;&#24615;&#30340;&#24605;&#32771;&#26041;&#24335;&#8212;&#8212;&#25105;&#20204;&#31216;&#20043;&#20026;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#28085;&#30422;&#20102;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#32467;&#26500;&#32500;&#24230;&#65292;&#23427;&#23558;&#37096;&#20998;&#21487;&#20197;&#27979;&#35797;&#30340;&#22240;&#26524;&#30693;&#35782;&#34701;&#20837;&#65292;&#32780;&#19981;&#26159;&#20551;&#35774;&#24863;&#20852;&#36259;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#30693;&#35782;&#26159;&#23436;&#25972;&#30340;&#25110;&#23436;&#20840;&#19981;&#23384;&#22312;&#30340;&#65307;&#65288;2&#65289;&#21442;&#25968;&#32500;&#24230;&#65292;&#23427;&#28085;&#30422;&#20102;&#25429;&#25417;&#24863;&#20852;&#36259;&#30340;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#31867;&#22411;&#30340;&#21442;&#25968;&#24418;&#24335;&#65307;&#65288;3&#65289;&#26102;&#38388;&#32500;&#24230;&#65292;&#23427;&#25429;&#25417;&#24863;&#20852;&#36259;&#30340;&#21464;&#37327;&#22914;&#20309;&#22312;&#26102;&#38388;&#19978;&#30456;&#20114;&#20316;&#29992;&#65288;&#21487;&#33021;&#26377;&#22240;&#26524;&#20851;&#31995;&#65289;&#12290;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#21508;&#31181;&#23454;&#38469;&#38382;&#39064;&#19978;&#21462;&#24471;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.02186v2 Announce Type: replace-cross Abstract: Causality has the potential to truly transform the way we solve a large number of real-world problems. Yet, so far, its potential largely remains to be unlocked as causality often requires crucial assumptions which cannot be tested in practice. To address this challenge, we propose a new way of thinking about causality -- we call this causal deep learning. Our causal deep learning framework spans three dimensions: (1) a structural dimension, which incorporates partial yet testable causal knowledge rather than assuming either complete or no causal knowledge among the variables of interest; (2) a parametric dimension, which encompasses parametric forms that capture the type of relationships among the variables of interest; and (3) a temporal dimension, which captures exposure times or how the variables of interest interact (possibly causally) over time. Causal deep learning enables us to make progress on a variety of real-world pr
&lt;/p&gt;</description></item><item><title>&#26102;&#24207;&#24046;&#24322;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#36830;&#32493;&#26102;&#38388;&#27493;&#39588;&#20013;&#30340;&#20272;&#35745;&#26102;&#24207;&#19981;&#19968;&#33268;&#24230;&#26469;&#25311;&#21512;&#20540;&#20989;&#25968;&#65292;&#20855;&#26377;&#32479;&#35745;&#20248;&#21183;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20540;&#20272;&#35745;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20004;&#20010;&#29366;&#24577;&#30340;&#20540;&#24046;&#20272;&#35745;&#20013;&#33719;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2301.13289</link><description>&lt;p&gt;
&#20851;&#20110;&#26102;&#24207;&#24046;&#24322;&#23398;&#20064;&#30340;&#32479;&#35745;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
On the Statistical Benefits of Temporal Difference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13289
&lt;/p&gt;
&lt;p&gt;
&#26102;&#24207;&#24046;&#24322;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#36830;&#32493;&#26102;&#38388;&#27493;&#39588;&#20013;&#30340;&#20272;&#35745;&#26102;&#24207;&#19981;&#19968;&#33268;&#24230;&#26469;&#25311;&#21512;&#20540;&#20989;&#25968;&#65292;&#20855;&#26377;&#32479;&#35745;&#20248;&#21183;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20540;&#20272;&#35745;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20004;&#20010;&#29366;&#24577;&#30340;&#20540;&#24046;&#20272;&#35745;&#20013;&#33719;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#20851;&#20110;&#21160;&#20316;&#21644;&#38271;&#26399;&#22870;&#21169;&#30340;&#25968;&#25454;&#38598;&#65292;&#30452;&#25509;&#20272;&#35745;&#26041;&#27861;&#36890;&#36807;&#23558;&#20540;&#20989;&#25968;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#39044;&#27979;&#35823;&#24046;&#26368;&#23567;&#21270;&#26469;&#25311;&#21512;&#12290;&#32780;&#26102;&#24207;&#24046;&#24322;&#23398;&#20064;(TD)&#26041;&#27861;&#21017;&#36890;&#36807;&#26368;&#23567;&#21270;&#22312;&#36830;&#32493;&#26102;&#38388;&#27493;&#39588;&#20013;&#36827;&#34892;&#30340;&#20272;&#35745;&#20043;&#38388;&#30340;&#26102;&#24207;&#19981;&#19968;&#33268;&#31243;&#24230;&#26469;&#25311;&#21512;&#20540;&#20989;&#25968;&#12290;&#38024;&#23545;&#26377;&#38480;&#29366;&#24577;Markov&#38142;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#32479;&#35745;&#20248;&#21183;&#30340;&#28165;&#26224;&#28176;&#36827;&#29702;&#35770;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#36870;&#36712;&#36857;&#27719;&#38598;&#31995;&#25968;&#23436;&#20840;&#21051;&#30011;&#20102;&#20540;&#20272;&#35745;&#22343;&#26041;&#35823;&#24046;&#30340;&#30334;&#20998;&#27604;&#20943;&#23569;&#12290;&#26681;&#25454;&#38382;&#39064;&#32467;&#26500;&#30340;&#19981;&#21516;&#65292;&#36825;&#31181;&#20943;&#23569;&#21487;&#20197;&#26159;&#24040;&#22823;&#30340;&#25110;&#19981;&#23384;&#22312;&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#29366;&#24577;&#30340;&#20540;&#24046;&#20272;&#35745;&#21487;&#20197;&#26377;&#24040;&#22823;&#30340;&#25913;&#36827;&#65306;TD&#30340;&#35823;&#24046;&#21463;&#21040;&#38382;&#39064;&#36712;&#36857;&#20132;&#21449;&#26102;&#38388;&#30340;&#30028;&#38480;&#65292;&#32780;&#36825;&#20010;&#30028;&#38480;&#21487;&#33021;&#36828;&#23567;&#20110;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.13289v3 Announce Type: replace Abstract: Given a dataset on actions and resulting long-term rewards, a direct estimation approach fits value functions that minimize prediction error on the training data. Temporal difference learning (TD) methods instead fit value functions by minimizing the degree of temporal inconsistency between estimates made at successive time-steps. Focusing on finite state Markov chains, we provide a crisp asymptotic theory of the statistical advantages of this approach. First, we show that an intuitive inverse trajectory pooling coefficient completely characterizes the percent reduction in mean-squared error of value estimates. Depending on problem structure, the reduction could be enormous or nonexistent. Next, we prove that there can be dramatic improvements in estimates of the difference in value-to-go for two states: TD's errors are bounded in terms of a novel measure - the problem's trajectory crossing time - which can be much smaller than the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26377;&#38480;&#22823;&#23567;&#20915;&#31574;&#26641;&#22312;Markov&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#30340;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;OMDTs&#65306;&#26368;&#20248;MDP&#20915;&#31574;&#26641;&#12290;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30452;&#25509;&#26368;&#22823;&#21270;&#20915;&#31574;&#26641;&#30340;&#26399;&#26395;&#25240;&#25187;&#22238;&#25253;&#12290;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#34920;&#29616;&#20026;&#27425;&#20248;&#12290;</title><link>https://arxiv.org/abs/2301.13185</link><description>&lt;p&gt;
Markov&#20915;&#31574;&#36807;&#31243;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Optimal Decision Tree Policies for Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26377;&#38480;&#22823;&#23567;&#20915;&#31574;&#26641;&#22312;Markov&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#30340;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;OMDTs&#65306;&#26368;&#20248;MDP&#20915;&#31574;&#26641;&#12290;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30452;&#25509;&#26368;&#22823;&#21270;&#20915;&#31574;&#26641;&#30340;&#26399;&#26395;&#25240;&#25187;&#22238;&#25253;&#12290;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#34920;&#29616;&#20026;&#27425;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#35299;&#37322;&#24615;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23398;&#20064;&#36825;&#31181;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#20687;&#20915;&#31574;&#26641;&#21644;&#35268;&#21017;&#21015;&#34920;&#36825;&#26679;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31574;&#30053;&#65292;&#30001;&#20110;&#20854;&#19981;&#21487;&#24494;&#24615;&#65292;&#24456;&#38590;&#36827;&#34892;&#20248;&#21270;&#12290;&#23613;&#31649;&#29616;&#26377;&#25216;&#26415;&#21487;&#20197;&#23398;&#20064;&#21487;&#39564;&#35777;&#30340;&#20915;&#31574;&#26641;&#31574;&#30053;&#65292;&#20294;&#19981;&#33021;&#20445;&#35777;&#23398;&#20064;&#32773;&#29983;&#25104;&#30340;&#20915;&#31574;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#38480;&#22823;&#23567;&#20915;&#31574;&#26641;&#22312;Markov&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#30340;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;OMDTs&#65306;&#26368;&#20248;MDP&#20915;&#31574;&#26641;&#12290;&#32473;&#23450;&#29992;&#25143;&#23450;&#20041;&#30340;&#22823;&#23567;&#38480;&#21046;&#21644;MDP&#24418;&#24335;&#65292;OMDT&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30452;&#25509;&#26368;&#22823;&#21270;&#20915;&#31574;&#26641;&#30340;&#26399;&#26395;&#25240;&#25187;&#22238;&#25253;&#12290;&#36890;&#36807;&#20026;&#19981;&#21516;&#30340;MDP&#35757;&#32451;&#26368;&#20248;&#20915;&#31574;&#26641;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#30740;&#31350;&#20102;&#29616;&#26377;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#34920;&#29616;&#20026;&#27425;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.13185v2 Announce Type: replace Abstract: Interpretability of reinforcement learning policies is essential for many real-world tasks but learning such interpretable policies is a hard problem. Particularly rule-based policies such as decision trees and rules lists are difficult to optimize due to their non-differentiability. While existing techniques can learn verifiable decision tree policies there is no guarantee that the learners generate a decision that performs optimally. In this work, we study the optimization of size-limited decision trees for Markov Decision Processes (MPDs) and propose OMDTs: Optimal MDP Decision Trees. Given a user-defined size limit and MDP formulation OMDT directly maximizes the expected discounted return for the decision tree using Mixed-Integer Linear Programming. By training optimal decision tree policies for different MDPs we empirically study the optimality gap for existing imitation learning techniques and find that they perform sub-optimall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#35843;&#33410;&#30340;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#21644;&#36866;&#24212;&#31639;&#27861;&#65292;&#25361;&#25112;&#20102;&#23545;&#19987;&#23478;&#30340;&#20449;&#24515;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#36951;&#25022;&#30028;&#38480;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2301.07530</link><description>&lt;p&gt;
&#20048;&#35266;&#35843;&#33410;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimistically Tempered Online Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.07530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#35843;&#33410;&#30340;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#21644;&#36866;&#24212;&#31639;&#27861;&#65292;&#25361;&#25112;&#20102;&#23545;&#19987;&#23478;&#30340;&#20449;&#24515;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#36951;&#25022;&#30028;&#38480;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20048;&#35266;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#20197;&#21033;&#29992;&#19987;&#23478;&#24847;&#35265;&#65292;&#20551;&#35774;&#19987;&#23478;&#24847;&#35265;&#24635;&#26159;&#26377;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21487;&#20197;&#21512;&#29702;&#22320;&#23545;&#36825;&#20123;&#24847;&#35265;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#22312;&#32447;&#31639;&#27861;&#25552;&#20379;&#30340;&#23398;&#20064;&#20449;&#24687;&#30340;&#30456;&#20851;&#24615;&#25552;&#20986;&#36136;&#30097;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#23545;&#19987;&#23478;&#30340;&#20449;&#24515;&#20551;&#35774;&#65292;&#24182;&#24320;&#21457;&#20102;&#20048;&#35266;&#35843;&#33410;&#65288;OT&#65289;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#20197;&#21450;&#22312;&#32447;&#31639;&#27861;&#30340;OT&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#21160;&#24577;&#36951;&#25022;&#30028;&#38480;&#30340;&#31283;&#22266;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#26368;&#32456;&#39564;&#35777;&#20102;OT&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.07530v2 Announce Type: replace Abstract: Optimistic Online Learning algorithms have been developed to exploit expert advices, assumed optimistically to be always useful. However, it is legitimate to question the relevance of such advices \emph{w.r.t.} the learning information provided by gradient-based online algorithms. In this work, we challenge the confidence assumption on the expert and develop the \emph{optimistically tempered} (OT) online learning framework as well as OT adaptations of online algorithms. Our algorithms come with sound theoretical guarantees in the form of dynamic regret bounds, and we eventually provide experimental validation of the usefulness of the OT approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;$k$-means++&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#25509;&#36817;&#26368;&#20248;&#30340;&#36816;&#34892;&#26102;&#38388;&#20869;&#35299;&#20915;&#21021;&#22987;&#32858;&#31867;&#20013;&#24515;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2211.15118</link><description>&lt;p&gt;
&#19968;&#20010;&#26356;&#24555;&#30340;$k$-means++&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Faster $k$-means++ Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.15118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;$k$-means++&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#25509;&#36817;&#26368;&#20248;&#30340;&#36816;&#34892;&#26102;&#38388;&#20869;&#35299;&#20915;&#21021;&#22987;&#32858;&#31867;&#20013;&#24515;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$k$-means++&#26159;&#36873;&#25321;$k$-means&#32858;&#31867;&#31639;&#27861;&#21021;&#22987;&#32858;&#31867;&#20013;&#24515;&#30340;&#37325;&#35201;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21487;&#20197;&#20197;&#25509;&#36817;&#26368;&#20248;&#30340;&#36816;&#34892;&#26102;&#38388;&#35299;&#20915;$k$-means++&#38382;&#39064;&#12290;&#32473;&#23450;$\mathbb{R}^d$&#20013;&#30340;$n$&#20010;&#25968;&#25454;&#28857;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#38656;&#35201;$\widetilde{O}(k)$&#27425;&#36845;&#20195;&#65292;&#32780;&#27599;&#27425;&#36845;&#20195;&#38656;&#35201;$\widetilde{O}(nd k)$&#30340;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#24635;&#36816;&#34892;&#26102;&#38388;&#20026;$\widetilde{O}(n d k^2)$&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;\textsc{FastKmeans++}&#65292;&#23427;&#21482;&#38656;&#35201;$\widetilde{O}(nd + nk^2)$&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.15118v2 Announce Type: replace-cross Abstract: $k$-means++ is an important algorithm for choosing initial cluster centers for the $k$-means clustering algorithm. In this work, we present a new algorithm that can solve the $k$-means++ problem with nearly optimal running time. Given $n$ data points in $\mathbb{R}^d$, the current state-of-the-art algorithm runs in $\widetilde{O}(k )$ iterations, and each iteration takes $\widetilde{O}(nd k)$ time. The overall running time is thus $\widetilde{O}(n d k^2)$. We propose a new algorithm \textsc{FastKmeans++} that only takes in $\widetilde{O}(nd + nk^2)$ time, in total.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#39640;&#32500;&#38750;&#23450;&#21521;&#22270;&#27169;&#22411;&#20013;&#22788;&#29702;&#20219;&#24847;&#28151;&#21512;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#28508;&#21464;&#37327;&#39640;&#26031;copula&#26694;&#26550;&#20013;&#24212;&#29992;&#32463;&#20856;&#30340;&#22810;&#39033;&#21644;&#22810;&#24207;&#30456;&#20851;&#30340;&#24605;&#24819;&#12290;</title><link>https://arxiv.org/abs/2211.11700</link><description>&lt;p&gt;
&#39640;&#32500;&#38750;&#23450;&#21521;&#22270;&#27169;&#22411;&#20013;&#30340;&#20219;&#24847;&#28151;&#21512;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
High-Dimensional Undirected Graphical Models for Arbitrary Mixed Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.11700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#39640;&#32500;&#38750;&#23450;&#21521;&#22270;&#27169;&#22411;&#20013;&#22788;&#29702;&#20219;&#24847;&#28151;&#21512;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#28508;&#21464;&#37327;&#39640;&#26031;copula&#26694;&#26550;&#20013;&#24212;&#29992;&#32463;&#20856;&#30340;&#22810;&#39033;&#21644;&#22810;&#24207;&#30456;&#20851;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#27169;&#22411;&#26159;&#25506;&#32034;&#22797;&#26434;&#22810;&#21464;&#37327;&#25968;&#25454;&#20013;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#23398;&#20064;&#36825;&#20123;&#22270;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#21464;&#37327;&#37117;&#26159;&#36830;&#32493;&#25110;&#31163;&#25955;&#30340;&#24773;&#20917;&#19979;&#24050;&#32463;&#24456;&#25104;&#29087;&#65292;&#21253;&#25324;&#39640;&#32500;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#30340;&#21464;&#37327;&#65288;&#20363;&#22914;&#36830;&#32493;&#12289;&#35745;&#25968;&#12289;&#20108;&#20540;&#12289;&#26377;&#24207;&#31561;&#65289;&#65292;&#20854;&#32852;&#21512;&#20998;&#26512;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36827;&#23637;&#24050;&#32463;&#23637;&#31034;&#20102;&#22914;&#20309;&#22788;&#29702;&#20108;&#20540;-&#36830;&#32493;&#24773;&#20917;&#65292;&#20294;&#26159;&#19968;&#33324;&#30340;&#28151;&#21512;&#21464;&#37327;&#31867;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31616;&#21333;&#32780;&#26377;&#29992;&#22320;&#35266;&#23519;&#21040;&#65292;&#20851;&#20110;&#22810;&#39033;&#19982;&#22810;&#24207;&#30456;&#20851;&#30340;&#32463;&#20856;&#24605;&#24819;&#21487;&#20197;&#22312;&#28508;&#21464;&#37327;&#39640;&#26031;copula&#26694;&#26550;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.11700v2 Announce Type: replace-cross Abstract: Graphical models are an important tool in exploring relationships between variables in complex, multivariate data. Methods for learning such graphical models are well developed in the case where all variables are either continuous or discrete, including in high-dimensions. However, in many applications data span variables of different types (e.g. continuous, count, binary, ordinal, etc.), whose principled joint analysis is nontrivial. Latent Gaussian copula models, in which all variables are modeled as transformations of underlying jointly Gaussian variables, represent a useful approach. Recent advances have shown how the binary-continuous case can be tackled, but the general mixed variable type regime remains challenging. In this work, we make the simple yet useful observation that classical ideas concerning polychoric and polyserial correlations can be leveraged in a latent Gaussian copula framework. Building on this observati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25351;&#23548;&#19979;&#30340;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#25913;&#36827;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#37319;&#29992;&#22270;&#34920;&#31034;&#27861;&#26469;&#32534;&#30721;&#23436;&#25972;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#26041;&#26696;&#21644;&#26032;&#39062;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21152;&#24555;&#35299;&#20915;&#26041;&#26696;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2211.10936</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25351;&#23548;&#19979;&#30340;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#25913;&#36827;&#21551;&#21457;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning Guided Improvement Heuristic for Job Shop Scheduling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.10936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25351;&#23548;&#19979;&#30340;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#25913;&#36827;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#37319;&#29992;&#22270;&#34920;&#31034;&#27861;&#26469;&#32534;&#30721;&#23436;&#25972;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#26041;&#26696;&#21644;&#26032;&#39062;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21152;&#24555;&#35299;&#20915;&#26041;&#26696;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24212;&#29992;&#20110;&#35299;&#20915;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#26102;&#65292;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#26500;&#24314;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#24615;&#33021;&#20173;&#36828;&#31163;&#26368;&#20248;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#24213;&#23618;&#22270;&#34920;&#31034;&#26041;&#26696;&#19981;&#36866;&#21512;&#23545;&#27599;&#20010;&#26500;&#24314;&#27493;&#39588;&#20013;&#30340;&#37096;&#20998;&#35299;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DRL&#25351;&#23548;&#19979;&#30340;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#25913;&#36827;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#37319;&#29992;&#22270;&#34920;&#31034;&#27861;&#26469;&#32534;&#30721;&#23436;&#25972;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#26041;&#26696;&#65292;&#21253;&#21547;&#20004;&#20010;&#27169;&#22359;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#25913;&#36827;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#21160;&#24577;&#25299;&#25169;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#22312;&#25913;&#36827;&#36807;&#31243;&#20013;&#21152;&#24555;&#35299;&#20915;&#26041;&#26696;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#21487;&#20197;&#21516;&#26102;&#35780;&#20272;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#19982;&#38382;&#39064;&#35268;&#27169;&#21576;&#32447;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.10936v3 Announce Type: replace-cross Abstract: Recent studies in using deep reinforcement learning (DRL) to solve Job-shop scheduling problems (JSSP) focus on construction heuristics. However, their performance is still far from optimality, mainly because the underlying graph representation scheme is unsuitable for modelling partial solutions at each construction step. This paper proposes a novel DRL-guided improvement heuristic for solving JSSP, where graph representation is employed to encode complete solutions. We design a Graph Neural-Network-based representation scheme, consisting of two modules to effectively capture the information of dynamic topology and different types of nodes in graphs encountered during the improvement process. To speed up solution evaluation during improvement, we present a novel message-passing mechanism that can evaluate multiple solutions simultaneously. We prove that the computational complexity of our method scales linearly with problem siz
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#32622;&#25442;&#31561;&#21464;&#37327;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#31181;&#26550;&#26500;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479; QNNs &#36935;&#21040;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#36139;&#30240;&#30340;&#39640;&#21407;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#22312;&#23569;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2210.09974</link><description>&lt;p&gt;
&#12298;&#32622;&#25442;&#31561;&#21464;&#37327;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#20445;&#35777;&#12299;
&lt;/p&gt;
&lt;p&gt;
Theoretical Guarantees for Permutation-Equivariant Quantum Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.09974
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#32622;&#25442;&#31561;&#21464;&#37327;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#31181;&#26550;&#26500;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479; QNNs &#36935;&#21040;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#36139;&#30240;&#30340;&#39640;&#21407;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#22312;&#23569;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#37322;&#25918;&#20854;&#20840;&#37096;&#28508;&#21147;&#20043;&#21069;&#65292;&#25105;&#20204;&#24517;&#39035;&#20811;&#26381;&#19968;&#20123;&#25361;&#25112;&#12290;&#20363;&#22914;&#65292;&#22522;&#20110;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#30340;&#27169;&#22411;&#21487;&#33021;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#36807;&#22810;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#36139;&#30240;&#30340;&#39640;&#21407;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#20960;&#20309;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;GQML&#65289;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;GQML &#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#25105;&#20204;&#24212;&#35813;&#35774;&#35745;&#32534;&#30721;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#30340;&#26550;&#26500;&#65292;&#20363;&#22914;&#31561;&#21464; QNNs&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20855;&#26377;&#32622;&#25442;&#23545;&#31216;&#24615;&#65288;&#21363;&#23545;&#31216;&#32676; $S_n$&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#26500;&#24314; $S_n$-equivariant QNNs&#12290;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#30740;&#31350;&#65292;&#35777;&#26126;&#23427;&#20204;&#19981;&#20250;&#36973;&#36935;&#36139;&#30240;&#30340;&#39640;&#21407;&#38382;&#39064;&#65292;&#33021;&#22815;&#24555;&#36895;&#23454;&#29616;&#36807;&#21442;&#25968;&#21270;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#20223;&#30495;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.09974v3 Announce Type: replace-cross Abstract: Despite the great promise of quantum machine learning models, there are several challenges one must overcome before unlocking their full potential. For instance, models based on quantum neural networks (QNNs) can suffer from excessive local minima and barren plateaus in their training landscapes. Recently, the nascent field of geometric quantum machine learning (GQML) has emerged as a potential solution to some of those issues. The key insight of GQML is that one should design architectures, such as equivariant QNNs, encoding the symmetries of the problem at hand. Here, we focus on problems with permutation symmetry (i.e., the group of symmetry $S_n$), and show how to build $S_n$-equivariant QNNs. We provide an analytical study of their performance, proving that they do not suffer from barren plateaus, quickly reach overparametrization, and generalize well from small amounts of data. To verify our results, we perform numerical s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#22312;&#20854;&#20013;&#19981;&#21516;&#30340;&#20013;&#24515;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#26631;&#31614;&#20934;&#21017;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#35757;&#32451;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#27169;&#22411;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#25351;&#23548;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2210.02042</link><description>&lt;p&gt;
FedMT: &#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedMT: Federated Learning with Mixed-type Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.02042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#22312;&#20854;&#20013;&#19981;&#21516;&#30340;&#20013;&#24515;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#26631;&#31614;&#20934;&#21017;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#35757;&#32451;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#27169;&#22411;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#25351;&#23548;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#20998;&#31867;&#22120;&#65288;&#20363;&#22914;&#28145;&#24230;&#32593;&#32476;&#65289;&#22312;&#22810;&#20010;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#22312;&#36825;&#20123;&#20013;&#24515;&#20043;&#38388;&#20132;&#25442;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;&#20256;&#32479;&#30340;FL&#35774;&#32622;&#20013;&#65292;&#36890;&#24120;&#22312;&#25152;&#26377;&#21442;&#19982;&#35757;&#32451;&#30340;&#20013;&#24515;&#20013;&#37319;&#29992;&#30456;&#21516;&#30340;&#26631;&#31614;&#20934;&#21017;&#12290;&#36825;&#20010;&#38480;&#21046;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;FL&#30340;&#36866;&#29992;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#30142;&#30149;&#35786;&#26029;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#24456;&#21487;&#33021;&#22312;&#20020;&#24202;&#20013;&#24515;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#19982;&#20256;&#32479;FL&#30340;&#35774;&#32622;&#19981;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#37325;&#35201;&#20294;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;FL&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;FL&#65292;&#20854;&#20013;&#21508;&#20010;&#20013;&#24515;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#26631;&#31614;&#20934;&#21017;&#65292;&#20174;&#32780;&#23548;&#33268;&#20013;&#24515;&#38388;&#26631;&#31614;&#31354;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#23545;&#20026;&#20256;&#32479;&#35774;&#32622;&#35774;&#35745;&#30340;&#29616;&#26377;FL&#26041;&#27861;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#35757;&#32451;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29702;&#35770;&#25351;&#23548;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.02042v3 Announce Type: replace-cross Abstract: In federated learning (FL), classifiers (e.g., deep networks) are trained on datasets from multiple centers without exchanging data across them, and thus improves sample efficiency. In the classical setting of FL, the same labeling criterion is usually employed across all centers being involved in training. This constraint greatly limits the applicability of FL. For example, standards used for disease diagnosis are more likely to be different across clinical centers, which mismatches the classical FL setting. In this paper, we consider an important yet under-explored setting of FL, namely FL with mixed-type labels where different labeling criteria can be employed by various centers, leading to inter-center label space differences and challenging existing FL methods designed for the classical setting. To effectively and efficiently train models with mixed-type labels, we propose a theory-guided and model-agnostic approach that ca
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26041;&#24046;&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#19982;MLP&#25237;&#24433;&#22120;&#32467;&#21512;&#65292;&#22312;&#23398;&#20064;&#34920;&#31034;&#30340;&#29305;&#24449;&#20043;&#38388;&#23454;&#29616;&#20102;&#25104;&#23545;&#29420;&#31435;&#24615;&#65292;&#24182;&#23545;&#25104;&#23545;&#29420;&#31435;&#24615;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22495;&#22806;&#27867;&#21270;&#26041;&#38754;&#30340;&#20215;&#20540;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2209.14905</link><description>&lt;p&gt;
&#26041;&#24046;&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#22312;&#33258;&#30417;&#30563;&#34920;&#31034;&#20013;&#24378;&#21046;&#23454;&#29616;&#25104;&#23545;&#29420;&#31435;&#24615;
&lt;/p&gt;
&lt;p&gt;
Variance Covariance Regularization Enforces Pairwise Independence in Self-Supervised Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.14905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26041;&#24046;&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#19982;MLP&#25237;&#24433;&#22120;&#32467;&#21512;&#65292;&#22312;&#23398;&#20064;&#34920;&#31034;&#30340;&#29305;&#24449;&#20043;&#38388;&#23454;&#29616;&#20102;&#25104;&#23545;&#29420;&#31435;&#24615;&#65292;&#24182;&#23545;&#25104;&#23545;&#29420;&#31435;&#24615;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22495;&#22806;&#27867;&#21270;&#26041;&#38754;&#30340;&#20215;&#20540;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;VICReg&#12289;Barlow Twins&#25110;W-MSE&#65289;&#36890;&#36807;&#32422;&#26463;&#25110;&#27491;&#21017;&#21270;&#23427;&#20204;&#30340;&#25237;&#24433;&#22120;&#36755;&#20986;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#26469;&#36991;&#20813;&#32852;&#21512;&#23884;&#20837;&#32467;&#26500;&#30340;&#22349;&#22604;&#12290;&#26412;&#30740;&#31350;&#31361;&#20986;&#20102;&#36825;&#31181;&#31574;&#30053;&#30340;&#37325;&#35201;&#24615;&#36136;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#26041;&#24046;&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#65288;VCReg&#65289;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#8220;VCReg&#19982;MLP&#25237;&#24433;&#22120;&#32467;&#21512;&#65292;&#22312;&#23398;&#20064;&#34920;&#31034;&#30340;&#29305;&#24449;&#20043;&#38388;&#23454;&#29616;&#20102;&#25104;&#23545;&#29420;&#31435;&#24615;&#8221;&#12290;&#36825;&#20010;&#32467;&#26524;&#36890;&#36807;&#23558;&#24212;&#29992;&#20110;&#25237;&#24433;&#22120;&#36755;&#20837;&#30340;&#26680;&#29420;&#31435;&#24615;&#26631;&#20934;&#19982;&#24212;&#29992;&#20110;&#25237;&#24433;&#22120;&#36755;&#20986;&#30340;VCReg&#30456;&#32467;&#21512;&#26469;&#24471;&#20986;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65306;&#65288;i&#65289;&#25105;&#20204;&#31361;&#20986;&#20102;&#21738;&#20123;&#25237;&#24433;&#22120;&#30340;&#29305;&#24615;&#26377;&#21033;&#20110;&#25104;&#23545;&#29420;&#31435;&#24615;&#65292;&#65288;ii&#65289;&#25105;&#20204;&#35777;&#26126;&#25104;&#23545;&#29420;&#31435;&#24615;&#23545;&#20110;&#22495;&#22806;&#27867;&#21270;&#20855;&#26377;&#30410;&#22788;&#65292;&#65288;iii&#65289;&#25105;&#20204;&#35777;&#26126;VCReg&#30340;&#33539;&#22260;&#36229;&#20986;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#23427;&#26469;&#35299;&#20915;&#29420;&#31435;&#25104;&#20998;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.14905v2 Announce Type: replace Abstract: Self-Supervised Learning (SSL) methods such as VICReg, Barlow Twins or W-MSE avoid collapse of their joint embedding architectures by constraining or regularizing the covariance matrix of their projector's output. This study highlights important properties of such strategy, which we coin Variance-Covariance regularization (VCReg). More precisely, we show that {\em VCReg combined to a MLP projector enforces pairwise independence between the features of the learned representation}. This result emerges by bridging VCReg applied on the projector's output to kernel independence criteria applied on the projector's input. We empirically validate our findings where (i) we put in evidence which projector's characteristics favor pairwise independence, (ii) we demonstrate pairwise independence to be beneficial for out-of-domain generalization, (iii) we demonstrate that the scope of VCReg goes beyond SSL by using it to solve Independent Component
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#30417;&#30563;&#28145;&#24230;&#23884;&#20837;&#21644;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#30340;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#20840;&#38754;&#29702;&#35299;&#21355;&#26143;&#22270;&#20687;&#24207;&#21015;&#30340;&#22823;&#35268;&#27169;&#31354;&#38388;-&#26102;&#38388;&#35821;&#20041;&#20998;&#26512;&#26041;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#12289;&#22320;&#24418;&#21464;&#24322;&#24615;&#21644;&#22270;&#20687;&#22797;&#26434;&#24615;&#31561;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#24213;&#23618;&#30340;&#31354;&#38388;-&#26102;&#38388;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#24863;&#20852;&#36259;&#21306;&#22495;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2208.13504</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#26080;&#30417;&#30563;&#30340;&#21355;&#26143;&#22270;&#20687;&#24207;&#21015;&#31354;&#38388;-&#26102;&#38388;&#35821;&#20041;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Large-scale unsupervised spatio-temporal semantic analysis of vast regions from satellite images sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.13504
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#28145;&#24230;&#23884;&#20837;&#21644;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#30340;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#20840;&#38754;&#29702;&#35299;&#21355;&#26143;&#22270;&#20687;&#24207;&#21015;&#30340;&#22823;&#35268;&#27169;&#31354;&#38388;-&#26102;&#38388;&#35821;&#20041;&#20998;&#26512;&#26041;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#12289;&#22320;&#24418;&#21464;&#24322;&#24615;&#21644;&#22270;&#20687;&#22797;&#26434;&#24615;&#31561;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#24213;&#23618;&#30340;&#31354;&#38388;-&#26102;&#38388;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#24863;&#20852;&#36259;&#21306;&#22495;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#26143;&#22270;&#20687;&#30340;&#26102;&#38388;&#24207;&#21015;&#26159;&#20998;&#26512;&#24863;&#20852;&#36259;&#21306;&#22495;&#30340;&#19968;&#31181;&#26497;&#20854;&#23453;&#36149;&#21644;&#20016;&#23500;&#30340;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#22240;&#32032;&#65292;&#22914;&#32570;&#20047;&#31934;&#30830;&#30340;&#26631;&#27880;&#25968;&#25454;&#12289;&#22320;&#24418;&#23454;&#20307;&#30340;&#23450;&#20041;&#21644;&#21464;&#24322;&#24615;&#65292;&#20197;&#21450;&#22270;&#20687;&#21450;&#20854;&#34701;&#21512;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#22823;&#35268;&#27169;&#30693;&#35782;&#30340;&#33258;&#21160;&#33719;&#21462;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#21644;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21355;&#26143;&#22270;&#20687;&#24207;&#21015;&#20013;&#36827;&#34892;&#22823;&#21306;&#22495;&#30340;&#31354;&#38388;-&#26102;&#38388;&#20998;&#31867;&#23398;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#23884;&#20837;&#21644;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#30340;&#32467;&#21512;&#65292;&#20197;&#25429;&#25417;&#22320;&#38754;&#30340;&#35821;&#20041;&#29305;&#24615;&#21450;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#65292;&#20174;&#32780;&#20840;&#38754;&#20102;&#35299;&#24863;&#20852;&#36259;&#21306;&#22495;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#30340;&#26032;&#22411;&#36807;&#31243;&#26469;&#25913;&#36827;&#23884;&#20837;&#24182;&#21033;&#29992;&#24213;&#23618;&#30340;&#31354;&#38388;-&#26102;&#38388;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.13504v3 Announce Type: replace-cross Abstract: Temporal sequences of satellite images constitute a highly valuable and abundant resource for analyzing regions of interest. However, the automatic acquisition of knowledge on a large scale is a challenging task due to different factors such as the lack of precise labeled data, the definition and variability of the terrain entities, or the inherent complexity of the images and their fusion. In this context, we present a fully unsupervised and general methodology to conduct spatio-temporal taxonomies of large regions from sequences of satellite images. Our approach relies on a combination of deep embeddings and time series clustering to capture the semantic properties of the ground and its evolution over time, providing a comprehensive understanding of the region of interest. The proposed method is enhanced by a novel procedure specifically devised to refine the embedding and exploit the underlying spatio-temporal patterns. We us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#23545;&#25239;&#24615;&#26597;&#35810;&#40065;&#26834;&#24615;&#30340;&#21160;&#24577;&#32500;&#25252;&#26680;&#23494;&#24230;&#20272;&#35745;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20122;&#20108;&#27425;&#31354;&#38388;&#22797;&#26434;&#24230;&#21644;&#20122;&#32447;&#24615;&#26356;&#26032;&#26102;&#38388;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2208.03915</link><description>&lt;p&gt;
&#21160;&#24577;&#32500;&#25252;&#26680;&#23494;&#24230;&#20272;&#35745;&#25968;&#25454;&#32467;&#26500;&#65306;&#20174;&#23454;&#36341;&#21040;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Dynamic Maintenance of Kernel Density Estimation Data Structure: From Practice to Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.03915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#23545;&#25239;&#24615;&#26597;&#35810;&#40065;&#26834;&#24615;&#30340;&#21160;&#24577;&#32500;&#25252;&#26680;&#23494;&#24230;&#20272;&#35745;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20122;&#20108;&#27425;&#31354;&#38388;&#22797;&#26434;&#24230;&#21644;&#20122;&#32447;&#24615;&#26356;&#26032;&#26102;&#38388;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kernel density estimation (KDE)&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#35813;&#38382;&#39064;&#23450;&#20041;&#22914;&#19979;&#65306;&#32473;&#23450;&#19968;&#20010;&#26680;&#20989;&#25968;$f(x,y)$&#21644;&#19968;&#32452;&#28857;$\{x_1, x_2, \cdots, x_n \} \subset \mathbb{R}^d$&#65292;&#25105;&#20204;&#24076;&#26395;&#35745;&#31639;&#20219;&#24847;&#26597;&#35810;&#28857;$y \in \mathbb{R}^d$&#30340;$\frac{1}{n}\sum_{i=1}^{n} f(x_i,y)$&#12290;&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#25968;&#25454;&#32467;&#26500;&#26469;&#39640;&#25928;&#35745;&#31639;KDE&#30340;&#36235;&#21183;&#26085;&#30410;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;KDE&#25968;&#25454;&#32467;&#26500;&#25552;&#20379;&#30340;&#26159;&#38745;&#24577;&#35774;&#32622;&#19979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23545;&#20110;&#21160;&#24577;&#21464;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#24182;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20855;&#26377;&#23545;&#25239;&#24615;&#26597;&#35810;&#40065;&#26834;&#24615;&#30340;&#21160;&#24577;&#32500;&#25252;KDE&#25968;&#25454;&#32467;&#26500;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;KDE&#25968;&#25454;&#32467;&#26500;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;KDE&#25968;&#25454;&#32467;&#26500;&#21482;&#38656;&#35201;&#20122;&#20108;&#27425;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#32467;&#26500;&#25903;&#25345;&#25968;&#25454;&#38598;&#30340;&#21160;&#24577;&#26356;&#26032;&#65292;&#19988;&#26356;&#26032;&#26102;&#38388;&#20026;&#20122;&#32447;&#24615;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.03915v2 Announce Type: replace Abstract: Kernel density estimation (KDE) stands out as a challenging task in machine learning. The problem is defined in the following way: given a kernel function $f(x,y)$ and a set of points $\{x_1, x_2, \cdots, x_n \} \subset \mathbb{R}^d$, we would like to compute $\frac{1}{n}\sum_{i=1}^{n} f(x_i,y)$ for any query point $y \in \mathbb{R}^d$. Recently, there has been a growing trend of using data structures for efficient KDE. However, the proposed KDE data structures focus on static settings. The robustness of KDE data structures over dynamic changing data distributions is not addressed. In this work, we focus on the dynamic maintenance of KDE data structures with robustness to adversarial queries. Especially, we provide a theoretical framework of KDE data structures. In our framework, the KDE data structures only require subquadratic spaces. Moreover, our data structure supports the dynamic update of the dataset in sublinear time. Furtherm
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32508;&#36848;&#20102;&#22522;&#20110;GAN&#30340;&#34394;&#20551;&#26032;&#38395;&#30340;&#29983;&#25104;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#24110;&#21161;&#35299;&#20915;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2207.01390</link><description>&lt;p&gt;
&#34394;&#20551;&#26032;&#38395;: &#22522;&#20110;GAN&#30340;&#36924;&#30495;3D&#20307;&#31215;&#25968;&#25454;&#29983;&#25104;--&#31995;&#32479;&#32508;&#36848;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
FakeNews: GAN-based generation of realistic 3D volumetric data -- A systematic review and taxonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.01390
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32508;&#36848;&#20102;&#22522;&#20110;GAN&#30340;&#34394;&#20551;&#26032;&#38395;&#30340;&#29983;&#25104;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#24110;&#21161;&#35299;&#20915;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#65288;&#22914;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65289;&#30340;&#22823;&#35268;&#27169;&#26222;&#21450;&#65292;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#20307;&#31215;&#25968;&#25454;&#38750;&#24120;&#37325;&#35201;&#65292;&#28085;&#30422;&#20102;&#30142;&#30149;&#35786;&#26029;&#21644;&#27835;&#30103;&#30417;&#27979;&#31561;&#26041;&#38754;&#12290;&#24403;&#25968;&#25454;&#38598;&#36275;&#22815;&#26102;&#65292;&#21487;&#20197;&#35757;&#32451;&#27169;&#22411;&#26469;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#36825;&#20123;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26377;&#20123;&#24773;&#20917;&#19979;&#22823;&#37327;&#25968;&#25454;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#20363;&#22914;&#65292;&#32597;&#35265;&#30142;&#30149;&#21644;&#38544;&#31169;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#21463;&#38480;&#12290;&#22312;&#38750;&#21307;&#23398;&#39046;&#22495;&#65292;&#33719;&#21462;&#36275;&#22815;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#20063;&#21487;&#33021;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#21487;&#20197;&#26159;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#36924;&#30495;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#36825;&#20123;&#26426;&#21046;&#30340;&#23384;&#22312;&#26159;&#19968;&#39033;&#37325;&#35201;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#22240;&#20026;&#25968;&#25454;&#24517;&#39035;&#20855;&#26377;&#33391;&#22909;&#30340;&#36136;&#37327;&#12289;&#36924;&#30495;&#65292;&#24182;&#19988;&#27809;&#26377;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#20851;&#20110;&#20307;&#31215;&#25968;&#25454;&#30340;&#22823;&#37096;&#20998;&#20986;&#29256;&#29289;&#20391;&#37325;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#25968;&#25454;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.01390v2 Announce Type: replace-cross Abstract: With the massive proliferation of data-driven algorithms, such as deep learning-based approaches, the availability of high-quality data is of great interest. Volumetric data is very important in medicine, as it ranges from disease diagnoses to therapy monitoring. When the dataset is sufficient, models can be trained to help doctors with these tasks. Unfortunately, there are scenarios where large amounts of data is unavailable. For example, rare diseases and privacy issues can lead to restricted data availability. In non-medical fields, the high cost of obtaining enough high-quality data can also be a concern. A solution to these problems can be the generation of realistic synthetic data using Generative Adversarial Networks (GANs). The existence of these mechanisms is a good asset, especially in healthcare, as the data must be of good quality, realistic, and without privacy issues. Therefore, most of the publications on volumetr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20998;&#24067;&#31283;&#23450;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#20998;&#24067;&#21464;&#21270;&#26465;&#20214;&#19979;&#30340;&#31283;&#23450;&#24615;&#12290;&#36890;&#36807;&#37327;&#21270;&#39044;&#27979;&#26426;&#21046;&#23376;&#32676;&#20307;&#20043;&#38388;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20986;&#20102;&#23398;&#20064;&#20551;&#35774;&#21644;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#12290;&#36890;&#36807;&#31283;&#23450;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;SRM&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#39044;&#27979;&#26426;&#21046;&#21464;&#21270;&#26102;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2206.02990</link><description>&lt;p&gt;
&#25552;&#39640;&#23376;&#32676;&#20307;&#38388;&#20998;&#24067;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Distributional Stability among Sub-populations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.02990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20998;&#24067;&#31283;&#23450;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#20998;&#24067;&#21464;&#21270;&#26465;&#20214;&#19979;&#30340;&#31283;&#23450;&#24615;&#12290;&#36890;&#36807;&#37327;&#21270;&#39044;&#27979;&#26426;&#21046;&#23376;&#32676;&#20307;&#20043;&#38388;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20986;&#20102;&#23398;&#20064;&#20551;&#35774;&#21644;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#12290;&#36890;&#36807;&#31283;&#23450;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;SRM&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#39044;&#27979;&#26426;&#21046;&#21464;&#21270;&#26102;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#35843;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#20998;&#24067;&#21464;&#21270;&#26465;&#20214;&#19979;&#30340;&#31283;&#23450;&#24615;&#26159;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#65288;Out-of-Distribution&#65292;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;&#20174;&#22240;&#26524;&#23398;&#20064;&#20013;&#34893;&#29983;&#20986;&#26469;&#30340;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#36861;&#27714;&#23545;&#20110;&#22810;&#20010;&#35757;&#32451;&#29615;&#22659;&#30340;&#20005;&#26684;&#19981;&#21464;&#24615;&#12290;&#34429;&#28982;&#30452;&#35266;&#19978;&#26159;&#21512;&#29702;&#30340;&#65292;&#20294;&#26159;&#35201;&#23398;&#20064;&#20005;&#26684;&#30340;&#19981;&#21464;&#24615;&#24615;&#36136;&#38656;&#35201;&#23545;&#29615;&#22659;&#30340;&#21487;&#29992;&#24615;&#21644;&#36136;&#37327;&#20570;&#20986;&#24378;&#20551;&#35774;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#8220;&#20998;&#24067;&#31283;&#23450;&#24615;&#8221;&#27010;&#24565;&#26469;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#12290;&#23427;&#22312;&#39044;&#27979;&#26426;&#21046;&#30340;&#23376;&#32676;&#20307;&#20043;&#38388;&#30340;&#29305;&#23450;&#23610;&#24230;&#19978;&#37327;&#21270;&#20102;&#20854;&#31283;&#23450;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#20551;&#35774;&#65292;&#24182;&#25512;&#23548;&#20102;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#12290;&#22312;&#29702;&#35770;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#26032;&#22411;&#31283;&#23450;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;SRM&#65289;&#31639;&#27861;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#39044;&#27979;&#26426;&#21046;&#65288;$Y|X$-shifts&#65289;&#21464;&#21270;&#26102;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.02990v2 Announce Type: replace Abstract: Enhancing the stability of machine learning algorithms under distributional shifts is at the heart of the Out-of-Distribution (OOD) Generalization problem. Derived from causal learning, recent works of invariant learning pursue strict invariance with multiple training environments. Although intuitively reasonable, strong assumptions on the availability and quality of environments are made to learn the strict invariance property. In this work, we come up with the ``distributional stability" notion to mitigate such limitations. It quantifies the stability of prediction mechanisms among sub-populations down to a prescribed scale. Based on this, we propose the learnability assumption and derive the generalization error bound under distribution shifts. Inspired by theoretical analyses, we propose our novel stable risk minimization (SRM) algorithm to enhance the model's stability w.r.t. shifts in prediction mechanisms ($Y|X$-shifts). Experi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Frank-Wolfe&#31639;&#27861;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#24863;&#30693;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#21151;&#33021;&#30340;&#33539;&#25968;&#32422;&#26463;&#21644;SFW&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#27425;&#23494;&#38598;&#35757;&#32451;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#23494;&#38598;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#23545;&#21387;&#32553;&#27604;&#40065;&#26834;&#24615;&#21644;&#22788;&#29702;&#21367;&#31215;&#28388;&#27874;&#22120;&#21098;&#26525;&#21644;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#30340;&#24773;&#20917;&#19979;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2205.11921</link><description>&lt;p&gt;
&#20351;&#29992;Frank-Wolfe&#31639;&#27861;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#30340;&#21387;&#32553;&#24863;&#30693;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Compression-aware Training of Neural Networks using Frank-Wolfe
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.11921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Frank-Wolfe&#31639;&#27861;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#24863;&#30693;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#21151;&#33021;&#30340;&#33539;&#25968;&#32422;&#26463;&#21644;SFW&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#27425;&#23494;&#38598;&#35757;&#32451;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#23494;&#38598;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#23545;&#21387;&#32553;&#27604;&#40065;&#26834;&#24615;&#21644;&#22788;&#29702;&#21367;&#31215;&#28388;&#27874;&#22120;&#21098;&#26525;&#21644;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#30340;&#24773;&#20917;&#19979;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#37325;&#26032;&#35757;&#32451;&#65292;&#35201;&#20040;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#24378;&#22823;&#30340;&#20559;&#24046;&#65292;&#20197;&#20415;&#25910;&#25947;&#21040;&#19968;&#20010;&#31232;&#30095;&#35299;&#12290;&#32780;&#31532;&#19977;&#31181;&#33539;&#24335;&#65292;&#8220;&#21387;&#32553;&#24863;&#30693;&#8221;&#35757;&#32451;&#26088;&#22312;&#36890;&#36807;&#21333;&#20010;&#23494;&#38598;&#35757;&#32451;&#36816;&#34892;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#23494;&#38598;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#22810;&#31181;&#21387;&#32553;&#27604;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#36991;&#20813;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#22810;&#21151;&#33021;&#30340;&#33539;&#25968;&#32422;&#26463;&#21644;&#38543;&#26426;Frank-Wolfe(SFW)&#31639;&#27861;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#20419;&#20351;&#25910;&#25947;&#21040;&#34920;&#29616;&#33391;&#22909;&#30340;&#35299;&#65292;&#21516;&#26102;&#22312;&#21367;&#31215;&#28388;&#27874;&#22120;&#21098;&#26525;&#21644;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20248;&#20110;&#29616;&#26377;&#30340;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#26680;&#33539;&#25968;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#23427;&#36824;&#38656;&#35201;&#26174;&#33879;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#30452;&#21040;&#21387;&#32553;&#36807;&#31243;&#27604;&#36739;&#22909;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.11921v2 Announce Type: replace Abstract: Many existing Neural Network pruning approaches rely on either retraining or inducing a strong bias in order to converge to a sparse solution throughout training. A third paradigm, 'compression-aware' training, aims to obtain state-of-the-art dense models that are robust to a wide range of compression ratios using a single dense training run while also avoiding retraining. We propose a framework centered around a versatile family of norm constraints and the Stochastic Frank-Wolfe (SFW) algorithm that encourage convergence to well-performing solutions while inducing robustness towards convolutional filter pruning and low-rank matrix decomposition. Our method is able to outperform existing compression-aware approaches and, in the case of low-rank matrix decomposition, it also requires significantly less computational resources than approaches based on nuclear-norm regularization. Our findings indicate that dynamically adjusting the lear
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#31209;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#34920;&#31034;&#36873;&#25321;&#23545;&#20110;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;&#25552;&#20986;&#20102;ReLEX&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#39640;&#25928;&#34920;&#31034;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#32447;&#29256;&#26412;ReLEX-UCB&#24635;&#26159;&#19981;&#27604;&#27809;&#26377;&#34920;&#31034;&#36873;&#25321;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#24046;&#65292;&#24182;&#22312;&#34920;&#31034;&#20989;&#25968;&#31867;&#20855;&#26377;&#8220;&#35206;&#30422;&#24230;&#8221;&#24615;&#36136;&#26102;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24120;&#25968;&#36951;&#25022;&#12290;&#23545;&#20110;&#31163;&#32447;&#29256;&#26412;ReLEX-LCB&#65292;&#21487;&#20197;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2106.11935</link><description>&lt;p&gt;
&#20302;&#31209;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#21487;&#35777;&#26126;&#39640;&#25928;&#30340;&#34920;&#31034;&#36873;&#25321;&#65306;&#20174;&#22312;&#32447;&#21040;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Representation Selection in Low-rank Markov Decision Processes: From Online to Offline RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2106.11935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#31209;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#34920;&#31034;&#36873;&#25321;&#23545;&#20110;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;&#25552;&#20986;&#20102;ReLEX&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#39640;&#25928;&#34920;&#31034;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#32447;&#29256;&#26412;ReLEX-UCB&#24635;&#26159;&#19981;&#27604;&#27809;&#26377;&#34920;&#31034;&#36873;&#25321;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#24046;&#65292;&#24182;&#22312;&#34920;&#31034;&#20989;&#25968;&#31867;&#20855;&#26377;&#8220;&#35206;&#30422;&#24230;&#8221;&#24615;&#36136;&#26102;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24120;&#25968;&#36951;&#25022;&#12290;&#23545;&#20110;&#31163;&#32447;&#29256;&#26412;ReLEX-LCB&#65292;&#21487;&#20197;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#30340;&#25104;&#21151;&#22312;&#20110;&#20854;&#23398;&#20064;&#36866;&#21512;&#25506;&#32034;&#21644;&#21033;&#29992;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#29702;&#35299;&#34920;&#31034;&#36873;&#25321;&#22914;&#20309;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#20302;&#31209;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#65292;&#20854;&#20013;&#36716;&#31227;&#26680;&#33021;&#22815;&#20197;&#21452;&#32447;&#24615;&#24418;&#24335;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;ReLEX&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ReLEX&#30340;&#22312;&#32447;&#29256;&#26412;ReLEX-UCB&#24635;&#26159;&#19981;&#27604;&#27809;&#26377;&#34920;&#31034;&#36873;&#25321;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#24046;&#65292;&#24182;&#22312;&#34920;&#31034;&#20989;&#25968;&#31867;&#22312;&#25972;&#20010;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#19978;&#20855;&#26377;&#8220;&#35206;&#30422;&#24230;&#8221;&#24615;&#36136;&#26102;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24120;&#25968;&#36951;&#25022;&#12290;&#23545;&#20110;&#20854;&#31163;&#32447;&#23545;&#24212;&#29289;ReLEX-LCB&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#22914;&#26524;&#34920;&#31034;&#20989;&#25968;&#31867;&#20855;&#26377;&#8220;&#35206;&#30422;&#24230;&#8221;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2106.11935v2 Announce Type: replace Abstract: The success of deep reinforcement learning (DRL) lies in its ability to learn a representation that is well-suited for the exploration and exploitation task. To understand how the choice of representation can improve the efficiency of reinforcement learning (RL), we study representation selection for a class of low-rank Markov Decision Processes (MDPs) where the transition kernel can be represented in a bilinear form. We propose an efficient algorithm, called ReLEX, for representation learning in both online and offline RL. Specifically, we show that the online version of ReLEX, called ReLEX-UCB, always performs no worse than the state-of-the-art algorithm without representation selection, and achieves a strictly better constant regret if the representation function class has a "coverage" property over the entire state-action space. For the offline counterpart, ReLEX-LCB, we show that the algorithm can find the optimal policy if the r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#65288;TTSA&#65289;&#30340;&#24191;&#20041;&#20998;&#26512;&#65292;&#21033;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65288;CLT&#65289;&#25581;&#31034;&#20102;TTSA&#21463;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#24433;&#21709;&#30340;&#32806;&#21512;&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#25299;&#23637;&#20102;&#20256;&#32479;SGD&#30340;&#39640;&#25928;&#37319;&#26679;&#31574;&#30053;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;GTD&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09339</link><description>&lt;p&gt;
&#20004;&#26102;&#38388;&#23610;&#24230;&#24102;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;&#38543;&#26426;&#36924;&#36817;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65306;&#29702;&#35770;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Central Limit Theorem for Two-Timescale Stochastic Approximation with Markovian Noise: Theory and Applications. (arXiv:2401.09339v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#65288;TTSA&#65289;&#30340;&#24191;&#20041;&#20998;&#26512;&#65292;&#21033;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65288;CLT&#65289;&#25581;&#31034;&#20102;TTSA&#21463;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#24433;&#21709;&#30340;&#32806;&#21512;&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#25299;&#23637;&#20102;&#20256;&#32479;SGD&#30340;&#39640;&#25928;&#37319;&#26679;&#31574;&#30053;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;GTD&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#65288;TTSA&#65289;&#26159;&#26368;&#36890;&#29992;&#30340;&#36845;&#20195;&#38543;&#26426;&#31639;&#27861;&#26694;&#26550;&#20043;&#19968;&#12290;&#36825;&#21253;&#25324;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;SGD&#21464;&#31181;&#21644;&#29992;&#20110;&#21452;&#23618;&#25110;&#26497;&#23567;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#31867;&#20284;&#26799;&#24230;-based&#26102;&#24207;&#24046;&#24322;&#65288;GTD&#65289;&#31639;&#27861;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65288;CLT&#65289;&#23545;&#24102;&#25511;&#21046;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;TTSA&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#28176;&#36817;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;TTSA&#21463;&#24213;&#23618;&#39532;&#23572;&#21487;&#22827;&#38142;&#24433;&#21709;&#30340;&#32806;&#21512;&#21160;&#21147;&#23398;&#65292;&#36825;&#22312;&#20197;&#21069;&#20165;&#32771;&#34385;&#38789;&#24046;&#24322;&#22122;&#22768;&#30340;TTSA&#30340;CLT&#32467;&#26524;&#20013;&#27809;&#26377;&#24471;&#21040;&#35299;&#20915;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;CLT&#65292;&#25105;&#20204;&#23558;&#39640;&#25928;&#37319;&#26679;&#31574;&#30053;&#30340;&#24212;&#29992;&#33539;&#22260;&#20174;&#20256;&#32479;SGD&#25193;&#23637;&#21040;&#20102;&#26356;&#24191;&#27867;&#30340;TTSA&#32972;&#26223;&#19979;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#65292;&#20174;&#32780;&#25193;&#22823;&#20102;&#32993;&#31561;&#20154;&#65288;2022&#65289;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;CLT&#32467;&#26524;&#25512;&#23548;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;GTD&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two-timescale stochastic approximation (TTSA) is among the most general frameworks for iterative stochastic algorithms. This includes well-known stochastic optimization methods such as SGD variants and those designed for bilevel or minimax problems, as well as reinforcement learning like the family of gradient-based temporal difference (GTD) algorithms. In this paper, we conduct an in-depth asymptotic analysis of TTSA under controlled Markovian noise via central limit theorem (CLT), uncovering the coupled dynamics of TTSA influenced by the underlying Markov chain, which has not been addressed by previous CLT results of TTSA only with Martingale difference noise. Building upon our CLT, we expand its application horizon of efficient sampling strategies from vanilla SGD to a wider TTSA context in distributed learning, thus broadening the scope of Hu et al. (2022). In addition, we leverage our CLT result to deduce the statistical properties of GTD algorithms with nonlinear function approxi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21464;&#25442;&#30340;&#26032;&#22411;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20132;&#36890;&#24066;&#22330;&#21033;&#29575;&#30340;&#39044;&#27979;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#23646;&#24615;&#21644;&#29305;&#24449;&#21464;&#25442;&#26680;&#20989;&#25968;&#65292;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#29305;&#24449;&#65292;&#24182;&#22312;&#39044;&#27979;&#36807;&#31243;&#20013;&#20934;&#30830;&#35782;&#21035;&#23395;&#33410;&#24615;&#21644;&#21046;&#24230;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2401.04857</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#21464;&#25442;&#36827;&#34892;&#20132;&#36890;&#24066;&#22330;&#21033;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transportation Market Rate Forecast Using Signature Transform. (arXiv:2401.04857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21464;&#25442;&#30340;&#26032;&#22411;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20132;&#36890;&#24066;&#22330;&#21033;&#29575;&#30340;&#39044;&#27979;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#23646;&#24615;&#21644;&#29305;&#24449;&#21464;&#25442;&#26680;&#20989;&#25968;&#65292;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#29305;&#24449;&#65292;&#24182;&#22312;&#39044;&#27979;&#36807;&#31243;&#20013;&#20934;&#30830;&#35782;&#21035;&#23395;&#33410;&#24615;&#21644;&#21046;&#24230;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20122;&#39532;&#36874;&#22312;&#20132;&#36890;&#24066;&#22330;&#21033;&#29575;&#39044;&#27979;&#19978;&#20381;&#36182;&#31532;&#19977;&#26041;&#65292;&#23613;&#31649;&#36825;&#20123;&#39044;&#27979;&#36136;&#37327;&#24046;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#34429;&#28982;&#20132;&#36890;&#24066;&#22330;&#21033;&#29575;&#36890;&#24120;&#24456;&#38590;&#20934;&#30830;&#39044;&#27979;&#65292;&#20294;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21464;&#25442;&#30340;&#26032;&#22411;&#32479;&#35745;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#39044;&#27979;&#21644;&#33258;&#36866;&#24212;&#27169;&#22411;&#26469;&#39044;&#27979;&#24066;&#22330;&#21033;&#29575;&#12290;&#36825;&#31181;&#26032;&#25216;&#26415;&#22522;&#20110;&#29305;&#24449;&#21464;&#25442;&#30340;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#12290;&#31532;&#19968;&#20010;&#26159;&#20854;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#65292;&#23427;&#32447;&#24615;&#21270;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#23558;&#39044;&#27979;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;&#22238;&#24402;&#20998;&#26512;&#65307;&#31532;&#20108;&#20010;&#26159;&#29305;&#24449;&#21464;&#25442;&#26680;&#20989;&#25968;&#65292;&#23427;&#20801;&#35768;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#36827;&#34892;&#35745;&#31639;&#26377;&#25928;&#30340;&#30456;&#20284;&#24615;&#27604;&#36739;&#12290;&#32467;&#21512;&#36215;&#26469;&#65292;&#36825;&#20123;&#23646;&#24615;&#20801;&#35768;&#36827;&#34892;&#39640;&#25928;&#30340;&#29305;&#24449;&#29983;&#25104;&#65292;&#24182;&#22312;&#39044;&#27979;&#36807;&#31243;&#20013;&#26356;&#31934;&#30830;&#22320;&#35782;&#21035;&#23395;&#33410;&#24615;&#21644;&#21046;&#24230;&#36716;&#25442;&#12290;&#27169;&#22411;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26032;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#24066;&#22330;&#21033;&#29575;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, Amazon relies on third parties for transportation marketplace rate forecasts, despite the poor quality and lack of interpretability of these forecasts. While transportation marketplace rates are typically very challenging to forecast accurately, we have developed a novel signature-based statistical technique to address these challenges and built a predictive and adaptive model to forecast marketplace rates. This novel technique is based on two key properties of the signature transform. The first is its universal nonlinearity which linearizes the feature space and hence translates the forecasting problem into a linear regression analysis; the second is the signature kernel which allows for comparing computationally efficiently similarities between time series data. Combined, these properties allow for efficient feature generation and more precise identification of seasonality and regime switching in the forecasting process. Preliminary result by the model shows that this new 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#40657;&#30418;&#25915;&#20987;&#30340;&#20998;&#31867;&#27861;&#65292;&#25581;&#31034;&#20102;&#26410;&#24320;&#21457;&#30340;&#23041;&#32961;&#31354;&#38388;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26576;&#20123;&#35774;&#32622;&#19978;&#24050;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17534</link><description>&lt;p&gt;
SoK&#65306;&#35780;&#20272;&#40657;&#30418;&#25915;&#20987;&#20013;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
SoK: Pitfalls in Evaluating Black-Box Attacks. (arXiv:2310.17534v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17534
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#40657;&#30418;&#25915;&#20987;&#30340;&#20998;&#31867;&#27861;&#65292;&#25581;&#31034;&#20102;&#26410;&#24320;&#21457;&#30340;&#23041;&#32961;&#31354;&#38388;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26576;&#20123;&#35774;&#32622;&#19978;&#24050;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#28041;&#21450;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#23545;&#23545;&#25163;&#30340;&#30693;&#35782;&#20551;&#35774;&#19981;&#21516;&#65292;&#30446;&#21069;&#30340;&#25991;&#29486;&#32570;&#20047;&#22260;&#32469;&#23041;&#32961;&#27169;&#22411;&#36827;&#34892;&#32452;&#32455;&#30340;&#36830;&#36143;&#24615;&#12290;&#20026;&#20102;&#31995;&#32479;&#21270;&#35813;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#23041;&#32961;&#31354;&#38388;&#30340;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#20102;&#21453;&#39304;&#31890;&#24230;&#12289;&#20132;&#20114;&#24335;&#26597;&#35810;&#30340;&#35775;&#38382;&#21644;&#25915;&#20987;&#32773;&#21487;&#29992;&#30340;&#36741;&#21161;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#19977;&#20010;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#26032;&#20998;&#31867;&#27861;&#25552;&#20379;&#20102;&#19977;&#20010;&#20851;&#38190;&#35265;&#35299;&#12290;1) &#23613;&#31649;&#26377;&#24191;&#27867;&#25991;&#29486;&#65292;&#20173;&#23384;&#22312;&#35768;&#22810;&#26410;&#24320;&#21457;&#30340;&#23041;&#32961;&#31354;&#38388;&#65292;&#26080;&#27861;&#36890;&#36807;&#20174;&#24050;&#30693;&#39046;&#22495;&#30340;&#25216;&#26415;&#31616;&#21333;&#22320;&#25913;&#36827;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22522;&#20110;&#24050;&#30693;&#39046;&#22495;&#20174;&#23436;&#25972;&#32622;&#20449;&#21521;&#37327;&#35775;&#38382;&#30340;&#25216;&#26415;&#36866;&#24212;&#21040;&#35775;&#38382;&#21069;k&#20010;&#32622;&#20449;&#24471;&#20998;&#30340;&#36739;&#23569;&#30740;&#31350;&#30340;&#35774;&#32622;&#20013;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#20294;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#23427;&#22312;&#20165;&#33719;&#24471;&#39044;&#27979;&#26631;&#31614;&#30340;&#26356;&#20005;&#26684;&#35774;&#32622;&#20013;&#20173;&#28982;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous works study black-box attacks on image classifiers. However, these works make different assumptions on the adversary's knowledge and current literature lacks a cohesive organization centered around the threat model. To systematize knowledge in this area, we propose a taxonomy over the threat space spanning the axes of feedback granularity, the access of interactive queries, and the quality and quantity of the auxiliary data available to the attacker. Our new taxonomy provides three key insights. 1) Despite extensive literature, numerous under-explored threat spaces exist, which cannot be trivially solved by adapting techniques from well-explored settings. We demonstrate this by establishing a new state-of-the-art in the less-studied setting of access to top-k confidence scores by adapting techniques from well-explored settings of accessing the complete confidence vector, but show how it still falls short of the more restrictive setting that only obtains the prediction label, h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#36890;&#29992;&#38750;&#21442;&#25968;&#22240;&#26524;&#28508;&#21464;&#37327;&#27169;&#22411;&#21644;&#36890;&#29992;&#36716;&#25442;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#38750;&#32806;&#21512;&#24178;&#39044;&#24314;&#31435;&#20102;&#22240;&#26524;&#34920;&#36798;&#23398;&#20064;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#21487;&#23454;&#29616;&#24615;&#32467;&#26524;&#12290;&#22312;&#19981;&#30693;&#36947;&#20855;&#20307;&#24178;&#39044;&#23545;&#24212;&#30340;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#32467;&#26524;&#20445;&#35777;&#20102;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#21644;&#21464;&#37327;&#30340;&#23436;&#32654;&#24674;&#22797;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.15450</link><description>&lt;p&gt;
&#36890;&#29992;&#22240;&#26524;&#34920;&#36798;&#23398;&#20064;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#21487;&#23454;&#29616;&#24615;
&lt;/p&gt;
&lt;p&gt;
General Identifiability and Achievability for Causal Representation Learning. (arXiv:2310.15450v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#36890;&#29992;&#38750;&#21442;&#25968;&#22240;&#26524;&#28508;&#21464;&#37327;&#27169;&#22411;&#21644;&#36890;&#29992;&#36716;&#25442;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#38750;&#32806;&#21512;&#24178;&#39044;&#24314;&#31435;&#20102;&#22240;&#26524;&#34920;&#36798;&#23398;&#20064;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#21487;&#23454;&#29616;&#24615;&#32467;&#26524;&#12290;&#22312;&#19981;&#30693;&#36947;&#20855;&#20307;&#24178;&#39044;&#23545;&#24212;&#30340;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#32467;&#26524;&#20445;&#35777;&#20102;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#21644;&#21464;&#37327;&#30340;&#23436;&#32654;&#24674;&#22797;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#36890;&#29992;&#38750;&#21442;&#25968;&#22240;&#26524;&#28508;&#21464;&#37327;&#27169;&#22411;&#21644;&#23558;&#28508;&#21464;&#37327;&#25968;&#25454;&#26144;&#23556;&#21040;&#35266;&#27979;&#25968;&#25454;&#30340;&#36890;&#29992;&#36716;&#25442;&#27169;&#22411;&#19979;&#30340;&#22240;&#26524;&#34920;&#36798;&#23398;&#20064;&#12290;&#36890;&#36807;&#22312;&#28508;&#22312;&#22240;&#26524;&#22270;&#20013;&#27599;&#20010;&#33410;&#28857;&#36827;&#34892;&#20004;&#20010;&#30828;&#24615;&#38750;&#32806;&#21512;&#24178;&#39044;&#26469;&#24314;&#31435;&#21487;&#35782;&#21035;&#24615;&#21644;&#21487;&#23454;&#29616;&#24615;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20154;&#20204;&#19981;&#30693;&#36947;&#21738;&#20010;&#24178;&#39044;&#29615;&#22659;&#23545;&#24212;&#30340;&#33410;&#28857;&#26159;&#30456;&#21516;&#30340;&#65288;&#22240;&#27492;&#26159;&#38750;&#32806;&#21512;&#29615;&#22659;&#65289;&#12290;&#22312;&#21487;&#35782;&#21035;&#24615;&#26041;&#38754;&#65292;&#26412;&#25991;&#30830;&#20445;&#22312;&#38750;&#32806;&#21512;&#24178;&#39044;&#19979;&#33021;&#22815;&#23436;&#32654;&#24674;&#22797;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#21644;&#21464;&#37327;&#12290;&#22312;&#21487;&#23454;&#29616;&#24615;&#26041;&#38754;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#21033;&#29992;&#35266;&#27979;&#21644;&#24178;&#39044;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35813;&#31639;&#27861;&#30340;&#21487;&#39564;&#35777;&#30340;&#20445;&#35777;&#65292;&#20197;&#24674;&#22797;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#21644;&#21464;&#37327;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#24471;&#20998;&#21464;&#21270;&#26469;&#20272;&#35745;&#36716;&#25442;&#22120;&#30340;&#36870;&#21644;&#38543;&#21518;&#30340;&#28508;&#21464;&#37327;&#12290;&#35813;&#20998;&#26512;&#36824;...
&lt;/p&gt;
&lt;p&gt;
This paper focuses on causal representation learning (CRL) under a general nonparametric causal latent model and a general transformation model that maps the latent data to the observational data. It establishes \textbf{identifiability} and \textbf{achievability} results using two hard \textbf{uncoupled} interventions per node in the latent causal graph. Notably, one does not know which pair of intervention environments have the same node intervened (hence, uncoupled environments). For identifiability, the paper establishes that perfect recovery of the latent causal model and variables is guaranteed under uncoupled interventions. For achievability, an algorithm is designed that uses observational and interventional data and recovers the latent causal model and variables with provable guarantees for the algorithm. This algorithm leverages score variations across different environments to estimate the inverse of the transformer and, subsequently, the latent variables. The analysis, addit
&lt;/p&gt;</description></item><item><title>&#22312;&#36125;&#21494;&#26031;&#20027;&#21160;&#20803;&#23398;&#20064;&#20013;&#65292;&#36138;&#23146;&#36861;&#27714;&#21487;&#36716;&#31227;&#30693;&#35782;&#21487;&#33021;&#20250;&#25439;&#23475;&#23545;&#21487;&#36716;&#31227;&#21442;&#25968;&#30340;&#20272;&#35745;&#65292;&#23398;&#20064;&#32773;&#38754;&#20020;&#20219;&#21153;&#35782;&#21035;&#21644;&#21487;&#36716;&#31227;&#30693;&#35782;&#33719;&#21462;&#20043;&#38388;&#30340;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/2310.14968</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#20027;&#21160;&#20803;&#23398;&#20064;&#30340;&#22522;&#26412;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
The Fundamental Dilemma of Bayesian Active Meta-learning. (arXiv:2310.14968v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14968
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36125;&#21494;&#26031;&#20027;&#21160;&#20803;&#23398;&#20064;&#20013;&#65292;&#36138;&#23146;&#36861;&#27714;&#21487;&#36716;&#31227;&#30693;&#35782;&#21487;&#33021;&#20250;&#25439;&#23475;&#23545;&#21487;&#36716;&#31227;&#21442;&#25968;&#30340;&#20272;&#35745;&#65292;&#23398;&#20064;&#32773;&#38754;&#20020;&#20219;&#21153;&#35782;&#21035;&#21644;&#21487;&#36716;&#31227;&#30693;&#35782;&#33719;&#21462;&#20043;&#38388;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#20272;&#35745;&#22312;&#22810;&#20010;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#25968;&#25454;&#31232;&#32570;&#20219;&#21153;&#29615;&#22659;&#20013;&#25512;&#24191;&#30340;&#21442;&#25968;&#12290;&#36125;&#21494;&#26031;&#20027;&#21160;&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#39034;&#24207;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#30340;&#24418;&#24335;&#65292;&#20026;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#20027;&#21160;&#20803;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#22312;&#24403;&#21069;&#20219;&#21153;&#30340;&#29305;&#27530;&#29305;&#24449;&#65288;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#65289;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#21487;&#36716;&#31227;&#30340;&#30693;&#35782;&#65288;&#20272;&#35745;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36138;&#23146;&#36861;&#27714;&#36825;&#20010;&#30446;&#26631;&#23454;&#38469;&#19978;&#21487;&#33021;&#20250;&#25439;&#23475;&#23545;&#21487;&#36716;&#31227;&#21442;&#25968;&#30340;&#20272;&#35745;&#65288;&#24341;&#36215;&#25152;&#35859;&#30340;&#36127;&#36801;&#31227;&#65289;&#12290;&#23398;&#20064;&#32773;&#38754;&#20020;&#30528;&#19968;&#20010;&#31867;&#20284;&#20294;&#19981;&#21516;&#20110;&#21208;&#25506;-&#21033;&#29992;&#22256;&#22659;&#30340;&#22256;&#22659;&#65306;&#20182;&#20204;&#24212;&#35813;&#33457;&#36153;&#20182;&#20204;&#30340;&#33719;&#21462;&#39044;&#31639;&#26469;&#36861;&#27714;&#21487;&#36716;&#31227;&#30340;&#30693;&#35782;&#65292;&#36824;&#26159;&#29992;&#26469;&#30830;&#23450;&#24403;&#21069;&#20219;&#21153;&#29305;&#23450;&#30340;&#21442;&#25968;&#65311;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#19968;&#20123;&#20219;&#21153;&#23384;&#22312;&#19981;&#21487;&#36991;&#20813;&#19988;&#20219;&#24847;&#22823;&#30340;&#36127;&#36801;&#31227;&#23041;&#32961;&#65292;&#20219;&#21153;&#30340;&#35782;&#21035;&#23545;&#20110;&#37325;&#26032;&#23547;&#25214;&#21487;&#36801;&#31227;&#21442;&#25968;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications involve estimation of parameters that generalize across multiple diverse, but related, data-scarce task environments. Bayesian active meta-learning, a form of sequential optimal experimental design, provides a framework for solving such problems. The active meta-learner's goal is to gain transferable knowledge (estimate the transferable parameters) in the presence of idiosyncratic characteristics of the current task (task-specific parameters). We show that in such a setting, greedy pursuit of this goal can actually hurt estimation of the transferable parameters (induce so-called negative transfer). The learner faces a dilemma akin to but distinct from the exploration--exploitation dilemma: should they spend their acquisition budget pursuing transferable knowledge, or identifying the current task-specific parameters? We show theoretically that some tasks pose an inevitable and arbitrarily large threat of negative transfer, and that task identification is critical to re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#38454;&#27573;&#35782;&#21035;&#26377;&#38480;&#24322;&#24120;&#31867;&#21035;&#30340;&#23569;&#37327;&#26631;&#35760;&#24322;&#24120;&#65292;&#24182;&#22312;&#27979;&#35797;&#38454;&#27573;&#26816;&#27979;&#21040;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#30340;&#24322;&#24120;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.12294</link><description>&lt;p&gt;
&#24320;&#25918;&#24335;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Open-Set Multivariate Time-Series Anomaly Detection. (arXiv:2310.12294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#38454;&#27573;&#35782;&#21035;&#26377;&#38480;&#24322;&#24120;&#31867;&#21035;&#30340;&#23569;&#37327;&#26631;&#35760;&#24322;&#24120;&#65292;&#24182;&#22312;&#27979;&#35797;&#38454;&#27573;&#26816;&#27979;&#21040;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#30340;&#24322;&#24120;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#20165;&#20551;&#35774;&#26377;&#27491;&#24120;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#32780;&#19968;&#20123;&#30417;&#30563;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#21152;&#20837;&#26631;&#35760;&#30340;&#24322;&#24120;&#26679;&#26412;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#24322;&#24120;&#31867;&#22411;&#23545;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#35828;&#22312;&#21306;&#20998;&#27491;&#24120;&#25968;&#25454;&#26102;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#32780;&#30417;&#30563;&#26041;&#27861;&#20165;&#33021;&#26816;&#27979;&#31867;&#20284;&#20110;&#35757;&#32451;&#26399;&#38388;&#23384;&#22312;&#30340;&#24322;&#24120;&#65292;&#26080;&#27861;&#25512;&#24191;&#21040;&#26410;&#35265;&#24322;&#24120;&#31867;&#21035;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#21487;&#20197;&#30475;&#21040;&#26469;&#33258;&#26377;&#38480;&#24322;&#24120;&#31867;&#21035;&#30340;&#23569;&#37327;&#26631;&#35760;&#24322;&#24120;&#65292;&#24182;&#26088;&#22312;&#22312;&#27979;&#35797;&#38454;&#27573;&#26816;&#27979;&#21040;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#30340;&#24322;&#24120;&#31867;&#21035;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#22810;&#21464;&#37327;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65288;MOSAD&#65289;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous methods for time series anomaly detection (TSAD) methods have emerged in recent years. Most existing methods are unsupervised and assume the availability of normal training samples only, while few supervised methods have shown superior performance by incorporating labeled anomalous samples in the training phase. However, certain anomaly types are inherently challenging for unsupervised methods to differentiate from normal data, while supervised methods are constrained to detecting anomalies resembling those present during training, failing to generalize to unseen anomaly classes. This paper is the first attempt in providing a novel approach for the open-set TSAD problem, in which a small number of labeled anomalies from a limited class of anomalies are visible in the training phase, with the objective of detecting both seen and unseen anomaly classes in the test phase. The proposed method, called Multivariate Open-Set timeseries Anomaly Detection (MOSAD) consists of three prim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;MMD&#36317;&#31163;&#21644;&#32463;&#20856;&#30340;drop and relearn&#21407;&#29702;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#38543;&#26426;&#26862;&#26519;&#20013;&#26816;&#27979;&#24433;&#21709;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#37327;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#31454;&#20105;&#23545;&#25163;&#12290;</title><link>http://arxiv.org/abs/2310.12115</link><description>&lt;p&gt;
&#22522;&#20110;MMD&#30340;&#20998;&#24067;&#38543;&#26426;&#26862;&#26519;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
MMD-based Variable Importance for Distributional Random Forest. (arXiv:2310.12115v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;MMD&#36317;&#31163;&#21644;&#32463;&#20856;&#30340;drop and relearn&#21407;&#29702;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#38543;&#26426;&#26862;&#26519;&#20013;&#26816;&#27979;&#24433;&#21709;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#37327;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#38543;&#26426;&#26862;&#26519;&#65288;DRF&#65289;&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#22522;&#20110;&#26862;&#26519;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#32473;&#23450;&#36755;&#20837;&#21464;&#37327;&#30340;&#22810;&#20803;&#36755;&#20986;&#30340;&#20840;&#26465;&#20214;&#20998;&#24067;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#30340;drop and relearn&#21407;&#29702;&#21644;MMD&#36317;&#31163;&#30340;DRF&#21464;&#37327;&#37325;&#35201;&#24615;&#31639;&#27861;&#12290;&#20256;&#32479;&#30340;&#37325;&#35201;&#24615;&#24230;&#37327;&#21482;&#33021;&#21457;&#29616;&#23545;&#36755;&#20986;&#22343;&#20540;&#26377;&#24433;&#21709;&#30340;&#21464;&#37327;&#65292;&#32780;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#26356;&#26222;&#36941;&#22320;&#21457;&#29616;&#24433;&#21709;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24341;&#20837;&#30340;&#37325;&#35201;&#24615;&#24230;&#37327;&#26159;&#19968;&#33268;&#30340;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#27169;&#25311;&#25968;&#25454;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#23454;&#35777;&#24615;&#33021;&#65292;&#24182;&#19988;&#36229;&#36234;&#20102;&#31454;&#20105;&#23545;&#25163;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#36882;&#24402;&#29305;&#24449;&#28040;&#38500;&#39640;&#25928;&#22320;&#36873;&#25321;&#21464;&#37327;&#65292;&#22240;&#27492;&#21487;&#20197;&#25552;&#20379;&#23567;&#22411;&#21464;&#37327;&#38598;&#21512;&#26469;&#26500;&#24314;&#20934;&#30830;&#30340;&#26465;&#20214;&#36755;&#20986;&#20998;&#24067;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional Random Forest (DRF) is a flexible forest-based method to estimate the full conditional distribution of a multivariate output of interest given input variables. In this article, we introduce a variable importance algorithm for DRFs, based on the well-established drop and relearn principle and MMD distance. While traditional importance measures only detect variables with an influence on the output mean, our algorithm detects variables impacting the output distribution more generally. We show that the introduced importance measure is consistent, exhibits high empirical performance on both real and simulated data, and outperforms competitors. In particular, our algorithm is highly efficient to select variables through recursive feature elimination, and can therefore provide small sets of variables to build accurate estimates of conditional output distributions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;DPZero&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#19982;&#32500;&#24230;&#26080;&#20851;&#19988;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#20869;&#23384;&#21644;&#38544;&#31169;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.09639</link><description>&lt;p&gt;
DPZero&#65306;&#19982;&#32500;&#24230;&#26080;&#20851;&#19988;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DPZero: Dimension-Independent and Differentially Private Zeroth-Order Optimization. (arXiv:2310.09639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09639
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;DPZero&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#19982;&#32500;&#24230;&#26080;&#20851;&#19988;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#20869;&#23384;&#21644;&#38544;&#31169;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32454;&#35843;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#30340;&#24191;&#27867;&#23454;&#36341;&#20013;&#65292;&#38754;&#20020;&#30528;&#20869;&#23384;&#21644;&#38544;&#31169;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#38543;&#30528;LLM&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#38271;&#65292;&#36798;&#21040;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#26041;&#27861;&#25152;&#38656;&#30340;&#20869;&#23384;&#28040;&#32791;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#12290;&#20854;&#27425;&#65292;&#32771;&#34385;&#21040;LLM&#20542;&#21521;&#20110;&#35760;&#24518;&#21644;&#27844;&#38706;&#25935;&#24863;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24517;&#39035;&#20445;&#25252;&#32454;&#35843;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#38646;&#38454;&#26041;&#27861;&#19982;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#30456;&#32467;&#21512;&#29992;&#20110;LLM&#30340;&#32454;&#35843;&#30340;&#28508;&#21147;&#12290;&#38646;&#38454;&#26041;&#27861;&#20165;&#20381;&#36182;&#21069;&#21521;&#20256;&#36882;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20869;&#23384;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#23427;&#20204;&#19982;&#26631;&#20934;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#32467;&#21512;&#22312;&#19968;&#36215;&#20250;&#23548;&#33268;&#32500;&#24230;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DPZero&#65292;&#19968;&#31181;&#20855;&#26377;&#36817;&#20046;&#32500;&#24230;&#26080;&#20851;&#29575;&#30340;&#26032;&#22411;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
The widespread practice of fine-tuning pretrained large language models (LLMs) on domain-specific data faces two major challenges in memory and privacy. First, as the size of LLMs continue to grow, encompassing billions of parameters, the memory demands of gradient-based training methods via backpropagation become prohibitively high. Second, given the tendency of LLMs to memorize and disclose sensitive training data, the privacy of fine-tuning data must be respected. To this end, we explore the potential of zeroth-order methods in differentially private optimization for fine-tuning LLMs. Zeroth-order methods, which rely solely on forward passes, substantially reduce memory consumption during training. However, directly combining them with standard differential privacy mechanism poses dimension-dependent complexity. To bridge the gap, we introduce DPZero, a novel differentially private zeroth-order algorithm with nearly dimension-independent rates. Our theoretical analysis reveals that 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#19979;&#30028;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32452;&#21512;&#31639;&#27861;&#26469;&#27714;&#35299;&#26368;&#23567;&#26368;&#22823;&#30456;&#20851;&#32858;&#31867;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#23436;&#20840;&#22270;&#19978;&#36798;&#21040;4&#30340;&#36817;&#20284;&#32467;&#26524;&#12290;&#36890;&#36807;&#36138;&#23146;&#32852;&#21512;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#25193;&#23637;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38388;&#19978;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.09196</link><description>&lt;p&gt;
&#19968;&#20010;&#27714;&#35299;&#26368;&#23567;&#26368;&#22823;&#30456;&#20851;&#32858;&#31867;&#38382;&#39064;&#30340;4&#36817;&#20284;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A 4-approximation algorithm for min max correlation clustering. (arXiv:2310.09196v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#19979;&#30028;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32452;&#21512;&#31639;&#27861;&#26469;&#27714;&#35299;&#26368;&#23567;&#26368;&#22823;&#30456;&#20851;&#32858;&#31867;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#23436;&#20840;&#22270;&#19978;&#36798;&#21040;4&#30340;&#36817;&#20284;&#32467;&#26524;&#12290;&#36890;&#36807;&#36138;&#23146;&#32852;&#21512;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#25193;&#23637;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38388;&#19978;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#27714;&#35299;&#26368;&#23567;&#26368;&#22823;&#30456;&#20851;&#32858;&#31867;&#38382;&#39064;&#30340;&#19979;&#30028;&#25216;&#26415;&#65292;&#24182;&#22522;&#20110;&#27492;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23436;&#20840;&#22270;&#30340;&#32452;&#21512;4&#36817;&#20284;&#31639;&#27861;&#12290;&#36825;&#25913;&#36827;&#20102;&#20043;&#21069;&#20351;&#29992;&#32447;&#24615;&#35268;&#21010;&#20844;&#24335;&#65288;Kalhan&#31561;&#65292;2019&#65289;&#33719;&#24471;&#30340;&#36817;&#20284;&#20445;&#35777;&#20026;5&#21644;&#20351;&#29992;&#32452;&#21512;&#31639;&#27861;&#65288;Davies&#31561;&#65292;2023&#65289;&#33719;&#24471;&#30340;&#36817;&#20284;&#20445;&#35777;&#20026;4&#30340;&#26368;&#20339;&#24050;&#30693;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#36138;&#23146;&#32852;&#21512;&#21551;&#21457;&#24335;&#31639;&#27861;&#25193;&#23637;&#20102;&#35813;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#23427;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a lower bounding technique for the min max correlation clustering problem and, based on this technique, a combinatorial 4-approximation algorithm for complete graphs. This improves upon the previous best known approximation guarantees of 5, using a linear program formulation (Kalhan et al., 2019), and 4, for a combinatorial algorithm (Davies et al., 2023). We extend this algorithm by a greedy joining heuristic and show empirically that it improves the state of the art in solution quality and runtime on several benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#36827;&#21270;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#32467;&#21512;&#20026;&#20248;&#21270;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26426;&#20250;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#36827;&#21270;&#35745;&#31639;&#31639;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#25552;&#20379;&#23545;&#25628;&#32034;&#31354;&#38388;&#21644;&#31181;&#32676;&#21160;&#24577;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.08748</link><description>&lt;p&gt;
&#36827;&#21270;&#21160;&#24577;&#20248;&#21270;&#19982;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Dynamic Optimization and Machine Learning. (arXiv:2310.08748v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08748
&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#32467;&#21512;&#20026;&#20248;&#21270;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26426;&#20250;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#36827;&#21270;&#35745;&#31639;&#31639;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#25552;&#20379;&#23545;&#25628;&#32034;&#31354;&#38388;&#21644;&#31181;&#32676;&#21160;&#24577;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#35745;&#31639;(EC)&#20316;&#20026;&#19968;&#31181;&#21463;&#33258;&#28982;&#28176;&#36827;&#21457;&#23637;&#26426;&#21046;&#21551;&#21457;&#30340;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24050;&#32463;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;EC&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#20572;&#28382;&#12289;&#22810;&#26679;&#24615;&#25439;&#22833;&#12289;&#35745;&#31639;&#22797;&#26434;&#24615;&#12289;&#31181;&#32676;&#21021;&#22987;&#21270;&#21644;&#36807;&#26089;&#25910;&#25947;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#23398;&#20064;&#31639;&#27861;&#19982;&#36827;&#21270;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#38598;&#25104;&#21033;&#29992;&#20102;EC&#31639;&#27861;&#22312;&#36845;&#20195;&#25628;&#32034;&#36807;&#31243;&#20013;&#29983;&#25104;&#30340;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#23545;&#25628;&#32034;&#31354;&#38388;&#21644;&#31181;&#32676;&#21160;&#24577;&#30340;&#27934;&#23519;&#12290;&#31867;&#20284;&#22320;&#65292;&#36827;&#21270;&#31639;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;(ML)&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#30456;&#20114;&#30340;&#65292;&#22240;&#20026;EC&#26041;&#27861;&#20026;&#20248;&#21270;&#22122;&#22768;&#12289;&#19981;&#20934;&#30830;&#21644;&#21160;&#24577;&#30446;&#26631;&#20989;&#25968;&#25152;&#25551;&#36848;&#30340;&#22797;&#26434;ML&#20219;&#21153;&#25552;&#20379;&#20102;&#26497;&#22909;&#30340;&#26426;&#20250;&#12290;&#36825;&#20123;&#28151;&#21512;&#25216;&#26415;&#34987;&#31216;&#20026;&#36827;&#21270;&#26426;&#22120;&#23398;&#20064;(EML)&#65292;&#24050;&#22312;ML&#36807;&#31243;&#30340;&#21508;&#20010;&#38454;&#27573;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Computation (EC) has emerged as a powerful field of Artificial Intelligence, inspired by nature's mechanisms of gradual development. However, EC approaches often face challenges such as stagnation, diversity loss, computational complexity, population initialization, and premature convergence. To overcome these limitations, researchers have integrated learning algorithms with evolutionary techniques. This integration harnesses the valuable data generated by EC algorithms during iterative searches, providing insights into the search space and population dynamics. Similarly, the relationship between evolutionary algorithms and Machine Learning (ML) is reciprocal, as EC methods offer exceptional opportunities for optimizing complex ML tasks characterized by noisy, inaccurate, and dynamic objective functions. These hybrid techniques, known as Evolutionary Machine Learning (EML), have been applied at various stages of the ML process. EC techniques play a vital role in tasks such
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#37325;&#22797;&#22996;&#25176;&#36873;&#25321;&#38382;&#39064;&#23637;&#24320;&#30740;&#31350;&#65292;&#36890;&#36807;&#21160;&#24577;&#23459;&#24067;&#21512;&#26684;&#38598;&#21512;&#20197;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20998;&#24067;&#26469;&#20943;&#36731;&#20195;&#29702;&#20154;&#30340;&#33258;&#31169;&#34892;&#20026;&#65292;&#36827;&#32780;&#26368;&#23567;&#21270;&#19982;&#26368;&#20248;&#21512;&#26684;&#38598;&#21512;&#20043;&#38388;&#30340;&#21518;&#24724;&#12290;</title><link>http://arxiv.org/abs/2310.04884</link><description>&lt;p&gt;
&#37325;&#22797;&#22996;&#25176;&#36873;&#25321;&#30340;&#21518;&#24724;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regret Analysis of Repeated Delegated Choice. (arXiv:2310.04884v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#37325;&#22797;&#22996;&#25176;&#36873;&#25321;&#38382;&#39064;&#23637;&#24320;&#30740;&#31350;&#65292;&#36890;&#36807;&#21160;&#24577;&#23459;&#24067;&#21512;&#26684;&#38598;&#21512;&#20197;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20998;&#24067;&#26469;&#20943;&#36731;&#20195;&#29702;&#20154;&#30340;&#33258;&#31169;&#34892;&#20026;&#65292;&#36827;&#32780;&#26368;&#23567;&#21270;&#19982;&#26368;&#20248;&#21512;&#26684;&#38598;&#21512;&#20043;&#38388;&#30340;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#37325;&#22797;&#22996;&#25176;&#36873;&#25321;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#32771;&#34385;&#20811;&#33713;&#22240;&#20271;&#26684;&#21644;&#20811;&#33713;&#22240;&#20271;&#26684;(EC'18)&#22312;&#32447;&#23398;&#20064;&#21464;&#31181;&#30340;&#27169;&#22411;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#19968;&#20010;&#22996;&#25176;&#20154;&#19982;&#19968;&#20010;&#25317;&#26377;&#22806;&#29983;&#35299;&#38598;&#30340;&#20195;&#29702;&#20154;&#36827;&#34892;&#37325;&#22797;&#20114;&#21160;&#65292;&#20197;&#23547;&#25214;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27599;&#20010;&#35299;&#20915;&#26041;&#26696;&#23545;&#22996;&#25176;&#20154;&#21644;&#20195;&#29702;&#20154;&#37117;&#21487;&#20197;&#20135;&#29983;&#19981;&#21516;&#30340;&#25928;&#29992;&#65292;&#20195;&#29702;&#20154;&#21487;&#33021;&#20197;&#33258;&#31169;&#30340;&#26041;&#24335;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#26368;&#22823;&#21270;&#33258;&#36523;&#30340;&#25928;&#29992;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#34892;&#20026;&#65292;&#22996;&#25176;&#20154;&#23459;&#24067;&#19968;&#20010;&#21512;&#26684;&#38598;&#21512;&#65292;&#31579;&#25481;&#19968;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22996;&#25176;&#20154;&#20107;&#20808;&#24182;&#19981;&#20102;&#35299;&#35299;&#20915;&#26041;&#26696;&#30340;&#20998;&#24067;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#22996;&#25176;&#20154;&#21160;&#24577;&#22320;&#23459;&#24067;&#21508;&#31181;&#21512;&#26684;&#38598;&#21512;&#65292;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20998;&#24067;&#12290;&#22996;&#25176;&#20154;&#30340;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#19982;&#20107;&#21518;&#26368;&#20248;&#21512;&#26684;&#38598;&#21512;&#30456;&#27604;&#30340;&#32047;&#31215;&#21518;&#24724;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#38382;&#39064;&#35774;&#32622;&#30340;&#20004;&#20010;&#32500;&#24230;&#65292;&#21363;&#20195;&#29702;&#20154;&#26159;&#26681;&#25454;&#30524;&#21069;&#21033;&#30410;&#34892;&#20107;&#36824;&#26159;&#22312;&#22238;&#21512;&#20043;&#38388;&#21046;&#23450;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a study on a repeated delegated choice problem, which is the first to consider an online learning variant of Kleinberg and Kleinberg, EC'18. In this model, a principal interacts repeatedly with an agent who possesses an exogenous set of solutions to search for efficient ones. Each solution can yield varying utility for both the principal and the agent, and the agent may propose a solution to maximize its own utility in a selfish manner. To mitigate this behavior, the principal announces an eligible set which screens out a certain set of solutions. The principal, however, does not have any information on the distribution of solutions in advance. Therefore, the principal dynamically announces various eligible sets to efficiently learn the distribution. The principal's objective is to minimize cumulative regret compared to the optimal eligible set in hindsight. We explore two dimensions of the problem setup, whether the agent behaves myopically or strategizes across the rounds,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#27169;&#22411;&#20869;&#23398;&#20064;&#37327;&#23376;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#23398;&#20064;&#22120;&#21644;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#36890;&#36807;&#22312;&#23494;&#30721;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#32463;&#20856;&#35835;&#20986;&#37327;&#23376;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#26159;&#37327;&#23376;&#30828;&#20214;&#23433;&#20840;&#39046;&#22495;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02075</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#23398;&#20064;&#37327;&#23376;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Quantum Processes with Quantum Statistical Queries. (arXiv:2310.02075v1 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#27169;&#22411;&#20869;&#23398;&#20064;&#37327;&#23376;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#23398;&#20064;&#22120;&#21644;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#36890;&#36807;&#22312;&#23494;&#30721;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#32463;&#20856;&#35835;&#20986;&#37327;&#23376;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#26159;&#37327;&#23376;&#30828;&#20214;&#23433;&#20840;&#39046;&#22495;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22797;&#26434;&#30340;&#37327;&#23376;&#36807;&#31243;&#26159;&#37327;&#23376;&#35745;&#31639;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#35768;&#22810;&#39046;&#22495;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#24212;&#29992;&#20110;&#37327;&#23376;&#22522;&#20934;&#27979;&#35797;&#12289;&#23494;&#30721;&#20998;&#26512;&#21644;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#65288;QSQ&#65289;&#27169;&#22411;&#20869;&#30740;&#31350;&#37327;&#23376;&#36807;&#31243;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23545;&#37327;&#23376;&#36807;&#31243;&#65288;QPSQs&#65289;&#36827;&#34892;&#32479;&#35745;&#26597;&#35810;&#30340;&#31532;&#19968;&#20010;&#27491;&#24335;&#23450;&#20041;&#12290;&#35813;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;QPSQ&#23398;&#20064;&#22120;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#37327;&#23376;&#36807;&#31243;&#65292;&#24182;&#38468;&#24102;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25968;&#20540;&#27169;&#25311;&#26469;&#23637;&#31034;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#22312;&#23494;&#30721;&#20998;&#26512;&#20013;&#24212;&#29992;&#35813;&#26694;&#26550;&#65292;&#31361;&#20986;&#20102;&#32463;&#20856;&#35835;&#20986;&#37327;&#23376;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#65288;CR-QPUFs&#65289;&#30340;&#33030;&#24369;&#24615;&#65292;&#35299;&#20915;&#20102;&#37327;&#23376;&#30828;&#20214;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24320;&#25918;&#38382;&#39064;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#26397;&#30528;&#28145;&#20837;&#29702;&#35299;&#37327;&#23376;&#36807;&#31243;&#23398;&#20064;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning complex quantum processes is a central challenge in many areas of quantum computing and quantum machine learning, with applications in quantum benchmarking, cryptanalysis, and variational quantum algorithms. This paper introduces the first learning framework for studying quantum process learning within the Quantum Statistical Query (QSQ) model, providing the first formal definition of statistical queries to quantum processes (QPSQs). The framework allows us to propose an efficient QPSQ learner for arbitrary quantum processes accompanied by a provable performance guarantee. We also provide numerical simulations to demonstrate the efficacy of this algorithm. The practical relevance of this framework is exemplified through application in cryptanalysis, highlighting vulnerabilities of Classical-Readout Quantum Physical Unclonable Functions (CR-QPUFs), addressing an important open question in the field of quantum hardware security. This work marks a significant step towards underst
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#20998;&#31867;&#22120;&#23637;&#31034;&#20102;&#35760;&#24405;&#30772;&#32426;&#24405;&#30340;&#20154;&#31867;&#24418;&#29366;&#20559;&#22909;&#12289;&#25509;&#36817;&#20154;&#31867;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#20934;&#30830;&#24615;&#12289;&#19982;&#20154;&#31867;&#20998;&#31867;&#38169;&#35823;&#30340;&#26368;&#20808;&#36827;&#23545;&#40784;&#20197;&#21450;&#29702;&#35299;&#26576;&#20123;&#30693;&#35273;&#24187;&#35937;&#30340;&#26032;&#20852;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#38646;&#26679;&#26412;&#29983;&#25104;&#27169;&#22411;&#20986;&#22855;&#22320;&#25509;&#36817;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.16779</link><description>&lt;p&gt;
&#29983;&#25104;&#20998;&#31867;&#22120;&#30340;&#26377;&#36259;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intriguing properties of generative classifiers. (arXiv:2309.16779v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16779
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20998;&#31867;&#22120;&#23637;&#31034;&#20102;&#35760;&#24405;&#30772;&#32426;&#24405;&#30340;&#20154;&#31867;&#24418;&#29366;&#20559;&#22909;&#12289;&#25509;&#36817;&#20154;&#31867;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#20934;&#30830;&#24615;&#12289;&#19982;&#20154;&#31867;&#20998;&#31867;&#38169;&#35823;&#30340;&#26368;&#20808;&#36827;&#23545;&#40784;&#20197;&#21450;&#29702;&#35299;&#26576;&#20123;&#30693;&#35273;&#24187;&#35937;&#30340;&#26032;&#20852;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#38646;&#26679;&#26412;&#29983;&#25104;&#27169;&#22411;&#20986;&#22855;&#22320;&#25509;&#36817;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#23545;&#35937;&#30340;&#26368;&#20339;&#33539;&#24335;&#26159;&#21028;&#21035;&#24335;&#25512;&#29702;&#65288;&#24555;&#36895;&#20294;&#28508;&#22312;&#23481;&#26131;&#20986;&#29616;&#24555;&#25463;&#23398;&#20064;&#65289;&#36824;&#26159;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#65288;&#36739;&#24930;&#20294;&#28508;&#22312;&#26356;&#31283;&#20581;&#65289;&#65311;&#25105;&#20204;&#20511;&#37492;&#20102;&#26368;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#23637;&#65292;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36716;&#21270;&#20026;&#20998;&#31867;&#22120;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#20854;&#34892;&#20026;&#65292;&#24182;&#23558;&#20854;&#19982;&#21028;&#21035;&#27169;&#22411;&#21644;&#20154;&#31867;&#24515;&#29702;&#29289;&#29702;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25253;&#36947;&#20102;&#29983;&#25104;&#20998;&#31867;&#22120;&#30340;&#22235;&#20010;&#26377;&#36259;&#30340;&#26032;&#20852;&#29305;&#24615;&#65306;&#23427;&#20204;&#26174;&#31034;&#20986;&#30772;&#32426;&#24405;&#30340;&#20154;&#31867;&#24418;&#29366;&#20559;&#22909;&#65288;&#23545;&#20110;Imagen&#36798;&#21040;99%&#65289;&#65292;&#25509;&#36817;&#20154;&#31867;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#20934;&#30830;&#24615;&#65292;&#19982;&#20154;&#31867;&#20998;&#31867;&#38169;&#35823;&#30340;&#26368;&#20808;&#36827;&#23545;&#40784;&#20197;&#21450;&#23427;&#20204;&#29702;&#35299;&#26576;&#20123;&#30693;&#35273;&#24187;&#35937;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#30446;&#21069;&#27169;&#25311;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#30340;&#20027;&#23548;&#33539;&#24335;&#26159;&#21028;&#21035;&#24335;&#25512;&#29702;&#65292;&#38646;&#26679;&#26412;&#29983;&#25104;&#27169;&#22411;&#20986;&#22855;&#22320;&#25509;&#36817;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is the best paradigm to recognize objects -- discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data. We report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MPHD&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#25628;&#32034;&#31354;&#38388;&#19978;&#23454;&#29616;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;MPHD&#30340;&#26377;&#25928;&#24615;&#21644;&#22312;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16597</link><description>&lt;p&gt;
&#24322;&#36136;&#25628;&#32034;&#31354;&#38388;&#19978;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces. (arXiv:2309.16597v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MPHD&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#25628;&#32034;&#31354;&#38388;&#19978;&#23454;&#29616;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;MPHD&#30340;&#26377;&#25928;&#24615;&#21644;&#22312;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#65288;&#36890;&#24120;&#26159;&#39640;&#26031;&#36807;&#31243;&#65289;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#12290;&#20026;&#20102;&#30830;&#20445;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#8220;&#35757;&#32451;&#8221;&#20989;&#25968;&#30340;&#35266;&#23519;&#32467;&#26524;&#26469;&#33258;&#21160;&#35774;&#35745;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#12290;&#36825;&#20123;&#35757;&#32451;&#20989;&#25968;&#36890;&#24120;&#38656;&#35201;&#19982;&#8220;&#27979;&#35797;&#8221;&#20989;&#25968;&#65288;&#24453;&#20248;&#21270;&#30340;&#40657;&#30418;&#20989;&#25968;&#65289;&#20855;&#26377;&#30456;&#21516;&#30340;&#23450;&#20041;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MPHD&#30340;&#27169;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#26144;&#23556;&#21040;&#20998;&#23618;&#39640;&#26031;&#36807;&#31243;&#30340;&#35268;&#33539;&#12290;MPHD&#21487;&#20197;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#26080;&#32541;&#38598;&#25104;&#65292;&#23454;&#29616;&#24322;&#36136;&#25628;&#32034;&#31354;&#38388;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;MPHD&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a popular black-box function optimization method, which makes sequential decisions based on a Bayesian model, typically a Gaussian process (GP), of the function. To ensure the quality of the model, transfer learning approaches have been developed to automatically design GP priors by learning from observations on "training" functions. These training functions are typically required to have the same domain as the "test" function (black-box function to be optimized). In this paper, we introduce MPHD, a model pre-training method on heterogeneous domains, which uses a neural net mapping from domain-specific contexts to specifications of hierarchical GPs. MPHD can be seamlessly integrated with BO to transfer knowledge across heterogeneous search spaces. Our theoretical and empirical results demonstrate the validity of MPHD and its superior performance on challenging black-box function optimization tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#34920;&#24449;&#21508;&#31181;&#21160;&#21147;&#31995;&#32479;&#30340;&#21560;&#24341;&#30406;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#21487;&#39044;&#27979;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#19988;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.15732</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21560;&#24341;&#30406;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-based Analysis of Basins of Attraction. (arXiv:2309.15732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#34920;&#24449;&#21508;&#31181;&#21160;&#21147;&#31995;&#32479;&#30340;&#21560;&#24341;&#30406;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#21487;&#39044;&#27979;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#19988;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#34920;&#24449;&#19981;&#21516;&#21160;&#21147;&#31995;&#32479;&#21560;&#24341;&#30406;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#21487;&#39044;&#27979;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#22312;&#25506;&#32034;&#21160;&#21147;&#31995;&#32479;&#30340;&#19981;&#21516;&#21442;&#25968;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#20256;&#32479;&#26041;&#27861;&#22312;&#34920;&#24449;&#22810;&#20010;&#21560;&#24341;&#30406;&#26102;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;CNN&#20307;&#31995;&#32467;&#26500;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#65292;&#21363;&#20351;&#20351;&#29992;&#36807;&#26102;&#30340;&#20307;&#31995;&#32467;&#26500;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study showcases the effectiveness of convolutional neural networks (CNNs) in characterizing the complexity and unpredictability of basins of attraction for diverse dynamical systems. This novel method is optimal for exploring different parameters of dynamical systems since the conventional methods are computationally expensive for characterizing multiple basins of attraction. Additionally, our research includes a comparison of different CNN architectures for this task showing the superiority of our proposed characterization method over the conventional methods, even with obsolete architectures.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32418;&#22806;&#25506;&#27979;&#22120;&#27169;&#22411;&#21644;&#31890;&#23376;&#30896;&#25758;&#20107;&#20214;&#27169;&#25311;&#22120;&#26469;&#29983;&#25104;&#31616;&#21270;&#25968;&#25454;&#65292;&#20197;&#20415;&#20110;&#39640;&#33021;&#29289;&#29702;&#30740;&#31350;&#21644;&#25945;&#32946;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#21644;&#35299;&#20915;&#26041;&#26696;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.03780</link><description>&lt;p&gt;
&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#31616;&#21270;&#27169;&#25311;&#65306;&#25968;&#25454;&#39537;&#21160;&#29289;&#29702;&#30740;&#31350;&#30340;&#25240;&#20013;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reduced Simulations for High-Energy Physics, a Middle Ground for Data-Driven Physics Research. (arXiv:2309.03780v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03780
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32418;&#22806;&#25506;&#27979;&#22120;&#27169;&#22411;&#21644;&#31890;&#23376;&#30896;&#25758;&#20107;&#20214;&#27169;&#25311;&#22120;&#26469;&#29983;&#25104;&#31616;&#21270;&#25968;&#25454;&#65292;&#20197;&#20415;&#20110;&#39640;&#33021;&#29289;&#29702;&#30740;&#31350;&#21644;&#25945;&#32946;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#21644;&#35299;&#20915;&#26041;&#26696;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20122;&#21407;&#23376;&#31890;&#23376;&#36712;&#36857;&#37325;&#24314;&#65288;&#36861;&#36394;&#65289;&#26159;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#36861;&#36394;&#20219;&#21153;&#22312;&#35745;&#31639;&#19978;&#26497;&#20855;&#25361;&#25112;&#24615;&#65292;&#30446;&#21069;&#20351;&#29992;&#30340;&#20256;&#32479;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#19981;&#33021;&#32447;&#24615;&#25193;&#23637;&#12290;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#31616;&#21270;&#20102;&#30340;&#38382;&#39064;&#25551;&#36848;&#21644;&#25152;&#20195;&#34920;&#30340;&#25968;&#25454;&#23558;&#26377;&#21161;&#20110;&#35299;&#20915;&#26041;&#26696;&#30340;&#25506;&#32034;&#24037;&#20316;&#27969;&#31243;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#32463;&#36807;&#31616;&#21270;&#30340;&#34394;&#25311;&#25506;&#27979;&#22120;&#65288;REDVID&#65289;&#20316;&#20026;&#22797;&#26434;&#24230;&#31616;&#21270;&#25506;&#27979;&#22120;&#27169;&#22411;&#21644;&#31890;&#23376;&#30896;&#25758;&#20107;&#20214;&#27169;&#25311;&#22120;&#30340;&#32452;&#21512;&#12290;REDVID&#26088;&#22312;&#20316;&#20026;&#19968;&#20010;&#27169;&#25311;-&#24490;&#29615;&#26469;&#39640;&#25928;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#31616;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;&#19982;&#29289;&#29702;&#31934;&#30830;&#27169;&#25311;&#30456;&#27604;&#65292;&#25105;&#20204;&#24037;&#20855;&#30340;&#23436;&#20840;&#21442;&#25968;&#21270;&#29305;&#24615;&#20801;&#35768;&#29983;&#25104;&#19981;&#21516;&#23618;&#27425;&#30340;&#30740;&#31350;&#21644;&#25945;&#32946;&#31616;&#21270;&#25968;&#25454;&#12290;&#30001;&#20110;&#31616;&#21270;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
Subatomic particle track reconstruction (tracking) is a vital task in High-Energy Physics experiments. Tracking is exceptionally computationally challenging and fielded solutions, relying on traditional algorithms, do not scale linearly. Machine Learning (ML) assisted solutions are a promising answer. We argue that a complexity-reduced problem description and the data representing it, will facilitate the solution exploration workflow. We provide the REDuced VIrtual Detector (REDVID) as a complexity-reduced detector model and particle collision event simulator combo. REDVID is intended as a simulation-in-the-loop, to both generate synthetic data efficiently and to simplify the challenge of ML model design. The fully parametric nature of our tool, with regards to system-level configuration, while in contrast to physics-accurate simulations, allows for the generation of simplified data for research and education, at different levels. Resulting from the reduced complexity, we showcase the 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.01937</link><description>&lt;p&gt;
&#20351;&#29992;&#32452;&#21512;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Training Data Protection with Compositional Diffusion Models. (arXiv:2308.01937v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01937
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#65292;&#19968;&#31181;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#19978;&#35757;&#32451;&#19981;&#21516;&#25193;&#25955;&#27169;&#22411;&#65288;&#25110;&#25552;&#31034;&#65289;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#21333;&#29420;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#23396;&#31435;&#29366;&#24577;&#19979;&#12289;&#22312;&#19981;&#21516;&#26102;&#38388;&#12289;&#22312;&#19981;&#21516;&#20998;&#24067;&#21644;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#21518;&#32493;&#32452;&#21512;&#20197;&#36798;&#21040;&#19982;&#21516;&#26102;&#35757;&#32451;&#25152;&#26377;&#25968;&#25454;&#30340;&#29702;&#24819;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#27169;&#22411;&#21482;&#21253;&#21547;&#20854;&#22312;&#35757;&#32451;&#26399;&#38388;&#25509;&#35302;&#21040;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#31181;&#24418;&#24335;&#30340;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#12290;&#29305;&#21035;&#26159;&#65292;CDM&#26159;&#31532;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#36951;&#24536;&#21644;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20801;&#35768;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;CDM&#36824;&#21487;&#20197;&#30830;&#23450;&#29983;&#25104;&#29305;&#23450;&#26679;&#26412;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Compartmentalized Diffusion Models (CDM), a method to train different diffusion models (or prompts) on distinct data sources and arbitrarily compose them at inference time. The individual models can be trained in isolation, at different times, and on different distributions and domains and can be later composed to achieve performance comparable to a paragon model trained on all data simultaneously. Furthermore, each model only contains information about the subset of the data it was exposed to during training, enabling several forms of training data protection. In particular, CDMs are the first method to enable both selective forgetting and continual learning for large-scale diffusion models, as well as allowing serving customized models based on the user's access rights. CDMs also allow determining the importance of a subset of the data in generating particular samples.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#22522;&#20110;&#27491;&#21017;&#21270;&#23494;&#24230;&#30340; Sobolev &#33539;&#25968;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#21644;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#33391;&#22909;&#30340;&#35299;&#65292;&#35813;&#26041;&#27861;&#22312; Anomaly Detection benchmark &#20013;&#25490;&#21517;&#31532;&#20108;&#12290;</title><link>http://arxiv.org/abs/2307.13763</link><description>&lt;p&gt;
&#38544;&#24335;&#24402;&#19968;&#21270;&#26174;&#24335;&#27491;&#21017;&#21270;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Implicitly Normalized Explicitly Regularized Density Estimation. (arXiv:2307.13763v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13763
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#22522;&#20110;&#27491;&#21017;&#21270;&#23494;&#24230;&#30340; Sobolev &#33539;&#25968;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#21644;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#33391;&#22909;&#30340;&#35299;&#65292;&#35813;&#26041;&#27861;&#22312; Anomaly Detection benchmark &#20013;&#25490;&#21517;&#31532;&#20108;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#22522;&#20110;&#27491;&#21017;&#21270;&#23494;&#24230;&#30340; Sobolev &#33539;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#26680;&#23494;&#24230;&#20272;&#35745;&#26377;&#26126;&#26174;&#24046;&#24322;&#65292;&#21487;&#20197;&#28165;&#26224;&#35299;&#37322;&#27169;&#22411;&#30340;&#20559;&#24046;&#12290;&#34429;&#28982;&#25105;&#20204;&#26080;&#27861;&#24471;&#21040;&#30456;&#20851;&#26680;&#20989;&#25968;&#30340;&#38381;&#21512;&#35299;&#26512;&#24418;&#24335;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#37319;&#26679;&#36827;&#34892;&#36817;&#20284;&#12290;&#20915;&#23450;&#23494;&#24230;&#30340;&#20248;&#21270;&#38382;&#39064;&#26159;&#38750;&#20984;&#30340;&#65292;&#26631;&#20934;&#30340;&#26799;&#24230;&#26041;&#27861;&#25928;&#26524;&#19981;&#22909;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#21644;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#33391;&#22909;&#30340;&#35299;&#12290;&#26368;&#21518;&#65292;&#34429;&#28982;&#35813;&#26041;&#27861;&#25552;&#20379;&#30340;&#26159;&#38750;&#24402;&#19968;&#21270;&#30340;&#23494;&#24230;&#65292;&#26080;&#27861;&#20351;&#29992;&#23545;&#25968;&#20284;&#28982;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#37319;&#29992;&#22522;&#20110; Fisher &#25955;&#24230;&#30340;&#20998;&#25968;&#21305;&#37197;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#26368;&#36817;&#30340;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214; ADBench &#19978;&#35780;&#20272;&#20102;&#24471;&#21040;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#36229;&#36807;15&#20010;&#31639;&#27861;&#20013;&#25490;&#21517;&#31532;&#20108;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new approach to non-parametric density estimation, that is based on regularizing a Sobolev norm of the density. This method is provably different from Kernel Density Estimation, and makes the bias of the model clear and interpretable. While there is no closed analytic form for the associated kernel, we show that one can approximate it using sampling. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, we show that with an appropriate initialization and using natural gradients, one can obtain well performing solutions. Finally, while the approach provides unnormalized densities, which prevents the use of log-likelihood for cross validation, we show that one can instead adapt Fisher Divergence based Score Matching methods for this task. We evaluate the resulting method on the comprehensive recent Anomaly Detection benchmark suite, ADBench, and find that it ranks second best, among more than 15 al
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37329;&#23646;&#27687;&#21270;&#29289;&#20256;&#24863;&#22120;&#38453;&#21015;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#28151;&#21512;&#29289;&#20013;&#35782;&#21035;&#20986;&#19981;&#21516;&#30340;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOCs&#65289;&#65292;&#19988;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#22238;&#24402;&#20998;&#26512;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06556</link><description>&lt;p&gt;
&#22522;&#20110;&#37329;&#23646;&#27687;&#21270;&#29289;&#30340;&#27668;&#20307;&#20256;&#24863;&#22120;&#38453;&#21015;&#22312;&#22797;&#26434;&#28151;&#21512;&#29289;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;VOCs&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Metal Oxide-based Gas Sensor Array for the VOCs Analysis in Complex Mixtures using Machine Learning. (arXiv:2307.06556v1 [physics.app-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37329;&#23646;&#27687;&#21270;&#29289;&#20256;&#24863;&#22120;&#38453;&#21015;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#28151;&#21512;&#29289;&#20013;&#35782;&#21035;&#20986;&#19981;&#21516;&#30340;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOCs&#65289;&#65292;&#19988;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#22238;&#24402;&#20998;&#26512;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21628;&#21560;&#20013;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOCs&#65289;&#30340;&#26816;&#27979;&#27491;&#22312;&#25104;&#20026;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#30142;&#30149;&#26089;&#26399;&#26816;&#27979;&#30340;&#21487;&#34892;&#36884;&#24452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19977;&#20010;&#37329;&#23646;&#27687;&#21270;&#29289;&#30005;&#26497;&#30340;&#20256;&#24863;&#22120;&#38453;&#21015;&#65292;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#28151;&#21512;&#29289;&#20013;&#35782;&#21035;&#20986;&#22235;&#31181;&#19981;&#21516;&#30340;VOCs&#12290;&#37329;&#23646;&#27687;&#21270;&#29289;&#20256;&#24863;&#22120;&#38453;&#21015;&#32463;&#36807;&#19981;&#21516;VOC&#27987;&#24230;&#30340;&#27979;&#35797;&#65292;&#21253;&#25324;&#20057;&#37255;&#12289;&#19993;&#37230;&#12289;&#30002;&#33519;&#21644;&#27695;&#20223;&#12290;&#20174;&#21333;&#19968;&#27668;&#20307;&#21644;&#28151;&#21512;&#29289;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#20351;&#29992;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#12289;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#12289;&#20915;&#31574;&#26641;&#12289;&#32447;&#24615;&#22238;&#24402;&#12289;&#36923;&#36753;&#22238;&#24402;&#12289;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#12289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#12290;KNN&#21644;RF&#22312;&#23545;&#27668;&#20307;&#28151;&#21512;&#29289;&#20013;&#30340;&#19981;&#21516;&#21464;&#21270;&#21270;&#23398;&#21697;&#36827;&#34892;&#20998;&#31867;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;99%&#12290;&#22312;&#22238;&#24402;&#20998;&#26512;&#20013;&#65292;KNN&#30340;&#32467;&#26524;&#26368;&#22909;&#65292;R2&#20540;&#36229;&#36807;0.99&#65292;LOD&#20540;&#20026;0.012&#12289;0.015&#12289;0.014&#21644;0.025 PPM&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of Volatile Organic Compounds (VOCs) from the breath is becoming a viable route for the early detection of diseases non-invasively. This paper presents a sensor array with three metal oxide electrodes that can use machine learning methods to identify four distinct VOCs in a mixture. The metal oxide sensor array was subjected to various VOC concentrations, including ethanol, acetone, toluene and chloroform. The dataset obtained from individual gases and their mixtures were analyzed using multiple machine learning algorithms, such as Random Forest (RF), K-Nearest Neighbor (KNN), Decision Tree, Linear Regression, Logistic Regression, Naive Bayes, Linear Discriminant Analysis, Artificial Neural Network, and Support Vector Machine. KNN and RF have shown more than 99% accuracy in classifying different varying chemicals in the gas mixtures. In regression analysis, KNN has delivered the best results with R2 value of more than 0.99 and LOD of 0.012, 0.015, 0.014 and 0.025 PPM for pred
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26446;&#23545;&#31216;&#23558;&#24322;&#26500;&#25968;&#25454;&#20013;&#30340;PDEs&#34920;&#31034;&#36827;&#34892;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;&#19981;&#21464;&#20219;&#21153;&#30340;&#24615;&#33021;&#24182;&#25913;&#36827;&#20102;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#26102;&#38388;&#25512;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05432</link><description>&lt;p&gt;
&#21033;&#29992;&#26446;&#23545;&#31216;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning with Lie Symmetries for Partial Differential Equations. (arXiv:2307.05432v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26446;&#23545;&#31216;&#23558;&#24322;&#26500;&#25968;&#25454;&#20013;&#30340;PDEs&#34920;&#31034;&#36827;&#34892;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;&#19981;&#21464;&#20219;&#21153;&#30340;&#24615;&#33021;&#24182;&#25913;&#36827;&#20102;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#26102;&#38388;&#25512;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#26041;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#20026;&#25968;&#20540;&#27714;&#35299;&#22120;&#25552;&#20379;&#20102;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#33021;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20135;&#29983;&#24191;&#27867;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#26045;&#32852;&#21512;&#23884;&#20837;&#26041;&#27861;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26694;&#26550;&#65292;&#20174;&#24322;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;PDEs&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#30340;&#34920;&#31034;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#22312;&#19981;&#21464;&#20219;&#21153;&#65288;&#22914;&#22238;&#24402;PDE&#30340;&#31995;&#25968;&#65289;&#19978;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#25552;&#39640;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#26102;&#38388;&#25512;&#36827;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#22312;&#26410;&#26469;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning for differential equations paves the way for computationally efficient alternatives to numerical solvers, with potentially broad impacts in science and engineering. Though current algorithms typically require simulated training data tailored to a given setting, one may instead wish to learn useful information from heterogeneous sources, or from real dynamical systems observations that are messy or incomplete. In this work, we learn general-purpose representations of PDEs from heterogeneous data by implementing joint embedding methods for self-supervised learning (SSL), a framework for unsupervised representation learning that has had notable success in computer vision. Our representation outperforms baseline approaches to invariant tasks, such as regressing the coefficients of a PDE, while also improving the time-stepping performance of neural solvers. We hope that our proposed methodology will prove useful in the eventual development of general-purpose foundation mode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24322;&#26041;&#24046;&#31070;&#32463;&#22238;&#24402;&#27169;&#22411;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#26102;&#30340;&#22256;&#38590;&#65292;&#24182;&#20174;&#32479;&#35745;&#29289;&#29702;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;&#20316;&#32773;&#35777;&#26126;&#20102;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#19981;&#20165;&#36866;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#32780;&#19988;&#24050;&#32463;&#22312;&#36807;&#21442;&#25968;&#21270;&#26465;&#20214;&#39640;&#26031;&#20284;&#28982;&#27169;&#22411;&#30340;&#22330;&#35770;&#20013;&#23384;&#22312;&#12290;&#25968;&#20540;&#27714;&#35299;&#32467;&#26524;&#19982;&#23454;&#35777;&#27169;&#22411;&#25311;&#21512;&#30340;&#23450;&#24615;&#19968;&#33268;&#24615;&#35777;&#26126;&#20102;&#30456;&#21464;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2306.16717</link><description>&lt;p&gt;
&#29702;&#35299;&#28145;&#24230;&#24322;&#26041;&#24046;&#22238;&#24402;&#30340;&#30149;&#24577;
&lt;/p&gt;
&lt;p&gt;
Understanding Pathologies of Deep Heteroskedastic Regression. (arXiv:2306.16717v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16717
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24322;&#26041;&#24046;&#31070;&#32463;&#22238;&#24402;&#27169;&#22411;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#26102;&#30340;&#22256;&#38590;&#65292;&#24182;&#20174;&#32479;&#35745;&#29289;&#29702;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;&#20316;&#32773;&#35777;&#26126;&#20102;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#19981;&#20165;&#36866;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#32780;&#19988;&#24050;&#32463;&#22312;&#36807;&#21442;&#25968;&#21270;&#26465;&#20214;&#39640;&#26031;&#20284;&#28982;&#27169;&#22411;&#30340;&#22330;&#35770;&#20013;&#23384;&#22312;&#12290;&#25968;&#20540;&#27714;&#35299;&#32467;&#26524;&#19982;&#23454;&#35777;&#27169;&#22411;&#25311;&#21512;&#30340;&#23450;&#24615;&#19968;&#33268;&#24615;&#35777;&#26126;&#20102;&#30456;&#21464;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#25253;&#21578;&#20102;&#22312;&#20351;&#29992;&#24322;&#26041;&#24046;&#31070;&#32463;&#22238;&#24402;&#27169;&#22411;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#24314;&#27169;&#26102;&#20986;&#29616;&#30340;&#36127;&#38754;&#32467;&#26524;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#20110;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#65292;&#22343;&#20540;&#32593;&#32476;&#21644;&#26041;&#24046;&#32593;&#32476;&#36275;&#22815;&#24378;&#22823;&#65292;&#21487;&#20197;&#25311;&#21512;&#27599;&#20010;&#25968;&#25454;&#28857;&#65288;&#21516;&#26102;&#23558;&#39044;&#27979;&#30340;&#26041;&#24046;&#25910;&#32553;&#21040;&#38646;&#65289;&#65292;&#25110;&#32773;&#23398;&#20064;&#19968;&#20010;&#24658;&#23450;&#30340;&#39044;&#27979;&#65292;&#36755;&#20986;&#26041;&#24046;&#24688;&#22909;&#21305;&#37197;&#27599;&#20010;&#39044;&#27979;&#27531;&#24046;&#65288;&#21363;&#23558;&#30446;&#26631;&#35299;&#37322;&#20026;&#32431;&#22122;&#22768;&#65289;&#12290;&#26412;&#25991;&#20174;&#32479;&#35745;&#29289;&#29702;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20123;&#22256;&#38590;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35266;&#23519;&#21040;&#30340;&#19981;&#31283;&#23450;&#24615;&#19981;&#29305;&#23450;&#20110;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#32780;&#26159;&#24050;&#32463;&#23384;&#22312;&#20110;&#36807;&#21442;&#25968;&#21270;&#26465;&#20214;&#39640;&#26031;&#20284;&#28982;&#27169;&#22411;&#30340;&#22330;&#35770;&#20013;&#12290;&#22312;&#36731;&#24494;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#25968;&#20540;&#27714;&#35299;&#30340;&#38750;&#21442;&#25968;&#33258;&#30001;&#33021;&#12290;&#24471;&#21040;&#30340;&#35299;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#30340;&#23454;&#35777;&#27169;&#22411;&#25311;&#21512;&#20855;&#26377;&#33391;&#22909;&#30340;&#23450;&#24615;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#29305;&#21035;&#35777;&#26126;&#20102;&#30456;&#21464;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent studies have reported negative results when using heteroskedastic neural regression models to model real-world data. In particular, for overparameterized models, the mean and variance networks are powerful enough to either fit every single data point (while shrinking the predicted variances to zero), or to learn a constant prediction with an output variance exactly matching every predicted residual (i.e., explaining the targets as pure noise). This paper studies these difficulties from the perspective of statistical physics. We show that the observed instabilities are not specific to any neural network architecture but are already present in a field theory of an overparameterized conditional Gaussian likelihood model. Under light assumptions, we derive a nonparametric free energy that can be solved numerically. The resulting solutions show excellent qualitative agreement with empirical model fits on real-world data and, in particular, prove the existence of phase transit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#26368;&#20248;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#26102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15056</link><description>&lt;p&gt;
&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#26368;&#20248;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Differentially Private Learning with Public Data. (arXiv:2306.15056v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#26368;&#20248;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#26102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#33021;&#22815;&#30830;&#20445;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19981;&#27844;&#28431;&#31169;&#23494;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24046;&#20998;&#38544;&#31169;&#30340;&#20195;&#20215;&#26159;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#25110;&#26679;&#26412;&#22797;&#26434;&#24230;&#22686;&#21152;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#21487;&#33021;&#21487;&#20197;&#35775;&#38382;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#36741;&#21161;&#20844;&#20849;&#25968;&#25454;&#12290;&#36825;&#20419;&#20351;&#20102;&#26368;&#36817;&#30740;&#31350;&#20844;&#20849;&#25968;&#25454;&#22312;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#26377;&#19968;&#23450;&#25968;&#37327;&#30340;&#20844;&#20849;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#20197;&#19979;&#22522;&#26412;&#24320;&#25918;&#38382;&#39064;&#65306;1.&#22312;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#22522;&#20110;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#30340;&#26368;&#20248;&#65288;&#26368;&#22351;&#24773;&#20917;&#65289;&#35823;&#24046;&#26159;&#22810;&#23569;&#65311;&#21738;&#20123;&#31639;&#27861;&#26159;&#26368;&#20248;&#30340;&#65311;2.&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#22312;&#23454;&#36341;&#20013;&#25913;&#36827;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#35757;&#32451;&#65311;&#25105;&#20204;&#22312;&#26412;&#22320;&#27169;&#22411;&#21644;&#20013;&#24515;&#27169;&#22411;&#30340;&#24046;&#20998;&#38544;&#31169;&#38382;&#39064;&#19979;&#32771;&#34385;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#19977;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#26368;&#20248;&#35823;&#24046;&#29575;&#30340;&#32039;&#23494;&#65288;&#26368;&#39640;&#24120;&#25968;&#22240;&#23376;&#65289;&#19979;&#30028;&#21644;&#19978;&#30028;&#12290;&#36825;&#19977;&#20010;&#38382;&#39064;&#26159;&#65306;&#22343;&#20540;&#20272;&#35745;&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#20984;&#22855;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy (DP) ensures that training a machine learning model does not leak private data. However, the cost of DP is lower model accuracy or higher sample complexity. In practice, we may have access to auxiliary public data that is free of privacy concerns. This has motivated the recent study of what role public data might play in improving the accuracy of DP models. In this work, we assume access to a given amount of public data and settle the following fundamental open questions: 1. What is the optimal (worst-case) error of a DP model trained over a private data set while having access to side public data? What algorithms are optimal? 2. How can we harness public data to improve DP model training in practice? We consider these questions in both the local and central models of DP. To answer the first question, we prove tight (up to constant factors) lower and upper bounds that characterize the optimal error rates of three fundamental problems: mean estimation, empirical ris
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#38480;&#65292;&#22312;&#26377;&#30028;&#21644;&#19968;&#33324;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#20013;&#22343;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30028;&#38480;&#36824;&#33021;&#22815;&#20445;&#25345;&#38543;&#26102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12214</link><description>&lt;p&gt;
&#26356;&#22810;&#30340;PAC-Bayes Bounds&#65306;&#20174;&#26377;&#30028;&#25439;&#22833;&#21040;&#20855;&#26377;&#19968;&#33324;&#24615;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#65292;&#21040;&#20219;&#20309;&#26102;&#38388;&#22343;&#26377;&#25928;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime-validity. (arXiv:2306.12214v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#38480;&#65292;&#22312;&#26377;&#30028;&#21644;&#19968;&#33324;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#20013;&#22343;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30028;&#38480;&#36824;&#33021;&#22815;&#20445;&#25345;&#38543;&#26102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25439;&#22833;&#25552;&#20986;&#20102;&#26032;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#38480;&#12290;&#39318;&#20808;&#65292;&#38024;&#23545;&#26377;&#30028;&#33539;&#22260;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Catoni&#30028;&#30340;&#21152;&#24378;&#29256;&#26412;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#21442;&#25968;&#20540;&#30340;&#32479;&#19968;&#30028;&#12290;&#36825;&#23548;&#33268;&#20102;&#26032;&#30340;&#24555;&#36895;&#36895;&#29575;&#21644;&#28151;&#21512;&#36895;&#29575;&#19978;&#38480;&#65292;&#36825;&#20123;&#19978;&#38480;&#21487;&#35299;&#37322;&#24615;&#24378;&#19988;&#27604;&#25991;&#29486;&#20013;&#20808;&#21069;&#30028;&#38480;&#26356;&#32039;&#12290;&#20854;&#27425;&#65292;&#38024;&#23545;&#26356;&#19968;&#33324;&#30340;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#26080;&#21442;&#25968;&#19978;&#38480;&#65306;&#24403;&#25439;&#22833;&#30340;&#32047;&#31215;&#29983;&#25104;&#20989;&#25968;&#26377;&#30028;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;PAC-Bayes Chernoff&#31867;&#27604;&#65292;&#21478;&#19968;&#20010;&#19978;&#38480;&#26159;&#25439;&#22833;&#30340;&#20108;&#38454;&#30697;&#26377;&#30028;&#12290;&#36825;&#20004;&#20010;&#19978;&#38480;&#26159;&#21033;&#29992;&#19968;&#31181;&#22522;&#20110;&#21487;&#33021;&#20107;&#20214;&#31354;&#38388;&#30340;&#31163;&#25955;&#21270;&#30340;&#26032;&#25216;&#26415;&#33719;&#24471;&#30340;&#65292;&#8220;&#22312;&#27010;&#29575;&#8221;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30028;&#38480;&#30340;&#31616;&#21333;&#25216;&#26415;&#23558;&#25152;&#26377;&#20808;&#21069;&#32467;&#26524;&#25193;&#23637;&#21040;&#20219;&#20309;&#26102;&#38388;&#26377;&#25928;&#30340;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present new high-probability PAC-Bayes bounds for different types of losses. Firstly, for losses with a bounded range, we present a strengthened version of Catoni's bound that holds uniformly for all parameter values. This leads to new fast rate and mixed rate bounds that are interpretable and tighter than previous bounds in the literature. Secondly, for losses with more general tail behaviors, we introduce two new parameter-free bounds: a PAC-Bayes Chernoff analogue when the loss' cumulative generating function is bounded, and a bound when the loss' second moment is bounded. These two bounds are obtained using a new technique based on a discretization of the space of possible events for the "in probability" parameter optimization problem. Finally, we extend all previous results to anytime-valid bounds using a simple technique applicable to any existing bound.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22522;&#20110;&#35013;&#37197;&#32447;&#25968;&#25454;&#30340;&#21322;&#21512;&#25104;&#21046;&#36896;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2306.10816</link><description>&lt;p&gt;
$\texttt{causalAssembly}$: &#29992;&#20110;&#22522;&#20934;&#22240;&#26524;&#21457;&#29616;&#30340;&#29983;&#25104;&#30495;&#23454;&#29983;&#20135;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
$\texttt{causalAssembly}$: Generating Realistic Production Data for Benchmarking Causal Discovery. (arXiv:2306.10816v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22522;&#20110;&#35013;&#37197;&#32447;&#25968;&#25454;&#30340;&#21322;&#21512;&#25104;&#21046;&#36896;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#24182;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#38752;&#28789;&#27963;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#26469;&#22788;&#29702;&#22797;&#26434;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#30495;&#23454;&#25968;&#25454;&#28304;&#20013;&#30495;&#27491;&#30340;&#22240;&#26524;&#20851;&#31995;&#20173;&#19981;&#20026;&#20154;&#25152;&#30693;&#65292;&#22240;&#27492;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#20805;&#20998;&#30340;&#32463;&#39564;&#39564;&#35777;&#12290;&#36825;&#20010;&#38382;&#39064;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#29615;&#32469;&#21512;&#36866;&#39640;&#36136;&#37327;&#25968;&#25454;&#21457;&#24067;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#32452;&#22797;&#26434;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#21046;&#36896;&#36807;&#31243;&#20013;&#35013;&#37197;&#32447;&#30340;&#27979;&#37327;&#25968;&#25454;&#12290;&#20511;&#21161;&#20110;&#23545;&#29289;&#29702;&#23398;&#30340;&#28145;&#20837;&#30740;&#31350;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#33021;&#22815;&#25552;&#20379;&#22320;&#38754;&#30340;&#22240;&#26524;&#20851;&#31995;&#23545;&#29031;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#35013;&#37197;&#32447;&#25968;&#25454;&#21644;&#30456;&#20851;&#30340;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#26469;&#26500;&#24314;&#19968;&#20010;&#31995;&#32479;&#65292;&#29983;&#25104;&#21322;&#21512;&#25104;&#36896;&#25968;&#25454;&#26469;&#25903;&#25345;&#22522;&#20934;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#20223;&#30495;&#25216;&#26415;&#26469;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#20223;&#21407;&#22987;&#35013;&#37197;&#32447;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#65292;&#20445;&#30041;&#20102;&#36807;&#31243;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#23427;&#20204;&#30340;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms for causal discovery have recently undergone rapid advances and increasingly draw on flexible nonparametric methods to process complex data. With these advances comes a need for adequate empirical validation of the causal relationships learned by different algorithms. However, for most real data sources true causal relations remain unknown. This issue is further compounded by privacy concerns surrounding the release of suitable high-quality data. To help address these challenges, we gather a complex dataset comprising measurements from an assembly line in a manufacturing context. This line consists of numerous physical processes for which we are able to provide ground truth causal relationships on the basis of a detailed study of the underlying physics. We use the assembly line data and associated ground truth information to build a system for generation of semisynthetic manufacturing data that supports benchmarking of causal discovery methods. To accomplish this, we employ 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#21270;&#38543;&#26426;&#29983;&#25104;&#26641;&#30340;GNN&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24471;&#27604;&#20854;&#20182;&#32463;&#20856;&#31639;&#27861;&#26356;&#24555;&#19988;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2306.04828</link><description>&lt;p&gt;
&#32447;&#24615;&#21270;&#38543;&#26426;&#29983;&#25104;&#26641;&#19982;GNN&#30340;&#24555;&#36895;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Fast and Effective GNN Training with Linearized Random Spanning Trees. (arXiv:2306.04828v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#21270;&#38543;&#26426;&#29983;&#25104;&#26641;&#30340;GNN&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24471;&#27604;&#20854;&#20182;&#32463;&#20856;&#31639;&#27861;&#26356;&#24555;&#19988;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32473;&#23450;&#22270;&#24418;&#32467;&#26500;&#25968;&#25454;&#30340;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#35757;&#32451;GNN&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32447;&#24615;&#21270;&#20174;&#36755;&#20837;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;&#38543;&#26426;&#29983;&#25104;&#26641;&#24471;&#21040;&#19968;&#31995;&#21015;&#36335;&#24452;&#22270;&#26469;&#36880;&#27493;&#31934;&#32454;&#21270;&#26435;&#37325;&#26356;&#26032;&#25805;&#20316;&#12290;&#36335;&#24452;&#22270;&#34987;&#35774;&#35745;&#20026;&#20445;&#30041;&#21407;&#22987;&#22270;&#30340;&#22522;&#26412;&#25299;&#25169;&#21644;&#33410;&#28857;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#36335;&#24452;&#22270;&#30340;&#31232;&#30095;&#24615;&#20351;&#24471;GNN&#35757;&#32451;&#26356;&#36731;&#20415;&#65292;&#38500;&#20102;&#21487;&#25193;&#23637;&#24615;&#22806;&#65292;&#36824;&#26377;&#21161;&#20110;&#32531;&#35299;&#36807;&#24230;&#21387;&#32553;&#21644;&#36807;&#24230;&#24179;&#28369;&#31561;&#32463;&#20856;&#35757;&#32451;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#22270;&#24418;&#21367;&#31215;&#32593;&#32476;&#65292;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#21644;&#27979;&#35797;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new effective and scalable framework for training GNNs in supervised node classification tasks, given graph-structured data. Our approach increasingly refines the weight update operations on a sequence of path graphs obtained by linearizing random spanning trees extracted from the input network. The path graphs are designed to retain essential topological and node information of the original graph. At the same time, the sparsity of path graphs enables a much lighter GNN training which, besides scalability, helps in mitigating classical training issues, like over-squashing and over-smoothing. We carry out an extensive experimental investigation on a number of real-world graph benchmarks, where we apply our framework to graph convolutional networks, showing simultaneous improvement of both training speed and test accuracy, as compared to well-known baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GromovMatcher&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#33258;&#21160;&#21512;&#24182;LC-MS&#25968;&#25454;&#38598;&#65292;&#21487;&#25552;&#39640;&#25968;&#25454;&#23545;&#40784;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#21512;&#24182;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.03218</link><description>&lt;p&gt;
&#29992;&#20110;&#26080;&#30446;&#26631;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#33258;&#21160;&#23545;&#40784;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Optimal transport for automatic alignment of untargeted metabolomic data. (arXiv:2306.03218v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GromovMatcher&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#33258;&#21160;&#21512;&#24182;LC-MS&#25968;&#25454;&#38598;&#65292;&#21487;&#25552;&#39640;&#25968;&#25454;&#23545;&#40784;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#21512;&#24182;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28082;&#30456;&#33394;&#35889;-&#36136;&#35889;&#65288;LC-MS&#65289;&#36890;&#36807;&#27979;&#37327;&#29983;&#29289;&#26631;&#26412;&#20013;&#30340;&#22823;&#37327;&#20195;&#35874;&#29289;&#25512;&#21160;&#33647;&#29289;&#30740;&#21457;&#65292;&#30142;&#30149;&#35786;&#26029;&#21644;&#39118;&#38505;&#39044;&#27979;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;LC-MS&#30340;&#20302;&#36890;&#37327;&#23545;&#20110;&#29983;&#29289;&#26631;&#35760;&#29289;&#21457;&#29616;&#65292;&#27880;&#37322;&#21644;&#23454;&#39564;&#27604;&#36739;&#26500;&#25104;&#20102;&#20027;&#35201;&#25361;&#25112;&#65292;&#38656;&#35201;&#21512;&#24182;&#22810;&#20010;&#25968;&#25454;&#38598;&#12290;&#24403;&#21069;&#30340;&#25968;&#25454;&#27744;&#21270;&#26041;&#27861;&#30001;&#20110;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#36229;&#21442;&#25968;&#20381;&#36182;&#24615;&#30340;&#33030;&#24369;&#24615;&#32780;&#36935;&#21040;&#23454;&#38469;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;GromovMatcher&#65292;&#19968;&#31181;&#28789;&#27963;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#33258;&#21160;&#32467;&#21512;LC-MS&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#24378;&#24230;&#30456;&#20851;&#32467;&#26500;&#65292;GromovMatcher&#25552;&#20379;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#23545;&#40784;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#35813;&#31639;&#27861;&#21487;&#25193;&#23637;&#21040;&#38656;&#35201;&#26368;&#23567;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#25968;&#21315;&#20010;&#29305;&#24449;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#32925;&#30284;&#21644;&#33008;&#33146;&#30284;&#30340;&#23454;&#39564;&#24739;&#32773;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Untargeted metabolomic profiling through liquid chromatography-mass spectrometry (LC-MS) measures a vast array of metabolites within biospecimens, advancing drug development, disease diagnosis, and risk prediction. However, the low throughput of LC-MS poses a major challenge for biomarker discovery, annotation, and experimental comparison, necessitating the merging of multiple datasets. Current data pooling methods encounter practical limitations due to their vulnerability to data variations and hyperparameter dependence. Here we introduce GromovMatcher, a flexible and user-friendly algorithm that automatically combines LC-MS datasets using optimal transport. By capitalizing on feature intensity correlation structures, GromovMatcher delivers superior alignment accuracy and robustness compared to existing approaches. This algorithm scales to thousands of features requiring minimal hyperparameter tuning. Applying our method to experimental patient studies of liver and pancreatic cancer, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#29702;&#35770;&#26469;&#25913;&#36827;&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#20998;&#26512;&#65292;&#25512;&#32763;&#20102;&#29616;&#26377;&#25216;&#26415;&#23545;&#36890;&#20449;&#22270;&#36127;&#38754;&#24433;&#21709;&#30340;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;D-SGD&#22312;&#20984;&#35774;&#32622;&#20013;&#19982;&#32463;&#20856;SGD&#31639;&#27861;&#27867;&#21270;&#30028;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2306.02939</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#19982;&#27867;&#21270;&#20998;&#26512;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Stability and Generalization Analysis of the Decentralized SGD Algorithm. (arXiv:2306.02939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#29702;&#35770;&#26469;&#25913;&#36827;&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#20998;&#26512;&#65292;&#25512;&#32763;&#20102;&#29616;&#26377;&#25216;&#26415;&#23545;&#36890;&#20449;&#22270;&#36127;&#38754;&#24433;&#21709;&#30340;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;D-SGD&#22312;&#20984;&#35774;&#32622;&#20013;&#19982;&#32463;&#20856;SGD&#31639;&#27861;&#27867;&#21270;&#30028;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#31639;&#27861;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(D-SGD)&#31639;&#27861;&#30340;&#26032;&#30340;&#27867;&#21270;&#35823;&#24046;&#20998;&#26512;&#26041;&#27861;&#12290;&#24471;&#21040;&#30340;&#32467;&#26524;&#22823;&#22823;&#25913;&#36827;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#25512;&#32763;&#20102;&#23427;&#20204;&#20851;&#20110;&#36890;&#20449;&#22270;&#23545;&#27867;&#21270;&#30340;&#36127;&#38754;&#24433;&#21709;&#30340;&#35266;&#28857;&#12290;&#20363;&#22914;&#65292;&#22312;&#20984;&#35774;&#32622;&#20013;&#65292;&#26080;&#35770;&#22270;&#30340;&#36873;&#25321;&#22914;&#20309;&#65292;D-SGD&#20855;&#26377;&#19982;&#32463;&#20856;SGD&#31639;&#27861;&#30456;&#21516;&#30340;&#27867;&#21270;&#30028;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#21453;&#30452;&#35273;&#30340;&#32467;&#26524;&#26469;&#33258;&#20110;&#32771;&#34385;&#26412;&#22320;&#21442;&#25968;&#30340;&#24179;&#22343;&#20540;&#65292;&#36825;&#20250;&#38544;&#34255;&#19968;&#20010;&#19982;&#20998;&#24067;&#24335;&#22330;&#26223;&#19981;&#20860;&#23481;&#30340;&#26368;&#32456;&#20840;&#23616;&#24179;&#22343;&#21270;&#27493;&#39588;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#20513;&#23548;&#20998;&#26512;&#26412;&#22320;&#21442;&#25968;&#30340;&#19978;&#30830;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22270;&#30830;&#23454;&#23545;&#27867;&#21270;&#20135;&#29983;&#24433;&#21709;&#12290;&#19982;&#20043;&#21069;&#30340;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#21363;&#20351;&#23545;&#20110;&#38750;&#36830;&#25509;&#22270;&#20063;&#33021;&#20135;&#29983;&#38750;&#24179;&#20961;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new generalization error analysis for the Decentralized Stochastic Gradient Descent (D-SGD) algorithm based on algorithmic stability. The obtained results largely improve upon state-of-the-art results, and even invalidate their claims that the communication graph has a detrimental effect on generalization. For instance, we show that in convex settings, D-SGD has the same generalization bounds as the classical SGD algorithm, no matter the choice of graph. We exhibit that this counter-intuitive result comes from considering the average of local parameters, which hides a final global averaging step incompatible with the decentralized scenario. In light of this observation, we advocate to analyze the supremum over local parameters and show that in this case, the graph does have an impact on the generalization. Unlike prior results, our analysis yields non-vacuous bounds even for non-connected graphs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#38469;&#20195;&#20215;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;&#8220;&#26377;&#23475;&#8221;&#26597;&#35810;&#30340;&#25968;&#37327;&#65292;&#25552;&#39640;&#20102;&#40657;&#30418;&#25915;&#20987;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.02895</link><description>&lt;p&gt;
&#19981;&#30772;&#22351;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#19979;&#35268;&#36991;&#23427;&#30340;&#20998;&#31867;&#8212;&#8212;&#22522;&#20110;&#23454;&#38469;&#20195;&#20215;&#30340;&#40657;&#30418;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Evading Black-box Classifiers Without Breaking Eggs. (arXiv:2306.02895v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#38469;&#20195;&#20215;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;&#8220;&#26377;&#23475;&#8221;&#26597;&#35810;&#30340;&#25968;&#37327;&#65292;&#25552;&#39640;&#20102;&#40657;&#30418;&#25915;&#20987;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20915;&#31574;&#30340;&#35268;&#36991;&#25915;&#20987;&#26159;&#36890;&#36807;&#19981;&#26029;&#26597;&#35810;&#40657;&#30418;&#20998;&#31867;&#22120;&#26469;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#26412;&#25991;&#35748;&#20026;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#24335;&#22312;&#22788;&#29702;&#23545;&#23433;&#20840;&#24615;&#25935;&#24863;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26102;&#26377;&#32570;&#38519;&#12290;&#22240;&#20026;&#36825;&#20123;&#31995;&#32479;&#20027;&#35201;&#30446;&#30340;&#26159;&#36807;&#28388;&#20986;&#26377;&#23475;&#25968;&#25454;&#65288;&#20363;&#22914;&#24694;&#24847;&#36719;&#20214;&#12289;&#26377;&#23475;&#20869;&#23481;&#31561;&#65289;&#65292;&#25152;&#20197;&#26597;&#35810;&#30340;&#20195;&#20215;&#26159;&#19981;&#23545;&#31561;&#30340;&#65292;&#19968;&#26086;&#26597;&#35810;&#34987;&#26816;&#27979;&#20986;&#26159;&#26377;&#23475;&#30340;&#65292;&#23601;&#20250;&#35302;&#21457;&#39069;&#22806;&#30340;&#23433;&#20840;&#36807;&#28388;&#65292;&#20363;&#22914;&#20351;&#29992;&#38480;&#21046;&#25110;&#36134;&#25143;&#26242;&#20572;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20915;&#31574;&#30340;&#25915;&#20987;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#8220;&#26377;&#23475;&#8221;&#26597;&#35810;&#65292;&#23548;&#33268;&#23427;&#20204;&#24456;&#21487;&#33021;&#23545;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#26080;&#25928;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#36890;&#36807;&#20943;&#23569;&#8220;&#26377;&#23475;&#8221;&#26597;&#35810;&#30340;&#25968;&#37327;&#65288;&#26368;&#22810;&#21487;&#20197;&#20943;&#23569; $1.5$ &#20493;&#21040; $7.3$ &#20493;&#65289;&#65292;&#20197;&#23454;&#29616;&#26356;&#21152;&#26377;&#25928;&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;&#20294;&#36825;&#20123;&#25915;&#20987;&#30340;&#27491;&#24120;&#26597;&#35810;&#25968;&#37327;&#22823;&#22823;&#22686;&#21152;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22312;&#23454;&#38469;&#20195;&#20215;&#24230;&#37327;&#19979;&#26500;&#24314;&#26356;&#26377;&#25928;&#30340;&#40657;&#30418;&#25915;&#20987;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-based evasion attacks repeatedly query a black-box classifier to generate adversarial examples. Prior work measures the cost of such attacks by the total number of queries made to the classifier. We argue this metric is flawed. Most security-critical machine learning systems aim to weed out "bad" data (e.g., malware, harmful content, etc). Queries to such systems carry a fundamentally asymmetric cost: queries detected as "bad" come at a higher cost because they trigger additional security filters, e.g., usage throttling or account suspension. Yet, we find that existing decision-based attacks issue a large number of "bad" queries, which likely renders them ineffective against security-critical systems. We then design new attacks that reduce the number of bad queries by $1.5$-$7.3\times$, but often at a significant increase in total (non-bad) queries. We thus pose it as an open problem to build black-box attacks that are more effective under realistic cost metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#38454;&#26021;&#21147;&#28145;&#24230;&#38598;&#25104; (FoRDE) &#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#36755;&#20837;&#26799;&#24230;&#26469;&#22686;&#24378;&#22810;&#26679;&#24615;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.02775</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#30340;&#36755;&#20837;&#26799;&#24230;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Input gradient diversity for neural network ensembles. (arXiv:2306.02775v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#38454;&#26021;&#21147;&#28145;&#24230;&#38598;&#25104; (FoRDE) &#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#36755;&#20837;&#26799;&#24230;&#26469;&#22686;&#24378;&#22810;&#26679;&#24615;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104; (DE) &#36890;&#36807;&#23427;&#20204;&#30340;&#21151;&#33021;&#22810;&#26679;&#24615;&#22312;&#20934;&#30830;&#24615;&#12289;&#26657;&#20934;&#24615;&#21644;&#25269;&#25239;&#24178;&#25200;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#22522;&#20110;&#31890;&#23376;&#30340;&#21464;&#20998;&#25512;&#26029; (ParVI) &#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#32593;&#32476;&#30456;&#20284;&#24615;&#20869;&#26680;&#30340;&#25490;&#26021;&#39033;&#26469;&#22686;&#24378;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#26435;&#37325;&#31354;&#38388;&#25490;&#26021;&#26159;&#20302;&#25928;&#30340;&#65292;&#32780;&#30452;&#25509;&#21151;&#33021;&#31354;&#38388;&#25490;&#26021;&#34987;&#21457;&#29616;&#23545; DE &#30340;&#25913;&#36827;&#24456;&#23567;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110; ParVI &#30340;&#19968;&#38454;&#26021;&#21147;&#28145;&#24230;&#38598;&#25104; (FoRDE)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#26799;&#24230;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#12290;&#30001;&#20110;&#36755;&#20837;&#26799;&#24230;&#21807;&#19968;&#22320;&#30830;&#23450;&#20102;&#19968;&#20010;&#20989;&#25968;&#24182;&#19988;&#27604;&#26435;&#37325;&#23567;&#24471;&#22810;&#65292;&#25152;&#20197;&#36825;&#31181;&#26041;&#27861;&#20445;&#35777;&#20102;&#38598;&#21512;&#25104;&#21592;&#22312;&#21151;&#33021;&#19978;&#26159;&#19981;&#21516;&#30340;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#22810;&#26679;&#21270;&#36755;&#20837;&#26799;&#24230;&#40723;&#21169;&#27599;&#20010;&#32593;&#32476;&#23398;&#20064;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#36825;&#26377;&#26395;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Ensembles (DEs) demonstrate improved accuracy, calibration and robustness to perturbations over single neural networks partly due to their functional diversity. Particle-based variational inference (ParVI) methods enhance diversity by formalizing a repulsion term based on a network similarity kernel. However, weight-space repulsion is inefficient due to over-parameterization, while direct function-space repulsion has been found to produce little improvement over DEs. To sidestep these difficulties, we propose First-order Repulsive Deep Ensemble (FoRDE), an ensemble learning method based on ParVI, which performs repulsion in the space of first-order input gradients. As input gradients uniquely characterize a function up to translation and are much smaller in dimension than the weights, this method guarantees that ensemble members are functionally different. Intuitively, diversifying the input gradients encourages each network to learn different features, which is expected to improv
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#28857;&#21608;&#22260;&#26377;&#22823;&#30340;&#20960;&#20046;&#30830;&#23450;&#30340;&#32622;&#20449;&#37051;&#22495;&#65292;&#36825;&#23548;&#33268;&#29616;&#20195;&#27169;&#22411;&#26657;&#20934;&#38754;&#20020;&#37325;&#35201;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2306.00740</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19968;&#33268;&#32622;&#20449;&#29616;&#35937;&#21450;&#20854;&#23545;&#26657;&#20934;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
A Uniform Confidence Phenomenon in Deep Learning and its Implications for Calibration. (arXiv:2306.00740v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00740
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#28857;&#21608;&#22260;&#26377;&#22823;&#30340;&#20960;&#20046;&#30830;&#23450;&#30340;&#32622;&#20449;&#37051;&#22495;&#65292;&#36825;&#23548;&#33268;&#29616;&#20195;&#27169;&#22411;&#26657;&#20934;&#38754;&#20020;&#37325;&#35201;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#24778;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23649;&#27425;&#34920;&#29616;&#20986;&#22312;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#20272;&#35745;&#19981;&#20339;&#30340;&#24773;&#20917;&#8212;&#8212;&#25442;&#21477;&#35805;&#35828;&#65292;&#23427;&#20204;&#22312;&#38169;&#35823;&#26102;&#32463;&#24120;&#36807;&#24230;&#33258;&#20449;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#27169;&#22411;&#26657;&#20934;&#65292;&#24182;&#20197;&#20462;&#25913;&#35757;&#32451;&#26041;&#26696;&#21644;&#35757;&#32451;&#21518;&#26657;&#20934;&#31243;&#24207;&#30340;&#24418;&#24335;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29616;&#20195;&#27169;&#22411;&#26657;&#20934;&#30340;&#37325;&#35201;&#38556;&#30861;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#28857;&#21608;&#22260;&#26377;&#22823;&#30340;&#20960;&#20046;&#30830;&#23450;&#30340;&#32622;&#20449;&#37051;&#22495;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#24456;&#22810;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#23545;&#20013;&#37117;&#20250;&#20986;&#29616;&#65288;&#22312;&#22270;&#20687;&#20998;&#31867;&#30340;&#32972;&#26223;&#19979;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#36825;&#31181;&#29616;&#35937;&#20986;&#29616;&#26102;&#65292;&#22312;&#31867;&#21035;&#20043;&#38388;&#23384;&#22312;&#37325;&#21472;&#30340;&#22823;&#31867;&#25968;&#25454;&#20998;&#24067;&#20013;&#65292;&#21363;&#20351;&#22312;&#24212;&#29992;&#26657;&#20934;&#21518;&#20063;&#19981;&#33021;&#33719;&#24471;&#27604;&#38543;&#26426;&#26356;&#22909;&#30340;&#28176;&#36817;&#26657;&#20934;&#27169;&#22411;&#65288;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive generalization capabilities of deep neural networks, they have been repeatedly shown to poorly estimate their predictive uncertainty - in other words, they are frequently overconfident when they are wrong. Fixing this issue is known as model calibration, and has consequently received much attention in the form of modified training schemes and post-training calibration procedures. In this work, we present a significant hurdle to the calibration of modern models: deep neural networks have large neighborhoods of almost certain confidence around their training points. We demonstrate in our experiments that this phenomenon consistently arises (in the context of image classification) across many model and dataset pairs. Furthermore, we prove that when this phenomenon holds, for a large class of data distributions with overlaps between classes, it is not possible to obtain a model that is asymptotically better than random (with respect to calibration) even after applyin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#27169;&#25311;&#26102;&#38388;&#28436;&#21270;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#37319;&#26679;&#26469;&#36866;&#24212;&#27874;&#20989;&#25968;&#30340;&#28508;&#22312;&#20302;&#32500;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#38543;&#26426;&#37327;&#23376;&#21147;&#23398;&#26041;&#31243;&#65292;&#20855;&#26377;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.19685</link><description>&lt;p&gt;
&#28145;&#24230;&#38543;&#26426;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Deep Stochastic Mechanics. (arXiv:2305.19685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#27169;&#25311;&#26102;&#38388;&#28436;&#21270;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#37319;&#26679;&#26469;&#36866;&#24212;&#27874;&#20989;&#25968;&#30340;&#28508;&#22312;&#20302;&#32500;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#38543;&#26426;&#37327;&#23376;&#21147;&#23398;&#26041;&#31243;&#65292;&#20855;&#26377;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#27169;&#25311;&#26102;&#38388;&#28436;&#21270;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#21463;&#38543;&#26426;&#21147;&#23398;&#21644;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#21551;&#21457;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#20174;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#20013;&#37319;&#26679;&#26469;&#36866;&#24212;&#27874;&#20989;&#25968;&#28508;&#22312;&#30340;&#20302;&#32500;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#26356;&#39640;&#30340;&#32500;&#24230;&#19978;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#38543;&#26426;&#37327;&#23376;&#21147;&#23398;&#26041;&#31243;&#65292;&#32467;&#26524;&#20855;&#26377;&#19982;&#32500;&#25968;&#25968;&#37327;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#24182;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#29992;&#20110;&#37327;&#23376;&#21147;&#23398;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel deep-learning-based approach for numerical simulation of a time-evolving Schr\"odinger equation inspired by stochastic mechanics and generative diffusion models. Unlike existing approaches, which exhibit computational complexity that scales exponentially in the problem dimension, our method allows us to adapt to the latent low-dimensional structure of the wave function by sampling from the Markovian diffusion. Depending on the latent dimension, our method may have far lower computational complexity in higher dimensions. Moreover, we propose novel equations for stochastic quantum mechanics, resulting in linear computational complexity with respect to the number of dimensions. Numerical simulations verify our theoretical findings and show a significant advantage of our method compared to other deep-learning-based approaches used for quantum mechanics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25200;&#21160;&#36741;&#21161;&#26679;&#26412;&#21512;&#25104;&#65288;PASS&#65289;&#26041;&#27861;&#65292;&#21487;&#20174;&#22797;&#26434;&#25968;&#25454;&#20013;&#32472;&#21046;&#21487;&#38752;&#32467;&#35770;&#65292;&#24182;&#36890;&#36807;&#20272;&#35745;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#21644;&#33945;&#29305;&#21345;&#32599;&#23454;&#39564;&#35777;&#26126;&#20219;&#20309;&#32479;&#35745;&#25968;&#25454;&#30340;&#20272;&#35745;&#20998;&#24067;&#12290;&#36827;&#19968;&#27493;&#25512;&#20986;&#25200;&#21160;&#36741;&#21161;&#25512;&#29702;&#65288;PAI&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#26377;&#25928;&#24615;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.18671</link><description>&lt;p&gt;
&#25200;&#21160;&#36741;&#21161;&#26679;&#26412;&#21512;&#25104;&#65306;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Perturbation-Assisted Sample Synthesis: A Novel Approach for Uncertainty Quantification. (arXiv:2305.18671v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25200;&#21160;&#36741;&#21161;&#26679;&#26412;&#21512;&#25104;&#65288;PASS&#65289;&#26041;&#27861;&#65292;&#21487;&#20174;&#22797;&#26434;&#25968;&#25454;&#20013;&#32472;&#21046;&#21487;&#38752;&#32467;&#35770;&#65292;&#24182;&#36890;&#36807;&#20272;&#35745;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#21644;&#33945;&#29305;&#21345;&#32599;&#23454;&#39564;&#35777;&#26126;&#20219;&#20309;&#32479;&#35745;&#25968;&#25454;&#30340;&#20272;&#35745;&#20998;&#24067;&#12290;&#36827;&#19968;&#27493;&#25512;&#20986;&#25200;&#21160;&#36741;&#21161;&#25512;&#29702;&#65288;PAI&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#26377;&#25928;&#24615;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25200;&#21160;&#36741;&#21161;&#26679;&#26412;&#21512;&#25104;&#65288;PASS&#65289;&#8221;&#30340;&#26032;&#22411;&#29983;&#25104;&#22120;&#65292;&#26088;&#22312;&#20174;&#22797;&#26434;&#25968;&#25454;&#20013;&#32472;&#21046;&#21487;&#38752;&#30340;&#32467;&#35770;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31561;&#39640;&#32423;&#24314;&#27169;&#25216;&#26415;&#26102;&#12290; PASS&#21033;&#29992;&#25200;&#21160;&#29983;&#25104;&#38752;&#36817;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#21253;&#25324;&#25968;&#23383;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#31867;&#22411;&#65292;&#22914;&#22522;&#22240;&#34920;&#36798;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#36890;&#36807;&#20272;&#35745;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#24182;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;PASS&#25552;&#39640;&#20102;&#20272;&#35745;&#31934;&#24230;&#65292;&#24182;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#23454;&#39564;&#35777;&#26126;&#20102;&#20219;&#20309;&#32479;&#35745;&#25968;&#25454;&#30340;&#20272;&#35745;&#20998;&#24067;&#12290;&#22522;&#20110;PASS&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#25512;&#29702;&#26694;&#26550;&#31216;&#20026;&#8220;&#25200;&#21160;&#36741;&#21161;&#25512;&#29702;&#65288;PAI&#65289;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#26377;&#25928;&#24615;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#22312;&#20851;&#38190;&#25512;&#29702;&#20013;&#65292;PAI&#20351;&#24471;&#22312;&#19981;&#30693;&#36947;&#24341;&#23548;&#20998;&#24067;&#65288;&#22914;&#27169;&#25311;&#20013;&#65289;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#24471;&#20986;&#20934;&#30830;&#30340;&#32467;&#35770;&#65292;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#30340;&#25968;&#25454;&#12290;&#22312;&#38750;&#20851;&#38190;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35757;&#32451;PASS&#20351;&#29992;&#20013;&#38388;&#21464;&#37327;&#25554;&#34917;&#31574;&#30053;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#25991;&#26412;&#29983;&#25104;&#65292;PASS&#21644;PAI&#37117;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel generator called Perturbation-Assisted Sample Synthesis (PASS), designed for drawing reliable conclusions from complex data, especially when using advanced modeling techniques like deep neural networks. PASS utilizes perturbation to generate synthetic data that closely mirrors the distribution of raw data, encompassing numerical and unstructured data types such as gene expression, images, and text. By estimating the data-generating distribution and leveraging large pre-trained generative models, PASS enhances estimation accuracy, providing an estimated distribution of any statistic through Monte Carlo experiments. Building on PASS, we propose a generative inference framework called Perturbation-Assisted Inference (PAI), which offers a statistical guarantee of validity. In pivotal inference, PAI enables accurate conclusions without knowing a pivotal's distribution as in simulations, even with limited data. In non-pivotal situations, we train PASS using an i
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20613;&#37324;&#21494;&#21464;&#25442;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32452;&#30340;&#28508;&#22312;&#32447;&#24615;&#20316;&#29992;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#38544;&#34255;&#32467;&#26500;&#30340;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2305.18484</link><description>&lt;p&gt;
&#31070;&#32463;&#20613;&#37324;&#21494;&#21464;&#25442;&#65306;&#31561;&#21464;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural Fourier Transform: A General Approach to Equivariant Representation Learning. (arXiv:2305.18484v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18484
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20613;&#37324;&#21494;&#21464;&#25442;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32452;&#30340;&#28508;&#22312;&#32447;&#24615;&#20316;&#29992;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#38544;&#34255;&#32467;&#26500;&#30340;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#21462;&#25968;&#25454;&#38544;&#34255;&#32467;&#26500;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20854;&#20013;&#31561;&#21464;&#20851;&#31995;&#27010;&#24565;&#36215;&#30528;&#20013;&#24515;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30740;&#31350;&#37117;&#24314;&#31435;&#22312;&#24314;&#31569;&#29702;&#35770;&#21644;&#23545;&#25968;&#25454;&#24418;&#24335;&#30340;&#30456;&#24212;&#20551;&#35774;&#20043;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;NFT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#32452;&#30340;&#28508;&#22312;&#32447;&#24615;&#20316;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#32780;&#26080;&#38656;&#20551;&#35774;&#20851;&#20110;&#32452;&#22914;&#20309;&#20316;&#29992;&#20110;&#25968;&#25454;&#30340;&#26174;&#24335;&#30693;&#35782;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;NFT&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#34920;&#26126;&#31561;&#21464;&#29305;&#24449;&#30340;&#23384;&#22312;&#65292;&#21363;&#22312;&#31561;&#21464;&#24615;&#23398;&#20064;&#20013;&#26222;&#36941;&#20551;&#23450;&#30340;&#65292;&#31561;&#20215;&#20110;&#25968;&#25454;&#31354;&#38388;&#20013;&#23384;&#22312;&#19968;&#32452;&#19981;&#21464;&#26680;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#23454;&#39564;&#32467;&#26524;&#65292;&#28436;&#31034;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#20851;&#20110;&#25805;&#20316;&#32452;&#30340;&#30693;&#35782;&#30340;&#20856;&#22411;&#22330;&#26223;&#20013;&#24212;&#29992;NFT&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetry learning has proven to be an effective approach for extracting the hidden structure of data, with the concept of equivariance relation playing the central role. However, most of the current studies are built on architectural theory and corresponding assumptions on the form of data. We propose Neural Fourier Transform (NFT), a general framework of learning the latent linear action of the group without assuming explicit knowledge of how the group acts on data. We present the theoretical foundations of NFT and show that the existence of a linear equivariant feature, which has been assumed ubiquitously in equivariance learning, is equivalent to the existence of a group invariant kernel on the dataspace. We also provide experimental results to demonstrate the application of NFT in typical scenarios with varying levels of knowledge about the acting group.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#20799;&#31461;&#33041;&#32959;&#30244;&#30340;BraTS&#25361;&#25112;&#65292;&#26088;&#22312;&#35780;&#20272;&#20799;&#31461;&#33041;&#33014;&#36136;&#30244;&#30340;&#20307;&#31215;&#20998;&#21106;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#20799;&#31461;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#32959;&#30244;&#26159;&#20799;&#31461;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#24182;&#19988;&#23545;&#36825;&#20123;&#23454;&#20307;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.17033</link><description>&lt;p&gt;
&#12298;&#22823;&#33041;&#32959;&#30244;&#20998;&#21106;&#65288;BraTS&#65289;&#25361;&#25112;2023&#65306;&#20851;&#27880;&#20799;&#31185;&#65288;CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs&#65289;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs). (arXiv:2305.17033v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17033
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#20799;&#31461;&#33041;&#32959;&#30244;&#30340;BraTS&#25361;&#25112;&#65292;&#26088;&#22312;&#35780;&#20272;&#20799;&#31461;&#33041;&#33014;&#36136;&#30244;&#30340;&#20307;&#31215;&#20998;&#21106;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#20799;&#31461;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#32959;&#30244;&#26159;&#20799;&#31461;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#24182;&#19988;&#23545;&#36825;&#20123;&#23454;&#20307;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#32959;&#30244;&#26159;&#20799;&#31461;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#26368;&#24120;&#35265;&#21407;&#22240;&#12290;&#20799;&#31461;&#39640;&#32423;&#21035;&#33014;&#36136;&#30244;&#30340;&#20116;&#24180;&#29983;&#23384;&#29575;&#19981;&#21040;20&#65285;&#12290;&#30001;&#20110;&#32597;&#35265;&#65292;&#23545;&#36825;&#20123;&#23454;&#20307;&#30340;&#35786;&#26029;&#36890;&#24120;&#20250;&#24310;&#36831;&#65292;&#20854;&#27835;&#30103;&#20027;&#35201;&#22522;&#20110;&#21382;&#21490;&#27835;&#30103;&#29702;&#24565;&#65292;&#24182;&#19988;&#20020;&#24202;&#35797;&#39564;&#38656;&#35201;&#22810;&#26426;&#26500;&#21512;&#20316;&#12290;MICCAI&#22823;&#33041;&#32959;&#30244;&#20998;&#21106;&#65288;BraTS&#65289;&#25361;&#25112;&#26159;&#19968;&#20010;&#37324;&#31243;&#30865;&#24335;&#30340;&#31038;&#21306;&#22522;&#20934;&#20107;&#20214;&#65292;&#24050;&#32463;&#25104;&#21151;&#21019;&#24314;&#36164;&#28304;12&#24180;&#65292;&#29992;&#20110;&#25104;&#20154;&#33014;&#36136;&#30244;&#30340;&#20998;&#21106;&#21644;&#20998;&#26512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023&#25361;&#25112;&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#20799;&#31461;&#33041;&#32959;&#30244;&#30340;BraTS&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;&#22810;&#20010;&#22269;&#38469;&#21512;&#20316;&#32452;&#32455;&#19987;&#27880;&#20110;&#20799;&#31185;&#31070;&#32463;&#32959;&#30244;&#21644;&#20020;&#24202;&#35797;&#39564;&#30340;&#25968;&#25454;&#12290;BraTS-PEDs 2023&#25361;&#25112;&#20391;&#37325;&#20110;&#35780;&#20272;&#29992;&#20110;&#20799;&#31461;&#33041;&#33014;&#36136;&#30244;&#30340;&#20307;&#31215;&#20998;&#21106;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pediatric tumors of the central nervous system are the most common cause of cancer-related death in children. The five-year survival rate for high-grade gliomas in children is less than 20\%. Due to their rarity, the diagnosis of these entities is often delayed, their treatment is mainly based on historic treatment concepts, and clinical trials require multi-institutional collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a landmark community benchmark event with a successful history of 12 years of resource creation for the segmentation and analysis of adult glioma. Here we present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which represents the first BraTS challenge focused on pediatric brain tumors with data acquired across multiple international consortia dedicated to pediatric neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on benchmarking the development of volumentric segmentation algorithms for pediatric brain gli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#21464;&#37327;&#22788;&#29702;&#24773;&#20917;&#19979;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25972;&#20010;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#25512;&#26029;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#26576;&#20123;&#32479;&#35745;&#37327;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#25919;&#31574;&#21046;&#23450;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.15742</link><description>&lt;p&gt;
&#26102;&#38388;&#21464;&#21270;&#22788;&#29702;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Generative Models for Time-Varying Treatments. (arXiv:2305.15742v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#21464;&#37327;&#22788;&#29702;&#24773;&#20917;&#19979;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25972;&#20010;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#25512;&#26029;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#26576;&#20123;&#32479;&#35745;&#37327;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#25919;&#31574;&#21046;&#23450;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#26159;&#27979;&#35797;&#26032;&#30103;&#27861;&#30340;&#24120;&#29992;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#24179;&#22343;&#25928;&#24212;&#20250;&#25513;&#30422;&#21453;&#20107;&#23454;&#20998;&#24067;&#20013;&#37325;&#35201;&#30340;&#20010;&#20307;&#29305;&#24449;&#65292;&#21487;&#33021;&#20250;&#24341;&#36215;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#26102;&#38388;&#35774;&#32622;&#20013;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#22788;&#29702;&#26159;&#26102;&#24207;&#30340;&#21644;&#26102;&#21464;&#30340;&#65292;&#23545;&#21453;&#20107;&#23454;&#20998;&#24067;&#20135;&#29983;&#20102;&#38169;&#32508;&#22797;&#26434;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#25429;&#33719;&#25972;&#20010;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#20801;&#35768;&#23545;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#26576;&#20123;&#32479;&#35745;&#37327;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#12290;&#36825;&#20351;&#24471;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23588;&#20854;&#36866;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#25919;&#31574;&#21046;&#23450;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#36890;&#36807;&#36793;&#38469;&#32467;&#26500;&#27169;&#22411;&#35880;&#24910;&#22320;&#35299;&#20915;&#20102;&#35266;&#23519;&#25968;&#25454;&#21644;&#30446;&#26631;&#21453;&#20107;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating average causal effects is a common practice to test new treatments. However, the average effect ''masks'' important individual characteristics in the counterfactual distribution, which may lead to safety, fairness, and ethical concerns. This issue is exacerbated in the temporal setting, where the treatment is sequential and time-varying, leading to an intricate influence on the counterfactual distribution. In this paper, we propose a novel conditional generative modeling approach to capture the whole counterfactual distribution, allowing efficient inference on certain statistics of the counterfactual distribution. This makes the proposed approach particularly suitable for healthcare and public policy making. Our generative modeling approach carefully tackles the distribution mismatch in the observed data and the targeted counterfactual distribution via a marginal structural model. Our method outperforms state-of-the-art baselines on both synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#35760;&#26102;&#38388;&#28857;&#36807;&#31243;&#20013;&#25552;&#21462;&#20854;&#32479;&#35745;&#30452;&#35273;&#30340;&#20107;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#22120;&#20197;&#21382;&#21490;&#35266;&#23519;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#21487;&#33021;&#21457;&#29983;&#30340;&#39640;&#36136;&#37327;&#38543;&#21518;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#25928;&#12289;&#28789;&#27963;&#21644;&#34920;&#31034;&#33021;&#21147;&#31561;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.12569</link><description>&lt;p&gt;
&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#26159;&#26631;&#35760;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#24517;&#22791;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional Generative Modeling is All You Need for Marked Temporal Point Processes. (arXiv:2305.12569v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#35760;&#26102;&#38388;&#28857;&#36807;&#31243;&#20013;&#25552;&#21462;&#20854;&#32479;&#35745;&#30452;&#35273;&#30340;&#20107;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#22120;&#20197;&#21382;&#21490;&#35266;&#23519;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#21487;&#33021;&#21457;&#29983;&#30340;&#39640;&#36136;&#37327;&#38543;&#21518;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#25928;&#12289;&#28789;&#27963;&#21644;&#34920;&#31034;&#33021;&#21147;&#31561;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24314;&#27169;&#30340;&#36827;&#27493;&#20351;&#24471;&#20174;&#19978;&#19979;&#25991;&#20449;&#24687;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#20869;&#23481;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#22914;&#20309;&#25945;&#27169;&#22411;&#30693;&#36947;&#20309;&#26102;&#29983;&#25104;&#20869;&#23481;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#26631;&#35760;&#26102;&#38388;&#28857;&#36807;&#31243;&#20013;&#25552;&#21462;&#20854;&#32479;&#35745;&#30452;&#35273;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#24178;&#20928;&#12289;&#28789;&#27963;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#28041;&#21450;&#22810;&#32500;&#26631;&#35760;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#25105;&#20204;&#26088;&#22312;&#25429;&#25417;&#28857;&#36807;&#31243;&#30340;&#20998;&#24067;&#32780;&#19981;&#38656;&#26126;&#30830;&#25351;&#23450;&#26465;&#20214;&#24378;&#24230;&#25110;&#27010;&#29575;&#23494;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#22120;&#65292;&#20197;&#20107;&#20214;&#21382;&#21490;&#20026;&#36755;&#20837;&#24182;&#29983;&#25104;&#22312;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#20107;&#20214;&#19979;&#65292;&#21487;&#33021;&#21457;&#29983;&#30340;&#39640;&#36136;&#37327;&#38543;&#21518;&#20107;&#20214;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#21033;&#30410;&#65292;&#21253;&#25324;&#22312;&#23398;&#20064;&#27169;&#22411;&#21644;&#29983;&#25104;&#26679;&#26412;&#26041;&#38754;&#30340;&#24322;&#24120;&#25928;&#29575;&#20197;&#21450;&#30456;&#24403;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#26469;&#25429;&#25417;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in generative modeling have made it possible to generate high-quality content from context information, but a key question remains: how to teach models to know when to generate content? To answer this question, this study proposes a novel event generative model that draws its statistical intuition from marked temporal point processes, and offers a clean, flexible, and computationally efficient solution for a wide range of applications involving multi-dimensional marks. We aim to capture the distribution of the point process without explicitly specifying the conditional intensity or probability density. Instead, we use a conditional generator that takes the history of events as input and generates the high-quality subsequent event that is likely to occur given the prior observations. The proposed framework offers a host of benefits, including exceptional efficiency in learning the model and generating samples, as well as considerable representational power to capture
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#24847;&#26102;&#24310;&#30340;&#38750;&#31283;&#24577;&#22312;&#32447;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;DOGD&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#33719;&#24471;$O(\sqrt{dT}(P_T+1))$&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#21516;&#26102;&#24403;&#24310;&#36831;&#19981;&#25913;&#21464;&#26799;&#24230;&#21040;&#36798;&#39034;&#24207;&#26102;&#65292;&#33258;&#21160;&#23558;&#21160;&#24577;&#36951;&#25022;&#20943;&#23569;&#21040;$O(\sqrt{S}(1+P_T))$&#12290;</title><link>http://arxiv.org/abs/2305.12131</link><description>&lt;p&gt;
&#20219;&#24847;&#26102;&#24310;&#30340;&#38750;&#31283;&#24577;&#22312;&#32447;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-stationary Online Convex Optimization with Arbitrary Delays. (arXiv:2305.12131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#24847;&#26102;&#24310;&#30340;&#38750;&#31283;&#24577;&#22312;&#32447;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;DOGD&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#33719;&#24471;$O(\sqrt{dT}(P_T+1))$&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#21516;&#26102;&#24403;&#24310;&#36831;&#19981;&#25913;&#21464;&#26799;&#24230;&#21040;&#36798;&#39034;&#24207;&#26102;&#65292;&#33258;&#21160;&#23558;&#21160;&#24577;&#36951;&#25022;&#20943;&#23569;&#21040;$O(\sqrt{S}(1+P_T))$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20197;&#26799;&#24230;&#25110;&#20854;&#20182;&#20989;&#25968;&#20449;&#24687;&#21487;&#20197;&#20219;&#24847;&#24310;&#36831;&#20026;&#29305;&#28857;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#19982;&#20043;&#21069;&#30740;&#31350;&#31283;&#24577;&#29615;&#22659;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#24310;&#36831;OCO&#65292;&#24182;&#26088;&#22312;&#26368;&#23567;&#21270;&#19982;&#20219;&#20309;&#27604;&#36739;&#22120;&#24207;&#21015;&#30456;&#20851;&#30340;&#21160;&#24577;&#36951;&#25022;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21363;DOGD&#65292;&#35813;&#31639;&#27861;&#26681;&#25454;&#20854;&#21040;&#36798;&#39034;&#24207;&#20026;&#27599;&#20010;&#24310;&#36831;&#26799;&#24230;&#25191;&#34892;&#28176;&#21464;&#19979;&#38477;&#27493;&#39588;&#12290;&#23613;&#31649;&#23427;&#24456;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#30340;&#26032;&#22411;&#20998;&#26512;&#34920;&#26126;&#65292;DOGD&#21487;&#20197;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#33719;&#24471;$O(\sqrt{dT}(P_T+1))$&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;$d$&#26159;&#26368;&#22823;&#24310;&#36831;&#65292;$T$&#26159;&#26102;&#38388;&#36328;&#24230;&#65292;$P_T$&#26159;&#27604;&#36739;&#22120;&#30340;&#36335;&#24452;&#38271;&#24230;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#24310;&#36831;&#19981;&#25913;&#21464;&#28176;&#21464;&#30340;&#21040;&#36798;&#39034;&#24207;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#23558;&#21160;&#24577;&#36951;&#25022;&#20943;&#23569;&#21040;$O(\sqrt{S}(1+P_T))$&#65292;&#20854;&#20013;$S$&#26159;&#24310;&#36831;&#20043;&#21644;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;DOGD&#25193;&#23637;&#20026;&#26356;&#36890;&#29992;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#23454;&#29616;&#20102;&#19982;DOGD&#30456;&#21516;&#30340;&#36951;&#25022;&#30028;&#12290;&#24191;&#27867;&#30340;&#27169;&#25311;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online convex optimization (OCO) with arbitrary delays, in which gradients or other information of functions could be arbitrarily delayed, has received increasing attention recently. Different from previous studies that focus on stationary environments, this paper investigates the delayed OCO in non-stationary environments, and aims to minimize the dynamic regret with respect to any sequence of comparators. To this end, we first propose a simple algorithm, namely DOGD, which performs a gradient descent step for each delayed gradient according to their arrival order. Despite its simplicity, our novel analysis shows that DOGD can attain an $O(\sqrt{dT}(P_T+1)$ dynamic regret bound in the worst case, where $d$ is the maximum delay, $T$ is the time horizon, and $P_T$ is the path length of comparators. More importantly, in case delays do not change the arrival order of gradients, it can automatically reduce the dynamic regret to $O(\sqrt{S}(1+P_T))$, where $S$ is the sum of delays. Furtherm
&lt;/p&gt;</description></item><item><title>iCaloFlow&#26159;&#19968;&#20010;&#22522;&#20110;&#24402;&#32435;&#27969;&#30340;&#24555;&#36895;&#25506;&#27979;&#22120;&#27169;&#25311;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#39640;&#36798;&#20197;&#24448;10-100&#20493;&#30340;&#20998;&#36776;&#29575;&#36827;&#34892;&#24555;&#36895;&#12289;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2305.11934</link><description>&lt;p&gt;
&#22522;&#20110;&#24402;&#32435;&#27969;&#30340;&#24555;&#36895;&#31890;&#23376;&#25506;&#27979;&#22120;&#27169;&#25311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Inductive CaloFlow. (arXiv:2305.11934v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11934
&lt;/p&gt;
&lt;p&gt;
iCaloFlow&#26159;&#19968;&#20010;&#22522;&#20110;&#24402;&#32435;&#27969;&#30340;&#24555;&#36895;&#25506;&#27979;&#22120;&#27169;&#25311;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#39640;&#36798;&#20197;&#24448;10-100&#20493;&#30340;&#20998;&#36776;&#29575;&#36827;&#34892;&#24555;&#36895;&#12289;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#31890;&#23376;&#25506;&#27979;&#22120;&#21709;&#24212;&#26159;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#35745;&#31639;&#27969;&#31243;&#20013;&#26368;&#26114;&#36149;&#30340;&#27493;&#39588;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24402;&#19968;&#21270;&#27969;&#21487;&#20197;&#21152;&#24555;&#27492;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#31934;&#24230;&#27700;&#24179;&#65292;&#20294;&#23558;&#27492;&#26041;&#27861;&#25193;&#23637;&#21040;&#19982;&#26410;&#26469;&#25506;&#27979;&#22120;&#21319;&#32423;&#30456;&#20851;&#30340;&#26356;&#39640;&#20998;&#36776;&#29575;&#26102;&#20250;&#23548;&#33268;&#38480;&#21046;&#24615;&#30340;&#20869;&#23384;&#32422;&#26463;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#24402;&#32435;&#31995;&#21015;&#24402;&#19968;&#21270;&#27969;&#30340;&#24555;&#36895;&#25506;&#27979;&#22120;&#27169;&#25311;&#26694;&#26550;iCaloFlow&#65292;&#23427;&#26159;&#22312;&#25104;&#23545;&#30340;&#36830;&#32493;&#33021;&#37327;&#27785;&#31215;&#23618;&#20013;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#22686;&#21152;&#37319;&#26679;&#36895;&#24230;&#32780;&#19981;&#22833;&#34920;&#29616;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#24072;&#29983;&#33976;&#39311;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;CaloChallenge2022&#30340;&#25968;&#25454;&#38598;2&#21644;&#25968;&#25454;&#38598;3&#20013;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;iCaloFlow&#21487;&#20197;&#23454;&#29616;&#24402;&#19968;&#21270;&#27969;&#22312;&#36827;&#34892;&#24555;&#36895;&#12289;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#26102;&#30340;&#28508;&#21147;&#65292;&#36825;&#20123;&#27169;&#25311;&#23545;&#24212;&#30340;&#25506;&#27979;&#22120;&#20960;&#20309;&#32422;&#27604;&#20197;&#21069;&#32771;&#34385;&#30340;&#39640;10-100&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating particle detector response is the single most expensive step in the Large Hadron Collider computational pipeline. Recently it was shown that normalizing flows can accelerate this process while achieving unprecedented levels of accuracy, but scaling this approach up to higher resolutions relevant for future detector upgrades leads to prohibitive memory constraints. To overcome this problem, we introduce Inductive CaloFlow (iCaloFlow), a framework for fast detector simulation based on an inductive series of normalizing flows trained on the pattern of energy depositions in pairs of consecutive calorimeter layers. We further use a teacher-student distillation to increase sampling speed without loss of expressivity. As we demonstrate with Datasets 2 and 3 of the CaloChallenge2022, iCaloFlow can realize the potential of normalizing flows in performing fast, high-fidelity simulation on detector geometries that are ~ 10 - 100 times higher granularity than previously considered.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#22270;&#19978;&#23450;&#20041;&#30340;&#36716;&#31227;&#31639;&#23376;&#21450;&#20854;&#35889;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#24191;&#20041;&#36716;&#31227;&#31639;&#23376;&#30340;&#26377;&#21521;&#22270;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11766</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#36716;&#31227;&#31639;&#23376;&#65306;&#35889;&#32858;&#31867;&#21450;&#20854;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Transfer operators on graphs: Spectral clustering and beyond. (arXiv:2305.11766v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#22270;&#19978;&#23450;&#20041;&#30340;&#36716;&#31227;&#31639;&#23376;&#21450;&#20854;&#35889;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#24191;&#20041;&#36716;&#31227;&#31639;&#23376;&#30340;&#26377;&#21521;&#22270;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21644;&#32593;&#32476;&#22312;&#24314;&#27169;&#21644;&#20998;&#26512;&#22797;&#26434;&#30340;&#30456;&#20851;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20363;&#22914;&#20132;&#36890;&#32593;&#32476;&#65292;&#38598;&#25104;&#30005;&#36335;&#65292;&#30005;&#21147;&#32593;&#26684;&#65292;&#24341;&#25991;&#22270;&#20197;&#21450;&#29983;&#29289;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#22312;&#22270;&#19978;&#23450;&#20041;&#20102;&#36716;&#31227;&#31639;&#23376;&#65292;&#22914;Koopman&#31639;&#23376;&#21644;Perron-Frobenius&#31639;&#23376;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#35889;&#29305;&#24615;&#65292;&#24341;&#20837;&#20102;&#36825;&#20123;&#31639;&#23376;&#30340;Galerkin&#25237;&#24433;&#65292;&#24182;&#35828;&#26126;&#20102;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#38477;&#20302;&#34920;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#21521;&#22270;&#35889;&#32858;&#31867;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;Koopman&#31639;&#23376;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#24191;&#20041;&#36716;&#31227;&#31639;&#23376;&#30340;&#26377;&#21521;&#22270;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#35777;&#26126;&#20102;&#25152;&#24471;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19981;&#21516;&#32858;&#31867;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs and networks play an important role in modeling and analyzing complex interconnected systems such as transportation networks, integrated circuits, power grids, citation graphs, and biological and artificial neural networks. Graph clustering algorithms can be used to detect groups of strongly connected vertices and to derive coarse-grained models. We define transfer operators such as the Koopman operator and the Perron-Frobenius operator on graphs, study their spectral properties, introduce Galerkin projections of these operators, and illustrate how reduced representations can be estimated from data. In particular, we show that spectral clustering of undirected graphs can be interpreted in terms of eigenfunctions of the Koopman operator and propose novel clustering algorithms for directed graphs based on generalized transfer operators. We demonstrate the efficacy of the resulting algorithms on several benchmark problems and provide different interpretations of clusters.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;MARL&#31639;&#27861;&#65292;BiLL&#65292;&#23427;&#23398;&#20064;&#19968;&#20010;&#21452;&#23618;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#35813;&#27169;&#22411;&#22312;&#39030;&#23618;&#23398;&#20064;&#20840;&#23616;&#29366;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24213;&#23618;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#28508;&#22312;&#36712;&#36857;&#29992;&#20110;&#31574;&#30053;&#23398;&#20064;&#12290;&#22312;SMAC&#21644;Flatland&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#22522;&#20934;&#31639;&#27861;&#65292;&#21253;&#25324;&#22312;Super Hard SMAC&#22320;&#22270;&#19978;&#12290;</title><link>http://arxiv.org/abs/2304.06011</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21452;&#23618;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Bi-level Latent Variable Model for Sample-Efficient Multi-Agent Reinforcement Learning. (arXiv:2304.06011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06011
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;MARL&#31639;&#27861;&#65292;BiLL&#65292;&#23427;&#23398;&#20064;&#19968;&#20010;&#21452;&#23618;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#35813;&#27169;&#22411;&#22312;&#39030;&#23618;&#23398;&#20064;&#20840;&#23616;&#29366;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24213;&#23618;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#28508;&#22312;&#36712;&#36857;&#29992;&#20110;&#31574;&#30053;&#23398;&#20064;&#12290;&#22312;SMAC&#21644;Flatland&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#22522;&#20934;&#31639;&#27861;&#65292;&#21253;&#25324;&#22312;Super Hard SMAC&#22320;&#22270;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#31639;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#36890;&#24120;&#20855;&#26377;&#36739;&#39640;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;MARL&#31639;&#27861;&#65306;BiLL(&#21452;&#23618;&#28508;&#21464;&#37327;&#27169;&#22411;&#23398;&#20064;)&#65292;&#35813;&#31639;&#27861;&#20174;&#39640;&#32500;&#24230;&#36755;&#20837;&#20013;&#23398;&#20064;&#21452;&#23618;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#12290;&#22312;&#39030;&#23618;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#20840;&#23616;&#29366;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#32534;&#30721;&#19982;&#34892;&#20026;&#23398;&#20064;&#30456;&#20851;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#22312;&#24213;&#23618;&#65292;&#27169;&#22411;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#32473;&#23450;&#26469;&#33258;&#39030;&#23618;&#30340;&#20840;&#23616;&#28508;&#22312;&#34920;&#31034;&#12290;&#27169;&#22411;&#29983;&#25104;&#28508;&#22312;&#36712;&#36857;&#20197;&#29992;&#20110;&#31574;&#30053;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;SMAC&#21644;Flatland&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#22522;&#20934;&#31639;&#27861;&#65292;&#21253;&#25324;&#22312;&#20004;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;Super Hard SMAC&#22320;&#22270;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their potential in real-world applications, multi-agent reinforcement learning (MARL) algorithms often suffer from high sample complexity. To address this issue, we present a novel model-based MARL algorithm, BiLL (Bi-Level Latent Variable Model-based Learning), that learns a bi-level latent variable model from high-dimensional inputs. At the top level, the model learns latent representations of the global state, which encode global information relevant to behavior learning. At the bottom level, it learns latent representations for each agent, given the global latent representations from the top level. The model generates latent trajectories to use for policy learning. We evaluate our algorithm on complex multi-agent tasks in the challenging SMAC and Flatland environments. Our algorithm outperforms state-of-the-art model-free and model-based baselines in sample efficiency, including on two extremely challenging Super Hard SMAC maps.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20248;&#21270;&#20102;&#19968;&#31181;&#40657;&#30418;&#20132;&#26131;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312; DA &#33021;&#28304;&#24066;&#22330;&#19978;&#30001;&#20013;&#22411;&#29983;&#20135;&#32773;&#33258;&#21160;&#36827;&#34892;&#20132;&#26131;&#12290;</title><link>http://arxiv.org/abs/2303.16266</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33021;&#28304;&#20132;&#26131;&#31574;&#30053;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning for optimization of energy trading strategy. (arXiv:2303.16266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20248;&#21270;&#20102;&#19968;&#31181;&#40657;&#30418;&#20132;&#26131;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312; DA &#33021;&#28304;&#24066;&#22330;&#19978;&#30001;&#20013;&#22411;&#29983;&#20135;&#32773;&#33258;&#21160;&#36827;&#34892;&#20132;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#33021;&#28304;&#26469;&#33258;&#22823;&#37327;&#23567;&#22411;&#29983;&#20135;&#32773;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#36825;&#20123;&#26469;&#28304;&#30340;&#25928;&#29575;&#26159;&#19981;&#31283;&#23450;&#30340;&#65292;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#20063;&#26159;&#38543;&#26426;&#30340;&#65292;&#21152;&#21095;&#20102;&#33021;&#28304;&#24066;&#22330;&#24179;&#34913;&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#22269;&#23478;&#65292;&#36825;&#31181;&#24179;&#34913;&#26159;&#22312;&#39044;&#27979;&#26085;&#65288;DA&#65289;&#33021;&#28304;&#24066;&#22330;&#19978;&#23436;&#25104;&#30340;&#12290;&#26412;&#25991;&#32771;&#34385;&#30001;&#20013;&#22411;&#29983;&#20135;&#32773;&#22312;DA&#33021;&#28304;&#24066;&#22330;&#19978;&#30340;&#33258;&#21160;&#21270;&#20132;&#26131;&#12290;&#25105;&#20204;&#23558;&#27492;&#27963;&#21160;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#35268;&#33539;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;&#21487;&#20197;&#20351;&#29992;&#29616;&#23454;&#25968;&#25454;&#20248;&#21270;&#21363;&#29992;&#31574;&#30053;&#12290;&#25105;&#20204;&#21512;&#25104;&#21442;&#25968;&#21270;&#20132;&#26131;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#20248;&#21270;&#23427;&#20204;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20248;&#21270;&#19968;&#20010;&#40657;&#30418;&#20132;&#26131;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21033;&#29992;&#26469;&#33258;&#29615;&#22659;&#30340;&#21487;&#29992;&#20449;&#24687;&#26469;&#24433;&#21709;&#26410;&#26469;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
An increasing part of energy is produced from renewable sources by a large number of small producers. The efficiency of these sources is volatile and, to some extent, random, exacerbating the energy market balance problem. In many countries, that balancing is performed on day-ahead (DA) energy markets. In this paper, we consider automated trading on a DA energy market by a medium size prosumer. We model this activity as a Markov Decision Process and formalize a framework in which a ready-to-use strategy can be optimized with real-life data. We synthesize parametric trading strategies and optimize them with an evolutionary algorithm. We also use state-of-the-art reinforcement learning algorithms to optimize a black-box trading strategy fed with available information from the environment that can impact future prices.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;Few-Shot&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#65292;&#21457;&#29616;&#26410;&#26631;&#35760;&#25968;&#25454;&#23545;&#25552;&#39640;&#21322;&#30417;&#30563;FSOD&#26377;&#22909;&#22788;&#12290;&#21463;&#27492;&#21457;&#29616;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SoftER Teacher&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#25913;&#36827;FSOD&#65292;&#19981;&#38656;&#35201;&#20016;&#23500;&#30340;&#26631;&#31614;&#65292;&#24182;&#33021;&#22312;&#24615;&#33021;&#26041;&#38754;&#36229;&#36234;&#24378;&#26377;&#21147;&#30340;&#30417;&#30563;&#26816;&#27979;&#22120;&#65292;&#32780;&#19988;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2303.05739</link><description>&lt;p&gt;
&#20351;&#29992;SoftER Teacher&#22686;&#24378;&#21322;&#30417;&#30563;Few-Shot&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Boosting Semi-Supervised Few-Shot Object Detection with SoftER Teacher. (arXiv:2303.05739v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;Few-Shot&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#65292;&#21457;&#29616;&#26410;&#26631;&#35760;&#25968;&#25454;&#23545;&#25552;&#39640;&#21322;&#30417;&#30563;FSOD&#26377;&#22909;&#22788;&#12290;&#21463;&#27492;&#21457;&#29616;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SoftER Teacher&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#25913;&#36827;FSOD&#65292;&#19981;&#38656;&#35201;&#20016;&#23500;&#30340;&#26631;&#31614;&#65292;&#24182;&#33021;&#22312;&#24615;&#33021;&#26041;&#38754;&#36229;&#36234;&#24378;&#26377;&#21147;&#30340;&#30417;&#30563;&#26816;&#27979;&#22120;&#65292;&#32780;&#19988;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Few-shot&#30446;&#26631;&#26816;&#27979;&#65288;FSOD&#65289;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#26816;&#27979;&#26032;&#27010;&#24565;&#12290;&#29616;&#26377;&#30340;FSOD&#26041;&#27861;&#20551;&#23450;&#26377;&#20016;&#23500;&#30340;&#22522;&#30784;&#26631;&#31614;&#26469;&#36866;&#24212;&#26032;&#23545;&#35937;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;FSOD&#20219;&#21153;&#65292;&#32771;&#34385;&#21040;&#22522;&#30784;&#21644;&#26032;&#26631;&#31614;&#21516;&#26102;&#24456;&#23569;&#30340;&#29616;&#23454;&#24773;&#20917;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#36890;&#36807;&#21306;&#22495;&#25552;&#35758;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#21322;&#30417;&#30563;FSOD&#30340;&#26174;&#30528;&#33021;&#21147;&#12290;&#21463;&#27492;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SoftER Teacher&#65292;&#19968;&#31181;&#32467;&#21512;&#21306;&#22495;&#25552;&#35758;&#19978;&#30340;&#20266;&#26631;&#35760;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#24378;&#22823;&#26816;&#27979;&#22120;&#65292;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#25913;&#36827;FSOD&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20016;&#23500;&#30340;&#26631;&#31614;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SoftER Teacher&#36229;&#36234;&#20102;&#24378;&#26377;&#21147;&#30340;&#30417;&#30563;&#26816;&#27979;&#22120;&#30340;&#26032;&#24615;&#33021;&#65292;&#20165;&#20351;&#29992;&#25152;&#38656;&#22522;&#30784;&#26631;&#31614;&#30340;10&#65285;&#65292;&#32780;&#19981;&#20250;&#20986;&#29616;&#20043;&#21069;&#26041;&#27861;&#20013;&#35266;&#23519;&#21040;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#25581;&#31034;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;Few-Shot&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#28508;&#22312;&#20851;&#31995;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot object detection (FSOD) is an emerging problem aimed at detecting novel concepts from few exemplars. Existing approaches to FSOD assume abundant base labels to adapt to novel objects. This paper studies the task of semi-supervised FSOD by considering a realistic scenario in which both base and novel labels are simultaneously scarce. We explore the utility of unlabeled data and discover its remarkable ability to boost semi-supervised FSOD by way of region proposals. Motivated by this finding, we introduce SoftER Teacher, a robust detector combining pseudo-labeling with representation learning on region proposals, to harness unlabeled data for improved FSOD without relying on abundant labels. Extensive experiments show that SoftER Teacher surpasses the novel performance of a strong supervised detector using only 10% of required base labels, without experiencing catastrophic forgetting observed in prior approaches. Our work also sheds light on a potential relationship between sem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36882;&#24402;&#35745;&#31639;&#36817;&#20284;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20195;&#29702;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2211.01595</link><description>&lt;p&gt;
&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning in Non-Markovian Environments. (arXiv:2211.01595v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36882;&#24402;&#35745;&#31639;&#36817;&#20284;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20195;&#29702;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;Van Roy&#21450;&#20854;&#21512;&#20316;&#32773;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#22312;&#20219;&#24847;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#65292;&#24182;&#26126;&#30830;&#20102;&#24403;&#22312;&#35813;&#33539;&#24335;&#19978;&#24212;&#29992;Q&#23398;&#20064;&#31639;&#27861;&#26102;&#65292;&#30001;&#20110;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#24341;&#36215;&#30340;&#38169;&#35823;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25105;&#20204;&#24314;&#35758;&#20195;&#29702;&#35774;&#35745;&#30340;&#26631;&#20934;&#24212;&#26159;&#23547;&#25214;&#26576;&#20123;&#26465;&#20214;&#35268;&#24459;&#30340;&#33391;&#22909;&#36817;&#20284;&#12290;&#21463;&#32463;&#20856;&#38543;&#26426;&#25511;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#38382;&#39064;&#24402;&#32467;&#20026;&#36882;&#24402;&#35745;&#31639;&#36817;&#20284;&#20805;&#20998;&#32479;&#35745;&#37327;&#30340;&#38382;&#39064;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20195;&#29702;&#35774;&#35745;&#26041;&#26696;&#65292;&#25105;&#20204;&#22312;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#25968;&#20540;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the novel paradigm developed by Van Roy and coauthors for reinforcement learning in arbitrary non-Markovian environments, we propose a related formulation and explicitly pin down the error caused by non-Markovianity of observations when the Q-learning algorithm is applied on this formulation. Based on this observation, we propose that the criterion for agent design should be to seek good approximations for certain conditional laws. Inspired by classical stochastic control, we show that our problem reduces to that of recursive computation of approximate sufficient statistics. This leads to an autoencoder-based scheme for agent design which is then numerically tested on partially observed reinforcement learning environments.
&lt;/p&gt;</description></item></channel></rss>