<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>CBQ&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37325;&#26500;&#22411;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;CBQ&#36890;&#36807;&#20351;&#29992;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#24314;&#31435;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;CBQ&#36824;&#37319;&#29992;&#20102;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#30340;&#21462;&#25972;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2312.07950</link><description>&lt;p&gt;
&#36328;&#22359;&#37327;&#21270;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CBQ: Cross-Block Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.07950
&lt;/p&gt;
&lt;p&gt;
CBQ&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37325;&#26500;&#22411;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;CBQ&#36890;&#36807;&#20351;&#29992;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#24314;&#31435;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;CBQ&#36824;&#37319;&#29992;&#20102;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#30340;&#21462;&#25972;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#22312;&#20197;&#26497;&#20302;&#25104;&#26412;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#21482;&#20851;&#27880;&#22788;&#29702;&#21333;&#20010;&#23618;&#25110;&#21333;&#20010;&#22359;&#20869;&#30340;&#24322;&#24120;&#20540;&#65292;&#24573;&#30053;&#20102;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#20302;&#20301;&#35774;&#32622;&#20013;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22359;&#38388;&#37325;&#26500;&#30340;&#36328;&#22359;PTQ&#26041;&#27861;CBQ&#12290;CBQ&#37319;&#29992;&#20102;&#19968;&#31181;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#23454;&#29616;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;&#27492;&#22806;&#65292;CBQ&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#65288;CFP&#65289;&#26469;&#25233;&#21046;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#30340;&#24322;&#24120;&#20540;&#65292;&#24182;&#37197;&#21512;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;LoRA&#21462;&#25972;&#25216;&#26415;&#23454;&#29616;&#31934;&#30830;&#30340;&#26435;&#37325;&#37327;&#21270;&#12290;&#36825;&#20123;&#21019;&#26032;&#20351;CBQ&#19981;&#20165;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#65292;&#36824;&#33021;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CBQ&#22312;&#20302;&#20301;&#37327;&#21270;&#65288;W4A4&#65292;W4A8&#31561;&#65289;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) has played a key role in compressing large language models (LLMs) with ultra-low costs. However, existing PTQ methods only focus on handling the outliers within one layer or one block, which ignores the dependency of blocks and leads to severe performance degradation in low-bit settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ employs a cross-block dependency using a homologous reconstruction scheme, establishing long-range dependencies across multiple blocks to minimize error accumulation. Furthermore, CBQ incorporates a coarse-to-fine preprocessing (CFP) strategy for suppressing weight and activation outliers, coupled with an adaptive LoRA-Rounding technique for precise weight quantization. These innovations enable CBQ to not only handle extreme outliers effectively but also improve overall quantization accuracy. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W
&lt;/p&gt;</description></item><item><title>LightningNet&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20998;&#24067;&#24335;&#22270;&#24418;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#32454;&#32990;&#32593;&#32476;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#22312;&#25903;&#25345;&#29289;&#32852;&#32593;&#21644;&#36793;&#32536;&#35774;&#22791;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.18810</link><description>&lt;p&gt;
LightningNet&#65306;&#22522;&#20110;&#20998;&#24067;&#24335;&#22270;&#30340;&#36793;&#32536;&#32454;&#32990;&#32593;&#32476;&#24615;&#33021;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
LightningNet: Distributed Graph-based Cellular Network Performance Forecasting for the Edge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18810
&lt;/p&gt;
&lt;p&gt;
LightningNet&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20998;&#24067;&#24335;&#22270;&#24418;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#32454;&#32990;&#32593;&#32476;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#22312;&#25903;&#25345;&#29289;&#32852;&#32593;&#21644;&#36793;&#32536;&#35774;&#22791;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26426;&#32593;&#32476;&#22312;&#25552;&#20379;&#20114;&#32852;&#32593;&#25509;&#20837;&#26041;&#38754;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#22240;&#20026;&#23427;&#26159;&#21807;&#19968;&#20855;&#26377;&#26222;&#36941;&#31227;&#21160;&#24615;&#25903;&#25345;&#30340;&#20840;&#29699;&#35268;&#27169;&#22522;&#30784;&#35774;&#26045;&#12290;&#20026;&#20102;&#31649;&#29702;&#21644;&#32500;&#25252;&#22823;&#35268;&#27169;&#32593;&#32476;&#65292;&#31227;&#21160;&#32593;&#32476;&#36816;&#33829;&#21830;&#38656;&#35201;&#21450;&#26102;&#20449;&#24687;&#65292;&#29978;&#33267;&#20934;&#30830;&#30340;&#24615;&#33021;&#39044;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LightningNet&#30340;&#36731;&#37327;&#32423;&#20998;&#24067;&#24335;&#22522;&#20110;&#22270;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#32454;&#32990;&#32593;&#32476;&#24615;&#33021;&#65292;&#21487;&#20197;&#25429;&#33719;&#32593;&#32476;&#27969;&#37327;&#20013;&#20986;&#29616;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;LightningNet&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#25216;&#26415;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31867;&#20284;&#30340;&#36164;&#28304;&#20351;&#29992;&#37197;&#32622;&#25991;&#20214;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#24605;&#24819;&#36824;&#22312;&#20110;&#29305;&#21035;&#35774;&#35745;&#20026;&#25903;&#25345;&#29289;&#32852;&#32593;&#21644;&#36793;&#32536;&#35774;&#22791;&#65292;&#22312;&#19982;NVIDIA Jetson&#36827;&#34892;&#24615;&#33021;&#23454;&#39564;&#21518;&#65292;&#25105;&#20204;&#30340;&#34920;&#29616;&#29978;&#33267;&#36229;&#36807;&#20102;&#24403;&#21069;&#39046;&#20808;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18810v1 Announce Type: cross  Abstract: The cellular network plays a pivotal role in providing Internet access, since it is the only global-scale infrastructure with ubiquitous mobility support. To manage and maintain large-scale networks, mobile network operators require timely information, or even accurate performance forecasts. In this paper, we propose LightningNet, a lightweight and distributed graph-based framework for forecasting cellular network performance, which can capture spatio-temporal dependencies that arise in the network traffic. LightningNet achieves a steady performance increase over state-of-the-art forecasting techniques, while maintaining a similar resource usage profile. Our architecture ideology also excels in the respect that it is specifically designed to support IoT and edge devices, giving us an even greater step ahead of the current state-of-the-art, as indicated by our performance experiments with NVIDIA Jetson.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20026;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#19988;&#21463;ViT&#23884;&#20837;&#26465;&#20214;&#32422;&#26463;&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18807</link><description>&lt;p&gt;
ECoDepth: &#26377;&#25928;&#35843;&#25972;&#25193;&#25955;&#27169;&#22411;&#20197;&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20026;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#19988;&#21463;ViT&#23884;&#20837;&#26465;&#20214;&#32422;&#26463;&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#20047;&#35270;&#24046;&#32447;&#32034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#65288;SIDE&#65289;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#22270;&#20687;&#20013;&#30340;&#38452;&#24433;&#21644;&#19978;&#19979;&#25991;&#32447;&#32034;&#12290;&#25105;&#20204;&#20174;&#24050;&#26377;&#30740;&#31350;&#30340;&#21551;&#21457;&#20013;&#25506;&#35752;&#20351;&#29992;&#20174;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20197;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#30340;SIDE&#27169;&#22411;&#65292;&#20854;&#21463;&#21040;ViT&#23884;&#20837;&#30340;&#26465;&#20214;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18807v1 Announce Type: cross  Abstract: In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embedding
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.18802</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Long-form factuality in large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#24320;&#25918;&#24615;&#20027;&#39064;&#30340;&#20107;&#23454;&#24615;&#25552;&#31034;&#26102;&#65292;&#32463;&#24120;&#29983;&#25104;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#22312;&#24320;&#25918;&#39046;&#22495;&#20013;&#23545;&#27169;&#22411;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;GPT-4&#29983;&#25104;&#20102;&#19968;&#20010;&#21517;&#20026;LongFact&#30340;&#25552;&#31034;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#21315;&#20010;&#22218;&#25324;38&#20010;&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;Search-Augmented Factuality Evaluator&#65288;SAFE&#65289;&#30340;&#26041;&#27861;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#33258;&#21160;&#35780;&#20272;&#22120;&#12290;SAFE&#21033;&#29992;LLM&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#19968;&#32452;&#21333;&#29420;&#30340;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#20197;&#21450;&#30830;&#23450;&#19968;&#20010;&#20107;&#23454;&#26159;&#21542;&#24471;&#21040;&#25628;&#32034;&#32467;&#26524;&#25903;&#25345;&#30340;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#26469;&#35780;&#20272;&#27599;&#20010;&#20107;&#23454;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#35758;&#23558;F1&#20998;&#25968;&#25193;&#23637;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24179;&#34913;&#20102;&#22238;&#24212;&#20013;&#25903;&#25345;&#20107;&#23454;&#30340;&#30334;&#20998;&#27604;&#65288;&#31934;&#24230;&#65289;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18802v1 Announce Type: cross  Abstract: Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#26448;&#26009;&#22270;&#20687;&#65292;&#24314;&#31435;&#20102;ImageNet-D&#22522;&#20934;&#35780;&#20272;&#28145;&#24230;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#19968;&#31995;&#21015;&#35270;&#35273;&#27169;&#22411;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;&#39640;&#36798;60%&#12290;</title><link>https://arxiv.org/abs/2403.18775</link><description>&lt;p&gt;
ImageNet-D: &#22312;&#25193;&#25955;&#21512;&#25104;&#23545;&#35937;&#19978;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#26448;&#26009;&#22270;&#20687;&#65292;&#24314;&#31435;&#20102;ImageNet-D&#22522;&#20934;&#35780;&#20272;&#28145;&#24230;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#19968;&#31995;&#21015;&#35270;&#35273;&#27169;&#22411;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;&#39640;&#36798;60%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#35270;&#35273;&#24863;&#30693;&#40065;&#26834;&#24615;&#24314;&#31435;&#20102;&#20005;&#26684;&#30340;&#22522;&#20934;&#12290;&#21512;&#25104;&#22270;&#20687;&#65292;&#22914;ImageNet-C&#12289;ImageNet-9&#21644;Stylized ImageNet&#65292;&#25552;&#20379;&#20102;&#23545;&#21512;&#25104;&#30772;&#22351;&#12289;&#32972;&#26223;&#21644;&#32441;&#29702;&#30340;&#29305;&#23450;&#31867;&#22411;&#35780;&#20272;&#65292;&#28982;&#32780;&#36825;&#20123;&#40065;&#26834;&#24615;&#22522;&#20934;&#21463;&#38480;&#20110;&#25351;&#23450;&#30340;&#21464;&#20307;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#21512;&#25104;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#21512;&#25104;&#38590;&#22270;&#20687;&#30340;&#25968;&#25454;&#28304;&#26469;&#35780;&#20272;&#28145;&#24230;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#27604;&#20219;&#20309;&#20808;&#21069;&#24037;&#20316;&#26356;&#22810;&#26679;&#21270;&#30340;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#26448;&#26009;&#22270;&#20687;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#22522;&#20934;&#31216;&#20026;ImageNet-D&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ImageNet-D&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#35270;&#35273;&#27169;&#22411;&#30340;&#26174;&#33879;&#20934;&#30830;&#24615;&#19979;&#38477;&#65292;&#20174;&#26631;&#20934;ResNet&#35270;&#35273;&#20998;&#31867;&#22120;&#21040;&#26368;&#26032;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;CLIP&#21644;MiniGPT-4&#65292;&#23558;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#38477;&#20302;&#20102;&#39640;&#36798;60\%&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18775v1 Announce Type: cross  Abstract: We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models' robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\%. Our work suggests that diffusion models 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#23383;&#27700;&#21360;&#26694;&#26550;RAW&#65292;&#23558;&#21487;&#23398;&#20064;&#30340;&#27700;&#21360;&#30452;&#25509;&#24341;&#20837;&#21040;&#21407;&#22987;&#22270;&#29255;&#25968;&#25454;&#20013;&#65292;&#25903;&#25345;&#21508;&#31181;&#29983;&#25104;&#26550;&#26500;&#24182;&#25552;&#20379;&#20102;&#23545;&#29305;&#23450;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#21487;&#35777;&#26126;&#20445;&#35777;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;</title><link>https://arxiv.org/abs/2403.18774</link><description>&lt;p&gt;
&#19968;&#20010;&#20855;&#26377;&#21487;&#35777;&#26126;&#20445;&#35777;&#30340;&#31283;&#20581;&#32780;&#28789;&#27963;&#30340;&#21363;&#25554;&#21363;&#29992;&#25968;&#23383;&#27700;&#21360;&#26694;&#26550;&#65292;&#29992;&#20110;AI&#29983;&#25104;&#30340;&#22270;&#29255;
&lt;/p&gt;
&lt;p&gt;
RAW: A Robust and Agile Plug-and-Play Watermark Framework for AI-Generated Images with Provable Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18774
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#23383;&#27700;&#21360;&#26694;&#26550;RAW&#65292;&#23558;&#21487;&#23398;&#20064;&#30340;&#27700;&#21360;&#30452;&#25509;&#24341;&#20837;&#21040;&#21407;&#22987;&#22270;&#29255;&#25968;&#25454;&#20013;&#65292;&#25903;&#25345;&#21508;&#31181;&#29983;&#25104;&#26550;&#26500;&#24182;&#25552;&#20379;&#20102;&#23545;&#29305;&#23450;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#21487;&#35777;&#26126;&#20445;&#35777;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#38450;&#27490;&#23545;AI&#29983;&#25104;&#30340;&#22270;&#29255;&#30340;&#28508;&#22312;&#28389;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RAW&#30340;&#31283;&#20581;&#32780;&#28789;&#27963;&#30340;&#21363;&#25554;&#21363;&#29992;&#25968;&#23383;&#27700;&#21360;&#26816;&#27979;&#26694;&#26550;&#12290;&#19982;&#20256;&#32479;&#30340;&#32534;&#30721;-&#35299;&#30721;&#26041;&#27861;&#19981;&#21516;&#65292;&#20256;&#32479;&#26041;&#27861;&#23558;&#22266;&#23450;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#20316;&#20026;&#27700;&#21360;&#23884;&#20837;&#21040;&#28508;&#22312;&#34920;&#31034;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#23558;&#21487;&#23398;&#20064;&#30340;&#27700;&#21360;&#24341;&#20837;&#21040;&#21407;&#22987;&#22270;&#29255;&#25968;&#25454;&#20013;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19982;&#27700;&#21360;&#19968;&#36215;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#27700;&#21360;&#30340;&#23384;&#22312;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#19982;&#21508;&#31181;&#29983;&#25104;&#26550;&#26500;&#20860;&#23481;&#65292;&#24182;&#25903;&#25345;&#35757;&#32451;&#21518;&#21363;&#26102;&#36827;&#34892;&#27700;&#21360;&#27880;&#20837;&#12290;&#36890;&#36807;&#32467;&#21512;&#26368;&#20808;&#36827;&#30340;&#24179;&#28369;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#29305;&#23450;&#23545;&#25239;&#25915;&#20987;&#38024;&#23545;&#27700;&#21360;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#23558;&#24102;&#26377;&#27700;&#21360;&#30340;&#22270;&#29255;&#35823;&#20998;&#31867;&#30340;&#35823;&#25253;&#29575;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18774v1 Announce Type: cross  Abstract: Safeguarding intellectual property and preventing potential misuse of AI-generated images are of paramount importance. This paper introduces a robust and agile plug-and-play watermark detection framework, dubbed as RAW. As a departure from traditional encoder-decoder methods, which incorporate fixed binary codes as watermarks within latent representations, our approach introduces learnable watermarks directly into the original image data. Subsequently, we employ a classifier that is jointly trained with the watermark to detect the presence of the watermark. The proposed framework is compatible with various generative architectures and supports on-the-fly watermark injection after training. By incorporating state-of-the-art smoothing techniques, we show that the framework provides provable guarantees regarding the false positive rate for misclassifying a watermarked image, even in the presence of certain adversarial attacks targeting wa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;K-means&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#31454;&#20105;&#24615;&#38543;&#26426;&#26679;&#26412;&#22823;&#23567;&#20248;&#21270;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;Big-means&#20013;&#30340;&#24182;&#34892;&#22823;&#25968;&#25454;&#32858;&#31867;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.18766</link><description>&lt;p&gt;
&#36890;&#36807;&#31454;&#20105;&#24615;&#38543;&#26426;&#26679;&#26412;&#22823;&#23567;&#20248;&#21270;&#22312;Big-means&#20013;&#23454;&#29616;&#20248;&#36234;&#30340;&#24182;&#34892;&#22823;&#25968;&#25454;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Superior Parallel Big Data Clustering through Competitive Stochastic Sample Size Optimization in Big-means
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;K-means&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#31454;&#20105;&#24615;&#38543;&#26426;&#26679;&#26412;&#22823;&#23567;&#20248;&#21270;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;Big-means&#20013;&#30340;&#24182;&#34892;&#22823;&#25968;&#25454;&#32858;&#31867;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-means&#32858;&#31867;&#31639;&#27861;&#65292;&#26159;&#23545;&#20256;&#32479;Big-means&#26041;&#27861;&#30340;&#36827;&#27493;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#24182;&#34892;&#22788;&#29702;&#12289;&#38543;&#26426;&#25277;&#26679;&#21644;&#31454;&#20105;&#24615;&#20248;&#21270;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#20026;&#22823;&#25968;&#25454;&#24212;&#29992;&#35774;&#35745;&#30340;&#21487;&#25193;&#23637;&#21464;&#20307;&#12290;&#23427;&#35299;&#20915;&#20102;&#20256;&#32479;&#25216;&#26415;&#36890;&#24120;&#38754;&#20020;&#30340;&#21487;&#20280;&#32553;&#24615;&#21644;&#35745;&#31639;&#26102;&#38388;&#25361;&#25112;&#12290;&#35813;&#31639;&#27861;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#27599;&#20010;&#24037;&#20316;&#20154;&#21592;&#30340;&#26679;&#26412;&#22823;&#23567;&#65292;&#20248;&#21270;&#24615;&#33021;&#12290;&#36825;&#20123;&#26679;&#26412;&#22823;&#23567;&#30340;&#25968;&#25454;&#19981;&#26029;&#34987;&#20998;&#26512;&#65292;&#20419;&#36827;&#20102;&#25214;&#21040;&#26368;&#26377;&#25928;&#37197;&#32622;&#30340;&#35782;&#21035;&#12290;&#36890;&#36807;&#22312;&#20351;&#29992;&#19981;&#21516;&#26679;&#26412;&#22823;&#23567;&#30340;&#24037;&#20316;&#20154;&#21592;&#20043;&#38388;&#24341;&#20837;&#31454;&#20105;&#22240;&#32032;&#65292;&#36827;&#19968;&#27493;&#21050;&#28608;&#20102;Big-means&#31639;&#27861;&#20869;&#30340;&#25928;&#29575;&#12290;&#26412;&#36136;&#19978;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#24182;&#34892;&#35745;&#31639;&#29615;&#22659;&#20013;&#37319;&#29992;&#38543;&#26426;&#12289;&#31454;&#20105;&#24615;&#25277;&#26679;&#31574;&#30053;&#65292;&#24179;&#34913;&#20102;&#35745;&#31639;&#26102;&#38388;&#21644;&#32858;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18766v1 Announce Type: cross  Abstract: This paper introduces a novel K-means clustering algorithm, an advancement on the conventional Big-means methodology. The proposed method efficiently integrates parallel processing, stochastic sampling, and competitive optimization to create a scalable variant designed for big data applications. It addresses scalability and computation time challenges typically faced with traditional techniques. The algorithm adjusts sample sizes dynamically for each worker during execution, optimizing performance. Data from these sample sizes are continually analyzed, facilitating the identification of the most efficient configuration. By incorporating a competitive element among workers using different sample sizes, efficiency within the Big-means algorithm is further stimulated. In essence, the algorithm balances computational time and clustering quality by employing a stochastic, competitive sampling strategy in a parallel computing setting.
&lt;/p&gt;</description></item><item><title>CaT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#32422;&#26463;&#38598;&#25104;&#21040;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#32456;&#32467;&#30340;&#26041;&#24335;&#37325;&#26032;&#21046;&#23450;&#32422;&#26463;&#65292;&#36798;&#21040;&#20102;&#20986;&#33394;&#30340;&#32422;&#26463;&#20381;&#20174;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18765</link><description>&lt;p&gt;
CaT: &#32422;&#26463;&#20316;&#20026;&#22235;&#36275;&#34892;&#36208;&#24378;&#21270;&#23398;&#20064;&#30340;&#32456;&#32467;
&lt;/p&gt;
&lt;p&gt;
CaT: Constraints as Terminations for Legged Locomotion Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18765
&lt;/p&gt;
&lt;p&gt;
CaT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#32422;&#26463;&#38598;&#25104;&#21040;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#32456;&#32467;&#30340;&#26041;&#24335;&#37325;&#26032;&#21046;&#23450;&#32422;&#26463;&#65292;&#36798;&#21040;&#20102;&#20986;&#33394;&#30340;&#32422;&#26463;&#20381;&#20174;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65288;&#22914;&#22235;&#36275;&#21160;&#24577;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35299;&#31639;&#22120;&#26410;&#33021;&#20135;&#29983;&#36981;&#23432;&#20005;&#26684;&#32422;&#26463;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20027;&#24352;&#23558;&#32422;&#26463;&#38598;&#25104;&#21040;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#32422;&#26463;&#20316;&#20026;&#32456;&#32467;&#65288;CaT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#31163;&#24320;&#20256;&#32479;&#30340;&#21463;&#38480;RL&#20844;&#24335;&#65292;&#36890;&#36807;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#38543;&#26426;&#32456;&#32467;&#26469;&#37325;&#26032;&#21046;&#23450;&#32422;&#26463;&#65306;&#20219;&#20309;&#32422;&#26463;&#36829;&#35268;&#37117;&#20250;&#35302;&#21457;RL&#20195;&#29702;&#21487;&#20197;&#33719;&#24471;&#28508;&#22312;&#26410;&#26469;&#22870;&#21169;&#30340;&#32456;&#32467;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#31181;&#20844;&#24335;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#29616;&#25104;RL&#31639;&#27861;&#65288;&#22914;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65289;&#36827;&#34892;&#26368;&#23567;&#21270;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#24341;&#20837;&#19981;&#24517;&#35201;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#65292;&#23548;&#33268;&#20986;&#33394;&#30340;&#32422;&#26463;&#20381;&#20174;&#24615;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18765v1 Announce Type: cross  Abstract: Deep Reinforcement Learning (RL) has demonstrated impressive results in solving complex robotic tasks such as quadruped locomotion. Yet, current solvers fail to produce efficient policies respecting hard constraints. In this work, we advocate for integrating constraints into robot learning and present Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing from classical constrained RL formulations, we reformulate constraints through stochastic terminations during policy learning: any violation of a constraint triggers a probability of terminating potential future rewards the RL agent could attain. We propose an algorithmic approach to this formulation, by minimally modifying widely used off-the-shelf RL algorithms in robot learning (such as Proximal Policy Optimization). Our approach leads to excellent constraint adherence without introducing undue complexity and computational overhead, thus mitigating barriers 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#33016;&#37096;X&#23556;&#32447;&#19978;&#35782;&#21035;&#20122;&#20020;&#24202;&#21160;&#33033;&#30828;&#21270;&#65292;&#20026;&#26816;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18756</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#28145;&#24230;&#23398;&#20064;&#22312;&#33016;&#37096;X&#23556;&#32447;&#19978;&#26816;&#27979;&#20122;&#20020;&#24202;&#21160;&#33033;&#30828;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detection of subclinical atherosclerosis by image-based deep learning on chest x-ray
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#33016;&#37096;X&#23556;&#32447;&#19978;&#35782;&#21035;&#20122;&#20020;&#24202;&#21160;&#33033;&#30828;&#21270;&#65292;&#20026;&#26816;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#26159;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#35782;&#21035;&#33016;&#37096;&#27491;&#20301;X&#23556;&#32447;&#19978;&#30340;&#20122;&#20020;&#24202;&#21160;&#33033;&#30828;&#21270;&#12290;&#36890;&#36807;460&#20363;&#21021;&#32423;&#39044;&#38450;&#24739;&#32773;&#65288;58.4%&#30007;&#24615;&#65292;&#20013;&#38388;&#24180;&#40836;63 [51-74]&#23681;&#65289;&#30340;&#33016;&#37096;X&#23556;&#32447;&#65288;80%&#35757;&#32451;&#38431;&#21015;&#65292;20%&#20869;&#37096;&#39564;&#35777;&#38431;&#21015;&#65289;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#20896;&#29366;&#21160;&#33033;&#38041;&#21270;&#65288;CAC&#65289;&#35780;&#20998;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65288;AI-CAC&#27169;&#22411;&#65289;&#65292;&#36825;&#20123;&#24739;&#32773;&#22312;&#20020;&#24202;&#21407;&#22240;&#19979;&#26377;&#21487;&#29992;&#30340;&#25104;&#23545;&#33016;&#37096;X&#23556;&#32447;&#21644;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#65292;&#32780;&#19988;&#36825;&#20004;&#39033;&#26816;&#26597;&#22312;3&#20010;&#26376;&#20869;&#23436;&#25104;&#12290;&#22312;&#26469;&#33258;&#30456;&#21516;&#26426;&#26500;&#30340;90&#21517;&#30149;&#20154;&#30340;&#26102;&#38388;&#29420;&#31435;&#38431;&#21015;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#39564;&#35777;&#65288;&#22806;&#37096;&#39564;&#35777;&#65289;&#12290;&#36890;&#36807;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#35780;&#20272;AI-CAC&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#20026;&#20027;&#35201;&#32467;&#26524;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#20013;&#20301;AI-CAC&#35780;&#20998;&#20026;35&#65288;0-388&#65289;&#65292;28.9%&#30340;&#24739;&#32773;&#27809;&#26377;AI-CAC&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18756v1 Announce Type: cross  Abstract: Aims. To develop a deep-learning based system for recognition of subclinical atherosclerosis on a plain frontal chest x-ray. Methods and Results. A deep-learning algorithm to predict coronary artery calcium (CAC) score (the AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20% internal validation cohort) of primary prevention patients (58.4% male, median age 63 [51-74] years) with available paired chest x-ray and chest computed tomography (CT) indicated for any clinical reason and performed within 3 months. The CAC score calculated on chest CT was used as ground truth. The model was validated on an temporally-independent cohort of 90 patients from the same institution (external validation). The diagnostic accuracy of the AI-CAC model assessed by the area under the curve (AUC) was the primary outcome. Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC. AUC of the AI-CAC model to identify a CA
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.18742</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#31867;&#21453;&#39304;&#23545;&#40784;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Learning Dynamics of Alignment with Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#24050;&#25104;&#20026;&#23433;&#20840;&#37096;&#32626;&#27169;&#22411;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#34429;&#28982;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29702;&#35770;&#19978;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#25105;&#20204;&#27491;&#24335;&#23637;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36824;&#25581;&#31034;&#20102;&#19968;&#20010;&#22797;&#26434;&#29616;&#35937;&#65292;&#21363;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#20855;&#26377;&#26356;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#24403;&#20195;LLMs&#21644;&#23545;&#40784;&#20219;&#21153;&#19978;&#22312;&#23454;&#35777;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24378;&#21270;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#23545;&#40784;&#26041;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#20813;&#36131;&#22768;&#26126;&#65306;&#26412;&#25991;&#21253;&#21547;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18742v1 Announce Type: cross  Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potent
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#36816;&#34892;&#25968;&#25454;&#30340;&#29983;&#23384;&#24314;&#27169;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24555;&#29031;&#25968;&#25454;&#35757;&#32451;&#30340;&#29983;&#23384;&#27169;&#22411;&#65292;&#38024;&#23545;&#38750;&#21516;&#36136;&#37319;&#26679;&#25968;&#25454;&#65292;&#36890;&#36807;&#21516;&#36136;&#37319;&#26679;&#20351;&#24471;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#33021;&#22815;&#24212;&#29992;&#24182;&#20135;&#29983;&#29702;&#24819;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.18739</link><description>&lt;p&gt;
&#22522;&#20110;&#36816;&#34892;&#25968;&#25454;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#23450;&#29992;&#36884;&#29983;&#23384;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Usage-Specific Survival Modeling Based on Operational Data and Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18739
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#36816;&#34892;&#25968;&#25454;&#30340;&#29983;&#23384;&#24314;&#27169;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24555;&#29031;&#25968;&#25454;&#35757;&#32451;&#30340;&#29983;&#23384;&#27169;&#22411;&#65292;&#38024;&#23545;&#38750;&#21516;&#36136;&#37319;&#26679;&#25968;&#25454;&#65292;&#36890;&#36807;&#21516;&#36136;&#37319;&#26679;&#20351;&#24471;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#33021;&#22815;&#24212;&#29992;&#24182;&#20135;&#29983;&#29702;&#24819;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35268;&#21010;&#32500;&#25252;&#26102;&#65292;&#20934;&#30830;&#39044;&#27979;&#38646;&#37096;&#20214;&#25925;&#38556;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#65292;&#36890;&#36807;&#23545;&#36825;&#20123;&#25925;&#38556;&#26102;&#38388;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#29983;&#23384;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#29992;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20256;&#32479;&#29983;&#23384;&#27169;&#22411;&#65292;&#20351;&#29992;&#22312;&#29305;&#23450;&#26102;&#38388;&#36830;&#32493;&#25910;&#38598;&#21644;&#23384;&#20648;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#31216;&#20026;&#24555;&#29031;&#12290;&#36825;&#31181;&#31867;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24615;&#26159;&#23427;&#21487;&#20197;&#21253;&#21547;&#26469;&#33258;&#29305;&#23450;&#20010;&#20307;&#30340;&#22810;&#20010;&#24555;&#29031;&#65292;&#36825;&#23548;&#33268;&#26631;&#20934;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#65292;&#22240;&#20026;&#25968;&#25454;&#19981;&#26159;&#29420;&#31435;&#30340;&#12290;&#28982;&#32780;&#65292;&#35813;&#35770;&#25991;&#34920;&#26126;&#65292;&#22914;&#26524;&#25968;&#25454;&#20197;&#25152;&#26377;&#20010;&#20307;&#30340;&#25152;&#26377;&#24555;&#29031;&#26102;&#38388;&#30456;&#21516;&#30340;&#29305;&#23450;&#26684;&#24335;&#23384;&#22312;&#65292;&#31216;&#20026;&#21516;&#36136;&#37319;&#26679;&#65292;&#21017;&#21487;&#20197;&#24212;&#29992;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#24182;&#20135;&#29983;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#24182;&#38750;&#21516;&#36136;&#37319;&#26679;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18739v1 Announce Type: new  Abstract: Accurate predictions of when a component will fail are crucial when planning maintenance, and by modeling the distribution of these failure times, survival models have shown to be particularly useful in this context. The presented methodology is based on conventional neural network-based survival models that are trained using data that is continuously gathered and stored at specific times, called snapshots. An important property of this type of training data is that it can contain more than one snapshot from a specific individual which results in that standard maximum likelihood training can not be directly applied since the data is not independent. However, the papers show that if the data is in a specific format where all snapshot times are the same for all individuals, called homogeneously sampled, maximum likelihood training can be applied and produce desirable results. In many cases, the data is not homogeneously sampled and in this
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#19982;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;(KPCA)&#30340;&#39640;&#25928;&#26694;&#26550;&#29992;&#20110;&#36816;&#31639;&#23398;&#20064;&#65292;KPCA-DeepONet&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18735</link><description>&lt;p&gt;
&#38754;&#21521;&#36816;&#31639;&#23398;&#20064;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Nonlinear model reduction for operator learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18735
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#19982;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;(KPCA)&#30340;&#39640;&#25928;&#26694;&#26550;&#29992;&#20110;&#36816;&#31639;&#23398;&#20064;&#65292;KPCA-DeepONet&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#31639;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#36924;&#36817;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;&#28145;&#24230;&#36816;&#31639;&#32593;&#32476;(DeepONets)&#26159;&#35813;&#39046;&#22495;&#30340;&#19968;&#20010;&#26174;&#33879;&#26550;&#26500;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#27169;&#22411;&#31616;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;DeepONet&#25193;&#23637;&#65292;proper orthogonal decomposition (POD)-DeepONet&#65292;&#24050;&#33021;&#22815;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20934;&#30830;&#24230;&#26041;&#38754;&#32988;&#36807;&#20854;&#20182;&#26550;&#26500;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#24819;&#27861;&#25193;&#23637;&#21040;&#38750;&#32447;&#24615;&#27169;&#22411;&#38454;&#38477;&#30340;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;(KPCA)&#32467;&#21512;&#30340;&#39640;&#25928;&#26694;&#26550;&#29992;&#20110;&#36816;&#31639;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;KPCA-DeepONet&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;POD-DeepONet&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18735v1 Announce Type: new  Abstract: Operator learning provides methods to approximate mappings between infinite-dimensional function spaces. Deep operator networks (DeepONets) are a notable architecture in this field. Recently, an extension of DeepONet based on model reduction and neural networks, proper orthogonal decomposition (POD)-DeepONet, has been able to outperform other architectures in terms of accuracy for several benchmark tests. We extend this idea towards nonlinear model order reduction by proposing an efficient framework that combines neural networks with kernel principal component analysis (KPCA) for operator learning. Our results demonstrate the superior performance of KPCA-DeepONet over POD-DeepONet.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#26469;&#22686;&#24378;&#21046;&#36896;&#36136;&#37327;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#26080;&#20851;&#29305;&#24449;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20026;&#38477;&#20302;&#21046;&#36896;&#25104;&#26412;&#21644;&#26356;&#22909;&#29702;&#35299;&#35757;&#32451;&#21518;&#30340;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.18731</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#22686;&#24378;&#21046;&#36896;&#36136;&#37327;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Enhancing Manufacturing Quality Prediction Models through the Integration of Explainability Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18731
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#26469;&#22686;&#24378;&#21046;&#36896;&#36136;&#37327;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#26080;&#20851;&#29305;&#24449;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20026;&#38477;&#20302;&#21046;&#36896;&#25104;&#26412;&#21644;&#26356;&#22909;&#29702;&#35299;&#35757;&#32451;&#21518;&#30340;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#26469;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#22312;&#39044;&#27979;&#38115;&#21066;&#36807;&#31243;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#21046;&#36896;&#19994;&#26696;&#20363;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#39318;&#20808;&#35757;&#32451;ML&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#35782;&#21035;&#24182;&#28040;&#38500;&#26080;&#20851;&#29305;&#24449;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#31181;&#36807;&#31243;&#30340;&#23436;&#21892;&#32467;&#26524;&#23548;&#33268;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#20026;&#38477;&#20302;&#21046;&#36896;&#25104;&#26412;&#21644;&#26356;&#22909;&#29702;&#35299;&#35757;&#32451;&#21518;&#30340;ML&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#35813;&#30740;&#31350;&#31361;&#20986;&#20102;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#22312;&#35299;&#37322;&#21644;&#20248;&#21270;&#21046;&#36896;&#39046;&#22495;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#29992;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18731v1 Announce Type: new  Abstract: This research presents a method that utilizes explainability techniques to amplify the performance of machine learning (ML) models in forecasting the quality of milling processes, as demonstrated in this paper through a manufacturing use case. The methodology entails the initial training of ML models, followed by a fine-tuning phase where irrelevant features identified through explainability methods are eliminated. This procedural refinement results in performance enhancements, paving the way for potential reductions in manufacturing costs and a better understanding of the trained ML models. This study highlights the usefulness of explainability techniques in both explaining and optimizing predictive models in the manufacturing realm.
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.18717</link><description>&lt;p&gt;
&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning for Deep Causal Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18717
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#22238;&#31572;&#8220;&#22914;&#26524;$y$&#21464;&#20026;$z$&#65292;$x$&#20250;&#22914;&#20309;&#21464;&#21270;&#65311;&#8221;&#36825;&#31867;&#38382;&#39064;&#30340;&#27169;&#22411;&#23545;&#20110;&#25512;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#33021;&#22815;&#35299;&#20915;&#36825;&#31867;&#21453;&#20107;&#23454;&#38382;&#39064;&#30340;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#30446;&#21069;&#35201;&#27714;&#25152;&#26377;&#30456;&#20851;&#21464;&#37327;&#22343;&#24050;&#34987;&#35266;&#23519;&#21040;&#65292;&#24182;&#19988;&#30456;&#24212;&#30340;&#26631;&#31614;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21487;&#29992;&#12290;&#25105;&#20204;&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18717v1 Announce Type: cross  Abstract: Developing models that can answer questions of the form "How would $x$ change if $y$ had been $z$?" is fundamental for advancing medical image analysis. Training causal generative models that address such counterfactual questions, though, currently requires that all relevant variables have been observed and that corresponding labels are available in training data. However, clinical data may not have complete records for all patients and state of the art causal generative models are unable to take full advantage of this. We thus develop, for the first time, a semi-supervised deep causal generative model that exploits the causal relationships between variables to maximise the use of all available data. We explore this in the setting where each sample is either fully labelled or fully unlabelled, as well as the more clinically realistic case of having different labels missing for each sample. We leverage techniques from causal inference t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#20803;&#32990;&#33258;&#21160;&#26426;&#27169;&#22411;&#21644;CNN-LSTM&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25104;&#21151;&#39044;&#27979;&#20132;&#36890;&#27969;&#65292;&#20805;&#20998;&#21033;&#29992;&#20132;&#36890;&#27969;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.18710</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20803;&#32990;&#33258;&#21160;&#26426;&#27169;&#22411;&#21644;CNN-LSTM&#26550;&#26500;&#36827;&#34892;&#20132;&#36890;&#27969;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Traffic Flow Prediction using Cellular Automata-based Model and CNN-LSTM architecture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#20803;&#32990;&#33258;&#21160;&#26426;&#27169;&#22411;&#21644;CNN-LSTM&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25104;&#21151;&#39044;&#27979;&#20132;&#36890;&#27969;&#65292;&#20805;&#20998;&#21033;&#29992;&#20132;&#36890;&#27969;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#39044;&#27979;&#20132;&#36890;&#27969;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20294;&#21462;&#24471;&#20102;&#19981;&#21516;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#23545;&#20110;&#20132;&#36890;&#27969;&#31995;&#32479;&#26469;&#35828;&#36825;&#26679;&#30340;&#25968;&#25454;&#30446;&#21069;&#24182;&#19981;&#23481;&#26131;&#33719;&#24471;&#12290;&#20854;&#27425;&#65292;&#21363;&#20351;&#26377;&#25968;&#25454;&#65292;&#31070;&#32463;&#32593;&#32476;&#20063;&#38656;&#35201;&#35775;&#38382;&#28085;&#30422;&#22823;&#22810;&#25968;&#21487;&#33021;&#30340;&#20132;&#36890;&#27969;&#21160;&#24577;&#30340;&#21382;&#21490;&#25968;&#25454;&#25165;&#33021;&#25104;&#21151;&#39044;&#27979;&#26410;&#26469;&#30340;&#20132;&#36890;&#29366;&#24577;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20851;&#20110;&#20132;&#36890;&#27969;&#21160;&#24577;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#23613;&#31649;&#24050;&#32463;&#26377;&#30456;&#24403;&#22810;&#30340;&#29616;&#26377;&#30693;&#35782;&#24211;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#25104;&#21151;&#39044;&#27979;&#20132;&#36890;&#27969;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#20803;&#32990;&#33258;&#21160;&#26426;&#30340;&#20132;&#36890;&#27969;&#32479;&#35745;&#21147;&#23398;&#27169;&#22411;&#29983;&#25104;&#20132;&#36890;&#27969;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18710v1 Announce Type: new  Abstract: Recent works have attempted to use deep learning to predict future states of traffic flow, but have met with mixed results. These approaches face two key challenges. First, training deep learning neural networks requires large amounts of training data which are not yet easily available for traffic flow systems. Second, even when data is available, the neural networks require access to historical data that covers most possible traffic flow dynamics to successfully predict future traffic states. Specifically, these deep learning approaches do not fully leverage domain-knowledge about traffic flow dynamics, despite a significant existing knowledge-base. In this work, we propose to solve both issues using a Convolutional Neural Network (CNNs) with Long Short Term Memory (LSTM) deep learning architecture to successfully predict traffic flow, while leveraging a cellular automata-based statistical mechanics model of traffic flow to generate tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#19968;&#32452;&#21463;&#38480;&#32806;&#21512;&#24341;&#20837;&#30340;&#26465;&#20214;Wasserstein&#36317;&#31163;&#65292;&#23427;&#31561;&#20110;&#21518;&#39564;Wasserstein&#36317;&#31163;&#30340;&#26399;&#26395;&#65292;&#25512;&#23548;&#20102;&#20854;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#36817;&#20284;&#36895;&#24230;&#22330;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18705</link><description>&lt;p&gt;
&#20855;&#26377;&#36125;&#21494;&#26031;OT&#27969;&#21305;&#37197;&#24212;&#29992;&#30340;&#26465;&#20214;Wasserstein&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Conditional Wasserstein Distances with Applications in Bayesian OT Flow Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#19968;&#32452;&#21463;&#38480;&#32806;&#21512;&#24341;&#20837;&#30340;&#26465;&#20214;Wasserstein&#36317;&#31163;&#65292;&#23427;&#31561;&#20110;&#21518;&#39564;Wasserstein&#36317;&#31163;&#30340;&#26399;&#26395;&#65292;&#25512;&#23548;&#20102;&#20854;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#36817;&#20284;&#36895;&#24230;&#22330;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36870;&#38382;&#39064;&#20013;&#65292;&#35768;&#22810;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#26368;&#23567;&#21270;&#32852;&#21512;&#24230;&#37327;&#19982;&#20854;&#23398;&#20064;&#36924;&#36817;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#36817;&#20284;&#21518;&#39564;&#27979;&#24230;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;Kullback--Leibler&#20998;&#27495;&#30340;&#24773;&#20917;&#19979;&#20063;&#25511;&#21046;&#21518;&#39564;&#27979;&#24230;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20294;&#19968;&#33324;&#26469;&#35828;&#23545;&#20110;Wasserstein&#36317;&#31163;&#24182;&#19981;&#25104;&#31435;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#32452;&#21463;&#38480;&#32806;&#21512;&#24341;&#20837;&#20102;&#19968;&#31181;&#26465;&#20214;Wasserstein&#36317;&#31163;&#65292;&#23427;&#31561;&#20110;&#21518;&#39564;Wasserstein&#36317;&#31163;&#30340;&#26399;&#26395;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26465;&#20214;Wasserstein-1&#27969;&#30340;&#23545;&#20598;&#24418;&#24335;&#20197;&#19968;&#31181;&#38750;&#24120;&#33258;&#28982;&#30340;&#26041;&#24335;&#31867;&#20284;&#20110;&#26465;&#20214;Wasserstein GAN&#25991;&#29486;&#20013;&#30340;&#25439;&#22833;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#26465;&#20214;Wasserstein&#36317;&#31163;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#34920;&#24449;&#30456;&#24212;&#30340;&#27979;&#22320;&#32447;&#21644;&#36895;&#24230;&#22330;&#20197;&#21450;&#27969;ODE&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#25918;&#23485;&#26465;&#20214;Wasserstein&#36317;&#31163;&#26469;&#36817;&#20284;&#36895;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18705v1 Announce Type: new  Abstract: In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback--Leibler divergence, this is in general not hold true for the Wasserstein distance. In this paper, we introduce a conditional Wasserstein distance via a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. Interestingly, the dual formulation of the conditional Wasserstein-1 flow resembles losses in the conditional Wasserstein GAN literature in a quite natural way. We derive theoretical properties of the conditional Wasserstein distance, characterize the corresponding geodesics and velocity fields as well as the flow ODEs. Subsequently, we propose to approximate the velocity fields by relaxing the conditional Wasserstein dista
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558; FPGA &#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20197;&#25552;&#39640;&#26080;&#20154;&#26426;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.18703</link><description>&lt;p&gt;
&#22522;&#20110; FPGA &#30340;&#26080;&#20154;&#26426;&#31070;&#32463;&#25512;&#21147;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fpga-Based Neural Thrust Controller for UAVs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558; FPGA &#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20197;&#25552;&#39640;&#26080;&#20154;&#26426;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#65288;UAVs&#65289;&#30340;&#20986;&#29616;&#36890;&#36807;&#25552;&#20379;&#22810;&#21151;&#33021;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#21487;&#35775;&#38382;&#24179;&#21488;&#65292;&#25913;&#21892;&#20102;&#21508;&#31181;&#39046;&#22495;&#23454;&#26045;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#23436;&#25104;&#26356;&#24191;&#27867;&#30340;&#20219;&#21153;&#65292;&#36234;&#26469;&#36234;&#38656;&#35201;&#22686;&#24378;&#26426;&#36733;&#35745;&#31639;&#24615;&#33021;&#20197;&#24212;&#23545;&#19981;&#26029;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#29615;&#22659;&#26465;&#20214;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#30475;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#19982;&#24378;&#21270;&#23398;&#20064;&#19968;&#36215;&#65292;&#20197;&#25552;&#39640;&#26080;&#20154;&#26426;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#12290;&#28982;&#32780;&#65292;DNNs &#30340;&#35745;&#31639;&#35201;&#27714;&#23545;&#35768;&#22810; UAVs &#19978;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#26500;&#25104;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#29616;&#22330;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGAs&#65289;&#20316;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#12289;&#39640;&#24615;&#33021;&#12289;&#33021;&#37327;&#21644;&#26102;&#38388;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37197;&#22791; Artix-7 FPGA &#30340;&#26032;&#22411;&#30828;&#20214;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18703v1 Announce Type: cross  Abstract: The advent of unmanned aerial vehicles (UAVs) has improved a variety of fields by providing a versatile, cost-effective and accessible platform for implementing state-of-the-art algorithms. To accomplish a broader range of tasks, there is a growing need for enhanced on-board computing to cope with increasing complexity and dynamic environmental conditions. Recent advances have seen the application of Deep Neural Networks (DNNs), particularly in combination with Reinforcement Learning (RL), to improve the adaptability and performance of UAVs, especially in unknown environments. However, the computational requirements of DNNs pose a challenge to the limited computing resources available on many UAVs. This work explores the use of Field Programmable Gate Arrays (FPGAs) as a viable solution to this challenge, offering flexibility, high performance, energy and time efficiency. We propose a novel hardware board equipped with an Artix-7 FPGA 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#31216;&#20026;&#27491;&#20132;&#38170;&#28857;&#22238;&#24402;&#25439;&#22833;&#65292;&#29992;&#20110;&#35299;&#24320;&#23884;&#20837;&#32858;&#31867;&#65292;&#26174;&#33879;&#22686;&#24378;&#23884;&#20837;&#30340;&#29420;&#29305;&#24615;</title><link>https://arxiv.org/abs/2403.18699</link><description>&lt;p&gt;
&#20855;&#26377;&#27491;&#20132;&#38170;&#28857;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;CLOA&#65289;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning with Orthonormal Anchors (CLOA)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#31216;&#20026;&#27491;&#20132;&#38170;&#28857;&#22238;&#24402;&#25439;&#22833;&#65292;&#29992;&#20110;&#35299;&#24320;&#23884;&#20837;&#32858;&#31867;&#65292;&#26174;&#33879;&#22686;&#24378;&#23884;&#20837;&#30340;&#29420;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#26816;&#26597;InfoNCE&#25439;&#22833;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#65292;&#21363;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#34920;&#29616;&#20986;&#38480;&#21046;&#24615;&#34892;&#20026;&#65292;&#23548;&#33268;&#23884;&#20837;&#36235;&#20110;&#34701;&#21512;&#20026;&#19968;&#20010;&#22855;&#24322;&#28857;&#30340;&#25910;&#25947;&#29616;&#35937;&#12290;&#36825;&#31181;&#8220;&#36807;&#24230;&#34701;&#21512;&#8221;&#25928;&#24212;&#23545;&#21518;&#32493;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23884;&#20837;&#22312;&#31561;&#20110;&#25110;&#23616;&#38480;&#20110;&#31209;-1&#32447;&#24615;&#23376;&#31354;&#38388;&#26102;&#34920;&#31034;InfoNCE&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#65292;&#21033;&#29992;&#19982;&#24494;&#35843;&#38454;&#27573;&#20856;&#22411;&#20351;&#29992;&#30340;&#30456;&#21516;&#25110;&#26356;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#27491;&#20132;&#38170;&#28857;&#22238;&#24402;&#25439;&#22833;&#65292;&#26088;&#22312;&#35299;&#24320;&#23884;&#20837;&#32858;&#31867;&#65292;&#26174;&#33879;&#22686;&#24378;&#27599;&#20010;&#23884;&#20837;&#30340;&#29420;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18699v1 Announce Type: cross  Abstract: This study focuses on addressing the instability issues prevalent in contrastive learning, specifically examining the InfoNCE loss function and its derivatives. We reveal a critical observation that these loss functions exhibit a restrictive behavior, leading to a convergence phenomenon where embeddings tend to merge into a singular point. This "over-fusion" effect detrimentally affects classification accuracy in subsequent supervised-learning tasks. Through theoretical analysis, we demonstrate that embeddings, when equalized or confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In response to this challenge, our research introduces an innovative strategy that leverages the same or fewer labeled data than typically used in the fine-tuning phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to disentangle embedding clusters, significantly enhancing the distinctiveness of each embedding 
&lt;/p&gt;</description></item><item><title>&#27604;&#36739;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#20351;&#29992;InceptionTime&#30452;&#25509;&#20998;&#31867;&#21644;&#20351;&#29992;&#23567;&#27874;&#21464;&#25442;&#22270;&#20687;&#20877;&#20998;&#31867;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#22343;&#33719;&#24471;90%&#20197;&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#20854;&#20013;&#30452;&#25509;&#20998;&#31867;&#26041;&#27861;&#36798;&#21040;&#20102;95.2%&#12290;</title><link>https://arxiv.org/abs/2403.18687</link><description>&lt;p&gt;
InceptionTime&#19982;&#23567;&#27874;--&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
InceptionTime vs. Wavelet -- A comparison for time series classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18687
&lt;/p&gt;
&lt;p&gt;
&#27604;&#36739;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#20351;&#29992;InceptionTime&#30452;&#25509;&#20998;&#31867;&#21644;&#20351;&#29992;&#23567;&#27874;&#21464;&#25442;&#22270;&#20687;&#20877;&#20998;&#31867;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#22343;&#33719;&#24471;90%&#20197;&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#20854;&#20013;&#30452;&#25509;&#20998;&#31867;&#26041;&#27861;&#36798;&#21040;&#20102;95.2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#27425;&#22768;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#19968;&#31181;&#26159;&#22522;&#20110;&#30452;&#25509;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#20351;&#29992;&#20102;InceptionTime&#32593;&#32476;&#30340;&#33258;&#23450;&#20041;&#23454;&#29616;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#29983;&#25104;&#20449;&#21495;&#30340;&#23567;&#27874;&#21464;&#25442;&#30340;2D&#22270;&#20687;&#65292;&#28982;&#21518;&#20351;&#29992;ResNet&#23454;&#29616;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#65292;&#20004;&#31181;&#26041;&#27861;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#22343;&#22312;90%&#20197;&#19978;&#65292;&#20854;&#20013;&#30452;&#25509;&#26041;&#27861;&#36798;&#21040;&#20102;95.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18687v1 Announce Type: new  Abstract: Neural networks were used to classify infrasound data. Two different approaches were compared. One based on the direct classification of time series data, using a custom implementation of the InceptionTime network. For the other approach, we generated 2D images of the wavelet transformation of the signals, which were subsequently classified using a ResNet implementation. Choosing appropriate hyperparameter settings, both achieve a classification accuracy of above 90 %, with the direct approach reaching 95.2 %.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26465;&#20214;&#65292;&#21487;&#20197;&#22312;&#23646;&#24615;&#25968;&#37327;&#12289;&#22522;&#25968;&#21644;&#26679;&#26412;&#22823;&#23567;&#30340;&#19981;&#21516;&#32452;&#21512;&#19979;&#20445;&#25345;&#22810;&#21464;&#37327;&#23545;&#31216;&#19981;&#30830;&#23450;&#24615;&#30340;&#33391;&#22909;&#36136;&#37327;&#65292;&#20026;&#23646;&#24615;&#36873;&#25321;&#36807;&#31243;&#25552;&#20379;&#20102;&#26032;&#30340;&#26377;&#29992;&#26631;&#20934;</title><link>https://arxiv.org/abs/2403.18685</link><description>&lt;p&gt;
&#29992;&#20110;&#23646;&#24615;&#36873;&#25321;&#30340;&#22810;&#21464;&#37327;&#23545;&#31216;&#19981;&#30830;&#23450;&#24615;&#20013;&#30340;&#26679;&#26412;&#20195;&#34920;&#24615;
&lt;/p&gt;
&lt;p&gt;
Representatividad Muestral en la Incertidumbre Sim\'etrica Multivariada para la Selecci\'on de Atributos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18685
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26465;&#20214;&#65292;&#21487;&#20197;&#22312;&#23646;&#24615;&#25968;&#37327;&#12289;&#22522;&#25968;&#21644;&#26679;&#26412;&#22823;&#23567;&#30340;&#19981;&#21516;&#32452;&#21512;&#19979;&#20445;&#25345;&#22810;&#21464;&#37327;&#23545;&#31216;&#19981;&#30830;&#23450;&#24615;&#30340;&#33391;&#22909;&#36136;&#37327;&#65292;&#20026;&#23646;&#24615;&#36873;&#25321;&#36807;&#31243;&#25552;&#20379;&#20102;&#26032;&#30340;&#26377;&#29992;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#27169;&#25311;&#25216;&#26415;&#20998;&#26512;&#20102;&#22810;&#21464;&#37327;&#23545;&#31216;&#19981;&#30830;&#23450;&#24615;&#65288;MSU&#65289;&#27979;&#37327;&#30340;&#34892;&#20026;&#65292;&#23545;&#21547;&#26377;&#20449;&#24687;&#21644;&#38750;&#20449;&#24687;&#38543;&#26426;&#29983;&#25104;&#29305;&#24449;&#30340;&#21508;&#31181;&#28151;&#21512;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#23646;&#24615;&#25968;&#37327;&#12289;&#23427;&#20204;&#30340;&#22522;&#25968;&#21644;&#26679;&#26412;&#22823;&#23567;&#22914;&#20309;&#24433;&#21709;MSU&#12290;&#36890;&#36807;&#35266;&#23519;&#32467;&#26524;&#65292;&#22312;&#36825;&#20010;&#35770;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26465;&#20214;&#65292;&#21487;&#20197;&#22312;&#36825;&#19977;&#20010;&#22240;&#32032;&#30340;&#19981;&#21516;&#32452;&#21512;&#19979;&#20445;&#25345;MSU&#30340;&#33391;&#22909;&#36136;&#37327;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;&#26032;&#26631;&#20934;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#38477;&#32500;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18685v1 Announce Type: cross  Abstract: In this work, we analyze the behavior of the multivariate symmetric uncertainty (MSU) measure through the use of statistical simulation techniques under various mixes of informative and non-informative randomly generated features. Experiments show how the number of attributes, their cardinalities, and the sample size affect the MSU. In this thesis, through observation of results, it is proposed an heuristic condition that preserves good quality in the MSU under different combinations of these three factors, providing a new useful criterion to help drive the process of dimension reduction.   -  En el presente trabajo hemos analizado el comportamiento de una versi\'on multivariada de la incertidumbre sim\'etrica a trav\'es de t\'ecnicas de simulaci\'on estad\'isticas sobre varias combinaciones de atributos informativos y no-informativos generados de forma aleatoria. Los experimentos muestran como el n\'umero de atributos, sus cardinali
&lt;/p&gt;</description></item><item><title>TransFusion&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#23450;&#20041;&#20102;&#23545;&#27604;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#29702;&#35770;&#26497;&#38480;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#20197;&#25913;&#21892;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.18681</link><description>&lt;p&gt;
TransFusion&#65306;&#20855;&#26377;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TransFusion: Contrastive Learning with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18681
&lt;/p&gt;
&lt;p&gt;
TransFusion&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#23450;&#20041;&#20102;&#23545;&#27604;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#29702;&#35770;&#26497;&#38480;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#20197;&#25913;&#21892;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;TransFusion&#65292;&#26088;&#22312;&#20351;&#23545;&#27604;&#23398;&#20064;&#30340;&#36807;&#31243;&#26356;&#20855;&#20998;&#26512;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290; TransFusion&#30001;&#27880;&#24847;&#21147;&#22359;&#32452;&#25104;&#65292;&#20854;&#20013;&#30340;softmax&#34987;&#26367;&#25442;&#20026;ReLU&#65292;&#24182;&#19988;&#20854;&#26368;&#32456;&#22359;&#30340;&#21152;&#26435;&#21644;&#25805;&#20316;&#34987;&#25130;&#26029;&#65292;&#20197;&#20351;&#37051;&#25509;&#30697;&#38453;&#25104;&#20026;&#36755;&#20986;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26368;&#23567;&#21270;&#20854;&#36755;&#20986;&#19982;&#30446;&#26631;&#20851;&#32852;&#30697;&#38453;&#20043;&#38388;&#30340;Jensen-Shannon&#25955;&#24230;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#30697;&#38453;&#25351;&#31034;&#27599;&#23545;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#30456;&#21516;&#31867;&#21035;&#25110;&#19981;&#21516;&#31867;&#21035;&#12290; TransFusion&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23450;&#20041;&#20102;&#22238;&#31572;&#35813;&#39046;&#22495;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#29702;&#35770;&#26497;&#38480;&#65306;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#22823;&#32423;&#21035;&#21644;&#26377;&#25928;&#23545;&#27604;&#23398;&#20064;&#25152;&#38656;&#30340;&#26368;&#23567;&#25209;&#37327;&#22823;&#23567;&#12290; &#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TransFusion&#25104;&#21151;&#22320;&#25552;&#21462;&#20986;&#33021;&#22815;&#20174;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#20998;&#31163;&#38598;&#32676;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18681v1 Announce Type: cross  Abstract: This paper proposes a novel framework, TransFusion, designed to make the process of contrastive learning more analytical and explainable. TransFusion consists of attention blocks whose softmax being replaced by ReLU, and its final block's weighted-sum operation is truncated to leave the adjacency matrix as the output. The model is trained by minimizing the Jensen-Shannon Divergence between its output and the target affinity matrix, which indicates whether each pair of samples belongs to the same or different classes. The main contribution of TransFusion lies in defining a theoretical limit for answering two fundamental questions in the field: the maximum level of data augmentation and the minimum batch size required for effective contrastive learning. Furthermore, experimental results indicate that TransFusion successfully extracts features that isolate clusters from complex real-world data, leading to improved classification accuracy 
&lt;/p&gt;</description></item><item><title>NL-ITI&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#25506;&#27979;&#21644;&#22810;&#20196;&#29260;&#24178;&#39044;&#65292;&#25104;&#21151;&#25913;&#36827;&#20102;ITI&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#22810;&#36873;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.18680</link><description>&lt;p&gt;
NL-ITI&#65306;&#20248;&#21270;&#25506;&#27979;&#21644;&#24178;&#39044;&#20197;&#25913;&#36827;ITI&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18680
&lt;/p&gt;
&lt;p&gt;
NL-ITI&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#25506;&#27979;&#21644;&#22810;&#20196;&#29260;&#24178;&#39044;&#65292;&#25104;&#21151;&#25913;&#36827;&#20102;ITI&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#22810;&#36873;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#23481;&#26131;&#36820;&#22238;&#34394;&#20551;&#20449;&#24687;&#65292;&#36825;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25512;&#29702;&#26102;&#24178;&#39044;(Inference-Time-Intervention, ITI)&#26041;&#27861;&#24341;&#20837;&#30340;&#33539;&#24335;&#12290;&#39318;&#20808;&#65292;ITI&#26041;&#27861;&#35782;&#21035;&#21253;&#21547;&#26368;&#22810;&#25152;&#38656;&#30693;&#35782;&#31867;&#22411;(&#20363;&#22914;&#30495;&#23454;&#20449;&#24687;)&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#38543;&#21518;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;LLM&#28608;&#27963;&#34987;&#31227;&#21160;&#21040;&#25152;&#36873;&#27880;&#24847;&#21147;&#22836;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#25506;&#27979;&#21644;&#22810;&#20196;&#29260;&#24178;&#39044;-&#38750;&#32447;&#24615;ITI(NL-ITI)&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;ITI&#26694;&#26550;&#12290;NL-ITI&#22312;&#22810;&#20010;&#22810;&#36873;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;TruthfulQA&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#22522;&#20934;&#19978;&#30456;&#23545;&#20110;&#22522;&#32447;ITI&#32467;&#26524;&#25253;&#21578;&#20102;&#32422;14%&#30340;MC1&#25351;&#26631;&#25913;&#36827;&#12290;NL-ITI&#36824;&#22312;&#20854;&#20182;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#25104;&#32489;-&#22312;MMLU&#30340;&#21830;&#19994;&#20262;&#29702;&#23376;&#39046;&#22495;&#19978;&#65292;&#27604;&#22522;&#32447;LLaMA2-7B&#26377;&#32422;18%&#30340;MC1&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;NL-ITI&#22312;&#25928;&#26524;&#26356;&#22909;&#30340;&#21516;&#26102;&#20063;&#26356;&#23569;&#20405;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18680v1 Announce Type: new  Abstract: Large Language Models (LLM) are prone to returning false information. It constitutes one of major challenges in the AI field. In our work, we explore paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it identifies attention heads, which contain the highest amount of desired type of knowledge (e.g., truthful). Afterwards, during inference, LLM activations are shifted for chosen subset of attention heads. We further improved the ITI framework by introducing a nonlinear probing and multi-token intervention - Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice benchmarks, including TruthfulQA, on which we report around 14% MC1 metric improvement with respect to the baseline ITI results. NL-ITI achieves also encouraging results on other testsets - on Business Ethics subdomain of MMLU, around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI performs better while being less invasive in t
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#36229;&#36234;&#35757;&#32451;&#38598;&#30340;&#20107;&#23454;&#26680;&#26597;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#20986;&#26032;&#39062;&#30340;&#23545;&#25239;&#31639;&#27861;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#26356;&#21152;&#31283;&#20581;&#12290;</title><link>https://arxiv.org/abs/2403.18671</link><description>&lt;p&gt;
&#36229;&#36234;&#35757;&#32451;&#38598;&#30340;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
Fact Checking Beyond Training Set
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18671
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#36229;&#36234;&#35757;&#32451;&#38598;&#30340;&#20107;&#23454;&#26680;&#26597;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#20986;&#26032;&#39062;&#30340;&#23545;&#25239;&#31639;&#27861;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26085;&#24120;&#35328;&#35770;&#30340;&#30495;&#23454;&#24615;&#32791;&#26102;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#35777;&#26126;&#65292;&#24120;&#29992;&#30340;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#65292;&#21363;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#65292;&#22312;&#20351;&#29992;&#19968;&#20010;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21518;&#65292;&#24403;&#22312;&#21478;&#19968;&#20010;&#39046;&#22495;&#20013;&#20351;&#29992;&#26102;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#27969;&#31243;&#30340;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#31639;&#27861;&#65292;&#20351;&#26816;&#32034;&#22120;&#32452;&#20214;&#33021;&#22815;&#25269;&#24481;&#20998;&#24067;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#39318;&#20808;&#22312;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#19978;&#35757;&#32451;&#19968;&#20010;&#21452;&#32534;&#30721;&#22120;&#65292;&#28982;&#21518;&#36890;&#36807;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#23545;&#20004;&#20010;&#29420;&#31435;&#30340;&#25991;&#26723;&#32534;&#30721;&#22120;&#21644;&#35328;&#35770;&#32534;&#30721;&#22120;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#12290;&#28982;&#21518;&#25105;&#20204;&#19987;&#27880;&#20110;&#38405;&#35835;&#22120;&#32452;&#20214;&#65292;&#24182;&#24314;&#35758;&#35757;&#32451;&#23427;&#20197;&#23545;&#35328;&#35770;&#21644;&#35777;&#25454;&#25991;&#26723;&#30340;&#39034;&#24207;&#19981;&#25935;&#24863;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#25903;&#25345;&#36825;&#26679;&#19968;&#20010;&#38405;&#35835;&#22120;&#26174;&#31034;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18671v1 Announce Type: new  Abstract: Evaluating the veracity of everyday claims is time consuming and in some cases requires domain expertise. We empirically demonstrate that the commonly used fact checking pipeline, known as the retriever-reader, suffers from performance deterioration when it is trained on the labeled data from one domain and used in another domain. Afterwards, we delve into each component of the pipeline and propose novel algorithms to address this problem. We propose an adversarial algorithm to make the retriever component robust against distribution shift. Our core idea is to initially train a bi-encoder on the labeled source data, and then, to adversarially train two separate document and claim encoders using unlabeled target data. We then focus on the reader component and propose to train it such that it is insensitive towards the order of claims and evidence documents. Our empirical evaluations support the hypothesis that such a reader shows a higher
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19982;&#20020;&#24202;&#32972;&#26223;&#30456;&#19968;&#33268;&#30340;&#26032;&#39062;&#29983;&#21629;&#20307;&#24449;&#39044;&#27979;&#24615;&#33021;&#25351;&#26631;&#65292;&#36890;&#36807;&#25429;&#25417;&#19982;&#20020;&#24202;&#35268;&#33539;&#30340;&#20559;&#24046;&#12289;&#25972;&#20307;&#36235;&#21183;&#21644;&#36235;&#21183;&#20559;&#24046;&#65292;&#20026;&#26089;&#26399;&#21457;&#29616;&#19981;&#33391;&#20107;&#20214;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.18668</link><description>&lt;p&gt;
&#20197;&#30456;&#20851;&#24615;&#20026;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Aiming for Relevance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18668
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19982;&#20020;&#24202;&#32972;&#26223;&#30456;&#19968;&#33268;&#30340;&#26032;&#39062;&#29983;&#21629;&#20307;&#24449;&#39044;&#27979;&#24615;&#33021;&#25351;&#26631;&#65292;&#36890;&#36807;&#25429;&#25417;&#19982;&#20020;&#24202;&#35268;&#33539;&#30340;&#20559;&#24046;&#12289;&#25972;&#20307;&#36235;&#21183;&#21644;&#36235;&#21183;&#20559;&#24046;&#65292;&#20026;&#26089;&#26399;&#21457;&#29616;&#19981;&#33391;&#20107;&#20214;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#20013;&#65292;&#29983;&#21629;&#20307;&#24449;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#20204;&#29992;&#20110;&#36319;&#36394;&#24739;&#32773;&#30340;&#29366;&#24577;&#65292;&#24182;&#35782;&#21035;&#20020;&#24202;&#19978;&#26174;&#33879;&#30340;&#21464;&#21270;&#12290;&#39044;&#27979;&#29983;&#21629;&#20307;&#24449;&#36712;&#36857;&#23545;&#20110;&#26089;&#26399;&#21457;&#29616;&#19981;&#33391;&#20107;&#20214;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#25351;&#26631;&#22914;RMSE&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#36825;&#20123;&#39044;&#27979;&#30340;&#30495;&#27491;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#29983;&#21629;&#20307;&#24449;&#39044;&#27979;&#24615;&#33021;&#25351;&#26631;&#65292;&#19982;&#20020;&#24202;&#32972;&#26223;&#30456;&#19968;&#33268;&#65292;&#20851;&#27880;&#19982;&#20020;&#24202;&#35268;&#33539;&#30340;&#20559;&#24046;&#12289;&#25972;&#20307;&#36235;&#21183;&#21644;&#36235;&#21183;&#20559;&#24046;&#12290;&#36825;&#20123;&#25351;&#26631;&#28304;&#33258;&#36890;&#36807;&#19982;ICU&#20020;&#24202;&#21307;&#29983;&#30340;&#35775;&#35848;&#33719;&#24471;&#30340;&#23454;&#35777;&#25928;&#29992;&#26354;&#32447;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#65288;MIMIC&#21644;eICU&#65289;&#39564;&#35777;&#20102;&#36825;&#20123;&#25351;&#26631;&#30340;&#26377;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#25351;&#26631;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#24471;&#21040;&#22312;&#39044;&#27979;&#20020;&#24202;&#37325;&#35201;&#20107;&#20214;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#27169;&#22411;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#20020;&#24202;&#23454;&#36341;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18668v1 Announce Type: cross  Abstract: Vital signs are crucial in intensive care units (ICUs). They are used to track the patient's state and to identify clinically significant changes. Predicting vital sign trajectories is valuable for early detection of adverse events. However, conventional machine learning metrics like RMSE often fail to capture the true clinical relevance of such predictions. We introduce novel vital sign prediction performance metrics that align with clinical contexts, focusing on deviations from clinical norms, overall trends, and trend deviations. These metrics are derived from empirical utility curves obtained in a previous study through interviews with ICU clinicians. We validate the metrics' usefulness using simulated and real clinical datasets (MIMIC and eICU). Furthermore, we employ these metrics as loss functions for neural networks, resulting in models that excel in predicting clinically significant events. This research paves the way for clin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#23384;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#27573;&#23450;&#20041;&#39118;&#38505;&#20989;&#25968;&#21644;&#23494;&#24230;&#20989;&#25968;&#65292;&#26377;&#25928;&#25193;&#23637;&#20102;&#26631;&#20934;&#27169;&#22411;&#24182;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18664</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#27573;&#29983;&#23384;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Network-Based Piecewise Survival Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#23384;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#27573;&#23450;&#20041;&#39118;&#38505;&#20989;&#25968;&#21644;&#23494;&#24230;&#20989;&#25968;&#65292;&#26377;&#25928;&#25193;&#23637;&#20102;&#26631;&#20934;&#27169;&#22411;&#24182;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#23384;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#26159;&#22522;&#20110;&#23545;&#26102;&#38388;&#36827;&#34892;&#20998;&#21106;&#30340;&#39118;&#38505;&#20989;&#25968;&#21644;&#23494;&#24230;&#20989;&#25968;&#30340;&#20998;&#27573;&#23450;&#20041;&#32780;&#25351;&#23450;&#30340;&#65307;&#25991;&#20013;&#23637;&#31034;&#20102;&#24120;&#25968;&#21644;&#32447;&#24615;&#20998;&#27573;&#23450;&#20041;&#65292;&#24471;&#21040;&#20102;&#22235;&#20010;&#27169;&#22411;&#30340;&#31995;&#21015;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#24120;&#29992;&#30340;&#31163;&#25955;&#26102;&#38388;&#21644;&#20998;&#27573;&#25351;&#25968;&#27169;&#22411;&#30340;&#24310;&#20280;&#65292;&#20174;&#32780;&#20026;&#36825;&#32452;&#26631;&#20934;&#27169;&#22411;&#22686;&#21152;&#20102;&#28789;&#27963;&#24615;&#12290;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#38598;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#30456;&#36739;&#20110;&#39640;&#24230;&#34920;&#36798;&#33021;&#21147;&#30340;&#26368;&#26032;&#33021;&#37327;&#27169;&#22411;&#65292;&#20165;&#38656;&#35201;&#19968;&#23567;&#37096;&#20998;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18664v1 Announce Type: cross  Abstract: In this paper, a family of neural network-based survival models is presented. The models are specified based on piecewise definitions of the hazard function and the density function on a partitioning of the time; both constant and linear piecewise definitions are presented, resulting in a family of four models. The models can be seen as an extension of the commonly used discrete-time and piecewise exponential models and thereby add flexibility to this set of standard models. Using a simulated dataset the models are shown to perform well compared to the highly expressive, state-of-the-art energy-based model, while only requiring a fraction of the computation time.
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#39640;&#25928;&#30340;&#20107;&#20214;&#20851;&#32852;&#27169;&#22411;&#23545;&#23558;&#30456;&#20851;&#20107;&#20214;&#20998;&#32452;&#21040;&#31751;&#20013;&#20197;&#24555;&#36895;&#35299;&#20915;&#20027;&#35201;&#25925;&#38556;&#24182;&#38477;&#20302;&#29616;&#22330;&#24037;&#31243;&#24072;&#30340;&#30130;&#21171;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.18639</link><description>&lt;p&gt;
&#22823;&#22411;&#20113;&#31995;&#32479;&#20013;&#30340;&#20381;&#36182;&#24863;&#30693;&#20107;&#20214;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Dependency Aware Incident Linking in Large Cloud Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18639
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#39640;&#25928;&#30340;&#20107;&#20214;&#20851;&#32852;&#27169;&#22411;&#23545;&#23558;&#30456;&#20851;&#20107;&#20214;&#20998;&#32452;&#21040;&#31751;&#20013;&#20197;&#24555;&#36895;&#35299;&#20915;&#20027;&#35201;&#25925;&#38556;&#24182;&#38477;&#20302;&#29616;&#22330;&#24037;&#31243;&#24072;&#30340;&#30130;&#21171;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36827;&#34892;&#20102;&#37325;&#22823;&#21487;&#38752;&#24615;&#25913;&#36827;&#65292;&#20294;&#22823;&#35268;&#27169;&#20113;&#26381;&#21153;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#36973;&#36935;&#21487;&#33021;&#26174;&#33879;&#24433;&#21709;&#26381;&#21153;&#21487;&#29992;&#24615;&#21644;&#23458;&#25143;&#28385;&#24847;&#24230;&#30340;&#29983;&#20135;&#20107;&#25925;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20107;&#20214;&#21487;&#33021;&#23548;&#33268;&#22810;&#20010;&#19979;&#28216;&#25925;&#38556;&#65292;&#30001;&#20110;&#32423;&#32852;&#25928;&#24212;&#32780;&#22312;&#19981;&#21516;&#30340;&#20381;&#36182;&#26381;&#21153;&#20043;&#38388;&#21019;&#24314;&#20960;&#20010;&#30456;&#20851;&#20107;&#20214;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#29616;&#22330;&#24037;&#31243;&#24072;(OCEs)&#22312;&#23396;&#31435;&#24773;&#20917;&#19979;&#26816;&#26597;&#36825;&#20123;&#20107;&#20214;&#65292;&#36825;&#23548;&#33268;&#22823;&#37327;&#25163;&#21160;&#25805;&#20316;&#24182;&#22686;&#21152;&#20102;&#20107;&#20214;&#32531;&#35299;&#30340;&#24635;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#39640;&#25928;&#30340;&#20107;&#20214;&#20851;&#32852;&#27169;&#22411;&#23545;&#20110;&#23558;&#30456;&#20851;&#20107;&#20214;&#20998;&#32452;&#21040;&#31751;&#20013;&#20197;&#24555;&#36895;&#35299;&#20915;&#20027;&#35201;&#25925;&#38556;&#24182;&#38477;&#20302;&#29616;&#22330;&#24037;&#31243;&#24072;&#30340;&#30130;&#21171;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20107;&#20214;&#20851;&#32852;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#20107;&#20214;&#30340;&#25991;&#26412;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#22914;&#26631;&#39064;&#12289;&#25551;&#36848;&#12289;&#20005;&#37325;&#24615;&#12289;&#21463;&#24433;&#21709;&#32452;&#20214;&#65289;&#65292;&#20174;&#32780;&#26410;&#33021;&#21033;&#29992;&#26381;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18639v1 Announce Type: cross  Abstract: Despite significant reliability efforts, large-scale cloud services inevitably experience production incidents that can significantly impact service availability and customer's satisfaction. Worse, in many cases one incident can lead to multiple downstream failures due to cascading effects that creates several related incidents across different dependent services. Often time On-call Engineers (OCEs) examine these incidents in silos that lead to significant amount of manual toil and increase the overall time-to-mitigate incidents. Therefore, developing efficient incident linking models is of paramount importance for grouping related incidents into clusters so as to quickly resolve major outages and reduce on-call fatigue. Existing incident linking methods mostly leverages textual and contextual information of incidents (e.g., title, description, severity, impacted components), thus failing to leverage the inter-dependencies between serv
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;Transformer&#30340;&#21019;&#26032;&#26550;&#26500;&#22312;&#21330;&#20013;&#20998;&#21106;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#25913;&#21892;&#21330;&#20013;&#30340;&#35786;&#26029;&#19982;&#20998;&#21106;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18637</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#21330;&#20013;&#20998;&#21106;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Transformers-based architectures for stroke segmentation: A review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;Transformer&#30340;&#21019;&#26032;&#26550;&#26500;&#22312;&#21330;&#20013;&#20998;&#21106;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#25913;&#21892;&#21330;&#20013;&#30340;&#35786;&#26029;&#19982;&#20998;&#21106;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21330;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20840;&#29699;&#20581;&#24247;&#38382;&#39064;&#65292;&#38656;&#35201;&#31934;&#30830;&#39640;&#25928;&#30340;&#35786;&#26029;&#24037;&#20855;&#20197;&#20415;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20986;&#29616;&#25913;&#21464;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#26684;&#23616;&#12290;&#26368;&#36817;&#65292;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;Transformer&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#28145;&#20837;&#25506;&#35752;&#24212;&#29992;&#20110;&#21330;&#20013;&#20998;&#21106;&#39046;&#22495;&#30340;&#26368;&#26032;Transformer&#26550;&#26500;&#12290;&#23427;&#39318;&#20808;&#25506;&#35752;&#20102;&#21330;&#20013;&#30149;&#29702;&#23398;&#12289;&#25104;&#20687;&#26041;&#27861;&#20197;&#21450;&#20934;&#30830;&#35786;&#26029;&#21644;&#20998;&#21106;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#38543;&#21518;&#65292;&#32508;&#36848;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer&#30340;&#22522;&#26412;&#24605;&#24819;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#23427;&#20204;&#26550;&#26500;&#22797;&#26434;&#24615;&#21644;&#36171;&#20104;&#20854;&#33021;&#21147;&#30340;&#22522;&#26412;&#26426;&#21046;&#30340;&#35814;&#32454;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18637v1 Announce Type: cross  Abstract: Stroke remains a significant global health concern, necessitating precise and efficient diagnostic tools for timely intervention and improved patient outcomes. The emergence of deep learning methodologies has transformed the landscape of medical image analysis. Recently, Transformers, initially designed for natural language processing, have exhibited remarkable capabilities in various computer vision applications, including medical image analysis. This comprehensive review aims to provide an in-depth exploration of the cutting-edge Transformer-based architectures applied in the context of stroke segmentation. It commences with an exploration of stroke pathology, imaging modalities, and the challenges associated with accurate diagnosis and segmentation. Subsequently, the review delves into the fundamental ideas of Transformers, offering detailed insights into their architectural intricacies and the underlying mechanisms that empower the
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;BERT&#33719;&#21462;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#23884;&#20837;&#26469;&#34920;&#24449;&#35821;&#38899;&#36716;&#24405;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#36825;&#27604;&#20351;&#29992;Glove&#23884;&#20837;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#21516;&#26102;&#23545;&#32467;&#21512;&#38899;&#39057;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#31574;&#30053;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34701;&#21512;&#22768;&#23398;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#31995;&#32479;&#26159;&#26377;&#30410;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.18635</link><description>&lt;p&gt;
&#20351;&#29992;&#22768;&#23398;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#29305;&#24449;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#30340;&#34701;&#21512;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fusion approaches for emotion recognition from speech using acoustic and text-based features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18635
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;BERT&#33719;&#21462;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#23884;&#20837;&#26469;&#34920;&#24449;&#35821;&#38899;&#36716;&#24405;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#36825;&#27604;&#20351;&#29992;Glove&#23884;&#20837;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#21516;&#26102;&#23545;&#32467;&#21512;&#38899;&#39057;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#31574;&#30053;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34701;&#21512;&#22768;&#23398;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#31995;&#32479;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#22768;&#23398;&#21644;&#22522;&#20110;&#25991;&#26412;&#29305;&#24449;&#23545;&#35821;&#38899;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;BERT&#33719;&#21462;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#23884;&#20837;&#26469;&#34920;&#24449;&#35821;&#38899;&#36716;&#24405;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#36825;&#27604;&#20351;&#29992;Glove&#23884;&#20837;&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#31574;&#30053;&#26469;&#32467;&#21512;&#38899;&#39057;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#23545;IEMOCAP&#21644;MSP-PODCAST&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#34701;&#21512;&#22768;&#23398;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#31995;&#32479;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#26159;&#26377;&#30410;&#30340;&#65292;&#23613;&#31649;&#22312;&#35780;&#20272;&#30340;&#34701;&#21512;&#26041;&#27861;&#20013;&#35266;&#23519;&#21040;&#30340;&#24046;&#24322;&#24456;&#32454;&#24494;&#12290;&#26368;&#21518;&#65292;&#38024;&#23545;IEMOCAP&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#20110;&#23450;&#20041;&#20132;&#21449;&#39564;&#35777;&#25240;&#21472;&#30340;&#26631;&#20934;&#26631;&#20934;&#23545;&#32467;&#26524;&#30340;&#24040;&#22823;&#24433;&#21709;&#25928;&#26524;&#12290;&#29305;&#21035;&#26159;&#65292;&#20026;&#35813;&#25968;&#25454;&#38598;&#21019;&#24314;&#25240;&#21472;&#30340;&#26631;&#20934;&#26041;&#27861;&#23548;&#33268;&#25991;&#26412;&#31995;&#32479;&#24615;&#33021;&#30340;&#39640;&#24230;&#20048;&#35266;&#20272;&#35745;&#65292;&#36825;&#34920;&#26126;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#21487;&#33021;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18635v1 Announce Type: new  Abstract: In this paper, we study different approaches for classifying emotions from speech using acoustic and text-based features. We propose to obtain contextualized word embeddings with BERT to represent the information contained in speech transcriptions and show that this results in better performance than using Glove embeddings. We also propose and compare different strategies to combine the audio and text modalities, evaluating them on IEMOCAP and MSP-PODCAST datasets. We find that fusing acoustic and text-based systems is beneficial on both datasets, though only subtle differences are observed across the evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect that the criteria used to define the cross-validation folds have on results. In particular, the standard way of creating folds for this dataset results in a highly optimistic estimation of performance for the text-based system, suggesting that some previous works ma
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#38024;&#23545;&#38463;&#26681;&#24311;&#22320;&#21306;&#30340;&#24773;&#20917;&#65292;&#21457;&#23637;&#21644;&#35780;&#20272;&#20102;&#29992;&#20110;&#35782;&#21035;2&#22411;&#31958;&#23615;&#30149;&#65288;T2D&#65289;&#21644;&#31958;&#23615;&#30149;&#21069;&#26399;&#65288;PD&#65289;&#39118;&#38505;&#20154;&#32676;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#37096;&#20998;&#27169;&#22411;&#22312;&#20004;&#20010;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18631</link><description>&lt;p&gt;
&#39318;&#27425;&#22312;&#38463;&#26681;&#24311;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35782;&#21035;&#31958;&#23615;&#30149;&#39118;&#38505;&#20154;&#32676;&#30340;&#21021;&#27493;&#20307;&#39564;
&lt;/p&gt;
&lt;p&gt;
First Experiences with the Identification of People at Risk for Diabetes in Argentina using Machine Learning Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18631
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#38024;&#23545;&#38463;&#26681;&#24311;&#22320;&#21306;&#30340;&#24773;&#20917;&#65292;&#21457;&#23637;&#21644;&#35780;&#20272;&#20102;&#29992;&#20110;&#35782;&#21035;2&#22411;&#31958;&#23615;&#30149;&#65288;T2D&#65289;&#21644;&#31958;&#23615;&#30149;&#21069;&#26399;&#65288;PD&#65289;&#39118;&#38505;&#20154;&#32676;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#37096;&#20998;&#27169;&#22411;&#22312;&#20004;&#20010;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;2&#22411;&#31958;&#23615;&#30149;&#65288;T2D&#65289;&#21644;&#31958;&#23615;&#30149;&#21069;&#26399;&#65288;PD&#65289;&#23545;&#21307;&#23398;&#26159;&#19968;&#20010;&#30495;&#27491;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#32570;&#20047;&#30149;&#21407;&#24615;&#30151;&#29366;&#21644;&#24050;&#30693;&#30456;&#20851;&#21361;&#38505;&#22240;&#32032;&#12290;&#23613;&#31649;&#26377;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25552;&#35758;&#20351;&#24471;&#35782;&#21035;&#24739;&#30149;&#39118;&#38505;&#30340;&#20154;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#36825;&#31181;&#30149;&#30151;&#30340;&#24615;&#36136;&#20351;&#24471;&#19968;&#20010;&#36866;&#29992;&#20110;&#19968;&#31181;&#20154;&#32676;&#30340;&#27169;&#22411;&#26410;&#24517;&#36866;&#29992;&#20110;&#21478;&#19968;&#31181;&#20154;&#32676;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#38463;&#26681;&#24311;&#29305;&#21035;&#21457;&#23637;&#21644;&#35780;&#20272;&#29992;&#20110;&#35782;&#21035;T2D&#21644;PD&#39118;&#38505;&#20154;&#32676;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25968;&#25454;&#24211;&#32463;&#36807;&#24443;&#24213;&#39044;&#22788;&#29702;&#65292;&#29983;&#25104;&#20102;&#19977;&#20010;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#21040;&#35760;&#24405;&#25968;&#21644;&#21487;&#29992;&#21464;&#37327;&#30340;&#26435;&#34913;&#12290;&#24212;&#29992;&#20102;5&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#27169;&#22411;&#21518;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#20013;&#19968;&#20123;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;RF&#12289;DT&#21644;ANN&#23637;&#29616;&#20102;&#24456;&#22823;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18631v1 Announce Type: new  Abstract: Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for medicine due to the absence of pathogenic symptoms and the lack of known associated risk factors. Even though some proposals for machine learning models enable the identification of people at risk, the nature of the condition makes it so that a model suitable for one population may not necessarily be suitable for another. In this article, the development and assessment of predictive models to identify people at risk for T2D and PD specifically in Argentina are discussed. First, the database was thoroughly preprocessed and three specific datasets were generated considering a compromise between the number of records and the amount of available variables. After applying 5 different classification models, the results obtained show that a very good performance was observed for two datasets with some of these models. In particular, RF, DT, and ANN demonstrated great c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;CNN Lipschitz&#24120;&#25968;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#22823;&#21367;&#31215;&#22359;&#20026;&#19968;&#31995;&#21015;&#36739;&#23567;&#22359;&#65292;&#24182;&#36890;&#36807;&#35843;&#33410;&#21010;&#20998;&#22240;&#23376;&#20197;&#20248;&#20808;&#32771;&#34385;&#20934;&#30830;&#24615;&#25110;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18613</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;CNN&#30340;Lipschitz&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Scalable Lipschitz Estimation for CNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18613
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;CNN Lipschitz&#24120;&#25968;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#22823;&#21367;&#31215;&#22359;&#20026;&#19968;&#31995;&#21015;&#36739;&#23567;&#22359;&#65292;&#24182;&#36890;&#36807;&#35843;&#33410;&#21010;&#20998;&#22240;&#23376;&#20197;&#20248;&#20808;&#32771;&#34385;&#20934;&#30830;&#24615;&#25110;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#27491;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#23545;&#36890;&#29992;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#35780;&#20272;&#24456;&#26377;&#29992;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#30456;&#20851;&#24212;&#29992;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#20272;&#35745;Lipschitz&#24120;&#25968;&#30340;&#26041;&#27861;&#21487;&#33021;&#24456;&#32039;&#33268;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;CNNs&#26102;&#65292;&#23427;&#20204;&#30340;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;CNNs Lipschitz&#24120;&#25968;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#32852;&#21512;&#23618;&#21644;&#23485;&#24230;&#21010;&#20998;&#23558;&#22823;&#21367;&#31215;&#22359;&#20998;&#21106;&#20026;&#19968;&#31995;&#21015;&#36739;&#23567;&#30340;&#22359;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22823;&#22359;&#30340;Lipschitz&#24120;&#25968;&#19978;&#30028;&#19982;&#36739;&#23567;&#22359;&#30340;Lipschitz&#24120;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#21464;&#21270;&#21010;&#20998;&#22240;&#23376;&#65292;&#24471;&#21040;&#30340;&#26041;&#27861;&#21487;&#20197;&#35843;&#33410;&#20197;&#20248;&#20808;&#32771;&#34385;&#20934;&#30830;&#24615;&#25110;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#25903;&#25345;&#24182;&#34892;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18613v1 Announce Type: new  Abstract: Estimating the Lipschitz constant of deep neural networks is of growing interest as it is useful for informing on generalisability and adversarial robustness. Convolutional neural networks (CNNs) in particular, underpin much of the recent success in computer vision related applications. However, although existing methods for estimating the Lipschitz constant can be tight, they have limited scalability when applied to CNNs. To tackle this, we propose a novel method to accelerate Lipschitz constant estimation for CNNs. The core idea is to divide a large convolutional block via a joint layer and width-wise partition, into a collection of smaller blocks. We prove an upper-bound on the Lipschitz constant of the larger block in terms of the Lipschitz constants of the smaller blocks. Through varying the partition factor, the resulting method can be adjusted to prioritise either accuracy or scalability and permits parallelisation. We demonstrate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24322;&#36136;Peridynamic&#31070;&#32463;&#31639;&#23376;&#65288;HeteroPNO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23454;&#39564;&#27979;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#38750;&#23616;&#37096;&#26412;&#26500;&#23450;&#24459;&#21644;&#26448;&#26009;&#24494;&#32467;&#26500;&#65292;&#20197;&#20415;&#25429;&#33719;&#29983;&#29289;&#32452;&#32455;&#20013;&#30340;&#32420;&#32500;&#26041;&#21521;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.18597</link><description>&lt;p&gt;
&#24322;&#36136;Peridynamic&#31070;&#32463;&#31639;&#23376;: &#20174;&#25968;&#23383;&#22270;&#20687;&#30456;&#20851;&#27979;&#37327;&#20013;&#21457;&#29616;&#29983;&#29289;&#32452;&#32455;&#26412;&#26500;&#23450;&#24459;&#21644;&#24494;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Peridynamic Neural Operators: Discover Biotissue Constitutive Law and Microstructure From Digital Image Correlation Measurements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18597
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24322;&#36136;Peridynamic&#31070;&#32463;&#31639;&#23376;&#65288;HeteroPNO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23454;&#39564;&#27979;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#38750;&#23616;&#37096;&#26412;&#26500;&#23450;&#24459;&#21644;&#26448;&#26009;&#24494;&#32467;&#26500;&#65292;&#20197;&#20415;&#25429;&#33719;&#29983;&#29289;&#32452;&#32455;&#20013;&#30340;&#32420;&#32500;&#26041;&#21521;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#32452;&#32455;&#26159;&#39640;&#24230;&#26377;&#26426;&#21270;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#29305;&#23450;&#30340;&#33014;&#21407;&#32420;&#32500;&#25490;&#21015;&#65292;&#20174;&#28857;&#21040;&#28857;&#37117;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#31181;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#23545;&#32452;&#32455;&#21151;&#33021;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#27492;&#21457;&#29616;&#21644;&#29702;&#35299;&#36825;&#31181;&#32420;&#32500;&#26041;&#21521;&#30340;&#20998;&#24067;&#20174;&#23454;&#39564;&#27979;&#37327;&#25968;&#25454;&#65292;&#22914;&#25968;&#23383;&#22270;&#20687;&#30456;&#20851;&#25968;&#25454;&#20013;&#23588;&#20026;&#20851;&#38190;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24322;&#36136;Peridynamic&#31070;&#32463;&#31639;&#23376;&#65288;HeteroPNO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#25968;&#25454;&#30340;&#24322;&#36136;&#21508;&#21521;&#24322;&#24615;&#26448;&#26009;&#26412;&#26500;&#24314;&#27169;&#12290;&#26088;&#22312;&#20174;&#21152;&#36733;&#22330;&#20301;&#31227;&#22330;&#27979;&#37327;&#20013;&#23398;&#20064;&#38750;&#23616;&#37096;&#26412;&#26500;&#23450;&#24459;&#20197;&#21450;&#26448;&#26009;&#24494;&#32467;&#26500;&#65292;&#20197;&#24322;&#36136;&#32420;&#32500;&#23450;&#21521;&#22330;&#30340;&#24418;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#20197;&#31070;&#32463;&#32593;&#32476;&#20026;&#22522;&#30784;&#30340;&#26680;&#20989;&#25968;&#21644;&#38750;&#23616;&#37096;&#38190;&#21147;&#30340;&#22343;&#21248;&#26412;&#26500;&#23450;&#24459;&#65292;&#20197;&#25429;&#25417;&#23436;&#25972;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18597v1 Announce Type: cross  Abstract: Human tissues are highly organized structures with specific collagen fiber arrangements varying from point to point. The effects of such heterogeneity play an important role for tissue function, and hence it is of critical to discover and understand the distribution of such fiber orientations from experimental measurements, such as the digital image correlation data. To this end, we introduce the heterogeneous peridynamic neural operator (HeteroPNO) approach, for data-driven constitutive modeling of heterogeneous anisotropic materials. The goal is to learn both a nonlocal constitutive law together with the material microstructure, in the form of a heterogeneous fiber orientation field, from loading field-displacement field measurements. To this end, we propose a two-phase learning approach. Firstly, we learn a homogeneous constitutive law in the form of a neural network-based kernel function and a nonlocal bond force, to capture comple
&lt;/p&gt;</description></item><item><title>&#32479;&#19968;&#36755;&#20837;&#20250;&#38477;&#20302;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#28608;&#27963;&#31232;&#30095;&#24230;&#65292;&#22686;&#21152;&#33021;&#37327;&#28040;&#32791;&#21644;&#20915;&#31574;&#24310;&#36831;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#36825;&#19968;&#28857;&#25552;&#20132;&#28023;&#32501;&#31034;&#20363;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.18587</link><description>&lt;p&gt;
&#32479;&#19968;&#36755;&#20837;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#28608;&#27963;&#31232;&#30095;&#24230;&#21644;&#33021;&#37327;-&#24310;&#36831;&#25915;&#20987;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency Attacks in Computer Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18587
&lt;/p&gt;
&lt;p&gt;
&#32479;&#19968;&#36755;&#20837;&#20250;&#38477;&#20302;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#28608;&#27963;&#31232;&#30095;&#24230;&#65292;&#22686;&#21152;&#33021;&#37327;&#28040;&#32791;&#21644;&#20915;&#31574;&#24310;&#36831;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#36825;&#19968;&#28857;&#25552;&#20132;&#28023;&#32501;&#31034;&#20363;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#25928;&#29575;&#22312;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#33021;&#37327;&#28040;&#32791;&#21644;&#20915;&#31574;&#24310;&#36831;&#26159;&#20445;&#35777;&#21487;&#25345;&#32493;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#33021;&#37327;&#28040;&#32791;&#21644;&#20915;&#31574;&#24310;&#36831;&#19981;&#20855;&#22791;&#23545;&#25239;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#25512;&#26029;&#26102;&#35745;&#31639;&#24182;&#25552;&#20132;&#25152;&#35859;&#30340;&#28023;&#32501;&#31034;&#20363;&#65292;&#20174;&#32780;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#20915;&#31574;&#24310;&#36831;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#21046;&#20316;&#20102;&#20855;&#26377;&#36739;&#20302;&#28608;&#27963;&#31232;&#30095;&#24230;&#30340;&#36755;&#20837;&#65292;&#32780;&#36825;&#20123;&#36755;&#20837;&#26412;&#26469;&#21487;&#20197;&#29992;&#20110;&#21152;&#36895;&#35745;&#31639;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#20123;&#33021;&#37327;-&#24310;&#36831;&#25915;&#20987;&#26159;&#22914;&#20309;&#20943;&#23569;&#28608;&#27963;&#31232;&#30095;&#24230;&#30340;&#26426;&#29702;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616;&#36755;&#20837;&#30340;&#32479;&#19968;&#24615;&#26159;&#20851;&#38190;&#30340;&#25512;&#21160;&#22240;&#32032;&#12290;&#32479;&#19968;&#22270;&#20687;&#65292;&#20063;&#23601;&#26159;&#22823;&#37096;&#20998;&#20026;&#24179;&#22374;&#12289;&#39068;&#33394;&#22343;&#21248;&#30340;&#34920;&#38754;&#26500;&#25104;&#30340;&#22270;&#20687;&#65292;&#30001;&#20110;&#21367;&#31215;&#12289;&#25209;&#37327;&#35268;&#33539;&#21270;&#31561;&#29305;&#23450;&#30456;&#20114;&#20316;&#29992;&#65292;&#20250;&#35302;&#21457;&#26356;&#22810;&#30340;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18587v1 Announce Type: cross  Abstract: Resource efficiency plays an important role for machine learning nowadays. The energy and decision latency are two critical aspects to ensure a sustainable and practical application. Unfortunately, the energy consumption and decision latency are not robust against adversaries. Researchers have recently demonstrated that attackers can compute and submit so-called sponge examples at inference time to increase the energy consumption and decision latency of neural networks. In computer vision, the proposed strategy crafts inputs with less activation sparsity which could otherwise be used to accelerate the computation. In this paper, we analyze the mechanism how these energy-latency attacks reduce activation sparsity. In particular, we find that input uniformity is a key enabler. A uniform image, that is, an image with mostly flat, uniformly colored surfaces, triggers more activations due to a specific interplay of convolution, batch normal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#24402;&#19968;&#21270;&#27969;&#21644;&#24067;&#23572;&#26465;&#20214;&#30340;&#31616;&#21333;&#26550;&#26500;&#65292;&#23558;&#39640;&#33021;&#29289;&#29702;&#27169;&#25311;&#20107;&#20214;&#30340;&#20998;&#24067;&#26377;&#25928;&#22320;&#36716;&#25442;&#20026;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#26657;&#27491;&#26041;&#27861;&#65292;&#24182;&#22312;&#29609;&#20855;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.18582</link><description>&lt;p&gt;
&#29992;&#21333;&#19968;&#24402;&#19968;&#21270;&#27969;&#21644;&#19968;&#20010;&#24320;&#20851;&#25913;&#36827;&#39640;&#33021;&#29289;&#29702;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
One flow to correct them all: improving simulations in high-energy physics with a single normalising flow and a switch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18582
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#24402;&#19968;&#21270;&#27969;&#21644;&#24067;&#23572;&#26465;&#20214;&#30340;&#31616;&#21333;&#26550;&#26500;&#65292;&#23558;&#39640;&#33021;&#29289;&#29702;&#27169;&#25311;&#20107;&#20214;&#30340;&#20998;&#24067;&#26377;&#25928;&#22320;&#36716;&#25442;&#20026;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#26657;&#27491;&#26041;&#27861;&#65292;&#24182;&#22312;&#29609;&#20855;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#20107;&#20214;&#26159;&#20960;&#20046;&#25152;&#26377;&#39640;&#33021;&#29289;&#29702;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#20013;&#30340;&#19981;&#23436;&#32654;&#21487;&#33021;&#23548;&#33268;&#35266;&#27979;&#25968;&#25454;&#21644;&#27169;&#25311;&#20107;&#20214;&#20043;&#38388;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24046;&#24322;&#12290;&#36825;&#31181;&#27169;&#25311;&#19981;&#23436;&#20840;&#24615;&#23545;&#30456;&#20851;&#21487;&#35266;&#27979;&#37327;&#30340;&#24433;&#21709;&#24517;&#39035;&#36890;&#36807;&#27604;&#20363;&#22240;&#23376;&#12289;&#26435;&#37325;&#25110;&#20462;&#25913;&#21487;&#35266;&#27979;&#37327;&#21450;&#20854;&#30456;&#20851;&#24615;&#30340;&#20998;&#24067;&#26469;&#26377;&#25928;&#22320;&#36827;&#34892;&#26657;&#27491;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26657;&#27491;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#21333;&#19968;&#27491;&#24120;&#21270;&#27969;&#21644;&#24067;&#23572;&#26465;&#20214;&#30340;&#31616;&#21333;&#26550;&#26500;&#23558;&#19968;&#20010;&#22810;&#32500;&#20998;&#24067;&#65288;&#27169;&#25311;&#65289;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#22810;&#32500;&#20998;&#24067;&#65288;&#25968;&#25454;&#65289;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#29609;&#20855;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#25968;&#25454;&#38598;&#23384;&#22312;&#20960;&#20010;&#21487;&#35266;&#27979;&#37327;&#21450;&#20854;&#30456;&#20851;&#24615;&#30340;&#38750;&#24179;&#20961;&#27169;&#25311;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18582v1 Announce Type: cross  Abstract: Simulated events are key ingredients in almost all high-energy physics analyses. However, imperfections in the simulation can lead to sizeable differences between the observed data and simulated events. The effects of such mismodelling on relevant observables must be corrected either effectively via scale factors, with weights or by modifying the distributions of the observables and their correlations. We introduce a correction method that transforms one multidimensional distribution (simulation) into another one (data) using a simple architecture based on a single normalising flow with a boolean condition. We demonstrate the effectiveness of the method on a physics-inspired toy dataset with non-trivial mismodelling of several observables and their correlations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#24433;&#21709;&#26368;&#22823;&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#25910;&#38598;&#20102;&#30456;&#20851;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.18579</link><description>&lt;p&gt;
&#20851;&#20110;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Optimizing Hyperparameters for Quantum Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#24433;&#21709;&#26368;&#22823;&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#25910;&#38598;&#20102;&#30456;&#20851;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26085;&#30410;&#22686;&#24378;&#30340;&#33021;&#21147;&#19982;&#35757;&#32451;&#25152;&#38656;&#30340;&#28023;&#37327;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#23494;&#19981;&#21487;&#20998;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#36890;&#24120;&#34987;&#22806;&#21253;&#21040;&#39640;&#24615;&#33021;&#35745;&#31639;&#35774;&#26045;&#20013;&#65292;&#25105;&#20204;&#22312;&#37027;&#37324;&#24320;&#22987;&#32463;&#21382;&#21040;&#20256;&#32479;&#39640;&#24615;&#33021;&#35745;&#31639;&#30828;&#20214;&#25193;&#23637;&#30340;&#38480;&#21046;&#65292;&#27491;&#22914;&#25705;&#23572;&#23450;&#24459;&#25152;&#39044;&#27979;&#30340;&#37027;&#26679;&#12290;&#23613;&#31649;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#24182;&#34892;&#21270;&#21644;&#20248;&#21270;&#24037;&#20316;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#25968;&#21608;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#36825;&#19982;&#24040;&#22823;&#30340; $CO_2$ &#25490;&#25918;&#26377;&#20851;&#12290;&#37327;&#23376;&#35745;&#31639;&#65292;&#29305;&#21035;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#65292;&#21487;&#20197;&#25552;&#20379;&#26174;&#33879;&#30340;&#29702;&#35770;&#21152;&#36895;&#21644;&#22686;&#24378;&#30340;&#34920;&#29616;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451; QML &#27169;&#22411;&#38656;&#35201;&#35843;&#25972;&#21508;&#31181;&#36229;&#21442;&#25968;&#65292;&#36825;&#26159;&#19968;&#39033;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#27425;&#20248;&#36873;&#25321;&#21487;&#33021;&#20250;&#26497;&#22823;&#24433;&#21709;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#24615;&#21644;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#25910;&#38598;&#20102;&#20851;&#20110; QML &#27169;&#22411;&#24615;&#33021;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18579v1 Announce Type: new  Abstract: The increasing capabilities of Machine Learning (ML) models go hand in hand with an immense amount of data and computational power required for training. Therefore, training is usually outsourced into HPC facilities, where we have started to experience limits in scaling conventional HPC hardware, as theorized by Moore's law. Despite heavy parallelization and optimization efforts, current state-of-the-art ML models require weeks for training, which is associated with an enormous $CO_2$ footprint. Quantum Computing, and specifically Quantum Machine Learning (QML), can offer significant theoretical speed-ups and enhanced expressive power. However, training QML models requires tuning various hyperparameters, which is a nontrivial task and suboptimal choices can highly affect the trainability and performance of the models. In this study, we identify the most impactful hyperparameters and collect data about the performance of QML models. We co
&lt;/p&gt;</description></item><item><title>SteinGen&#26159;&#19968;&#31181;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#26679;&#26412;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;Stein&#26041;&#27861;&#21644;MCMC&#21160;&#21147;&#23398;&#65292;&#36866;&#29992;&#20110;&#21482;&#26377;&#19968;&#27425;&#35266;&#23519;&#21040;&#30340;&#22270;&#24418;&#65292;&#36991;&#20813;&#20102;&#21442;&#25968;&#20272;&#35745;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.18578</link><description>&lt;p&gt;
SteinGen: &#29983;&#25104;&#24544;&#23454;&#21644;&#22810;&#26679;&#21270;&#30340;&#22270;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
SteinGen: Generating Fidelitous and Diverse Graph Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18578
&lt;/p&gt;
&lt;p&gt;
SteinGen&#26159;&#19968;&#31181;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#26679;&#26412;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;Stein&#26041;&#27861;&#21644;MCMC&#21160;&#21147;&#23398;&#65292;&#36866;&#29992;&#20110;&#21482;&#26377;&#19968;&#27425;&#35266;&#23519;&#21040;&#30340;&#22270;&#24418;&#65292;&#36991;&#20813;&#20102;&#21442;&#25968;&#20272;&#35745;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20445;&#30041;&#29305;&#24449;&#32467;&#26500;&#24182;&#20419;&#36827;&#26679;&#26412;&#22810;&#26679;&#24615;&#30340;&#22270;&#24418;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#22270;&#24418;&#35266;&#23519;&#25968;&#37327;&#36739;&#23569;&#26102;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20165;&#20174;&#19968;&#20010;&#35266;&#23519;&#21040;&#30340;&#22270;&#24418;&#29983;&#25104;&#22270;&#24418;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#22270;&#24418;&#30340;&#35774;&#32622;&#20013;&#20197;&#25351;&#25968;&#38543;&#26426;&#22270;&#24418;&#27169;&#22411;&#30340;&#24418;&#24335;&#34920;&#36798;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#29983;&#25104;&#36807;&#31243;SteinGen&#32467;&#21512;&#20102;Stein&#26041;&#27861;&#21644;&#22522;&#20110;MCMC&#30340;&#39532;&#23572;&#21487;&#22827;&#21160;&#21147;&#23398;&#30340;&#24605;&#24819;&#65292;&#35813;&#21160;&#21147;&#23398;&#22522;&#20110;&#30446;&#26631;&#27169;&#22411;&#30340;Stein&#31639;&#23376;&#12290;SteinGen&#20351;&#29992;&#19982;e&#30456;&#20851;&#32852;&#30340;Glauber&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18578v1 Announce Type: cross  Abstract: Generating graphs that preserve characteristic structures while promoting sample diversity can be challenging, especially when the number of graph observations is small. Here, we tackle the problem of graph generation from only one observed graph. The classical approach of graph generation from parametric models relies on the estimation of parameters, which can be inconsistent or expensive to compute due to intractable normalisation constants. Generative modelling based on machine learning techniques to generate high-quality graph samples avoids parameter estimation but usually requires abundant training samples. Our proposed generating procedure, SteinGen, which is phrased in the setting of graphs as realisations of exponential random graph models, combines ideas from Stein's method and MCMC by employing Markovian dynamics which are based on a Stein operator for the target model. SteinGen uses the Glauber dynamics associated with an e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27700;&#37197;&#31995;&#32479;&#30340;&#29289;&#29702;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#27700;&#21147;&#21407;&#29702;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#37325;&#24314;&#27700;&#21147;&#29366;&#24577;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.18570</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27700;&#37197;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Graph Neural Networks for Water Distribution Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18570
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27700;&#37197;&#31995;&#32479;&#30340;&#29289;&#29702;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#27700;&#21147;&#21407;&#29702;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#37325;&#24314;&#27700;&#21147;&#29366;&#24577;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#37197;&#31995;&#32479;&#65288;WDS&#65289;&#26159;&#22478;&#24066;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#12290;&#30001;&#20110;&#19990;&#30028;70%&#30340;&#20154;&#21475;&#21487;&#33021;&#20250;&#22312;2050&#24180;&#29983;&#27963;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#65292;&#22240;&#27492;&#23545;&#20110;WDS&#30340;&#39640;&#25928;&#20223;&#30495;&#21644;&#35268;&#21010;&#24037;&#20855;&#22312;&#23454;&#29616;&#32852;&#21512;&#22269;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65288;SDG&#65289;6 - &#8220;&#20026;&#25152;&#26377;&#20154;&#25552;&#20379;&#28165;&#27905;&#27700;&#21644;&#21355;&#29983;&#35774;&#26045;&#8221;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#20223;&#30495;&#22120;&#65292;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#26159;&#19968;&#20010;&#29992;&#20110;WDS&#20013;&#30340;&#27700;&#21147;&#29366;&#24577;&#20272;&#35745;&#30340;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#19968;&#31181;&#36882;&#24402;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#20960;&#20010;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#23618;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#21019;&#26032;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#27700;&#21147;&#21407;&#29702;&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#25512;&#26029;&#20986;&#20004;&#20010;&#39069;&#22806;&#30340;&#27700;&#21147;&#29366;&#24577;&#29305;&#24449;&#65292;&#20174;&#32780;&#37325;&#24314;&#21487;&#29992;&#30340;&#22320;&#38754;&#23454;&#20917;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18570v1 Announce Type: cross  Abstract: Water distribution systems (WDS) are an integral part of critical infrastructure which is pivotal to urban development. As 70% of the world's population will likely live in urban environments in 2050, efficient simulation and planning tools for WDS play a crucial role in reaching UN's sustainable developmental goal (SDG) 6 - "Clean water and sanitation for all". In this realm, we propose a novel and efficient machine learning emulator, more precisely, a physics-informed deep learning (DL) model, for hydraulic state estimation in WDS. Using a recursive approach, our model only needs a few graph convolutional neural network (GCN) layers and employs an innovative algorithm based on message passing. Unlike conventional machine learning tasks, the model uses hydraulic principles to infer two additional hydraulic state features in the process of reconstructing the available ground truth feature in an unsupervised manner. To the best of our k
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#21160;&#24577;IR&#25481;&#30005;&#39044;&#27979;&#30340;PDN&#24863;&#30693;GNN-CNN&#24322;&#26500;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;PDNGraph&#22270;&#32467;&#26500;&#21644;&#21452;&#20998;&#25903;&#24322;&#26500;&#32593;&#32476;PDNNet&#65292;&#20197;&#21516;&#26102;&#32771;&#34385;PDN&#32467;&#26500;&#21644;&#21333;&#20803;-PDN&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#39044;&#27979;IR&#25481;&#30005;&#12290;</title><link>https://arxiv.org/abs/2403.18569</link><description>&lt;p&gt;
PDNNet&#65306;&#38754;&#21521;&#21160;&#24577;IR&#25481;&#30005;&#39044;&#27979;&#30340;PDN&#24863;&#30693;GNN-CNN&#24322;&#26500;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18569
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#21160;&#24577;IR&#25481;&#30005;&#39044;&#27979;&#30340;PDN&#24863;&#30693;GNN-CNN&#24322;&#26500;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;PDNGraph&#22270;&#32467;&#26500;&#21644;&#21452;&#20998;&#25903;&#24322;&#26500;&#32593;&#32476;PDNNet&#65292;&#20197;&#21516;&#26102;&#32771;&#34385;PDN&#32467;&#26500;&#21644;&#21333;&#20803;-PDN&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#39044;&#27979;IR&#25481;&#30005;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#28304;&#20379;&#24212;&#32593;&#32476;&#65288;PDN&#65289;&#19978;&#30340;IR&#25481;&#30005;&#19982;PDN&#30340;&#37197;&#32622;&#21644;&#30005;&#27969;&#28040;&#32791;&#23494;&#20999;&#30456;&#20851;&#12290;&#38543;&#30528;&#38598;&#25104;&#30005;&#36335;&#65288;IC&#65289;&#35774;&#35745;&#30340;&#19981;&#26029;&#22686;&#22823;&#65292;&#21160;&#24577;IR&#25481;&#30005;&#20223;&#30495;&#21464;&#24471;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;IR&#25481;&#30005;&#39044;&#27979;&#34987;&#25506;&#32034;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#32771;&#34385;&#19981;&#20165;&#22914;&#20309;&#27491;&#30830;&#34920;&#31034;&#21333;&#20803;-PDN&#20851;&#31995;&#65292;&#36824;&#32771;&#34385;&#22914;&#20309;&#22312;&#29305;&#24449;&#32858;&#21512;&#36807;&#31243;&#20013;&#27169;&#25311;IR&#25481;&#30005;&#36981;&#24490;&#20854;&#29289;&#29702;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#32467;&#26500;&#65292;PDNGraph&#65292;&#32479;&#19968;&#20102;PDN&#32467;&#26500;&#21644;&#32454;&#31890;&#24230;&#21333;&#20803;-PDN&#20851;&#31995;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#20998;&#25903;&#24322;&#26500;&#32593;&#32476;&#65292;PDNNet&#65292;&#23558;&#20004;&#20010;&#24182;&#34892;&#30340;GNN-CNN&#20998;&#25903;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#26377;&#21033;&#20110;&#25429;&#25417;&#19978;&#36848;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18569v1 Announce Type: cross  Abstract: IR drop on the power delivery network (PDN) is closely related to PDN's configuration and cell current consumption. As the integrated circuit (IC) design is growing larger, dynamic IR drop simulation becomes computationally unaffordable and machine learning based IR drop prediction has been explored as a promising solution. Although CNN-based methods have been adapted to IR drop prediction task in several works, the shortcomings of overlooking PDN configuration is non-negligible. In this paper, we consider not only how to properly represent cell-PDN relation, but also how to model IR drop following its physical nature in the feature aggregation procedure. Thus, we propose a novel graph structure, PDNGraph, to unify the representations of the PDN structure and the fine-grained cell-PDN relation. We further propose a dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN branches to favorably capture the above feat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#39044;&#35757;&#32451;&#26469;&#22686;&#24378;&#20851;&#38190;&#35789;&#26816;&#27979;&#27169;&#22411;&#22312;&#22024;&#26434;&#26465;&#20214;&#19979;&#30340;&#31283;&#20581;&#24615;&#65292;&#21457;&#29616;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20248;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18560</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23454;&#29616;&#25239;&#22122;&#22768;&#20851;&#38190;&#35789;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Noise-Robust Keyword Spotting through Self-supervised Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#39044;&#35757;&#32451;&#26469;&#22686;&#24378;&#20851;&#38190;&#35789;&#26816;&#27979;&#27169;&#22411;&#22312;&#22024;&#26434;&#26465;&#20214;&#19979;&#30340;&#31283;&#20581;&#24615;&#65292;&#21457;&#29616;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20248;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21161;&#25163;&#29616;&#22312;&#24050;&#32463;&#24191;&#27867;&#21487;&#29992;&#65292;&#20026;&#20102;&#21551;&#21160;&#23427;&#20204;&#65292;&#20351;&#29992;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;KWS&#65289;&#31639;&#27861;&#12290;&#29616;&#20195;KWS&#31995;&#32479;&#20027;&#35201;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#25165;&#33021;&#36798;&#21040;&#33391;&#22909;&#24615;&#33021;&#12290;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#26410;&#26631;&#35760;&#25968;&#25454;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#22312;&#24178;&#20928;&#26465;&#20214;&#19979;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;Data2Vec&#31561;SSL&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#22686;&#24378;KWS&#27169;&#22411;&#22312;&#22024;&#26434;&#26465;&#20214;&#19979;&#30340;&#31283;&#20581;&#24615;&#65292;&#36825;&#26041;&#38754;&#30740;&#31350;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#12290;&#23545;&#19977;&#31181;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36827;&#34892;KWS&#30340;&#24494;&#35843;&#12290;&#36825;&#20123;&#27169;&#22411;&#28982;&#21518;&#34987;&#27979;&#35797;&#24182;&#19982;&#20351;&#29992;&#20004;&#31181;&#22522;&#20934;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#20854;&#20013;&#19968;&#31181;&#26159;&#20351;&#29992;&#24178;&#20928;&#25968;&#25454;&#36827;&#34892;&#26631;&#20934;&#35757;&#32451;&#65292;&#21478;&#19968;&#31181;&#26159;&#22810;&#26679;&#24335;&#35757;&#32451;&#65288;MTR&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20248;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18560v1 Announce Type: cross  Abstract: Voice assistants are now widely available, and to activate them a keyword spotting (KWS) algorithm is used. Modern KWS systems are mainly trained using supervised learning methods and require a large amount of labelled data to achieve a good performance. Leveraging unlabelled data through self-supervised learning (SSL) has been shown to increase the accuracy in clean conditions. This paper explores how SSL pretraining such as Data2Vec can be used to enhance the robustness of KWS models in noisy conditions, which is under-explored.   Models of three different sizes are pretrained using different pretraining approaches and then fine-tuned for KWS. These models are then tested and compared to models trained using two baseline supervised learning methods, one being standard training using clean data and the other one being multi-style training (MTR). The results show that pretraining and fine-tuning on clean data is superior to supervised 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#20851;&#27880;&#35821;&#22659;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20013;&#25991;&#38405;&#35835;&#20219;&#21153;&#20013;&#30340;&#20957;&#35270;&#25345;&#32493;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.18542</link><description>&lt;p&gt;
&#20851;&#27880;&#35821;&#22659;&#35821;&#20041;&#30456;&#20851;&#24615;&#39044;&#27979;&#20013;&#25991;&#21477;&#23376;&#38405;&#35835;
&lt;/p&gt;
&lt;p&gt;
Attention-aware semantic relevance predicting Chinese sentence reading
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18542
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#20851;&#27880;&#35821;&#22659;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20013;&#25991;&#38405;&#35835;&#20219;&#21153;&#20013;&#30340;&#20957;&#35270;&#25345;&#32493;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26377;&#24433;&#21709;&#21147;&#30340;&#35745;&#31639;&#27169;&#22411;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#39044;&#27979;&#20154;&#31867;&#22914;&#20309;&#29702;&#35299;&#21644;&#22788;&#29702;&#21477;&#23376;&#12290;&#26412;&#30740;&#31350;&#21463;Transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#31639;&#27861;&#21644;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20851;&#27880;&#35821;&#22659;&#8221;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#35821;&#22659;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#32771;&#34385;&#20102;&#35821;&#22659;&#37096;&#20998;&#30340;&#19981;&#21516;&#36129;&#29486;&#21644;&#26399;&#26395;&#25928;&#24212;&#65292;&#20351;&#20854;&#33021;&#22815;&#20805;&#20998;&#25972;&#21512;&#35821;&#22659;&#20449;&#24687;&#12290;&#20851;&#27880;&#35821;&#22659;&#26041;&#27861;&#36824;&#26377;&#21161;&#20110;&#27169;&#25311;&#29616;&#26377;&#30340;&#38405;&#35835;&#27169;&#22411;&#24182;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#8220;&#20851;&#27880;&#35821;&#22659;&#8221;&#35821;&#20041;&#30456;&#20851;&#24615;&#24230;&#37327;&#26631;&#20934;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#35760;&#24405;&#22312;&#30524;&#21160;&#36861;&#36394;&#35821;&#26009;&#24211;&#20013;&#30340;&#20013;&#25991;&#38405;&#35835;&#20219;&#21153;&#20013;&#20957;&#35270;&#25345;&#32493;&#26102;&#38388;&#12290;&#35813;&#30740;&#31350;&#30340;&#21457;&#29616;&#36827;&#19968;&#27493;&#20026;&#29616;&#26377;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18542v1 Announce Type: new  Abstract: In recent years, several influential computational models and metrics have been proposed to predict how humans comprehend and process sentence. One particularly promising approach is contextual semantic similarity. Inspired by the attention algorithm in Transformer and human memory mechanisms, this study proposes an ``attention-aware'' approach for computing contextual semantic relevance. This new approach takes into account the different contributions of contextual parts and the expectation effect, allowing it to incorporate contextual information fully. The attention-aware approach also facilitates the simulation of existing reading models and evaluate them. The resulting ``attention-aware'' metrics of semantic relevance can more accurately predict fixation durations in Chinese reading tasks recorded in an eye-tracking corpus than those calculated by existing approaches. The study's findings further provide strong support for the prese
&lt;/p&gt;</description></item><item><title>skscope&#26159;&#19968;&#20010;Python&#24211;&#65292;&#36890;&#36807;&#21482;&#38656;&#32534;&#20889;&#30446;&#26631;&#20989;&#25968;&#65292;&#23601;&#33021;&#24555;&#36895;&#23454;&#29616;&#31232;&#30095;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#19979;&#65292;&#20854;&#39640;&#25928;&#23454;&#29616;&#20351;&#24471;&#27714;&#35299;&#22120;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#31232;&#30095;&#35299;&#65292;&#36895;&#24230;&#27604;&#22522;&#20934;&#20984;&#27714;&#35299;&#22120;&#24555;80&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.18540</link><description>&lt;p&gt;
skscope&#65306;Python&#20013;&#30340;&#24555;&#36895;&#31232;&#30095;&#32422;&#26463;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
skscope: Fast Sparsity-Constrained Optimization in Python
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18540
&lt;/p&gt;
&lt;p&gt;
skscope&#26159;&#19968;&#20010;Python&#24211;&#65292;&#36890;&#36807;&#21482;&#38656;&#32534;&#20889;&#30446;&#26631;&#20989;&#25968;&#65292;&#23601;&#33021;&#24555;&#36895;&#23454;&#29616;&#31232;&#30095;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#19979;&#65292;&#20854;&#39640;&#25928;&#23454;&#29616;&#20351;&#24471;&#27714;&#35299;&#22120;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#31232;&#30095;&#35299;&#65292;&#36895;&#24230;&#27604;&#22522;&#20934;&#20984;&#27714;&#35299;&#22120;&#24555;80&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31232;&#30095;&#32422;&#26463;&#20248;&#21270;&#65288;SCO&#65289;&#19978;&#24212;&#29992;&#36845;&#20195;&#27714;&#35299;&#22120;&#38656;&#35201;&#32321;&#29712;&#30340;&#25968;&#23398;&#25512;&#23548;&#21644;&#20180;&#32454;&#30340;&#32534;&#31243;/&#35843;&#35797;&#65292;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#27714;&#35299;&#22120;&#30340;&#24191;&#27867;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24211;skscope&#65292;&#20197;&#20811;&#26381;&#27492;&#38556;&#30861;&#12290;&#20511;&#21161;skscope&#65292;&#29992;&#25143;&#21482;&#38656;&#32534;&#20889;&#30446;&#26631;&#20989;&#25968;&#21363;&#21487;&#35299;&#20915;SCO&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20004;&#20010;&#20363;&#23376;&#28436;&#31034;&#20102;skscope&#30340;&#26041;&#20415;&#20043;&#22788;&#65292;&#20854;&#20013;&#21482;&#38656;&#22235;&#34892;&#20195;&#30721;&#23601;&#21487;&#20197;&#35299;&#20915;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#21644;&#36235;&#21183;&#36807;&#28388;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;skscope&#30340;&#39640;&#25928;&#23454;&#29616;&#20351;&#24471;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;&#21487;&#20197;&#24555;&#36895;&#33719;&#24471;&#31232;&#30095;&#35299;&#65292;&#32780;&#26080;&#38656;&#32771;&#34385;&#21442;&#25968;&#31354;&#38388;&#30340;&#39640;&#32500;&#24230;&#12290;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#65292;skscope&#20013;&#30340;&#21487;&#29992;&#27714;&#35299;&#22120;&#21487;&#20197;&#23454;&#29616;&#27604;&#22522;&#20934;&#20984;&#27714;&#35299;&#22120;&#33719;&#24471;&#30340;&#31454;&#20105;&#26494;&#24347;&#35299;&#39640;&#36798;80&#20493;&#30340;&#21152;&#36895;&#24230;&#12290;skscope&#24050;&#32463;&#21457;&#24067;&#22312;Python&#36719;&#20214;&#21253;&#32034;&#24341;&#65288;PyPI&#65289;&#21644;Conda&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18540v1 Announce Type: cross  Abstract: Applying iterative solvers on sparsity-constrained optimization (SCO) requires tedious mathematical deduction and careful programming/debugging that hinders these solvers' broad impact. In the paper, the library skscope is introduced to overcome such an obstacle. With skscope, users can solve the SCO by just programming the objective function. The convenience of skscope is demonstrated through two examples in the paper, where sparse linear regression and trend filtering are addressed with just four lines of code. More importantly, skscope's efficient implementation allows state-of-the-art solvers to quickly attain the sparse solution regardless of the high dimensionality of parameter space. Numerical experiments reveal the available solvers in skscope can achieve up to 80x speedup on the competing relaxation solutions obtained via the benchmarked convex solver. skscope is published on the Python Package Index (PyPI) and Conda, and its 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#23433;&#20840;&#21644;&#31283;&#20581;RL&#39046;&#22495;&#30340;&#25506;&#32034;&#65292;&#35782;&#21035;&#24182;&#36827;&#19968;&#27493;&#29702;&#35299;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#37096;&#32626;RL&#31995;&#32479;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#31639;&#27861;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#36848;&#20197;&#22686;&#24378;RL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18539</link><description>&lt;p&gt;
&#23433;&#20840;&#31283;&#20581;&#30340;&#24378;&#21270;&#23398;&#20064;: &#21407;&#21017;&#19982;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Safe and Robust Reinforcement-Learning: Principles and Practice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#23433;&#20840;&#21644;&#31283;&#20581;RL&#39046;&#22495;&#30340;&#25506;&#32034;&#65292;&#35782;&#21035;&#24182;&#36827;&#19968;&#27493;&#29702;&#35299;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#37096;&#32626;RL&#31995;&#32479;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#31639;&#27861;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#36848;&#20197;&#22686;&#24378;RL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#35299;&#20915;&#30456;&#23545;&#22797;&#26434;&#30340;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#28982;&#32780;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#37096;&#32626;RL&#31995;&#32479;&#38754;&#20020;&#19982;&#23433;&#20840;&#21644;&#31283;&#20581;&#24615;&#26377;&#20851;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#23433;&#20840;&#21644;&#31283;&#20581;RL&#39046;&#22495;&#30340;&#20027;&#35201;&#32500;&#24230;&#65292;&#28085;&#30422;&#31639;&#27861;&#12289;&#20262;&#29702;&#21644;&#23454;&#38469;&#32771;&#34385;&#22240;&#32032;&#65292;&#35782;&#21035;&#24182;&#36827;&#19968;&#27493;&#29702;&#35299;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#36817;&#24180;&#26469;&#33268;&#21147;&#20110;&#35299;&#20915;&#19982;RL&#24212;&#29992;&#30456;&#20851;&#22266;&#26377;&#39118;&#38505;&#30340;&#26041;&#27861;&#21644;&#24320;&#25918;&#38382;&#39064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#12290;&#22312;&#35752;&#35770;&#24182;&#25552;&#20986;&#23433;&#20840;&#21644;&#31283;&#20581;RL&#30340;&#23450;&#20041;&#21518;&#65292;&#26412;&#25991;&#23558;&#29616;&#26377;&#30740;&#31350;&#24037;&#20316;&#24402;&#31867;&#20026;&#19981;&#21516;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;RL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#35832;&#22914;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12289;&#20248;&#21270;&#26041;&#27861;&#23398;&#12289;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26435;&#34913;&#20197;&#21450;&#23545;&#25239;&#24615;&#31561;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18539v1 Announce Type: new  Abstract: Reinforcement Learning (RL) has shown remarkable success in solving relatively complex tasks, yet the deployment of RL systems in real-world scenarios poses significant challenges related to safety and robustness. This paper aims to identify and further understand those challenges thorough the exploration of the main dimensions of the safe and robust RL landscape, encompassing algorithmic, ethical, and practical considerations. We conduct a comprehensive review of methodologies and open problems that summarizes the efforts in recent years to address the inherent risks associated with RL applications.   After discussing and proposing definitions for both safe and robust RL, the paper categorizes existing research works into different algorithmic approaches that enhance the safety and robustness of RL agents. We examine techniques such as uncertainty estimation, optimisation methodologies, exploration-exploitation trade-offs, and adversari
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#30028;&#38480;&#25351;&#23548;&#30340;&#23618;&#27425;&#21270;VAE&#65288;BG-VAE&#65289;&#65292;&#29992;&#20110;&#25913;&#21892;&#31070;&#32463;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#21487;&#21464;&#36895;&#29575;NIC&#12290;</title><link>https://arxiv.org/abs/2403.18535</link><description>&lt;p&gt;
&#23618;&#27425;&#21270;VAE&#25351;&#23548;&#30340;&#29702;&#35770;&#30028;&#38480;&#29992;&#20110;&#31070;&#32463;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18535
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#30028;&#38480;&#25351;&#23548;&#30340;&#23618;&#27425;&#21270;VAE&#65288;BG-VAE&#65289;&#65292;&#29992;&#20110;&#25913;&#21892;&#31070;&#32463;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#21487;&#21464;&#36895;&#29575;NIC&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#19982;&#29575;&#22833;&#30495;&#29702;&#35770;&#20043;&#38388;&#30340;&#37325;&#35201;&#29702;&#35770;&#32852;&#31995;&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;VAEs&#26469;&#20272;&#35745;&#22270;&#20687;&#20449;&#24687;&#29575;&#22833;&#30495;&#20989;&#25968;&#30340;&#29702;&#35770;&#19978;&#38480;&#12290;&#36825;&#20123;&#20272;&#35745;&#30340;&#29702;&#35770;&#30028;&#38480;&#22823;&#22823;&#36229;&#36807;&#20102;&#29616;&#26377;&#31070;&#32463;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#65288;NICs&#65289;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#30028;&#38480;&#24341;&#23548;&#30340;&#23618;&#27425;&#21270;VAE&#65288;BG-VAE&#65289;&#29992;&#20110;NIC&#12290;&#25152;&#25552;&#20986;&#30340;BG-VAE&#21033;&#29992;&#29702;&#35770;&#30028;&#38480;&#26469;&#25351;&#23548;NIC&#27169;&#22411;&#20197;&#25552;&#21319;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#23618;&#27425;&#21270;VAE&#23454;&#29616;&#20102;BG-VAE&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#38500;&#20102;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#24615;&#33021;&#20248;&#36234;&#30340;&#12289;&#21487;&#21464;&#36895;&#29575;&#30340;NIC&#65292;&#26080;&#35770;&#26159;&#22312;&#32771;&#34385;&#29575;&#22833;&#30495;&#24615;&#33021;&#36824;&#26159;&#35745;&#31639;&#22797;&#26434;&#24615;&#26102;&#37117;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;BG-VAE&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18535v1 Announce Type: cross  Abstract: Recent studies reveal a significant theoretical link between variational autoencoders (VAEs) and rate-distortion theory, notably in utilizing VAEs to estimate the theoretical upper bound of the information rate-distortion function of images. Such estimated theoretical bounds substantially exceed the performance of existing neural image codecs (NICs). To narrow this gap, we propose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC. The proposed BG-VAE leverages the theoretical bound to guide the NIC model towards enhanced performance. We implement the BG-VAE using Hierarchical VAEs and demonstrate its effectiveness through extensive experiments. Along with advanced neural network blocks, we provide a versatile, variable-rate NIC that outperforms existing methods when considering both rate-distortion performance and computational complexity. The code is available at BG-VAE.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#22823;&#22411;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#22312;&#23545;&#35937;-&#23646;&#24615;&#32452;&#21512;&#27867;&#21270;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#20026;&#27867;&#21270;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.18525</link><description>&lt;p&gt;
&#35821;&#35328;&#22312;CLIP&#23545;&#35937;-&#23646;&#24615;&#32452;&#21512;&#27867;&#21270;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Language Plays a Pivotal Role in the Object-Attribute Compositional Generalization of CLIP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#22823;&#22411;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#22312;&#23545;&#35937;-&#23646;&#24615;&#32452;&#21512;&#27867;&#21270;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#20026;&#27867;&#21270;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18525v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25688;&#35201;:&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;CLIP&#65292;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#19979;&#23637;&#29616;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#35843;&#26597;&#36825;&#31181;&#33021;&#21147;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27839;&#29992;&#20102;&#30456;&#21516;&#30340;&#24605;&#36335;&#65292;&#20294;&#19987;&#27880;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454; - &#20855;&#26377;&#26032;&#39062;&#23646;&#24615;-&#23545;&#35937;&#23545;&#32452;&#21512;&#30340;&#22270;&#20687; - &#24182;&#30740;&#31350;&#36825;&#26679;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25104;&#21151;&#22320;&#23558;&#36825;&#20123;&#22270;&#20687;&#20998;&#31867;&#21040;&#32452;&#21512;&#31867;&#21035;&#20013;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;ImageNet-AO&#30340;&#30495;&#23454;&#22270;&#20687;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;CLIP&#35757;&#32451;&#38598;&#20013;&#19981;&#22826;&#21487;&#33021;&#36935;&#21040;&#30340;&#23545;&#35937;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#22823;&#22411;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#65288;&#22914;OpenAI CLIP&#65292;LAION-400M&#21644;LAION-2B&#65289;&#22312;&#26377;&#25928;&#30340;&#32452;&#21512;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#27604;&#21463;&#30417;&#30563;&#27169;&#22411;&#21644;&#36890;&#36807;&#36739;&#23567;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#65288;&#22914;CC-12M&#21644;YFCC-15M&#65289;&#34920;&#29616;&#20986;&#25968;&#37327;&#32423;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#20851;&#20110;&#35268;&#27169;&#12289;&#25968;&#25454;&#38598;&#21644;&#27867;&#21270;&#20043;&#38388;&#20851;&#31995;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18525v1 Announce Type: cross  Abstract: Vision-language models, such as CLIP, have shown promising Out-of-Distribution (OoD) generalization under various types of distribution shifts. Recent studies attempted to investigate the leading cause of this capability. In this work, we follow the same path, but focus on a specific type of OoD data - images with novel compositions of attribute-object pairs - and study whether such models can successfully classify those images into composition classes. We carefully designed an authentic image test dataset called ImageNet-AO, consisting of attributes for objects that are unlikely encountered in the CLIP training sets. We found that CLIPs trained with large datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude improvement in effective compositional OoD generalization compared to both supervised models and CLIPs trained with smaller datasets, such as CC-12M and YFCC-15M. Our results provide evidence that the sca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32447;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;ADAM&#30340;&#21160;&#37327;&#39033;&#38598;&#25104;&#21040;Armijo&#32447;&#25628;&#32034;&#20013;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#24182;&#19988;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#21644;Adam&#30340;&#35843;&#25972;&#23398;&#20064;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.18519</link><description>&lt;p&gt;
&#25913;&#36827;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32447;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Line Search Methods for Large Scale Neural Network Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32447;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;ADAM&#30340;&#21160;&#37327;&#39033;&#38598;&#25104;&#21040;Armijo&#32447;&#25628;&#32034;&#20013;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#24182;&#19988;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#21644;Adam&#30340;&#35843;&#25972;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#32447;&#25628;&#32034;&#26041;&#27861;&#22312;&#20256;&#32479;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#30340;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28040;&#38500;&#20102;&#38656;&#35201;&#29305;&#23450;&#23398;&#20064;&#29575;&#35843;&#24230;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#35782;&#21035;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#32447;&#25628;&#32034;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22686;&#24378;&#25514;&#26045;&#65292;&#24182;&#23545;&#20854;&#25928;&#26524;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;&#27604;&#20197;&#24448;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#26356;&#22797;&#26434;&#30340;&#25968;&#25454;&#39046;&#22495;&#19978;&#27979;&#35797;&#20102;&#36825;&#20123;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;ADAM&#30340;&#21160;&#37327;&#39033;&#38598;&#25104;&#21040;Armijo&#32447;&#25628;&#32034;&#20013;&#30340;&#25628;&#32034;&#26041;&#21521;&#20013;&#65292;&#25913;&#36827;&#20102;Armijo&#32447;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#36825;&#26159;&#20197;&#21069;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#26041;&#27861;&#23481;&#26131;&#22833;&#36133;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20248;&#21270;&#26041;&#27861;&#32988;&#36807;&#20197;&#21069;&#30340;Armijo&#23454;&#29616;&#21644;Adam&#30340;&#35843;&#25972;&#23398;&#20064;&#29575;&#35843;&#24230;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#37325;&#28857;&#25918;&#22312;NLP&#21644;&#22270;&#20687;&#25968;&#25454;&#39046;&#22495;&#30340;Transformer&#21644;CNN&#19978;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20197;Python&#21253;&#30340;&#24418;&#24335;&#20844;&#24320;&#21457;&#24067;&#65292;&#21487;&#20197;&#19979;&#36733;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18519v1 Announce Type: cross  Abstract: In recent studies, line search methods have shown significant improvements in the performance of traditional stochastic gradient descent techniques, eliminating the need for a specific learning rate schedule. In this paper, we identify existing issues in state-of-the-art line search methods, propose enhancements, and rigorously evaluate their effectiveness. We test these methods on larger datasets and more complex data domains than before. Specifically, we improve the Armijo line search by integrating the momentum term from ADAM in its search direction, enabling efficient large-scale training, a task that was previously prone to failure using Armijo line search methods. Our optimization approach outperforms both the previous Armijo implementation and tuned learning rate schedules for Adam. Our evaluation focuses on Transformers and CNNs in the domains of NLP and image data. Our work is publicly available as a Python package, which prov
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#31216;&#20026;&#22343;&#21248;&#27491;&#21017;&#21270;&#23610;&#24230;&#19981;&#21464;&#30340;&#26356;&#19968;&#33324;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#20013;&#23610;&#24230;&#19981;&#21464;&#24615;&#23548;&#33268;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#25928;&#26524;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#29702;&#35299;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#20316;&#29992;&#24182;&#25351;&#23548;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2403.18517</link><description>&lt;p&gt;
&#38024;&#23545;&#27491;&#21017;&#21270;&#38750;&#36127;&#23610;&#24230;&#19981;&#21464;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#30340;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Algorithms for Regularized Nonnegative Scale-invariant Low-rank Approximation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18517
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#31216;&#20026;&#22343;&#21248;&#27491;&#21017;&#21270;&#23610;&#24230;&#19981;&#21464;&#30340;&#26356;&#19968;&#33324;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#20013;&#23610;&#24230;&#19981;&#21464;&#24615;&#23548;&#33268;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#25928;&#26524;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#29702;&#35299;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#20316;&#29992;&#24182;&#25351;&#23548;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#38750;&#36127;&#20302;&#31209;&#36924;&#36817;&#65292;&#22914;&#31232;&#30095;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#25110;&#31232;&#30095;&#30340;&#38750;&#36127;Tucker&#20998;&#35299;&#65292;&#26159;&#20855;&#26377;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#30340;&#38477;&#32500;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20998;&#25903;&#12290;&#28982;&#32780;&#65292;&#20174;&#23454;&#36341;&#35282;&#24230;&#26469;&#30475;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#22240;&#32032;&#29305;&#24615;&#20197;&#21450;&#32570;&#20047;&#25903;&#25345;&#36825;&#20123;&#36873;&#25321;&#30340;&#29702;&#35770;&#65292;&#27491;&#21017;&#21270;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#31995;&#25968;&#30340;&#36873;&#25321;&#65292;&#20197;&#21450;&#39640;&#25928;&#31639;&#27861;&#30340;&#35774;&#35745;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#19968;&#20010;&#31216;&#20026;&#22343;&#21248;&#27491;&#21017;&#21270;&#23610;&#24230;&#19981;&#21464;&#30340;&#26356;&#19968;&#33324;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#23610;&#24230;&#19981;&#21464;&#24615;&#23548;&#33268;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#20855;&#26377;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#30410;&#21644;&#26377;&#23475;&#25928;&#26524;&#12290;&#36825;&#19968;&#21457;&#29616;&#20351;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#20013;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#20316;&#29992;&#65292;&#25351;&#23548;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18517v1 Announce Type: new  Abstract: Regularized nonnegative low-rank approximations such as sparse Nonnegative Matrix Factorization or sparse Nonnegative Tucker Decomposition are an important branch of dimensionality reduction models with enhanced interpretability. However, from a practical perspective, the choice of regularizers and regularization coefficients, as well as the design of efficient algorithms, is challenging because of the multifactor nature of these models and the lack of theory to back these choices. This paper aims at improving upon these issues. By studying a more general model called the Homogeneous Regularized Scale-Invariant, we prove that the scale-invariance inherent to low-rank approximation models causes an implicit regularization with both unexpected beneficial and detrimental effects. This observation allows to better understand the effect of regularization functions in low-rank approximation models, to guide the choice of the regularization hyp
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;3D&#27491;&#35268;&#21270;&#27969;&#25216;&#26415;&#24320;&#21457;&#20986;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;CT-3DFlow&#65292;&#36890;&#36807;&#22312;&#20581;&#24247;&#25968;&#25454;&#19978;&#36827;&#34892;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#22312;&#24739;&#32773;&#32423;&#21035;&#30340;&#33016;&#37096;CT&#25968;&#25454;&#20013;&#23454;&#29616;&#30149;&#29702;&#24615;&#32954;&#37096;&#30149;&#21464;&#30340;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.18514</link><description>&lt;p&gt;
&#21033;&#29992;3D&#27491;&#35268;&#21270;&#27969;&#36827;&#34892;&#26080;&#30417;&#30563;&#26816;&#27979;&#30149;&#29702;&#24615;&#32954;&#37096;CT&#25195;&#25551;
&lt;/p&gt;
&lt;p&gt;
CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection of Pathological Pulmonary CT scans
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18514
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;3D&#27491;&#35268;&#21270;&#27969;&#25216;&#26415;&#24320;&#21457;&#20986;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;CT-3DFlow&#65292;&#36890;&#36807;&#22312;&#20581;&#24247;&#25968;&#25454;&#19978;&#36827;&#34892;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#22312;&#24739;&#32773;&#32423;&#21035;&#30340;&#33016;&#37096;CT&#25968;&#25454;&#20013;&#23454;&#29616;&#30149;&#29702;&#24615;&#32954;&#37096;&#30149;&#21464;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#30149;&#29702;&#24615;&#26816;&#27979;&#21487;&#20197;&#36890;&#36807;&#20165;&#22312;&#20581;&#24247;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#24182;&#22312;&#25512;&#26029;&#26102;&#27979;&#37327;&#19982;&#35757;&#32451;&#38598;&#30340;&#20559;&#24046;&#26469;&#23454;&#29616;&#65292;&#20363;&#22914;&#22522;&#20110;CNN&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#21333;&#31867;&#20998;&#31867;&#22120;&#65292;&#25110;&#22522;&#20110;&#37325;&#26500;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#22914;&#33258;&#32534;&#30721;&#22120;&#65292;GAN&#21644;&#25193;&#25955;&#27169;&#22411;&#12290; &#27491;&#35268;&#21270;&#27969;&#65288;NF&#65289;&#20855;&#26377;&#30452;&#25509;&#36890;&#36807;&#21487;&#36870;&#26550;&#26500;&#23398;&#20064;&#35757;&#32451;&#26679;&#26412;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#33021;&#21147;&#12290; &#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#24615;&#36136;&#22312;&#19968;&#31181;&#21517;&#20026;CT-3DFlow&#30340;&#26032;&#22411;3D NF&#27169;&#22411;&#20013;&#65292;&#19987;&#38376;&#20026;&#33016;&#37096;CT&#25968;&#25454;&#20013;&#30340;&#24739;&#32773;&#32423;&#32954;&#37096;&#30149;&#29702;&#26816;&#27979;&#36827;&#34892;&#23450;&#21046;&#12290; &#25105;&#20204;&#22312;&#20581;&#24247;&#30340;3D&#32954;&#37096;CT&#22359;&#19978;&#36827;&#34892;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#26816;&#27979;&#20854;&#23545;&#25968;&#20284;&#28982;&#20998;&#24067;&#30340;&#20559;&#24046;&#20316;&#20026;&#24322;&#24120;&#12290; &#25105;&#20204;&#20174;&#24739;&#32773;&#30340;CT&#25195;&#25551;&#20013;&#27719;&#24635;&#22359;&#32423;&#20284;&#28982;&#20540;&#65292;&#20197;&#25552;&#20379;&#24739;&#32773;&#32423;&#30340;'&#27491;&#24120;'/'&#24322;&#24120;'&#39044;&#27979;&#12290; &#20351;&#29992;&#22806;&#20998;&#24067;&#26816;&#27979;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18514v1 Announce Type: cross  Abstract: Unsupervised pathology detection can be implemented by training a model on healthy data only and measuring the deviation from the training set upon inference, for example with CNN-based feature extraction and one-class classifiers, or reconstruction-score-based methods such as AEs, GANs and Diffusion models. Normalizing Flows (NF) have the ability to directly learn the probability distribution of training examples through an invertible architecture. We leverage this property in a novel 3D NF-based model named CT-3DFlow, specifically tailored for patient-level pulmonary pathology detection in chest CT data. Our model is trained unsupervised on healthy 3D pulmonary CT patches, and detects deviations from its log-likelihood distribution as anomalies. We aggregate patches-level likelihood values from a patient's CT scan to provide a patient-level 'normal'/'abnormal' prediction. Out-of-distribution detection performance is evaluated using e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#40065;&#26834;&#20998;&#24067;&#24335;&#26368;&#22823;&#19968;&#33268;&#24615;&#31639;&#27861;&#65288;RD-MC&#65289;&#65292;&#36890;&#36807;&#23558;&#26368;&#22823;&#19968;&#33268;&#24615;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#22120;&#26041;&#27861;&#27714;&#35299;&#65292;&#20165;&#20351;&#29992;&#21333;&#32452;&#20272;&#35745;&#20174;&#32780;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#26356;&#21152;&#25239;&#22122;&#22768;&#12290;</title><link>https://arxiv.org/abs/2403.18509</link><description>&lt;p&gt;
&#22312;&#22122;&#22768;&#38142;&#36335;&#19978;&#30340;&#20998;&#24067;&#24335;&#26368;&#22823;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Distributed Maximum Consensus over Noisy Links
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18509
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#40065;&#26834;&#20998;&#24067;&#24335;&#26368;&#22823;&#19968;&#33268;&#24615;&#31639;&#27861;&#65288;RD-MC&#65289;&#65292;&#36890;&#36807;&#23558;&#26368;&#22823;&#19968;&#33268;&#24615;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#22120;&#26041;&#27861;&#27714;&#35299;&#65292;&#20165;&#20351;&#29992;&#21333;&#32452;&#20272;&#35745;&#20174;&#32780;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#26356;&#21152;&#25239;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22122;&#22768;&#40065;&#26834;&#20998;&#24067;&#24335;&#26368;&#22823;&#19968;&#33268;&#24615;&#65288;RD-MC&#65289;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#22024;&#26434;&#36890;&#20449;&#38142;&#36335;&#30340;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#20272;&#35745;&#26368;&#22823;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37325;&#26032;&#23450;&#20041;&#20102;&#26368;&#22823;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23558;&#20854;&#20316;&#20026;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#27714;&#35299;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#22120;&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#19982;&#20381;&#36182;&#22810;&#32452;&#22122;&#22768;&#27745;&#26579;&#20272;&#35745;&#30340;&#29616;&#26377;&#31639;&#27861;&#19981;&#21516;&#65292;RD-MC&#37319;&#29992;&#21333;&#32452;&#20272;&#35745;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20943;&#36731;&#38142;&#36335;&#22122;&#22768;&#30340;&#24433;&#21709;&#24182;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#23558;&#31227;&#21160;&#24179;&#22343;&#24212;&#29992;&#20110;&#26412;&#22320;&#20272;&#35745;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#20223;&#30495;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RD-MC&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#22823;&#19968;&#33268;&#24615;&#31639;&#27861;&#22312;&#36890;&#20449;&#38142;&#36335;&#22122;&#22768;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18509v1 Announce Type: cross  Abstract: We introduce a distributed algorithm, termed noise-robust distributed maximum consensus (RD-MC), for estimating the maximum value within a multi-agent network in the presence of noisy communication links. Our approach entails redefining the maximum consensus problem as a distributed optimization problem, allowing a solution using the alternating direction method of multipliers. Unlike existing algorithms that rely on multiple sets of noise-corrupted estimates, RD-MC employs a single set, enhancing both robustness and efficiency. To further mitigate the effects of link noise and improve robustness, we apply moving averaging to the local estimates. Through extensive simulations, we demonstrate that RD-MC is significantly more robust to communication link noise compared to existing maximum-consensus algorithms.
&lt;/p&gt;</description></item><item><title>&#23558;Armijo&#32447;&#25628;&#32034;&#19982;Adam&#20248;&#21270;&#22120;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#22312;&#26412;&#22320;&#21333;&#20803;&#25191;&#34892;&#32447;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#22312;Transformer&#24494;&#35843;&#20013;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18506</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#25628;&#32034;&#26041;&#27861;&#21152;&#36895;Transformer&#24494;&#35843;&#30340;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Faster Convergence for Transformer Fine-tuning with Line Search Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18506
&lt;/p&gt;
&lt;p&gt;
&#23558;Armijo&#32447;&#25628;&#32034;&#19982;Adam&#20248;&#21270;&#22120;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#22312;&#26412;&#22320;&#21333;&#20803;&#25191;&#34892;&#32447;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#22312;Transformer&#24494;&#35843;&#20013;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32447;&#25628;&#32034;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#20256;&#32479;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#32447;&#25628;&#32034;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#26032;&#39062;&#19988;&#22791;&#21463;&#27426;&#36814;&#30340;Transformer&#26550;&#26500;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;Armijo&#32447;&#25628;&#32034;&#19982;Adam&#20248;&#21270;&#22120;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#23558;&#32593;&#32476;&#26550;&#26500;&#32454;&#20998;&#20026;&#21512;&#29702;&#30340;&#21333;&#20803;&#65292;&#22312;&#36825;&#20123;&#26412;&#22320;&#21333;&#20803;&#19978;&#20998;&#21035;&#25191;&#34892;&#32447;&#25628;&#32034;&#12290;&#25105;&#20204;&#30340;&#20248;&#21270;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;Adam&#20248;&#21270;&#22120;&#65292;&#22312;&#23567;&#25968;&#25454;&#38598;&#25110;&#23567;&#35757;&#32451;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#27979;&#35797;&#26696;&#20363;&#20013;&#34920;&#29616;&#30456;&#31561;&#25110;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20316;&#20026;&#19968;&#20010;Python&#21253;&#20844;&#24320;&#21487;&#29992;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;PyTorch&#20248;&#21270;&#22120;&#65292;&#19982;&#20219;&#24847;&#32593;&#32476;&#26550;&#26500;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18506v1 Announce Type: cross  Abstract: Recent works have shown that line search methods greatly increase performance of traditional stochastic gradient descent methods on a variety of datasets and architectures [1], [2]. In this work we succeed in extending line search methods to the novel and highly popular Transformer architecture and dataset domains in natural language processing. More specifically, we combine the Armijo line search with the Adam optimizer and extend it by subdividing the networks architecture into sensible units and perform the line search separately on these local units. Our optimization method outperforms the traditional Adam optimizer and achieves significant performance improvements for small data sets or small training budgets, while performing equal or better for other tested cases. Our work is publicly available as a python package, which provides a hyperparameter-free pytorch optimizer that is compatible with arbitrary network architectures.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23581;&#35797;&#30452;&#25509;&#20174;&#38075;&#23380;&#22270;&#20687;&#20013;&#35780;&#20272;&#23721;&#30707;&#30340;&#23721;&#24615;&#21644;&#30719;&#29289;&#21547;&#37327;&#65292;&#20197;&#25903;&#25345;&#21644;&#21152;&#36895;&#22320;&#19979;&#22320;&#36136;&#21208;&#25506;&#12290;</title><link>https://arxiv.org/abs/2403.18495</link><description>&lt;p&gt;
&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#30452;&#25509;&#39044;&#27979;&#38075;&#23380;&#22270;&#20687;&#20013;&#30340;&#30719;&#29289;&#21547;&#37327;
&lt;/p&gt;
&lt;p&gt;
Direct mineral content prediction from drill core images via transfer learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18495
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23581;&#35797;&#30452;&#25509;&#20174;&#38075;&#23380;&#22270;&#20687;&#20013;&#35780;&#20272;&#23721;&#30707;&#30340;&#23721;&#24615;&#21644;&#30719;&#29289;&#21547;&#37327;&#65292;&#20197;&#25903;&#25345;&#21644;&#21152;&#36895;&#22320;&#19979;&#22320;&#36136;&#21208;&#25506;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#37096;&#22320;&#19979;&#21208;&#25506;&#23545;&#20110;&#30719;&#19994;&#12289;&#30707;&#27833;&#21644;&#22825;&#28982;&#27668;&#24037;&#19994;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#20063;&#22312;&#21270;&#23398;&#25110;&#26680;&#24223;&#29289;&#22788;&#32622;&#12289;&#20197;&#21450;&#22320;&#28909;&#33021;&#31995;&#32479;&#21487;&#34892;&#24615;&#35780;&#20272;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#65292;&#20165;&#36890;&#36807;&#38075;&#23380;&#22270;&#20687;&#30340;&#20998;&#26512;&#26469;&#35780;&#20272;&#23721;&#30707;&#30340;&#23721;&#24615;&#21644;&#30719;&#29289;&#21547;&#37327;&#30340;&#28508;&#21147;&#65292;&#26088;&#22312;&#25903;&#25345;&#21644;&#21152;&#36895;&#22320;&#19979;&#22320;&#36136;&#21208;&#25506;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18495v1 Announce Type: cross  Abstract: Deep subsurface exploration is important for mining, oil and gas industries, as well as in the assessment of geological units for the disposal of chemical or nuclear waste, or the viability of geothermal energy systems. Typically, detailed examinations of subsurface formations or units are performed on cuttings or core materials extracted during drilling campaigns, as well as on geophysical borehole data, which provide detailed information about the petrophysical properties of the rocks. Depending on the volume of rock samples and the analytical program, the laboratory analysis and diagnostics can be very time-consuming. This study investigates the potential of utilizing machine learning, specifically convolutional neural networks (CNN), to assess the lithology and mineral content solely from analysis of drill core images, aiming to support and expedite the subsurface geological exploration. The paper outlines a comprehensive methodolo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#26799;&#24230;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#24182;&#22312;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#20013;&#21457;&#29616;&#20102;&#31532;&#19977;&#38454;&#27573;"&#24635;&#25193;&#25955;"&#65292;&#20854;&#29305;&#24449;&#26159;&#22343;&#21248;&#30340;&#23398;&#20064;&#36895;&#29575;&#21644;&#26799;&#24230;&#65292;&#36825;&#19968;&#38454;&#27573;&#26631;&#24535;&#30528;&#24555;&#36895;&#30340;&#35757;&#32451;&#25910;&#25947;&#21644;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18494</link><description>&lt;p&gt;
&#22312;PINNs&#20013;&#23398;&#20064;&#65306;&#30456;&#21464;&#12289;&#24635;&#25193;&#25955;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning in PINNs: Phase transition, total diffusion, and generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18494
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#26799;&#24230;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#24182;&#22312;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#20013;&#21457;&#29616;&#20102;&#31532;&#19977;&#38454;&#27573;"&#24635;&#25193;&#25955;"&#65292;&#20854;&#29305;&#24449;&#26159;&#22343;&#21248;&#30340;&#23398;&#20064;&#36895;&#29575;&#21644;&#26799;&#24230;&#65292;&#36825;&#19968;&#38454;&#27573;&#26631;&#24535;&#30528;&#24555;&#36895;&#30340;&#35757;&#32451;&#25910;&#25947;&#21644;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26799;&#24230;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#30340;&#35270;&#35282;&#30740;&#31350;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#25506;&#35752;&#20102;Adam&#31561;&#19968;&#38454;&#20248;&#21270;&#22120;&#22312;&#38750;&#20984;&#30446;&#26631;&#20013;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#22312;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#20013;&#35299;&#37322;&#28418;&#31227;/&#25193;&#25955;&#30456;&#65292;&#32858;&#28966;&#26799;&#24230;&#22343;&#19968;&#24615;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#8220;&#24635;&#25193;&#25955;&#8221;&#30340;&#31532;&#19977;&#38454;&#27573;&#65292;&#20854;&#29305;&#24449;&#26159;&#23398;&#20064;&#36895;&#29575;&#21644;&#26799;&#24230;&#22343;&#21248;&#65292;&#36825;&#19968;&#38454;&#27573;&#26631;&#24535;&#30528;SNR&#24613;&#21095;&#22686;&#21152;&#65292;&#26679;&#26412;&#31354;&#38388;&#20013;&#27531;&#24046;&#22343;&#21248;&#19988;&#35757;&#32451;&#25910;&#25947;&#26368;&#24555;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#26469;&#21152;&#36895;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#25193;&#25955;&#65292;&#20174;&#32780;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20449;&#24687;&#21387;&#32553;&#29616;&#35937;&#65292;&#30830;&#23450;&#22312;&#24635;&#25193;&#25955;&#38454;&#27573;&#28608;&#27963;&#21457;&#29983;&#26174;&#33879;&#39281;&#21644;&#35825;&#23548;&#30340;&#21387;&#32553;&#65292;&#26356;&#28145;&#30340;&#23618;&#27425;&#32463;&#21382;&#21487;&#20197;&#24573;&#30053;&#30340;&#20449;&#24687;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18494v1 Announce Type: new  Abstract: We investigate the learning dynamics of fully-connected neural networks through the lens of gradient signal-to-noise ratio (SNR), examining the behavior of first-order optimizers like Adam in non-convex objectives. By interpreting the drift/diffusion phases in the information bottleneck theory, focusing on gradient homogeneity, we identify a third phase termed ``total diffusion", characterized by equilibrium in the learning rates and homogeneous gradients. This phase is marked by an abrupt SNR increase, uniform residuals across the sample space and the most rapid training convergence. We propose a residual-based re-weighting scheme to accelerate this diffusion in quadratic loss functions, enhancing generalization. We also explore the information compression phenomenon, pinpointing a significant saturation-induced compression of activations at the total diffusion phase, with deeper layers experiencing negligible information loss. Supporte
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22825;&#27668;&#39044;&#25253;&#25968;&#25454;&#20316;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20272;&#35745;&#33976;&#25955;&#21457;&#30340;&#36755;&#20837;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;FAO56-PM&#26041;&#27861;&#35745;&#31639;ET0&#26102;&#22826;&#38451;&#36752;&#23556;&#21442;&#25968;&#19981;&#26131;&#33719;&#21462;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.18489</link><description>&lt;p&gt;
&#21033;&#29992;&#22825;&#27668;&#39044;&#25253;&#25968;&#25454;&#20316;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20272;&#35745;&#33976;&#25955;&#21457;&#30340;&#36755;&#20837;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Employing Weather Forecast Data as Input to the Estimation of Evapotranspiration by Deep Neural Network Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18489
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22825;&#27668;&#39044;&#25253;&#25968;&#25454;&#20316;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20272;&#35745;&#33976;&#25955;&#21457;&#30340;&#36755;&#20837;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;FAO56-PM&#26041;&#27861;&#35745;&#31639;ET0&#26102;&#22826;&#38451;&#36752;&#23556;&#21442;&#25968;&#19981;&#26131;&#33719;&#21462;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#32771;&#33976;&#25955;&#21457;&#65288;ET0&#65289;&#26159;&#35774;&#35745;&#26234;&#33021;&#28748;&#28297;&#35843;&#24230;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#22240;&#20026;&#23427;&#36890;&#36807;&#31995;&#25968;&#19982;&#20316;&#29289;&#30340;&#27700;&#38656;&#27714;&#30456;&#20851;&#12290;&#32852;&#21512;&#22269;&#31918;&#39135;&#21644;&#20892;&#19994;&#32452;&#32455;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#30340;ET0&#35745;&#31639;&#26041;&#27861;&#65288;FAO56PM&#65289;&#65292;&#22522;&#20110;Penman-Monteith&#26041;&#31243;&#30340;&#21442;&#25968;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#20351;&#29992;FAO56-PM&#26041;&#27861;&#35745;&#31639;ET0&#38656;&#35201;&#22235;&#20010;&#20027;&#35201;&#30340;&#22825;&#27668;&#21442;&#25968;&#65306;&#28201;&#24230;&#12289;&#28287;&#24230;&#12289;&#39118;&#36895;&#21644;&#22826;&#38451;&#36752;&#23556;&#65288;SR&#65289;&#12290;&#19968;&#31181;&#39044;&#27979;&#26410;&#26469;&#20960;&#22825;&#30340;&#27599;&#26085;ET0&#20540;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#20813;&#36153;&#25552;&#20379;&#30340;&#22825;&#27668;&#39044;&#25253;&#26381;&#21153;&#65288;WFSs&#65289;&#65292;&#36825;&#20123;&#26381;&#21153;&#21487;&#20272;&#35745;&#22810;&#31181;&#27668;&#35937;&#21442;&#25968;&#38271;&#36798;&#26410;&#26469;15&#22825;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#38382;&#39064;&#22312;&#20110;&#24403;&#21069;&#22823;&#22810;&#25968;&#22312;&#32447;&#26381;&#21153;&#27809;&#26377;&#25552;&#20379;SR&#20316;&#20026;&#20813;&#36153;&#30340;&#39044;&#27979;&#21442;&#25968;&#65292;&#36890;&#24120;&#36825;&#26679;&#30340;&#39044;&#27979;&#38656;&#35201;&#25903;&#20184;&#37329;&#38065;&#12290;&#22240;&#27492;&#65292;&#20986;&#29616;&#20102;&#20960;&#31181;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18489v1 Announce Type: new  Abstract: Reference Evapotranspiration (ET0) is a key parameter for designing smart irrigation scheduling, since it is related by a coefficient to the water needs of a crop. The United Nations Food and Agriculture Organization, proposed a standard method for ET0 computation (FAO56PM), based on the parameterization of the Penman-Monteith equation, that is widely adopted in the literature. To compute ET0 using the FAO56-PM method, four main weather parameters are needed: temperature, humidity, wind, and solar radiation (SR). One way to make daily ET0 estimations for future days is to use freely available weather forecast services (WFSs), where many meteorological parameters are estimated up to the next 15 days. A problem with this method is that currently, SR is not provided as a free forecast parameter on most of those online services or, normally, such forecasts present a financial cost penalty. For this reason, several ET0 estimation models using
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20351;&#29992;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#29305;&#23450;&#20027;&#20307;&#12289;&#20250;&#35805;&#21644;&#31867;&#21035;&#30340;EEG&#25968;&#25454;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#20284;&#12290;</title><link>https://arxiv.org/abs/2403.18486</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#20174;&#20107;&#20214;&#30456;&#20851;&#30005;&#20301;&#33539;&#24335;&#20013;&#21512;&#25104;&#33041;&#30005;&#22270;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Synthesizing EEG Signals from Event-Related Potential Paradigms with Conditional Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18486
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20351;&#29992;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#29305;&#23450;&#20027;&#20307;&#12289;&#20250;&#35805;&#21644;&#31867;&#21035;&#30340;EEG&#25968;&#25454;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#39046;&#22495;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#24471;&#20197;&#32531;&#35299;&#12290;&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#20808;&#21069;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#22312;&#37319;&#26679;&#28789;&#27963;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#25110;&#38656;&#35201;EEG&#25968;&#25454;&#30340;&#26367;&#20195;&#34920;&#31034;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#30452;&#25509;&#29983;&#25104;&#29305;&#23450;&#20027;&#20307;&#12289;&#20250;&#35805;&#21644;&#31867;&#21035;&#30340;EEG&#25968;&#25454;&#12290;&#38500;&#20102;&#24120;&#29992;&#30340;&#25351;&#26631;&#22806;&#65292;&#36824;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#29983;&#25104;&#26679;&#26412;&#30340;&#29305;&#23450;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19982;&#27599;&#20010;&#20027;&#20307;&#12289;&#20250;&#35805;&#21644;&#31867;&#21035;&#30340;&#30495;&#23454;&#25968;&#25454;&#30456;&#20284;&#30340;EEG&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18486v1 Announce Type: cross  Abstract: Data scarcity in the brain-computer interface field can be alleviated through the use of generative models, specifically diffusion models. While diffusion models have previously been successfully applied to electroencephalogram (EEG) data, existing models lack flexibility w.r.t.~sampling or require alternative representations of the EEG data. To overcome these limitations, we introduce a novel approach to conditional diffusion models that utilizes classifier-free guidance to directly generate subject-, session-, and class-specific EEG data. In addition to commonly used metrics, domain-specific metrics are employed to evaluate the specificity of the generated samples. The results indicate that the proposed model can generate EEG data that resembles real data for each subject, session, and class.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SingularTrajectory&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#36890;&#29992;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#32479;&#19968;&#19981;&#21516;&#20219;&#21153;&#30340;&#20154;&#20307;&#36816;&#21160;&#21160;&#24577;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.18452</link><description>&lt;p&gt;
SingularTrajectory: &#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#36890;&#29992;&#36712;&#36857;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SingularTrajectory&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#36890;&#29992;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#32479;&#19968;&#19981;&#21516;&#20219;&#21153;&#30340;&#20154;&#20307;&#36816;&#21160;&#21160;&#24577;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SingularTrajectory&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#36890;&#29992;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#20943;&#23569;&#20116;&#31181;&#20219;&#21153;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#35813;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#32479;&#19968;&#21508;&#31181;&#20154;&#31867;&#21160;&#21147;&#23398;&#34920;&#31034;&#22312;&#30456;&#20851;&#20219;&#21153;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18452v1 Announce Type: cross  Abstract: There are five types of trajectory prediction tasks: deterministic, stochastic, domain adaptation, momentary observation, and few-shot. These associated tasks are defined by various factors, such as the length of input paths, data split and pre-processing methods. Interestingly, even though they commonly take sequential coordinates of observations as input and infer future paths in the same coordinates as output, designing specialized architectures for each task is still necessary. For the other task, generality issues can lead to sub-optimal performances. In this paper, we propose SingularTrajectory, a diffusion-based universal trajectory prediction framework to reduce the performance gap across the five tasks. The core of SingularTrajectory is to unify a variety of human dynamics representations on the associated tasks. To do this, we first build a Singular space to project all types of motion patterns from each task into one embeddi
&lt;/p&gt;</description></item><item><title>CoRAST&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;(FMs)&#22686;&#24378;&#20102;&#20998;&#24067;&#24335;&#12289;&#30456;&#20851;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.18451</link><description>&lt;p&gt;
CoRAST&#65306;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;CPS&#21644;IoT&#20013;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#30456;&#20851;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in Resource-Constrained CPS and IoT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18451
&lt;/p&gt;
&lt;p&gt;
CoRAST&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;(FMs)&#22686;&#24378;&#20102;&#20998;&#24067;&#24335;&#12289;&#30456;&#20851;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;(FMs)&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#29702;&#35299;&#24322;&#26500;&#25968;&#25454;&#38598;&#20013;&#22797;&#26434;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#21033;&#29992;&#20998;&#24067;&#24335;&#21644;&#22810;&#26679;&#21270;&#30340;&#29615;&#22659;&#25968;&#25454;&#12290;&#19982;&#32852;&#21512;&#23398;&#20064;&#31561;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#19981;&#21516;&#65292;&#23427;&#20204;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;FMs&#21487;&#20197;&#23558;&#19981;&#21516;&#30340;&#36755;&#20837;&#36716;&#25442;&#20026;&#23884;&#20837;&#12290;&#36825;&#19968;&#36807;&#31243;&#26377;&#21161;&#20110;&#25972;&#21512;&#21508;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#24182;&#23558;&#20808;&#21069;&#30340;&#23398;&#20064;&#24212;&#29992;&#20110;&#26032;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#31995;&#32479;&#20013;&#37096;&#32626;FMs&#20250;&#24102;&#26469;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoRAST&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;FMs&#22686;&#24378;&#20102;&#20998;&#24067;&#24335;&#12289;&#30456;&#20851;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#26512;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#26381;&#21153;&#22120;&#30340;FM&#65292;CoRAST&#21487;&#20197;&#21033;&#29992;&#29616;&#26377;&#30340;&#29615;&#22659;&#20449;&#24687;&#26469;&#25552;&#21462;&#20256;&#24863;&#22120;&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#38388;&#12289;&#31354;&#38388;&#21644;&#36328;&#27169;&#24577;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18451v1 Announce Type: cross  Abstract: Foundation models (FMs) emerge as a promising solution to harness distributed and diverse environmental data by leveraging prior knowledge to understand the complicated temporal and spatial correlations within heterogeneous datasets. Unlike distributed learning frameworks such as federated learning, which often struggle with multimodal data, FMs can transform diverse inputs into embeddings. This process facilitates the integration of information from various modalities and the application of prior learning to new domains. However, deploying FMs in resource-constrained edge systems poses significant challenges. To this end, we introduce CoRAST, a novel learning framework that utilizes FMs for enhanced analysis of distributed, correlated heterogeneous data. Utilizing a server-based FM, CoRAST can exploit existing environment information to extract temporal, spatial, and cross-modal correlations among sensor data. This enables CoRAST to o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LMTraj&#65288;&#22522;&#20110;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;&#22120;&#65289;&#65292;&#23558;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#36716;&#21270;&#20026;&#19968;&#31181;&#31867;&#20284;&#20110;&#38382;&#31572;&#38382;&#39064;&#30340;&#24418;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#20540;&#21644;&#22270;&#20687;&#25968;&#25454;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#32487;&#32493;&#29983;&#25104;&#19982;&#36712;&#36857;&#30456;&#20851;&#30340;&#22810;&#27169;&#24577;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.18447</link><description>&lt;p&gt;
&#35821;&#35328;&#33021;&#21542;&#32988;&#36807;&#25968;&#20540;&#22238;&#24402;&#65311;&#22522;&#20110;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LMTraj&#65288;&#22522;&#20110;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;&#22120;&#65289;&#65292;&#23558;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#36716;&#21270;&#20026;&#19968;&#31181;&#31867;&#20284;&#20110;&#38382;&#31572;&#38382;&#39064;&#30340;&#24418;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#20540;&#21644;&#22270;&#20687;&#25968;&#25454;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#32487;&#32493;&#29983;&#25104;&#19982;&#36712;&#36857;&#30456;&#20851;&#30340;&#22810;&#27169;&#24577;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18447v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#29983;&#25104;&#24615;&#33021;&#26041;&#38754;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#36817;&#26399;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LMTraj&#65288;&#22522;&#20110;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;&#22120;&#65289;&#65292;&#23558;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#37325;&#26032;&#35774;&#35745;&#20026;&#19968;&#31181;&#31867;&#20284;&#20110;&#38382;&#31572;&#38382;&#39064;&#30340;&#24418;&#24335;&#12290;&#19982;&#20256;&#32479;&#30340;&#23558;&#36712;&#36857;&#22352;&#26631;&#24207;&#21015;&#35270;&#20026;&#36830;&#32493;&#20449;&#21495;&#30340;&#25968;&#20540;&#22238;&#24402;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#31867;&#20284;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#31163;&#25955;&#20449;&#21495;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#36712;&#36857;&#22352;&#26631;&#30340;&#36755;&#20837;&#31354;&#38388;&#36716;&#25442;&#25104;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#12290;&#22312;&#36825;&#37324;&#65292;&#34892;&#20154;&#30340;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#36712;&#36857;&#34987;&#36716;&#25442;&#20026;&#19968;&#20010;&#25991;&#26412;&#25552;&#31034;&#65292;&#22330;&#26223;&#22270;&#20687;&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#34987;&#25551;&#36848;&#20026;&#25991;&#26412;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#36716;&#25442;&#21518;&#30340;&#25968;&#20540;&#21644;&#22270;&#20687;&#25968;&#25454;&#34987;&#21253;&#35013;&#36827;&#38382;&#31572;&#27169;&#26495;&#20013;&#65292;&#20379;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#32487;&#32493;&#29983;&#25104;&#19982;&#36712;&#36857;&#30456;&#20851;&#30340;&#22810;&#27169;&#24577;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#35757;&#32451;&#31574;&#30053;&#65292;&#21516;&#26102;&#20248;&#21270;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18447v1 Announce Type: new  Abstract: Language models have demonstrated impressive ability in context understanding and generative performance. Inspired by the recent success of language foundation models, in this paper, we propose LMTraj (Language-based Multimodal Trajectory predictor), which recasts the trajectory prediction task into a sort of question-answering problem. Departing from traditional numerical regression models, which treat the trajectory coordinate sequence as continuous signals, we consider them as discrete signals like text prompts. Specially, we first transform an input space for the trajectory coordinate into the natural language space. Here, the entire time-series trajectories of pedestrians are converted into a text prompt, and scene images are described as text information through image captioning. The transformed numerical and image data are then wrapped into the question-answering template for use in a language model. Next, to guide the language mo
&lt;/p&gt;</description></item><item><title>FRESCO&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#20998;&#23618;&#25511;&#21046;&#26550;&#26500;&#26469;&#31616;&#21270;&#33021;&#28304;&#24066;&#22330;&#23454;&#26045;&#30340;&#26694;&#26550;&#65292;&#26680;&#24515;&#27010;&#24565;&#26159;&#21019;&#24314;&#19968;&#20010;&#21512;&#20316;&#35774;&#32622;&#65292;&#20351;&#21508;&#20010;&#20010;&#20307;&#30446;&#26631;&#24471;&#20197;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.18444</link><description>&lt;p&gt;
FRESCO&#65306;&#21512;&#20316;&#20248;&#21270;&#30340;&#32852;&#37030;&#24378;&#21270;&#33021;&#28304;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FRESCO: Federated Reinforcement Energy System for Cooperative Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18444
&lt;/p&gt;
&lt;p&gt;
FRESCO&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#20998;&#23618;&#25511;&#21046;&#26550;&#26500;&#26469;&#31616;&#21270;&#33021;&#28304;&#24066;&#22330;&#23454;&#26045;&#30340;&#26694;&#26550;&#65292;&#26680;&#24515;&#27010;&#24565;&#26159;&#21019;&#24314;&#19968;&#20010;&#21512;&#20316;&#35774;&#32622;&#65292;&#20351;&#21508;&#20010;&#20010;&#20307;&#30446;&#26631;&#24471;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#20852;&#36215;&#65292;&#33021;&#28304;&#32593;&#26684;&#20013;&#20986;&#29616;&#20102;&#26032;&#30340;&#21160;&#24577;&#65292;&#25215;&#35834;&#21019;&#36896;&#19968;&#20010;&#26356;&#28165;&#27905;&#12289;&#26356;&#21442;&#19982;&#24335;&#30340;&#33021;&#28304;&#32593;&#26684;&#65292;&#22312;&#36825;&#20010;&#32593;&#26684;&#20013;&#65292;&#25216;&#26415;&#22312;&#23454;&#29616;&#25152;&#38656;&#28789;&#27963;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20197;&#23454;&#29616;&#19979;&#19968;&#20195;&#32593;&#26684;&#30340;&#24895;&#26223;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FRESCO&#65292;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#20998;&#23618;&#25511;&#21046;&#26550;&#26500;&#26469;&#31616;&#21270;&#33021;&#28304;&#24066;&#22330;&#23454;&#26045;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#35777;&#26126;&#30340;&#26680;&#24515;&#27010;&#24565;&#26159;&#65292;&#35753;&#36138;&#23146;&#20195;&#29702;&#21463;&#39640;&#32423;&#20195;&#29702;&#21464;&#21270;&#30340;&#26465;&#20214;&#25903;&#37197;&#65292;&#21019;&#36896;&#20986;&#19968;&#20010;&#21512;&#20316;&#35774;&#32622;&#65292;&#36825;&#23558;&#20801;&#35768;&#23454;&#29616;&#25152;&#26377;&#20010;&#20307;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18444v1 Announce Type: new  Abstract: The rise in renewable energy is creating new dynamics in the energy grid that promise to create a cleaner and more participative energy grid, where technology plays a crucial part in making the required flexibility to achieve the vision of the next-generation grid. This work presents FRESCO, a framework that aims to ease the implementation of energy markets using a hierarchical control architecture of reinforcement learning agents trained using federated learning. The core concept we are proving is that having greedy agents subject to changing conditions from a higher level agent creates a cooperative setup that will allow for fulfilling all the individual objectives. This paper presents a general overview of the framework, the current progress, and some insights we obtained from the recent results.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;Trust Region Policy Optimization&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#38477;&#20302;&#26234;&#33021;&#30005;&#32593;&#30340;&#33021;&#28304;&#25490;&#25918;&#21644;&#25104;&#26412;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18439</link><description>&lt;p&gt;
&#26234;&#33021;&#30005;&#32593;&#30340;&#24191;&#20041;&#31574;&#30053;&#23398;&#20064;&#65306;FL TRPO&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generalized Policy Learning for Smart Grids: FL TRPO Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18439
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;Trust Region Policy Optimization&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#38477;&#20302;&#26234;&#33021;&#30005;&#32593;&#30340;&#33021;&#28304;&#25490;&#25918;&#21644;&#25104;&#26412;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30005;&#32593;&#39046;&#22495;&#38656;&#35201;&#22686;&#24378;&#29616;&#26377;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#30340;&#33021;&#21147;&#65307;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#19982;&#27492;&#30446;&#26631;&#19968;&#33268;&#65292;&#22240;&#20026;&#23427;&#23637;&#31034;&#20102;&#22312;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#26174;&#30528;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#26234;&#33021;&#30005;&#32593;&#24212;&#29992;&#65292;&#36825;&#20123;&#24212;&#29992;&#24448;&#24448;&#28041;&#21450;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#65292;&#36825;&#20123;&#22240;&#32032;&#38459;&#30861;&#20102;&#32447;&#24615;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;FL&#19982;Trust Region Policy Optimization (FL TRPO)&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#38477;&#20302;&#33021;&#28304;&#30456;&#20851;&#25490;&#25918;&#21644;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25581;&#31034;&#20102;&#28508;&#22312;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#37319;&#29992;&#20010;&#24615;&#21270;&#30340;&#32534;&#30721;&#26041;&#27861;&#26469;&#25429;&#25417;&#29420;&#29305;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#29702;&#35299;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#26368;&#20339;&#31574;&#30053;&#65292;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#20197;&#21069;&#26410;&#35265;&#30340;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#65292;&#35777;&#23454;&#20102;&#20854;&#22312;&#26377;&#25928;&#23398;&#20064;&#26041;&#38754;&#30340;&#39640;&#25928;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18439v1 Announce Type: new  Abstract: The smart grid domain requires bolstering the capabilities of existing energy management systems; Federated Learning (FL) aligns with this goal as it demonstrates a remarkable ability to train models on heterogeneous datasets while maintaining data privacy, making it suitable for smart grid applications, which often involve disparate data distributions and interdependencies among features that hinder the suitability of linear models. This paper introduces a framework that combines FL with a Trust Region Policy Optimization (FL TRPO) aiming to reduce energy-associated emissions and costs. Our approach reveals latent interconnections and employs personalized encoding methods to capture unique insights, understanding the relationships between features and optimal strategies, allowing our model to generalize to previously unseen data. Experimental results validate the robustness of our approach, affirming its proficiency in effectively learn
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#38024;&#23545;&#20840;&#29699;&#26893;&#34987;&#27963;&#21160;&#24314;&#27169;&#65292;&#22312;&#20013;&#26399;&#22825;&#27668;&#39044;&#27979;&#25104;&#21151;&#20351;&#29992;&#30340;Transformer&#27169;&#22411;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#35843;&#25972;&#39044;&#35757;&#32451;FourCastNet&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#22825;&#27668;&#27169;&#22411;&#33021;&#25552;&#21319;&#20840;&#29699;&#26893;&#34987;&#27963;&#21160;&#20272;&#31639;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.18438</link><description>&lt;p&gt;
&#20840;&#29699;&#26893;&#34987;&#24314;&#27169;&#19982;&#39044;&#35757;&#32451;&#22825;&#27668;Transformer
&lt;/p&gt;
&lt;p&gt;
Global Vegetation Modeling with Pre-Trained Weather Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18438
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#38024;&#23545;&#20840;&#29699;&#26893;&#34987;&#27963;&#21160;&#24314;&#27169;&#65292;&#22312;&#20013;&#26399;&#22825;&#27668;&#39044;&#27979;&#25104;&#21151;&#20351;&#29992;&#30340;Transformer&#27169;&#22411;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#35843;&#25972;&#39044;&#35757;&#32451;FourCastNet&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#22825;&#27668;&#27169;&#22411;&#33021;&#25552;&#21319;&#20840;&#29699;&#26893;&#34987;&#27963;&#21160;&#20272;&#31639;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#26893;&#34987;&#27169;&#22411;&#21487;&#20197;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#26893;&#34987;&#27963;&#21160;&#19982;&#29983;&#24577;&#31995;&#32479;&#36807;&#31243;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#30001;&#20110;&#38271;&#26399;&#36235;&#21183;&#21644;&#28201;&#24230;&#12289;&#38477;&#27700;&#30340;&#30701;&#26399;&#21464;&#21270;&#23545;&#26893;&#34987;&#27963;&#21160;&#26377;&#24433;&#21709;&#65292;&#25105;&#20204;&#21463;&#21040;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20013;&#26399;&#22825;&#27668;&#39044;&#27979;&#20013;&#21462;&#24471;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#23558;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#35757;&#32451;FourCastNet&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#65292;&#20197;&#27169;&#25311;&#26893;&#34987;&#27963;&#21160;&#24182;&#32771;&#34385;&#27668;&#20505;&#21464;&#21270;&#30340;&#30701;&#26399;&#21160;&#24577;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#23398;&#20064;&#30340;&#22823;&#27668;&#29366;&#24577;&#30340;&#20840;&#29699;&#34920;&#31034;&#22914;&#20309;&#36716;&#31227;&#65292;&#20197;&#27169;&#25311;&#24402;&#19968;&#21270;&#24046;&#24322;&#26893;&#34987;&#25351;&#25968;&#65288;NDVI&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;\SI{0.25}{\degree}&#30340;&#20998;&#36776;&#29575;&#20840;&#29699;&#20272;&#35745;&#26893;&#34987;&#27963;&#21160;&#65292;&#21482;&#20381;&#36182;&#20110;&#27668;&#35937;&#25968;&#25454;&#12290;&#25105;&#20204;&#35777;&#26126;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22825;&#27668;&#27169;&#22411;&#33021;&#25552;&#21319;NDVI&#20272;&#31639;&#25928;&#26524;&#65292;&#30456;&#36739;&#20110;&#21333;&#29420;&#23398;&#20064;NDVI&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18438v1 Announce Type: new  Abstract: Accurate vegetation models can produce further insights into the complex interaction between vegetation activity and ecosystem processes. Previous research has established that long-term trends and short-term variability of temperature and precipitation affect vegetation activity. Motivated by the recent success of Transformer-based Deep Learning models for medium-range weather forecasting, we adapt the publicly available pre-trained FourCastNet to model vegetation activity while accounting for the short-term dynamics of climate variability. We investigate how the learned global representation of the atmosphere's state can be transferred to model the normalized difference vegetation index (NDVI). Our model globally estimates vegetation activity at a resolution of \SI{0.25}{\degree} while relying only on meteorological data. We demonstrate that leveraging pre-trained weather models improves the NDVI estimates compared to learning an NDVI 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#20316;&#24335;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#22810;&#20010;&#21512;&#20316;&#32773;&#33021;&#22815;&#22312;&#19981;&#25259;&#38706;&#24050;&#26377;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20998;&#20139;&#26032;&#39046;&#22495;&#30340;&#39044;&#27979;&#32467;&#26524;&#21644;&#26032;&#33719;&#24471;&#30340;&#26631;&#31614;&#26469;&#25506;&#32034;&#26032;&#39046;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#38544;&#31169;&#23433;&#20840;&#12289;&#36164;&#28304;&#20849;&#20139;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.18436</link><description>&lt;p&gt;
&#26377;&#26465;&#20214;&#20449;&#20219;&#29615;&#22659;&#20013;&#30340;&#21327;&#20316;&#24335;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Collaborative Active Learning in Conditional Trust Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18436
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#20316;&#24335;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#22810;&#20010;&#21512;&#20316;&#32773;&#33021;&#22815;&#22312;&#19981;&#25259;&#38706;&#24050;&#26377;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20998;&#20139;&#26032;&#39046;&#22495;&#30340;&#39044;&#27979;&#32467;&#26524;&#21644;&#26032;&#33719;&#24471;&#30340;&#26631;&#31614;&#26469;&#25506;&#32034;&#26032;&#39046;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#38544;&#31169;&#23433;&#20840;&#12289;&#36164;&#28304;&#20849;&#20139;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26377;&#26465;&#20214;&#20449;&#20219;&#29615;&#22659;&#20013;&#30340;&#21327;&#20316;&#24335;&#20027;&#21160;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#20010;&#21512;&#20316;&#32773;&#36890;&#36807;&#21033;&#29992;&#20854;&#32467;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#33021;&#21147;&#26469;&#25506;&#32034;&#19968;&#20010;&#26032;&#39046;&#22495;&#30340;&#33539;&#20363;&#65292;&#32780;&#19981;&#38656;&#36879;&#38706;&#20182;&#20204;&#29616;&#26377;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#21512;&#20316;&#32773;&#20204;&#20998;&#20139;&#26469;&#33258;&#26032;&#39046;&#22495;&#30340;&#39044;&#27979;&#32467;&#26524;&#21644;&#26032;&#33719;&#24471;&#30340;&#26631;&#31614;&#12290;&#36825;&#31181;&#21327;&#20316;&#25552;&#20379;&#20102;&#20960;&#20010;&#20248;&#21183;&#65306;(a) &#36890;&#36807;&#28040;&#38500;&#30452;&#25509;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#25259;&#38706;&#38656;&#27714;&#65292;&#35299;&#20915;&#20102;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65307;(b) &#26080;&#38656;&#30452;&#25509;&#25968;&#25454;&#20132;&#25442;&#65292;&#23454;&#29616;&#20102;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#28304;&#21644;&#27934;&#35265;&#65307;&#20197;&#21450;(c) &#36890;&#36807;&#20849;&#20139;&#26631;&#35760;&#25104;&#26412;&#65292;&#20419;&#36827;&#20102;&#25104;&#26412;&#25928;&#30410;&#21644;&#36164;&#28304;&#25928;&#29575;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#22909;&#22788;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#23454;&#29616;&#19978;&#36848;&#30446;&#26631;&#30340;&#21327;&#20316;&#24335;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#20223;&#30495;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21327;&#20316;&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;AUC&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18436v1 Announce Type: new  Abstract: In this paper, we investigate collaborative active learning, a paradigm in which multiple collaborators explore a new domain by leveraging their combined machine learning capabilities without disclosing their existing data and models. Instead, the collaborators share prediction results from the new domain and newly acquired labels. This collaboration offers several advantages: (a) it addresses privacy and security concerns by eliminating the need for direct model and data disclosure; (b) it enables the use of different data sources and insights without direct data exchange; and (c) it promotes cost-effectiveness and resource efficiency through shared labeling costs. To realize these benefits, we introduce a collaborative active learning framework designed to fulfill the aforementioned objectives. We validate the effectiveness of the proposed framework through simulations. The results demonstrate that collaboration leads to higher AUC sco
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;U-Sketch&#26694;&#26550;&#65292;&#20855;&#26377;U-Net&#31867;&#22411;&#30340;&#28508;&#22312;&#36793;&#32536;&#39044;&#27979;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25913;&#36827;&#33609;&#22270;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#31354;&#38388;&#24067;&#23616;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.18425</link><description>&lt;p&gt;
U-Sketch: &#19968;&#31181;&#29992;&#20110;&#33609;&#22270;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18425
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;U-Sketch&#26694;&#26550;&#65292;&#20855;&#26377;U-Net&#31867;&#22411;&#30340;&#28508;&#22312;&#36793;&#32536;&#39044;&#27979;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25913;&#36827;&#33609;&#22270;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#31354;&#38388;&#24067;&#23616;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20135;&#29983;&#20102;&#31526;&#21512;&#30456;&#24212;&#25991;&#26412;&#25552;&#31034;&#30340;&#36924;&#30495;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#33609;&#22270;&#21040;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#20173;&#26377;&#25152;&#27424;&#32570;&#65292;&#29983;&#25104;&#22270;&#20687;&#30340;&#31354;&#38388;&#24067;&#23616;&#19981;&#20165;&#35201;&#36981;&#24490;&#25991;&#26412;&#25552;&#31034;&#65292;&#36824;&#24517;&#39035;&#32039;&#23494;&#36319;&#38543;&#26576;&#20123;&#21442;&#32771;&#33609;&#22270;&#30340;&#36718;&#24275;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#20351;&#29992;MLP&#28508;&#22312;&#36793;&#32536;&#39044;&#27979;&#22120;&#26469;&#24341;&#23548;&#21512;&#25104;&#22270;&#20687;&#30340;&#31354;&#38388;&#24067;&#23616;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#39044;&#27979;&#36793;&#32536;&#22320;&#22270;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;MLP&#30340;&#36880;&#20687;&#32032;&#25805;&#20316;&#24182;&#26410;&#23558;&#31354;&#38388;&#24067;&#23616;&#20316;&#20026;&#25972;&#20307;&#32771;&#34385;&#36827;&#26469;&#65292;&#38656;&#35201;&#22823;&#37327;&#21435;&#22122;&#36845;&#20195;&#25165;&#33021;&#29983;&#25104;&#20196;&#20154;&#28385;&#24847;&#30340;&#22270;&#20687;&#65292;&#23548;&#33268;&#26102;&#38388;&#25928;&#29575;&#20302;&#19979;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;U-Sketch&#65292;&#36825;&#26159;&#19968;&#20010;&#24341;&#20837;&#20102;U-Net&#31867;&#22411;&#28508;&#22312;&#36793;&#32536;&#39044;&#27979;&#22120;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18425v1 Announce Type: cross  Abstract: Diffusion models have demonstrated remarkable performance in text-to-image synthesis, producing realistic and high resolution images that faithfully adhere to the corresponding text-prompts. Despite their great success, they still fall behind in sketch-to-image synthesis tasks, where in addition to text-prompts, the spatial layout of the generated images has to closely follow the outlines of certain reference sketches. Employing an MLP latent edge predictor to guide the spatial layout of the synthesized image by predicting edge maps at each denoising step has been recently proposed. Despite yielding promising results, the pixel-wise operation of the MLP does not take into account the spatial layout as a whole, and demands numerous denoising iterations to produce satisfactory images, leading to time inefficiency. To this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge predictor, which is capable of efficiently
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Semantic Robust Defence (SemRoDe)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23439;&#35266;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#40065;&#26834;&#24615;&#65292;&#23398;&#20064;&#20102;&#33021;&#22815;&#36830;&#25509;&#23545;&#25239;&#39046;&#22495;&#21644;&#22522;&#26412;&#39046;&#22495;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.18423</link><description>&lt;p&gt;
SemRoDe: &#23439;&#35266;&#23545;&#25239;&#35757;&#32451;&#20197;&#23398;&#20064;&#23545;&#21333;&#35789;&#32423;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18423
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Semantic Robust Defence (SemRoDe)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23439;&#35266;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#40065;&#26834;&#24615;&#65292;&#23398;&#20064;&#20102;&#33021;&#22815;&#36830;&#25509;&#23545;&#25239;&#39046;&#22495;&#21644;&#22522;&#26412;&#39046;&#22495;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#20173;&#28982;&#20196;&#20154;&#25285;&#24551;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#65292;&#20294;&#20854;&#22312;&#38450;&#24481;&#21333;&#35789;&#32423;&#25915;&#20987;&#26041;&#38754;&#30340;&#25913;&#36827;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#40065;&#26834;&#38450;&#24481;&#65288;SemRoDe&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#23439;&#35266;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#65292;&#26088;&#22312;&#22686;&#24378;LMs&#30340;&#40065;&#26834;&#24615;&#12290;&#21463;&#21040;&#22270;&#20687;&#39046;&#22495;&#26368;&#36817;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#35752;&#24182;&#30830;&#35748;&#22312;&#20687;&#35821;&#35328;&#36825;&#26679;&#30340;&#31163;&#25955;&#25968;&#25454;&#35774;&#32622;&#20013;&#65292;&#36890;&#36807;&#21333;&#35789;&#26367;&#25442;&#29983;&#25104;&#30340;&#23545;&#25239;&#26679;&#26412;&#30830;&#23454;&#23646;&#20110;&#19968;&#20010;&#23637;&#29616;&#19982;&#22522;&#26412;&#22495;&#20855;&#26377;&#39640;Wasserstein&#36317;&#31163;&#30340;&#23545;&#25239;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#20010;&#33021;&#22815;&#36830;&#25509;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;&#25105;&#20204;&#20551;&#35774;&#22914;&#26524;&#26679;&#26412;&#34987;&#25237;&#24433;&#21040;&#19968;&#20010;&#38750;&#23545;&#25239;&#39046;&#22495;&#65292;&#32780;&#26159;&#25237;&#24433;&#21040;&#19968;&#20010;&#20855;&#26377;&#26368;&#23567;&#20559;&#31227;&#30340;&#39046;&#22495;&#65292;&#37027;&#20040;&#21487;&#33021;&#20250;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18423v1 Announce Type: new  Abstract: Language models (LMs) are indispensable tools for natural language processing tasks, but their vulnerability to adversarial attacks remains a concern. While current research has explored adversarial training techniques, their improvements to defend against word-level attacks have been limited. In this work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing inspiration from recent studies in the image domain, we investigate and later confirm that in a discrete data setting such as language, adversarial samples generated via word substitutions do indeed belong to an adversarial domain exhibiting a high Wasserstein distance from the base domain. Our method learns a robust representation that bridges these two domains. We hypothesize that if samples were not projected into an adversarial domain, but instead to a domain with minimal shift, it w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25299;&#25169;&#29702;&#35770;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#23545;Transformer&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20855;&#26377;&#39640;&#38454;&#25512;&#29702;&#30340;&#29305;&#28857;&#65292;&#24182;&#19982;&#20854;&#20182;&#24120;&#35265;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;</title><link>https://arxiv.org/abs/2403.18415</link><description>&lt;p&gt;
Transformer&#32593;&#32476;&#30340;&#25299;&#25169;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Topos of Transformer Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18415
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#29702;&#35770;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#23545;Transformer&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20855;&#26377;&#39640;&#38454;&#25512;&#29702;&#30340;&#29305;&#28857;&#65292;&#24182;&#19982;&#20854;&#20182;&#24120;&#35265;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#36828;&#36828;&#36229;&#36234;&#25152;&#26377;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32972;&#21518;&#30340;&#24341;&#25806;&#12290;&#25105;&#20204;&#36890;&#36807;&#25299;&#25169;&#29702;&#35770;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#23545;Transformer&#26550;&#26500;&#34920;&#36798;&#33021;&#21147;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#20174;&#36825;&#20010;&#35266;&#28857;&#20986;&#21457;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#24120;&#35265;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22914;&#21367;&#31215;&#32593;&#32476;&#12289;&#24490;&#29615;&#32593;&#32476;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#21487;&#20197;&#23884;&#20837;&#22312;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#39044;&#25299;&#25169;&#20013;&#65292;&#20294;Transformer&#24517;&#28982;&#23384;&#22312;&#20110;&#20854;&#25299;&#25169;&#23436;&#22791;&#24615;&#20013;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#34920;&#26126;&#36825;&#20004;&#20010;&#32593;&#32476;&#23478;&#26063;&#23454;&#20363;&#21270;&#20102;&#19981;&#21516;&#30340;&#36923;&#36753;&#29255;&#27573;&#65306;&#21069;&#32773;&#26159;&#19968;&#38454;&#30340;&#65292;&#32780;Transformers&#26159;&#39640;&#38454;&#25512;&#29702;&#26426;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25299;&#25169;&#29702;&#35770;&#19982;&#26550;&#26500;&#25628;&#32034;&#21644;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#20102;&#31867;&#27604;&#65292;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#32435;&#20837;&#20102;&#25511;&#21046;&#35770;&#20195;&#29702;&#30340;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18415v1 Announce Type: new  Abstract: The transformer neural network has significantly out-shined all other neural network architectures as the engine behind large language models. We provide a theoretical analysis of the expressivity of the transformer architecture through the lens of topos theory. From this viewpoint, we show that many common neural network architectures, such as the convolutional, recurrent and graph convolutional networks, can be embedded in a pretopos of piecewise-linear functions, but that the transformer necessarily lives in its topos completion. In particular, this suggests that the two network families instantiate different fragments of logic: the former are first order, whereas transformers are higher-order reasoners. Furthermore, we draw parallels with architecture search and gradient descent, integrating our analysis in the framework of cybernetic agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#21333;&#19968;&#30340;Vision Language Model (VLM) &#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#39057;&#38382;&#31572;&#65292;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#21333;&#20010;&#21512;&#25104;&#22270;&#20687;&#20197;&#23454;&#29616;&#35270;&#39057;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.18406</link><description>&lt;p&gt;
&#22270;&#20687;&#32593;&#26684;&#21487;&#33021;&#27604;&#35270;&#39057;&#26356;&#26377;&#20215;&#20540;&#65306;&#20351;&#29992;VLM&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#39057;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#21333;&#19968;&#30340;Vision Language Model (VLM) &#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#39057;&#38382;&#31572;&#65292;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#21333;&#20010;&#21512;&#25104;&#22270;&#20687;&#20197;&#23454;&#29616;&#35270;&#39057;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#20154;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#29992;&#20110;&#36830;&#25509;&#35270;&#39057;&#27169;&#24577;&#30340;&#31574;&#30053;&#12290;&#20854;&#20013;&#19968;&#31181;&#31361;&#20986;&#30340;&#31574;&#30053;&#28041;&#21450;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#65288;VideoLMs&#65289;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#25509;&#21475;&#23558;&#20808;&#36827;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#19982;LLMs&#36830;&#25509;&#36215;&#26469;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#21478;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#22810;&#20010;&#38454;&#27573;&#36328;&#27169;&#24577;&#36827;&#34892;&#27169;&#24577;&#26725;&#25509;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#29616;&#25104;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;VideoLMs&#21644;LLMs&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#21364;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#21482;&#20351;&#29992;&#21333;&#19968;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#12290;&#25105;&#20204;&#30340;&#20986;&#21457;&#28857;&#26159;&#35270;&#39057;&#21253;&#21547;&#19968;&#31995;&#21015;&#22270;&#20687;&#25110;&#24103;&#65292;&#36825;&#20123;&#22270;&#20687;&#19982;&#26102;&#38388;&#20449;&#24687;&#20132;&#32455;&#22312;&#19968;&#36215;&#30340;&#31616;&#21333;&#27934;&#23519;&#12290;&#35270;&#39057;&#29702;&#35299;&#30340;&#31934;&#39635;&#22312;&#20110;&#24039;&#22937;&#22320;&#31649;&#29702;&#27599;&#20010;&#24103;&#30340;&#26102;&#38388;&#26041;&#38754;&#20197;&#21450;&#31354;&#38388;&#32454;&#33410;&#12290;&#21021;&#22987;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#25490;&#21015;&#22810;&#20010;&#24103;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#21333;&#20010;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18406v1 Announce Type: cross  Abstract: Stimulated by the sophisticated reasoning capabilities of recent Large Language Models (LLMs), a variety of strategies for bridging video modality have been devised. A prominent strategy involves Video Language Models (VideoLMs), which train a learnable interface with video data to connect advanced vision encoders with LLMs. Recently, an alternative strategy has surfaced, employing readily available foundation models, such as VideoLMs and LLMs, across multiple stages for modality bridging. In this study, we introduce a simple yet novel strategy where only a single Vision Language Model (VLM) is utilized. Our starting point is the plain insight that a video comprises a series of images, or frames, interwoven with temporal information. The essence of video comprehension lies in adeptly managing the temporal aspects along with the spatial details of each frame. Initially, we transform a video into a single composite image by arranging mul
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30005;&#32593;&#39057;&#29575;&#36827;&#34892;&#30005;&#32593;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#39057;&#35889;&#22270;&#24182;&#34701;&#21512;&#22810;&#20010;&#20998;&#31867;&#22120;&#65292;&#26368;&#32456;&#35774;&#35745;&#20986;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#34701;&#21512;&#36807;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20854;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18402</link><description>&lt;p&gt;
&#22312;&#22810;&#20998;&#31867;&#22120;&#34701;&#21512;&#26694;&#26550;&#20013;&#23545;&#30005;&#32593;&#20998;&#31867;&#20351;&#29992;&#30005;&#32593;&#39057;&#29575;&#30340;&#39057;&#35889;&#22270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On Spectrogram Analysis in a Multiple Classifier Fusion Framework for Power Grid Classification Using Electric Network Frequency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18402
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30005;&#32593;&#39057;&#29575;&#36827;&#34892;&#30005;&#32593;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#39057;&#35889;&#22270;&#24182;&#34701;&#21512;&#22810;&#20010;&#20998;&#31867;&#22120;&#65292;&#26368;&#32456;&#35774;&#35745;&#20986;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#34701;&#21512;&#36807;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20854;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#32593;&#39057;&#29575;&#65288;ENF&#65289;&#20316;&#20026;&#20379;&#30005;&#31995;&#32479;&#22266;&#26377;&#30340;&#29420;&#29305;&#26631;&#35782;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;ENF&#30340;&#30005;&#32593;&#20998;&#31867;&#26032;&#26041;&#27861;&#12290;&#20174;&#19981;&#21516;&#30005;&#32593;&#30340;&#38899;&#39057;&#21644;&#21151;&#29575;&#35760;&#24405;&#20013;&#29983;&#25104;&#39057;&#35889;&#22270;&#65292;&#25581;&#31034;&#26377;&#21161;&#20110;&#36890;&#36807;&#22810;&#20010;&#20998;&#31867;&#22120;&#34701;&#21512;&#36827;&#34892;&#30005;&#32593;&#20998;&#31867;&#30340;&#29420;&#29305;ENF&#27169;&#24335;&#12290;&#38024;&#23545;One-vs-All&#20998;&#31867;&#65292;&#24320;&#21457;&#20102;&#22235;&#31181;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20197;&#21450;&#20351;&#29992;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#36827;&#34892;&#20248;&#21270;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#35813;&#36807;&#31243;&#23545;&#27599;&#20010;&#26679;&#26412;&#29983;&#25104;&#22823;&#37327;&#39044;&#27979;&#32467;&#26524;&#65292;&#28982;&#21518;&#32534;&#21046;&#24182;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#27973;&#23618;&#22810;&#26631;&#31614;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27169;&#25311;&#34701;&#21512;&#36807;&#31243;&#65292;&#26368;&#32456;&#23548;&#33268;&#23545;&#27599;&#20010;&#26679;&#26412;&#30340;&#30830;&#23450;&#31867;&#21035;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39564;&#35777;&#21644;&#27979;&#35797;&#20934;&#30830;&#24615;&#22343;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#65292;&#31361;&#26174;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18402v1 Announce Type: new  Abstract: The Electric Network Frequency (ENF) serves as a unique signature inherent to power distribution systems. Here, a novel approach for power grid classification is developed, leveraging ENF. Spectrograms are generated from audio and power recordings across different grids, revealing distinctive ENF patterns that aid in grid classification through a fusion of classifiers. Four traditional machine learning classifiers plus a Convolutional Neural Network (CNN), optimized using Neural Architecture Search, are developed for One-vs-All classification. This process generates numerous predictions per sample, which are then compiled and used to train a shallow multi-label neural network specifically designed to model the fusion process, ultimately leading to the conclusive class prediction for each sample. Experimental findings reveal that both validation and testing accuracy outperform those of current state-of-the-art classifiers, underlining the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#25913;&#36827;&#30340;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(mDCGAN)&#65292;&#38024;&#23545;&#39640;&#36136;&#37327;&#33402;&#26415;&#21697;&#29983;&#25104;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35299;&#20915;&#20102;&#26222;&#36941;&#35757;&#32451;&#38382;&#39064;&#65292;&#26377;&#25928;&#25506;&#32034;&#25277;&#35937;&#32472;&#30011;&#20013;&#30340;&#39068;&#33394;&#21644;&#31508;&#35302;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.18397</link><description>&lt;p&gt;
&#20351;&#29992;&#25913;&#36827;&#30340;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#25277;&#35937;&#33402;&#26415;&#20013;&#36827;&#34892;&#39068;&#33394;&#21644;&#31508;&#35302;&#27169;&#24335;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Colour and Brush Stroke Pattern Recognition in Abstract Art using Modified Deep Convolutional Generative Adversarial Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#25913;&#36827;&#30340;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(mDCGAN)&#65292;&#38024;&#23545;&#39640;&#36136;&#37327;&#33402;&#26415;&#21697;&#29983;&#25104;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35299;&#20915;&#20102;&#26222;&#36941;&#35757;&#32451;&#38382;&#39064;&#65292;&#26377;&#25928;&#25506;&#32034;&#25277;&#35937;&#32472;&#30011;&#20013;&#30340;&#39068;&#33394;&#21644;&#31508;&#35302;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#33402;&#26415;&#26159;&#19968;&#31181;&#24191;&#21463;&#27426;&#36814;&#12289;&#34987;&#24191;&#27867;&#35752;&#35770;&#30340;&#33402;&#26415;&#24418;&#24335;&#65292;&#36890;&#24120;&#33021;&#22815;&#25551;&#32472;&#20986;&#33402;&#26415;&#23478;&#30340;&#24773;&#24863;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#23581;&#35797;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#36793;&#32536;&#26816;&#27979;&#12289;&#31508;&#35302;&#21644;&#24773;&#24863;&#35782;&#21035;&#31639;&#27861;&#26469;&#30740;&#31350;&#25277;&#35937;&#33402;&#26415;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;(GAN)&#23545;&#24191;&#27867;&#20998;&#24067;&#30340;&#25277;&#35937;&#32472;&#30011;&#36827;&#34892;&#30740;&#31350;&#12290; GAN&#20855;&#26377;&#23398;&#20064;&#21644;&#20877;&#29616;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26377;&#25928;&#22320;&#25506;&#32034;&#21644;&#30740;&#31350;&#29983;&#25104;&#30340;&#22270;&#20687;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#20811;&#26381;&#24120;&#35265;&#35757;&#32451;&#38382;&#39064;&#30340;&#39640;&#25928;GAN&#26550;&#26500;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#39640;&#36136;&#37327;&#33402;&#26415;&#21697;&#29983;&#25104;&#30340;&#25913;&#36827;DCGAN(mDCGAN)&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#23545;&#25152;&#20570;&#20462;&#25913;&#30340;&#28145;&#20837;&#25506;&#35752;&#65292;&#28145;&#20837;&#30740;&#31350;DCGAN&#30340;&#22797;&#26434;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18397v1 Announce Type: cross  Abstract: Abstract Art is an immensely popular, discussed form of art that often has the ability to depict the emotions of an artist. Many researchers have made attempts to study abstract art in the form of edge detection, brush stroke and emotion recognition algorithms using machine and deep learning. This papers describes the study of a wide distribution of abstract paintings using Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and reproduce a distribution enabling researchers and scientists to effectively explore and study the generated image space. However, the challenge lies in developing an efficient GAN architecture that overcomes common training pitfalls. This paper addresses this challenge by introducing a modified-DCGAN (mDCGAN) specifically designed for high-quality artwork generation. The approach involves a thorough exploration of the modifications made, delving into the intricate workings of DCGANs, opt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24352;&#37327;&#30340;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#19968;&#33268;&#24615;&#21644;&#29305;&#24322;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#30456;&#20284;&#24615;&#27979;&#37327;&#21644;&#22270;&#20449;&#24687;&#21033;&#29992;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18393</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#30340;&#19968;&#33268;&#24615;&#21644;&#29305;&#24322;&#24615;&#22810;&#35270;&#35282;&#32858;&#31867;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tensor-based Graph Learning with Consistency and Specificity for Multi-view Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24352;&#37327;&#30340;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#19968;&#33268;&#24615;&#21644;&#29305;&#24322;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#30456;&#20284;&#24615;&#27979;&#37327;&#21644;&#22270;&#20449;&#24687;&#21033;&#29992;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#22810;&#35270;&#35282;&#32858;&#31867;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#29616;&#26377;&#30340;&#22270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#22522;&#20110;&#27010;&#29575;&#37051;&#23621;&#26500;&#24314;&#33258;&#36866;&#24212;&#37051;&#23621;&#22270;&#65292;&#28982;&#21518;&#23398;&#20064;&#19968;&#33268;&#24615;&#22270;&#36827;&#34892;&#32858;&#31867;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#20004;&#20010;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#27431;&#27663;&#36317;&#31163;&#26469;&#34913;&#37327;&#30456;&#20284;&#24615;&#65292;&#36825;&#22312;&#35768;&#22810;&#30495;&#23454;&#22330;&#26223;&#20013;&#25429;&#25417;&#25968;&#25454;&#28857;&#38388;&#30340;&#20869;&#22312;&#32467;&#26500;&#26102;&#35777;&#26126;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#20854;&#27425;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20165;&#20851;&#27880;&#19968;&#33268;&#24615;&#22270;&#65292;&#24573;&#30053;&#20102;&#29305;&#23450;&#35270;&#22270;&#30340;&#22270;&#20449;&#24687;&#12290;&#38024;&#23545;&#19978;&#36848;&#32570;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24352;&#37327;&#30340;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#19968;&#33268;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#22312;&#26031;&#33922;&#24343;&#23572;&#27969;&#24418;&#19978;&#35745;&#31639;&#30456;&#20284;&#36317;&#31163;&#20197;&#20445;&#30041;&#20869;&#22312;str
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18393v1 Announce Type: new  Abstract: Graph learning is widely recognized as a crucial technique in multi-view clustering. Existing graph learning methods typically involve constructing an adaptive neighbor graph based on probabilistic neighbors and then learning a consensus graph to for clustering, however, they are confronted with two limitations. Firstly, they often rely on Euclidean distance to measure similarity when constructing the adaptive neighbor graph, which proves inadequate in capturing the intrinsic structure among data points in many real-world scenarios. Secondly, most of these methods focus solely on consensus graph, ignoring view-specific graph information. In response to the aforementioned drawbacks, we in this paper propose a novel tensor-based graph learning framework that simultaneously considers consistency and specificity for multi-view clustering. Specifically, we calculate the similarity distance on the Stiefel manifold to preserve the intrinsic str
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#20026;&#31867;&#22686;&#37327;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#20026;&#22270;&#20687;&#29983;&#25104;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2403.18383</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#26159;&#33391;&#22909;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generative Multi-modal Models are Good Class-Incremental Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18383
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#20026;&#31867;&#22686;&#37327;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#20026;&#22270;&#20687;&#29983;&#25104;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#20998;&#31867;&#22120;&#23545;&#24403;&#21069;&#20219;&#21153;&#30340;&#20559;&#35265;&#32780;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#21028;&#21035;&#27169;&#22411;&#30340;&#29305;&#24615;&#25152;&#33268;&#12290;&#38543;&#30528;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25105;&#20204;&#23558;&#25506;&#35752;&#23558;&#21028;&#21035;&#27169;&#22411;&#26367;&#25442;&#20026;&#29983;&#25104;&#27169;&#22411;&#20197;&#29992;&#20110;CIL&#12290;&#28982;&#32780;&#65292;&#20174;&#21028;&#21035;&#27169;&#22411;&#36807;&#28193;&#21040;&#29983;&#25104;&#27169;&#22411;&#38656;&#35201;&#35299;&#20915;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#20449;&#24687;&#36716;&#31227;&#21040;&#19981;&#21516;&#31867;&#21035;&#30340;&#20998;&#31867;&#20013;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#38656;&#35201;&#23558;CIL&#20219;&#21153;&#32622;&#20110;&#29983;&#25104;&#26694;&#26550;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#65288;GMM&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#20351;&#29992;&#32463;&#35843;&#25972;&#30340;&#29983;&#25104;&#27169;&#22411;&#20026;&#22270;&#20687;&#29983;&#25104;&#26631;&#31614;&#12290;&#22312;&#33719;&#24471;&#35814;&#32454;&#25991;&#26412;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18383v1 Announce Type: cross  Abstract: In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic forgetting caused by the classifier's bias towards the current task has long posed a significant challenge. It is mainly caused by the characteristic of discriminative models. With the growing popularity of the generative multi-modal models, we would explore replacing discriminative models with generative ones for CIL. However, transitioning from discriminative to generative models requires addressing two key challenges. The primary challenge lies in transferring the generated textual information into the classification of distinct categories. Additionally, it requires formulating the task of CIL within a generative framework. To this end, we propose a novel generative multi-modal model (GMM) framework for class-incremental learning. Our approach directly generates labels for images using an adapted generative model. After obtaining the detailed text, we use 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MLPs&#30340;IIP-Mixer&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#20869;&#37096;&#34917;&#19969;&#21644;&#36328;&#34917;&#19969;&#32500;&#24230;&#36827;&#34892;&#28151;&#21512;&#25805;&#20316;&#65292;&#23454;&#29616;&#38146;&#31163;&#23376;&#30005;&#27744;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.18379</link><description>&lt;p&gt;
IIP-Mixer&#65306;&#29992;&#20110;&#30005;&#27744;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#30340;Intra-Inter Patch&#28151;&#21512;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining Useful Life Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18379
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MLPs&#30340;IIP-Mixer&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#20869;&#37096;&#34917;&#19969;&#21644;&#36328;&#34917;&#19969;&#32500;&#24230;&#36827;&#34892;&#28151;&#21512;&#25805;&#20316;&#65292;&#23454;&#29616;&#38146;&#31163;&#23376;&#30005;&#27744;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20272;&#35745;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#21097;&#20313;&#23551;&#21629;&#23545;&#20110;&#32500;&#25345;&#21487;&#20805;&#30005;&#30005;&#27744;&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#31283;&#23450;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28041;&#21450;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#24577;&#65292;&#36825;&#39033;&#20219;&#21153;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32593;&#32476;&#65292;&#22914;&#21464;&#21387;&#22120;&#21644;Informer&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24050;&#32463;&#25104;&#20026;&#27969;&#34892;&#30340;&#26550;&#26500;&#12290;&#23613;&#31649;&#23427;&#20204;&#26377;&#25928;&#65292;&#20294;&#36825;&#20123;&#20855;&#26377;&#20016;&#23500;&#21442;&#25968;&#30340;&#27169;&#22411;&#24517;&#39035;&#32791;&#36153;&#22823;&#37327;&#35757;&#32451;&#26102;&#38388;&#25165;&#33021;&#25581;&#31034;&#26102;&#38388;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;MLP-Mixer&#30340;&#26550;&#26500;&#65292;&#21517;&#20026;&#8220;Intra-Inter Patch Mixer&#8221;&#65288;IIP-Mixer&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#20165;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#27839;&#30528;&#20869;&#37096;&#34917;&#19969;&#21644;&#36328;&#34917;&#19969;&#32500;&#24230;&#36827;&#34892;&#28151;&#21512;&#25805;&#20316;&#26469;&#25552;&#21462;&#26377;&#20851;&#30005;&#27744;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#30340;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;IIP-Mixer&#21253;&#25324;&#24182;&#34892;&#30340;&#21452;&#22836;&#28151;&#21512;&#22120;&#23618;&#65306;&#20869;&#37096;&#34917;&#19969;&#28151;&#21512;ML
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18379v1 Announce Type: cross  Abstract: Accurately estimating the Remaining Useful Life (RUL) of lithium-ion batteries is crucial for maintaining the safe and stable operation of rechargeable battery management systems. However, this task is often challenging due to the complex temporal dynamics involved. Recently, attention-based networks, such as Transformers and Informer, have been the popular architecture in time series forecasting. Despite their effectiveness, these models with abundant parameters necessitate substantial training time to unravel temporal patterns. To tackle these challenges, we propose a simple MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which is an architecture based exclusively on multi-layer perceptrons (MLPs), extracting information by mixing operations along both intra-patch and inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer comprises parallel dual-head mixer layers: the intra-patch mixing ML
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#30340;&#20851;&#27880;Stragglers&#30340;&#20302;&#24310;&#36831;&#21516;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;stragglers&#21516;&#27493;&#20256;&#36882;&#37096;&#20998;&#26799;&#24230;&#65292;&#20174;&#32780;&#25913;&#21892;&#22312;&#35757;&#32451;&#24310;&#36831;&#32422;&#26463;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18375</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#27425;&#27169;&#22411;&#26356;&#26032;&#30340;&#20851;&#27880;Stragglers&#30340;&#20302;&#24310;&#36831;&#21516;&#27493;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stragglers-Aware Low-Latency Synchronous Federated Learning via Layer-Wise Model Updates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18375
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#30340;&#20851;&#27880;Stragglers&#30340;&#20302;&#24310;&#36831;&#21516;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;stragglers&#21516;&#27493;&#20256;&#36882;&#37096;&#20998;&#26799;&#24230;&#65292;&#20174;&#32780;&#25913;&#21892;&#22312;&#35757;&#32451;&#24310;&#36831;&#32422;&#26463;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#27493;&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#21327;&#20316;&#36793;&#32536;&#23398;&#20064;&#30340;&#19968;&#31181;&#27969;&#34892;&#33539;&#24335;&#12290;&#23427;&#36890;&#24120;&#28041;&#21450;&#19968;&#32452;&#24322;&#26500;&#35774;&#22791;&#22312;&#26412;&#22320;&#24182;&#34892;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;(NN)&#27169;&#22411;&#65292;&#24182;&#23450;&#26399;&#36827;&#34892;&#38598;&#20013;&#32858;&#21512;&#12290;&#30001;&#20110;&#19968;&#20123;&#35774;&#22791;&#21487;&#33021;&#20855;&#26377;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#21487;&#21464;&#30340;&#21487;&#29992;&#24615;&#65292;FL&#30340;&#24310;&#36831;&#23545;stragglers&#38750;&#24120;&#25935;&#24863;&#12290;&#20256;&#32479;&#26041;&#27861;&#20250;&#20002;&#24323;stragglers&#25191;&#34892;&#30340;&#19981;&#23436;&#25972;&#30340;&#27169;&#22411;&#20869;&#26356;&#26032;&#65292;&#25913;&#21464;&#26412;&#22320;&#24037;&#20316;&#37327;&#21644;&#20307;&#31995;&#32467;&#26500;&#30340;&#25968;&#37327;&#65292;&#25110;&#32773;&#36716;&#32780;&#20351;&#29992;&#24322;&#27493;&#35774;&#32622;&#65307;&#25152;&#26377;&#36825;&#20123;&#37117;&#20250;&#24433;&#21709;&#22312;&#20005;&#26684;&#30340;&#35757;&#32451;&#24310;&#36831;&#32422;&#26463;&#19979;&#30340;&#35757;&#32451;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#27880;stragglers&#30340;&#22522;&#20110;&#23618;&#27425;&#30340;&#32852;&#37030;&#23398;&#20064;(SALF)&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;(NNs)&#30340;&#20248;&#21270;&#36807;&#31243;&#20197;&#23618;&#27425;&#26041;&#24335;&#26356;&#26032;&#20840;&#23616;&#27169;&#22411;&#12290;SALF&#20801;&#35768;stragglers&#21516;&#27493;&#20256;&#36882;&#37096;&#20998;&#26799;&#24230;&#65292;&#20351;&#24471;&#20840;&#23616;&#27169;&#22411;&#30340;&#27599;&#19968;&#23618;&#37117;&#21487;&#20197;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18375v1 Announce Type: new  Abstract: Synchronous federated learning (FL) is a popular paradigm for collaborative edge learning. It typically involves a set of heterogeneous devices locally training neural network (NN) models in parallel with periodic centralized aggregations. As some of the devices may have limited computational resources and varying availability, FL latency is highly sensitive to stragglers. Conventional approaches discard incomplete intra-model updates done by stragglers, alter the amount of local workload and architecture, or resort to asynchronous settings; which all affect the trained model performance under tight training latency constraints. In this work, we propose straggler-aware layer-wise federated learning (SALF) that leverages the optimization procedure of NNs via backpropagation to update the global model in a layer-wise fashion. SALF allows stragglers to synchronously convey partial gradients, having each layer of the global model be updated 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#33337;&#21482;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#25991;&#26412;&#26465;&#20214;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#24182;&#22312;&#31867;&#21035;&#24863;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#26368;&#20339;&#26041;&#24335;&#20445;&#23384;&#33337;&#21482;&#30340;&#20851;&#38190;&#32454;&#33410;&#12290;</title><link>https://arxiv.org/abs/2403.18370</link><description>&lt;p&gt;
&#35270;&#37326;&#20013;&#30340;&#33337;&#21482;&#65306;&#33337;&#21482;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Ship in Sight: Diffusion Models for Ship-Image Super Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18370
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#33337;&#21482;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#25991;&#26412;&#26465;&#20214;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#24182;&#22312;&#31867;&#21035;&#24863;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#26368;&#20339;&#26041;&#24335;&#20445;&#23384;&#33337;&#21482;&#30340;&#20851;&#38190;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20027;&#35201;&#21463;&#21040;&#19981;&#26029;&#22686;&#38271;&#30340;&#23545;&#21508;&#31181;&#22270;&#20687;&#29983;&#25104;&#23376;&#20219;&#21153;&#65288;&#22914;&#20462;&#34917;&#12289;&#21435;&#22122;&#21644;&#36229;&#20998;&#36776;&#29575;&#65289;&#39640;&#36136;&#37327;&#32467;&#26524;&#30340;&#38656;&#27714;&#39537;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28145;&#20837;&#25506;&#35752;&#20102;&#33337;&#21482;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#65292;&#36825;&#23545;&#27839;&#28023;&#21644;&#28207;&#21475;&#30417;&#35270;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#24212;&#29992;&#26426;&#20250;&#65292;&#21033;&#29992;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#23398;&#21040;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#25991;&#26412;&#26465;&#20214;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#24182;&#22312;&#31867;&#21035;&#24863;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#26368;&#20339;&#26041;&#24335;&#20445;&#23384;&#33337;&#21482;&#30340;&#20851;&#38190;&#32454;&#33410;&#22312;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18370v1 Announce Type: cross  Abstract: In recent years, remarkable advancements have been achieved in the field of image generation, primarily driven by the escalating demand for high-quality outcomes across various image generation subtasks, such as inpainting, denoising, and super resolution. A major effort is devoted to exploring the application of super-resolution techniques to enhance the quality of low-resolution images. In this context, our method explores in depth the problem of ship image super resolution, which is crucial for coastal and port surveillance. We investigate the opportunity given by the growing interest in text-to-image diffusion models, taking advantage of the prior knowledge that such foundation models have already learned. In particular, we present a diffusion-model-based architecture that leverages text conditioning during training while being class-aware, to best preserve the crucial details of the ships during the generation of the super-resolut
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#24037;&#19994;&#29289;&#32852;&#32593;&#29992;&#25143;&#35774;&#22791;&#65288;IIoT UEs&#65289;&#30340;&#24847;&#22270;&#24863;&#30693;DRL&#19978;&#34892;&#21160;&#24577;&#35843;&#24230;&#22120;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#23398;&#20064;&#22914;&#20309;&#35843;&#24230;&#36890;&#20449;&#36164;&#28304;&#65292;&#24182;&#21033;&#29992;&#22270;&#32467;&#26500;&#30340;&#31616;&#21270;&#26041;&#26696;&#21152;&#36895;&#25910;&#25947;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#35843;&#24230;&#26041;&#26696;&#65292;&#33021;&#26377;&#25928;&#20445;&#35777;IIoT UEs&#30340;&#24847;&#22270;&#34920;&#36798;&#12290;</title><link>https://arxiv.org/abs/2403.18364</link><description>&lt;p&gt;
&#38754;&#21521;5G-NR&#30340;&#24847;&#22270;&#24863;&#30693;DRL&#19978;&#34892;&#21160;&#24577;&#35843;&#24230;&#22120;
&lt;/p&gt;
&lt;p&gt;
Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18364
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#24037;&#19994;&#29289;&#32852;&#32593;&#29992;&#25143;&#35774;&#22791;&#65288;IIoT UEs&#65289;&#30340;&#24847;&#22270;&#24863;&#30693;DRL&#19978;&#34892;&#21160;&#24577;&#35843;&#24230;&#22120;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#23398;&#20064;&#22914;&#20309;&#35843;&#24230;&#36890;&#20449;&#36164;&#28304;&#65292;&#24182;&#21033;&#29992;&#22270;&#32467;&#26500;&#30340;&#31616;&#21270;&#26041;&#26696;&#21152;&#36895;&#25910;&#25947;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#35843;&#24230;&#26041;&#26696;&#65292;&#33021;&#26377;&#25928;&#20445;&#35777;IIoT UEs&#30340;&#24847;&#22270;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25903;&#25345;&#24037;&#19994;&#29289;&#32852;&#32593;&#29992;&#25143;&#35774;&#22791;&#65288;IIoT UEs&#65289;&#20855;&#26377;&#24847;&#22270;&#65288;&#21363;&#25152;&#35831;&#27714;&#30340;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#65289;&#21644;&#38543;&#26426;&#27969;&#37327;&#21040;&#36798;&#30340;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#38598;&#20013;&#21160;&#24577;&#35843;&#24230;&#22120;&#65292;&#29992;&#20110;&#23398;&#20064;&#22914;&#20309;&#22312;IIoT UEs&#20043;&#38388;&#35843;&#24230;&#21487;&#29992;&#36890;&#20449;&#36164;&#28304;&#30340;&#26102;&#38388;&#39057;&#29575;&#36164;&#28304;&#12290;&#25152;&#25552;&#20986;&#30340;&#35843;&#24230;&#22120;&#21033;&#29992;RL&#26694;&#26550;&#26469;&#36866;&#24212;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#21644;&#27969;&#37327;&#21040;&#36798;&#20013;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#31616;&#21270;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;RL&#26694;&#26550;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20960;&#31181;&#20256;&#32479;&#35843;&#24230;&#26041;&#26696;&#65288;&#22914;&#36718;&#35810;&#12289;&#21322;&#38745;&#24577;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#65289;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26234;&#33021;&#35843;&#24230;&#22120;&#22312;&#20445;&#35777;IIoT UEs&#25152;&#34920;&#36798;&#30340;&#24847;&#22270;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18364v1 Announce Type: cross  Abstract: We investigate the problem of supporting Industrial Internet of Things user equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and random traffic arrival. A deep reinforcement learning (DRL) based centralized dynamic scheduler for time-frequency resources is proposed to learn how to schedule the available communication resources among the IIoT UEs. The proposed scheduler leverages an RL framework to adapt to the dynamic changes in the wireless communication system and traffic arrivals. Moreover, a graph-based reduction scheme is proposed to reduce the state and action space of the RL framework to allow fast convergence and a better learning strategy. Simulation results demonstrate the effectiveness of the proposed intelligent scheduler in guaranteeing the expressed intent of IIoT UEs compared to several traditional scheduling schemes, such as round-robin, semi-static, and heuristic approaches. The proposed sche
&lt;/p&gt;</description></item><item><title>MKL&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#26377;&#25928;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;&#22797;&#26434;&#30340;&#30417;&#30563;&#24335;&#22810;&#32452;&#23398;&#25972;&#21512;&#26041;&#27861;&#31454;&#20105;</title><link>https://arxiv.org/abs/2403.18355</link><description>&lt;p&gt;
&#30417;&#30563;&#22810;&#26680;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#22810;&#32452;&#23398;&#25968;&#25454;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Supervised Multiple Kernel Learning approaches for multi-omics data integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18355
&lt;/p&gt;
&lt;p&gt;
MKL&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#26377;&#25928;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;&#22797;&#26434;&#30340;&#30417;&#30563;&#24335;&#22810;&#32452;&#23398;&#25972;&#21512;&#26041;&#27861;&#31454;&#20105;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36890;&#37327;&#25216;&#26415;&#30340;&#36827;&#23637;&#23548;&#33268;&#36234;&#26469;&#36234;&#22810;&#30340;&#32452;&#23398;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#22810;&#31181;&#24322;&#36136;&#25968;&#25454;&#28304;&#30340;&#38598;&#25104;&#30446;&#21069;&#26159;&#29983;&#29289;&#23398;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#39046;&#22495;&#30340;&#19968;&#20010;&#38382;&#39064;&#12290;&#22810;&#26680;&#23398;&#20064;&#65288;MKL&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#28789;&#27963;&#21644;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32771;&#34385;&#22810;&#32452;&#23398;&#36755;&#20837;&#30340;&#22810;&#26679;&#24615;&#65292;&#23613;&#31649;&#23427;&#22312;&#22522;&#22240;&#32452;&#25968;&#25454;&#25366;&#25496;&#20013;&#26159;&#19968;&#31181;&#19981;&#24120;&#29992;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#19981;&#21516;&#26680;&#34701;&#21512;&#31574;&#30053;&#30340;&#26032;&#39062;MKL&#26041;&#27861;&#12290;&#20026;&#20102;&#20174;&#36755;&#20837;&#26680;&#30340;&#20803;&#26680;&#20013;&#23398;&#20064;&#65292;&#25105;&#20204;&#23558;&#26080;&#30417;&#30563;&#38598;&#25104;&#31639;&#27861;&#35843;&#25972;&#20026;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#30417;&#30563;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#27979;&#35797;&#20102;&#29992;&#20110;&#26680;&#34701;&#21512;&#21644;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;MKL&#30340;&#27169;&#22411;&#21487;&#20197;&#19982;&#26356;&#22797;&#26434;&#12289;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#24335;&#22810;&#32452;&#23398;&#25972;&#21512;&#26041;&#27861;&#31454;&#20105;&#12290;&#22810;&#26680;&#23398;&#20064;&#20026;&#22810;&#32452;&#23398;&#22522;&#22240;&#32452;&#25968;&#25454;&#20013;&#30340;&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18355v1 Announce Type: cross  Abstract: Advances in high-throughput technologies have originated an ever-increasing availability of omics datasets. The integration of multiple heterogeneous data sources is currently an issue for biology and bioinformatics. Multiple kernel learning (MKL) has shown to be a flexible and valid approach to consider the diverse nature of multi-omics inputs, despite being an underused tool in genomic data mining.We provide novel MKL approaches based on different kernel fusion strategies.To learn from the meta-kernel of input kernels, we adaptedunsupervised integration algorithms for supervised tasks with support vector machines.We also tested deep learning architectures for kernel fusion and classification.The results show that MKL-based models can compete with more complex, state-of-the-art, supervised multi-omics integrative approaches. Multiple kernel learning offers a natural framework for predictive models in multi-omics genomic data. Our resu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#20892;&#19994;&#22330;&#26223;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;&#65292;&#33021;&#22815;&#27169;&#25311;&#26893;&#29289;&#30340;&#29983;&#38271;&#38454;&#27573;&#12289;&#22303;&#22756;&#26465;&#20214;&#21644;&#20809;&#29031;&#21464;&#21270;&#65292;&#20026;&#31934;&#20934;&#20892;&#19994;&#20013;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#25552;&#20379;&#20102;&#20840;&#38754;&#36164;&#28304;&#65292;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#22312;&#25552;&#20379;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;</title><link>https://arxiv.org/abs/2403.18351</link><description>&lt;p&gt;
&#29983;&#25104;&#29992;&#20110;&#22522;&#20110;&#35270;&#35273;&#30340;&#20892;&#19994;&#24212;&#29992;&#30340;&#22810;&#26679;&#21270;&#20892;&#19994;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating Diverse Agricultural Data for Vision-Based Farming Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18351
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#20892;&#19994;&#22330;&#26223;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;&#65292;&#33021;&#22815;&#27169;&#25311;&#26893;&#29289;&#30340;&#29983;&#38271;&#38454;&#27573;&#12289;&#22303;&#22756;&#26465;&#20214;&#21644;&#20809;&#29031;&#21464;&#21270;&#65292;&#20026;&#31934;&#20934;&#20892;&#19994;&#20013;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#25552;&#20379;&#20102;&#20840;&#38754;&#36164;&#28304;&#65292;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#22312;&#25552;&#20379;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#30340;&#20892;&#19994;&#22330;&#26223;&#65292;&#37325;&#28857;&#26159;&#22823;&#35910;&#20316;&#29289;&#20197;&#21450;&#21508;&#31181;&#26434;&#33609;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#36825;&#20123;&#26893;&#29289;&#30340;&#19981;&#21516;&#29983;&#38271;&#38454;&#27573;&#12289;&#22810;&#26679;&#21270;&#30340;&#22303;&#22756;&#26465;&#20214;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#19979;&#30340;&#38543;&#26426;&#30000;&#22320;&#24067;&#23616;&#12290;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#32441;&#29702;&#21644;&#29615;&#22659;&#22240;&#32032;&#25972;&#21512;&#21040;&#31243;&#24207;&#21270;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#22686;&#24378;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#36924;&#30495;&#24230;&#21644;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#24102;&#26377;&#35821;&#20041;&#26631;&#31614;&#30340;12,000&#24352;&#22270;&#20687;&#65292;&#20026;&#31934;&#20934;&#20892;&#19994;&#20013;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#36164;&#28304;&#65292;&#22914;&#29992;&#20110;&#33258;&#20027;&#38500;&#33609;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#36890;&#36807;&#23558;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#20892;&#19994;&#22270;&#20687;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#20026;&#20892;&#19994;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;-effective&#30340;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18351v1 Announce Type: cross  Abstract: We present a specialized procedural model for generating synthetic agricultural scenes, focusing on soybean crops, along with various weeds. This model is capable of simulating distinct growth stages of these plants, diverse soil conditions, and randomized field arrangements under varying lighting conditions. The integration of real-world textures and environmental factors into the procedural generation process enhances the photorealism and applicability of the synthetic data. Our dataset includes 12,000 images with semantic labels, offering a comprehensive resource for computer vision tasks in precision agriculture, such as semantic segmentation for autonomous weed control. We validate our model's effectiveness by comparing the synthetic data against real agricultural images, demonstrating its potential to significantly augment training data for machine learning models in agriculture. This approach not only provides a cost-effective s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#24555;&#36895;&#27169;&#31946;c-&#22343;&#20540;&#25216;&#26415;&#65292;&#29992;&#20110;&#24555;&#36895;&#26816;&#27979;&#22826;&#38451;&#26085;&#20885;&#31354;&#27934;&#65288;CHs&#65289;&#30340;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.18347</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#27169;&#31946;&#30340;&#26041;&#27861;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#22826;&#38451;&#26085;&#20885;&#31354;&#27934;
&lt;/p&gt;
&lt;p&gt;
A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal Holes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#24555;&#36895;&#27169;&#31946;c-&#22343;&#20540;&#25216;&#26415;&#65292;&#29992;&#20110;&#24555;&#36895;&#26816;&#27979;&#22826;&#38451;&#26085;&#20885;&#31354;&#27934;&#65288;CHs&#65289;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#38451;&#26085;&#20885;&#31354;&#27934;&#65288;CHs&#65289;&#30340;&#26816;&#27979;&#21644;&#20998;&#26512;&#26159;&#22826;&#38451;&#29289;&#29702;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#22320;&#30913;&#39118;&#26292;&#65292;&#36825;&#23545;&#21508;&#31181;&#31354;&#38388;&#21644;&#22320;&#38754;&#31995;&#32479;&#37117;&#20135;&#29983;&#30452;&#25509;&#25110;&#38388;&#25509;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22826;&#38451;&#31185;&#23398;&#23478;&#20381;&#36182;&#25163;&#21160;&#32472;&#21046;&#26041;&#27861;&#26469;&#26816;&#27979;CHs&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#19968;&#20123;&#33258;&#21160;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#24050;&#34987;&#29992;&#20110;&#26816;&#27979;CHs&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#24555;&#36895;&#20934;&#30830;&#26816;&#27979;CHs&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#24555;&#36895;&#27169;&#31946;c-&#22343;&#20540;&#25216;&#26415;&#65292;&#29992;&#20110;&#24555;&#36895;&#26816;&#27979;CHs&#21306;&#22495;&#12290;&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#36827;&#34892;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#24555;&#36895;&#27169;&#31946;c-&#22343;&#20540;&#65288;QCFFCM&#65289;&#23545;&#22826;&#38451;&#22270;&#20687;&#36827;&#34892;&#20102;&#20998;&#21106;&#65292;&#28982;&#21518;&#22312;&#21518;&#32493;&#38454;&#27573;&#20174;&#20013;&#25552;&#21462;&#20986;CHs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18347v1 Announce Type: cross  Abstract: The detection and analysis of the solar coronal holes (CHs) is an important field of study in the domain of solar physics. Mainly, it is required for the proper prediction of the geomagnetic storms which directly or indirectly affect various space and ground-based systems. For the detection of CHs till date, the solar scientist depends on manual hand-drawn approaches. However, with the advancement of image processing technologies, some automated image segmentation methods have been used for the detection of CHs. In-spite of this, fast and accurate detection of CHs are till a major issues. Here in this work, a novel quantum computing-based fast fuzzy c-mean technique has been developed for fast detection of the CHs region. The task has been carried out in two stages, in first stage the solar image has been segmented using a quantum computing based fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted out from the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20154;&#24037;&#31070;&#32463;&#23402;&#29983;&#20307;&#65292;&#32467;&#21512;&#20102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#25968;&#25454;&#34701;&#21512;&#21644;&#21453;&#21521;&#20256;&#25773;&#25439;&#22833;&#26799;&#24230;&#26469;&#23454;&#29616;&#20998;&#24067;&#24335;&#36807;&#31243;&#38142;&#20013;&#30340;&#27969;&#31243;&#20248;&#21270;&#21644;&#25345;&#32493;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.18343</link><description>&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#23402;&#29983;&#20307;--&#20998;&#24067;&#24335;&#36807;&#31243;&#38142;&#20013;&#30340;&#27969;&#31243;&#20248;&#21270;&#21644;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
The Artificial Neural Twin -- Process Optimization and Continual Learning in Distributed Process Chains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18343
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20154;&#24037;&#31070;&#32463;&#23402;&#29983;&#20307;&#65292;&#32467;&#21512;&#20102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#25968;&#25454;&#34701;&#21512;&#21644;&#21453;&#21521;&#20256;&#25773;&#25439;&#22833;&#26799;&#24230;&#26469;&#23454;&#29616;&#20998;&#24067;&#24335;&#36807;&#31243;&#38142;&#20013;&#30340;&#27969;&#31243;&#20248;&#21270;&#21644;&#25345;&#32493;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#36807;&#31243;&#20248;&#21270;&#21644;&#25511;&#21046;&#23545;&#20110;&#25552;&#39640;&#32463;&#27982;&#21644;&#29983;&#24577;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#20027;&#26435;&#12289;&#19981;&#21516;&#30340;&#30446;&#26631;&#65292;&#25110;&#32773;&#23454;&#26045;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#38459;&#30861;&#20102;&#25972;&#20307;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#30340;AI&#26041;&#27861;&#22312;&#36807;&#31243;&#27169;&#22411;&#21644;&#24037;&#19994;&#20256;&#24863;&#22120;&#20013;&#32463;&#24120;&#38656;&#35201;&#23450;&#26399;&#24494;&#35843;&#20197;&#36866;&#24212;&#20998;&#24067;&#28418;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20154;&#24037;&#31070;&#32463;&#23402;&#29983;&#20307;&#65292;&#23427;&#32467;&#21512;&#20102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#21487;&#24494;&#20998;&#25968;&#25454;&#34701;&#21512;&#26469;&#20272;&#35745;&#20998;&#24067;&#24335;&#36807;&#31243;&#27493;&#39588;&#30340;&#29366;&#24577;&#21450;&#20854;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#36890;&#36807;&#23558;&#30456;&#20114;&#36830;&#25509;&#30340;&#36807;&#31243;&#27493;&#39588;&#35270;&#20026;&#20934;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#21487;&#20197;&#21453;&#21521;&#20256;&#25773;&#25439;&#22833;&#26799;&#24230;&#65292;&#29992;&#20110;&#27969;&#31243;&#20248;&#21270;&#25110;&#27169;&#22411;&#24494;&#35843;&#21040;&#36807;&#31243;&#21442;&#25968;&#25110;AI&#27169;&#22411;&#20013;&#12290;&#35813;&#27010;&#24565;&#22312;&#34394;&#25311;&#26426;&#32452;&#27169;&#25311;&#20013;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18343v1 Announce Type: new  Abstract: Industrial process optimization and control is crucial to increase economic and ecologic efficiency. However, data sovereignty, differing goals, or the required expert knowledge for implementation impede holistic implementation. Further, the increasing use of data-driven AI-methods in process models and industrial sensory often requires regular fine-tuning to accommodate distribution drifts. We propose the Artificial Neural Twin, which combines concepts from model predictive control, deep learning, and sensor networks to address these issues. Our approach introduces differentiable data fusion to estimate the state of distributed process steps and their dependence on input data. By treating the interconnected process steps as a quasi neural-network, we can backpropagate loss gradients for process optimization or model fine-tuning to process parameters or AI models respectively. The concept is demonstrated on a virtual machine park simulat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#32771;&#34385;&#32467;&#26500;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#29992;&#20110;&#23439;&#35266;&#35010;&#32441;&#34920;&#38754;&#20998;&#21106;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18337</link><description>&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#32467;&#26500;&#30456;&#20284;&#24615;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#23439;&#35266;&#35010;&#32441;&#34920;&#38754;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Macroscale fracture surface segmentation via semi-supervised learning considering the structural similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18337
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#32771;&#34385;&#32467;&#26500;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#29992;&#20110;&#23439;&#35266;&#35010;&#32441;&#34920;&#38754;&#20998;&#21106;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25130;&#33267;&#30446;&#21069;&#65292;&#29992;&#20110;&#26680;&#33021;&#39046;&#22495;&#31561;&#39046;&#22495;&#30340;&#26448;&#26009;&#30340;&#23433;&#20840;&#35780;&#20272;&#36890;&#24120;&#20381;&#36182;&#20110;&#21033;&#29992;&#23439;&#35266;&#27010;&#24565;&#30340;&#26029;&#35010;&#21147;&#23398;&#20998;&#26512;&#65292;&#20854;&#20013;&#20840;&#23616;&#36733;&#33655;&#37327;K&#25110;J&#19982;&#26448;&#26009;&#30340;&#26029;&#35010;&#38887;&#24615;&#26354;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;&#26412;&#30740;&#31350;&#33539;&#22260;&#20869;&#24314;&#31435;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#23439;&#35266;&#32423;&#21035;&#35010;&#32441;&#34920;&#38754;&#20998;&#21106;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#21019;&#24314;&#20102;&#19977;&#20010;&#19981;&#21516;&#19988;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#26469;&#20998;&#26512;&#32467;&#26500;&#30456;&#20284;&#24615;&#23545;&#20998;&#21106;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#35780;&#20272;&#26448;&#26009;&#21644;&#35797;&#26679;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#19981;&#21516;&#65292;&#20197;&#21450;&#19981;&#21516;&#23454;&#39564;&#23460;&#22270;&#20687;&#33719;&#21462;&#20013;&#30340;&#22270;&#20687;&#24341;&#36215;&#30340;&#27874;&#21160;&#65292;&#32467;&#26500;&#30456;&#20284;&#24615;&#21508;&#19981;&#30456;&#21516;&#12290;&#25968;&#25454;&#38598;&#23545;&#24212;&#20856;&#22411;&#30340;&#29420;&#31435;&#23454;&#39564;&#23460;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18337v1 Announce Type: new  Abstract: To this date the safety assessment of materials, used for example in the nuclear power sector, commonly relies on a fracture mechanical analysis utilizing macroscopic concepts, where a global load quantity K or J is compared to the materials fracture toughness curve. Part of the experimental effort involved in these concepts is dedicated to the quantitative analysis of fracture surfaces. Within the scope of this study a methodology for the semi-supervised training of deep learning models for fracture surface segmentation on a macroscopic level was established. Therefore, three distinct and unique datasets were created to analyze the influence of structural similarity on the segmentation capability. The structural similarity differs due to the assessed materials and specimen, as well as imaging-induced variance due to fluctuations in image acquisition in different laboratories. The datasets correspond to typical isolated laboratory condit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#33647;&#29289;&#19981;&#33391;&#21453;&#24212;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#26085;&#35821;&#25991;&#26412;&#65292;&#21253;&#21547;&#20102;12&#31181;&#23454;&#20307;&#31867;&#22411;&#12289;&#22235;&#31181;&#23646;&#24615;&#31867;&#22411;&#21644;13&#31181;&#20851;&#31995;&#31867;&#22411;&#30340;&#27880;&#37322;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24320;&#21457;&#29616;&#23454;&#19990;&#30028;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.18336</link><description>&lt;p&gt;
&#38754;&#21521;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#26085;&#35821;&#30340;&#33647;&#29289;&#35686;&#25106;&#25968;&#25454;&#38598;&#65306;&#36328;&#35821;&#35328;&#26631;&#27880;&#33647;&#29289;&#19981;&#33391;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
A Dataset for Pharmacovigilance in German, French, and Japanese: Annotating Adverse Drug Reactions across Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#33647;&#29289;&#19981;&#33391;&#21453;&#24212;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#26085;&#35821;&#25991;&#26412;&#65292;&#21253;&#21547;&#20102;12&#31181;&#23454;&#20307;&#31867;&#22411;&#12289;&#22235;&#31181;&#23646;&#24615;&#31867;&#22411;&#21644;13&#31181;&#20851;&#31995;&#31867;&#22411;&#30340;&#27880;&#37322;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24320;&#21457;&#29616;&#23454;&#19990;&#30028;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#28304;&#22312;&#25581;&#31034;&#33647;&#29289;&#19981;&#33391;&#21453;&#24212;&#65288;ADRs&#65289;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#35752;&#35770;&#25968;&#37327;&#20063;&#22312;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20020;&#24202;&#35821;&#26009;&#24211;&#20027;&#35201;&#22260;&#32469;&#33521;&#25991;&#31185;&#23398;&#25991;&#31456;&#23637;&#24320;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#26085;&#35821;&#20013;&#20851;&#20110;ADR&#30340;&#25991;&#26412;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#26469;&#28304;&#21253;&#25324;&#24739;&#32773;&#35770;&#22363;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#20020;&#24202;&#25253;&#21578;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#21253;&#21547;&#20102;12&#31181;&#23454;&#20307;&#31867;&#22411;&#12289;&#22235;&#31181;&#23646;&#24615;&#31867;&#22411;&#21644;13&#31181;&#20851;&#31995;&#31867;&#22411;&#30340;&#27880;&#37322;&#12290;&#36825;&#26377;&#21161;&#20110;&#20026;&#21307;&#30103;&#20445;&#20581;&#24320;&#21457;&#29616;&#23454;&#19990;&#30028;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#32479;&#35745;&#25968;&#25454;&#20197;&#20984;&#26174;&#19982;&#35821;&#26009;&#24211;&#30456;&#20851;&#30340;&#26576;&#20123;&#25361;&#25112;&#65292;&#24182;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#65292;&#20026;&#22312;&#21333;&#19968;&#35821;&#35328;&#20869;&#37096;&#21644;&#36328;&#35821;&#35328;&#20043;&#38388;&#25552;&#21462;&#23454;&#20307;&#21644;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18336v1 Announce Type: new  Abstract: User-generated data sources have gained significance in uncovering Adverse Drug Reactions (ADRs), with an increasing number of discussions occurring in the digital world. However, the existing clinical corpora predominantly revolve around scientific articles in English. This work presents a multilingual corpus of texts concerning ADRs gathered from diverse sources, including patient fora, social media, and clinical reports in German, French, and Japanese. Our corpus contains annotations covering 12 entity types, four attribute types, and 13 relation types. It contributes to the development of real-world multilingual language models for healthcare. We provide statistics to highlight certain challenges associated with the corpus and conduct preliminary experiments resulting in strong baselines for extracting entities and relations between these entities, both within and across languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#36319;&#36394;&#31574;&#30053;&#21644;&#33258;&#21160;&#26631;&#35760;&#31639;&#27861;&#65292;&#38024;&#23545;&#20107;&#20214;&#30456;&#26426;&#20013;&#30340;&#38590;&#20197;&#36776;&#35782;&#30340;&#23545;&#35937;&#65292;&#25581;&#31034;&#20854;&#29305;&#24449;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.18330</link><description>&lt;p&gt;
&#20351;&#29992;&#36319;&#36394;&#36741;&#21161;&#30340;&#20107;&#20214;&#30456;&#26426;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Tracking-Assisted Object Detection with Event Cameras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#36319;&#36394;&#31574;&#30053;&#21644;&#33258;&#21160;&#26631;&#35760;&#31639;&#27861;&#65292;&#38024;&#23545;&#20107;&#20214;&#30456;&#26426;&#20013;&#30340;&#38590;&#20197;&#36776;&#35782;&#30340;&#23545;&#35937;&#65292;&#25581;&#31034;&#20854;&#29305;&#24449;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20107;&#20214;&#30456;&#26426;&#20855;&#26377;&#39640;&#21160;&#24577;&#33539;&#22260;&#21644;&#26080;&#21160;&#24577;&#27169;&#31946;&#31561;&#29305;&#27530;&#23646;&#24615;&#65292;&#20107;&#20214;&#39537;&#21160;&#30446;&#26631;&#26816;&#27979;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29305;&#24449;&#30340;&#19981;&#21516;&#27493;&#24615;&#21644;&#31232;&#30095;&#24615;&#23548;&#33268;&#20102;&#30001;&#20110;&#30456;&#26426;&#19982;&#20043;&#27809;&#26377;&#30456;&#23545;&#36816;&#21160;&#32780;&#23548;&#33268;&#30340;&#30475;&#19981;&#35265;&#30340;&#23545;&#35937;&#65292;&#36825;&#23545;&#20219;&#21153;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#30475;&#19981;&#35265;&#30340;&#23545;&#35937;&#35270;&#20026;&#20266;&#36974;&#25377;&#23545;&#35937;&#65292;&#24182;&#26088;&#22312;&#25581;&#31034;&#20854;&#29305;&#24449;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#35937;&#30340;&#21487;&#35265;&#24615;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#26631;&#35760;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#29616;&#26377;&#30340;&#20107;&#20214;&#30456;&#26426;&#25968;&#25454;&#38598;&#19978;&#38468;&#21152;&#39069;&#22806;&#30340;&#21487;&#35265;&#24615;&#26631;&#31614;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#36319;&#36394;&#31574;&#30053;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18330v1 Announce Type: cross  Abstract: Event-based object detection has recently garnered attention in the computer vision community due to the exceptional properties of event cameras, such as high dynamic range and no motion blur. However, feature asynchronism and sparsity cause invisible objects due to no relative motion to the camera, posing a significant challenge in the task. Prior works have studied various memory mechanisms to preserve as many features as possible at the current time, guided by temporal clues. While these implicit-learned memories retain some short-term information, they still struggle to preserve long-term features effectively. In this paper, we consider those invisible objects as pseudo-occluded objects and aim to reveal their features. Firstly, we introduce visibility attribute of objects and contribute an auto-labeling algorithm to append additional visibility labels on an existing event camera dataset. Secondly, we exploit tracking strategies fo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20840;&#20998;&#24067;&#24335;NMF&#65292;&#36890;&#36807;Paillier&#23494;&#30721;&#31995;&#32479;&#20445;&#25252;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#36991;&#20813;&#20102;&#21407;&#22987;&#25968;&#25454;&#26292;&#38706;&#12290;</title><link>https://arxiv.org/abs/2403.18326</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Distributed Nonnegative Matrix Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18326
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20840;&#20998;&#24067;&#24335;NMF&#65292;&#36890;&#36807;Paillier&#23494;&#30721;&#31995;&#32479;&#20445;&#25252;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#36991;&#20813;&#20102;&#21407;&#22987;&#25968;&#25454;&#26292;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#34920;&#31034;&#24037;&#20855;&#65292;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#39046;&#22495;&#26377;&#30528;&#20247;&#22810;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#32452;&#32593;&#32476;&#20013;&#20998;&#24067;&#24335;&#37096;&#32626;NMF&#20250;&#24341;&#20837;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#20256;&#32479;&#26041;&#27861;&#28041;&#21450;&#22312;&#32593;&#32476;&#20195;&#29702;&#20043;&#38388;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#31639;&#27861;&#65292;&#29992;&#20110;&#20840;&#20998;&#24067;&#24335;NMF&#65292;&#23558;&#20998;&#24067;&#24335;&#22823;&#25968;&#25454;&#30697;&#38453;&#20998;&#35299;&#20026;&#24038;&#21491;&#30697;&#38453;&#22240;&#23376;&#65292;&#21516;&#26102;&#20445;&#25252;&#27599;&#20010;&#20195;&#29702;&#30340;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#12290;&#23427;&#20419;&#36827;&#20102;&#20195;&#29702;&#20043;&#38388;&#24038;&#30697;&#38453;&#22240;&#23376;&#30340;&#21327;&#20316;&#20272;&#35745;&#65292;&#24182;&#20351;&#23427;&#20204;&#33021;&#22815;&#20272;&#35745;&#21508;&#33258;&#30340;&#21491;&#22240;&#23376;&#32780;&#19981;&#26292;&#38706;&#21407;&#22987;&#25968;&#25454;&#12290;&#20026;&#20102;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#65292;&#25105;&#20204;&#21033;&#29992;Paillier&#23494;&#30721;&#31995;&#32479;&#22312;&#30456;&#37051;&#20195;&#29702;&#20043;&#38388;&#23433;&#20840;&#22320;&#36827;&#34892;&#20449;&#24687;&#20132;&#25442;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20844;&#38053;&#21152;&#23494;&#30340;&#27010;&#29575;&#24615;&#38750;&#23545;&#31216;&#31639;&#27861;&#65292;&#20801;&#35768;&#23545;&#21152;&#23494;&#25968;&#25454;&#36827;&#34892;&#35745;&#31639;&#32780;&#19981;&#35299;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18326v1 Announce Type: cross  Abstract: Nonnegative matrix factorization (NMF) is an effective data representation tool with numerous applications in signal processing and machine learning. However, deploying NMF in a decentralized manner over ad-hoc networks introduces privacy concerns due to the conventional approach of sharing raw data among network agents. To address this, we propose a privacy-preserving algorithm for fully-distributed NMF that decomposes a distributed large data matrix into left and right matrix factors while safeguarding each agent's local data privacy. It facilitates collaborative estimation of the left matrix factor among agents and enables them to estimate their respective right factors without exposing raw data. To ensure data privacy, we secure information exchanges between neighboring agents utilizing the Paillier cryptosystem, a probabilistic asymmetric algorithm for public-key cryptography that allows computations on encrypted data without decr
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#31639;&#27861;&#22312;&#37329;&#34701;&#29359;&#32618;&#39044;&#38450;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#65292;&#24182;&#25552;&#20379;&#25903;&#25345;&#25913;&#36827;&#30340;&#37329;&#34701;&#39118;&#38505;&#31649;&#29702;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.18322</link><description>&lt;p&gt;
&#37327;&#23376;&#31639;&#27861;&#65306;&#37329;&#34701;&#29359;&#32618;&#39044;&#38450;&#30340;&#26032;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Quantum Algorithms: A New Frontier in Financial Crime Prevention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18322
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31639;&#27861;&#22312;&#37329;&#34701;&#29359;&#32618;&#39044;&#38450;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#65292;&#24182;&#25552;&#20379;&#25903;&#25345;&#25913;&#36827;&#30340;&#37329;&#34701;&#39118;&#38505;&#31649;&#29702;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#29359;&#32618;&#36805;&#36895;&#34067;&#24310;&#19988;&#24840;&#21457;&#22797;&#26434;&#65292;&#38656;&#35201;&#25552;&#20379;&#31283;&#20581;&#26377;&#25928;&#30340;&#26032;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#37327;&#23376;&#31639;&#27861;&#22312;&#25171;&#20987;&#37329;&#34701;&#29359;&#32618;&#20013;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#27604;&#36739;&#20256;&#32479;&#26041;&#27861;&#12289;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#37327;&#23376;&#26041;&#27861;&#65292;&#31361;&#20986;&#20102;&#37327;&#23376;&#35745;&#31639;&#30340;&#20248;&#21183;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#21644;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#65288;QAI&#65289;&#31561;&#20808;&#36827;&#26041;&#27861;&#20316;&#20026;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#39044;&#38450;&#37329;&#34701;&#29359;&#32618;&#65292;&#21253;&#25324;&#27927;&#38065;&#12289;&#37329;&#34701;&#29359;&#32618;&#26816;&#27979;&#12289;&#21152;&#23494;&#36135;&#24065;&#25915;&#20987;&#21644;&#24066;&#22330;&#25805;&#32437;&#12290;&#36825;&#20123;&#37327;&#23376;&#26041;&#27861;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#22266;&#26377;&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#20811;&#26381;&#21463;&#20256;&#32479;&#26041;&#27861;&#21046;&#32422;&#30340;&#23616;&#38480;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#38416;&#26126;&#20102;&#37327;&#23376;&#35745;&#31639;&#22914;&#20309;&#25903;&#25345;&#25913;&#36827;&#30340;&#37329;&#34701;&#39118;&#38505;&#31649;&#29702;&#20998;&#26512;&#12290;&#37329;&#34701;&#26426;&#26500;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18322v1 Announce Type: new  Abstract: Financial crimes fast proliferation and sophistication require novel approaches that provide robust and effective solutions. This paper explores the potential of quantum algorithms in combating financial crimes. It highlights the advantages of quantum computing by examining traditional and Machine Learning (ML) techniques alongside quantum approaches. The study showcases advanced methodologies such as Quantum Machine Learning (QML) and Quantum Artificial Intelligence (QAI) as powerful solutions for detecting and preventing financial crimes, including money laundering, financial crime detection, cryptocurrency attacks, and market manipulation. These quantum approaches leverage the inherent computational capabilities of quantum computers to overcome limitations faced by classical methods. Furthermore, the paper illustrates how quantum computing can support enhanced financial risk management analysis. Financial institutions can improve thei
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23454;&#29616;&#20102;&#20027;&#25104;&#20998;&#20998;&#26512;&#31639;&#27861;&#22312;NVIDIA GPU&#21644;Kalray&#22810;&#26680;&#22788;&#29702;&#22120;&#19978;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#39640;&#24615;&#33021;&#35745;&#31639;&#24179;&#21488;&#24182;&#34892;&#24615;&#30340;&#25216;&#24039;&#65292;&#20197;&#20943;&#23569;&#22788;&#29702;&#39640;&#20809;&#35889;&#22270;&#20687;&#25152;&#38656;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.18321</link><description>&lt;p&gt;
&#23558;&#20027;&#25104;&#20998;&#20998;&#26512;&#24212;&#29992;&#20110;&#39640;&#24615;&#33021;&#35745;&#31639;&#35774;&#22791;&#36827;&#34892;&#39640;&#20809;&#35889;&#38477;&#32500;&#65306;&#32467;&#26524;&#19982;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Implementation of the Principal Component Analysis onto High-Performance Computer Facilities for Hyperspectral Dimensionality Reduction: Results and Comparisons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18321
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23454;&#29616;&#20102;&#20027;&#25104;&#20998;&#20998;&#26512;&#31639;&#27861;&#22312;NVIDIA GPU&#21644;Kalray&#22810;&#26680;&#22788;&#29702;&#22120;&#19978;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#39640;&#24615;&#33021;&#35745;&#31639;&#24179;&#21488;&#24182;&#34892;&#24615;&#30340;&#25216;&#24039;&#65292;&#20197;&#20943;&#23569;&#22788;&#29702;&#39640;&#20809;&#35889;&#22270;&#20687;&#25152;&#38656;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20809;&#35889;&#25104;&#20687;&#31639;&#27861;&#30340;&#39640;&#25928;&#24615;&#33021;&#20851;&#38190;&#22312;&#20110;&#38477;&#32500;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#20043;&#31867;&#30340;&#38477;&#32500;&#31639;&#27861;&#30001;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#29305;&#24615;&#65292;&#24314;&#35758;&#23558;&#20854;&#24212;&#29992;&#20110;&#39640;&#24615;&#33021;&#35745;&#31639;&#26550;&#26500;&#29992;&#20110;&#24212;&#29992;&#22312;&#20005;&#26684;&#30340;&#24310;&#36831;&#38480;&#21046;&#19979;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;PCA&#31639;&#27861;&#22312;&#20004;&#31181;&#19981;&#21516;&#39640;&#24615;&#33021;&#35774;&#22791;&#19978;&#30340;&#23454;&#29616;&#65292;&#20998;&#21035;&#20026;NVIDIA&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#65288;GPU&#65289;&#21644;Kalray&#22810;&#26680;&#22788;&#29702;&#22120;&#65292;&#25581;&#31034;&#20102;&#19968;&#31995;&#21015;&#26377;&#20215;&#20540;&#30340;&#25216;&#24039;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#39640;&#24615;&#33021;&#35745;&#31639;&#24179;&#21488;&#22266;&#26377;&#30340;&#24182;&#34892;&#24615;&#65292;&#20174;&#32780;&#20943;&#23569;&#22788;&#29702;&#32473;&#23450;&#39640;&#20809;&#35889;&#22270;&#20687;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18321v1 Announce Type: new  Abstract: Dimensionality reduction represents a critical preprocessing step in order to increase the efficiency and the performance of many hyperspectral imaging algorithms. However, dimensionality reduction algorithms, such as the Principal Component Analysis (PCA), suffer from their computationally demanding nature, becoming advisable for their implementation onto high-performance computer architectures for applications under strict latency constraints. This work presents the implementation of the PCA algorithm onto two different high-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and a Kalray manycore, uncovering a highly valuable set of tips and tricks in order to take full advantage of the inherent parallelism of these high-performance computing platforms, and hence, reducing the time that is required to process a given hyperspectral image. Moreover, the achieved results obtained with different hyperspectral images have
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#22788;&#29702;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#37051;&#22495;&#23545;&#27604;&#25439;&#22833;&#65288;MM-NCL&#65289;&#20989;&#25968;&#65292;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#32447;&#24615;&#25506;&#27979;&#21644;&#38646;-shot&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18316</link><description>&lt;p&gt;
&#29992;&#20110;&#22312;&#32447;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Contrastive Learning for Online Clinical Time-Series Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18316
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#22788;&#29702;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#37051;&#22495;&#23545;&#27604;&#25439;&#22833;&#65288;MM-NCL&#65289;&#20989;&#25968;&#65292;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#32447;&#24615;&#25506;&#27979;&#21644;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26469;&#33258;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#31181;&#25968;&#25454;&#24418;&#24335;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#30417;&#30563;&#35774;&#32622;&#20013;&#25104;&#21151;&#21033;&#29992;&#20102;&#22810;&#31181;&#24418;&#24335;&#65292;&#25105;&#20204;&#23558;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;ICU&#25968;&#25454;&#65292;&#29305;&#21035;&#20851;&#27880;&#20020;&#24202;&#31508;&#35760;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#29992;&#20110;&#20020;&#24202;&#30456;&#20851;&#30340;&#22312;&#32447;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#25439;&#22833;&#20989;&#25968;&#22810;&#27169;&#24577;&#37051;&#22495;&#23545;&#27604;&#25439;&#22833;&#65288;MM-NCL&#65289;&#65292;&#19968;&#20010;&#36719;&#37051;&#22495;&#20989;&#25968;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20986;&#33394;&#32447;&#24615;&#25506;&#27979;&#21644;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18316v1 Announce Type: new  Abstract: Electronic Health Record (EHR) datasets from Intensive Care Units (ICU) contain a diverse set of data modalities. While prior works have successfully leveraged multiple modalities in supervised settings, we apply advanced self-supervised multi-modal contrastive learning techniques to ICU data, specifically focusing on clinical notes and time-series for clinically relevant online prediction tasks. We introduce a loss function Multi-Modal Neighborhood Contrastive Loss (MM-NCL), a soft neighborhood function, and showcase the excellent linear probe and zero-shot performance of our approach.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#30340;&#28909;&#21147;&#23398;&#19968;&#33268;&#26448;&#26009;&#27169;&#22411;&#65292;&#29992;&#20110;&#30740;&#31350;&#30701;&#32420;&#32500;/&#32858;&#21512;&#29289;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#30340;&#34892;&#20026;&#65292;&#36890;&#36807;&#32452;&#21512;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#39044;&#27979;&#20869;&#37096;&#21464;&#37327;&#24182;&#23450;&#20041;&#25972;&#20010;&#31995;&#32479;&#30340;&#28909;&#21147;&#23398;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.18310</link><description>&lt;p&gt;
&#19968;&#31181;&#28909;&#21147;&#23398;&#19968;&#33268;&#30340;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#26448;&#26009;&#27169;&#22411;&#65292;&#29992;&#20110;&#30701;&#32420;&#32500;/&#32858;&#21512;&#29289;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;
&lt;/p&gt;
&lt;p&gt;
A thermodynamically consistent physics-informed deep learning material model for short fiber/polymer nanocomposites
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18310
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#30340;&#28909;&#21147;&#23398;&#19968;&#33268;&#26448;&#26009;&#27169;&#22411;&#65292;&#29992;&#20110;&#30740;&#31350;&#30701;&#32420;&#32500;/&#32858;&#21512;&#29289;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#30340;&#34892;&#20026;&#65292;&#36890;&#36807;&#32452;&#21512;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#39044;&#27979;&#20869;&#37096;&#21464;&#37327;&#24182;&#23450;&#20041;&#25972;&#20010;&#31995;&#32479;&#30340;&#28909;&#21147;&#23398;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#65288;PIDL&#65289;&#30340;&#26412;&#26500;&#27169;&#22411;&#65292;&#29992;&#20110;&#30740;&#31350;&#22312;&#21508;&#31181;&#29615;&#22659;&#26465;&#20214;&#19979;&#30701;&#32420;&#32500;&#22686;&#24378;&#32435;&#31859;&#31890;&#23376;&#22635;&#20805;&#29615;&#27687;&#26641;&#33026;&#30340;&#31896;&#24377;-&#31896;&#22609;&#34892;&#20026;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#20197;&#24378;&#21046;&#25191;&#34892;&#28909;&#21147;&#23398;&#21407;&#29702;&#65292;&#20174;&#32780;&#24471;&#21040;&#28909;&#21147;&#23398;&#19968;&#33268;&#30340;&#26412;&#26500;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#23558;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#19982;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#39044;&#27979;&#34920;&#24449;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#20869;&#37096;&#32791;&#25955;&#25152;&#38656;&#30340;&#20869;&#37096;&#21464;&#37327;&#12290;&#27492;&#22806;&#65292;&#21478;&#19968;&#20010;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25351;&#31034;&#33258;&#30001;&#33021;&#20989;&#25968;&#65292;&#20174;&#32780;&#23450;&#20041;&#25972;&#20010;&#31995;&#32479;&#30340;&#28909;&#21147;&#23398;&#29366;&#24577;&#12290;PIDL&#27169;&#22411;&#26368;&#21021;&#38024;&#23545;&#19977;&#32500;&#24773;&#20917;&#36827;&#34892;&#24320;&#21457;&#65292;&#36890;&#36807;&#20174;&#32463;&#20856;&#26412;&#26500;&#27169;&#22411;&#20013;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#30452;&#25509;&#25552;&#21462;&#24490;&#29615;&#21152;&#36733;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18310v1 Announce Type: cross  Abstract: This work proposes a physics-informed deep learning (PIDL)-based constitutive model for investigating the viscoelastic-viscoplastic behavior of short fiber-reinforced nanoparticle-filled epoxies under various ambient conditions. The deep-learning model is trained to enforce thermodynamic principles, leading to a thermodynamically consistent constitutive model. To accomplish this, a long short-term memory network is combined with a feed-forward neural network to predict internal variables required for characterizing the internal dissipation of the nanocomposite materials. In addition, another feed-forward neural network is used to indicate the free-energy function, which enables defining the thermodynamic state of the entire system. The PIDL model is initially developed for the three-dimensional case by generating synthetic data from a classical constitutive model. The model is then trained by extracting the data directly from cyclic lo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;SolarCNN&#26041;&#27861;&#65292;&#21033;&#29992;&#36741;&#21161;&#27880;&#24847;&#21147;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#21319;&#22826;&#38451;&#27963;&#21160;&#21306;&#30913;&#22270;&#30340;&#36229;&#20998;&#36776;&#29575;&#65292;&#20197;&#22686;&#24378;&#23545;&#26497;&#31471;&#31354;&#38388;&#22825;&#27668;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.18302</link><description>&lt;p&gt;
&#21033;&#29992;SDO/HMI&#25968;&#25454;&#21644;&#36741;&#21161;&#27880;&#24847;&#21147;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#21319;SOHO/MDI&#22826;&#38451;&#27963;&#21160;&#21306;&#30913;&#22270;&#30340;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using SDO/HMI Data and an Attention-Aided Convolutional Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18302
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;SolarCNN&#26041;&#27861;&#65292;&#21033;&#29992;&#36741;&#21161;&#27880;&#24847;&#21147;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#21319;&#22826;&#38451;&#27963;&#21160;&#21306;&#30913;&#22270;&#30340;&#36229;&#20998;&#36776;&#29575;&#65292;&#20197;&#22686;&#24378;&#23545;&#26497;&#31471;&#31354;&#38388;&#22825;&#27668;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#19968;&#30452;&#26159;&#22270;&#20687;&#22788;&#29702;&#21644;&#35782;&#21035;&#20013;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36741;&#21161;&#27880;&#24847;&#21147;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#29992;&#20110;&#22826;&#38451;&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;SolarCNN&#65292;&#26088;&#22312;&#22686;&#24378;&#30001;Michelson Doppler Imager&#65288;MDI&#65289;&#22312;&#22826;&#38451;&#21644;&#22826;&#38451;&#22280;&#35266;&#27979;&#21355;&#26143;&#65288;SOHO&#65289;&#19978;&#25910;&#38598;&#30340;&#22826;&#38451;&#27963;&#21160;&#21306;&#65288;AR&#65289;&#30340;&#39034;&#35270;&#30913;&#22270;&#30340;&#36136;&#37327;&#12290;&#29992;&#20110;&#35757;&#32451;SolarCNN&#30340;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#26159;&#30001;&#22826;&#38451;&#21160;&#21147;&#23398;&#35266;&#27979;&#21355;&#26143;&#65288;SDO&#65289;&#19978;&#25645;&#36733;&#30340;Helioseismic&#21644;Magnetic Imager&#65288;HMI&#65289;&#25910;&#38598;&#30340;&#39034;&#35270;&#30913;&#22270;&#12290;&#22826;&#38451;AR&#30001;&#24378;&#30913;&#22330;&#32452;&#25104;&#65292;&#22312;&#20854;&#20013;&#30913;&#33021;&#37327;&#21487;&#20197;&#31361;&#28982;&#37322;&#25918;&#65292;&#20174;&#32780;&#20135;&#29983;&#26497;&#31471;&#31354;&#38388;&#22825;&#27668;&#20107;&#20214;&#65292;&#22914;&#22826;&#38451;&#32768;&#26001;&#12289;&#26085;&#20885;&#29289;&#36136;&#25243;&#23556;&#21644;&#22826;&#38451;&#39640;&#33021;&#31890;&#23376;&#12290;SOHO/MDI&#35206;&#30422;&#22826;&#38451;&#31532;23&#20010;&#21608;&#26399;&#65292;&#36825;&#20010;&#21608;&#26399;&#27604;&#31532;24&#20010;&#21608;&#26399;&#26356;&#24378;&#28872;&#65292;&#21253;&#21547;&#26356;&#22810;&#21943;&#21457;&#20107;&#20214;&#12290;&#22686;&#24378;&#30340;SOHO/MDI&#30913;&#22270;&#21487;&#20197;&#26356;&#22909;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18302v1 Announce Type: cross  Abstract: Image super-resolution has been an important subject in image processing and recognition. Here, we present an attention-aided convolutional neural network (CNN) for solar image super-resolution. Our method, named SolarCNN, aims to enhance the quality of line-of-sight (LOS) magnetograms of solar active regions (ARs) collected by the Michelson Doppler Imager (MDI) on board the Solar and Heliospheric Observatory (SOHO). The ground-truth labels used for training SolarCNN are the LOS magnetograms collected by the Helioseismic and Magnetic Imager (HMI) on board the Solar Dynamics Observatory (SDO). Solar ARs consist of strong magnetic fields in which magnetic energy can suddenly be released to produce extreme space weather events, such as solar flares, coronal mass ejections, and solar energetic particles. SOHO/MDI covers Solar Cycle 23, which is stronger with more eruptive events than Cycle 24. Enhanced SOHO/MDI magnetograms allow for bette
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SelMix&#65292;&#19968;&#31181;&#36873;&#25321;&#24615;&#28151;&#21512;&#30340;&#24265;&#20215;&#24494;&#35843;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;&#38750;&#21487;&#20998;&#35299;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.18301</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#28151;&#21512;&#24494;&#35843;&#20197;&#20248;&#21270;&#38750;&#21487;&#20998;&#35299;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18301
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SelMix&#65292;&#19968;&#31181;&#36873;&#25321;&#24615;&#28151;&#21512;&#30340;&#24265;&#20215;&#24494;&#35843;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;&#38750;&#21487;&#20998;&#35299;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#20351;&#29992;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#22823;&#37327;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#20174;&#32780;&#37319;&#29992;&#20102;&#21508;&#31181;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#20043;&#21069;&#65292;&#24517;&#39035;&#20005;&#26684;&#35780;&#20272;&#23427;&#20204;&#22312;&#35832;&#22914;&#26368;&#22351;&#24773;&#20917;&#21484;&#22238;&#29575;&#20043;&#31867;&#30340;&#24615;&#33021;&#25351;&#26631;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#28385;&#36275;&#20844;&#24179;&#24615;&#31561;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32463;&#39564;&#25216;&#26415;&#22312;&#36825;&#20123;&#23454;&#38469;&#30340;&#12289;&#38750;&#21487;&#20998;&#35299;&#30340;&#24615;&#33021;&#30446;&#26631;&#19978;&#25552;&#20379;&#20102;&#27425;&#20248;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29702;&#35770;&#25216;&#26415;&#38656;&#35201;&#20026;&#27599;&#20010;&#24615;&#33021;&#30446;&#26631;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19968;&#20010;&#26032;&#27169;&#22411;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SelMix&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#24615;&#28151;&#21512;&#30340;&#24265;&#20215;&#24494;&#35843;&#25216;&#26415;&#65292;&#29992;&#20110;&#38024;&#23545;&#25152;&#38656;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18301v1 Announce Type: cross  Abstract: The rise in internet usage has led to the generation of massive amounts of data, resulting in the adoption of various supervised and semi-supervised machine learning algorithms, which can effectively utilize the colossal amount of data to train models. However, before deploying these models in the real world, these must be strictly evaluated on performance measures like worst-case recall and satisfy constraints such as fairness. We find that current state-of-the-art empirical techniques offer sub-optimal performance on these practical, non-decomposable performance objectives. On the other hand, the theoretical techniques necessitate training a new model from scratch for each performance objective. To bridge the gap, we propose SelMix, a selective mixup-based inexpensive fine-tuning technique for pre-trained models, to optimize for the desired objective. The core idea of our framework is to determine a sampling distribution to perform a
&lt;/p&gt;</description></item><item><title>GeNet&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12289;&#21033;&#29992;&#32534;&#30721;&#22120;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#24182;&#21033;&#29992;&#35299;&#30721;&#22120;&#37325;&#24314;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#12290;</title><link>https://arxiv.org/abs/2403.18296</link><description>&lt;p&gt;
GeNet:&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18296
&lt;/p&gt;
&lt;p&gt;
GeNet&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12289;&#21033;&#29992;&#32534;&#30721;&#22120;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#24182;&#21033;&#29992;&#35299;&#30721;&#22120;&#37325;&#24314;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35821;&#20041;&#36890;&#20449;&#20219;&#21153;&#26041;&#27861;&#20381;&#36182;&#20110;&#20102;&#35299;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26469;&#20943;&#36731;&#36890;&#36947;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22312;&#29305;&#23450;&#30340;SNR&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GeNet&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#26088;&#22312;&#25269;&#25239;&#22122;&#22768;&#65292;&#20174;&#32780;&#20419;&#36827;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#65288;TOC&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#36755;&#20837;&#25968;&#25454;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12290;&#28982;&#21518;&#21033;&#29992;&#22522;&#20110;GNN&#30340;&#32534;&#30721;&#22120;&#20174;&#28304;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20123;&#25552;&#21462;&#30340;&#35821;&#20041;&#20449;&#24687;&#28982;&#21518;&#36890;&#36807;&#36890;&#36947;&#20256;&#36755;&#12290;&#22312;&#25509;&#25910;&#31471;&#65292;&#20351;&#29992;&#22522;&#20110;GNN&#30340;&#35299;&#30721;&#22120;&#20174;&#28304;&#25968;&#25454;&#20013;&#37325;&#24314;&#30456;&#20851;&#30340;&#35821;&#20041;&#20449;&#24687;&#20197;&#29992;&#20110;TOC&#12290;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GeNet&#22312;&#25239;&#22122;&#22768;TOC&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18296v1 Announce Type: cross  Abstract: Traditional approaches to semantic communication tasks rely on the knowledge of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these methods necessitate training under specific SNR conditions, entailing considerable time and computational resources. In this paper, we propose GeNet, a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at combating noise, thereby facilitating Task-Oriented Communication (TOC). We propose a novel approach where we first transform the input data image into graph structures. Then we leverage a GNN-based encoder to extract semantic information from the source data. This extracted semantic information is then transmitted through the channel. At the receiver's end, a GNN-based decoder is utilized to reconstruct the relevant semantic information from the source data for TOC. Through experimental evaluation, we show GeNet's effectiveness in anti-noise TOC while decoup
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#29305;&#23450;&#20999;&#29255;&#37325;&#26657;&#20934;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#20219;&#24847;&#20998;&#24067;&#29255;&#27573;&#19978;&#33719;&#24471;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.18286</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#37325;&#26657;&#20934;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Recalibration of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18286
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#29305;&#23450;&#20999;&#29255;&#37325;&#26657;&#20934;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#20219;&#24847;&#20998;&#24067;&#29255;&#27573;&#19978;&#33719;&#24471;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#25552;&#21462;&#20986;&#26657;&#20934;&#33391;&#22909;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#21453;&#26144;&#20102;&#20854;&#27491;&#30830;&#24615;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;LMs&#22312;&#24191;&#27867;&#20998;&#24067;&#19978;&#21487;&#33021;&#20855;&#26377;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#65292;&#20294;&#36825;&#24448;&#24448;&#38544;&#34255;&#22312;&#26356;&#31364;&#20998;&#29255;&#20869;&#23384;&#22312;&#26174;&#33879;&#30340;&#26657;&#20934;&#19981;&#20934;&#30830;&#24615;&#65288;&#20363;&#22914;&#65292;&#22312;&#25968;&#23398;&#20013;&#23384;&#22312;&#31995;&#32479;&#24615;&#36807;&#24230;&#33258;&#20449;&#21487;&#33021;&#20250;&#24179;&#34913;&#21382;&#21490;&#20013;&#30340;&#31995;&#32479;&#24615;&#19981;&#36275;&#33258;&#20449;&#65292;&#20174;&#32780;&#22312;&#24635;&#20307;&#19978;&#23454;&#29616;&#23436;&#32654;&#26657;&#20934;&#65289;&#12290;&#20026;&#20102;&#33719;&#24471;&#20219;&#20309;&#20998;&#24067;&#29255;&#27573;&#30340;&#26657;&#20934;&#33391;&#22909;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#29305;&#23450;&#20999;&#29255;&#37325;&#26657;&#20934;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#37325;&#26032;&#26657;&#20934;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25509;&#21463;&#26469;&#33258;&#20219;&#20309;&#32473;&#23450;&#20999;&#29255;&#30340;&#23569;&#37327;&#26080;&#26631;&#31614;&#31034;&#20363;&#65292;&#24182;&#39044;&#27979;&#19968;&#26465;&#37325;&#26032;&#26144;&#23556;&#32622;&#20449;&#24230;&#20998;&#25968;&#20197;&#20351;&#20854;&#23545;&#35813;&#20999;&#29255;&#26356;&#20934;&#30830;&#30340;&#26354;&#32447;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#37325;&#26032;&#26657;&#20934;&#20219;&#24847;&#26032;&#30340;&#20999;&#29255;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#35813;&#20999;&#29255;&#30340;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18286v1 Announce Type: cross  Abstract: Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify d
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#28176;&#21464;&#21464;&#21270;&#26399;&#38388;&#20934;&#30830;&#25429;&#33719;&#38598;&#32676;&#32467;&#26500;&#24182;&#26816;&#27979;&#38598;&#32676;&#32467;&#26500;&#21464;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18269</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#28151;&#21512;&#22797;&#26434;&#24230;&#36827;&#34892;&#32858;&#31867;&#21464;&#21270;&#31526;&#21495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Clustering Change Sign Detection by Fusing Mixture Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18269
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#28176;&#21464;&#21464;&#21270;&#26399;&#38388;&#20934;&#30830;&#25429;&#33719;&#38598;&#32676;&#32467;&#26500;&#24182;&#26816;&#27979;&#38598;&#32676;&#32467;&#26500;&#21464;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#38598;&#32676;&#32467;&#26500;&#21464;&#21270;&#30340;&#26089;&#26399;&#26041;&#27861;&#12290;&#38598;&#32676;&#32467;&#26500;&#26159;&#25351;&#20351;&#29992;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#34920;&#31034;&#25968;&#25454;&#26102;&#30340;&#31163;&#25955;&#32467;&#26500;&#29305;&#24449;&#65292;&#20363;&#22914;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#38598;&#32676;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#38598;&#32676;&#32467;&#26500;&#36880;&#28176;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#65292;&#28151;&#21512;&#22797;&#26434;&#24230;&#65288;MC&#65289;&#30340;&#27010;&#24565;&#36890;&#36807;&#32771;&#34385;&#38598;&#32676;&#27604;&#20363;&#20559;&#24046;&#21644;&#38598;&#32676;&#20043;&#38388;&#30340;&#37325;&#21472;&#26469;&#24230;&#37327;&#36830;&#32493;&#30340;&#38598;&#32676;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MC&#34701;&#21512;&#20316;&#20026;MC&#30340;&#25193;&#23637;&#65292;&#20197;&#22788;&#29702;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#28151;&#21512;&#25968;&#23383;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#21512;&#24182;&#22810;&#20010;&#27169;&#22411;&#30340;&#34701;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28176;&#21464;&#21464;&#21270;&#30340;&#36807;&#28193;&#26399;&#38388;&#20934;&#30830;&#25429;&#33719;&#20102;&#38598;&#32676;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26816;&#26597;MC&#34701;&#21512;&#30340;&#36807;&#28193;&#26469;&#26816;&#27979;&#38598;&#32676;&#32467;&#26500;&#21464;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18269v1 Announce Type: cross  Abstract: This paper proposes an early detection method for cluster structural changes. Cluster structure refers to discrete structural characteristics, such as the number of clusters, when data are represented using finite mixture models, such as Gaussian mixture models. We focused on scenarios in which the cluster structure gradually changed over time. For finite mixture models, the concept of mixture complexity (MC) measures the continuous cluster size by considering the cluster proportion bias and overlap between clusters. In this paper, we propose MC fusion as an extension of MC to handle situations in which multiple mixture numbers are possible in a finite mixture model. By incorporating the fusion of multiple models, our approach accurately captured the cluster structure during transitional periods of gradual change. Moreover, we introduce a method for detecting changes in the cluster structure by examining the transition of MC fusion. We
&lt;/p&gt;</description></item><item><title>DSF-GAN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#32467;&#21512;&#19979;&#28216;&#39044;&#27979;&#27169;&#22411;&#30340;&#21453;&#39304;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#29983;&#25104;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21512;&#25104;&#26679;&#26412;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18267</link><description>&lt;p&gt;
DSF-GAN: &#19979;&#28216;&#21453;&#39304;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DSF-GAN: DownStream Feedback Generative Adversarial Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18267
&lt;/p&gt;
&lt;p&gt;
DSF-GAN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#32467;&#21512;&#19979;&#28216;&#39044;&#27979;&#27169;&#22411;&#30340;&#21453;&#39304;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#29983;&#25104;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21512;&#25104;&#26679;&#26412;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29992;&#24615;&#21644;&#38544;&#31169;&#24615;&#26159;&#34913;&#37327;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#36136;&#37327;&#30340;&#20004;&#20010;&#20851;&#38190;&#25351;&#26631;&#12290;&#23613;&#31649;&#22312;&#38544;&#31169;&#25514;&#26045;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#29983;&#25104;&#20855;&#26377;&#39640;&#23454;&#29992;&#24615;&#30340;&#21512;&#25104;&#26679;&#26412;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#21512;&#25104;&#26679;&#26412;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DownStream Feedback&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;DSF-GAN&#65289;&#30340;&#26032;&#26550;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32467;&#21512;&#20102;&#19979;&#28216;&#39044;&#27979;&#27169;&#22411;&#30340;&#21453;&#39304;&#65292;&#29992;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#22686;&#24378;&#29983;&#25104;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;DSF-GAN&#21033;&#29992;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#26469;&#22686;&#24378;&#21512;&#25104;&#26679;&#26412;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;DSF-GAN&#29983;&#25104;&#30340;&#21512;&#25104;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#30456;&#27604;&#20110;&#27809;&#26377;&#21453;&#39304;&#30340;&#30456;&#21516;GAN&#26550;&#26500;&#29983;&#25104;&#30340;&#26679;&#26412;&#12290;&#35780;&#20272;&#26159;&#22312;&#21516;&#19968;&#20010;&#39564;&#35777;&#38598;&#19978;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;re
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18267v1 Announce Type: cross  Abstract: Utility and privacy are two crucial measurements of the quality of synthetic tabular data. While significant advancements have been made in privacy measures, generating synthetic samples with high utility remains challenging. To enhance the utility of synthetic samples, we propose a novel architecture called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This approach incorporates feedback from a downstream prediction model during training to augment the generator's loss function with valuable information. Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of synthetic samples. To evaluate our method, we tested it using two popular datasets. Our experiments demonstrate improved model performance when training on synthetic samples generated by DSF-GAN, compared to those generated by the same GAN architecture without feedback. The evaluation was conducted on the same validation set comprising re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#25903;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#22312;&#25345;&#32493;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#24179;&#34913;&#65292;&#36890;&#36807;&#20998;&#25903;&#25193;&#23637;&#21644;&#21387;&#32553;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.18266</link><description>&lt;p&gt;
&#20998;&#25903;&#35843;&#25972;&#65306;&#22312;&#25345;&#32493;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#24179;&#34913;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Branch-Tuning: Balancing Stability and Plasticity for Continual Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#25903;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#22312;&#25345;&#32493;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#24179;&#34913;&#65292;&#36890;&#36807;&#20998;&#25903;&#25193;&#23637;&#21644;&#21387;&#32553;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#20174;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#24471;&#20986;&#36890;&#29992;&#34920;&#31034;&#30340;&#26377;&#25928;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29616;&#23454;&#24212;&#29992;&#19981;&#26029;&#25972;&#21512;&#26032;&#20869;&#23481;&#65292;SSL&#30340;&#39640;&#35745;&#31639;&#21644;&#36164;&#28304;&#38656;&#27714;&#38656;&#35201;&#25345;&#32493;&#23398;&#20064;&#32780;&#19981;&#26159;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#12290;&#26412;&#25991;&#21033;&#29992;&#20013;&#24515;&#26680;&#23545;&#40784;&#26469;&#23450;&#37327;&#20998;&#26512;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#65292;&#25581;&#31034;&#20102;&#25209;&#24402;&#19968;&#21270;&#23618;&#23545;&#31283;&#23450;&#24615;&#21644;&#21367;&#31215;&#23618;&#23545;&#21487;&#22609;&#24615;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21363;&#20998;&#25903;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#22312;&#25345;&#32493;SSL&#20013;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#20998;&#25903;&#35843;&#25972;&#30001;&#20998;&#25903;&#25193;&#23637;&#21644;&#21387;&#32553;&#32452;&#25104;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#21508;&#31181;SSL&#26041;&#27861;&#32780;&#19981;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18266v1 Announce Type: new  Abstract: Self-supervised learning (SSL) has emerged as an effective paradigm for deriving general representations from vast amounts of unlabeled data. However, as real-world applications continually integrate new content, the high computational and resource demands of SSL necessitate continual learning rather than complete retraining. This poses a challenge in striking a balance between stability and plasticity when adapting to new information. In this paper, we employ Centered Kernel Alignment for quantitatively analyzing model stability and plasticity, revealing the critical roles of batch normalization layers for stability and convolutional layers for plasticity. Motivated by this, we propose Branch-tuning, an efficient and straightforward method that achieves a balance between stability and plasticity in continual SSL. Branch-tuning consists of branch expansion and compression, and can be easily applied to various SSL methods without the need
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Visual Table&#65292;&#19968;&#31181;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#30340;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#26469;&#24357;&#34917;&#29616;&#26377;&#35270;&#35273;&#34920;&#31034;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.18252</link><description>&lt;p&gt;
&#36229;&#36234;&#23884;&#20837;&#65306;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#35270;&#35273;&#34920;&#26684;&#30340;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Visual Table&#65292;&#19968;&#31181;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#30340;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#26469;&#24357;&#34917;&#29616;&#26377;&#35270;&#35273;&#34920;&#31034;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22522;&#30707;&#65292;&#20174;&#20855;&#26377;&#20154;&#31867;&#27880;&#37322;&#26631;&#31614;&#30340;&#30417;&#30563;&#23398;&#20064;&#21457;&#23637;&#21040;&#23545;&#40784;&#26469;&#33258;&#20114;&#32852;&#32593;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#30340;&#35270;&#35273;&#34920;&#31034;&#65288;&#22914;CLIP&#23884;&#20837;&#65289;&#36890;&#24120;&#32570;&#20047;&#20851;&#38190;&#30340;&#22806;&#37096;&#19990;&#30028;&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Visual Table&#65292;&#36825;&#26159;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#12290;&#23427;&#25552;&#20379;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#30340;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#65292;&#21253;&#25324;&#22330;&#26223;&#25551;&#36848;&#21644;&#28085;&#30422;&#31867;&#21035;&#12289;&#23646;&#24615;&#21644;&#23454;&#20363;&#32423;&#21035;&#30693;&#35782;&#30340;&#22810;&#20010;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#25551;&#36848;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#20174;GPT4V&#30340;&#23567;&#35268;&#27169;&#27880;&#37322;&#20013;&#29983;&#25104;&#35270;&#35273;&#34920;&#26684;&#65292;&#24182;&#35757;&#32451;&#23427;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#35270;&#35273;&#34920;&#26684;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#34920;&#31034;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18252v1 Announce Type: cross  Abstract: Visual representation learning has been a cornerstone in computer vision, evolving from supervised learning with human-annotated labels to aligning image-text pairs from the Internet. Despite recent advancements in multi-modal large language models (MLLMs), the visual representations they rely on, such as CLIP embeddings, often lack access to external world knowledge critical for real-world visual reasoning. In this work, we propose Visual Table, a novel visual representation tailored for MLLMs. It provides hierarchical text descriptions of holistic visual scenes, consisting of a scene description and multiple object-centric descriptions that encompass categories, attributes, and knowledge at instance level. We further develop a scalable generator for visual table generation and train it on small-scale annotations from GPT4V. Extensive evaluations demonstrate that, with generated visual tables as additional visual representations, our 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;3D&#24418;&#29366;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;2D&#24179;&#38754;&#34920;&#31034;&#22686;&#24378;&#24314;&#27169;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#24418;&#29366;&#34920;&#31034;&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36830;&#32493;&#26377;&#21521;&#36317;&#31163;&#22330;&#34920;&#31034;&#65292;&#20174;&#32780;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#21644;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.18241</link><description>&lt;p&gt;
NeuSDFusion: &#19968;&#31181;&#31354;&#38388;&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;3D&#24418;&#29366;&#30340;&#23436;&#25104;&#12289;&#37325;&#24314;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18241
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;3D&#24418;&#29366;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;2D&#24179;&#38754;&#34920;&#31034;&#22686;&#24378;&#24314;&#27169;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#24418;&#29366;&#34920;&#31034;&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36830;&#32493;&#26377;&#21521;&#36317;&#31163;&#22330;&#34920;&#31034;&#65292;&#20174;&#32780;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#21644;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#24418;&#29366;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#26465;&#20214;&#21644;&#32422;&#26463;&#30340;&#21019;&#26032;&#24615;3D&#20869;&#23481;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23558;3D&#24418;&#29366;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#23616;&#37096;&#32452;&#20214;&#65292;&#23558;&#27599;&#20010;&#20803;&#32032;&#23396;&#31435;&#22788;&#29702;&#32780;&#19981;&#32771;&#34385;&#31354;&#38388;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;3D&#25968;&#25454;&#34920;&#31034;&#21644;&#24418;&#29366;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#22810;&#26679;&#24615;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#29983;&#25104;&#39640;&#24230;&#22810;&#26679;&#21270;&#19988;&#31526;&#21512;&#25351;&#23450;&#32422;&#26463;&#30340;3D&#24418;&#29366;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;3D&#24418;&#29366;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;2D&#24179;&#38754;&#34920;&#31034;&#26469;&#22686;&#24378;3D&#24418;&#29366;&#24314;&#27169;&#12290;&#20026;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#24182;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#31181;&#28151;&#21512;&#24418;&#29366;&#34920;&#31034;&#25216;&#26415;&#65292;&#30452;&#25509;&#20351;&#29992;&#27491;&#20132;&#30340;2D&#24179;&#38754;&#23398;&#20064;3D&#24418;&#29366;&#30340;&#36830;&#32493;&#26377;&#21521;&#36317;&#31163;&#22330;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20256;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18241v1 Announce Type: cross  Abstract: 3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling. To ensure spatial coherence and reduce memory usage, we incorporate a hybrid shape representation technique that directly learns a continuous signed distance field representation of the 3D shape using orthogonal 2D planes. Additionally, we meticulously enforce spatial correspondences across distinct planes using a tra
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#22270;&#20687;&#21464;&#21387;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#36229;&#22768;&#21069;&#21015;&#33146;&#30284;&#20998;&#31867;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.18233</link><description>&lt;p&gt;
&#29992;&#20110;&#21069;&#21015;&#33146;&#30284;&#36229;&#22768;&#25968;&#25454;&#26816;&#27979;&#30340;&#22270;&#20687;&#21464;&#21387;&#22120;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#22270;&#20687;&#21464;&#21387;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#36229;&#22768;&#21069;&#21015;&#33146;&#30284;&#20998;&#31867;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#36890;&#24120;&#20351;&#29992;&#21367;&#31215;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#31359;&#21050;&#30165;&#36857;&#21306;&#22495;&#30340;&#23567;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#20013;&#26816;&#27979;&#21069;&#21015;&#33146;&#30284;&#65288;PCa&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30001;&#20110;&#22320;&#38754;&#23454;&#20917;&#32452;&#32455;&#30149;&#29702;&#23398;&#26631;&#31614;&#26410;&#25551;&#36848;&#20010;&#21035;ROI&#30340;&#29305;&#24615;&#32780;&#23384;&#22312;&#26631;&#31614;&#24369;&#21270;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#22810;&#23610;&#24230;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#23558;&#21464;&#21387;&#22120;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#19982;CNN&#29305;&#24449;&#25552;&#21462;&#22120;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#20174;&#22810;&#20010;ROI&#20013;&#26816;&#27979;&#30284;&#30151;&#20197;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#20960;&#31181;&#22270;&#20687;&#21464;&#21387;&#22120;&#26550;&#26500;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#29992;&#20110;ROI&#23610;&#24230;&#21644;&#22810;&#23610;&#24230;&#20998;&#31867;&#65292;&#24182;&#27604;&#36739;&#20102;&#22522;&#20110;&#36229;&#22768;&#30340;&#21069;&#21015;&#33146;&#30284;&#20998;&#31867;&#20013;CNN&#21644;&#21464;&#21387;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#23558;ROI&#21644;&#26680;&#24515;&#39044;&#27979;&#32467;&#21512;&#36215;&#26469;&#30340;&#26032;&#22411;&#22810;&#30446;&#26631;&#23398;&#20064;&#31574;&#30053;&#20197;&#36827;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18233v1 Announce Type: cross  Abstract: PURPOSE: Deep learning methods for classifying prostate cancer (PCa) in ultrasound images typically employ convolutional networks (CNNs) to detect cancer in small regions of interest (ROI) along a needle trace region. However, this approach suffers from weak labelling, since the ground-truth histopathology labels do not describe the properties of individual ROIs. Recently, multi-scale approaches have sought to mitigate this issue by combining the context awareness of transformers with a CNN feature extractor to detect cancer from multiple ROIs using multiple-instance learning (MIL). In this work, we present a detailed study of several image transformer architectures for both ROI-scale and multi-scale classification, and a comparison of the performance of CNNs and transformers for ultrasound-based prostate cancer classification. We also design a novel multi-objective learning strategy that combines both ROI and core predictions to furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;Spikformer&#20013;&#20351;&#29992;&#20613;&#31435;&#21494;&#25110;&#23567;&#27874;&#22522;&#20195;&#26367;&#20256;&#32479;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20986;&#30340;FWformer&#22312;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21487;&#27604;&#29978;&#33267;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.18228</link><description>&lt;p&gt;
&#20613;&#31435;&#21494;&#25110;&#23567;&#27874;&#22522;&#20316;&#20026;Spikformer&#20013;&#39640;&#25928;&#35270;&#35273;&#20998;&#31867;&#30340;&#33258;&#27880;&#24847;&#21147;&#23545;&#24212;&#29289;
&lt;/p&gt;
&lt;p&gt;
Fourier or Wavelet bases as counterpart self-attention in spikformer for efficient visual classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;Spikformer&#20013;&#20351;&#29992;&#20613;&#31435;&#21494;&#25110;&#23567;&#27874;&#22522;&#20195;&#26367;&#20256;&#32479;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20986;&#30340;FWformer&#22312;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21487;&#27604;&#29978;&#33267;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#25928;&#39640;&#30340;Spikformer&#36890;&#36807;&#23558;&#29983;&#29289;&#21512;&#29702;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#21644;&#20154;&#24037;Transformer&#38598;&#25104;&#25552;&#20986;&#65292;&#20854;&#20013;&#20351;&#29992;&#33033;&#20914;&#33258;&#27880;&#24847;&#21147;&#65288;SSA&#65289;&#26082;&#21487;&#25552;&#39640;&#20934;&#30830;&#29575;&#21448;&#21487;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#31232;&#30095;&#33033;&#20914;&#24418;&#24335;&#35745;&#31639;&#26041;&#24335;&#20013;&#65292;&#33258;&#27880;&#24847;&#21147;&#24182;&#38750;&#24635;&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20256;&#32479;SSA&#65288;&#20351;&#29992;&#26469;&#33258;&#26597;&#35810;&#21644;&#38190;&#30340;&#21160;&#24577;&#22522;&#20989;&#25968;&#35745;&#31639;&#65289;&#26367;&#25442;&#20026;&#33033;&#20914;&#24418;&#24335;&#30340;&#20613;&#31435;&#21494;&#21464;&#25442;&#12289;&#23567;&#27874;&#21464;&#25442;&#20197;&#21450;&#23427;&#20204;&#30340;&#32452;&#21512;&#65288;&#20351;&#29992;&#22266;&#23450;&#30340;&#19977;&#35282;&#24418;&#25110;&#23567;&#27874;&#22522;&#20989;&#25968;&#65289;&#65292;&#22522;&#20110;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;&#65292;&#21363;&#23427;&#20204;&#37117;&#20351;&#29992;&#19968;&#32452;&#22522;&#20989;&#25968;&#36827;&#34892;&#20449;&#24687;&#21464;&#25442;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20613;&#31435;&#21494;&#25110;&#23567;&#27874;&#30340;Spikformer&#65288;FWformer&#65289;&#65292;&#24182;&#22312;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#38745;&#24577;&#22270;&#20687;&#21644;&#22522;&#20110;&#20107;&#20214;&#30340;&#35270;&#39057;&#25968;&#25454;&#38598;&#12290;FWformer&#22312;&#20934;&#30830;&#29575;&#26041;&#38754;&#33021;&#22815;&#36798;&#21040;&#21487;&#27604;&#29978;&#33267;&#26356;&#39640;&#30340;&#27700;&#24179;&#65288;0.4%-$
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18228v1 Announce Type: cross  Abstract: Energy-efficient spikformer has been proposed by integrating the biologically plausible spiking neural network (SNN) and artificial Transformer, whereby the Spiking Self-Attention (SSA) is used to achieve both higher accuracy and lower computational cost. However, it seems that self-attention is not always necessary, especially in sparse spike-form calculation manners. In this paper, we innovatively replace vanilla SSA (using dynamic bases calculating from Query and Key) with spike-form Fourier Transform, Wavelet Transform, and their combinations (using fixed triangular or wavelets bases), based on a key hypothesis that both of them use a set of basis functions for information transformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is proposed and verified in visual classification tasks, including both static image and event-based video datasets. The FWformer can achieve comparable or even higher accuracies ($0.4\%$-$
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;DPI&#31639;&#27861;&#65292;&#26088;&#22312;&#26816;&#27979;&#24694;&#24847;&#27969;&#37327;&#65292;&#36890;&#36807;&#23398;&#20064;&#22797;&#26434;&#30340;&#24207;&#21015;&#25968;&#25454;&#20869;&#23481;&#24182;&#25512;&#24191;&#21040;&#31867;&#20284;&#22330;&#26223;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.18223</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#26377;&#25928;&#36733;&#33655;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#20998;&#31867;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Transformer-Based Framework for Payload Malware Detection and Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;DPI&#31639;&#27861;&#65292;&#26088;&#22312;&#26816;&#27979;&#24694;&#24847;&#27969;&#37327;&#65292;&#36890;&#36807;&#23398;&#20064;&#22797;&#26434;&#30340;&#24207;&#21015;&#25968;&#25454;&#20869;&#23481;&#24182;&#25512;&#24191;&#21040;&#31867;&#20284;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24694;&#24847;&#32593;&#32476;&#23041;&#32961;&#22312;&#20837;&#20405;&#35745;&#31639;&#26426;&#32593;&#32476;&#26041;&#38754;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#26377;&#25928;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDSs&#65289;&#30340;&#38656;&#27714;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#19978;&#65292;IDSs&#20381;&#36182;&#20110;&#24322;&#24120;&#26816;&#27979;&#21644;&#22522;&#20110;&#29305;&#24449;&#24211;&#30340;&#26816;&#27979;&#25216;&#26415;&#26469;&#26816;&#27979;&#26410;&#35782;&#21035;&#21644;&#21487;&#30097;&#27963;&#21160;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;DPI&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#20174;&#36890;&#36807;&#32593;&#32476;&#20256;&#36755;&#30340;&#25968;&#25454;&#21253;&#20869;&#23481;&#20013;&#23398;&#20064;&#22797;&#26434;&#30340;&#27169;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#22522;&#20110;Transformer&#30340;DPI&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24102;&#26377;&#20998;&#31867;&#22120;&#22836;&#30340;&#36716;&#21464;&#22120;&#26816;&#27979;&#24694;&#24847;&#27969;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18223v1 Announce Type: cross  Abstract: As malicious cyber threats become more sophisticated in breaching computer networks, the need for effective intrusion detection systems (IDSs) becomes crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced to allow IDSs analyze the content of network packets, providing more context for identifying potential threats. IDSs traditionally rely on using anomaly-based and signature-based detection techniques to detect unrecognized and suspicious activity. Deep learning techniques have shown great potential in DPI for IDSs due to their efficiency in learning intricate patterns from the packet content being transmitted through the network. In this paper, we propose a revolutionary DPI algorithm based on transformers adapted for the purpose of detecting malicious traffic with a classifier head. Transformers learn the complex content of sequence data and generalize them well to similar scenarios thanks to their self-attent
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#26465;&#20214;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#30340;&#37096;&#32626;&#26041;&#27861;&#65292;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#21644;&#26412;&#22320;&#20449;&#24687;&#32858;&#21512;&#20570;&#20986;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.18222</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#26465;&#20214;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Deployment of Pre-trained Language-Conditioned Imitation Learning Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18222
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#26465;&#20214;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#30340;&#37096;&#32626;&#26041;&#27861;&#65292;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#21644;&#26412;&#22320;&#20449;&#24687;&#32858;&#21512;&#20570;&#20986;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#31574;&#30053;&#22312;&#26469;&#33258;&#19981;&#21516;&#20219;&#21153;&#21644;&#26426;&#22120;&#20154;&#24179;&#21488;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#20026;&#23454;&#29616;&#36890;&#29992;&#26426;&#22120;&#20154;&#24102;&#26469;&#24456;&#22823;&#24076;&#26395;&#65307;&#28982;&#32780;&#65292;&#21487;&#38752;&#22320;&#27867;&#21270;&#21040;&#26032;&#30340;&#29615;&#22659;&#26465;&#20214;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#26465;&#20214;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#30340;&#37096;&#32626;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#28201;&#24230;&#32553;&#25918;&#26469;&#26657;&#20934;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#26657;&#20934;&#30340;&#27169;&#22411;&#36890;&#36807;&#32858;&#21512;&#20505;&#36873;&#21160;&#20316;&#30340;&#26412;&#22320;&#20449;&#24687;&#26469;&#20570;&#20986;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#20351;&#29992;&#19977;&#20010;&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20854;&#28508;&#21147;&#26174;&#33879;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#29575;&#12290;&#38468;&#24102;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://github.com/BobWu1998/uncertainty_quant_all.git
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18222v1 Announce Type: cross  Abstract: Large-scale robotic policies trained on data from diverse tasks and robotic platforms hold great promise for enabling general-purpose robots; however, reliable generalization to new environment conditions remains a major challenge. Toward addressing this challenge, we propose a novel approach for uncertainty-aware deployment of pre-trained language-conditioned imitation learning agents. Specifically, we use temperature scaling to calibrate these models and exploit the calibrated model to make uncertainty-aware decisions by aggregating the local information of candidate actions. We implement our approach in simulation using three such pre-trained models, and showcase its potential to significantly enhance task completion rates. The accompanying code is accessible at the link: https://github.com/BobWu1998/uncertainty_quant_all.git
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20108;&#32500;&#21644;&#19977;&#32500;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#39044;&#21046;&#24211;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#35745;&#31639;&#25968;&#23398;&#24320;&#21457;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18219</link><description>&lt;p&gt;
&#20174;&#20108;&#32500;&#21040;&#19977;&#32500;&#29615;&#22659;&#30340;Q&#23398;&#20064;&#65306;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#24314;&#27169;&#33258;&#20027;&#23548;&#33322;&#32780;&#26080;&#38656;&#24211;
&lt;/p&gt;
&lt;p&gt;
From Two-Dimensional to Three-Dimensional Environment with Q-Learning: Modeling Autonomous Navigation with Reinforcement Learning and no Libraries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20108;&#32500;&#21644;&#19977;&#32500;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#39044;&#21046;&#24211;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#35745;&#31639;&#25968;&#23398;&#24320;&#21457;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#65292;&#20351;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#21453;&#39304;&#26426;&#21046;&#30340;&#20132;&#20114;&#33719;&#24471;&#26368;&#20248;&#20915;&#31574;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;RL&#20195;&#29702;&#22312;&#20108;&#32500;&#65288;2D&#65289;&#21644;&#19977;&#32500;&#65288;3D&#65289;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#30740;&#31350;&#36328;&#19981;&#21516;&#31354;&#38388;&#32500;&#24230;&#23398;&#20064;&#30340;&#21160;&#24577;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#27809;&#26377;&#29992;&#20110;&#23398;&#20064;&#30340;&#39044;&#21046;&#24211;&#65292;&#31639;&#27861;&#23436;&#20840;&#36890;&#36807;&#35745;&#31639;&#25968;&#23398;&#24320;&#21457;&#12290;&#26041;&#27861;&#35770;&#26694;&#26550;&#38598;&#20013;&#22312;RL&#21407;&#21017;&#19978;&#65292;&#37319;&#29992;Q&#23398;&#20064;&#20195;&#29702;&#31867;&#21644;&#38024;&#23545;&#27599;&#20010;&#31354;&#38388;&#32500;&#24230;&#37327;&#36523;&#23450;&#21046;&#30340;&#19981;&#21516;&#29615;&#22659;&#31867;&#12290;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#19968;&#20010;&#38382;&#39064;&#65306;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#19981;&#21516;&#31354;&#38388;&#32500;&#24230;&#30340;&#29615;&#22659;&#20013;&#22914;&#20309;&#36866;&#24212;&#21644;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;2D&#21644;3D&#35774;&#32622;&#20013;&#65311;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18219v1 Announce Type: cross  Abstract: Reinforcement learning (RL) algorithms have become indispensable tools in artificial intelligence, empowering agents to acquire optimal decision-making policies through interactions with their environment and feedback mechanisms. This study explores the performance of RL agents in both two-dimensional (2D) and three-dimensional (3D) environments, aiming to research the dynamics of learning across different spatial dimensions. A key aspect of this investigation is the absence of pre-made libraries for learning, with the algorithm developed exclusively through computational mathematics. The methodological framework centers on RL principles, employing a Q-learning agent class and distinct environment classes tailored to each spatial dimension. The research aims to address the question: How do reinforcement learning agents adapt and perform in environments of varying spatial dimensions, particularly in 2D and 3D settings? Through empirical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20844;&#24179;&#20108;&#20803;&#20998;&#31867;&#20013;&#25511;&#21046;&#20154;&#21475;&#24046;&#24322;&#30340;&#32479;&#35745;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#26497;&#23567;&#26497;&#23567;&#26368;&#20248;&#20998;&#31867;&#38169;&#35823;&#38480;&#21046;&#20154;&#21475;&#24046;&#24322;&#21040;&#29992;&#25143;&#25351;&#23450;&#38408;&#20540;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18216</link><description>&lt;p&gt;
&#20855;&#26377;&#26377;&#30028;&#20154;&#21475;&#24046;&#24322;&#30340;&#26497;&#23567;&#26497;&#23567;&#20844;&#24179;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Minimax Optimal Fair Classification with Bounded Demographic Disparity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20844;&#24179;&#20108;&#20803;&#20998;&#31867;&#20013;&#25511;&#21046;&#20154;&#21475;&#24046;&#24322;&#30340;&#32479;&#35745;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#26497;&#23567;&#26497;&#23567;&#26368;&#20248;&#20998;&#31867;&#38169;&#35823;&#38480;&#21046;&#20154;&#21475;&#24046;&#24322;&#21040;&#29992;&#25143;&#25351;&#23450;&#38408;&#20540;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32531;&#35299;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#19981;&#20844;&#24179;&#24433;&#21709;&#23545;&#30830;&#20445;&#20844;&#24179;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22823;&#37327;&#30740;&#31350;&#26088;&#22312;&#20943;&#23569;&#24046;&#24322;&#65292;&#20294;&#20351;&#29992;\emph{&#26377;&#38480;&#25968;&#25454;&#38598;}&#30340;&#25928;&#26524; -- &#32780;&#19981;&#26159;&#25972;&#20010;&#20154;&#21475; -- &#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20855;&#26377;&#20004;&#20010;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#20844;&#24179;&#20108;&#20803;&#20998;&#31867;&#30340;&#32479;&#35745;&#22522;&#30784;&#65292;&#37325;&#28857;&#26159;&#25511;&#21046;&#20154;&#21475;&#24046;&#24322;&#65292;&#21363;&#32676;&#20307;&#20043;&#38388;&#30340;&#25509;&#21463;&#29575;&#24046;&#24322;&#12290;&#23613;&#31649;&#21363;&#20351;&#26377;&#26080;&#38480;&#25968;&#25454;&#65292;&#20844;&#24179;&#21487;&#33021;&#20250;&#20197;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#20351;&#29992;&#26377;&#38480;&#26679;&#26412;&#20250;&#30001;&#20110;&#38656;&#35201;&#20272;&#35745;&#29305;&#23450;&#20110;&#32676;&#20307;&#30340;&#25509;&#21463;&#38408;&#20540;&#32780;&#20135;&#29983;&#39069;&#22806;&#25104;&#26412;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20877;&#23558;&#20154;&#21475;&#24046;&#24322;&#38480;&#21046;&#21040;&#29992;&#25143;&#25351;&#23450;&#38408;&#20540;&#26102;&#30340;&#26497;&#23567;&#26497;&#23567;&#26368;&#20248;&#20998;&#31867;&#38169;&#35823;&#12290;&#20026;&#20102;&#37327;&#21270;&#20844;&#24179;&#32422;&#26463;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;\emph{&#20844;&#24179;&#24863;&#30693;&#36229;&#39069;&#39118;&#38505;}&#30340;&#26032;&#39062;&#24230;&#37327;&#65292;&#24182;&#25512;&#23548;&#20102;&#19968;&#20010;&#26497;&#23567;&#26497;&#23567;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18216v1 Announce Type: cross  Abstract: Mitigating the disparate impact of statistical machine learning methods is crucial for ensuring fairness. While extensive research aims to reduce disparity, the effect of using a \emph{finite dataset} -- as opposed to the entire population -- remains unclear. This paper explores the statistical foundations of fair binary classification with two protected groups, focusing on controlling demographic disparity, defined as the difference in acceptance rates between the groups. Although fairness may come at the cost of accuracy even with infinite data, we show that using a finite sample incurs additional costs due to the need to estimate group-specific acceptance thresholds. We study the minimax optimal classification error while constraining demographic disparity to a user-specified threshold. To quantify the impact of fairness constraints, we introduce a novel measure called \emph{fairness-aware excess risk} and derive a minimax lower bou
&lt;/p&gt;</description></item><item><title>NeuroPictor&#36890;&#36807;&#30452;&#25509;&#35843;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;fMRI&#21040;&#22270;&#20687;&#30340;&#37325;&#24314;&#65292;&#22312;&#22810;&#20010;&#20010;&#20307;&#39044;&#35757;&#32451;&#21644;&#22810;&#23618;&#27425;&#30340;&#24341;&#23548;&#26465;&#20214;&#19979;&#65292;&#23454;&#29616;&#20102;&#26356;&#35814;&#32454;&#30340;&#22270;&#20687;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.18211</link><description>&lt;p&gt;
NeuroPictor: &#36890;&#36807;&#22810;&#20010;&#20010;&#20307;&#30340;&#39044;&#35757;&#32451;&#21644;&#22810;&#23618;&#35843;&#21046;&#20248;&#21270;fMRI&#21040;&#22270;&#20687;&#30340;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18211
&lt;/p&gt;
&lt;p&gt;
NeuroPictor&#36890;&#36807;&#30452;&#25509;&#35843;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;fMRI&#21040;&#22270;&#20687;&#30340;&#37325;&#24314;&#65292;&#22312;&#22810;&#20010;&#20010;&#20307;&#39044;&#35757;&#32451;&#21644;&#22810;&#23618;&#27425;&#30340;&#24341;&#23548;&#26465;&#20214;&#19979;&#65292;&#23454;&#29616;&#20102;&#26356;&#35814;&#32454;&#30340;&#22270;&#20687;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;fMRI&#21040;&#22270;&#20687;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;fMRI&#20449;&#21495;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#29305;&#23450;&#26465;&#20214;&#20851;&#32852;&#36215;&#26469;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#30452;&#25509;&#35843;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23558;fMRI&#21040;&#22270;&#20687;&#36807;&#31243;&#20998;&#20026;&#19977;&#20010;&#27493;&#39588;&#65306;i) fMRI&#26657;&#20934;&#32534;&#30721;&#65292;&#29992;&#20110;&#22788;&#29702;&#20849;&#20139;&#28508;&#22312;&#31354;&#38388;&#30340;&#22810;&#20010;&#20307;&#39044;&#35757;&#32451;&#65292;&#20197;&#26368;&#23567;&#21270;&#20010;&#20307;&#24046;&#24322;&#24182;&#23454;&#29616;&#21518;&#32493;&#30340;&#36328;&#20027;&#20307;&#35757;&#32451;&#65307;ii) fMRI&#21040;&#22270;&#20687;&#36328;&#20010;&#20307;&#39044;&#35757;&#32451;&#65292;&#24863;&#30693;&#22320;&#23398;&#20064;&#22914;&#20309;&#24341;&#23548;&#19981;&#21516;&#20010;&#20307;&#20043;&#38388;&#39640;&#20302;&#23618;&#27425;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#65307;iii) fMRI&#21040;&#22270;&#20687;&#21333;&#20010;&#20307;&#32454;&#21270;&#65292;&#31867;&#20284;&#20110;&#27493;&#39588;ii&#65292;&#20294;&#20391;&#37325;&#20110;&#36866;&#24212;&#29305;&#23450;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18211v1 Announce Type: cross  Abstract: Recent fMRI-to-image approaches mainly focused on associating fMRI signals with specific conditions of pre-trained diffusion models. These approaches, while producing high-quality images, capture only a limited aspect of the complex information in fMRI signals and offer little detailed control over image creation. In contrast, this paper proposes to directly modulate the generation process of diffusion models using fMRI signals. Our approach, NeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI calibrated-encoding, to tackle multi-individual pre-training for a shared latent space to minimize individual difference and enable the subsequent cross-subject training; ii) fMRI-to-image cross-subject pre-training, perceptually learning to guide diffusion model with high- and low-level conditions across different individuals; iii) fMRI-to-image single-subject refining, similar with step ii but focus on adapting to particula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#26399;&#21644;&#30701;&#26399;&#32422;&#26463;&#30340;&#26032;&#31639;&#27861;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20013;&#21487;&#20197;&#21516;&#26102;&#20445;&#35777;&#36710;&#36742;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18209</link><description>&lt;p&gt;
&#38271;&#30701;&#26399;&#32422;&#26463;&#39537;&#21160;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Long and Short-Term Constraints Driven Safe Reinforcement Learning for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#26399;&#21644;&#30701;&#26399;&#32422;&#26463;&#30340;&#26032;&#31639;&#27861;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20013;&#21487;&#20197;&#21516;&#26102;&#20445;&#35777;&#36710;&#36742;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#20915;&#31574;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#19982;&#29615;&#22659;&#20132;&#20114;&#65292;&#26080;&#27861;&#20445;&#35777;&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#26399;&#21644;&#30701;&#26399;&#32422;&#26463;&#65288;LSTC&#65289;&#30340;&#26032;&#31639;&#27861;&#29992;&#20110;&#23433;&#20840;RL&#12290;&#30701;&#26399;&#32422;&#26463;&#26088;&#22312;&#30830;&#20445;&#36710;&#36742;&#25506;&#27979;&#21040;&#30340;&#30701;&#26399;&#29366;&#24577;&#23433;&#20840;&#65292;&#32780;&#38271;&#26399;&#32422;&#26463;&#21017;&#30830;&#20445;&#25972;&#20307;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18209v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has been widely used in decision-making tasks, but it cannot guarantee the agent's safety in the training process due to the requirements of interaction with the environment, which seriously limits its industrial applications such as autonomous driving. Safe RL methods are developed to handle this issue by constraining the expected safety violation costs as a training objective, but they still permit unsafe state occurrence, which is unacceptable in autonomous driving tasks. Moreover, these methods are difficult to achieve a balance between the cost and return expectations, which leads to learning performance degradation for the algorithms. In this paper, we propose a novel algorithm based on the long and short-term constraints (LSTC) for safe RL. The short-term constraint aims to guarantee the short-term state safety that the vehicle explores, while the long-term constraint ensures the overall safety of the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#20013;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#36328;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#20934;&#30830;&#35786;&#26029;&#32467;&#26524;&#21644;&#20844;&#24179;&#24615;&#65292;&#20026;&#39640;&#32500;&#33016;&#37096;X&#23556;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#24102;&#26469;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2403.18196</link><description>&lt;p&gt;
&#36229;&#36234;&#34920;&#38754;&#25152;&#35265;&#65306;&#22522;&#20110;&#31181;&#26063;&#20581;&#24247;&#19981;&#24179;&#31561;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#30340;&#22810;&#26631;&#31614;&#33016;&#37096;X&#23556;&#32447;&#20998;&#31867;&#30340;&#23376;&#32452;&#20132;&#21449;&#20844;&#24179;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Looking Beyond What You See: An Empirical Analysis on Subgroup Intersectional Fairness for Multi-label Chest X-ray Classification Using Social Determinants of Racial Health Inequities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18196
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#20013;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#36328;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#20934;&#30830;&#35786;&#26029;&#32467;&#26524;&#21644;&#20844;&#24179;&#24615;&#65292;&#20026;&#39640;&#32500;&#33016;&#37096;X&#23556;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#24102;&#26469;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#33016;&#37096;X&#23556;&#32447;&#36827;&#34892;&#30142;&#30149;&#35786;&#26029;&#26041;&#38754;&#65292;&#23454;&#26045;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#36328;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#24046;&#24322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#35786;&#26029;&#32467;&#26524;&#65292;&#24182;&#30830;&#20445;&#22312;&#39640;&#32500;&#33016;&#37096;X&#23556;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#36328;&#20132;&#21449;&#32676;&#20307;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#19981;&#25304;&#19968;&#26684;&#22320;&#32771;&#34385;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#20013;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#26356;&#31934;&#32454;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#28041;&#21450;&#20351;&#29992;&#36328;&#32452;&#24179;&#34913;&#25968;&#25454;&#38598;&#37325;&#26032;&#35757;&#32451;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#21518;&#20998;&#31867;&#23618;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#24182;&#20026;&#22810;&#26631;&#31614;&#35774;&#32622;&#25972;&#21512;&#20102;&#31867;&#24179;&#34913;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;MIMIC-CXR&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18196v1 Announce Type: cross  Abstract: There has been significant progress in implementing deep learning models in disease diagnosis using chest X- rays. Despite these advancements, inherent biases in these models can lead to disparities in prediction accuracy across protected groups. In this study, we propose a framework to achieve accurate diagnostic outcomes and ensure fairness across intersectional groups in high-dimensional chest X- ray multi-label classification. Transcending traditional protected attributes, we consider complex interactions within social determinants, enabling a more granular benchmark and evaluation of fairness. We present a simple and robust method that involves retraining the last classification layer of pre-trained models using a balanced dataset across groups. Additionally, we account for fairness constraints and integrate class-balanced fine-tuning for multi-label settings. The evaluation of our method on the MIMIC-CXR dataset demonstrates that
&lt;/p&gt;</description></item><item><title>&#22810;&#26631;&#31614;&#25968;&#25454;&#20013;&#22256;&#38590;&#21644;&#19981;&#24179;&#34913;&#26679;&#26412;&#30340;&#31361;&#20986;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25209;&#27425;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.18192</link><description>&lt;p&gt;
&#36890;&#36807;&#31361;&#20986;&#22256;&#38590;&#21644;&#19981;&#24179;&#34913;&#26679;&#26412;&#36827;&#34892;&#22810;&#26631;&#31614;&#33258;&#36866;&#24212;&#25209;&#27425;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Multi-Label Adaptive Batch Selection by Highlighting Hard and Imbalanced Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18192
&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#25968;&#25454;&#20013;&#22256;&#38590;&#21644;&#19981;&#24179;&#34913;&#26679;&#26412;&#30340;&#31361;&#20986;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25209;&#27425;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2403.18192v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#20998;&#31867;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#24120;&#65292;&#23427;&#20204;&#37319;&#29992;&#32452;&#21512;&#23567;&#25209;&#27425;&#21644;&#20248;&#21270;&#22120;&#30340;&#35757;&#32451;&#27169;&#24335;&#65292;&#22312;&#26500;&#24314;&#23567;&#25209;&#27425;&#26102;&#27599;&#20010;&#26679;&#26412;&#34987;&#38543;&#26426;&#36873;&#25321;&#65292;&#27010;&#29575;&#30456;&#31561;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#26631;&#31614;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21487;&#33021;&#20250;&#20351;&#27169;&#22411;&#20542;&#21521;&#20110;&#20027;&#23548;&#26631;&#31614;&#65292;&#22240;&#20026;&#19982;&#23569;&#25968;&#26631;&#31614;&#30456;&#20851;&#30340;&#26679;&#26412;&#21487;&#33021;&#22312;&#27599;&#20010;&#23567;&#25209;&#27425;&#20013;&#20195;&#34920;&#19981;&#36275;&#12290;&#21516;&#26102;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#23569;&#25968;&#26631;&#31614;&#30456;&#20851;&#30340;&#23454;&#20363;&#20542;&#21521;&#20110;&#24341;&#36215;&#26356;&#22823;&#30340;&#25439;&#22833;&#12290;&#29616;&#26377;&#30340;&#21551;&#21457;&#24335;&#25209;&#27425;&#36873;&#25321;&#26041;&#27861;&#65292;&#20363;&#22914;&#36873;&#25321;&#23545;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;&#24456;&#39640;&#36129;&#29486;&#24230;&#30340;&#26679;&#26412;&#65292;&#21363;&#25439;&#22833;&#24456;&#39640;&#30340;&#26679;&#26412;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#21333;&#26631;&#31614;&#25968;&#25454;&#20013;&#30340;&#25439;&#22833;&#21644;&#27979;&#35797;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#25209;&#27425;&#36873;&#25321;&#26041;&#27861;&#23578;&#26410;&#34987;&#24212;&#29992;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18192v1 Announce Type: new  Abstract: Deep neural network models have demonstrated their effectiveness in classifying multi-label data from various domains. Typically, they employ a training mode that combines mini-batches with optimizers, where each sample is randomly selected with equal probability when constructing mini-batches. However, the intrinsic class imbalance in multi-label data may bias the model towards majority labels, since samples relevant to minority labels may be underrepresented in each mini-batch. Meanwhile, during the training process, we observe that instances associated with minority labels tend to induce greater losses. Existing heuristic batch selection methods, such as priority selection of samples with high contribution to the objective function, i.e., samples with high loss, have been proven to accelerate convergence while reducing the loss and test error in single-label data. However, batch selection methods have not yet been applied and validate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#23618;&#32858;&#31867;&#21387;&#32553;&#38750;&#32447;&#24615;&#29289;&#29702;&#27169;&#22411;&#30340;Koopman&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20256;&#32479;SVD&#21387;&#32553;&#65292;&#20998;&#23618;&#32858;&#31867;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.18181</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#32858;&#31867;&#21387;&#32553;&#38750;&#32447;&#24615;&#29289;&#29702;&#27169;&#22411;&#30340;Koopman&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Compression of the Koopman matrix for nonlinear physical models via hierarchical clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18181
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#23618;&#32858;&#31867;&#21387;&#32553;&#38750;&#32447;&#24615;&#29289;&#29702;&#27169;&#22411;&#30340;Koopman&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20256;&#32479;SVD&#21387;&#32553;&#65292;&#20998;&#23618;&#32858;&#31867;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20801;&#35768;&#20165;&#20174;&#25968;&#25454;&#39044;&#27979;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#12290; Koopman&#31639;&#23376;&#26159;&#20854;&#20013;&#20043;&#19968;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20351;&#29992;&#32447;&#24615;&#20998;&#26512;&#12290; Koopman&#31639;&#23376;&#30340;&#32447;&#24615;&#29305;&#24449;&#26377;&#26395;&#29702;&#35299;&#38750;&#32447;&#24615;&#21160;&#24577;&#24182;&#36827;&#34892;&#24555;&#36895;&#39044;&#27979;&#12290; &#25193;&#23637;&#30340;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;EDMD&#65289;&#26159;&#19968;&#31181;&#36817;&#20284;Koopman&#31639;&#23376;&#30340;&#26377;&#38480;&#32500;&#30697;&#38453;&#30340;&#26041;&#27861;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#32858;&#31867;&#21387;&#32553;Koopman&#30697;&#38453;&#30340;&#26041;&#27861;&#12290; &#36890;&#36807;&#23567;&#36710;&#25670;&#27169;&#22411;&#30340;&#25968;&#20540;&#28436;&#31034;&#21644;&#19982;&#20256;&#32479;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#20998;&#23618;&#32858;&#31867;&#27604;&#26420;&#32032;SVD&#21387;&#32553;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18181v1 Announce Type: new  Abstract: Machine learning methods allow the prediction of nonlinear dynamical systems from data alone. The Koopman operator is one of them, which enables us to employ linear analysis for nonlinear dynamical systems. The linear characteristics of the Koopman operator are hopeful to understand the nonlinear dynamics and perform rapid predictions. The extended dynamic mode decomposition (EDMD) is one of the methods to approximate the Koopman operator as a finite-dimensional matrix. In this work, we propose a method to compress the Koopman matrix using hierarchical clustering. Numerical demonstrations for the cart-pole model and comparisons with the conventional singular value decomposition (SVD) are shown; the results indicate that the hierarchical clustering performs better than the naive SVD compressions.
&lt;/p&gt;</description></item><item><title>&#26032;&#31639;&#27861;&#26088;&#22312;&#22312;&#23384;&#22312;&#25112;&#30053;&#20195;&#29702;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#26368;&#22823;&#36793;&#38469;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#12289;&#26377;&#38480;&#38169;&#35823;&#21644;&#26377;&#38480;&#25805;&#32437;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.18176</link><description>&lt;p&gt;
&#22312;&#32447;&#25112;&#30053;&#20998;&#31867;&#20013;&#30340;&#38169;&#35823;&#12289;&#25805;&#32437;&#21644;&#36793;&#38469;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Mistake, Manipulation and Margin Guarantees in Online Strategic Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18176
&lt;/p&gt;
&lt;p&gt;
&#26032;&#31639;&#27861;&#26088;&#22312;&#22312;&#23384;&#22312;&#25112;&#30053;&#20195;&#29702;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#26368;&#22823;&#36793;&#38469;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#12289;&#26377;&#38480;&#38169;&#35823;&#21644;&#26377;&#38480;&#25805;&#32437;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22312;&#32447;&#25112;&#30053;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#21040;&#36798;&#30340;&#20195;&#29702;&#21487;&#20197;&#25805;&#32437;&#20182;&#20204;&#30495;&#23454;&#30340;&#29305;&#24449;&#21521;&#37327;&#20197;&#33719;&#21462;&#19968;&#20010;&#27491;&#39044;&#27979;&#26631;&#31614;&#65292;&#21516;&#26102;&#25215;&#25285;&#21462;&#20915;&#20110;&#25805;&#32437;&#37327;&#30340;&#25104;&#26412;&#12290;&#23398;&#20064;&#32773;&#35797;&#22270;&#22312;&#21482;&#26377;&#25805;&#32437;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#20195;&#29702;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#22312;&#23398;&#20064;&#32773;&#21457;&#24067;&#20182;&#20204;&#30340;&#39044;&#27979;&#21518;&#65292;&#20195;&#29702;&#30340;&#30495;&#23454;&#26631;&#31614;&#20250;&#34987;&#25581;&#31034;&#12290;&#20808;&#21069;&#30340;&#31639;&#27861;&#22914;&#25112;&#30053;&#24863;&#30693;&#22120;&#22312;&#20851;&#20110;&#20195;&#29702;&#30495;&#23454;&#29305;&#24449;&#21521;&#37327;&#30340;&#36793;&#38469;&#20551;&#35774;&#19979;&#20445;&#35777;&#20102;&#26377;&#38480;&#30340;&#38169;&#35823;&#27425;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#24182;&#19981;&#33021;&#20445;&#35777;&#40723;&#21169;&#20195;&#29702;&#35802;&#23454;&#12290;&#20419;&#36827;&#35802;&#23454;&#19982;&#33719;&#24471;&#20805;&#36275;&#36793;&#38469;&#30340;&#39044;&#27979;&#23494;&#20999;&#30456;&#20851;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#26088;&#22312;&#22312;&#23384;&#22312;&#25112;&#30053;&#20195;&#29702;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#26368;&#22823;&#36793;&#38469;&#20998;&#31867;&#22120;&#30340;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#31181;&#20195;&#29702;&#25104;&#26412;&#32467;&#26500;&#19979;&#30340;&#25910;&#25947;&#24615;&#12289;&#26377;&#38480;&#38169;&#35823;&#21644;&#26377;&#38480;&#25805;&#32437;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18176v1 Announce Type: new  Abstract: We consider an online strategic classification problem where each arriving agent can manipulate their true feature vector to obtain a positive predicted label, while incurring a cost that depends on the amount of manipulation. The learner seeks to predict the agent's true label given access to only the manipulated features. After the learner releases their prediction, the agent's true label is revealed. Previous algorithms such as the strategic perceptron guarantee finitely many mistakes under a margin assumption on agents' true feature vectors. However, these are not guaranteed to encourage agents to be truthful. Promoting truthfulness is intimately linked to obtaining adequate margin on the predictions, thus we provide two new algorithms aimed at recovering the maximum margin classifier in the presence of strategic agent behavior. We prove convergence, finite mistake and finite manipulation guarantees for a variety of agent cost struct
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;ov-freeze&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.18159</link><description>&lt;p&gt;
&#22114;&#65281;&#25105;&#20204;&#20919;&#20923;&#65306;&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18159
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;ov-freeze&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#20998;&#21035;&#22312;NLP&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#36895;&#24230;&#24930;&#65292;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#39640;&#65292;&#36825;&#20351;&#24471;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#23427;&#20204;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#37327;&#21270;&#24863;&#30693;&#24494;&#35843;&#25216;&#26415;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD-QAT&#65289;&#26469;&#25913;&#21892;&#20351;&#29992;&#24120;&#29992;&#25968;&#25454;&#38598;&#25913;&#36827;4&#20301;&#37325;&#37327;&#37327;&#21270;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#23454;&#29616;&#27969;&#34892;&#30340;&#35821;&#35328;&#20351;&#29992;&#26696;&#20363;&#65292;&#22312;&#35774;&#22791;&#32842;&#22825;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#31181;&#24494;&#35843;&#33539;&#24335;&#65292;&#20316;&#20026;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#20256;&#25773;&#65292;&#25552;&#20379;&#23545;KD-QAT&#31283;&#23450;&#24615;&#30340;&#27934;&#23519;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;KD-QAT&#30340;&#26041;&#27861;&#23545;&#20302;&#20301;&#37327;&#21270;&#35823;&#24046;&#30340;&#33030;&#24369;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ov-freeze&#65292;&#19968;&#31181;&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#31616;&#21333;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18159v1 Announce Type: cross  Abstract: Large generative models, such as large language models (LLMs) and diffusion models have as revolutionized the fields of NLP and computer vision respectively. However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices. In this study, we propose a light-weight quantization aware fine tuning technique using knowledge distillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs using commonly available datasets to realize a popular language use case, on device chat applications. To improve this paradigm of finetuning, as main contributions, we provide insights into stability of KD-QAT by empirically studying the gradient propagation during training to better understand the vulnerabilities of KD-QAT based approaches to low-bit quantization errors. Based on our insights, we propose ov-freeze, a simple technique to stabilize the KD-QAT process. Finally, we expe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#37327;&#21270;&#20915;&#31574;&#26641;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21464;&#26641;&#32467;&#26500;&#21644;&#20915;&#31574;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18147</link><description>&lt;p&gt;
&#21010;&#20998;&#12289;&#24449;&#26381;&#12289;&#32467;&#21512;&#36125;&#21494;&#26031;&#20915;&#31574;&#26641;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Divide, Conquer, Combine Bayesian Decision Tree Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18147
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#37327;&#21270;&#20915;&#31574;&#26641;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21464;&#26641;&#32467;&#26500;&#21644;&#20915;&#31574;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#24120;&#29992;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#22240;&#20854;&#28789;&#27963;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#37327;&#21270;&#20915;&#31574;&#26641;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#25506;&#32034;&#26641;&#32467;&#26500;&#31354;&#38388;&#21644;&#19982;&#27599;&#20010;&#26641;&#32467;&#26500;&#30456;&#20851;&#32852;&#30340;&#20915;&#31574;&#21442;&#25968;&#31354;&#38388;&#12290;&#36890;&#36807;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#65292;&#20854;&#20013;&#26500;&#24314;&#39532;&#23572;&#21487;&#22827;&#38142;&#20197;&#25552;&#20379;&#25152;&#38656;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#26679;&#26412;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#32467;&#26500;&#21644;&#20915;&#31574;&#21442;&#25968;&#32039;&#23494;&#32806;&#21512;&#65307;&#26641;&#32467;&#26500;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#38656;&#35201;&#22823;&#19981;&#21516;&#30340;&#20915;&#31574;&#21442;&#25968;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#29616;&#26377;MCMC&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#25552;&#20986;&#21516;&#26102;&#25913;&#21464;&#26641;&#32467;&#26500;&#21644;&#20915;&#31574;&#21442;&#25968;&#30340;&#24314;&#35758;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#25277;&#26679;&#12290;&#26412;&#25991;&#37319;&#21462;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18147v1 Announce Type: new  Abstract: Decision trees are commonly used predictive models due to their flexibility and interpretability. This paper is directed at quantifying the uncertainty of decision tree predictions by employing a Bayesian inference approach. This is challenging because these approaches need to explore both the tree structure space and the space of decision parameters associated with each tree structure. This has been handled by using Markov Chain Monte Carlo (MCMC) methods, where a Markov Chain is constructed to provide samples from the desired Bayesian estimate. Importantly, the structure and the decision parameters are tightly coupled; small changes in the tree structure can demand vastly different decision parameters to provide accurate predictions. A challenge for existing MCMC approaches is proposing joint changes in both the tree structure and the decision parameters that result in efficient sampling. This paper takes a different approach, where ea
&lt;/p&gt;</description></item><item><title>HERTA&#26159;&#38024;&#23545;&#23637;&#24320;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20986;&#30340;&#19968;&#31181;&#39640;&#25928;&#19988;&#20005;&#26684;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#21152;&#36895;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#24182;&#20445;&#30041;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#36817;&#32447;&#24615;&#26102;&#38388;&#26368;&#22351;&#24773;&#20917;&#35757;&#32451;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.18142</link><description>&lt;p&gt;
HERTA&#65306;&#19968;&#31181;&#39640;&#25928;&#19988;&#20005;&#26684;&#30340;&#23637;&#24320;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18142
&lt;/p&gt;
&lt;p&gt;
HERTA&#26159;&#38024;&#23545;&#23637;&#24320;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20986;&#30340;&#19968;&#31181;&#39640;&#25928;&#19988;&#20005;&#26684;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#21152;&#36895;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#24182;&#20445;&#30041;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#36817;&#32447;&#24615;&#26102;&#38388;&#26368;&#22351;&#24773;&#20917;&#35757;&#32451;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#23637;&#24320;GNNs&#22312;&#20256;&#32479;&#35774;&#35745;&#19978;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#21040;&#35757;&#32451;&#25104;&#26412;&#26102;&#65292;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#30528;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#38598;&#20013;&#22312;&#27599;&#27425;&#36845;&#20195;&#30340;&#25928;&#29575;&#19978;&#65292;&#27809;&#26377;&#26368;&#22351;&#24773;&#20917;&#25910;&#25947;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20250;&#21521;&#21407;&#27169;&#22411;&#28155;&#21152;&#32452;&#20214;&#25110;&#36827;&#34892;&#20462;&#25913;&#65292;&#21487;&#33021;&#20250;&#30772;&#22351;&#23637;&#24320;GNNs&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HERTA&#65306;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#30340;&#39640;&#25928;&#19988;&#20005;&#26684;&#30340;&#23637;&#24320;GNNs&#35757;&#32451;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#32447;&#24615;&#26102;&#38388;&#26368;&#22351;&#24773;&#20917;&#35757;&#32451;&#20445;&#35777;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;HERTA&#25910;&#25947;&#21040;&#21407;&#22987;&#27169;&#22411;&#30340;&#26368;&#20248;&#35299;&#65292;&#20174;&#32780;&#20445;&#25345;&#20102;&#23637;&#24320;GNNs&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;HERTA&#30340;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35889;&#31232;&#30095;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18142v1 Announce Type: new  Abstract: As a variant of Graph Neural Networks (GNNs), Unfolded GNNs offer enhanced interpretability and flexibility over traditional designs. Nevertheless, they still suffer from scalability challenges when it comes to the training cost. Although many methods have been proposed to address the scalability issues, they mostly focus on per-iteration efficiency, without worst-case convergence guarantees. Moreover, those methods typically add components to or modify the original model, thus possibly breaking the interpretability of Unfolded GNNs. In this paper, we propose HERTA: a High-Efficiency and Rigorous Training Algorithm for Unfolded GNNs that accelerates the whole training process, achieving a nearly-linear time worst-case training guarantee. Crucially, HERTA converges to the optimum of the original model, thus preserving the interpretability of Unfolded GNNs. Additionally, as a byproduct of HERTA, we propose a new spectral sparsification met
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;GNN&#20013;&#30340;&#21518;&#38376;&#35757;&#32451;&#22270;&#65292;&#35774;&#35745;&#20102;&#19971;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#20197;&#26356;&#26377;&#25928;&#22320;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#25915;&#20987;&#36827;&#34892;&#20102;&#26041;&#27861;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.18136</link><description>&lt;p&gt;
&#20445;&#25252;GNN&#65306;&#22522;&#20110;&#35299;&#37322;&#30340;&#21518;&#38376;&#35757;&#32451;&#22270;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;GNN&#20013;&#30340;&#21518;&#38376;&#35757;&#32451;&#22270;&#65292;&#35774;&#35745;&#20102;&#19971;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#20197;&#26356;&#26377;&#25928;&#22320;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#25915;&#20987;&#36827;&#34892;&#20102;&#26041;&#27861;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs)&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#27969;&#34892;&#36215;&#26469;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#36825;&#21487;&#33021;&#20250;&#25439;&#23475;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#36947;&#24503;&#24212;&#29992;&#12290;&#26816;&#27979;&#36825;&#20123;&#25915;&#20987;&#23545;&#20110;&#20445;&#25345;GNN&#20998;&#31867;&#20219;&#21153;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26377;&#25928;&#30340;&#26816;&#27979;&#25216;&#26415;&#24182;&#19981;&#22810;&#35265;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;&#22270;&#32423;&#35299;&#37322;&#33021;&#22815;&#25552;&#20379;&#19968;&#20123;&#26377;&#38480;&#30340;&#35265;&#35299;&#65292;&#20294;&#23427;&#20204;&#22312;&#26816;&#27979;&#21518;&#38376;&#35302;&#21457;&#22120;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#26159;&#19981;&#19968;&#33268;&#19988;&#19981;&#23436;&#25972;&#30340;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#21462;&#24182;&#36716;&#25442;GNN&#35299;&#37322;&#26426;&#21046;&#30340;&#27425;&#35201;&#36755;&#20986;&#65292;&#35774;&#35745;&#20102;&#19971;&#31181;&#26356;&#26377;&#25928;&#22320;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25915;&#20987;&#26469;&#20005;&#26684;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26816;&#26597;&#20854;&#23545;&#21508;&#31181;&#25915;&#20987;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#36739;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18136v1 Announce Type: cross  Abstract: Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet they are vulnerable to backdoor attacks that can compromise their performance and ethical application. The detection of these attacks is crucial for maintaining the reliability and security of GNN classification tasks, but effective detection techniques are lacking. Following an initial investigation, we observed that while graph-level explanations can offer limited insights, their effectiveness in detecting backdoor triggers is inconsistent and incomplete. To bridge this gap, we extract and transform secondary outputs of GNN explanation mechanisms, designing seven novel metrics that more effectively detect backdoor attacks. Additionally, we develop an adaptive attack to rigorously evaluate our approach. We test our method on multiple benchmark datasets and examine its efficacy against various attack models. Our results show that our method can achieve high de
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#20851;&#32852;&#20851;&#31995;&#65292;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#36895;&#24230;&#24555;&#19978;&#30334;&#20493;</title><link>https://arxiv.org/abs/2403.18133</link><description>&lt;p&gt;
AE SemRL&#65306;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#35821;&#20041;&#20851;&#32852;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
AE SemRL: Learning Semantic Association Rules with Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18133
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#20851;&#32852;&#20851;&#31995;&#65292;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#36895;&#24230;&#24555;&#19978;&#30334;&#20493;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#65288;ARM&#65289;&#26159;&#23398;&#20064;&#25968;&#25454;&#29305;&#24449;&#20043;&#38388;&#36923;&#36753;&#35268;&#21017;&#20851;&#32852;&#30340;&#20219;&#21153;&#12290;&#20174;&#39640;&#32500;&#25968;&#20540;&#25968;&#25454;&#20013;&#25366;&#25496;&#20851;&#32852;&#35268;&#21017;&#65292;&#20363;&#22914;&#20174;&#26234;&#33021;&#29615;&#22659;&#20013;&#22823;&#37327;&#20256;&#24863;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#26159;&#19968;&#39033;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#25552;&#21462;&#20851;&#32852;&#35268;&#21017;&#65288;AE SemRL&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#19982;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#28304;&#30456;&#20851;&#30340;&#35821;&#20041;&#20449;&#24687;&#23384;&#22312;&#26102;&#65292;&#35821;&#20041;&#21487;&#20197;&#20419;&#36827;&#23398;&#20064;&#21487;&#25512;&#24191;&#21644;&#21487;&#35299;&#37322;&#30340;&#20851;&#32852;&#35268;&#21017;&#12290;&#23613;&#31649;&#36890;&#36807;&#39069;&#22806;&#30340;&#35821;&#20041;&#29305;&#24449;&#20016;&#23500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;AE SemRL&#20351;&#24471;&#20174;&#39640;&#32500;&#25968;&#25454;&#20013;&#23398;&#20064;&#20851;&#32852;&#35268;&#21017;&#21464;&#24471;&#21487;&#34892;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21487;&#20197;&#20174;&#33258;&#21160;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#28508;&#22312;&#34920;&#31034;&#20013;&#25552;&#21462;&#35821;&#20041;&#20851;&#32852;&#35268;&#21017;&#65292;&#24182;&#19988;&#27492;&#26041;&#27861;&#36895;&#24230;&#24555;&#19978;&#30334;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18133v1 Announce Type: cross  Abstract: Association Rule Mining (ARM) is the task of learning associations among data features in the form of logical rules. Mining association rules from high-dimensional numerical data, for example, time series data from a large number of sensors in a smart environment, is a computationally intensive task. In this study, we propose an Autoencoder-based approach to learn and extract association rules from time series data (AE SemRL). Moreover, we argue that in the presence of semantic information related to time series data sources, semantics can facilitate learning generalizable and explainable association rules. Despite enriching time series data with additional semantic features, AE SemRL makes learning association rules from high-dimensional data feasible. Our experiments show that semantic association rules can be extracted from a latent representation created by an Autoencoder and this method has in the order of hundreds of times faster
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#25968;&#25454;&#27969;&#65292;&#25512;&#33616;&#26080;&#25968;&#25454;&#30340;&#31867;&#36882;&#22686;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26041;&#27861;&#32988;&#36807;&#31454;&#20105;&#22522;&#32447;</title><link>https://arxiv.org/abs/2403.18132</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#25968;&#25454;&#25512;&#33616;&#26080;&#25968;&#25454;&#30340;&#31867;&#36882;&#22686;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recommendation of data-free class-incremental learning algorithms by simulating future data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18132
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#25968;&#25454;&#27969;&#65292;&#25512;&#33616;&#26080;&#25968;&#25454;&#30340;&#31867;&#36882;&#22686;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26041;&#27861;&#32988;&#36807;&#31454;&#20105;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#36882;&#22686;&#23398;&#20064;&#22788;&#29702;&#30001;&#31867;&#21035;&#25209;&#27425;&#32452;&#25104;&#30340;&#39034;&#24207;&#25968;&#25454;&#27969;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#31639;&#27861;&#26469;&#35299;&#20915;&#20174;&#26080;&#27861;&#23384;&#20648;&#36807;&#21435;&#31867;&#21035;&#30340;&#26679;&#26412;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#20026;&#29992;&#25143;&#23450;&#20041;&#30340;&#35774;&#32622;&#36873;&#25321;&#36866;&#24403;&#30340;&#31639;&#27861;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#65292;&#22240;&#20026;&#36825;&#20123;&#31639;&#27861;&#30340;&#30456;&#23545;&#24615;&#33021;&#21462;&#20915;&#20110;&#36882;&#22686;&#35774;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#25311;&#26410;&#26469;&#25968;&#25454;&#27969;&#30340;&#31639;&#27861;&#25512;&#33616;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#21021;&#22987;&#31867;&#21035;&#38598;&#21512;&#65292;&#23427;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#20174;&#30456;&#21516;&#30340;&#35270;&#35273;&#22495;&#27169;&#25311;&#26410;&#26469;&#31867;&#21035;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#27969;&#19978;&#35780;&#20272;&#20102;&#26368;&#36817;&#30340;&#31639;&#27861;&#65292;&#24182;&#25512;&#33616;&#22312;&#29992;&#25143;&#23450;&#20041;&#30340;&#36882;&#22686;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20845;&#31181;&#31639;&#27861;&#21644;&#20845;&#20010;&#36882;&#22686;&#35774;&#32622;&#65292;&#22312;&#19977;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#31454;&#20105;&#22522;&#32447;&#65292;&#24182;&#19988;&#24615;&#33021;&#25509;&#36817;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18132v1 Announce Type: cross  Abstract: Class-incremental learning deals with sequential data streams composed of batches of classes. Various algorithms have been proposed to address the challenging case where samples from past classes cannot be stored. However, selecting an appropriate algorithm for a user-defined setting is an open problem, as the relative performance of these algorithms depends on the incremental settings. To solve this problem, we introduce an algorithm recommendation method that simulates the future data stream. Given an initial set of classes, it leverages generative models to simulate future classes from the same visual domain. We evaluate recent algorithms on the simulated stream and recommend the one which performs best in the user-defined incremental setting. We illustrate the effectiveness of our method on three large datasets using six algorithms and six incremental settings. Our method outperforms competitive baselines, and performance is close 
&lt;/p&gt;</description></item><item><title>HealthGAT&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#23618;&#26041;&#27861;&#29983;&#25104;EHR&#23884;&#20837;&#65292;&#24182;&#24341;&#20837;&#23450;&#21046;&#30340;&#20197;EHR&#20026;&#20013;&#24515;&#30340;&#36741;&#21161;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20174;&#32780;&#22312;&#21307;&#23398;&#20195;&#30721;&#23884;&#20837;&#21644;&#22797;&#26434;&#21307;&#23398;&#20851;&#31995;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.18128</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#30340;HealthGAT
&lt;/p&gt;
&lt;p&gt;
HealthGAT: Node Classifications in Electronic Health Records using Graph Attention Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18128
&lt;/p&gt;
&lt;p&gt;
HealthGAT&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#23618;&#26041;&#27861;&#29983;&#25104;EHR&#23884;&#20837;&#65292;&#24182;&#24341;&#20837;&#23450;&#21046;&#30340;&#20197;EHR&#20026;&#20013;&#24515;&#30340;&#36741;&#21161;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20174;&#32780;&#22312;&#21307;&#23398;&#20195;&#30721;&#23884;&#20837;&#21644;&#22797;&#26434;&#21307;&#23398;&#20851;&#31995;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#22312;&#21307;&#30103;&#20445;&#20581;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#22823;&#22810;&#25968;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#21407;&#22987;&#65288;&#34920;&#26684;&#65289;&#26684;&#24335;&#30340;EHRs&#12290;&#20381;&#36182;&#21407;&#22987;&#25110;&#31616;&#21333;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#21487;&#33021;&#20250;&#22823;&#22823;&#38480;&#21046;&#21033;&#29992;EHR&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#25110;&#36866;&#29992;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HealthGAT&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26694;&#26550;&#65292;&#37319;&#29992;&#20998;&#23618;&#26041;&#27861;&#20174;EHR&#29983;&#25104;&#23884;&#20837;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#21307;&#30103;&#20195;&#30721;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;EHR&#25968;&#25454;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#23450;&#21046;&#30340;&#20197;EHR&#20026;&#20013;&#24515;&#30340;&#36741;&#21161;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#21033;&#29992;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#20016;&#23500;&#21307;&#23398;&#30693;&#35782;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#22797;&#26434;&#21307;&#23398;&#20851;&#31995;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;&#25968;&#25454;&#34920;&#31034;&#25216;&#26415;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;HealthGAT&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;va&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18128v1 Announce Type: new  Abstract: While electronic health records (EHRs) are widely used across various applications in healthcare, most applications use the EHRs in their raw (tabular) format. Relying on raw or simple data pre-processing can greatly limit the performance or even applicability of downstream tasks using EHRs. To address this challenge, we present HealthGAT, a novel graph attention network framework that utilizes a hierarchical approach to generate embeddings from EHR, surpassing traditional graph-based methods. Our model iteratively refines the embeddings for medical codes, resulting in improved EHR data analysis. We also introduce customized EHR-centric auxiliary pre-training tasks to leverage the rich medical knowledge embedded within the data. This approach provides a comprehensive analysis of complex medical relationships and offers significant advancement over standard data representation techniques. HealthGAT has demonstrated its effectiveness in va
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32416;&#27491;&#20102;&#20266;&#23545;&#25968;&#20284;&#28982;&#26041;&#27861;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.18127</link><description>&lt;p&gt;
&#20462;&#27491;&#20266;&#23545;&#25968;&#20284;&#28982;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Correction of Pseudo Log-Likelihood Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32416;&#27491;&#20102;&#20266;&#23545;&#25968;&#20284;&#28982;&#26041;&#27861;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#23545;&#25968;&#20284;&#28982;&#26159;&#19968;&#31181;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#26041;&#27861;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#20048;&#38431;&#12289;&#31038;&#20132;&#32593;&#32476;&#30340;&#24433;&#21709;&#26368;&#22823;&#21270;&#21644;&#22240;&#26524;&#20048;&#38431;&#12290;&#28982;&#32780;&#65292;&#22312;&#20808;&#21069;&#30340;&#25991;&#29486;&#20013;&#65292;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#21487;&#33021;&#27809;&#26377;&#30028;&#38480;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20182;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#27809;&#26377;&#23450;&#20041;&#12290;&#26412;&#25991;&#32473;&#20986;&#20102;&#19968;&#20010;&#26368;&#22823;&#20266;&#23545;&#25968;&#20284;&#28982;&#20272;&#35745;&#22833;&#36133;&#30340;&#21453;&#20363;&#65292;&#28982;&#21518;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#26469;&#32416;&#27491;\citep{li2017provably, zhang2022online, xiong2022combinatorial, feng2023combinatorial1, feng2023combinatorial2}&#20013;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18127v1 Announce Type: new  Abstract: Pseudo log-likelihood is a type of maximum likelihood estimation (MLE) method used in various fields including contextual bandits, influence maximization of social networks, and causal bandits. However, in previous literature \citep{li2017provably, zhang2022online, xiong2022combinatorial, feng2023combinatorial1, feng2023combinatorial2}, the log-likelihood function may not be bounded, which may result in the algorithm they proposed not well-defined. In this paper, we give a counterexample that the maximum pseudo log-likelihood estimation fails and then provide a solution to correct the algorithms in \citep{li2017provably, zhang2022online, xiong2022combinatorial, feng2023combinatorial1, feng2023combinatorial2}.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#38750;&#27491;&#24335;&#30340;&#25968;&#23398;&#38472;&#36848;&#32763;&#35793;&#20026;&#24418;&#24335;&#30340;Isabelle&#20195;&#30721;&#24182;&#36827;&#34892;&#33258;&#21160;&#39564;&#35777;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#21160;&#25298;&#32477;&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#26041;&#38754;&#19982;&#24418;&#24335;&#21270;&#38382;&#39064;&#38472;&#36848;&#19981;&#19968;&#33268;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.18120</link><description>&lt;p&gt;
&#19981;&#35201;&#30456;&#20449;&#65306;&#39564;&#35777;--&#29992;&#33258;&#21160;&#24418;&#24335;&#21270;&#20026;&#22522;&#30784;&#30340;LLM&#23450;&#37327;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18120
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#38750;&#27491;&#24335;&#30340;&#25968;&#23398;&#38472;&#36848;&#32763;&#35793;&#20026;&#24418;&#24335;&#30340;Isabelle&#20195;&#30721;&#24182;&#36827;&#34892;&#33258;&#21160;&#39564;&#35777;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#21160;&#25298;&#32477;&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#26041;&#38754;&#19982;&#24418;&#24335;&#21270;&#38382;&#39064;&#38472;&#36848;&#19981;&#19968;&#33268;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;Google&#30340;Minerva&#21644;OpenAI&#30340;GPT&#31995;&#21015;&#65292;&#27491;&#22312;&#36234;&#26469;&#36234;&#33021;&#22815;&#35299;&#20915;&#25968;&#23398;&#23450;&#37327;&#25512;&#29702;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#27493;&#39588;&#21644;&#31572;&#26696;&#20013;&#20173;&#28982;&#23384;&#22312;&#27809;&#26377;&#29702;&#30001;&#30340;&#36923;&#36753;&#21644;&#35745;&#31639;&#38169;&#35823;&#12290;&#26412;&#25991;&#21033;&#29992;LLMs&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#21253;&#21547;&#36275;&#22815;&#22810;&#30340;&#24418;&#24335;&#21270;&#25968;&#23398;&#31034;&#20363;&#65288;&#20363;&#22914;&#22312;Isabelle&#20013;&#65292;&#19968;&#20010;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#29615;&#22659;&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#34987;&#25552;&#31034;&#23558;&#38750;&#27491;&#24335;&#30340;&#25968;&#23398;&#38472;&#36848;&#32763;&#35793;&#21363;&#33258;&#21160;&#24418;&#24335;&#21270;&#20026;&#24418;&#24335;&#30340;Isabelle&#20195;&#30721;--&#35813;&#20195;&#30721;&#21487;&#20197;&#34987;&#33258;&#21160;&#39564;&#35777;&#20869;&#37096;&#19968;&#33268;&#24615;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#21160;&#25298;&#32477;&#37027;&#20123;&#20854;&#24418;&#24335;&#21270;&#29256;&#26412;&#22312;&#20854;&#20869;&#37096;&#25110;&#19982;&#24418;&#24335;&#21270;&#38382;&#39064;&#38472;&#36848;&#19981;&#19968;&#33268;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;GSM8K&#12289;MATH&#21644;MultiArith&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#30452;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18120v1 Announce Type: new  Abstract: Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22836;&#37096;&#23039;&#21183;&#20272;&#35745;&#20013;&#30340;&#22352;&#26631;&#31995;&#21644;&#27431;&#25289;&#35282;&#39034;&#24207;&#23450;&#20041;&#38382;&#39064;&#65292;&#39564;&#35777;&#20102;&#20043;&#21069;&#20316;&#21697;&#20013;&#20351;&#29992;&#30340;&#23039;&#21183;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#21644;&#32472;&#22270;&#20363;&#31243;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18104</link><description>&lt;p&gt;
&#25968;&#23398;&#22522;&#30784;&#21644;&#23436;&#25972;&#22836;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Mathematical Foundation and Corrections for Full Range Head Pose Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18104
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22836;&#37096;&#23039;&#21183;&#20272;&#35745;&#20013;&#30340;&#22352;&#26631;&#31995;&#21644;&#27431;&#25289;&#35282;&#39034;&#24207;&#23450;&#20041;&#38382;&#39064;&#65292;&#39564;&#35777;&#20102;&#20043;&#21069;&#20316;&#21697;&#20013;&#20351;&#29992;&#30340;&#23039;&#21183;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#21644;&#32472;&#22270;&#20363;&#31243;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26377;&#20851;&#22836;&#37096;&#23039;&#21183;&#20272;&#35745;&#65288;HPE&#65289;&#30340;&#20316;&#21697;&#25552;&#20379;&#20102;&#29992;&#20110;&#20174;&#38754;&#37096;&#20851;&#38190;&#28857;&#25110;&#30452;&#25509;&#20174;&#22836;&#37096;&#21306;&#22495;&#22270;&#20687;&#20013;&#25552;&#21462;&#27431;&#25289;&#35282;&#30340;&#31639;&#27861;&#25110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20316;&#21697;&#26410;&#25552;&#20379;&#25152;&#20351;&#29992;&#30340;&#22352;&#26631;&#31995;&#21644;&#27431;&#25289;&#25110;Tait-Bryan&#35282;&#24230;&#39034;&#24207;&#30340;&#28165;&#26224;&#23450;&#20041;&#12290;&#26412;&#25991;&#24443;&#24213;&#26816;&#26597;&#20102;300W-LP&#25968;&#25454;&#38598;&#20013;&#23450;&#20041;&#30340;&#27431;&#25289;&#35282;&#65292;&#22836;&#37096;&#23039;&#21183;&#20272;&#35745;&#22914;3DDFA-v2&#12289;6D-RepNet&#12289;WHENet&#31561;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#27431;&#25289;&#35282;&#32472;&#21046;&#20363;&#31243;&#30340;&#26377;&#25928;&#24615;&#12290;&#24517;&#35201;&#26102;&#65292;&#25105;&#20204;&#25512;&#26029;&#23427;&#20204;&#30340;&#22352;&#26631;&#31995;&#21644;&#20559;&#33322;&#12289;&#27178;&#28378;&#12289;&#20463;&#20208;&#30340;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18104v1 Announce Type: cross  Abstract: Numerous works concerning head pose estimation (HPE) offer algorithms or proposed neural network-based approaches for extracting Euler angles from either facial key points or directly from images of the head region. However, many works failed to provide clear definitions of the coordinate systems and Euler or Tait-Bryan angles orders in use. It is a well-known fact that rotation matrices depend on coordinate systems, and yaw, roll, and pitch angles are sensitive to their application order. Without precise definitions, it becomes challenging to validate the correctness of the output head pose and drawing routines employed in prior works. In this paper, we thoroughly examined the Euler angles defined in the 300W-LP dataset, head pose estimation such as 3DDFA-v2, 6D-RepNet, WHENet, etc, and the validity of their drawing routines of the Euler angles. When necessary, we infer their coordinate system and sequence of yaw, roll, pitch from pro
&lt;/p&gt;</description></item><item><title>&#35813;&#25945;&#31243;&#35752;&#35770;&#20102;&#22270;&#20687;&#21644;&#35270;&#35273;&#39046;&#22495;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#29702;&#24565;&#65292;&#36866;&#21512;&#23545;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#25110;&#24212;&#29992;&#24863;&#20852;&#36259;&#30340;&#26412;&#31185;&#29983;&#21644;&#30740;&#31350;&#29983;&#12290;</title><link>https://arxiv.org/abs/2403.18103</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#20687;&#21644;&#35270;&#35273;&#25193;&#25955;&#27169;&#22411;&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Tutorial on Diffusion Models for Imaging and Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25945;&#31243;&#35752;&#35770;&#20102;&#22270;&#20687;&#21644;&#35270;&#35273;&#39046;&#22495;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#29702;&#24565;&#65292;&#36866;&#21512;&#23545;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#25110;&#24212;&#29992;&#24863;&#20852;&#36259;&#30340;&#26412;&#31185;&#29983;&#21644;&#30740;&#31350;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#29983;&#25104;&#24037;&#20855;&#30340;&#24778;&#20154;&#22686;&#38271;&#20351;&#24471;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#31561;&#35768;&#22810;&#20196;&#20154;&#20852;&#22859;&#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#20123;&#29983;&#25104;&#24037;&#20855;&#32972;&#21518;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#25193;&#25955;&#27010;&#24565;&#65292;&#19968;&#31181;&#29305;&#27530;&#30340;&#37319;&#26679;&#26426;&#21046;&#65292;&#20811;&#26381;&#20102;&#20197;&#21069;&#26041;&#27861;&#20013;&#34987;&#35748;&#20026;&#22256;&#38590;&#30340;&#19968;&#20123;&#32570;&#28857;&#12290;&#26412;&#25945;&#31243;&#30340;&#30446;&#26631;&#26159;&#35752;&#35770;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#29702;&#24565;&#12290;&#26412;&#25945;&#31243;&#30340;&#30446;&#26631;&#21463;&#20247;&#21253;&#25324;&#23545;&#30740;&#31350;&#25193;&#25955;&#27169;&#22411;&#25110;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#35299;&#20915;&#20854;&#20182;&#38382;&#39064;&#24863;&#20852;&#36259;&#30340;&#26412;&#31185;&#29983;&#21644;&#30740;&#31350;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18103v1 Announce Type: new  Abstract: The astonishing growth of generative tools in recent years has empowered many exciting applications in text-to-image generation and text-to-video generation. The underlying principle behind these generative tools is the concept of diffusion, a particular sampling mechanism that has overcome some shortcomings that were deemed difficult in the previous approaches. The goal of this tutorial is to discuss the essential ideas underlying the diffusion models. The target audience of this tutorial includes undergraduate and graduate students who are interested in doing research on diffusion models or applying these models to solve other problems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;&#32422;&#26463;&#32858;&#31867;&#26041;&#27861;ECS&#65292;&#26088;&#22312;&#25214;&#21040;&#39640;&#36136;&#37327;&#19988;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#65292;&#24378;&#35843;&#22312;&#26500;&#24314;&#32858;&#31867;&#26102;&#24212;&#32771;&#34385;&#32463;&#20856;&#32858;&#31867;&#26631;&#20934;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18101</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#65306;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#22768;&#26126;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Clustering: A Constrained Declarative based Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18101
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;&#32422;&#26463;&#32858;&#31867;&#26041;&#27861;ECS&#65292;&#26088;&#22312;&#25214;&#21040;&#39640;&#36136;&#37327;&#19988;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#65292;&#24378;&#35843;&#22312;&#26500;&#24314;&#32858;&#31867;&#26102;&#24212;&#32771;&#34385;&#32463;&#20856;&#32858;&#31867;&#26631;&#20934;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;AI&#39046;&#22495;&#22312;&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#37117;&#22791;&#21463;&#20851;&#27880;&#65292;&#32780;&#22312;&#32858;&#31867;&#20013;&#26356;&#20026;&#37325;&#35201;&#65292;&#32858;&#31867;&#26159;&#19968;&#39033;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#20854;&#32467;&#26524;&#24517;&#39035;&#30001;&#39046;&#22495;&#19987;&#23478;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#22312;&#32463;&#20856;&#32858;&#31867;&#26631;&#20934;&#26041;&#38754;&#20855;&#26377;&#39640;&#36136;&#37327;&#19988;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#26500;&#24314;&#32858;&#31867;&#26102;&#24517;&#39035;&#32771;&#34385;&#36825;&#20004;&#20010;&#32500;&#24230;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#32858;&#31867;&#30340;&#19968;&#20010;&#33391;&#22909;&#30340;&#20840;&#23616;&#35299;&#37322;&#24212;&#35813;&#32771;&#34385;&#27599;&#20010;&#31751;&#30340;&#29305;&#24449;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#25551;&#36848;&#20854;&#23545;&#35937;&#30340;&#33021;&#21147;&#65288;&#35206;&#30422;&#29575;&#65289;&#65292;&#21516;&#26102;&#23558;&#20854;&#19982;&#20854;&#20182;&#31751;&#21306;&#20998;&#24320;&#65288;&#21306;&#20998;&#24230;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#30693;&#35782;&#19987;&#23478;&#22312;&#26399;&#26395;&#32858;&#31867;&#30340;&#32467;&#26500;&#25110;&#20854;&#35299;&#37322;&#26041;&#38754;&#30340;&#19981;&#21516;&#32423;&#21035;&#30340;&#30693;&#35782;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#19968;&#20010;&#31751;&#30340;&#35299;&#37322;&#26159;&#19968;&#32452;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;&#32422;&#26463;&#32858;&#31867;&#26041;&#27861;&#65292;&#31216;&#20026;ECS&#26469;&#25903;&#25345;&#22768;&#26126;&#24615;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18101v1 Announce Type: new  Abstract: The domain of explainable AI is of interest in all Machine Learning fields, and it is all the more important in clustering, an unsupervised task whose result must be validated by a domain expert. We aim at finding a clustering that has high quality in terms of classic clustering criteria and that is explainable, and we argue that these two dimensions must be considered when building the clustering. We consider that a good global explanation of a clustering should give the characteristics of each cluster taking into account their abilities to describe its objects (coverage) while distinguishing it from the other clusters (discrimination). Furthermore, we aim at leveraging expert knowledge, at different levels, on the structure of the expected clustering or on its explanations. In our framework an explanation of a cluster is a set of patterns, and we propose a novel interpretable constrained clustering method called ECS for declarative clu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36793;&#32536;&#35745;&#31639;&#21644;&#29289;&#32852;&#32593;&#30340;&#32452;&#21512;&#65292;&#21487;&#20197;&#20943;&#23569;&#24310;&#36831;&#65292;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.18100</link><description>&lt;p&gt;
&#36890;&#36807;&#20113;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#26234;&#33021;&#29289;&#32852;&#32593;&#30417;&#25511;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Driving Intelligent IoT Monitoring and Control through Cloud Computing and Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18100
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36793;&#32536;&#35745;&#31639;&#21644;&#29289;&#32852;&#32593;&#30340;&#32452;&#21512;&#65292;&#21487;&#20197;&#20943;&#23569;&#24310;&#36831;&#65292;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#20113;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#26234;&#33021;&#29289;&#32852;&#32593;&#30340;&#30417;&#25511;&#21644;&#25511;&#21046;&#12290;&#38543;&#30528;&#29289;&#32852;&#32593;&#21644;&#20113;&#32487;&#32493;&#22312;&#32593;&#32476;&#20013;&#29983;&#25104;&#22823;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#20316;&#20026;&#20256;&#24863;&#22120;&#35774;&#22791;&#65292;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#34987;&#21457;&#36865;&#21040;&#20113;&#31471;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#12289;&#39044;&#27979;&#21644;&#25968;&#25454;&#20998;&#26512;&#20197;&#23454;&#29616;&#19994;&#21153;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20113;&#35745;&#31639;&#27169;&#22411;&#21463;&#38480;&#20110;&#36317;&#31163;&#65292;&#23545;&#20110;&#32593;&#32476;&#36830;&#25509;&#36136;&#37327;&#19981;&#29702;&#24819;&#30340;&#29615;&#22659;&#21487;&#33021;&#22312;&#20851;&#38190;&#25805;&#20316;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#36793;&#32536;&#35745;&#31639;&#20316;&#20026;&#20998;&#24067;&#24335;&#35745;&#31639;&#26550;&#26500;&#65292;&#23558;&#22788;&#29702;&#24212;&#29992;&#31243;&#24207;&#12289;&#25968;&#25454;&#21644;&#26381;&#21153;&#30340;&#20301;&#32622;&#20174;&#32593;&#32476;&#30340;&#20013;&#24515;&#33410;&#28857;&#31227;&#21160;&#21040;&#32593;&#32476;&#30340;&#36923;&#36753;&#36793;&#32536;&#33410;&#28857;&#65292;&#20197;&#20943;&#23569;&#23545;&#20113;&#22788;&#29702;&#21644;&#25968;&#25454;&#20998;&#26512;&#30340;&#20381;&#36182;&#65292;&#23454;&#29616;&#36817;&#31471;&#25968;&#25454;&#22788;&#29702;&#21644;&#20998;&#26512;&#12290;&#29289;&#32852;&#32593;&#21644;&#36793;&#32536;&#35745;&#31639;&#30340;&#32467;&#21512;&#21487;&#20197;&#20943;&#23569;&#24310;&#36831;&#65292;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18100v1 Announce Type: new  Abstract: This article explores how to drive intelligent iot monitoring and control through cloud computing and machine learning. As iot and the cloud continue to generate large and diverse amounts of data as sensor devices in the network, the collected data is sent to the cloud for statistical analysis, prediction, and data analysis to achieve business objectives. However, because the cloud computing model is limited by distance, it can be problematic in environments where the quality of the Internet connection is not ideal for critical operations. Therefore, edge computing, as a distributed computing architecture, moves the location of processing applications, data and services from the central node of the network to the logical edge node of the network to reduce the dependence on cloud processing and analysis of data, and achieve near-end data processing and analysis. The combination of iot and edge computing can reduce latency, improve efficie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#35270;&#39057;&#22522;&#20110;&#25163;&#37096;&#20998;&#31867;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#20041;&#32858;&#31867;&#33258;&#21160;&#35782;&#21035;&#39048;&#26894;&#33034;&#39635;&#25439;&#20260;&#24739;&#32773;&#23478;&#24237;&#20013;&#30340;&#20027;&#35201;&#25163;&#37096;&#25569;&#25345;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.18094</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#37096;&#20998;&#31867;&#65306;&#33034;&#39635;&#25439;&#20260;&#24739;&#32773;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Personalized Video-Based Hand Taxonomy: Application for Individuals with Spinal Cord Injury
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#35270;&#39057;&#22522;&#20110;&#25163;&#37096;&#20998;&#31867;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#20041;&#32858;&#31867;&#33258;&#21160;&#35782;&#21035;&#39048;&#26894;&#33034;&#39635;&#25439;&#20260;&#24739;&#32773;&#23478;&#24237;&#20013;&#30340;&#20027;&#35201;&#25163;&#37096;&#25569;&#25345;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#37096;&#21151;&#33021;&#23545;&#20154;&#31867;&#30340;&#20114;&#21160;&#21644;&#29983;&#27963;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#33034;&#39635;&#25439;&#20260;&#65288;SCI&#65289;&#21487;&#33021;&#25439;&#23475;&#25163;&#37096;&#21151;&#33021;&#65292;&#38477;&#20302;&#29420;&#31435;&#24615;&#12290;&#22312;&#23478;&#24237;&#21644;&#31038;&#21306;&#29615;&#22659;&#20013;&#23545;&#21151;&#33021;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#38656;&#35201;&#38024;&#23545;&#25163;&#37096;&#21151;&#33021;&#21463;&#25439;&#20010;&#20307;&#30340;&#25163;&#25569;&#20998;&#31867;&#12290;&#30001;&#20110;&#26631;&#20934;&#20998;&#31867;&#20013;&#26410;&#20195;&#34920;&#30340;&#25569;&#25345;&#31867;&#22411;&#12289;&#20260;&#23475;&#27700;&#24179;&#38388;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#21644;&#26377;&#38480;&#25968;&#25454;&#65292;&#24320;&#21457;&#36825;&#26679;&#19968;&#31181;&#20998;&#31867;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#35821;&#20041;&#32858;&#31867;&#33258;&#21160;&#35782;&#21035;&#20027;&#35201;&#30340;&#29420;&#29305;&#25163;&#37096;&#25569;&#25345;&#21160;&#20316;&#65292;&#22312;&#20027;&#20307;&#35270;&#35282;&#35270;&#39057;&#20013;&#12290;19&#21517;&#39048;&#26894;SCI&#20010;&#20307;&#23478;&#20013;&#25910;&#38598;&#30340;&#20027;&#20307;&#35270;&#35282;&#35270;&#39057;&#34987;&#29992;&#26469;&#23545;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#25569;&#25345;&#21160;&#20316;&#36827;&#34892;&#32858;&#31867;&#12290;&#37319;&#29992;&#25972;&#21512;&#23039;&#21183;&#21644;&#22806;&#35266;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#21019;&#24314;&#20010;&#24615;&#21270;&#30340;&#25163;&#37096;&#20998;&#31867;&#12290;&#23450;&#37327;&#20998;&#26512;&#26174;&#31034;&#65292;&#32858;&#31867;&#32431;&#24230;&#20026;67.6%+-24.2%&#65292;&#20887;&#20313;&#24230;&#20026;18.0%+-21.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18094v1 Announce Type: cross  Abstract: Hand function is critical for our interactions and quality of life. Spinal cord injuries (SCI) can impair hand function, reducing independence. A comprehensive evaluation of function in home and community settings requires a hand grasp taxonomy for individuals with impaired hand function. Developing such a taxonomy is challenging due to unrepresented grasp types in standard taxonomies, uneven data distribution across injury levels, and limited data. This study aims to automatically identify the dominant distinct hand grasps in egocentric video using semantic clustering. Egocentric video recordings collected in the homes of 19 individual with cervical SCI were used to cluster grasping actions with semantic significance. A deep learning model integrating posture and appearance data was employed to create a personalized hand taxonomy. Quantitative analysis reveals a cluster purity of 67.6% +- 24.2% with with 18.0% +- 21.8% redundancy. Qua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#24207;&#21015;&#65292;&#25506;&#35752;&#28385;&#36275;&#29305;&#23450;&#32422;&#26463;&#30340;&#31574;&#30053;&#36335;&#24452;&#65292;&#21363;&#28385;&#36275;&#36335;&#24452;&#65292;&#23545;&#20110;&#26500;&#24314;&#32456;&#27490;&#20110;&#22343;&#34913;&#31574;&#30053;&#30340;&#36335;&#24452;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.18079</link><description>&lt;p&gt;
&#27491;&#24577;&#24418;&#24335;&#21338;&#24328;&#20013;&#30340;&#22343;&#34913;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Paths to Equilibrium in Normal-Form Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#24207;&#21015;&#65292;&#25506;&#35752;&#28385;&#36275;&#29305;&#23450;&#32422;&#26463;&#30340;&#31574;&#30053;&#36335;&#24452;&#65292;&#21363;&#28385;&#36275;&#36335;&#24452;&#65292;&#23545;&#20110;&#26500;&#24314;&#32456;&#27490;&#20110;&#22343;&#34913;&#31574;&#30053;&#30340;&#36335;&#24452;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#65292;&#26234;&#20307;&#20250;&#21453;&#22797;&#22312;&#26102;&#38388;&#19978;&#20132;&#20114;&#65292;&#24182;&#38543;&#30528;&#26032;&#25968;&#25454;&#30340;&#21040;&#26469;&#20462;&#35746;&#20182;&#20204;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#31995;&#21015;&#31574;&#30053;&#27010;&#20917;&#12290;&#26412;&#25991;&#30740;&#31350;&#28385;&#36275;&#19968;&#31181;&#30001;&#24378;&#21270;&#23398;&#20064;&#20013;&#25919;&#31574;&#26356;&#26032;&#21551;&#21457;&#30340;&#25104;&#23545;&#32422;&#26463;&#30340;&#31574;&#30053;&#24207;&#21015;&#65292;&#20854;&#20013;&#22312;&#31532; $t$ &#26399;&#26368;&#20248;&#24212;&#31572;&#30340;&#26234;&#20307;&#22312;&#19979;&#19968;&#26399; $t+1$ &#19981;&#20250;&#25913;&#21464;&#20854;&#31574;&#30053;&#12290;&#36825;&#31181;&#32422;&#26463;&#20165;&#35201;&#27714;&#20248;&#21270;&#26234;&#20307;&#19981;&#26356;&#25913;&#31574;&#30053;&#65292;&#20294;&#24182;&#19981;&#20197;&#20219;&#20309;&#26041;&#24335;&#38480;&#21046;&#20854;&#20182;&#38750;&#26368;&#20248;&#21270;&#26234;&#20307;&#65292;&#22240;&#27492;&#20801;&#35768;&#25506;&#32034;&#12290;&#20855;&#26377;&#27492;&#23646;&#24615;&#30340;&#24207;&#21015;&#34987;&#31216;&#20026;&#28385;&#36275;&#36335;&#24452;&#65292;&#24182;&#22312;&#35768;&#22810; MARL &#31639;&#27861;&#20013;&#33258;&#28982;&#20986;&#29616;&#12290;&#20851;&#20110;&#25112;&#30053;&#21160;&#24577;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#21338;&#24328;&#21644;&#21021;&#22987;&#31574;&#30053;&#27010;&#20917;&#65292;&#26159;&#21542;&#24635;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#32456;&#27490;&#20110;&#22343;&#34913;&#31574;&#30053;&#30340;&#28385;&#36275;&#36335;&#24452;&#65311;&#36825;&#20010;&#38382;&#39064;&#30340;&#35299;&#20915;&#23545;&#24212;&#30528;&#19968;&#20123;&#37325;&#35201;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18079v1 Announce Type: cross  Abstract: In multi-agent reinforcement learning (MARL), agents repeatedly interact across time and revise their strategies as new data arrives, producing a sequence of strategy profiles. This paper studies sequences of strategies satisfying a pairwise constraint inspired by policy updating in reinforcement learning, where an agent who is best responding in period $t$ does not switch its strategy in the next period $t+1$. This constraint merely requires that optimizing agents do not switch strategies, but does not constrain the other non-optimizing agents in any way, and thus allows for exploration. Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms. A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium strategy? The resolution of this question has implication
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#39044;&#27979;&#30446;&#26631;&#23548;&#21521;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;QoIs&#30340;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#26469;&#30830;&#23450;&#23454;&#39564;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.18072</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#30446;&#26631;&#23548;&#21521;&#36125;&#21494;&#26031;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#19982;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Goal-Oriented Bayesian Optimal Experimental Design for Nonlinear Models using Markov Chain Monte Carlo
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18072
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#39044;&#27979;&#30446;&#26631;&#23548;&#21521;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;QoIs&#30340;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#26469;&#30830;&#23450;&#23454;&#39564;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#65288;OED&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#21644;&#26368;&#22823;&#21270;&#23454;&#39564;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#22312;&#36125;&#21494;&#26031;&#26041;&#27861;&#19979;&#65292;&#20256;&#32479;&#30340;OED&#20250;&#26368;&#22823;&#21270;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#65288;EIG&#65289;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36890;&#24120;&#24863;&#20852;&#36259;&#30340;&#19981;&#26159;&#21442;&#25968;&#26412;&#36523;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#26041;&#24335;&#30340;&#39044;&#27979;&#24863;&#20852;&#36259;&#37327;&#65288;QoIs&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#35266;&#27979;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#39044;&#27979;&#30446;&#26631;&#23548;&#21521;OED&#65288;GO-OED&#65289;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23547;&#27714;&#25552;&#20379;&#23545;QoIs&#30340;&#26368;&#22823;EIG&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;QoI EIG&#30340;&#23884;&#22871;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#65292;&#20854;&#20013;&#37319;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#65292;&#21033;&#29992;&#26680;&#23494;&#24230;&#20272;&#35745;&#26469;&#35780;&#20272;&#21518;&#39564;&#39044;&#27979;&#23494;&#24230;&#21450;&#20854;&#19982;&#20808;&#39564;&#39044;&#27979;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#12290;GO-OED&#35774;&#35745;&#36890;&#36807;&#22312;&#35774;&#35745;&#31354;&#38388;&#20013;&#26368;&#22823;&#21270;EIG&#26469;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18072v1 Announce Type: cross  Abstract: Optimal experimental design (OED) provides a systematic approach to quantify and maximize the value of experimental data. Under a Bayesian approach, conventional OED maximizes the expected information gain (EIG) on model parameters. However, we are often interested in not the parameters themselves, but predictive quantities of interest (QoIs) that depend on the parameters in a nonlinear manner. We present a computational framework of predictive goal-oriented OED (GO-OED) suitable for nonlinear observation and prediction models, which seeks the experimental design providing the greatest EIG on the QoIs. In particular, we propose a nested Monte Carlo estimator for the QoI EIG, featuring Markov chain Monte Carlo for posterior sampling and kernel density estimation for evaluating the posterior-predictive density and its Kullback-Leibler divergence from the prior-predictive. The GO-OED design is then found by maximizing the EIG over the des
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#28023;&#27915;&#22403;&#22334;&#39046;&#22495;&#21462;&#24471;&#20102;&#38271;&#36275;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;YOLO&#31995;&#21015;&#22312;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#30740;&#31350;&#26174;&#31034;&#24403;&#21069;&#32570;&#20047;&#20840;&#38754;&#30340;&#27700;&#19979;&#22403;&#22334;&#25968;&#25454;&#24211;&#65292;&#36825;&#25104;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#29942;&#39048;&#12290;</title><link>https://arxiv.org/abs/2403.18067</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#28023;&#27915;&#22403;&#22334;&#36319;&#36394;&#21644;&#26816;&#27979;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
State of the art applications of deep learning within tracking and detecting marine debris: A survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18067
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#28023;&#27915;&#22403;&#22334;&#39046;&#22495;&#21462;&#24471;&#20102;&#38271;&#36275;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;YOLO&#31995;&#21015;&#22312;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#30740;&#31350;&#26174;&#31034;&#24403;&#21069;&#32570;&#20047;&#20840;&#38754;&#30340;&#27700;&#19979;&#22403;&#22334;&#25968;&#25454;&#24211;&#65292;&#36825;&#25104;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#28023;&#27915;&#22403;&#22334;&#38382;&#39064;&#20013;&#24050;&#32463;&#34987;&#25506;&#32034;&#20102;&#22823;&#32422;20&#24180;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#22312;&#36807;&#21435;&#20116;&#24180;&#20869;&#24555;&#36895;&#21457;&#23637;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26368;&#36817;28&#39033;&#28145;&#24230;&#23398;&#20064;&#22312;&#28023;&#27915;&#22403;&#22334;&#39046;&#22495;&#20013;&#26368;&#26032;&#21644;&#26368;&#37325;&#35201;&#36129;&#29486;&#30340;&#28145;&#20837;&#12289;&#26368;&#26032;&#30340;&#24635;&#32467;&#21644;&#20998;&#26512;&#12290;&#36890;&#36807;&#20132;&#21449;&#24341;&#29992;&#30740;&#31350;&#35770;&#25991;&#32467;&#26524;&#65292;YOLO&#31995;&#21015;&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#35768;&#22810;&#21463;&#23562;&#25964;&#30340;&#36129;&#29486;&#32773;&#26126;&#30830;&#34920;&#31034;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#25104;&#30340;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20840;&#38754;&#27700;&#19979;&#22403;&#22334;&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#20351;&#29992;&#30001;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#21644;&#26631;&#35760;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#22312;&#19968;&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;YOLOv5&#65292;&#24182;&#21457;&#29616;&#20934;&#30830;&#29575;&#36739;&#20302;&#65292;&#20551;&#38451;&#24615;&#29575;&#36739;&#39640;&#65292;&#31361;&#26174;&#20102;&#20840;&#38754;&#25968;&#25454;&#24211;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#24635;&#32467;&#20102;&#36229;&#36807;40&#39033;&#26410;&#26469;&#30340;&#30740;&#31350;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18067v1 Announce Type: cross  Abstract: Deep learning techniques have been explored within the marine litter problem for approximately 20 years but the majority of the research has developed rapidly in the last five years. We provide an in-depth, up to date, summary and analysis of 28 of the most recent and significant contributions of deep learning in marine debris. From cross referencing the research paper results, the YOLO family significantly outperforms all other methods of object detection but there are many respected contributions to this field that have categorically agreed that a comprehensive database of underwater debris is not currently available for machine learning. Using a small dataset curated and labelled by us, we tested YOLOv5 on a binary classification task and found the accuracy was low and the rate of false positives was high; highlighting the importance of a comprehensive database. We conclude this survey with over 40 future research recommendations an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120; (SCT)&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#20449;&#24687;&#30340;&#21367;&#31215;&#25805;&#20316;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#22797;&#26434;&#20613;&#37324;&#21494;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;&#30340;&#21327;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18063</link><description>&lt;p&gt;
&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120;&#65306;&#21327;&#35843;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#30340;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18063
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120; (SCT)&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#20449;&#24687;&#30340;&#21367;&#31215;&#25805;&#20316;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#22797;&#26434;&#20613;&#37324;&#21494;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;&#30340;&#21327;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20013;&#20351;&#29992;&#30340;Transformer&#24050;&#32463;&#36890;&#36807;&#21508;&#31181;&#32467;&#26500;&#36827;&#34892;&#20102;&#30740;&#31350; - &#22914;ViT&#12289;PVT&#21644;Swin&#12290;&#36825;&#20123;&#24037;&#20316;&#26088;&#22312;&#25913;&#36827;&#27880;&#24847;&#21147;&#26426;&#21046;&#24182;&#20351;&#20854;&#26356;&#21152;&#39640;&#25928;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#20154;&#20204;&#24863;&#21463;&#21040;&#20102;&#21253;&#21547;&#23616;&#37096;&#20449;&#24687;&#30340;&#38656;&#35201;&#65292;&#36825;&#23548;&#33268;&#22312;Transformer&#20013;&#24341;&#20837;&#21367;&#31215;&#65292;&#22914;CPVT&#21644;CvT&#12290;&#25105;&#20204;&#20351;&#29992;&#22797;&#26434;&#20613;&#31435;&#21494;&#22522;&#30784;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;AFNO&#12289;GFNet&#21644;Spectformer&#23454;&#29616;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#12290;&#25105;&#20204;&#25552;&#20513;&#32467;&#21512;&#25968;&#25454;&#30340;&#19977;&#31181;&#19981;&#21516;&#35270;&#22270; - &#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#38271;&#31243;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20165;&#20351;&#29992;&#23454;&#22495;&#20809;&#35889;&#34920;&#31034;&#30340;&#26368;&#31616;&#21333;&#20840;&#23616;&#34920;&#31034; - &#36890;&#36807;Hartley&#21464;&#25442;&#33719;&#24471;&#12290;&#25105;&#20204;&#22312;&#21021;&#22987;&#23618;&#20013;&#20351;&#29992;&#21367;&#31215;&#31639;&#23376;&#25429;&#25417;&#23616;&#37096;&#20449;&#24687;&#12290;&#36890;&#36807;&#36825;&#20004;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#33021;&#22815;&#20248;&#21270;&#24182;&#33719;&#24471;&#19968;&#20010;&#25552;&#20379;&#25913;&#36827;&#24615;&#33021;&#30340;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120;&#65288;SCT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18063v1 Announce Type: cross  Abstract: Transformers used in vision have been investigated through diverse architectures - ViT, PVT, and Swin. These have worked to improve the attention mechanism and make it more efficient. Differently, the need for including local information was felt, leading to incorporating convolutions in transformers such as CPVT and CvT. Global information is captured using a complex Fourier basis to achieve global token mixing through various methods, such as AFNO, GFNet, and Spectformer. We advocate combining three diverse views of data - local, global, and long-range dependence. We also investigate the simplest global representation using only the real domain spectral representation - obtained through the Hartley transform. We use a convolutional operator in the initial layers to capture local information. Through these two contributions, we are able to optimize and obtain a spectral convolution transformer (SCT) that provides improved performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#30340;R2D2&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#23398;&#20064;&#27169;&#22411;&#31995;&#21015;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#24179;&#22343;&#26041;&#27861;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.18052</link><description>&lt;p&gt;
&#22312;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#37327;&#21270;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;R2D2&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
R2D2 image reconstruction with model uncertainty quantification in radio astronomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#30340;R2D2&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#23398;&#20064;&#27169;&#22411;&#31995;&#21015;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#24179;&#22343;&#26041;&#27861;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#29992;&#20110;&#22825;&#25991;&#23398;&#20013;&#23556;&#30005;&#24178;&#28041;(RI)&#25104;&#20687;&#30340;&#8220;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#27531;&#24046;&#21040;&#27531;&#24046;DNN&#31995;&#21015;&#8221;(R2D2)&#26041;&#27861;&#12290;R2D2&#30340;&#37325;&#24314;&#24418;&#25104;&#20026;&#19968;&#31995;&#21015;&#27531;&#24046;&#22270;&#20687;&#65292;&#36845;&#20195;&#22320;&#20272;&#35745;&#20026;&#20197;&#21069;&#36845;&#20195;&#30340;&#22270;&#20687;&#20272;&#35745;&#21644;&#30456;&#20851;&#25968;&#25454;&#27531;&#24046;&#20026;&#36755;&#20837;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#20854;&#23398;&#20064;&#27169;&#22411;&#31995;&#21015;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#30740;&#31350;R2D2&#22270;&#20687;&#20272;&#35745;&#36807;&#31243;&#30340;&#31283;&#20581;&#24615;&#12290;&#37319;&#29992;&#38598;&#25104;&#24179;&#22343;&#26041;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#22810;&#20010;&#31995;&#21015;&#65292;&#26469;&#33258;&#22312;&#27599;&#27425;&#36845;&#20195;&#36807;&#31243;&#20013;&#23545;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#19981;&#21516;&#38543;&#26426;DNN&#21021;&#22987;&#21270;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#22810;&#20010;R2D2&#23454;&#20363;&#20063;&#21487;&#29992;&#20110;&#20135;&#29983;&#8220;R2D2&#26679;&#26412;&#8221;&#65292;&#20174;&#20013;&#32463;&#39564;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#36171;&#20104;&#31639;&#27861;&#32852;&#21512;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21151;&#33021;&#12290;&#37325;&#28857;&#25918;&#22312;RI imag
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18052v1 Announce Type: cross  Abstract: The ``Residual-to-Residual DNN series for high-Dynamic range imaging'' (R2D2) approach was recently introduced for Radio-Interferometric (RI) imaging in astronomy. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of Deep Neural Networks (DNNs) taking the previous iteration's image estimate and associated data residual as inputs. In this work, we investigate the robustness of the R2D2 image estimation process, by studying the uncertainty associated with its series of learned models. Adopting an ensemble averaging approach, multiple series can be trained, arising from different random DNN initializations of the training process at each iteration. The resulting multiple R2D2 instances can also be leveraged to generate ``R2D2 samples'', from which empirical mean and standard deviation endow the algorithm with a joint estimation and uncertainty quantification functionality. Focusing on RI imag
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#25511;&#21046;&#24212;&#29992;&#30340;&#28145;&#24230;&#22810;&#38754;&#20307;&#33258;&#32534;&#30721;&#22120;&#65292;&#22312;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#35745;&#31639;&#38750;&#32447;&#24615;&#25511;&#21046;&#22120;&#35774;&#35745;&#20013;&#23637;&#29616;&#20986;&#27604;&#26631;&#20934;&#32447;&#24615;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20854;&#29305;&#23450;&#26550;&#26500;&#20351;&#24471;&#23454;&#29616;&#26356;&#39640;&#38454;&#32423;&#25968;&#23637;&#24320;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#35745;&#31639;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2403.18044</link><description>&lt;p&gt;
&#28145;&#24230;&#22810;&#38754;&#20307;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#20302;&#32500;&#32447;&#24615;&#21442;&#25968;&#21464;&#21270;&#36924;&#36817;&#21644;&#38750;&#32447;&#24615;&#21453;&#39304;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep polytopic autoencoders for low-dimensional linear parameter-varying approximations and nonlinear feedback design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18044
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#25511;&#21046;&#24212;&#29992;&#30340;&#28145;&#24230;&#22810;&#38754;&#20307;&#33258;&#32534;&#30721;&#22120;&#65292;&#22312;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#35745;&#31639;&#38750;&#32447;&#24615;&#25511;&#21046;&#22120;&#35774;&#35745;&#20013;&#23637;&#29616;&#20986;&#27604;&#26631;&#20934;&#32447;&#24615;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20854;&#29305;&#23450;&#26550;&#26500;&#20351;&#24471;&#23454;&#29616;&#26356;&#39640;&#38454;&#32423;&#25968;&#23637;&#24320;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#38754;&#20307;&#33258;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#22810;&#38754;&#20307;&#20013;&#29366;&#24577;&#30340;&#20302;&#32500;&#21442;&#25968;&#21270;&#12290;&#23545;&#20110;&#38750;&#32447;&#24615;PDE&#65292;&#36825;&#24456;&#23481;&#26131;&#24212;&#29992;&#20110;&#20302;&#32500;&#32447;&#24615;&#21442;&#25968;&#21464;&#21270;(LPV)&#36924;&#36817;&#65292;&#22240;&#20026;&#23427;&#20204;&#24050;&#34987;&#29992;&#20110;&#36890;&#36807;&#29366;&#24577;&#30456;&#20851;Riccati&#26041;&#31243;&#30340;&#32423;&#25968;&#23637;&#24320;&#23454;&#29616;&#26377;&#25928;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#22120;&#35774;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#25511;&#21046;&#24212;&#29992;&#30340;&#22810;&#38754;&#20307;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#22312;&#35270;&#22270;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;LPV&#36924;&#36817;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#32447;&#24615;&#26041;&#27861;&#65292;&#20197;&#21450;&#29305;&#23450;&#26550;&#26500;&#22914;&#20309;&#22312;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#39640;&#38454;&#32423;&#25968;&#23637;&#24320;&#12290;&#25105;&#20204;&#36890;&#36807;&#24443;&#24213;&#30340;&#25968;&#20540;&#30740;&#31350;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#35745;&#31639;&#38750;&#32447;&#24615;&#25511;&#21046;&#22120;&#35774;&#35745;&#20013;&#30340;&#24615;&#36136;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18044v1 Announce Type: cross  Abstract: Polytopic autoencoders provide low-dimensional parametrizations of states in a polytope. For nonlinear PDEs, this is readily applied to low-dimensional linear parameter-varying (LPV) approximations as they have been exploited for efficient nonlinear controller design via series expansions of the solution to the state-dependent Riccati equation. In this work, we develop a polytopic autoencoder for control applications and show how it outperforms standard linear approaches in view of LPV approximations of nonlinear systems and how the particular architecture enables higher order series expansions at little extra computational effort. We illustrate the properties and potentials of this approach to computational nonlinear controller design for large-scale systems with a thorough numerical study.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;BCM&#65289;&#65292;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#23454;&#29616;&#27839;&#30528;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#21069;&#21521;&#21644;&#21518;&#21521;&#36941;&#21382;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32479;&#19968;&#20102;&#29983;&#25104;&#21644;&#32534;&#36753;&#22270;&#20687;&#31561;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.18035</link><description>&lt;p&gt;
&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Consistency Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18035
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;BCM&#65289;&#65292;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#23454;&#29616;&#27839;&#30528;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#21069;&#21521;&#21644;&#21518;&#21521;&#36941;&#21382;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32479;&#19968;&#20102;&#29983;&#25104;&#21644;&#32534;&#36753;&#22270;&#20687;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#36890;&#36807;&#36845;&#20195;&#21435;&#22122;&#19968;&#20010;&#38543;&#26426;&#21521;&#37327;&#33021;&#22815;&#29983;&#25104;&#38750;&#24120;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#36825;&#20010;&#36807;&#31243;&#23545;&#24212;&#20110;&#27839;&#30528;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;PF ODE&#65289;&#31227;&#21160;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;DMs&#36824;&#21487;&#20197;&#36890;&#36807;&#27839;&#30528;PF ODE&#21521;&#21518;&#31227;&#21160;&#23558;&#36755;&#20837;&#22270;&#20687;&#36716;&#25442;&#20026;&#22122;&#22768;&#65292;&#36825;&#26159;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#25554;&#20540;&#21644;&#22270;&#20687;&#32534;&#36753;&#65289;&#30340;&#20851;&#38190;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36807;&#31243;&#30340;&#36845;&#20195;&#24615;&#36136;&#38480;&#21046;&#20102;&#20854;&#36895;&#24230;&#65292;&#38459;&#30861;&#20102;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CMs&#65289;&#24050;&#32463;&#20986;&#29616;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#36817;&#20284;PF ODE&#30340;&#31215;&#20998;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#38656;&#35201;&#36845;&#20195;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#26174;&#24335;ODE&#27714;&#35299;&#22120;&#20351;&#24471;&#21453;&#28436;&#36807;&#31243;&#22797;&#26434;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;BCM&#65289;&#65292;&#23398;&#20064;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#27839;&#30528;PF ODE&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#36941;&#21382;&#65292;&#26377;&#25928;&#22320;&#32479;&#19968;&#29983;&#25104;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18035v1 Announce Type: new  Abstract: Diffusion models (DMs) are capable of generating remarkably high-quality samples by iteratively denoising a random vector, a process that corresponds to moving along the probability flow ordinary differential equation (PF ODE). Interestingly, DMs can also invert an input image to noise by moving backward along the PF ODE, a key operation for downstream tasks such as interpolation and image editing. However, the iterative nature of this process restricts its speed, hindering its broader application. Recently, Consistency Models (CMs) have emerged to address this challenge by approximating the integral of the PF ODE, thereby bypassing the need to iterate. Yet, the absence of an explicit ODE solver complicates the inversion process. To resolve this, we introduce the Bidirectional Consistency Model (BCM), which learns a single neural network that enables both forward and backward traversal along the PF ODE, efficiently unifying generation an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#37319;&#29992;&#21355;&#26143;&#22270;&#20687;&#21644;&#20854;&#20182;&#29289;&#31181;&#20986;&#29616;&#20449;&#24687;&#26469;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;R-Tran&#65292;&#21487;&#20197;&#21033;&#29992;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18028</link><description>&lt;p&gt;
&#20174;&#37096;&#20998;&#35266;&#27979;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Predicting species occurrence patterns from partial observations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18028
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#37319;&#29992;&#21355;&#26143;&#22270;&#20687;&#21644;&#20854;&#20182;&#29289;&#31181;&#20986;&#29616;&#20449;&#24687;&#26469;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;R-Tran&#65292;&#21487;&#20197;&#21033;&#29992;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24212;&#23545;&#29983;&#29289;&#22810;&#26679;&#24615;&#21644;&#27668;&#20505;&#21361;&#26426;&#65292;&#25105;&#20204;&#38656;&#35201;&#20102;&#35299;&#29289;&#31181;&#20998;&#24067;&#30340;&#20301;&#32622;&#20197;&#21450;&#36825;&#20123;&#27169;&#24335;&#22914;&#20309;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29289;&#31181;&#30340;&#35266;&#27979;&#25968;&#25454;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#65292;&#21487;&#29992;&#25968;&#25454;&#30340;&#37327;&#22312;&#19981;&#21516;&#20998;&#31867;&#32676;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#32473;&#23450;&#21355;&#26143;&#22270;&#20687;&#21644;&#20854;&#20182;&#29289;&#31181;&#20986;&#29616;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;&#12290;&#20026;&#20102;&#22312;&#27492;&#20219;&#21153;&#19978;&#35780;&#20272;&#31639;&#27861;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SatButterfly&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#34676;&#34678;&#30340;&#21355;&#26143;&#22270;&#20687;&#12289;&#29615;&#22659;&#25968;&#25454;&#21644;&#35266;&#27979;&#25968;&#25454;&#65292;&#26088;&#22312;&#19982;&#29616;&#26377;&#30340;&#40479;&#31867;&#35266;&#27979;&#25968;&#25454;&#38598;SatBird&#37197;&#23545;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;R-Tran&#65292;&#29992;&#20110;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#22320;&#26041;&#20351;&#29992;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;R-Tran&#22312;&#39044;&#27979;&#29289;&#31181;&#36973;&#36935;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18028v1 Announce Type: cross  Abstract: To address the interlinked biodiversity and climate crises, we need an understanding of where species occur and how these patterns are changing. However, observational data on most species remains very limited, and the amount of data available varies greatly between taxonomic groups. We introduce the problem of predicting species occurrence patterns given (a) satellite imagery, and (b) known information on the occurrence of other species. To evaluate algorithms on this task, we introduce SatButterfly, a dataset of satellite images, environmental data and observational data for butterflies, which is designed to pair with the existing SatBird dataset of bird observational data. To address this task, we propose a general model, R-Tran, for predicting species occurrence patterns that enables the use of partial observational data wherever found. We find that R-Tran outperforms other methods in predicting species encounter rates with partial
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#20102;&#20849;&#32858;&#28966;&#26174;&#24494;&#38236;&#21644;&#24191;&#22330;&#33639;&#20809;&#26174;&#24494;&#38236;&#20043;&#38388;&#30340;&#22270;&#20687;&#36136;&#37327;&#36716;&#25442;&#65292;&#25552;&#20379;&#20102;&#20302;&#22343;&#26041;&#35823;&#24046;&#12289;&#39640;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#39640;&#23792;&#20540;&#20449;&#22122;&#27604;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.18026</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#20132;&#21449;&#31995;&#32479;&#29983;&#29289;&#22270;&#20687;&#36136;&#37327;&#22686;&#24378;&#65292;&#29992;&#20316;&#24314;&#31435;&#22810;&#26426;&#26500;&#26174;&#24494;&#38236;&#21512;&#20316;&#32593;&#32476;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Cross-system biological image quality enhancement based on the generative adversarial network as a foundation for establishing a multi-institute microscopy cooperative network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18026
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#20102;&#20849;&#32858;&#28966;&#26174;&#24494;&#38236;&#21644;&#24191;&#22330;&#33639;&#20809;&#26174;&#24494;&#38236;&#20043;&#38388;&#30340;&#22270;&#20687;&#36136;&#37327;&#36716;&#25442;&#65292;&#25552;&#20379;&#20102;&#20302;&#22343;&#26041;&#35823;&#24046;&#12289;&#39640;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#39640;&#23792;&#20540;&#20449;&#22122;&#27604;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#31995;&#32479;&#30340;&#39640;&#36136;&#37327;&#33639;&#20809;&#25104;&#20687;&#21463;&#38480;&#20110;&#20809;&#28418;&#30333;&#21644;&#20809;&#27602;&#24615;&#31561;&#36807;&#31243;&#65292;&#20197;&#21450;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#21463;&#38480;&#20110;&#26368;&#26032;&#19968;&#20195;&#26174;&#24494;&#38236;&#30340;&#26377;&#38480;&#35775;&#38382;&#12290;&#27492;&#22806;&#65292;&#20302;&#26102;&#38388;&#20998;&#36776;&#29575;&#21487;&#33021;&#23548;&#33268;&#27963;&#20307;&#31995;&#32479;&#20013;&#30340;&#36816;&#21160;&#27169;&#31946;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#23545;&#25239;&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22522;&#20110;&#20302;&#36136;&#37327;&#65288;LQ&#65289;&#22270;&#20687;&#33719;&#24471;&#39640;&#36136;&#37327;&#65288;HQ&#65289;&#22270;&#20687;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#29992;&#20110;&#20004;&#31181;&#19981;&#21516;&#29420;&#31435;&#26174;&#24494;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#27604;&#20256;&#36882;&#65306;&#20849;&#32858;&#28966;&#26174;&#24494;&#38236;&#65288;&#20135;&#29983;HQ&#22270;&#20687;&#65289;&#21644;&#24191;&#22330;&#33639;&#20809;&#26174;&#24494;&#38236;&#65288;&#20135;&#29983;LQ&#22270;&#20687;&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#35777;&#26126;&#20102;&#36825;&#31181;&#20256;&#36882;&#26159;&#21487;&#33021;&#30340;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25910;&#21040;&#20855;&#26377;&#20302;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20540;&#12289;&#39640;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#65288;SSIM&#65289;&#21644;&#39640;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#20540;&#30340;HQ&#29983;&#25104;&#22270;&#20687;&#12290;&#23545;&#20110;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18026v1 Announce Type: cross  Abstract: High-quality fluorescence imaging of biological systems is limited by processes like photobleaching and phototoxicity, and also in many cases, by limited access to the latest generations of microscopes. Moreover, low temporal resolution can lead to a motion blur effect in living systems. Our work presents a deep learning (DL) generative-adversarial approach to the problem of obtaining high-quality (HQ) images based on their low-quality (LQ) equivalents. We propose a generative-adversarial network (GAN) for contrast transfer between two different separate microscopy systems: a confocal microscope (producing HQ images) and a wide-field fluorescence microscope (producing LQ images). Our model proves that such transfer is possible, allowing us to receive HQ-generated images characterized by low mean squared error (MSE) values, high structural similarity index (SSIM), and high peak signal-to-noise ratio (PSNR) values. For our best model in 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;LM&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.18025</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#23450;&#25513;&#30721;&#25439;&#22833;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#65306;&#20197;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18025
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;LM&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35843;&#25972;&#21040;&#26032;&#39046;&#22495;&#36890;&#24120;&#36890;&#36807;&#22312;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;LM&#65288;PLM&#65289;&#26469;&#23454;&#29616;&#12290;&#24494;&#35843;&#23558;&#26032;&#30693;&#35782;&#24341;&#20837;LM&#65292;&#20351;&#23427;&#33021;&#22815;&#29702;&#35299;&#21644;&#26377;&#25928;&#25191;&#34892;&#30446;&#26631;&#22495;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#21464;&#24471;&#19981;&#22815;&#25935;&#24863;&#65292;&#22914;&#26524;&#23427;&#24573;&#35270;&#20102;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#24191;&#27867;&#24046;&#24322;&#65288;&#20363;&#22914;&#22312;&#35789;&#20041;&#19978;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#24494;&#35843;&#19981;&#25935;&#24863;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#36866;&#24403;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65288;DS-terms&#65289;&#30340;&#37325;&#35201;&#24615;&#26469;&#26377;&#25928;&#33719;&#21462;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;MSLM&#21516;&#26102;&#23631;&#34109;DS&#26415;&#35821;&#21644;&#36890;&#29992;&#35789;&#65292;&#28982;&#21518;&#36890;&#36807;&#30830;&#20445;LM&#21463;&#21040;&#26356;&#22823;&#24809;&#32602;&#26469;&#23398;&#20064;&#29305;&#23450;&#20110;&#25513;&#30721;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18025v1 Announce Type: cross  Abstract: Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for in
&lt;/p&gt;</description></item><item><title>DORE&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#23450;&#20041;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#21253;&#21547;&#36229;&#36807;10&#19975;&#20010;&#23450;&#20041;&#65292;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18018</link><description>&lt;p&gt;
DORE&#65306;&#19968;&#20221;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#23450;&#20041;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DORE: A Dataset For Portuguese Definition Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18018
&lt;/p&gt;
&lt;p&gt;
DORE&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#23450;&#20041;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#21253;&#21547;&#36229;&#36807;10&#19975;&#20010;&#23450;&#20041;&#65292;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18018v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#23450;&#20041;&#24314;&#27169;&#65288;DM&#65289;&#26159;&#33258;&#21160;&#20026;&#29305;&#23450;&#21333;&#35789;&#29983;&#25104;&#35789;&#20856;&#23450;&#20041;&#30340;&#20219;&#21153;&#12290;&#20855;&#26377;DM&#33021;&#21147;&#30340;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#22312;&#22810;&#20010;&#21463;&#20247;&#20013;&#21463;&#30410;&#65292;&#22240;&#20026;DM&#34987;&#35270;&#20026;&#30417;&#30563;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#38382;&#39064;&#65292;&#36825;&#20123;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#24050;&#32463;&#21457;&#24067;&#20102;&#19968;&#20123;&#29992;&#20110;&#33521;&#35821;&#21644;&#20854;&#20182;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;DM&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#33889;&#33796;&#29273;&#35821;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#20013;/&#39640;&#36164;&#28304;&#35821;&#35328;&#65292;&#19988;&#34987;2&#20159;&#22810;&#27597;&#35821;&#20154;&#21475;&#20351;&#29992;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#33889;&#33796;&#29273;&#35821;&#30340;DM&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;DORE&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65307;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#23450;&#20041;&#24314;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;10&#19975;&#20010;&#23450;&#20041;&#12290;&#25105;&#20204;&#36824;&#22312;DORE&#19978;&#35780;&#20272;&#20102;&#20960;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;DM&#27169;&#22411;&#65292;&#24182;&#25253;&#21578;&#20102;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18018v1 Announce Type: new  Abstract: Definition modelling (DM) is the task of automatically generating a dictionary definition for a specific word. Computational systems that are capable of DM can have numerous applications benefiting a wide range of audiences. As DM is considered a supervised natural language generation problem, these systems require large annotated datasets to train the machine learning (ML) models. Several DM datasets have been released for English and other high-resource languages. While Portuguese is considered a mid/high-resource language in most natural language processing tasks and is spoken by more than 200 million native speakers, there is no DM dataset available for Portuguese. In this research, we fill this gap by introducing DORE; the first dataset for Definition MOdelling for PoRtuguEse containing more than 100,000 definitions. We also evaluate several deep learning based DM models on DORE and report the results. The dataset and the findings o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;Wasserstein&#22270;&#21305;&#37197;&#30340;&#21322;&#30417;&#30563;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21322;&#30417;&#30563;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20013;&#30340;&#22256;&#22659;</title><link>https://arxiv.org/abs/2403.17995</link><description>&lt;p&gt;
&#32771;&#34385;Wasserstein&#22270;&#21305;&#37197;&#30340;&#21322;&#30417;&#30563;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Image Captioning Considering Wasserstein Graph Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17995
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;Wasserstein&#22270;&#21305;&#37197;&#30340;&#21322;&#30417;&#30563;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21322;&#30417;&#30563;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20013;&#30340;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#32473;&#23450;&#22270;&#20687;&#30340;&#23383;&#24149;&#65292;&#20854;&#20851;&#38190;&#25361;&#25112;&#26159;&#23398;&#20064;&#20174;&#35270;&#35273;&#29305;&#24449;&#21040;&#33258;&#28982;&#35821;&#35328;&#29305;&#24449;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;&#25551;&#36848;&#22270;&#20687;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#30340;&#22256;&#22659;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;Wasserstein&#22270;&#21305;&#37197;&#30340;&#26032;&#22411;&#21322;&#30417;&#30563;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65288;SSIC-WGM&#65289;&#65292;&#20197;&#30417;&#30563;&#29983;&#25104;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17995v1 Announce Type: cross  Abstract: Image captioning can automatically generate captions for the given images, and the key challenge is to learn a mapping function from visual features to natural language features. Existing approaches are mostly supervised ones, i.e., each image has a corresponding sentence in the training set. However, considering that describing images always requires a huge of manpower, we usually have limited amount of described images (i.e., image-text pairs) and a large number of undescribed images in real-world applications. Thereby, a dilemma is the "Semi-Supervised Image Captioning". To solve this problem, we propose a novel Semi-Supervised Image Captioning method considering Wasserstein Graph Matching (SSIC-WGM), which turns to adopt the raw image inputs to supervise the generated sentences. Different from traditional single modal semi-supervised methods, the difficulty of semi-supervised cross-modal learning lies in constructing intermediately
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;TAPIR+&#65292;&#19987;&#27880;&#20110;&#32416;&#27491;&#38745;&#24577;&#25668;&#20687;&#26426;&#25293;&#25668;&#30340;&#35270;&#39057;&#20013;&#38745;&#24577;&#28857;&#30340;&#36319;&#36394;&#65292;&#24182;&#22312;ICCV 2023&#31532;1&#23626;&#24863;&#30693;&#27979;&#35797;&#25361;&#25112;&#30340;&#28857;&#36319;&#36394;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;</title><link>https://arxiv.org/abs/2403.17994</link><description>&lt;p&gt;
ICCV 2023&#31532;1&#23626;&#24863;&#30693;&#27979;&#35797;&#25361;&#25112;&#20013;&#28857;&#36319;&#36394;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Solution for Point Tracking Task of ICCV 1st Perception Test Challenge 2023
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17994
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;TAPIR+&#65292;&#19987;&#27880;&#20110;&#32416;&#27491;&#38745;&#24577;&#25668;&#20687;&#26426;&#25293;&#25668;&#30340;&#35270;&#39057;&#20013;&#38745;&#24577;&#28857;&#30340;&#36319;&#36394;&#65292;&#24182;&#22312;ICCV 2023&#31532;1&#23626;&#24863;&#30693;&#27979;&#35797;&#25361;&#25112;&#30340;&#28857;&#36319;&#36394;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36861;&#36394;&#35270;&#39057;&#20013;&#30340;&#20219;&#24847;&#29289;&#29702;&#34920;&#38754;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#30001;&#26102;&#38388;&#39044;&#27979;&#24341;&#36215;&#30340;&#32047;&#31215;&#35823;&#24046;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;TAP with confident static points (TAPIR+)&#65292;&#19987;&#27880;&#20110;&#32416;&#27491;&#38745;&#24577;&#25668;&#20687;&#26426;&#25293;&#25668;&#30340;&#35270;&#39057;&#20013;&#38745;&#24577;&#28857;&#30340;&#36319;&#36394;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#22810;&#31890;&#24230;&#25668;&#20687;&#26426;&#36816;&#21160;&#26816;&#27979;&#21644;&#22522;&#20110;CMR&#30340;&#28857;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#26368;&#32456;&#27979;&#35797;&#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#24471;&#20998;&#20026;0.46&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17994v1 Announce Type: cross  Abstract: This report proposes an improved method for the Tracking Any Point (TAP) task, which tracks any physical surface through a video. Several existing approaches have explored the TAP by considering the temporal relationships to obtain smooth point motion trajectories, however, they still suffer from the cumulative error caused by temporal prediction. To address this issue, we propose a simple yet effective approach called TAP with confident static points (TAPIR+), which focuses on rectifying the tracking of the static point in the videos shot by a static camera. To clarify, our approach contains two key components: (1) Multi-granularity Camera Motion Detection, which could identify the video sequence by the static camera shot. (2) CMR-based point trajectory prediction with one moving object segmentation approach to isolate the static point from the moving object. Our approach ranked first in the final test with a score of 0.46.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#23545;&#36890;&#36807;&#21019;&#26032;&#24615;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#28237;&#27969;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20026;AI&#21644;&#28237;&#27969;&#30740;&#31350;&#20043;&#38388;&#32039;&#23494;&#20132;&#32455;&#30340;&#26410;&#26469;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.17993</link><description>&lt;p&gt;
&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#33258;&#28982;&#26234;&#33021;&#30456;&#34701;&#21512;&#65306;&#20174;&#32479;&#35745;&#21147;&#23398;&#21040;&#20154;&#24037;&#26234;&#33021;&#20877;&#21040;&#28237;&#27969;
&lt;/p&gt;
&lt;p&gt;
Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17993
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23545;&#36890;&#36807;&#21019;&#26032;&#24615;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#28237;&#27969;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20026;AI&#21644;&#28237;&#27969;&#30740;&#31350;&#20043;&#38388;&#32039;&#23494;&#20132;&#32455;&#30340;&#26410;&#26469;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#21453;&#24605;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#26410;&#26469;&#35282;&#33394;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#28237;&#27969;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#26681;&#26893;&#20110;&#38750;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#26816;&#39564;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#21019;&#26032;&#24615;&#22320;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#28237;&#27969;&#27169;&#22411;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#23457;&#26597;&#20102;&#28237;&#27969;&#30740;&#31350;&#20013;&#30340;&#21508;&#31181;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#32479;&#35745;&#27969;&#20307;&#21147;&#23398;&#30340;&#21516;&#26102;&#21457;&#23637;&#20013;&#30340;&#28508;&#22312;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17993v1 Announce Type: cross  Abstract: The paper reflects on the future role of AI in scientific research, with a special focus on turbulence studies, and examines the evolution of AI, particularly through Diffusion Models rooted in non-equilibrium statistical mechanics. It underscores the significant impact of AI on advancing reduced, Lagrangian models of turbulence through innovative use of deep neural networks. Additionally, the paper reviews various other AI applications in turbulence research and outlines potential challenges and opportunities in the concurrent advancement of AI and statistical hydrodynamics. This discussion sets the stage for a future where AI and turbulence research are intricately intertwined, leading to more profound insights and advancements in both fields.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26465;&#20214;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#24615;&#30284;&#32454;&#32990;&#26816;&#27979;&#30340;&#22768;&#23376;&#26174;&#24494;&#38236;&#65292;&#23454;&#29616;&#20102;&#25209;&#38388;&#26657;&#20934;&#21644;&#26102;&#38388;&#20998;&#36776;&#22768;&#23376;&#20449;&#21495;&#31934;&#20934;&#32454;&#32990;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;89.22%&#30340;&#24179;&#34913;&#31934;&#24230;&#21644;0.5&#31186;&#30340;&#24555;&#36895;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.17992</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20219;&#21153;&#26465;&#20214;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35299;&#37322;&#24615;&#30284;&#32454;&#32990;&#26816;&#27979;&#30340;&#22768;&#23376;&#26174;&#24494;&#38236;
&lt;/p&gt;
&lt;p&gt;
Interpretable cancer cell detection with phonon microscopy using multi-task conditional neural networks for inter-batch calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17992
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26465;&#20214;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#24615;&#30284;&#32454;&#32990;&#26816;&#27979;&#30340;&#22768;&#23376;&#26174;&#24494;&#38236;&#65292;&#23454;&#29616;&#20102;&#25209;&#38388;&#26657;&#20934;&#21644;&#26102;&#38388;&#20998;&#36776;&#22768;&#23376;&#20449;&#21495;&#31934;&#20934;&#32454;&#32990;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;89.22%&#30340;&#24179;&#34913;&#31934;&#24230;&#21644;0.5&#31186;&#30340;&#24555;&#36895;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#22312;&#25581;&#31034;&#22768;&#23376;&#26174;&#24494;&#38236;&#65288;&#39640;&#39057;&#36229;&#22768;&#27874;&#65289;&#25968;&#25454;&#20013;&#30340;&#22522;&#26412;&#20449;&#24687;&#20197;&#35782;&#21035;&#30284;&#32454;&#32990;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#25216;&#26415;&#21463;&#21040;&#8220;&#25209;&#27425;&#25928;&#24212;&#8221;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#30001;&#20110;&#27599;&#27425;&#23454;&#39564;&#20043;&#38388;&#26080;&#27861;&#36991;&#20813;&#30340;&#25216;&#26415;&#21464;&#21270;&#25152;&#36896;&#25104;&#30340;&#65292;&#36825;&#20123;&#21464;&#21270;&#20250;&#20135;&#29983;&#28151;&#28102;&#21464;&#37327;&#65292;&#20351;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#23398;&#20064;&#21040;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26465;&#20214;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#26088;&#22312;&#21516;&#26102;&#23454;&#29616;&#25209;&#38388;&#26657;&#20934;&#65292;&#36890;&#36807;&#28040;&#38500;&#28151;&#28102;&#21464;&#37327;&#65292;&#24182;&#23545;&#22522;&#20110;&#26102;&#38388;&#20998;&#36776;&#22768;&#23376;&#20449;&#21495;&#30340;&#32454;&#32990;&#20934;&#30830;&#20998;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#23454;&#39564;&#25209;&#27425;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#32972;&#26223;&#12289;&#20581;&#24247;&#21644;&#30284;&#30151;&#21306;&#22495;&#20998;&#31867;&#30340;&#24179;&#34913;&#31934;&#24230;&#20026;89.22&#65285;&#65292;&#20132;&#21449;&#39564;&#35777;&#24179;&#22343;&#31934;&#24230;&#20026;89.07&#65285;&#12290;&#20998;&#31867;&#21487;&#22312;0.5&#31186;&#20869;&#23436;&#25104;&#65292;&#21482;&#38656;&#35201;&#31616;&#21333;&#30340;&#20808;&#21069;&#25209;&#27425;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17992v1 Announce Type: cross  Abstract: Advances in artificial intelligence (AI) show great potential in revealing underlying information from phonon microscopy (high-frequency ultrasound) data to identify cancerous cells. However, this technology suffers from the 'batch effect' that comes from unavoidable technical variations between each experiment, creating confounding variables that the AI model may inadvertently learn. We therefore present a multi-task conditional neural network framework to simultaneously achieve inter-batch calibration, by removing confounding variables, and accurate cell classification of time-resolved phonon-derived signals. We validate our approach by training and validating on different experimental batches, achieving a balanced precision of 89.22% and an average cross-validated precision of 89.07% for classifying background, healthy and cancerous regions. Classification can be performed in 0.5 seconds with only simple prior batch information requ
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#29616;&#26377;&#27700;&#21360;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#23481;&#26131;&#36890;&#36807;&#20445;&#30041;&#35821;&#20041;&#36716;&#25442;&#26469;&#31227;&#38500;&#36825;&#20123;&#27700;&#21360;&#12290;</title><link>https://arxiv.org/abs/2403.17983</link><description>&lt;p&gt;
LLM&#29983;&#25104;&#20195;&#30721;&#30340;&#27700;&#21360;&#25216;&#26415;&#26159;&#21542;&#20855;&#26377;&#40065;&#26834;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Watermarking LLM-Generated Code Robust?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17983
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#29616;&#26377;&#27700;&#21360;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#23481;&#26131;&#36890;&#36807;&#20445;&#30041;&#35821;&#20041;&#36716;&#25442;&#26469;&#31227;&#38500;&#36825;&#20123;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#29616;&#26377;&#27700;&#21360;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#29616;&#26377;&#20316;&#21697;&#34920;&#26126;&#27700;&#21360;&#25216;&#26415;&#23545;&#33258;&#28982;&#35821;&#35328;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#20445;&#30041;&#35821;&#20041;&#30340;&#36716;&#25442;&#24456;&#23481;&#26131;&#31227;&#38500;&#20195;&#30721;&#19978;&#30340;&#36825;&#20123;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17983v1 Announce Type: cross  Abstract: We present the first study of the robustness of existing watermarking techniques on Python code generated by large language models. Although existing works showed that watermarking can be robust for natural language, we show that it is easy to remove these watermarks on code by semantic-preserving transformations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20513;&#21033;&#29992;&#31532;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#38142;&#24314;&#27169;&#26469;&#25429;&#25417;&#21644;&#39044;&#27979;&#35843;&#26597;&#21644;&#27979;&#35797;&#31572;&#22797;&#20013;&#30340;&#24207;&#36143;&#20381;&#36182;&#20851;&#31995;&#65292;&#36991;&#20813;&#24573;&#30053;&#19978;&#19979;&#25991;&#27010;&#29575;&#22312;&#22609;&#36896;&#24515;&#29702;&#27979;&#35797;&#31572;&#22797;&#27169;&#24335;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17982</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#38142;&#27169;&#22411;&#29992;&#20110;&#26816;&#26597;&#24515;&#29702;&#27979;&#35797;&#20013;&#30340;&#21709;&#24212;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Markov chain models for inspecting response dynamics in psychological testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20513;&#21033;&#29992;&#31532;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#38142;&#24314;&#27169;&#26469;&#25429;&#25417;&#21644;&#39044;&#27979;&#35843;&#26597;&#21644;&#27979;&#35797;&#31572;&#22797;&#20013;&#30340;&#24207;&#36143;&#20381;&#36182;&#20851;&#31995;&#65292;&#36991;&#20813;&#24573;&#30053;&#19978;&#19979;&#25991;&#27010;&#29575;&#22312;&#22609;&#36896;&#24515;&#29702;&#27979;&#35797;&#31572;&#22797;&#27169;&#24335;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24515;&#29702;&#27979;&#35797;&#20013;&#32771;&#34385;&#19978;&#19979;&#25991;&#27010;&#29575;&#30340;&#37325;&#35201;&#24615;&#34987;&#24378;&#35843;&#65292;&#23613;&#31649;&#26041;&#27861;&#35770;&#25991;&#29486;&#20013;&#24191;&#27867;&#35752;&#35770;&#20102;&#39034;&#24207;&#25928;&#24212;&#30340;&#26080;&#22788;&#19981;&#22312;&#30340;&#24615;&#36136;&#12290;&#26412;&#30740;&#31350;&#20511;&#37492;&#20102;&#36335;&#24452;&#20381;&#36182;&#24615;&#12289;&#19968;&#38454;&#33258;&#30456;&#20851;&#24615;&#12289;&#29366;&#24577;&#20381;&#36182;&#24615;&#21644;&#30913;&#28382;&#25928;&#24212;&#31561;&#27010;&#24565;&#65292;&#35797;&#22270;&#35299;&#20915;&#26089;&#26399;&#31572;&#22797;&#22914;&#20309;&#25104;&#20026;&#27979;&#35797;&#12289;&#35843;&#26597;&#21644;&#38382;&#21367;&#20013;&#21518;&#32493;&#31572;&#26696;&#30340;&#38170;&#28857;&#12290;&#24341;&#20837;&#20174;&#37327;&#23376;&#29289;&#29702;&#23398;&#20013;&#23548;&#20986;&#30340;&#19981;&#20132;&#25442;&#35266;&#27979;&#30340;&#27010;&#24565;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#34920;&#24449;&#24515;&#29702;&#36807;&#31243;&#20197;&#21450;&#27979;&#37327;&#20202;&#22120;&#23545;&#21442;&#19982;&#32773;&#31572;&#22797;&#30340;&#24433;&#21709;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#20027;&#24352;&#21033;&#29992;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#38142;&#24314;&#27169;&#26469;&#25429;&#25417;&#21644;&#39044;&#27979;&#35843;&#26597;&#21644;&#27979;&#35797;&#31572;&#22797;&#20013;&#30340;&#24207;&#36143;&#20381;&#36182;&#20851;&#31995;&#12290;&#31532;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#38142;&#27169;&#22411;&#30340;&#24212;&#29992;&#22312;&#20110;&#20010;&#20307;&#20542;&#21521;&#20110;&#34920;&#29616;&#20026;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17982v1 Announce Type: cross  Abstract: The importance of considering contextual probabilities in shaping response patterns within psychological testing is underscored, despite the ubiquitous nature of order effects discussed extensively in methodological literature. Drawing from concepts such as path-dependency, first-order autocorrelation, state-dependency, and hysteresis, the present study is an attempt to address how earlier responses serve as an anchor for subsequent answers in tests, surveys, and questionnaires. Introducing the notion of non-commuting observables derived from quantum physics, I highlight their role in characterizing psychological processes and the impact of measurement instruments on participants' responses. We advocate for the utilization of first-order Markov chain modeling to capture and forecast sequential dependencies in survey and test responses. The employment of the first-order Markov chain model lies in individuals' propensity to exhibit parti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;EG-ConMix&#65292;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#21644;E-GraphSAGE&#27169;&#22359;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17980</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;EG-ConMix
&lt;/p&gt;
&lt;p&gt;
EG-ConMix: An Intrusion Detection Method based on Graph Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17980
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;EG-ConMix&#65292;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#21644;E-GraphSAGE&#27169;&#22359;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#35774;&#22791;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23433;&#20840;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#37096;&#32626;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#21487;&#20197;&#36890;&#36807;&#30417;&#25511;&#32593;&#32476;&#27969;&#37327;&#12289;&#26816;&#27979;&#21644;&#21457;&#29616;&#20837;&#20405;&#24182;&#21450;&#26102;&#21457;&#20986;&#23433;&#20840;&#35686;&#25253;&#26469;&#26368;&#23567;&#21270;&#23041;&#32961;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;E-GraphSAGE&#30340;EG-ConMix&#26041;&#27861;&#65292;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#27169;&#22359;&#26469;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#34701;&#21512;&#23545;&#27604;&#23398;&#20064;&#20197;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#32593;&#32476;&#27969;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17980v1 Announce Type: cross  Abstract: As the number of IoT devices increases, security concerns become more prominent. The impact of threats can be minimized by deploying Network Intrusion Detection System (NIDS) by monitoring network traffic, detecting and discovering intrusions, and issuing security alerts promptly. Most intrusion detection research in recent years has been directed towards the pair of traffic itself without considering the interrelationships among them, thus limiting the monitoring of complex IoT network attack events. Besides, anomalous traffic in real networks accounts for only a small fraction, which leads to a severe imbalance problem in the dataset that makes algorithmic learning and prediction extremely difficult. In this paper, we propose an EG-ConMix method based on E-GraphSAGE, incorporating a data augmentation module to fix the problem of data imbalance. In addition, we incorporate contrastive learning to discern the difference between normal 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20840;&#24687;&#20840;&#23616;&#21367;&#31215;&#32593;&#32476;&#65288;HGConv&#65289;&#21644;&#20840;&#24687;&#31616;&#21270;&#34920;&#31034;&#65288;HRR&#65289;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#26680;&#35745;&#31639;&#25110;&#35774;&#35745;&#65292;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.17978</link><description>&lt;p&gt;
&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#30340;&#38271;&#31243;&#39044;&#27979;&#20219;&#21153;&#30340;&#20840;&#24687;&#20840;&#23616;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Holographic Global Convolutional Networks for Long-Range Prediction Tasks in Malware Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20840;&#24687;&#20840;&#23616;&#21367;&#31215;&#32593;&#32476;&#65288;HGConv&#65289;&#21644;&#20840;&#24687;&#31616;&#21270;&#34920;&#31034;&#65288;HRR&#65289;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#26680;&#35745;&#31639;&#25110;&#35774;&#35745;&#65292;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26159;&#19968;&#20010;&#26377;&#36259;&#19988;&#26377;&#20215;&#20540;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#23545;&#29616;&#23454;&#19990;&#30028;&#26377;&#37325;&#35201;&#24433;&#21709;&#24182;&#20855;&#26377;&#29420;&#29305;&#30340;&#26426;&#22120;&#23398;&#20064;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#29616;&#26377;&#30340;&#38271;&#31243;&#25216;&#26415;&#21644;&#22522;&#20934;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20010;&#38382;&#39064;&#39046;&#22495;&#19981;&#22826;&#36866;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21033;&#29992;&#20840;&#24687;&#31616;&#21270;&#34920;&#31034;&#65288;HRR&#65289;&#23646;&#24615;&#26469;&#32534;&#30721;&#21644;&#35299;&#30721;&#24207;&#21015;&#20803;&#32032;&#29305;&#24449;&#30340;&#20840;&#24687;&#20840;&#23616;&#21367;&#31215;&#32593;&#32476;&#65288;HGConv&#65289;&#12290;&#19982;&#20854;&#20182;&#20840;&#23616;&#21367;&#31215;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#22797;&#26434;&#30340;&#26680;&#35745;&#31639;&#25110;&#31934;&#24515;&#35774;&#35745;&#30340;&#26680;&#12290;HGConv&#26680;&#34987;&#23450;&#20041;&#20026;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#30340;&#31616;&#21333;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#22312;Microsoft&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#25361;&#25112;&#36187;&#12289;Drebin&#21644;EMBER&#24694;&#24847;&#36719;&#20214;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#32467;&#26524;&#12290;&#22312;&#24207;&#21015;&#38271;&#24230;&#30340;&#23545;&#25968;&#32423;&#22797;&#26434;&#24230;&#19979;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;HGConv&#30340;&#36816;&#34892;&#26102;&#38388;&#26126;&#26174;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17978v1 Announce Type: cross  Abstract: Malware detection is an interesting and valuable domain to work in because it has significant real-world impact and unique machine-learning challenges. We investigate existing long-range techniques and benchmarks and find that they're not very suitable in this problem area. In this paper, we introduce Holographic Global Convolutional Networks (HGConv) that utilize the properties of Holographic Reduced Representations (HRR) to encode and decode features from sequence elements. Unlike other global convolutional methods, our method does not require any intricate kernel computation or crafted kernel design. HGConv kernels are defined as simple parameters learned through backpropagation. The proposed method has achieved new SOTA results on Microsoft Malware Classification Challenge, Drebin, and EMBER malware benchmarks. With log-linear complexity in sequence length, the empirical results demonstrate substantially faster run-time by HGConv c
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#29983;&#25104;&#24335;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861; DGDATA &#29420;&#29305;&#22320;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#36807;&#31243;&#20013;&#35782;&#21035;&#21644;&#25972;&#21512;&#20102;&#26102;&#38388;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.17958</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#24335;&#39046;&#22495;&#33258;&#36866;&#24212;&#19982;&#26102;&#38388;&#27880;&#24847;&#21147;&#36328;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Domain Adaptation with Temporal Attention for Cross-User Activity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17958
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#24335;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861; DGDATA &#29420;&#29305;&#22320;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#36807;&#31243;&#20013;&#35782;&#21035;&#21644;&#25972;&#21512;&#20102;&#26102;&#38388;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#20013;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#20551;&#35774;&#26159;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30446;&#30340;&#30340;&#25968;&#25454;&#26469;&#33258;&#30456;&#21516;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#23454;&#29616;&#20013;&#65292;&#24448;&#24448;&#25361;&#25112;&#36825;&#19968;&#27010;&#24565;&#65292;&#23588;&#20854;&#26159;&#22312;&#36328;&#29992;&#25143;HAR&#31561;&#22330;&#26223;&#20013;&#34920;&#29616;&#20026;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#12290;&#39046;&#22495;&#33258;&#36866;&#24212;&#26159;&#24212;&#23545;&#36328;&#29992;&#25143;HAR&#20219;&#21153;&#20013;&#36825;&#20123;&#25361;&#25112;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#20013;&#19968;&#20010;&#26126;&#26174;&#30340;&#32570;&#38519;&#26159;&#24573;&#35270;&#20102;&#22312;&#35843;&#25972;&#25968;&#25454;&#20998;&#24067;&#38454;&#27573;&#23884;&#20837;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;&#38024;&#23545;&#36825;&#19968;&#30095;&#28431;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#24102;&#26377;&#26102;&#38388;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29983;&#25104;&#24335;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DGDATA&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17958v1 Announce Type: cross  Abstract: In Human Activity Recognition (HAR), a predominant assumption is that the data utilized for training and evaluation purposes are drawn from the same distribution. It is also assumed that all data samples are independent and identically distributed ($\displaystyle i.i.d.$). Contrarily, practical implementations often challenge this notion, manifesting data distribution discrepancies, especially in scenarios such as cross-user HAR. Domain adaptation is the promising approach to address these challenges inherent in cross-user HAR tasks. However, a clear gap in domain adaptation techniques is the neglect of the temporal relation embedded within time series data during the phase of aligning data distributions. Addressing this oversight, our research presents the Deep Generative Domain Adaptation with Temporal Attention (DGDATA) method. This novel method uniquely recognises and integrates temporal relations during the domain adaptation proce
&lt;/p&gt;</description></item><item><title>Sort &amp; Slice &#26159;&#19968;&#31181;&#26131;&#20110;&#23454;&#29616;&#19988;&#26080;&#20301;&#30896;&#25758;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#27744;&#21270;&#25193;&#23637;&#36830;&#25509;&#25351;&#32441;&#30340;&#20122;&#32467;&#26500;</title><link>https://arxiv.org/abs/2403.17954</link><description>&lt;p&gt;
Sort &amp; Slice&#65306;&#19968;&#31181;&#31616;&#21333;&#19988;&#20248;&#36234;&#30340;&#25193;&#23637;&#36830;&#25509;&#25351;&#32441;&#30340;&#38750;&#21704;&#24076;&#25240;&#21472;&#22791;&#36873;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Sort &amp; Slice: A Simple and Superior Alternative to Hash-Based Folding for Extended-Connectivity Fingerprints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17954
&lt;/p&gt;
&lt;p&gt;
Sort &amp; Slice &#26159;&#19968;&#31181;&#26131;&#20110;&#23454;&#29616;&#19988;&#26080;&#20301;&#30896;&#25758;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#27744;&#21270;&#25193;&#23637;&#36830;&#25509;&#25351;&#32441;&#30340;&#20122;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17954v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#25193;&#23637;&#36830;&#25509;&#25351;&#32441;&#65288;ECFPs&#65289;&#26159;&#24403;&#21069;&#21270;&#23398;&#20449;&#24687;&#23398;&#21644;&#20998;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26222;&#36941;&#24037;&#20855;&#65292;&#26159;&#29992;&#20110;&#21270;&#23398;&#39044;&#27979;&#30340;&#26368;&#24120;&#35265;&#30340;&#20998;&#23376;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#20043;&#19968;&#12290;&#30001;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#21407;&#23376;&#29305;&#24449;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#22270;&#27744;&#21270;&#26041;&#27861;&#32858;&#21512;&#21040;&#21270;&#21512;&#29289;&#32423;&#34920;&#31034;&#20013;&#65307;&#30456;&#27604;&#20043;&#19979;&#65292;&#19968;&#32452;&#26816;&#27979;&#21040;&#30340;ECFP&#20122;&#32467;&#26500;&#40664;&#35748;&#24773;&#20917;&#19979;&#21482;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;&#21704;&#24076;&#30340;&#25240;&#21472;&#36807;&#31243;&#36716;&#25442;&#20026;&#20301;&#21521;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;&#20122;&#32467;&#26500;&#27744;&#21270;&#30340;&#24418;&#24335;&#25805;&#20316;&#26469;&#23545;&#32467;&#26500;&#25351;&#32441;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#22522;&#20110;&#21704;&#24076;&#30340;&#25240;&#21472;&#12289;&#31639;&#27861;&#24615;&#20122;&#32467;&#26500;&#36873;&#25321;&#21644;&#21508;&#31181;&#20854;&#20182;&#28508;&#22312;&#25216;&#26415;&#12290;&#25105;&#20204;&#32487;&#32493;&#25551;&#36848;Sort &amp; Slice&#65292;&#36825;&#26159;&#19968;&#31181;&#26131;&#20110;&#23454;&#29616;&#19988;&#26080;&#20301;&#30896;&#25758;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#27744;&#21270;ECFP&#20122;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17954v1 Announce Type: new  Abstract: Extended-connectivity fingerprints (ECFPs) are a ubiquitous tool in current cheminformatics and molecular machine learning, and one of the most prevalent molecular feature extraction techniques used for chemical prediction. Atom features learned by graph neural networks can be aggregated to compound-level representations using a large spectrum of graph pooling methods; in contrast, sets of detected ECFP substructures are by default transformed into bit vectors using only a simple hash-based folding procedure. We introduce a general mathematical framework for the vectorisation of structural fingerprints via a formal operation called substructure pooling that encompasses hash-based folding, algorithmic substructure-selection, and a wide variety of other potential techniques. We go on to describe Sort &amp; Slice, an easy-to-implement and bit-collision-free alternative to hash-based folding for the pooling of ECFP substructures. Sort &amp; Slice fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#23398;&#20064;&#12289;&#29702;&#35299;&#21644;&#25506;&#32034;&#21069;&#30651;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#26032;&#39062;&#30340;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.17942</link><description>&lt;p&gt;
&#20851;&#20110;&#29616;&#23454;&#29983;&#27963;&#21644;&#35745;&#31639;&#20013;&#30340;&#21069;&#30651;&#24615;&#30340;&#27880;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Note On Lookahead In Real Life And Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23398;&#20064;&#12289;&#29702;&#35299;&#21644;&#25506;&#32034;&#21069;&#30651;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#26032;&#39062;&#30340;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#20026;&#20102;&#20182;&#20204;&#30340;&#23384;&#22312;&#21644;&#25104;&#38271;&#32780;&#23450;&#20041;&#30340;&#19977;&#20010;&#26102;&#38388;&#21644;&#36923;&#36753;&#27010;&#24565;&#12290;&#25105;&#20204;&#20316;&#20026;&#20154;&#31867;&#26377;&#24184;&#33021;&#22815;&#21033;&#29992;&#25105;&#20204;&#30340;&#26234;&#24935;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#29289;&#29702;&#21457;&#29983;&#20043;&#21069;&#22312;&#22836;&#33041;&#20013;&#25191;&#34892;&#26576;&#39033;&#27963;&#21160;&#12290;&#36807;&#21435;&#30340;&#30693;&#35782;&#12289;&#29616;&#22312;&#30340;&#20919;&#38745;&#21644;&#26410;&#26469;&#30340;&#23637;&#26395;&#23545;&#24212;&#20110;&#29616;&#23454;&#29983;&#27963;&#20197;&#21450;&#35745;&#31639;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#19977;&#20010;&#27010;&#24565;&#65292;&#20998;&#21035;&#26159;&#22238;&#39038;&#12289;&#35266;&#23519;&#21644;&#21069;&#30651;&#12290;&#21069;&#30651;&#65288;LA&#65289;&#22788;&#29702;&#20449;&#24687;&#30340;&#26410;&#26469;&#39044;&#27979;&#21644;&#22788;&#29702;&#36755;&#20837;&#20197;&#25552;&#21069;&#29983;&#25104;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23398;&#20064;&#12289;&#29702;&#35299;&#21644;&#25506;&#32034;LA&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#26032;&#39062;&#30340;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#22522;&#20110;&#36755;&#20837;&#20449;&#24687;&#21487;&#29992;&#24615;&#30340;&#30693;&#21517;&#31639;&#27861;&#26694;&#26550;&#65292;&#20363;&#22914;&#33073;&#26426;&#12289;&#22312;&#32447;&#21644;&#21322;&#22312;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17942v1 Announce Type: cross  Abstract: Past, Present and Future are considered to be three temporal and logical concepts which are well defined by human beings for their existence and growth. We, as human beings, have the privilege of using our intelligence to mentally execute an activity before physical occurrence of the same in the real world. Knowledge of the past, aplomb of present and visualisation for the future correspond to three concepts such as look-back, look-at and look-ahead respectively in real life as well as in diversified domains of computing. Look-Ahead(LA) deals with the future prediction of information and processing of input to produce the output in advance. In this article, our main objective is to learn, understand and explore the concept of LA and design novel models as solution for real world problems. We present three well known algorithmic frameworks used in practice based on availability of input information such as offline, online and semi-onlin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;R2D2&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#38750;&#31515;&#21345;&#23572;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17905</link><description>&lt;p&gt;
&#20855;&#26377;R2D2&#30340;&#21487;&#25193;&#23637;&#38750;&#31515;&#21345;&#23572;&#30913;&#20849;&#25391;&#25104;&#20687;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17905
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;R2D2&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#38750;&#31515;&#21345;&#23572;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#31515;&#21345;&#23572;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#22312;&#22825;&#25991;&#25104;&#20687;&#20013;&#24341;&#20837;&#30340;&#8220;&#29992;&#20110;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#27531;&#24046;&#32423;&#32852;DNN&#31995;&#21015;&#65288;R2D2&#65289;&#8221;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;R2D2&#30340;&#37325;&#24314;&#34987;&#24418;&#25104;&#20026;&#27531;&#24046;&#22270;&#20687;&#30340;&#31995;&#21015;&#65292;&#34987;&#36845;&#20195;&#22320;&#20272;&#35745;&#20026;&#25509;&#21463;&#19978;&#19968;&#27425;&#36845;&#20195;&#30340;&#22270;&#20687;&#20272;&#35745;&#21644;&#30456;&#20851;&#25968;&#25454;&#27531;&#24046;&#20316;&#20026;&#36755;&#20837;&#30340;DNN&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17905v1 Announce Type: cross  Abstract: We propose a new approach for non-Cartesian magnetic resonance image reconstruction. While unrolled architectures provide robustness via data-consistency layers, embedding measurement operators in Deep Neural Network (DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP) approaches, where the denoising DNNs are blind to the measurement setting, are not affected by this limitation and have also proven effective, but their highly iterative nature also affects scalability. To address this scalability challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic range imaging (R2D2)" approach recently introduced in astronomical imaging. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of DNNs taking the previous iteration's image estimate and associated data residual as inputs. The method can be interpreted as a learned version of the Matching Pursuit algo
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#32593;&#26684;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#25968;&#25454;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25152;&#26377;&#26435;&#20998;&#24067;&#21040;&#27599;&#20010;&#25968;&#25454;&#39046;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#32852;&#21512;&#27835;&#29702;&#65292;&#20197;&#20811;&#26381;&#25968;&#25454;&#28304;&#28608;&#22686;&#21644;&#21450;&#26102;&#20998;&#26512;&#22788;&#29702;&#38656;&#27714;&#22686;&#38271;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.17878</link><description>&lt;p&gt;
&#29992;&#32852;&#37030;&#23398;&#20064;&#22686;&#24378;&#25968;&#25454;&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
Empowering Data Mesh with Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17878
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#32593;&#26684;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#25968;&#25454;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25152;&#26377;&#26435;&#20998;&#24067;&#21040;&#27599;&#20010;&#25968;&#25454;&#39046;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#32852;&#21512;&#27835;&#29702;&#65292;&#20197;&#20811;&#26381;&#25968;&#25454;&#28304;&#28608;&#22686;&#21644;&#21450;&#26102;&#20998;&#26512;&#22788;&#29702;&#38656;&#27714;&#22686;&#38271;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26550;&#26500;&#30340;&#28436;&#21464;&#35265;&#35777;&#20102;&#25968;&#25454;&#28246;&#30340;&#20852;&#36215;&#65292;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#31649;&#29702;&#30340;&#29942;&#39048;&#24182;&#25512;&#21160;&#26234;&#33021;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#38598;&#20013;&#21270;&#26550;&#26500;&#21463;&#21046;&#20110;&#25968;&#25454;&#28304;&#30340;&#28608;&#22686;&#21644;&#23545;&#21450;&#26102;&#20998;&#26512;&#22788;&#29702;&#30340;&#26085;&#30410;&#22686;&#38271;&#38656;&#27714;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#33539;&#24335;&#65292;&#25968;&#25454;&#32593;&#26684;&#12290;&#25968;&#25454;&#32593;&#26684;&#23558;&#39046;&#22495;&#35270;&#20026;&#39318;&#35201;&#20851;&#27880;&#28857;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25152;&#26377;&#26435;&#20174;&#20013;&#22830;&#22242;&#38431;&#20998;&#21457;&#21040;&#27599;&#20010;&#25968;&#25454;&#39046;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#32852;&#21512;&#27835;&#29702;&#26469;&#30417;&#25511;&#39046;&#22495;&#21450;&#20854;&#25968;&#25454;&#20135;&#21697;&#12290;&#20687;Paypal&#12289;Netflix&#21644;Zalando&#31561;&#35768;&#22810;&#20159;&#32654;&#20803;&#32452;&#32455;&#24050;&#32463;&#22522;&#20110;&#36825;&#31181;&#26032;&#26550;&#26500;&#36716;&#21464;&#20102;&#20182;&#20204;&#30340;&#25968;&#25454;&#20998;&#26512;&#27969;&#31243;&#12290;&#22312;&#36825;&#31181;&#21435;&#20013;&#24515;&#21270;&#26550;&#26500;&#20013;&#65292;&#25968;&#25454;&#30001;&#27599;&#20010;&#39046;&#22495;&#22242;&#38431;&#26412;&#22320;&#20445;&#23384;&#65292;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#26080;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#26377;&#25928;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17878v1 Announce Type: new  Abstract: The evolution of data architecture has seen the rise of data lakes, aiming to solve the bottlenecks of data management and promote intelligent decision-making. However, this centralized architecture is limited by the proliferation of data sources and the growing demand for timely analysis and processing. A new data paradigm, Data Mesh, is proposed to overcome these challenges. Data Mesh treats domains as a first-class concern by distributing the data ownership from the central team to each data domain, while keeping the federated governance to monitor domains and their data products. Many multi-million dollar organizations like Paypal, Netflix, and Zalando have already transformed their data analysis pipelines based on this new architecture. In this decentralized architecture where data is locally preserved by each domain team, traditional centralized machine learning is incapable of conducting effective analysis across multiple domains,
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#30830;&#23450;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#28176;&#36817;&#36125;&#21494;&#26031;&#39118;&#38505;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#19982;&#26368;&#20339;&#31639;&#27861;&#27604;&#36739;&#24471;&#20986;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.17767</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#28176;&#36817;&#36125;&#21494;&#26031;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Asymptotic Bayes risk of semi-supervised learning with uncertain labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17767
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#30830;&#23450;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#28176;&#36817;&#36125;&#21494;&#26031;&#39118;&#38505;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#19982;&#26368;&#20339;&#31639;&#27861;&#27604;&#36739;&#24471;&#20986;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#19978;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;&#35774;&#32622;&#65292;&#20854;&#20013;&#25968;&#25454;&#30340;&#26631;&#31614;&#19981;&#20687;&#36890;&#24120;&#37027;&#26679;&#20005;&#26684;&#65292;&#32780;&#26159;&#24102;&#26377;&#19981;&#30830;&#23450;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35745;&#31639;&#35813;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#39118;&#38505;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#35813;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#39118;&#38505;&#19982;&#30446;&#21069;&#24050;&#30693;&#30340;&#26368;&#20339;&#31639;&#27861;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#27604;&#36739;&#26368;&#32456;&#20026;&#35813;&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17767v1 Announce Type: cross  Abstract: This article considers a semi-supervised classification setting on a Gaussian mixture model, where the data is not labeled strictly as usual, but instead with uncertain labels. Our main aim is to compute the Bayes risk for this model. We compare the behavior of the Bayes risk and the best known algorithm for this model. This comparison eventually gives new insights over the algorithm.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#19981;&#21516;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#21457;&#29616;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#21462;&#20915;&#20110;&#22806;&#37096;&#21464;&#37327;&#65292;&#22914;&#25915;&#20987;&#31867;&#22411;&#12289;&#22797;&#26434;&#24615;&#21644;&#32593;&#32476;&#29615;&#22659;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#24182;&#38750;&#22987;&#32456;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2403.17458</link><description>&lt;p&gt;
&#26399;&#26395;&#19982;&#29616;&#23454;&#65306;&#23454;&#36341;&#20013;&#35780;&#20272;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Expectations Versus Reality: Evaluating Intrusion Detection Systems in Practice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17458
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#19981;&#21516;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#21457;&#29616;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#21462;&#20915;&#20110;&#22806;&#37096;&#21464;&#37327;&#65292;&#22914;&#25915;&#20987;&#31867;&#22411;&#12289;&#22797;&#26434;&#24615;&#21644;&#32593;&#32476;&#29615;&#22659;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#24182;&#38750;&#22987;&#32456;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#26368;&#36817;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23458;&#35266;&#27604;&#36739;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#26681;&#25454;&#20854;&#38656;&#27714;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#27809;&#26377;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#26368;&#22909;&#30340;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#22806;&#37096;&#21464;&#37327;&#65292;&#22914;&#25915;&#20987;&#31867;&#22411;&#12289;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#38598;&#20013;&#30340;&#32593;&#32476;&#29615;&#22659;&#12290;&#20363;&#22914;&#65292;BoT_IoT&#21644;Stratosphere IoT&#25968;&#25454;&#38598;&#37117;&#25429;&#33719;&#20102;&#19982;&#29289;&#32852;&#32593;&#30456;&#20851;&#30340;&#25915;&#20987;&#65292;&#20294;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;BoT_IoT&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#26102;&#34920;&#29616;&#26368;&#20339;&#65292;&#32780;&#22312;&#20351;&#29992;Stratosphere IoT&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#26102;HELAD&#34920;&#29616;&#26368;&#20339;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#25105;&#20204;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26368;&#39640;&#30340;&#24179;&#22343;F1&#20998;&#25968;&#65292;&#20294;&#24182;&#19981;&#24635;&#26159;&#34920;&#29616;&#26368;&#22909;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#20351;&#29992;&#25991;&#29486;&#21644;&#39033;&#30446;&#23384;&#20648;&#24211;&#20013;&#30340;IDS&#30340;&#22256;&#38590;&#65292;&#36825;&#20351;&#24471;&#23601;IDS&#36873;&#25321;&#24471;&#20986;&#26126;&#30830;&#32467;&#35770;&#21464;&#24471;&#22797;&#26434;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17458v1 Announce Type: cross  Abstract: Our paper provides empirical comparisons between recent IDSs to provide an objective comparison between them to help users choose the most appropriate solution based on their requirements. Our results show that no one solution is the best, but is dependent on external variables such as the types of attacks, complexity, and network environment in the dataset. For example, BoT_IoT and Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural network performed the best when tested using the BoT_IoT dataset while HELAD performed the best when tested using the Stratosphere IoT dataset. So although we found that a deep neural network solution had the highest average F1 scores on tested datasets, it is not always the best-performing one. We further discuss difficulties in using IDS from literature and project repositories, which complicated drawing definitive conclusions regarding IDS selection.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#27169;&#20223;&#23398;&#20064;&#22312;&#21463;&#32422;&#26463;&#35774;&#32622;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#19987;&#23478;&#34892;&#20026;&#21463;&#38480;&#21046;&#22240;&#32032;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17456</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Imitating Cost-Constrained Behaviors in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#27169;&#20223;&#23398;&#20064;&#22312;&#21463;&#32422;&#26463;&#35774;&#32622;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#19987;&#23478;&#34892;&#20026;&#21463;&#38480;&#21046;&#22240;&#32032;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#22797;&#26434;&#30340;&#35745;&#21010;&#21644;&#35843;&#24230;&#38382;&#39064;&#19968;&#30452;&#36890;&#36807;&#21508;&#31181;&#20248;&#21270;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#30340;&#27169;&#20223;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#21487;&#34892;&#26367;&#20195;&#26041;&#27861;&#12290;&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#30340;&#34892;&#20026;&#26469;&#23398;&#20064;&#22870;&#21169;&#65288;&#25110;&#20559;&#22909;&#65289;&#27169;&#22411;&#25110;&#30452;&#25509;&#34892;&#20026;&#31574;&#30053;&#12290;&#29616;&#26377;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26080;&#38480;&#21046;&#35774;&#32622;&#19979;&#30340;&#27169;&#20223;&#65288;&#20363;&#22914;&#65292;&#36710;&#36742;&#28040;&#32791;&#30340;&#29123;&#27833;&#37327;&#27809;&#26377;&#38480;&#21046;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#19987;&#23478;&#30340;&#34892;&#20026;&#19981;&#20165;&#21463;&#22870;&#21169;&#65288;&#25110;&#20559;&#22909;&#65289;&#30340;&#24433;&#21709;&#65292;&#36824;&#21463;&#32422;&#26463;&#30340;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#33258;&#21160;&#39550;&#39542;&#36865;&#36135;&#36710;&#30340;&#20915;&#31574;&#19981;&#20165;&#21462;&#20915;&#20110;&#36335;&#24452;&#20559;&#22909;/&#22870;&#21169;&#65288;&#26681;&#25454;&#36807;&#21435;&#30340;&#38656;&#27714;&#25968;&#25454;&#65289;&#65292;&#36824;&#21462;&#20915;&#20110;&#36710;&#36742;&#20869;&#30340;&#29123;&#27833;&#21644;&#36865;&#36798;&#26102;&#38388;&#31561;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17456v1 Announce Type: cross  Abstract: Complex planning and scheduling problems have long been solved using various optimization or heuristic approaches. In recent years, imitation learning that aims to learn from expert demonstrations has been proposed as a viable alternative to solving these problems. Generally speaking, imitation learning is designed to learn either the reward (or preference) model or directly the behavioral policy by observing the behavior of an expert. Existing work in imitation learning and inverse reinforcement learning has focused on imitation primarily in unconstrained settings (e.g., no limit on fuel consumed by the vehicle). However, in many real-world domains, the behavior of an expert is governed not only by reward (or preference) but also by constraints. For instance, decisions on self-driving delivery vehicles are dependent not only on the route preferences/rewards (depending on past demand data) but also on the fuel in the vehicle and the ti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#21464;&#21387;&#22120;&#22359;&#36827;&#34892;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#65292;&#20174;&#32780;&#25552;&#39640;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17343</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#30340;&#20813;&#36153;&#21161;&#25512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models are Free Boosters for Biomedical Imaging Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#21464;&#21387;&#22120;&#22359;&#36827;&#34892;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#65292;&#20174;&#32780;&#25552;&#39640;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#26159;&#20256;&#32479;&#19978;&#32570;&#20047;&#35821;&#35328;&#25110;&#25991;&#26412;&#25968;&#25454;&#30340;&#39046;&#22495;&#12290;&#35813;&#26041;&#27861;&#19981;&#21516;&#20110;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#39044;&#35757;&#32451;&#30340;LLMs&#20013;&#25552;&#21462;&#30340;&#20923;&#32467;&#21464;&#21387;&#22120;&#22359;&#20316;&#20026;&#21019;&#26032;&#30340;&#32534;&#30721;&#22120;&#23618;&#65292;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#12290;&#36825;&#31181;&#31574;&#30053;&#19982;&#36890;&#24120;&#20381;&#36182;&#20110;&#35821;&#35328;&#39537;&#21160;&#25552;&#31034;&#21644;&#36755;&#20837;&#30340;&#26631;&#20934;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#26694;&#26550;&#26377;&#30528;&#26174;&#33879;&#30340;&#19981;&#21516;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;LLMs&#33021;&#22815;&#25552;&#21319;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;2D&#21644;3D&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#65292;&#20805;&#24403;&#21363;&#25554;&#21363;&#29992;&#30340;&#21161;&#25512;&#22120;&#12290;&#26356;&#26377;&#36259;&#30340;&#26159;&#65292;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#22312;M&#30340;&#24191;&#27867;&#12289;&#26631;&#20934;&#21270;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17343v1 Announce Type: cross  Abstract: In this study, we uncover the unexpected efficacy of residual-based large language models (LLMs) as part of encoders for biomedical imaging tasks, a domain traditionally devoid of language or textual data. The approach diverges from established methodologies by utilizing a frozen transformer block, extracted from pre-trained LLMs, as an innovative encoder layer for the direct processing of visual tokens. This strategy represents a significant departure from the standard multi-modal vision-language frameworks, which typically hinge on language-driven prompts and inputs. We found that these LLMs could boost performance across a spectrum of biomedical imaging applications, including both 2D and 3D visual classification tasks, serving as plug-and-play boosters. More interestingly, as a byproduct, we found that the proposed framework achieved superior performance, setting new state-of-the-art results on extensive, standardized datasets in M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24212;&#29992;&#24341;&#23548;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#20026;&#24503;&#35821;&#21019;&#24314;&#20102;&#26368;&#22823;&#30340;&#20256;&#35760;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21457;&#24067;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.17143</link><description>&lt;p&gt;
&#24341;&#23548;&#36828;&#31243;&#30417;&#30563;&#29992;&#20110;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#65306;&#36866;&#24212;&#26032;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24212;&#29992;&#24341;&#23548;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#20026;&#24503;&#35821;&#21019;&#24314;&#20102;&#26368;&#22823;&#30340;&#20256;&#35760;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21457;&#24067;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;&#20851;&#31995;&#25277;&#21462;&#23545;&#20110;&#22312;&#25968;&#23383;&#20154;&#25991;&#23398;&#21644;&#30456;&#20851;&#23398;&#31185;&#32972;&#26223;&#19979;&#25552;&#21462;&#21644;&#29702;&#35299;&#20256;&#35760;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#31038;&#21306;&#23545;&#26500;&#24314;&#33021;&#22815;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#20851;&#31995;&#30340;&#25968;&#25454;&#38598;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26631;&#27880;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#65292;&#32780;&#19988;&#20165;&#38480;&#20110;&#33521;&#35821;&#12290;&#26412;&#25991;&#24212;&#29992;&#20102;&#24341;&#23548;&#24335;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#20026;&#24503;&#35821;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#20256;&#35760;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#36229;&#36807;80,000&#20010;&#23454;&#20363;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#20851;&#31995;&#31867;&#22411;&#65292;&#26159;&#26368;&#22823;&#30340;&#24503;&#35821;&#20256;&#35760;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;2000&#20010;&#23454;&#20363;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#19982;&#21033;&#29992;&#24341;&#23548;&#24335;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#32534;&#21046;&#30340;&#25968;&#25454;&#38598;&#19968;&#36215;&#21457;&#24067;&#12290;&#25105;&#20204;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17143v1 Announce Type: new  Abstract: Relation extraction is essential for extracting and understanding biographical information in the context of digital humanities and related subjects. There is a growing interest in the community to build datasets capable of training machine learning models to extract relationships. However, annotating such datasets can be expensive and time-consuming, in addition to being limited to English. This paper applies guided distant supervision to create a large biographical relationship extraction dataset for German. Our dataset, composed of more than 80,000 instances for nine relationship types, is the largest biographical German relationship extraction dataset. We also create a manually annotated dataset with 2000 instances to evaluate the models and release it together with the dataset compiled using guided distant supervision. We train several state-of-the-art machine learning models on the automatically created dataset and release them as 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#65292;&#20351;&#33151;&#24335;&#26426;&#22120;&#20154;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#25805;&#20316;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#23454;&#29616;&#20102;&#22312;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.16967</link><description>&lt;p&gt;
&#29992;&#20110;&#33151;&#24335;&#23450;&#28857;&#26426;&#22120;&#20154;&#36816;&#21160;&#25805;&#20316;&#30340;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Visual Whole-Body Control for Legged Loco-Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16967
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#65292;&#20351;&#33151;&#24335;&#26426;&#22120;&#20154;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#25805;&#20316;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#23454;&#29616;&#20102;&#22312;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#37197;&#22791;&#25163;&#33218;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#36827;&#34892;&#31227;&#21160;&#25805;&#20316;&#30340;&#38382;&#39064;&#65292;&#21363;&#33151;&#24335;&#23450;&#28857;&#25805;&#20316;&#12290;&#23613;&#31649;&#26426;&#22120;&#20154;&#30340;&#33151;&#36890;&#24120;&#29992;&#20110;&#31227;&#21160;&#65292;&#20294;&#36890;&#36807;&#36827;&#34892;&#20840;&#36523;&#25511;&#21046;&#65292;&#21487;&#20197;&#25193;&#22823;&#20854;&#25805;&#20316;&#33021;&#21147;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#20854;&#24037;&#20316;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#20351;&#29992;&#35270;&#35273;&#35266;&#27979;&#33258;&#20027;&#36827;&#34892;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;\ourFull~(\our)&#65292;&#30001;&#19968;&#20010;&#20302;&#32423;&#31574;&#30053;&#21644;&#19968;&#20010;&#39640;&#32423;&#31574;&#30053;&#32452;&#25104;&#12290;&#20302;&#32423;&#31574;&#30053;&#20351;&#29992;&#25152;&#26377;&#33258;&#30001;&#24230;&#26469;&#36319;&#36394;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;&#20301;&#32622;&#65292;&#39640;&#32423;&#31574;&#30053;&#26681;&#25454;&#35270;&#35273;&#36755;&#20837;&#25552;&#20986;&#26411;&#31471;&#25191;&#34892;&#22120;&#20301;&#32622;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#20102;&#20004;&#20010;&#32423;&#21035;&#30340;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#20102;&#20174;Sim&#21040;&#23454;&#29289;&#30340;&#36716;&#31227;&#20197;&#36827;&#34892;&#23454;&#38469;&#26426;&#22120;&#20154;&#37096;&#32626;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19981;&#21516;&#37197;&#32622;&#19979;&#65288;&#39640;&#24230;&#12289;&#65289;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#65292;&#30456;&#23545;&#22522;&#32447;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16967v1 Announce Type: cross  Abstract: We study the problem of mobile manipulation using legged robots equipped with an arm, namely legged loco-manipulation. The robot legs, while usually utilized for mobility, offer an opportunity to amplify the manipulation capabilities by conducting whole-body control. That is, the robot can control the legs and the arm at the same time to extend its workspace. We propose a framework that can conduct the whole-body control autonomously with visual observations. Our approach, namely \ourFull~(\our), is composed of a low-level policy using all degrees of freedom to track the end-effector manipulator position and a high-level policy proposing the end-effector position based on visual inputs. We train both levels of policies in simulation and perform Sim2Real transfer for real robot deployment. We perform extensive experiments and show significant improvements over baselines in picking up diverse objects in different configurations (heights,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16915</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31895;&#35843;&#20248;&#30340;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM-based IR&#65289;&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#22312;&#31895;&#35843;&#20248;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#25105;&#20204;&#26088;&#22312;&#20943;&#23569;&#24494;&#35843;&#30340;&#36127;&#25285;&#65292;&#25552;&#39640;&#19979;&#28216;IR&#20219;&#21153;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#31895;&#35843;&#20248;&#30340;&#26597;&#35810;-&#25991;&#26723;&#23545;&#39044;&#27979;&#65288;QDPP&#65289;&#65292;&#20854;&#39044;&#27979;&#26597;&#35810;-&#25991;&#26723;&#23545;&#30340;&#36866;&#24403;&#24615;&#12290;&#35780;&#20272;&#23454;&#39564;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#22235;&#20010;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#30340;MRR&#21644;/&#25110;nDCG@5&#12290;&#27492;&#22806;&#65292;&#26597;&#35810;&#39044;&#27979;&#20219;&#21153;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31895;&#35843;&#20248;&#20419;&#36827;&#20102;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
&lt;/p&gt;</description></item><item><title>DeepMachining&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#26159;&#39318;&#25209;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;</title><link>https://arxiv.org/abs/2403.16451</link><description>&lt;p&gt;
DeepMachining: &#38115;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#22312;&#32447;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeepMachining: Online Prediction of Machining Errors of Lathe Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16451
&lt;/p&gt;
&lt;p&gt;
DeepMachining&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#26159;&#39318;&#25209;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;DeepMachining&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#21152;&#24037;&#35823;&#24046;&#12290;&#25105;&#20204;&#22522;&#20110;&#24037;&#21378;&#30340;&#21046;&#36896;&#25968;&#25454;&#26500;&#24314;&#24182;&#35780;&#20272;&#20102;DeepMachining&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29305;&#23450;&#36710;&#24202;&#26426;&#24202;&#25805;&#20316;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#21152;&#24037;&#29366;&#24577;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#29305;&#23450;&#21152;&#24037;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DeepMachining&#22312;&#28041;&#21450;&#19981;&#21516;&#24037;&#20214;&#21644;&#20992;&#20855;&#30340;&#22810;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#39318;&#25209;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16451v1 Announce Type: cross  Abstract: We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states. Then, we fine-tune the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.
&lt;/p&gt;</description></item><item><title>MEDDAP&#36890;&#36807;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#20449;&#24687;&#26631;&#35760;&#26679;&#26412;&#65292;&#25552;&#21319;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#25928;&#29992;</title><link>https://arxiv.org/abs/2403.16335</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#21270;&#22686;&#24378;&#31649;&#36947;&#25552;&#21319;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#25928;&#29992;&#65306;MEDDAP
&lt;/p&gt;
&lt;p&gt;
MEDDAP: Medical Dataset Enhancement via Diversified Augmentation Pipeline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16335
&lt;/p&gt;
&lt;p&gt;
MEDDAP&#36890;&#36807;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#20449;&#24687;&#26631;&#35760;&#26679;&#26412;&#65292;&#25552;&#21319;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#20381;&#36182;&#20110;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#30103;&#26696;&#20363;&#20013;&#65292;&#25910;&#38598;&#21644;&#27880;&#37322;&#22823;&#35268;&#27169;&#25968;&#25454;&#36890;&#24120;&#26114;&#36149;&#19988;&#32791;&#26102;&#65292;&#23588;&#20854;&#26159;&#24403;&#20174;&#19994;&#32773;&#24050;&#34987;&#24037;&#20316;&#21344;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;MEDDAP&#30340;&#26032;&#22411;&#31649;&#36947;&#65292;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#20449;&#24687;&#26631;&#35760;&#26679;&#26412;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#23567;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16335v1 Announce Type: cross  Abstract: The effectiveness of Deep Neural Networks (DNNs) heavily relies on the abundance and accuracy of available training data. However, collecting and annotating data on a large scale is often both costly and time-intensive, particularly in medical cases where practitioners are already occupied with their duties. Moreover, ensuring that the model remains robust across various scenarios of image capture is crucial in medical domains, especially when dealing with ultrasound images that vary based on the settings of different devices and the manual operation of the transducer. To address this challenge, we introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion (SD) models to augment existing small datasets by automatically generating new informative labeled samples. Pretrained checkpoints for SD are typically based on natural images, and training them for medical images requires significant GPU resources due to their heavy 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20013;&#24515;&#25513;&#34109;&#30340;GLIP&#25216;&#26415;&#22312;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#20013;&#21462;&#20195;&#20102;&#38543;&#26426;&#25513;&#34109;&#65292;&#21033;&#29992;&#39640;&#26031;&#20998;&#24067;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#26131;&#20110;&#33719;&#24471;&#19988;&#36866;&#29992;&#20110;&#19981;&#20855;&#26377;&#26126;&#26174;&#20013;&#24515;&#28966;&#28857;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.15837</link><description>&lt;p&gt;
&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#20013;&#24515;&#25513;&#34109;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Centered Masking for Language-Image Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15837
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20013;&#24515;&#25513;&#34109;&#30340;GLIP&#25216;&#26415;&#22312;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#20013;&#21462;&#20195;&#20102;&#38543;&#26426;&#25513;&#34109;&#65292;&#21033;&#29992;&#39640;&#26031;&#20998;&#24067;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#26131;&#20110;&#33719;&#24471;&#19988;&#36866;&#29992;&#20110;&#19981;&#20855;&#26377;&#26126;&#26174;&#20013;&#24515;&#28966;&#28857;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;GLIP&#65289;&#30340;&#39640;&#26031;&#25513;&#34109;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#30452;&#25509;&#21644;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#22270;&#20687;&#34917;&#19969;&#36827;&#34892;&#25513;&#34109;&#12290;GLIP&#22522;&#20110;&#24555;&#36895;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;FLIP&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;CLIP&#27169;&#22411;&#26102;&#38543;&#26426;&#23631;&#34109;&#22270;&#20687;&#34917;&#19969;&#12290;GLIP&#23558;&#38543;&#26426;&#23631;&#34109;&#26367;&#25442;&#20026;&#20013;&#24515;&#25513;&#34109;&#65292;&#20351;&#29992;&#39640;&#26031;&#20998;&#24067;&#65292;&#24182;&#21463;&#21040;&#22270;&#20687;&#20013;&#24515;&#37325;&#35201;&#24615;&#30340;&#21551;&#21457;&#12290;&#22312;&#19968;&#31995;&#21015;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#65292;GLIP&#20445;&#30041;&#20102;&#19982;FLIP&#30456;&#21516;&#30340;&#35745;&#31639;&#33410;&#30465;&#33021;&#21147;&#65292;&#21516;&#26102;&#25913;&#21892;&#20102;&#24615;&#33021;&#65292;&#36825;&#26159;&#30001;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#25152;&#35777;&#23454;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GLIP&#30340;&#22909;&#22788;&#24456;&#23481;&#26131;&#33719;&#24471;&#65292;&#26080;&#38656;&#31934;&#32454;&#35843;&#25972;&#39640;&#26031;&#65292;&#20063;&#36866;&#29992;&#20110;&#21253;&#21547;&#26080;&#26126;&#26174;&#20013;&#24515;&#28966;&#28857;&#22270;&#29255;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15837v1 Announce Type: cross  Abstract: We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel, straightforward, and effective technique for masking image patches during pre-training of a vision-language model. GLIP builds on Fast Language-Image Pre-Training (FLIP), which randomly masks image patches while training a CLIP model. GLIP replaces random masking with centered masking, that uses a Gaussian distribution and is inspired by the importance of image patches at the center of the image. GLIP retains the same computational savings as FLIP, while improving performance across a range of downstream datasets and tasks, as demonstrated by our experimental results. We show the benefits of GLIP to be easy to obtain, requiring no delicate tuning of the Gaussian, and also applicable to data sets containing images without an obvious center focus.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.14814</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#26426;&#20250;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
The opportunities and risks of large language models in mental health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14814
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#21457;&#29983;&#29575;&#27491;&#22312;&#19978;&#21319;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#29616;&#26377;&#30340;&#24515;&#29702;&#20445;&#20581;&#27169;&#24335;&#26080;&#27861;&#20805;&#20998;&#25193;&#23637;&#20197;&#28385;&#36275;&#38656;&#27714;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#20855;&#26377;&#21019;&#36896;&#26032;&#39062;&#12289;&#22823;&#35268;&#27169;&#35299;&#20915;&#26041;&#26696;&#20197;&#25903;&#25345;&#24515;&#29702;&#20581;&#24247;&#30340;&#25215;&#35834;&#24863;&#21040;&#20048;&#35266;&#12290;&#23613;&#31649;&#23427;&#20204;&#36824;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;LLMs&#24050;&#34987;&#24212;&#29992;&#20110;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#24050;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#21033;&#29992;LLMs&#25552;&#20379;&#24515;&#29702;&#20581;&#24247;&#25945;&#32946;&#12289;&#35780;&#20272;&#21644;&#24178;&#39044;&#30340;&#21162;&#21147;&#65292;&#24182;&#31361;&#20986;&#20102;&#27599;&#20010;&#39046;&#22495;&#20013;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#30340;&#20851;&#38190;&#26426;&#20250;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23558;LLMs&#24212;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#25152;&#20276;&#38543;&#30340;&#39118;&#38505;&#65292;&#24182;&#40723;&#21169;&#37319;&#29992;&#31574;&#30053;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#36843;&#20999;&#38656;&#27714;&#24517;&#39035;&#19982;&#36127;&#36131;&#20219;&#30340;&#24515;&#29702;&#20581;&#24247;LLMs&#30340;&#24320;&#21457;&#12289;&#27979;&#35797;&#21644;&#37096;&#32626;&#30456;&#24179;&#34913;&#12290;&#29305;&#21035;&#20851;&#38190;&#30340;&#26159;&#30830;&#20445;&#24515;&#29702;&#20581;&#24247;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14814v1 Announce Type: cross  Abstract: Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this review, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. Especially critical is ensuring that mental he
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#31616;&#21270;&#21518;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#36890;&#36807;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#30340;&#32479;&#19968;&#35299;&#20915;&#20102;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.14623</link><description>&lt;p&gt;
&#31616;&#21270;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;
&lt;/p&gt;
&lt;p&gt;
Simplified Diffusion Schr\"odinger Bridge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14623
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#31616;&#21270;&#21518;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#36890;&#36807;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#30340;&#32479;&#19968;&#35299;&#20915;&#20102;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#31616;&#21270;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#20415;&#20110;&#23558;&#20854;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#32479;&#19968;&#36215;&#26469;&#65292;&#35299;&#20915;&#20102;DSB&#22312;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#22686;&#24378;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;SGM&#20316;&#20026;DSB&#30340;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36825;&#20004;&#20010;&#26694;&#26550;&#30340;&#20248;&#21183;&#65292;&#30830;&#20445;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25913;&#36827;&#20102;SGM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#26415;&#65292;&#23613;&#31649;&#23384;&#22312;&#29702;&#35770;&#36817;&#20284;&#65292;&#20294;&#23454;&#38469;&#19978;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#25311;&#21512;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#35777;&#23454;&#20102;&#31616;&#21270;&#30340;DSB&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#24037;&#20316;&#30340;&#36129;&#29486;&#20026;&#20808;&#36827;&#30340;&#29983;&#25104;&#24314;&#27169;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14623v1 Announce Type: new  Abstract: This paper introduces a novel theoretical simplification of the Diffusion Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based Generative Models (SGMs), addressing the limitations of DSB in complex data generation and enabling faster convergence and enhanced performance. By employing SGMs as an initial solution for DSB, our approach capitalizes on the strengths of both frameworks, ensuring a more efficient training process and improving the performance of SGM. We also propose a reparameterization technique that, despite theoretical approximations, practically improves the network's fitting capabilities. Our extensive experimental evaluations confirm the effectiveness of the simplified DSB, demonstrating its significant improvements. We believe the contributions of this work pave the way for advanced generative modeling. The code is available at https://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;Robust Average Gradient Algorithm&#65288;RAGA&#65289;&#65292;&#26412;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;RAGA&#30340;&#33391;&#22909;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13374</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#33258;&#36866;&#24212;&#30340;&#25308;&#21344;&#24237;&#24377;&#24615;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13374
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;Robust Average Gradient Algorithm&#65288;RAGA&#65289;&#65292;&#26412;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;RAGA&#30340;&#33391;&#22909;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22788;&#29702;&#20102;&#22312;&#23384;&#22312;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#24179;&#22343;&#26799;&#24230;&#31639;&#27861;&#65288;RAGA&#65289;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20960;&#20309;&#20013;&#20301;&#25968;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#21487;&#20197;&#33258;&#30001;&#36873;&#25321;&#26412;&#22320;&#26356;&#26032;&#30340;&#36718;&#25968;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24377;&#24615;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#25110;&#22343;&#21248;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25910;&#25947;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#24378;&#20984;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#22312;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#21482;&#35201;&#24694;&#24847;&#29992;&#25143;&#25968;&#25454;&#38598;&#30340;&#27604;&#20363;&#23567;&#20110;&#19968;&#21322;&#65292;RAGA&#23601;&#21487;&#20197;&#20197;$\mathcal{O}({1}/{T^{2/3- \delta}})$&#30340;&#36895;&#24230;&#23454;&#29616;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#25910;&#25947;&#65292;&#20854;&#20013;$T$&#20026;&#36845;&#20195;&#27425;&#25968;&#65292;$\delta \in (0, 2/3)$&#65292;&#23545;&#20110;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#21017;&#21576;&#32447;&#24615;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#31283;&#23450;&#28857;&#25110;&#20840;&#23616;&#26368;&#20248;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13374v1 Announce Type: new  Abstract: This paper deals with federated learning (FL) in the presence of malicious Byzantine attacks and data heterogeneity. A novel Robust Average Gradient Algorithm (RAGA) is proposed, which leverages the geometric median for aggregation and can freely select the round number for local updating. Different from most existing resilient approaches, which perform convergence analysis based on strongly-convex loss function or homogeneously distributed dataset, we conduct convergence analysis for not only strongly-convex but also non-convex loss function over heterogeneous dataset. According to our theoretical analysis, as long as the fraction of dataset from malicious users is less than half, RAGA can achieve convergence at rate $\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and $\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for strongly-convex loss function. Moreover, stationary point or global optim
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#32534;&#30721;&#24067;&#26009;&#27169;&#25311;&#30340;&#29289;&#29702;&#29305;&#24449;&#65292;&#23454;&#29616;&#24555;&#36895;&#21644;&#23454;&#26102;&#27169;&#25311;&#65292;&#24182;&#22312;&#19981;&#20351;&#29992;&#26032;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#27979;&#35797;&#34920;&#29616;&#20986;&#19982;&#22522;&#32447;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12820</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#24067;&#26009;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
A Physics-embedded Deep Learning Framework for Cloth Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#32534;&#30721;&#24067;&#26009;&#27169;&#25311;&#30340;&#29289;&#29702;&#29305;&#24449;&#65292;&#23454;&#29616;&#24555;&#36895;&#21644;&#23454;&#26102;&#27169;&#25311;&#65292;&#24182;&#22312;&#19981;&#20351;&#29992;&#26032;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#27979;&#35797;&#34920;&#29616;&#20986;&#19982;&#22522;&#32447;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#30340;&#24067;&#26009;&#27169;&#25311;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#25152;&#26399;&#26395;&#30340;&#12290;&#20026;&#25913;&#36827;&#21463;&#21147;&#20132;&#20114;&#12289;&#30896;&#25758;&#22788;&#29702;&#21644;&#25968;&#20540;&#31215;&#20998;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#12290;&#28145;&#24230;&#23398;&#20064;&#26377;&#28508;&#21147;&#23454;&#29616;&#24555;&#36895;&#21644;&#23454;&#26102;&#27169;&#25311;&#65292;&#20294;&#24120;&#35265;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#21442;&#25968;&#26469;&#25429;&#33719;&#24067;&#26009;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#32534;&#30721;&#24067;&#26009;&#27169;&#25311;&#29289;&#29702;&#29305;&#24449;&#30340;&#29289;&#29702;&#23884;&#20837;&#23398;&#20064;&#26694;&#26550;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#34920;&#31034;&#36136;&#28857;-&#24377;&#31783;&#31995;&#32479;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#20043;&#21518;&#35774;&#35745;&#20102;&#19977;&#20010;&#20998;&#25903;&#26469;&#23398;&#20064;&#24067;&#26009;&#29289;&#29702;&#30340;&#32447;&#24615;&#12289;&#38750;&#32447;&#24615;&#21644;&#26102;&#38388;&#23548;&#25968;&#29305;&#24449;&#12290;&#35813;&#26694;&#26550;&#36824;&#21487;&#20197;&#36890;&#36807;&#20256;&#32479;&#27169;&#25311;&#22120;&#25110;&#23376;&#31070;&#32463;&#32593;&#32476;&#19982;&#20854;&#20182;&#22806;&#37096;&#21147;&#21644;&#30896;&#25758;&#22788;&#29702;&#36827;&#34892;&#38598;&#25104;&#12290;&#27169;&#22411;&#22312;&#19981;&#20351;&#29992;&#26032;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19981;&#21516;&#30340;&#24067;&#26009;&#21160;&#30011;&#26696;&#20363;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#19982;&#22522;&#32447;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12820v1 Announce Type: cross  Abstract: Delicate cloth simulations have long been desired in computer graphics. Various methods were proposed to improve engaged force interactions, collision handling, and numerical integrations. Deep learning has the potential to achieve fast and real-time simulation, but common neural network structures often demand many parameters to capture cloth dynamics. This paper proposes a physics-embedded learning framework that directly encodes physical features of cloth simulation. The convolutional neural network is used to represent spatial correlations of the mass-spring system, after which three branches are designed to learn linear, nonlinear, and time derivate features of cloth physics. The framework can also integrate with other external forces and collision handling through either traditional simulators or sub neural networks. The model is tested across different cloth animation cases, without training with new data. Agreement with baselin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20989;&#25968;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#25968;&#23383;&#20581;&#24247;&#21644;&#32437;&#21521;&#30740;&#31350;&#20013;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#22797;&#26434;&#24615;&#65292;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#20219;&#21153;&#29305;&#23450;&#23884;&#20837;&#32452;&#20214;&#12289;&#25191;&#34892;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#21019;&#24314;&#30693;&#35782;&#22270;&#36827;&#34892;&#25968;&#25454;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.10158</link><description>&lt;p&gt;
&#20989;&#25968;&#22270;&#21367;&#31215;&#32593;&#32476;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#20419;&#36827;&#20581;&#24247;&#21644;&#31038;&#20250;&#20851;&#24576;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Functional Graph Convolutional Networks: A unified multi-task and multi-modal learning framework to facilitate health and social-care insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20989;&#25968;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#25968;&#23383;&#20581;&#24247;&#21644;&#32437;&#21521;&#30740;&#31350;&#20013;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#22797;&#26434;&#24615;&#65292;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#20219;&#21153;&#29305;&#23450;&#23884;&#20837;&#32452;&#20214;&#12289;&#25191;&#34892;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#21019;&#24314;&#30693;&#35782;&#22270;&#36827;&#34892;&#25968;&#25454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20989;&#25968;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;funGCN&#65289;&#26694;&#26550;&#65292;&#23558;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#25968;&#23383;&#20581;&#24247;&#21644;&#32437;&#21521;&#30740;&#31350;&#20013;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#12290;&#38543;&#30528;&#20581;&#24247;&#35299;&#20915;&#26041;&#26696;&#23545;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#21644;&#31038;&#20250;&#25903;&#25345;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#38271;&#65292;&#30830;&#20445;&#21508;&#24180;&#40836;&#27573;&#30340;&#20581;&#24247;&#29983;&#27963;&#21644;&#20419;&#36827;&#24184;&#31119;&#24863;&#65292;funGCN&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22810;&#20010;&#23454;&#20307;&#30340;&#22810;&#20803;&#32437;&#21521;&#25968;&#25454;&#65292;&#24182;&#30830;&#20445;&#21363;&#20351;&#22312;&#26679;&#26412;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#20063;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#31649;&#29702;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#23884;&#20837;&#32452;&#20214;&#12289;&#25191;&#34892;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#21019;&#24314;&#30693;&#35782;&#22270;&#20197;&#33719;&#21462;&#27934;&#23519;&#24615;&#25968;&#25454;&#35299;&#37322;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#21644;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#39564;&#35777;&#20102;funGCN&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10158v1 Announce Type: cross  Abstract: This paper introduces a novel Functional Graph Convolutional Network (funGCN) framework that combines Functional Data Analysis and Graph Convolutional Networks to address the complexities of multi-task and multi-modal learning in digital health and longitudinal studies. With the growing importance of health solutions to improve health care and social support, ensure healthy lives, and promote well-being at all ages, funGCN offers a unified approach to handle multivariate longitudinal data for multiple entities and ensures interpretability even with small sample sizes. Key innovations include task-specific embedding components that manage different data types, the ability to perform classification, regression, and forecasting, and the creation of a knowledge graph for insightful data interpretation. The efficacy of funGCN is validated through simulation experiments and a real-data application.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#32435;&#26031;&#36798;&#20811;&#20132;&#26131;&#25152;&#32929;&#31080;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;&#20013;&#38388;&#20215;&#26684;&#21464;&#21160;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#25805;&#20316;&#26694;&#26550;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09267</link><description>&lt;p&gt;
&#28145;&#24230;&#38480;&#20215;&#35746;&#21333;&#31807;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Limit Order Book Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09267
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#32435;&#26031;&#36798;&#20811;&#20132;&#26131;&#25152;&#32929;&#31080;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;&#20013;&#38388;&#20215;&#26684;&#21464;&#21160;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#25805;&#20316;&#26694;&#26550;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#23574;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25506;&#32034;&#20102;&#22312;&#32435;&#26031;&#36798;&#20811;&#20132;&#26131;&#25152;&#19978;&#20132;&#26131;&#30340;&#19968;&#32452;&#24322;&#36136;&#32929;&#31080;&#30340;&#39640;&#39057;&#38480;&#20215;&#35746;&#21333;&#31807;&#20013;&#38388;&#20215;&#26684;&#21464;&#21160;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#8220;LOBFrame&#8221;&#65292;&#19968;&#20010;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#21487;&#20197;&#39640;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#38480;&#20215;&#35746;&#21333;&#31807;&#25968;&#25454;&#65292;&#24182;&#23450;&#37327;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#21452;&#37325;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#32929;&#31080;&#30340;&#24494;&#35266;&#32467;&#26500;&#29305;&#24449;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#39640;&#39044;&#27979;&#33021;&#21147;&#19981;&#19968;&#23450;&#23545;&#24212;&#21487;&#25805;&#20316;&#30340;&#20132;&#26131;&#20449;&#21495;&#12290;&#25105;&#20204;&#35748;&#20026;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#25351;&#26631;&#26410;&#33021;&#20805;&#20998;&#35780;&#20272;&#38480;&#20215;&#35746;&#21333;&#31807;&#29615;&#22659;&#20013;&#39044;&#27979;&#30340;&#36136;&#37327;&#12290;&#20316;&#20026;&#26367;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#25805;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;&#20934;&#30830;&#39044;&#27979;&#30340;&#27010;&#29575;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09267v1 Announce Type: cross  Abstract: We exploit cutting-edge deep learning methodologies to explore the predictability of high-frequency Limit Order Book mid-price changes for a heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we release `LOBFrame', an open-source code base, to efficiently process large-scale Limit Order Book data and quantitatively assess state-of-the-art deep learning models' forecasting capabilities. Our results are twofold. We demonstrate that the stocks' microstructural characteristics influence the efficacy of deep learning methods and that their high forecasting power does not necessarily correspond to actionable trading signals. We argue that traditional machine learning metrics fail to adequately assess the quality of forecasts in the Limit Order Book context. As an alternative, we propose an innovative operational framework that assesses predictions' practicality by focusing on the probability of accurately forecasting com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#23545;&#27491;&#20132;&#22522;&#20998;&#27573;&#22810;&#39033;&#24335;&#36924;&#36817;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.08579</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#30340;&#27491;&#20132;&#22522;&#20998;&#27573;&#22810;&#39033;&#24335;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Optimized Orthogonal Basis Piecewise Polynomial Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#23545;&#27491;&#20132;&#22522;&#20998;&#27573;&#22810;&#39033;&#24335;&#36924;&#36817;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#27573;&#22810;&#39033;&#24335;&#65288;PPs&#65289;&#22312;&#20960;&#20010;&#24037;&#31243;&#23398;&#31185;&#20013;&#34987;&#20351;&#29992;&#65292;&#27604;&#22914;&#22312;&#36712;&#36857;&#35268;&#21010;&#20013;&#65292;&#29992;&#26469;&#36924;&#36817;&#20197;&#19968;&#32452;&#28857;&#32473;&#20986;&#30340;&#20301;&#32622;&#36718;&#24275;&#12290; &#37492;&#20110;&#36924;&#36817;&#30446;&#26631;&#20197;&#21450;&#29305;&#23450;&#39046;&#22495;&#35201;&#27714;&#65292;&#27604;&#22914;Ck-&#36830;&#32493;&#24615;&#65292;&#21487;&#20197;&#34987;&#26500;&#36896;&#20026;&#19968;&#20010;&#26041;&#31243;&#32452;&#65292;&#32467;&#26524;&#21487;&#20197;&#30452;&#25509;&#35745;&#31639;&#65292;&#36825;&#26679;&#30340;&#38381;&#24335;&#35299;&#23545;&#20110;&#22810;&#39033;&#24335;&#27425;&#25968;&#12289;&#22810;&#39033;&#24335;&#22522;&#30784;&#25110;&#32773;&#28155;&#21152;&#36827;&#19968;&#27493;&#30340;&#29305;&#23450;&#39046;&#22495;&#35201;&#27714;&#26041;&#38754;&#20855;&#26377;&#26377;&#38480;&#30340;&#28789;&#27963;&#24615;&#12290;&#36275;&#22815;&#22797;&#26434;&#30340;&#20248;&#21270;&#30446;&#26631;&#24456;&#24555;&#35201;&#27714;&#20351;&#29992;&#25968;&#20540;&#26041;&#27861;&#65292;&#27604;&#22914;&#26799;&#24230;&#19979;&#38477;&#12290;&#30001;&#20110;&#26799;&#24230;&#19979;&#38477;&#26159;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#26680;&#24515;&#65292;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26694;&#26550;&#27604;&#22914;TensorFlow&#25552;&#20379;&#20102;&#19968;&#22871;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#65292;&#21487;&#33021;&#36866;&#29992;&#20110;&#35757;&#32451;&#20219;&#21153;&#20043;&#22806;&#30340;&#24191;&#27867;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;PP&#27169;&#24335;&#30340;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08579v1 Announce Type: new  Abstract: Piecewise Polynomials (PPs) are utilized in several engineering disciplines, like trajectory planning, to approximate position profiles given in the form of a set of points. While the approximation target along with domain-specific requirements, like Ck -continuity, can be formulated as a system of equations and a result can be computed directly, such closed-form solutions posses limited flexibility with respect to polynomial degrees, polynomial bases or adding further domain-specific requirements. Sufficiently complex optimization goals soon call for the use of numerical methods, like gradient descent. Since gradient descent lies at the heart of training Artificial Neural Networks (ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set of gradient-based optimizers potentially suitable for a wide range of optimization problems beyond the training task for ANNs. Our approach is to utilize the versatility of PP mode
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.06054</link><description>&lt;p&gt;
&#20855;&#26377;&#25193;&#25955;&#20928;&#21270;&#30340;&#20998;&#31163;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Decoupled Data Consistency with Diffusion Purification for Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06054
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#31867;&#21035;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#22320;&#24314;&#27169;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#35299;&#20915;&#22270;&#20687;&#24674;&#22797;&#38382;&#39064;&#65292;&#35768;&#22810;&#29616;&#26377;&#25216;&#26415;&#36890;&#36807;&#23558;&#39069;&#22806;&#30340;&#20284;&#28982;&#26799;&#24230;&#27493;&#39588;&#32435;&#20837;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#21453;&#21521;&#37319;&#26679;&#36807;&#31243;&#20013;&#26469;&#23454;&#29616;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#26799;&#24230;&#27493;&#39588;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#25512;&#29702;&#26102;&#38388;&#12290;&#24403;&#20351;&#29992;&#21152;&#36895;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#22120;&#26102;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#27493;&#39588;&#36824;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#22256;&#38590;&#65292;&#22240;&#20026;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#30340;&#25968;&#37327;&#21463;&#38480;&#20110;&#21453;&#21521;&#37319;&#26679;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#23558;&#21453;&#21521;&#36807;&#31243;&#19982;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#20998;&#31163;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06054v1 Announce Type: cross  Abstract: Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involv
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#23545;&#25968;&#27491;&#23450;&#32534;&#30721;&#65288;LP&#65289;&#21644;LP&#37327;&#21270;&#65288;LPQ&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#22240;&#31639;&#27861;&#23547;&#25214;&#26368;&#20248;&#30340;LP&#21442;&#25968;&#65292;&#35774;&#35745;&#20102;&#32479;&#19968;&#30340;&#28151;&#21512;&#31934;&#24230;LP&#21152;&#36895;&#22120;&#65288;LPA&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#21160;&#24577;&#36866;&#24212;DNN&#21442;&#25968;&#20998;&#24067;&#65292;&#20943;&#23569;&#37327;&#21270;&#21644;&#23436;&#25972;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#24615;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.05465</link><description>&lt;p&gt;
&#20998;&#24067;&#24863;&#30693;&#23545;&#25968;&#27491;&#23450;&#32534;&#30721;&#30340;&#31639;&#27861;&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;DNN&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05465
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#23545;&#25968;&#27491;&#23450;&#32534;&#30721;&#65288;LP&#65289;&#21644;LP&#37327;&#21270;&#65288;LPQ&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#22240;&#31639;&#27861;&#23547;&#25214;&#26368;&#20248;&#30340;LP&#21442;&#25968;&#65292;&#35774;&#35745;&#20102;&#32479;&#19968;&#30340;&#28151;&#21512;&#31934;&#24230;LP&#21152;&#36895;&#22120;&#65288;LPA&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#21160;&#24577;&#36866;&#24212;DNN&#21442;&#25968;&#20998;&#24067;&#65292;&#20943;&#23569;&#37327;&#21270;&#21644;&#23436;&#25972;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#37327;&#21270;&#26041;&#27861;&#20351;&#29992;&#25972;&#25968;&#12289;&#23450;&#28857;&#25110;&#28014;&#28857;&#25968;&#25454;&#31867;&#22411;&#26102;&#65292;&#24448;&#24448;&#38590;&#20197;&#22312;&#20302;&#31934;&#24230;&#19979;&#25429;&#25417;&#19981;&#21516;&#30340;DNN&#21442;&#25968;&#20998;&#24067;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30789;&#24320;&#38144;&#21644;&#23494;&#38598;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#25968;&#27491;&#23450;&#65288;LP&#65289;&#32534;&#30721;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#27491;&#23450;&#21551;&#21457;&#30340;&#33258;&#36866;&#24212;&#12289;&#30828;&#20214;&#21451;&#22909;&#30340;&#25968;&#25454;&#31867;&#22411;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;LP&#20301;&#22495;&#21160;&#24577;&#36866;&#24212;DNN&#26435;&#37325;/&#28608;&#27963;&#20998;&#24067;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;LP&#37327;&#21270;&#65288;LPQ&#65289;&#65292;&#29992;&#20110;&#23547;&#25214;&#26368;&#20248;&#30340;&#36880;&#23618;LP&#21442;&#25968;&#65292;&#21516;&#26102;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;-&#23616;&#37096;&#23545;&#27604;&#30446;&#26631;&#20943;&#23569;&#37327;&#21270;&#21644;&#23436;&#25972;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#24615;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#28151;&#21512;&#31934;&#24230;LP&#21152;&#36895;&#22120;&#65288;LPA&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#21253;&#25324;&#23558;LP&#32435;&#20837;&#35745;&#31639;&#25968;&#25454;&#36890;&#36335;&#20013;&#30340;&#22788;&#29702;&#21333;&#20803;&#65288;PEs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05465v1 Announce Type: cross  Abstract: Traditional Deep Neural Network (DNN) quantization methods using integer, fixed-point, or floating-point data types struggle to capture diverse DNN parameter distributions at low precision, and often require large silicon overhead and intensive quantization-aware training. In this study, we introduce Logarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by posits that dynamically adapts to DNN weight/activation distributions by parameterizing LP bit fields. We also develop a novel genetic-algorithm based framework, LP Quantization (LPQ), to find optimal layer-wise LP parameters while reducing representational divergence between quantized and full-precision models through a novel global-local contrastive objective. Additionally, we design a unified mixed-precision LP accelerator (LPA) architecture comprising of processing elements (PEs) incorporating LP in the computational datapath. Our algorithm-hardware co-design
&lt;/p&gt;</description></item><item><title>NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;</title><link>https://arxiv.org/abs/2403.03100</link><description>&lt;p&gt;
NaturalSpeech 3: &#21033;&#29992;&#20998;&#35299;&#32534;&#35299;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03100
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#22312;&#35821;&#38899;&#36136;&#37327;&#12289;&#30456;&#20284;&#24230;&#21644;&#38901;&#24459;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#12290;&#37492;&#20110;&#35821;&#38899;&#22797;&#26434;&#22320;&#21253;&#21547;&#21508;&#31181;&#23646;&#24615;&#65288;&#20363;&#22914;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#65289;&#65292;&#32473;&#29983;&#25104;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#24819;&#27861;&#26159;&#23558;&#35821;&#38899;&#22240;&#23376;&#20998;&#35299;&#20026;&#20195;&#34920;&#19981;&#21516;&#23646;&#24615;&#30340;&#21508;&#20010;&#23376;&#31354;&#38388;&#65292;&#24182;&#21333;&#29420;&#29983;&#25104;&#23427;&#20204;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NaturalSpeech 3&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#26032;&#39062;&#30340;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;TTS&#31995;&#32479;&#65292;&#21487;&#20197;&#20197;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;1) &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#20998;&#35299;&#21521;&#37327;&#37327;&#21270;&#65288;FVQ&#65289;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#20998;&#35299;&#20026;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#30340;&#23376;&#31354;&#38388;&#65307;2) &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#65292;&#26681;&#25454;&#20854;&#30456;&#24212;&#30340;&#25552;&#31034;&#29983;&#25104;&#27599;&#20010;&#23376;&#31354;&#38388;&#20013;&#30340;&#23646;&#24615;&#12290;&#20511;&#21161;&#36825;&#31181;&#20998;&#35299;&#35774;&#35745;&#65292;NaturalSpeech 3&#33021;&#22815;ef
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03100v1 Announce Type: cross  Abstract: While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can ef
&lt;/p&gt;</description></item><item><title>&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#26041;&#26696;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#20445;&#35777;&#20102;&#27599;&#27425;&#36845;&#20195;&#24555;&#36895;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#65292;&#30028;&#38480;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;&#21644;&#28151;&#21512;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.11800</link><description>&lt;p&gt;
&#20855;&#26377;&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#65306;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#30340;&#26377;&#38480;&#26102;&#38388;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11800
&lt;/p&gt;
&lt;p&gt;
&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#26041;&#26696;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#20445;&#35777;&#20102;&#27599;&#27425;&#36845;&#20195;&#24555;&#36895;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#65292;&#30028;&#38480;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;&#21644;&#28151;&#21512;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#22823;&#35268;&#27169;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#20855;&#26377;&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#26041;&#26696;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;&#34429;&#28982;&#24310;&#36831;&#30340;&#24433;&#21709;&#22312;&#20248;&#21270;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#19982;&#24213;&#23618;&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#30456;&#20114;&#20316;&#29992;&#20197;&#22609;&#36896;SA&#30340;&#26377;&#38480;&#26102;&#38388;&#24615;&#33021;&#30340;&#26041;&#24335;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#35777;&#26126;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#24310;&#36831;&#30340;SA&#26356;&#26032;&#35268;&#21017;&#30830;&#20445;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#21040;SA&#36816;&#31639;&#31526;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#20855;&#26377;&#25351;&#25968;&#24555;&#36895;&#30340;&#36895;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;$\tau_{max}$&#21644;&#28151;&#21512;&#26102;&#38388;$\tau_{mix}$&#26041;&#38754;&#26159;\emph{&#32039;&#33268;&#30340;}&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#32039;&#23494;&#30028;&#38480;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24402;&#32435;&#35777;&#26126;&#25216;&#26415;&#65292;&#19982;&#21508;&#31181;&#29616;&#26377;&#24310;&#36831;&#20248;&#21270;&#20998;&#26512;&#19981;&#21516;&#65292;&#23427;&#20381;&#36182;&#20110;&#24314;&#31435;&#26410;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11800v1 Announce Type: cross  Abstract: Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling. While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood. In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \emph{last iterate} to a ball around the SA operator's fixed point. Notably, our bound is \emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing un
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.09283</link><description>&lt;p&gt;
&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#24212;&#29992;&#20013;&#24050;&#32463;&#24456;&#24120;&#35265;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#22238;&#22797;&#30340;&#39118;&#38505;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#31038;&#20250;&#20851;&#20999;&#65292;&#24182;&#28608;&#21457;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#22312;&#27492;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26368;&#36817;&#30740;&#31350;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#25688;&#35201;&#65292;&#22686;&#36827;&#23545;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#19968;&#37325;&#35201;&#35838;&#39064;&#12290;&#20026;&#20102;&#26041;&#20415;&#21442;&#32771;&#65292;&#25105;&#20204;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#23545;&#25152;&#26377;&#22312;&#27492;&#35843;&#26597;&#20013;&#25552;&#21040;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/niconi19/LLM-conversation-safety&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 Announce Type: new Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#65292;&#21033;&#29992;&#23884;&#22871;&#31890;&#23376;&#28388;&#27874;&#22120;&#21644;&#31435;&#20307;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#26469;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07868</link><description>&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;&#30340;&#23884;&#22871;&#31890;&#23376;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nesting Particle Filters for Experimental Design in Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#65292;&#21033;&#29992;&#23884;&#22871;&#31890;&#23376;&#28388;&#27874;&#22120;&#21644;&#31435;&#20307;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#26469;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#20132;&#25442;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#39118;&#38505;&#25935;&#24863;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20869;&#22806;SMC^2&#31639;&#27861;&#65292;&#20351;&#29992;&#23884;&#22871;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#65288;SMC&#65289;&#20272;&#35745;&#22120;&#26469;&#39044;&#27979;&#26399;&#26395;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#21040;&#31890;&#23376;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;pMCMC&#65289;&#26694;&#26550;&#20013;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#19982;&#26368;&#36817;&#20381;&#36182;&#20110;&#20559;&#20272;&#35745;&#22120;&#26469;&#25674;&#38144;&#20808;&#21069;&#23398;&#20064;&#35774;&#35745;&#31574;&#30053;&#30340;&#25104;&#26412;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#19968;&#32452;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#20540;&#39564;&#35777;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel approach to Bayesian Experimental Design (BED) for non-exchangeable data that formulates it as risk-sensitive policy optimization. We develop the Inside-Out SMC^2 algorithm that uses a nested sequential Monte Carlo (SMC) estimator of the expected information gain and embeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform gradient-based policy optimization. This is in contrast to recent approaches that rely on biased estimators of the expected information gain (EIG) to amortize the cost of experiments by learning a design policy in advance. Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21644;&#32858;&#31867;&#26041;&#27861;&#30340;&#20027;&#21160;&#23398;&#20064;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;&#22522;&#30784;&#27169;&#22411;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24471;&#21040;&#30340;&#65292;&#24182;&#33021;&#29983;&#25104;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#32039;&#20945;&#23884;&#20837;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.02561</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#32858;&#31867;&#20248;&#21270;&#20027;&#21160;&#23398;&#20064;&#21021;&#22987;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Foundation Model Makes Clustering a Better Initialization for Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21644;&#32858;&#31867;&#26041;&#27861;&#30340;&#20027;&#21160;&#23398;&#20064;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;&#22522;&#30784;&#27169;&#22411;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24471;&#21040;&#30340;&#65292;&#24182;&#33021;&#29983;&#25104;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#32039;&#20945;&#23884;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#20197;&#28385;&#36275;&#26377;&#38480;&#30340;&#26631;&#27880;&#39044;&#31639;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#35768;&#22810;&#26041;&#27861;&#38024;&#23545;&#21021;&#22987;&#21270;&#27169;&#22411;&#21518;&#30340;&#26679;&#26412;&#36873;&#25321;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#24517;&#19981;&#21487;&#23569;&#30340;&#21021;&#22987;&#21270;&#38454;&#27573;&#65292;&#21364;&#27809;&#26377;&#24341;&#36215;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#22823;&#22810;&#25968;&#37117;&#37319;&#29992;&#38543;&#26426;&#25277;&#26679;&#25110;&#32773;&#31616;&#21333;&#30340;&#32858;&#31867;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#25277;&#26679;&#23481;&#26131;&#20135;&#29983;&#27874;&#21160;&#65292;&#32780;&#31616;&#21333;&#32858;&#31867;&#22312;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#25968;&#25454;&#65289;&#26102;&#25910;&#25947;&#36895;&#24230;&#24930;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22522;&#30784;&#27169;&#22411;&#19982;&#32858;&#31867;&#26041;&#27861;&#32467;&#21512;&#65292;&#29992;&#20110;&#36873;&#25321;&#20027;&#21160;&#23398;&#20064;&#21021;&#22987;&#21270;&#38454;&#27573;&#30340;&#26679;&#26412;&#12290;&#22522;&#30784;&#27169;&#22411;&#26159;&#25351;&#22312;&#33258;&#30417;&#30563;&#33539;&#24335;&#19979;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#19988;&#32039;&#20945;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning selects the most informative samples from the unlabeled dataset to annotate in the context of a limited annotation budget. While numerous methods have been proposed for subsequent sample selection based on an initialized model, scant attention has been paid to the indispensable phase of active learning: selecting samples for model initialization. Most of the previous studies resort to random sampling or naive clustering. However, random sampling is prone to fluctuation, and naive clustering suffers from convergence speed, particularly when dealing with high-dimensional data such as imaging data. In this work, we propose to integrate foundation models with clustering methods to select samples for active learning initialization. Foundation models refer to those trained on massive datasets by the self-supervised paradigm and capable of generating informative and compacted embeddings for various downstream tasks. Leveraging these embeddings to replace raw features such as p
&lt;/p&gt;</description></item><item><title>OpenMoE&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#21457;&#24067;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#22797;&#29616;&#24615;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;MoE&#27169;&#22411;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#24182;&#19988;&#36827;&#34892;&#20102;&#23545;&#36335;&#30001;&#26426;&#21046;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.01739</link><description>&lt;p&gt;
OpenMoE&#65306;&#24320;&#28304;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#30340;&#26089;&#26399;&#21162;&#21147;
&lt;/p&gt;
&lt;p&gt;
OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01739
&lt;/p&gt;
&lt;p&gt;
OpenMoE&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#21457;&#24067;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#22797;&#29616;&#24615;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;MoE&#27169;&#22411;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#24182;&#19988;&#36827;&#34892;&#20102;&#23545;&#36335;&#30001;&#26426;&#21046;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24110;&#21161;&#24320;&#28304;&#31038;&#21306;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;&#28151;&#21512;&#19987;&#23478;(MoE)&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#21457;&#24067;&#20102;OpenMoE&#65292;&#19968;&#31995;&#21015;&#23436;&#20840;&#24320;&#25918;&#28304;&#30721;&#21644;&#21487;&#22797;&#29616;&#30340;&#20165;&#35299;&#30721;&#22120;MoE LLM&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;650M&#21040;34B&#65292;&#35757;&#32451;&#25968;&#25454;&#36229;&#36807;1T&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#23454;&#65292;MoE-based LLM&#21487;&#20197;&#25552;&#20379;&#27604;&#23494;&#38598;LLM&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#31361;&#20986;&#20102;&#26410;&#26469;LLM&#24320;&#21457;&#30340;&#28508;&#22312;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#23545;&#25105;&#20204;&#30340;OpenMoE&#27169;&#22411;&#20013;&#30340;&#36335;&#30001;&#26426;&#21046;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#65306;&#19978;&#19979;&#25991;&#26080;&#20851;&#19987;&#19994;&#21270;&#12289;&#26089;&#26399;&#36335;&#30001;&#23398;&#20064;&#21644;&#26411;&#23614;&#38477;&#20302;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;MoE&#27169;&#22411;&#20013;&#30340;&#36335;&#30001;&#20915;&#31574;&#20027;&#35201;&#22522;&#20110;&#26631;&#35760;ID&#65292;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#24456;&#23567;&#12290;&#26631;&#35760;&#21040;&#19987;&#23478;&#30340;&#20998;&#37197;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26089;&#26399;&#30830;&#23450;&#65292;&#24182;&#19988;&#22522;&#26412;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#31181;&#19981;&#23436;&#20840;&#30340;&#36335;&#30001;&#21487;&#33021;&#23548;&#33268;...
&lt;/p&gt;
&lt;p&gt;
To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development.   One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can resu
&lt;/p&gt;</description></item><item><title>SPO&#31639;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;KL&#25955;&#24230;&#21098;&#20999;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;PPO&#30340;&#20027;&#27969;&#21464;&#20307;&#65292;&#22312;Atari 2600&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#12289;&#26497;&#20302;&#30340;KL&#25955;&#24230;&#21644;&#26356;&#39640;&#30340;&#31574;&#30053;&#29109;&#65292;&#19988;&#23545;&#32593;&#32476;&#28145;&#24230;&#25110;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.16025</link><description>&lt;p&gt;
&#31616;&#21333;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simple Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16025
&lt;/p&gt;
&lt;p&gt;
SPO&#31639;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;KL&#25955;&#24230;&#21098;&#20999;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;PPO&#30340;&#20027;&#27969;&#21464;&#20307;&#65292;&#22312;Atari 2600&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#12289;&#26497;&#20302;&#30340;KL&#25955;&#24230;&#21644;&#26356;&#39640;&#30340;&#31574;&#30053;&#29109;&#65292;&#19988;&#23545;&#32593;&#32476;&#28145;&#24230;&#25110;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PPO&#65288;Proximal Policy Optimization&#65289;&#31639;&#27861;&#22312;&#35768;&#22810;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#34987;&#35748;&#20026;&#26159;TRPO&#65288;Trust Region Policy Optimization&#65289;&#31639;&#27861;&#30340;&#31616;&#21270;&#29256;&#26412;&#12290;&#28982;&#32780;&#65292;PPO&#20013;&#30340;&#27604;&#29575;&#21098;&#20999;&#25805;&#20316;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#22320;&#24378;&#21046;&#25191;&#34892;&#20449;&#20219;&#21306;&#22495;&#32422;&#26463;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21098;&#20999;&#26041;&#27861;&#65292;&#21363;Simple Policy Optimization&#65288;SPO&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#26087;&#31574;&#30053;&#21644;&#24403;&#21069;&#31574;&#30053;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#22312;Atari 2600&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;PPO&#30340;&#20027;&#27969;&#21464;&#20307;&#30456;&#27604;&#65292;SPO&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#26497;&#20302;&#30340;KL&#25955;&#24230;&#21644;&#26356;&#39640;&#30340;&#31574;&#30053;&#29109;&#65292;&#24182;&#19988;&#23545;&#32593;&#32476;&#28145;&#24230;&#25110;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;SPO&#20445;&#25345;&#20102;&#26080;&#32422;&#26463;&#19968;&#38454;&#31639;&#27861;&#30340;&#31616;&#21333;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.16025v2 Announce Type: replace  Abstract: PPO (Proximal Policy Optimization) algorithm has demonstrated excellent performance in many fields, and it is considered as a simple version of TRPO (Trust Region Policy Optimization) algorithm. However, the ratio clipping operation in PPO may not always effectively enforce the trust region constraints, this can be a potential factor affecting the stability of the algorithm. In this paper, we propose Simple Policy Optimization (SPO) algorithm, which introduces a novel clipping method for KL divergence between the old and current policies. Extensive experimental results in Atari 2600 environments indicate that, compared to the mainstream variants of PPO, SPO achieves better sample efficiency, extremely low KL divergence, and higher policy entropy, and is robust to the increase in network depth or complexity. More importantly, SPO maintains the simplicity of an unconstrained first-order algorithm. Code is available at https://github.co
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20855;&#26377;&#37096;&#20998;&#21160;&#24577;&#30693;&#35782;&#30340;&#22312;&#32447;Q&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#26377;&#38480;&#30340;&#20998;&#38598;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#35774;&#32622;&#19979;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;&#36951;&#25022;&#12290;</title><link>https://arxiv.org/abs/2312.12558</link><description>&lt;p&gt;
&#20855;&#26377;&#37096;&#20998;&#21160;&#24577;&#30693;&#35782;&#30340;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12558
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20855;&#26377;&#37096;&#20998;&#21160;&#24577;&#30693;&#35782;&#30340;&#22312;&#32447;Q&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#26377;&#38480;&#30340;&#20998;&#38598;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#35774;&#32622;&#19979;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;Q&#23398;&#20064;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24403;&#26576;&#20123;&#20851;&#20110;&#21160;&#24577;&#30340;&#20808;&#21069;&#30693;&#35782;&#21487;&#29992;&#25110;&#21487;&#20197;&#26377;&#25928;&#23398;&#20064;&#26102;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25353;&#29031;&#21152;&#24615;&#24178;&#25200;&#27169;&#22411;&#28436;&#21464;&#30340;&#31995;&#32479;&#65292;&#22312;&#26377;&#38480;&#30340;&#20998;&#38598;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#23545;$f$&#30340;&#23436;&#32654;&#30693;&#35782;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;$\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$&#30340;&#36951;&#25022;&#65292;&#20854;&#20013;$T$&#26159;&#19982;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#30340;&#24635;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12558v2 Announce Type: replace  Abstract: The problem of sample complexity of online reinforcement learning is often studied in the literature without taking into account any partial knowledge about the system dynamics that could potentially accelerate the learning process. In this paper, we study the sample complexity of online Q-learning methods when some prior knowledge about the dynamics is available or can be learned efficiently. We focus on systems that evolve according to an additive disturbance model of the form $S_{h+1} = f(S_h, A_h) + W_h$, where $f$ represents the underlying system dynamics, and $W_h$ are unknown disturbances independent of states and actions. In the setting of finite episodic Markov decision processes with $S$ states, $A$ actions, and episode length $H$, we present an optimistic Q-learning algorithm that achieves $\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$ regret under perfect knowledge of $f$, where $T$ is the total number of interactions with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#26500;&#25104;&#26041;&#27861;&#23558;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#39564;&#35777;&#38382;&#39064;&#25286;&#20998;&#20026;&#36739;&#23567;&#12289;&#26356;&#26131;&#22788;&#29702;&#30340;&#23376;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#39564;&#35777;&#20505;&#36873;&#24402;&#32435;&#19981;&#21464;&#24335;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.10842</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#26500;&#25104;&#24402;&#32435;&#19981;&#21464;&#24335;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Compositional Inductive Invariant Based Verification of Neural Network Controlled Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#26500;&#25104;&#26041;&#27861;&#23558;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#39564;&#35777;&#38382;&#39064;&#25286;&#20998;&#20026;&#36739;&#23567;&#12289;&#26356;&#26131;&#22788;&#29702;&#30340;&#23376;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#39564;&#35777;&#20505;&#36873;&#24402;&#32435;&#19981;&#21464;&#24335;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21040;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#65288;NNCS&#65289;&#30340;&#23433;&#20840;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#32435;&#19981;&#21464;&#24335;&#26041;&#27861;&#30340;NNCS&#23433;&#20840;&#39564;&#35777;&#26032;&#26041;&#27861;&#12290;&#22312;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;&#22823;&#12289;&#38750;&#32447;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#39564;&#35777;&#20505;&#36873;&#24402;&#32435;&#19981;&#21464;&#24335;&#30340;&#24402;&#32435;&#24615;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#30340;&#26500;&#25104;&#26041;&#27861;&#36890;&#36807;&#23558;&#24402;&#32435;&#24615;&#35777;&#26126;&#20041;&#21153;&#20998;&#35299;&#25104;&#26356;&#23567;&#12289;&#26356;&#26131;&#22788;&#29702;&#30340;&#23376;&#38382;&#39064;&#65292;&#20351;&#36825;&#19968;&#39564;&#35777;&#36807;&#31243;&#21464;&#24471;&#21487;&#31649;&#29702;&#12290;&#38500;&#20102;&#39640;&#23618;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#33258;&#21160;&#25512;&#26029;&#24517;&#35201;&#30340;&#20998;&#35299;&#35859;&#35789;&#33258;&#21160;&#39564;&#35777;&#32473;&#23450;&#20505;&#36873;&#24402;&#32435;&#19981;&#21464;&#24335;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#22312;&#25191;&#34892;&#26102;&#38388;&#19978;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#32553;&#30701;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10842v2 Announce Type: replace-cross  Abstract: The integration of neural networks into safety-critical systems has shown great potential in recent years. However, the challenge of effectively verifying the safety of Neural Network Controlled Systems (NNCS) persists. This paper introduces a novel approach to NNCS safety verification, leveraging the inductive invariant method. Verifying the inductiveness of a candidate inductive invariant in the context of NNCS is hard because of the scale and nonlinearity of neural networks. Our compositional method makes this verification process manageable by decomposing the inductiveness proof obligation into smaller, more tractable subproblems. Alongside the high-level method, we present an algorithm capable of automatically verifying the inductiveness of given candidates by automatically inferring the necessary decomposition predicates. The algorithm significantly outperforms the baseline method and shows remarkable reductions in execut
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;&#35270;&#39057;&#20013;&#24674;&#22797;&#28508;&#22312;&#21160;&#20316;&#20449;&#24687;&#65292;LAPO&#33021;&#22815;&#35757;&#32451;&#21487;&#20197;&#36805;&#36895;&#24494;&#35843;&#20026;&#19987;&#23478;&#32423;&#31574;&#30053;&#30340;&#28508;&#22312;&#21160;&#20316;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2312.10812</link><description>&lt;p&gt;
&#26080;&#38656;&#21160;&#20316;&#30340;&#34892;&#20026;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Act without Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10812
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#35270;&#39057;&#20013;&#24674;&#22797;&#28508;&#22312;&#21160;&#20316;&#20449;&#24687;&#65292;LAPO&#33021;&#22815;&#35757;&#32451;&#21487;&#20197;&#36805;&#36895;&#24494;&#35843;&#20026;&#19987;&#23478;&#32423;&#31574;&#30053;&#30340;&#28508;&#22312;&#21160;&#20316;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#26159;&#33719;&#21462;&#24378;&#22823;&#36890;&#29992;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20363;&#22914;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#33539;&#24335;&#23578;&#26410;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24471;&#20197;&#25512;&#24191;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Latent Action Policies&#65288;LAPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#35270;&#39057;&#20013;&#32431;&#31929;&#24674;&#22797;&#28508;&#22312;&#21160;&#20316;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20135;&#29983;&#28508;&#22312;&#21160;&#20316;&#31574;&#30053;&#12289;&#19990;&#30028;&#27169;&#22411;&#21644;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;LAPO&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20165;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#21160;&#24577;&#20174;&#35270;&#39057;&#20013;&#24674;&#22797;&#30495;&#23454;&#21160;&#20316;&#31354;&#38388;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36807;&#31243;&#29983;&#25104;&#29615;&#22659;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10812v2 Announce Type: replace-cross  Abstract: Pre-training large models on vast amounts of web data has proven to be an effective approach for obtaining powerful, general models in domains such as language and vision. However, this paradigm has not yet taken hold in reinforcement learning. This is because videos, the most abundant form of embodied behavioral data on the web, lack the action labels required by existing methods for imitating behavior from demonstrations. We introduce Latent Action Policies (LAPO), a method for recovering latent action information, and thereby latent-action policies, world models, and inverse dynamics models, purely from videos. LAPO is the first method able to recover the structure of the true action space just from observed dynamics, even in challenging procedurally-generated environments. LAPO enables training latent-action policies that can be rapidly fine-tuned into expert-level policies, either offline using a small action-labeled datas
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;Policy-Guided Trajectory Diffusion (PolyGRAD)&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#29983;&#25104;&#25972;&#20010;&#22312;&#32447;&#31574;&#30053;&#36712;&#36857;&#65292;&#36991;&#20813;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#38543;&#30528;&#36712;&#36857;&#38271;&#24230;&#22686;&#38271;&#32780;&#31215;&#32047;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2312.08533</link><description>&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#24341;&#23548;&#30340;&#36712;&#36857;&#25193;&#25955;&#23454;&#29616;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
World Models via Policy-Guided Trajectory Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08533
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;Policy-Guided Trajectory Diffusion (PolyGRAD)&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#29983;&#25104;&#25972;&#20010;&#22312;&#32447;&#31574;&#30053;&#36712;&#36857;&#65292;&#36991;&#20813;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#38543;&#30528;&#36712;&#36857;&#38271;&#24230;&#22686;&#38271;&#32780;&#31215;&#32047;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#26159;&#24320;&#21457;&#26234;&#33021;agent&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#36890;&#36807;&#39044;&#27979;&#19968;&#31995;&#21015;&#34892;&#21160;&#30340;&#32467;&#26524;&#65292;&#19990;&#30028;&#27169;&#22411;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#22312;&#8220;&#24819;&#35937;&#20013;&#8221;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#20248;&#21270;&#31574;&#30053;&#65292;&#21363;&#36890;&#36807;&#22312;&#32447;&#31574;&#30053;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#23454;&#29616;&#12290;&#29616;&#26377;&#30340;&#19990;&#30028;&#27169;&#22411;&#26159;&#33258;&#22238;&#24402;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#29366;&#24577;&#30340;&#21516;&#26102;&#20174;&#31574;&#30053;&#20013;&#37319;&#26679;&#19979;&#19968;&#20010;&#34892;&#21160;&#12290;&#38543;&#30528;&#36712;&#36857;&#38271;&#24230;&#30340;&#22686;&#38271;&#65292;&#39044;&#27979;&#35823;&#24046;&#24517;&#28982;&#20250;&#32047;&#31215;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;&#19981;&#26159;&#33258;&#22238;&#24402;&#30340;&#65292;&#32780;&#26159;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#29983;&#25104;&#25972;&#20010;&#22312;&#32447;&#31574;&#30053;&#36712;&#36857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Policy-Guided Trajectory Diffusion (PolyGRAD)&#65292;&#21033;&#29992;&#20102;&#38500;&#20102;&#31574;&#30053;&#30340;&#21160;&#20316;&#20998;&#24067;&#26799;&#24230;&#20043;&#22806;&#30340;&#19968;&#20010;&#21435;&#22122;&#27169;&#22411;&#65292;&#23558;&#26368;&#21021;&#38543;&#26426;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#36712;&#36857;&#25193;&#25955;&#25104;&#19968;&#20010;&#22312;&#32447;&#21512;&#25104;&#36712;&#36857;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;PolyGRAD&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08533v3 Announce Type: replace-cross  Abstract: World models are a powerful tool for developing intelligent agents. By predicting the outcome of a sequence of actions, world models enable policies to be optimised via on-policy reinforcement learning (RL) using synthetic data, i.e. in "in imagination". Existing world models are autoregressive in that they interleave predicting the next state with sampling the next action from the policy. Prediction error inevitably compounds as the trajectory length grows. In this work, we propose a novel world modelling approach that is not autoregressive and generates entire on-policy trajectories in a single pass through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory. We analyse the connections between PolyGRAD,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Fast LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#20302;&#31209;&#35843;&#25972;&#21487;&#20197;&#39640;&#25928;&#25209;&#22788;&#29702;&#24322;&#26500;&#35831;&#27714;&#65292;&#24182;&#22312;&#32489;&#25928;&#19978;&#20445;&#25345;&#31454;&#20105;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.05677</link><description>&lt;p&gt;
&#22522;&#20110;&#25209;&#22788;&#29702;&#30340;&#22522;&#30784;&#27169;&#22411;&#20302;&#31209;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Batched Low-Rank Adaptation of Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05677
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Fast LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#20302;&#31209;&#35843;&#25972;&#21487;&#20197;&#39640;&#25928;&#25209;&#22788;&#29702;&#24322;&#26500;&#35831;&#27714;&#65292;&#24182;&#22312;&#32489;&#25928;&#19978;&#20445;&#25345;&#31454;&#20105;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#22240;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#30697;&#38453;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#24182;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#32780;&#24341;&#36215;&#20851;&#27880;&#12290;&#34429;&#28982;LoRA&#25552;&#20379;&#20102;&#35768;&#22810;&#20248;&#28857;&#65292;&#20294;&#20854;&#22312;&#23454;&#26102;&#20026;&#21508;&#31181;&#20840;&#29699;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#26102;&#26080;&#27861;&#39640;&#25928;&#22788;&#29702;&#22810;&#20010;&#29305;&#23450;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#36825;&#20026;&#38656;&#35201;&#20026;&#27599;&#20010;&#20256;&#20837;&#35831;&#27714;&#20010;&#24615;&#21270;&#12289;&#29305;&#23450;&#20219;&#21153;&#36866;&#24212;&#30340;&#22330;&#26223;&#20013;&#36896;&#25104;&#20102;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#25209;&#22788;&#29702;&#20013;&#30340;&#27599;&#20010;&#36755;&#20837;&#31034;&#20363;&#37117;&#21487;&#20197;&#19982;&#20854;&#29420;&#29305;&#30340;&#20302;&#31209;&#36866;&#24212;&#26435;&#37325;&#30456;&#20851;&#32852;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24322;&#26500;&#35831;&#27714;&#30340;&#39640;&#25928;&#25209;&#22788;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#34920;&#26126;&#65292;FLoRA&#20445;&#30041;&#20102;LoRA&#30340;&#32489;&#25928;&#20248;&#28857;&#65292;&#22312;&#36328;&#36234;8&#31181;&#35821;&#35328;&#30340;MultiPL-E&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#20986;&#31454;&#20105;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05677v2 Announce Type: replace-cross  Abstract: Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While LoRA offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request. To mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that FLoRA retains the performance merits of LoRA, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and 
&lt;/p&gt;</description></item><item><title>CAFE&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#12289;&#33258;&#36866;&#24212;&#21644;&#24555;&#36895;&#30340;&#23884;&#20837;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#20998;&#37197;&#20869;&#23384;&#36164;&#28304;&#32473;&#37325;&#35201;&#29305;&#24449;&#24182;&#24341;&#20837;HotSketch&#25968;&#25454;&#32467;&#26500;&#23454;&#26102;&#25429;&#33719;&#28909;&#38376;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#23884;&#20837;&#34920;&#22312;&#22823;&#35268;&#27169;&#25512;&#33616;&#27169;&#22411;&#20013;&#20869;&#23384;&#38656;&#27714;&#22686;&#38271;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.03256</link><description>&lt;p&gt;
CAFE&#65306;&#38754;&#21521;&#22823;&#35268;&#27169;&#25512;&#33616;&#27169;&#22411;&#30340;&#32039;&#20945;&#12289;&#33258;&#36866;&#24212;&#21644;&#24555;&#36895;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale Recommendation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03256
&lt;/p&gt;
&lt;p&gt;
CAFE&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#12289;&#33258;&#36866;&#24212;&#21644;&#24555;&#36895;&#30340;&#23884;&#20837;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#20998;&#37197;&#20869;&#23384;&#36164;&#28304;&#32473;&#37325;&#35201;&#29305;&#24449;&#24182;&#24341;&#20837;HotSketch&#25968;&#25454;&#32467;&#26500;&#23454;&#26102;&#25429;&#33719;&#28909;&#38376;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#23884;&#20837;&#34920;&#22312;&#22823;&#35268;&#27169;&#25512;&#33616;&#27169;&#22411;&#20013;&#20869;&#23384;&#38656;&#27714;&#22686;&#38271;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#65288;DLRM&#65289;&#20013;&#23884;&#20837;&#34920;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#20869;&#23384;&#38656;&#27714;&#32473;&#27169;&#22411;&#35757;&#32451;&#21644;&#37096;&#32626;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#23884;&#20837;&#21387;&#32553;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#20869;&#23384;&#25928;&#29575;&#12289;&#20302;&#24310;&#36831;&#21644;&#36866;&#24212;&#21160;&#24577;&#25968;&#25454;&#20998;&#24067;&#31561;&#19977;&#20010;&#20851;&#38190;&#35774;&#35745;&#35201;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CAFE&#65292;&#19968;&#31181;&#32039;&#20945;&#12289;&#33258;&#36866;&#24212;&#21644;&#24555;&#36895;&#30340;&#23884;&#20837;&#21387;&#32553;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#35201;&#27714;&#12290;CAFE&#30340;&#35774;&#35745;&#29702;&#24565;&#26159;&#21160;&#24577;&#20998;&#37197;&#26356;&#22810;&#20869;&#23384;&#36164;&#28304;&#32473;&#37325;&#35201;&#29305;&#24449;&#65288;&#31216;&#20026;&#28909;&#38376;&#29305;&#24449;&#65289;&#65292;&#24182;&#20026;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#20998;&#37197;&#26356;&#23569;&#20869;&#23384;&#12290;&#22312;CAFE&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#33609;&#22270;&#25968;&#25454;&#32467;&#26500;&#65292;&#21629;&#21517;&#20026;HotSketch&#65292;&#29992;&#20110;&#25429;&#33719;&#29305;&#24449;&#37325;&#35201;&#24615;&#24182;&#23454;&#26102;&#25253;&#21578;&#28909;&#38376;&#29305;&#24449;&#12290;&#23545;&#20110;&#27599;&#20010;&#25253;&#21578;&#30340;&#28909;&#38376;&#29305;&#24449;&#65292;&#25105;&#20204;&#20026;&#20854;&#20998;&#37197;&#21807;&#19968;&#30340;&#23884;&#20837;&#12290;&#23545;&#20110;&#38750;&#28909;&#38376;&#29305;&#24449;&#65292;&#25105;&#20204;&#20801;&#35768;&#22810;&#20010;&#29305;&#24449;&#20849;&#20139;&#19968;&#20010;&#23884;&#20837;&#65292;&#20351;&#29992;&#21704;&#24076;embedd&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03256v2 Announce Type: replace  Abstract: Recently, the growing memory demands of embedding tables in Deep Learning Recommendation Models (DLRMs) pose great challenges for model training and deployment. Existing embedding compression solutions cannot simultaneously meet three key design requirements: memory efficiency, low latency, and adaptability to dynamic data distribution. This paper presents CAFE, a Compact, Adaptive, and Fast Embedding compression framework that addresses the above requirements. The design philosophy of CAFE is to dynamically allocate more memory resources to important features (called hot features), and allocate less memory to unimportant ones. In CAFE, we propose a fast and lightweight sketch data structure, named HotSketch, to capture feature importance and report hot features in real time. For each reported hot feature, we assign it a unique embedding. For the non-hot features, we allow multiple features to share one embedding by using hash embedd
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hourglass Tokenizer&#65288;HoT&#65289;&#30340;&#20462;&#21098;&#21644;&#24674;&#22797;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#39640;&#25928;&#22320;&#22522;&#20110;Transformer&#36827;&#34892;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#20855;&#26377;&#39640;&#35821;&#20041;&#22810;&#26679;&#24615;&#30340;&#20195;&#34920;&#24615;&#26631;&#35760;&#24182;&#28040;&#38500;&#35270;&#39057;&#24103;&#30340;&#20887;&#20313;&#65292;&#26368;&#32456;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.12028</link><description>&lt;p&gt;
&#39640;&#25928;&#22522;&#20110;Transformer&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;Hourglass Tokenizer
&lt;/p&gt;
&lt;p&gt;
Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12028
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hourglass Tokenizer&#65288;HoT&#65289;&#30340;&#20462;&#21098;&#21644;&#24674;&#22797;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#39640;&#25928;&#22320;&#22522;&#20110;Transformer&#36827;&#34892;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#20855;&#26377;&#39640;&#35821;&#20041;&#22810;&#26679;&#24615;&#30340;&#20195;&#34920;&#24615;&#26631;&#35760;&#24182;&#28040;&#38500;&#35270;&#39057;&#24103;&#30340;&#20887;&#20313;&#65292;&#26368;&#32456;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#24050;&#32463;&#25104;&#21151;&#24212;&#29992;&#20110;&#22522;&#20110;&#35270;&#39057;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35270;&#39057;&#23039;&#21183;Transformer&#65288;VPTs&#65289;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hourglass Tokenizer&#65288;HoT&#65289;&#30340;&#21363;&#25554;&#21363;&#29992;&#30340;&#20462;&#21098;&#21644;&#24674;&#22797;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#39640;&#25928;&#22320;&#22522;&#20110;Transformer&#36827;&#34892;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;HoT&#39318;&#20808;&#36890;&#36807;&#20462;&#21098;&#20887;&#20313;&#24103;&#30340;&#23039;&#21183;&#26631;&#35760;&#24320;&#22987;&#65292;&#28982;&#21518;&#20197;&#24674;&#22797;&#20840;&#38271;&#24230;&#26631;&#35760;&#32467;&#26463;&#65292;&#20174;&#32780;&#22312;&#20013;&#38388;&#30340;Transformer&#22359;&#20013;&#20135;&#29983;&#23569;&#37327;&#23039;&#21183;&#26631;&#35760;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#26377;&#25928;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#35760;&#20462;&#21098;&#38598;&#32676;&#65288;TPC&#65289;&#65292;&#21160;&#24577;&#36873;&#25321;&#19968;&#20123;&#20855;&#26377;&#39640;&#35821;&#20041;&#22810;&#26679;&#24615;&#30340;&#20195;&#34920;&#24615;&#26631;&#35760;&#65292;&#21516;&#26102;&#28040;&#38500;&#35270;&#39057;&#24103;&#30340;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26631;&#35760;&#24674;&#22797;&#27880;&#24847;&#21147;&#65288;TRA&#65289;&#26469;&#24674;&#22797;&#35814;&#32454;&#30340;&#26102;&#31354;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12028v2 Announce Type: replace-cross  Abstract: Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a plug-and-play pruning-and-recovering framework, called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose estimation from videos. Our HoT begins with pruning pose tokens of redundant frames and ends with recovering full-length tokens, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. To effectively achieve this, we propose a token pruning cluster (TPC) that dynamically selects a few representative tokens with high semantic diversity while eliminating the redundancy of video frames. In addition, we develop a token recovering attention (TRA) to restore the detailed spatio-temporal information b
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25361;&#25112;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#33539;&#24335;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#22120;&#36873;&#25321;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#20914;&#31361;&#30340;&#35282;&#33394;&#12290;</title><link>https://arxiv.org/abs/2311.04698</link><description>&lt;p&gt;
&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#25361;&#25112;&#24120;&#35265;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Challenging Common Paradigms in Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04698
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25361;&#25112;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#33539;&#24335;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#22120;&#36873;&#25321;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#20914;&#31361;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#20294;&#20854;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#24182;&#26410;&#24102;&#26469;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#30456;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#65288;STL&#65289;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#26356;&#28145;&#20837;&#20102;&#35299;MTL&#29305;&#23450;&#25361;&#25112;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;MTL&#20013;&#30340;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#20960;&#28857;&#20851;&#20110;STL&#30340;&#37325;&#35201;&#24433;&#21709;&#65306;&#39318;&#20808;&#65292;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#23545;MTL&#30340;&#24433;&#21709;&#21482;&#21463;&#21040;&#20102;&#36731;&#24494;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#30340;&#23454;&#35777;&#26041;&#27861;&#23637;&#31034;&#20102;&#24120;&#35265;STL&#24037;&#20855;&#65288;&#20363;&#22914;Adam&#20248;&#21270;&#22120;&#65289;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;Adam&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#19979;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#37096;&#20998;&#25439;&#22833;&#23610;&#24230;&#19981;&#21464;&#24615;&#12290;&#20854;&#27425;&#65292;&#26799;&#24230;&#20914;&#31361;&#30340;&#27010;&#24565;&#32463;&#24120;&#34987;&#25551;&#36848;&#20026;MTL&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#26799;&#24230;&#20914;&#31361;&#22312;MTL&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;STL&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#35282;&#24230;&#26799;&#24230;&#23545;&#40784;&#26041;&#38754;&#65292;&#25105;&#20204;&#27809;&#26377;&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04698v3 Announce Type: replace-cross  Abstract: While multi-task learning (MTL) has gained significant attention in recent years, its underlying mechanisms remain poorly understood. Recent methods did not yield consistent performance improvements over single task learning (STL) baselines, underscoring the importance of gaining more profound insights about challenges specific to MTL. In our study, we challenge paradigms in MTL in the context of STL: First, the impact of the choice of optimizer has only been mildly investigated in MTL. We show the pivotal role of common STL tools such as the Adam optimizer in MTL empirically in various experiments. To further investigate Adam's effectiveness, we theoretical derive a partial loss-scale invariance under mild assumptions. Second, the notion of gradient conflicts has often been phrased as a specific problem in MTL. We delve into the role of gradient conflicts in MTL and compare it to STL. For angular gradient alignment we find no 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#20013;&#28155;&#21152;&#39069;&#22806;&#31867;&#21035;&#30340;&#23545;&#25968;&#20960;&#29575;&#39033;&#65292;&#35774;&#35745;&#20351;&#20854;&#22312;&#36828;&#31163;&#35757;&#32451;&#25968;&#25454;&#26102;&#25903;&#37197;&#21407;&#22987;&#31867;&#21035;&#30340;&#23545;&#25968;&#20960;&#29575;&#65292;&#20174;&#32780;&#38450;&#27490;&#22312;&#36828;&#22788;&#27979;&#35797;&#25968;&#25454;&#19978;&#20986;&#29616;&#20219;&#24847;&#39640;&#30340;&#32622;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2311.03683</link><description>&lt;p&gt;
&#22312;&#28857;&#20272;&#35745;&#30340;&#21028;&#21035;&#24335;&#31070;&#32463;&#32593;&#32476;&#20013;&#38450;&#27490;&#36828;&#22788;&#25968;&#25454;&#30340;&#20219;&#24847;&#39640;&#32622;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Preventing Arbitrarily High Confidence on Far-Away Data in Point-Estimated Discriminative Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03683
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#20013;&#28155;&#21152;&#39069;&#22806;&#31867;&#21035;&#30340;&#23545;&#25968;&#20960;&#29575;&#39033;&#65292;&#35774;&#35745;&#20351;&#20854;&#22312;&#36828;&#31163;&#35757;&#32451;&#25968;&#25454;&#26102;&#25903;&#37197;&#21407;&#22987;&#31867;&#21035;&#30340;&#23545;&#25968;&#20960;&#29575;&#65292;&#20174;&#32780;&#38450;&#27490;&#22312;&#36828;&#22788;&#27979;&#35797;&#25968;&#25454;&#19978;&#20986;&#29616;&#20219;&#24847;&#39640;&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21028;&#21035;&#24335;&#35757;&#32451;&#12289;&#30830;&#23450;&#24615;&#31070;&#32463;&#32593;&#32476;&#26159;&#35299;&#20915;&#20998;&#31867;&#38382;&#39064;&#30340;&#20107;&#23454;&#19978;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#22495;&#20869;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#22312;&#22495;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#19978;&#36807;&#20110;&#33258;&#20449;&#12290;&#25105;&#20204;&#36890;&#36807;&#21521;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#28155;&#21152;&#19968;&#20010;&#39033;&#65292;&#35813;&#39033;&#23545;&#24212;&#20110;&#39069;&#22806;&#31867;&#21035;&#30340;&#23545;&#25968;&#20960;&#29575;&#65292;&#35774;&#35745;&#20026;&#24403;&#25105;&#20204;&#36828;&#31163;&#35757;&#32451;&#25968;&#25454;&#26102;&#25903;&#37197;&#21407;&#22987;&#31867;&#21035;&#30340;&#23545;&#25968;&#20960;&#29575;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#35777;&#26126;&#38450;&#27490;&#36828;&#22788;&#27979;&#35797;&#25968;&#25454;&#30340;&#20219;&#24847;&#39640;&#32622;&#20449;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#31616;&#21333;&#30340;&#21028;&#21035;&#28857;&#20272;&#35745;&#35757;&#32451;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03683v2 Announce Type: replace  Abstract: Discriminatively trained, deterministic neural networks are the de facto choice for classification problems. However, even though they achieve state-of-the-art results on in-domain test sets, they tend to be overconfident on out-of-distribution (OOD) data. For instance, ReLU networks - a popular class of neural network architectures - have been shown to almost always yield high confidence predictions when the test data are far away from the training set, even when they are trained with OOD data. We overcome this problem by adding a term to the output of the neural network that corresponds to the logit of an extra class, that we design to dominate the logits of the original classes as we move away from the training data.This technique provably prevents arbitrarily high confidence on far-away test data while maintaining a simple discriminative point-estimate training. Evaluation on various benchmarks demonstrates strong performance aga
&lt;/p&gt;</description></item><item><title>&#23558;&#22312;&#32447;&#21040;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#23450;&#20026;&#19968;&#20010;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#20869;&#22312;&#22870;&#21169;&#21644;UCB&#22312;&#35813;&#35774;&#32622;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2310.05723</link><description>&lt;p&gt;
&#22312;&#32447;&#21040;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36229;&#39046;&#22495;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05723
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22312;&#32447;&#21040;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#23450;&#20026;&#19968;&#20010;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#20869;&#22312;&#22870;&#21169;&#21644;UCB&#22312;&#35813;&#35774;&#32622;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#39044;&#35757;&#32451;&#37197;&#21512;&#38745;&#24577;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#22312;&#32447;&#24494;&#35843;&#65288;&#31163;&#32447;&#21040;&#22312;&#32447;&#65292;&#21363;OtO&#65289;&#26159;&#19968;&#20010;&#19982;&#29616;&#23454;&#19990;&#30028;RL&#37096;&#32626;&#36807;&#31243;&#24456;&#21305;&#37197;&#30340;&#33539;&#24335;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#26377;&#38480;&#30340;&#22312;&#32447;&#20132;&#20114;&#39044;&#31639;&#20869;&#25214;&#21040;&#24615;&#33021;&#26368;&#20339;&#30340;&#31574;&#30053;&#12290;&#20197;&#21069;&#22312;OtO&#35774;&#32622;&#20013;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#32416;&#27491;&#30001;&#31163;&#32447;RL&#31639;&#27861;&#30340;&#31574;&#30053;&#32422;&#26463;&#26426;&#21046;&#24341;&#20837;&#30340;&#20559;&#24046;&#19978;&#12290;&#36825;&#20123;&#32422;&#26463;&#20351;&#24471;&#23398;&#20064;&#30340;&#31574;&#30053;&#25509;&#36817;&#25910;&#38598;&#25968;&#25454;&#38598;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#34892;&#20026;&#31574;&#30053;&#36828;&#38750;&#26368;&#20248;&#65292;&#21017;&#36825;&#21487;&#33021;&#20250;&#19981;&#24517;&#35201;&#22320;&#38480;&#21046;&#31574;&#30053;&#24615;&#33021;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25918;&#24323;&#32422;&#26463;&#65292;&#25226;OtO RL&#20316;&#20026;&#19968;&#20010;&#25506;&#32034;&#38382;&#39064;&#26469;&#26694;&#23450;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#22312;&#32447;&#25968;&#25454;&#37319;&#38598;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#22522;&#20110;&#20869;&#22312;&#22870;&#21169;&#21644;UCB&#30340;&#20027;&#35201;&#22312;&#32447;RL&#25506;&#32034;&#26041;&#27861;&#22312;OtO&#35774;&#32622;&#20013;&#30340;&#25928;&#26524;&#65292;&#26174;&#31034;&#20869;&#22312;&#22870;&#21169;&#36890;&#36807;&#22870;&#21169;&#20989;&#25968;&#30340;&#20462;&#25913;&#22686;&#21152;&#20102;&#35757;&#32451;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05723v2 Announce Type: replace  Abstract: Offline pretraining with a static dataset followed by online fine-tuning (offline-to-online, or OtO) is a paradigm well matched to a real-world RL deployment process. In this scenario, we aim to find the best-performing policy within a limited budget of online interactions. Previous work in the OtO setting has focused on correcting for bias introduced by the policy-constraint mechanisms of offline RL algorithms. Such constraints keep the learned policy close to the behavior policy that collected the dataset, but we show this can unnecessarily limit policy performance if the behavior policy is far from optimal. Instead, we forgo constraints and frame OtO RL as an exploration problem that aims to maximize the benefit of online data-collection. We first study the major online RL exploration methods based on intrinsic rewards and UCB in the OtO setting, showing that intrinsic rewards add training instability through reward-function modif
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#36817;&#20284;&#26041;&#27861;&#21644;&#23618;&#27425;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25915;&#20987;FedAvg&#22330;&#26223;&#20013;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2308.06822</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36817;&#20284;&#21644;&#21152;&#26435;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Approximate and Weighted Data Reconstruction Attack in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.06822
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#36817;&#20284;&#26041;&#27861;&#21644;&#23618;&#27425;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25915;&#20987;FedAvg&#22330;&#26223;&#20013;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#20363;&#65292;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#19981;&#20849;&#20139;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#20316;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#34987;&#35748;&#20026;&#26159;&#36890;&#36807;&#35774;&#35745;&#20445;&#25252;&#38544;&#31169;&#30340;&#65292;&#20294;&#26368;&#36817;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22522;&#20110;&#22312;FL&#20013;&#20849;&#20139;&#30340;&#21442;&#25968;&#24674;&#22797;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#25915;&#20987;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#27700;&#24179;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#22330;&#26223;&#65292;&#22312;&#27492;&#22330;&#26223;&#20013;&#65292;&#23458;&#25143;&#31471;&#22312;&#22810;&#20010;&#23616;&#37096;&#35757;&#32451;&#27493;&#39588;&#20043;&#21518;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23458;&#25143;&#31471;&#23616;&#37096;&#35757;&#32451;&#36807;&#31243;&#30340;&#20013;&#38388;&#27169;&#22411;&#26356;&#26032;&#65292;&#20351;&#25915;&#20987;FedAvg&#22330;&#26223;&#21464;&#24471;&#21487;&#34892;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23618;&#27425;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21892;&#37325;&#26500;&#30340;&#25968;&#25454;&#36136;&#37327;&#12290;&#25105;&#20204;&#20026;&#19981;&#21516;&#23618;&#27425;&#30340;&#27169;&#22411;&#26356;&#26032;&#20998;&#37197;&#19981;&#21516;&#30340;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.06822v2 Announce Type: replace-cross  Abstract: Federated Learning (FL) is a distributed learning paradigm that enables multiple clients to collaborate on building a machine learning model without sharing their private data. Although FL is considered privacy-preserved by design, recent data reconstruction attacks demonstrate that an attacker can recover clients' training data based on the parameters shared in FL. However, most existing methods fail to attack the most widely used horizontal Federated Averaging (FedAvg) scenario, where clients share model parameters after multiple local training steps. To tackle this issue, we propose an interpolation-based approximation method, which makes attacking FedAvg scenarios feasible by generating the intermediate model updates of the clients' local training processes. Then, we design a layer-wise weighted loss function to improve the data quality of reconstruction. We assign different weights to model updates in different layers conc
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#19981;&#20165;&#26377;&#21161;&#20110;&#25913;&#36827;&#31639;&#27861;&#21644;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#36824;&#23637;&#31034;&#20102;&#19982;&#20854;&#20182;&#23398;&#20064;&#26694;&#26550;&#30456;&#32467;&#21512;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32852;&#37030; X &#23398;&#20064;&#20013;&#65292;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#31561;&#25552;&#20379;&#20102;&#26032;&#30340;&#30740;&#31350;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2102.12920</link><description>&lt;p&gt;
&#26032;&#20852;&#36235;&#21183;&#65306;&#20174;&#27169;&#22411;&#34701;&#21512;&#21040;&#32852;&#37030; X &#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Emerging Trends in Federated Learning: From Model Fusion to Federated X Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2102.12920
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#19981;&#20165;&#26377;&#21161;&#20110;&#25913;&#36827;&#31639;&#27861;&#21644;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#36824;&#23637;&#31034;&#20102;&#19982;&#20854;&#20182;&#23398;&#20064;&#26694;&#26550;&#30456;&#32467;&#21512;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32852;&#37030; X &#23398;&#20064;&#20013;&#65292;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#31561;&#25552;&#20379;&#20102;&#26032;&#30340;&#30740;&#31350;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#22810;&#26041;&#35745;&#31639;&#21644;&#27169;&#22411;&#32858;&#21512;&#26469;&#35299;&#32806;&#25968;&#25454;&#25910;&#38598;&#19982;&#27169;&#22411;&#35757;&#32451;&#12290;&#20316;&#20026;&#19968;&#31181;&#28789;&#27963;&#30340;&#23398;&#20064;&#35774;&#32622;&#65292;&#32852;&#37030;&#23398;&#20064;&#26377;&#28508;&#21147;&#19982;&#20854;&#20182;&#23398;&#20064;&#26694;&#26550;&#25972;&#21512;&#12290;&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#19982;&#20854;&#20182;&#23398;&#20064;&#31639;&#27861;&#30340;&#32467;&#21512;&#36827;&#34892;&#20102;&#37325;&#28857;&#35843;&#26597;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#23398;&#20064;&#31639;&#27861;&#20197;&#25913;&#36827;&#21407;&#22987;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#24182;&#22238;&#39038;&#20102;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#20363;&#22914;&#33258;&#36866;&#24212;&#32858;&#21512;&#12289;&#27491;&#21017;&#21270;&#12289;&#32858;&#31867;&#26041;&#27861;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;&#26681;&#25454;&#26032;&#20852;&#36235;&#21183;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#32852;&#37030;&#23398;&#20064;&#19982;&#20854;&#20182;&#23398;&#20064;&#33539;&#24335;&#30340;&#20132;&#38598;&#65292;&#31216;&#20026;&#32852;&#37030; X &#23398;&#20064;&#65292;&#20854;&#20013; X &#21253;&#25324;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#36825;&#39033;&#35843;&#26597;&#22238;&#39038;&#20102;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2102.12920v4 Announce Type: replace  Abstract: Federated learning is a new learning paradigm that decouples data collection and model training via multi-party computation and model aggregation. As a flexible learning setting, federated learning has the potential to integrate with other learning frameworks. We conduct a focused survey of federated learning in conjunction with other learning algorithms. Specifically, we explore various learning algorithms to improve the vanilla federated averaging algorithm and review model fusion methods such as adaptive aggregation, regularization, clustered methods, and Bayesian methods. Following the emerging trends, we also discuss federated learning in the intersection with other learning paradigms, termed federated X learning, where X includes multitask learning, meta-learning, transfer learning, unsupervised learning, and reinforcement learning. This survey reviews the state of the art, challenges, and future directions.
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#25163;&#20889;&#23383;&#31526;&#35782;&#21035;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#39640;&#22797;&#26434;&#24615;&#23383;&#31526;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.17098</link><description>&lt;p&gt;
CharNet&#65306;&#39640;&#22797;&#26434;&#24615;&#23383;&#31526;&#20998;&#31867;&#30340;&#24191;&#20041;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CharNet: Generalized Approach for High-Complexity Character Classification. (arXiv:2401.17098v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17098
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#25163;&#20889;&#23383;&#31526;&#35782;&#21035;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#39640;&#22797;&#26434;&#24615;&#23383;&#31526;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#25163;&#20889;&#23383;&#31526;&#35782;&#21035;&#65288;HCR&#65289;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19982;&#25171;&#21360;&#25991;&#26412;&#25968;&#25454;&#19981;&#21516;&#65292;&#25163;&#20889;&#23383;&#31526;&#25968;&#25454;&#38598;&#30001;&#20110;&#20154;&#31867;&#23548;&#33268;&#30340;&#20559;&#24046;&#32780;&#20855;&#26377;&#26356;&#22810;&#30340;&#21464;&#21270;&#12290;&#22522;&#20110;&#29420;&#29305;&#23383;&#31526;&#31867;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#35937;&#24418;&#25991;&#23383;&#25110;&#27721;&#23383;&#38889;&#23383;&#23383;&#31526;&#24207;&#21015;&#65292;&#32473;HCR&#38382;&#39064;&#24102;&#26469;&#26032;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#23398;&#20064;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#22270;&#20687;&#30340;&#39640;&#22797;&#26434;&#24615;&#32454;&#33410;&#12290;&#38543;&#30528;&#35745;&#31639;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#36827;&#19968;&#27493;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#29702;&#35770;&#30340;&#21457;&#23637;&#65292;&#19968;&#20123;&#30740;&#31350;&#22242;&#38431;&#24050;&#32463;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20986;&#29616;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#20197;&#39640;&#25928;&#29575;&#32780;&#38395;&#21517;&#65292;&#20294;&#35768;&#22810;&#24120;&#35265;&#30340;&#26041;&#27861;&#20173;&#28982;&#19981;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#38598;&#29305;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#30001;&#20110;&#22797;&#26434;&#30340;&#32467;&#26500;&#21644;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#29616;&#26377;&#26041;&#27861;&#32463;&#24120;&#38459;&#27490;&#35299;&#20915;&#26041;&#26696;&#27969;&#34892;&#36215;&#26469;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Handwritten character recognition (HCR) is a challenging problem for machine learning researchers. Unlike printed text data, handwritten character datasets have more variation due to human-introduced bias. With numerous unique character classes present, some data, such as Logographic Scripts or Sino-Korean character sequences, bring new complications to the HCR problem. The classification task on such datasets requires the model to learn high-complexity details of the images that share similar features. With recent advances in computational resource availability and further computer vision theory development, some research teams have effectively addressed the arising challenges. Although known for achieving high efficiency, many common approaches are still not generalizable and use dataset-specific solutions to achieve better results. Due to complex structure and high computing demands, existing methods frequently prevent the solutions from gaining popularity. This paper proposes a str
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12492</link><description>&lt;p&gt;
&#27604;&#36739;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35821;&#35328;&#24314;&#27169;&#65306;&#27169;&#25311;&#32676;&#20307;&#12289;&#20010;&#20307;&#29305;&#28857;&#36824;&#26159;&#20004;&#32773;&#20860;&#39038;&#65311;
&lt;/p&gt;
&lt;p&gt;
Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?. (arXiv:2401.12492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#23558;&#20154;&#30340;&#19978;&#19979;&#25991;&#32435;&#20837;&#20854;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20351;&#29992;&#32676;&#20307;&#23646;&#24615;&#65288;&#22914;45&#23681;&#20197;&#19978;&#30340;&#20154;&#32676;&#65289;&#36824;&#26159;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#26356;&#26377;&#25928;&#30340;&#38382;&#39064;&#23578;&#26410;&#30830;&#23450;&#12290;&#32676;&#20307;&#23646;&#24615;&#22312;&#25216;&#26415;&#19978;&#26356;&#23481;&#26131;&#23454;&#29616;&#65292;&#20294;&#26159;&#36807;&#20110;&#31895;&#31961;&#65306;&#24182;&#38750;&#25152;&#26377;45&#23681;&#20197;&#19978;&#30340;&#20154;&#37117;&#20197;&#30456;&#21516;&#30340;&#26041;&#24335;&#20070;&#20889;&#12290;&#30456;&#21453;&#65292;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#33021;&#22815;&#25429;&#25417;&#27599;&#20010;&#20154;&#36523;&#20221;&#30340;&#22797;&#26434;&#24615;&#65292;&#20801;&#35768;&#26356;&#20010;&#24615;&#21270;&#30340;&#34920;&#31034;&#65292;&#20294;&#25105;&#20204;&#21487;&#33021;&#38656;&#35201;&#27169;&#25311;&#26080;&#38480;&#25968;&#37327;&#30340;&#29992;&#25143;&#24182;&#19988;&#38656;&#35201;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36890;&#36807;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#23558;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#30340;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#65288;&#22914;&#24180;&#40836;&#20272;&#35745;&#25110;&#20010;&#24615;&#35780;&#20272;&#65289;&#30340;&#24615;&#33021;&#12290;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#65288;&#22914;&#31435;&#22330;&#21644;&#20027;&#39064;&#26816;&#27979;&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing has made progress in incorporating human context into its models, but whether it is more effective to use group-wise attributes (e.g., over-45-year-olds) or model individuals remains open. Group attributes are technically easier but coarse: not all 45-year-olds write the same way. In contrast, modeling individuals captures the complexity of each person's identity. It allows for a more personalized representation, but we may have to model an infinite number of users and require data that may be impossible to get. We compare modeling human context via group attributes, individual users, and combined approaches. Combining group and individual features significantly benefits user-level regression tasks like age estimation or personality assessment from a user's documents. Modeling individual users significantly improves the performance of single document-level classification tasks like stance and topic detection. We also find that individual-user modeling does w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#36755;&#20837;&#20984;&#24615;&#21644;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#36755;&#20837;&#20984;&#24615;Lipschitz&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#32593;&#32476;&#32467;&#26500;&#65292;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#24490;&#29615;&#21333;&#20803;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#31181;&#24037;&#31243;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.07494</link><description>&lt;p&gt;
&#36755;&#20837;&#20984;&#24615;Lipschitz RNN: &#19968;&#31181;&#29992;&#20110;&#24037;&#31243;&#20219;&#21153;&#30340;&#24555;&#36895;&#21644;&#40065;&#26834;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering Tasks. (arXiv:2401.07494v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07494
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#36755;&#20837;&#20984;&#24615;&#21644;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#36755;&#20837;&#20984;&#24615;Lipschitz&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#32593;&#32476;&#32467;&#26500;&#65292;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#24490;&#29615;&#21333;&#20803;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#31181;&#24037;&#31243;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#26159;&#30495;&#23454;&#19990;&#30028;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#22312;&#21516;&#26102;&#25110;&#20998;&#21035;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#36890;&#36807;&#20174;&#33258;&#28982;&#29289;&#29702;&#31995;&#32479;&#21644;&#29616;&#26377;&#25991;&#29486;&#20013;&#33719;&#21462;&#30340;&#35265;&#35299;&#65292;&#24050;&#30693;&#36755;&#20837;&#20984;&#24615;&#32467;&#26500;&#22686;&#24378;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#32780;Lipschitz&#32422;&#26463;&#32467;&#26500;&#22686;&#24378;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#20984;&#24615;&#21644;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#31216;&#20026;&#36755;&#20837;&#20984;&#24615;Lipschitz&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#27169;&#22411;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#24490;&#29615;&#21333;&#20803;&#65292;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#24037;&#31243;&#20219;&#21153;&#65292;&#21253;&#25324;&#22522;&#20934;MNIST&#22270;&#20687;&#20998;&#31867;&#12289;&#26032;&#21152;&#22369;LHT Holdings&#20844;&#21496;&#30340;&#23454;&#38469;&#22826;&#38451;&#33021;&#20809;&#20239;&#31995;&#32479;&#35268;&#21010;&#20013;&#30340;&#23454;&#26102;&#22826;&#38451;&#36752;&#23556;&#39044;&#27979;&#65292;&#20197;&#21450;&#21270;&#23398;&#21453;&#24212;&#22120;&#30340;&#23454;&#26102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20248;&#21270;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational efficiency and adversarial robustness are critical factors in real-world engineering applications. Yet, conventional neural networks often fall short in addressing both simultaneously, or even separately. Drawing insights from natural physical systems and existing literature, it is known that an input convex architecture enhances computational efficiency, while a Lipschitz-constrained architecture bolsters adversarial robustness. By leveraging the strengths of convexity and Lipschitz continuity, we develop a novel network architecture, termed Input Convex Lipschitz Recurrent Neural Networks. This model outperforms existing recurrent units across a spectrum of engineering tasks in terms of computational efficiency and adversarial robustness. These tasks encompass a benchmark MNIST image classification, real-world solar irradiance prediction for Solar PV system planning at LHT Holdings in Singapore, and real-time Model Predictive Control optimization for a chemical reactor.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#30340;&#21306;&#21035;&#65292;&#20197;&#35299;&#20915;&#28389;&#29992;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06712</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#36827;&#34892;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#23567;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Detection of Machine-Generated Text using Style Representations. (arXiv:2401.06712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#30340;&#21306;&#21035;&#65292;&#20197;&#35299;&#20915;&#28389;&#29992;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#25351;&#23548;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#20351;&#24471;&#20154;&#31867;&#20889;&#20316;&#30340;&#36924;&#30495;&#27169;&#20223;&#38754;&#20020;&#30528;&#37325;&#22823;&#28389;&#29992;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#26816;&#27979;&#19968;&#27573;&#25991;&#26412;&#26159;&#30001;&#35821;&#35328;&#27169;&#22411;&#36824;&#26159;&#20154;&#31867;&#25776;&#20889;&#32780;&#25104;&#26469;&#23545;&#25239;&#27492;&#31867;&#28389;&#29992;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#24335;&#34920;&#31034;&#30340;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#22312;&#38754;&#23545;&#25968;&#25454;&#36716;&#25442;&#26102;&#30340;&#35268;&#32422;&#19981;&#36275;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#20063;&#36991;&#20813;&#20102;&#22312;&#25512;&#29702;&#25110;&#26816;&#27979;&#26102;&#38656;&#35201;&#35775;&#38382;&#21487;&#33021;&#29983;&#25104;&#25991;&#26723;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for neural network-based detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that may have generated a document in question at inference or detection time, which is often impractical. In light of these challenges, we pursue a fundamentally d
&lt;/p&gt;</description></item><item><title>FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01483</link><description>&lt;p&gt;
FedSN&#65306;&#19968;&#20010;&#36866;&#29992;&#20110;LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01483
&lt;/p&gt;
&lt;p&gt;
FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#21355;&#26143;&#24050;&#32463;&#30001;&#21830;&#19994;&#20844;&#21496;&#25104;&#21151;&#22320;&#21457;&#23556;&#21644;&#37096;&#32626;&#21040;&#22826;&#31354;&#20013;&#65292;&#22914;SpaceX&#12290;&#30001;&#20110;LEO&#21355;&#26143;&#37197;&#22791;&#20102;&#22810;&#27169;&#20256;&#24863;&#22120;&#65292;&#23427;&#20204;&#19981;&#20165;&#29992;&#20110;&#36890;&#20449;&#65292;&#36824;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#22914;&#31354;&#38388;&#35843;&#21046;&#35782;&#21035;&#12289;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19982;LEO&#21355;&#26143;&#30340;&#26377;&#38480;&#25509;&#35302;&#26102;&#38388;&#65288;&#20363;&#22914;5&#20998;&#38047;&#65289;&#65292;&#22320;&#38754;&#31449;&#65288;GS&#65289;&#21487;&#33021;&#26080;&#27861;&#19979;&#36733;&#22914;&#27492;&#22823;&#37327;&#30340;&#21407;&#22987;&#24863;&#27979;&#25968;&#25454;&#36827;&#34892;&#38598;&#20013;&#27169;&#22411;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35201;&#22312;LEO&#21355;&#26143;&#19978;&#20351;&#29992;FL&#65292;&#25105;&#20204;&#20173;&#28982;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;i&#65289;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#65292;ii&#65289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#65292;&#20197;&#21450;iii&#65289;&#27169;&#22411;&#38472;&#26087;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSN&#30340;&#36890;&#29992;FL&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#19968;
&lt;/p&gt;
&lt;p&gt;
Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, an
&lt;/p&gt;</description></item><item><title>VIGraph&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#26469;&#35299;&#20915;&#22270;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23402;&#29983;&#23545;&#27604;&#31574;&#30053;&#25552;&#39640;&#29983;&#25104;&#33410;&#28857;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.01191</link><description>&lt;p&gt;
VIGraph&#65306;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification. (arXiv:2311.01191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01191
&lt;/p&gt;
&lt;p&gt;
VIGraph&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#26469;&#35299;&#20915;&#22270;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23402;&#29983;&#23545;&#27604;&#31574;&#30053;&#25552;&#39640;&#29983;&#25104;&#33410;&#28857;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#20026;&#33410;&#28857;&#20998;&#31867;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;SMOTE&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#24179;&#34913;&#22330;&#26223;&#26500;&#24314;&#36807;&#31243;&#20013;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#21512;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#32780;&#20854;&#28508;&#21147;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;SMOTE&#30340;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;VIGraph&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;VGAE&#65289;&#30340;&#26032;&#22411;SSL&#27169;&#22411;&#65292;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#29983;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;VIGraph&#22312;&#26500;&#24314;&#19981;&#24179;&#34913;&#22270;&#26102;&#20005;&#26684;&#36981;&#24490;&#19981;&#24179;&#34913;&#30340;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#22411;VGAE&#29983;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#12290;&#27492;&#22806;&#65292;VIGraph&#22312;&#35299;&#30721;&#38454;&#27573;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23402;&#29983;&#23545;&#27604;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#33410;&#28857;&#30340;&#25972;&#20307;&#36136;&#37327;&#12290;VIGraph&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33410;&#28857;&#65292;&#26080;&#38656;&#37325;&#26032;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance in graph data poses significant challenges for node classification. Existing methods, represented by SMOTE-based approaches, partially alleviate this issue but still exhibit limitations during imbalanced scenario construction. Self-supervised learning (SSL) offers a promising solution by synthesizing minority nodes from the data itself, yet its potential remains unexplored. In this paper, we analyze the limitations of SMOTE-based approaches and introduce VIGraph, a novel SSL model based on the self-supervised Variational Graph Auto-Encoder (VGAE) that leverages Variational Inference (VI) to generate minority nodes. Specifically, VIGraph strictly adheres to the concept of imbalance when constructing imbalanced graphs and utilizes the generative VGAE to generate minority nodes. Moreover, VIGraph introduces a novel Siamese contrastive strategy at the decoding phase to improve the overall quality of generated nodes. VIGraph can generate high-quality nodes without reintegrat
&lt;/p&gt;</description></item><item><title>Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.</title><link>http://arxiv.org/abs/2310.17072</link><description>&lt;p&gt;
&#31561;&#36317;&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;
&lt;/p&gt;
&lt;p&gt;
Isometric Motion Manifold Primitives. (arXiv:2310.17072v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17072
&lt;/p&gt;
&lt;p&gt;
Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;&#65288;MMP&#65289;&#20026;&#32473;&#23450;&#20219;&#21153;&#29983;&#25104;&#19968;&#31995;&#21015;&#36830;&#32493;&#36712;&#36857;&#27969;&#24418;&#65292;&#27599;&#19968;&#20010;&#36712;&#36857;&#27969;&#24418;&#37117;&#33021;&#25104;&#21151;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#30001;&#23545;&#27969;&#24418;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#35299;&#30721;&#20989;&#25968;&#20197;&#21450;&#28508;&#22312;&#22352;&#26631;&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#23494;&#24230;&#32452;&#25104;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#30001;&#20110;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20960;&#20309;&#25197;&#26354;&#65292;MMP&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;--&#36890;&#36807;&#21464;&#24418;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#30456;&#20284;&#30340;&#36816;&#21160;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#27861;&#30456;&#37051;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31561;&#36317;&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;&#65288;IMMP&#65289;&#65292;&#20854;&#28508;&#22312;&#22352;&#26631;&#31354;&#38388;&#20445;&#25345;&#20102;&#27969;&#24418;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#21644;&#20351;&#29992;&#20102;&#19968;&#20010;Riemannian&#24230;&#37327;&#65292;&#29992;&#20110;&#36816;&#21160;&#31354;&#38388;&#65288;&#21363;&#65292;&#21442;&#25968;&#21270;&#26354;&#32447;&#31354;&#38388;&#65289;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;CurveGeom Riemannian&#24230;&#37327;&#12290;&#23545;&#20110;&#24179;&#38754;&#38556;&#30861;&#36991;&#35753;&#36816;&#21160;&#21644;&#25512;&#21160;&#25805;&#32437;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IMMP&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;MMP&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Gabe-YHLee/IMMP&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Motion Manifold Primitive (MMP) produces, for a given task, a continuous manifold of trajectories each of which can successfully complete the task. It consists of the decoder function that parametrizes the manifold and the probability density in the latent coordinate space. In this paper, we first show that the MMP performance can significantly degrade due to the geometric distortion in the latent space -- by distortion, we mean that similar motions are not located nearby in the latent space. We then propose {\it Isometric Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the geometry of the manifold. For this purpose, we formulate and use a Riemannian metric for the motion space (i.e., parametric curve space), which we call a {\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding motions and pushing manipulation tasks show that IMMP significantly outperforms existing MMP methods. Code is available at https://github.com/Gabe-YHLee/IMMP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20840;&#23616;&#39044;&#31639;&#24179;&#34913;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#38024;&#23545;&#20855;&#26377;&#23545;&#25239;&#36755;&#20837;&#30340;&#21452;&#36793;&#36152;&#26131;&#30340;&#26080;&#24724;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#21453;&#39304;&#27169;&#22411;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.12370</link><description>&lt;p&gt;
&#21452;&#36793;&#36152;&#26131;&#20013;&#22522;&#20110;&#20840;&#23616;&#39044;&#31639;&#24179;&#34913;&#30340;&#26080;&#24724;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
No-Regret Learning in Bilateral Trade via Global Budget Balance. (arXiv:2310.12370v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20840;&#23616;&#39044;&#31639;&#24179;&#34913;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#38024;&#23545;&#20855;&#26377;&#23545;&#25239;&#36755;&#20837;&#30340;&#21452;&#36793;&#36152;&#26131;&#30340;&#26080;&#24724;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#21453;&#39304;&#27169;&#22411;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#36793;&#36152;&#26131;&#28041;&#21450;&#22312;&#20004;&#20010;&#25112;&#30053;&#20195;&#29702;&#20154;&#20043;&#38388;&#20419;&#36827;&#20132;&#26131;&#30340;&#25361;&#25112;&#65292;&#19968;&#20010;&#26159;&#21334;&#23478;&#65292;&#19968;&#20010;&#26159;&#20080;&#23478;&#65292;&#20004;&#32773;&#37117;&#23545;&#29289;&#21697;&#26377;&#31169;&#20154;&#20272;&#20540;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35813;&#38382;&#39064;&#30340;&#22312;&#32447;&#29256;&#26412;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#65292;&#37117;&#20250;&#20986;&#29616;&#19968;&#20010;&#26032;&#30340;&#21334;&#23478;&#21644;&#20080;&#23478;&#12290;&#23398;&#20064;&#32773;&#30340;&#20219;&#21153;&#26159;&#22312;&#19981;&#20102;&#35299;&#20182;&#20204;&#20272;&#20540;&#30340;&#24773;&#20917;&#19979;&#20026;&#27599;&#20010;&#20195;&#29702;&#20154;&#35774;&#32622;&#20215;&#26684;&#12290;&#21334;&#23478;&#21644;&#20080;&#23478;&#30340;&#24207;&#21015;&#30001;&#19968;&#20010;&#36951;&#24536;&#24615;&#23545;&#25163;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#65292;&#24050;&#30693;&#30340;&#36127;&#38754;&#32467;&#26524;&#25490;&#38500;&#20102;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#23398;&#20064;&#32773;&#24517;&#39035;&#20445;&#35777;&#39044;&#31639;&#24179;&#34913;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20840;&#23616;&#39044;&#31639;&#24179;&#34913;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#20165;&#35201;&#27714;&#20195;&#29702;&#20154;&#22312;&#25972;&#20010;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#25345;&#39044;&#31639;&#24179;&#34913;&#12290;&#36890;&#36807;&#35201;&#27714;&#20840;&#23616;&#39044;&#31639;&#24179;&#34913;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#39318;&#20010;&#38024;&#23545;&#20855;&#26377;&#23545;&#25239;&#36755;&#20837;&#30340;&#21452;&#36793;&#36152;&#26131;&#30340;&#26080;&#24724;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#21453;&#39304;&#27169;&#22411;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#20840;&#21453;&#39304;&#27169;&#22411;&#20013;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilateral trade revolves around the challenge of facilitating transactions between two strategic agents -- a seller and a buyer -- both of whom have a private valuations for the item. We study the online version of the problem, in which at each time step a new seller and buyer arrive. The learner's task is to set a price for each agent, without any knowledge about their valuations. The sequence of sellers and buyers is chosen by an oblivious adversary. In this setting, known negative results rule out the possibility of designing algorithms with sublinear regret when the learner has to guarantee budget balance for each iteration. In this paper, we introduce the notion of global budget balance, which requires the agent to be budget balance only over the entire time horizon. By requiring global budget balance, we provide the first no-regret algorithms for bilateral trade with adversarial inputs under various feedback models. First, we show that in the full-feedback model the learner can g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#35268;&#21010;&#30340;&#21487;&#35299;&#37322;&#21644;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#36755;&#20837;&#36716;&#21270;&#20026;&#27010;&#24565;&#34920;&#31034;&#12289;&#31526;&#21495;&#25277;&#35937;&#21644;&#25512;&#29702;&#20197;&#21450;&#23558;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#19982;&#30495;&#23454;&#19990;&#30028;&#34892;&#20026;&#20851;&#32852;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#26465;&#20214;&#30340;&#35270;&#35273;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2310.03325</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#21644;&#31526;&#21495;&#25512;&#29702;&#29992;&#20110;&#35270;&#35273;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning. (arXiv:2310.03325v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#35268;&#21010;&#30340;&#21487;&#35299;&#37322;&#21644;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#36755;&#20837;&#36716;&#21270;&#20026;&#27010;&#24565;&#34920;&#31034;&#12289;&#31526;&#21495;&#25277;&#35937;&#21644;&#25512;&#29702;&#20197;&#21450;&#23558;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#19982;&#30495;&#23454;&#19990;&#30028;&#34892;&#20026;&#20851;&#32852;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#26465;&#20214;&#30340;&#35270;&#35273;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35268;&#21010;&#27169;&#25311;&#20102;&#20154;&#31867;&#22312;&#25628;&#32034;&#21021;&#22987;&#35270;&#35273;&#29366;&#24577;&#21644;&#26368;&#32456;&#35270;&#35273;&#30446;&#26631;&#29366;&#24577;&#20043;&#38388;&#30340;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#26469;&#23454;&#29616;&#26399;&#26395;&#30446;&#26631;&#26102;&#25152;&#20570;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#35270;&#35273;&#20013;&#65292;&#35270;&#35273;&#35268;&#21010;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#22312;&#24341;&#23548;&#26234;&#33021;&#20307;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#25191;&#34892;&#26085;&#24120;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#21644;&#21487;&#25512;&#24191;&#30340;&#35270;&#35273;&#35268;&#21010;&#26694;&#26550;&#65292;&#21253;&#25324;&#65306;i&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26367;&#20195;&#30340;&#27010;&#24565;&#23398;&#20064;&#22120;&#65288;SCL&#65289;&#65292;&#23558;&#35270;&#35273;&#36755;&#20837;&#36716;&#21270;&#20026;&#20998;&#35299;&#30340;&#27010;&#24565;&#34920;&#31034;&#65307;ii&#65289;&#36890;&#36807;&#33258;&#23398;&#31526;&#21495;&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#30340;&#31526;&#21495;&#25277;&#35937;&#21644;&#25512;&#29702;&#65307;iii&#65289;&#23558;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#19982;&#35821;&#20041;&#30456;&#20284;&#30340;&#30495;&#23454;&#19990;&#30028;&#34892;&#20026;&#36827;&#34892;&#20851;&#32852;&#30340;&#35270;&#35273;&#22240;&#26524;&#36716;&#25442;&#27169;&#22411;&#65288;ViCT&#65289;&#12290;&#32473;&#23450;&#19968;&#20010;&#21021;&#22987;&#29366;&#24577;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#21644;&#22240;&#26524;&#36716;&#25442;&#30340;&#31526;&#21495;&#25512;&#29702;&#26041;&#27861;&#36827;&#34892;&#30446;&#26631;&#26465;&#20214;&#30340;&#35270;&#35273;&#35268;&#21010;&#65292;&#20197;&#36798;&#21040;&#30446;&#26631;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual planning simulates how humans make decisions to achieve desired goals in the form of searching for visual causal transitions between an initial visual state and a final visual goal state. It has become increasingly important in egocentric vision with its advantages in guiding agents to perform daily tasks in complex environments. In this paper, we propose an interpretable and generalizable visual planning framework consisting of i) a novel Substitution-based Concept Learner (SCL) that abstracts visual inputs into disentangled concept representations, ii) symbol abstraction and reasoning that performs task planning via the self-learned symbols, and iii) a Visual Causal Transition model (ViCT) that grounds visual causal transitions to semantically similar real-world actions. Given an initial state, we perform goal-conditioned visual planning with a symbolic reasoning method fueled by the learned representations and causal transitions to reach the goal state. To verify the effectiv
&lt;/p&gt;</description></item><item><title>ABScribe&#26159;&#19968;&#31181;&#30028;&#38754;&#65292;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#24555;&#36895;&#29983;&#25104;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20197;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#21576;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.00117</link><description>&lt;p&gt;
ABScribe: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models. (arXiv:2310.00117v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00117
&lt;/p&gt;
&lt;p&gt;
ABScribe&#26159;&#19968;&#31181;&#30028;&#38754;&#65292;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#24555;&#36895;&#29983;&#25104;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20197;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#21576;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#20070;&#20889;&#25991;&#26412;&#26469;&#25506;&#32034;&#26367;&#20195;&#24819;&#27861;&#26159;&#20889;&#20316;&#36807;&#31243;&#30340;&#20851;&#38190;&#12290;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#31616;&#21270;&#20889;&#20316;&#21464;&#21270;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30028;&#38754;&#23384;&#22312;&#21516;&#26102;&#32771;&#34385;&#22810;&#31181;&#21464;&#21270;&#30340;&#25361;&#25112;&#65306;&#22312;&#19981;&#35206;&#30422;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#26032;&#30340;&#29256;&#26412;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#32780;&#25353;&#39034;&#24207;&#31896;&#36148;&#23427;&#20204;&#21487;&#33021;&#20250;&#20351;&#25991;&#26723;&#21464;&#24471;&#26434;&#20081;&#65292;&#22686;&#21152;&#24037;&#20316;&#37327;&#65292;&#24182;&#25171;&#26029;&#20316;&#32773;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ABScribe&#65292;&#19968;&#31181;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#19988;&#32467;&#26500;&#21270;&#22320;&#25506;&#32034;&#20889;&#20316;&#21464;&#21270;&#30340;&#30028;&#38754;&#12290;&#36890;&#36807;ABScribe&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;LLM&#25552;&#31034;&#24555;&#36895;&#20135;&#29983;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20250;&#33258;&#21160;&#36716;&#25442;&#25104;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#12290;&#21464;&#20307;&#22312;&#25991;&#26412;&#27573;&#33853;&#20013;&#34987;&#23384;&#20648;&#22312;&#30456;&#37051;&#20301;&#32622;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#19978;&#30340;&#40736;&#26631;&#24748;&#20572;&#20132;&#20114;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;12&#21517;&#25776;&#20889;&#20154;&#21592;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;ABScribe&#33021;&#26174;&#33879;&#20943;&#36731;&#20219;&#21153;&#36127;&#33655;&#65288;d = 1.20, p &lt; 0.001&#65289;&#65292;&#25552;&#39640;&#29992;&#25143;&#30340;&#35748;&#30693;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art large language models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new versions without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly produce multiple variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text segments for rapid in-place comparisons using mouse-over interactions on a context toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p &lt; 0.001), enhances user perceptions o
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#31038;&#21306;&#32467;&#26500;&#26159;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20855;&#26377;&#31038;&#20250;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#31185;&#23398;&#23478;&#20204;&#20570;&#20986;&#20102;&#21162;&#21147;&#65292;&#20294;&#23578;&#26410;&#25214;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#31038;&#21306;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#21508;&#31181;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#21508;&#31181;&#32593;&#32476;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.11798</link><description>&lt;p&gt;
&#22270;&#20013;&#31038;&#21306;&#26816;&#27979;&#30340;&#32508;&#21512;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review of Community Detection in Graphs. (arXiv:2309.11798v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#31038;&#21306;&#32467;&#26500;&#26159;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20855;&#26377;&#31038;&#20250;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#31185;&#23398;&#23478;&#20204;&#20570;&#20986;&#20102;&#21162;&#21147;&#65292;&#20294;&#23578;&#26410;&#25214;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#31038;&#21306;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#21508;&#31181;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#21508;&#31181;&#32593;&#32476;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#30740;&#31350;&#26174;&#33879;&#20419;&#36827;&#20102;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#31038;&#21306;&#32467;&#26500;&#30340;&#29702;&#35299;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#31038;&#20250;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;&#23613;&#31649;&#36328;&#23398;&#31185;&#31185;&#23398;&#23478;&#31038;&#21306;&#30340;&#21162;&#21147;&#65292;&#20294;&#23578;&#26410;&#25214;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#35814;&#32454;&#20171;&#32461;&#20102;&#22270;&#20013;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#30340;&#32452;&#32455;&#21644;&#21151;&#33021;&#36215;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#31038;&#21306;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#23427;&#25351;&#30340;&#26159;&#23558;&#39030;&#28857;&#21010;&#20998;&#20026;&#20855;&#26377;&#24378;&#20869;&#37096;&#36830;&#25509;&#21644;&#36739;&#24369;&#36830;&#25509;&#30340;&#38598;&#32676;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#38416;&#36848;&#65292;&#21253;&#25324;&#25105;&#20204;&#35774;&#35745;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#31038;&#21306;&#26816;&#27979;&#22312;&#21508;&#31181;&#32593;&#32476;&#20013;&#30340;&#30495;&#23454;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of complex networks has significantly advanced our understanding of community structures which serves as a crucial feature of real-world graphs. Detecting communities in graphs is a challenging problem with applications in sociology, biology, and computer science. Despite the efforts of an interdisciplinary community of scientists, a satisfactory solution to this problem has not yet been achieved. This review article delves into the topic of community detection in graphs, which serves as a crucial role in understanding the organization and functioning of complex systems. We begin by introducing the concept of community structure, which refers to the arrangement of vertices into clusters, with strong internal connections and weaker connections between clusters. Then, we provide a thorough exposition of various community detection methods, including a new method designed by us. Additionally, we explore real-world applications of community detection in diverse networks. In concl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TRACE-GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#19994;&#20013;&#26080;&#30417;&#30563;&#25925;&#38556;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#23884;&#20837;&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#26469;&#39044;&#35757;&#32451;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24322;&#24120;&#24207;&#21015;&#21644;&#27491;&#24120;&#24207;&#21015;&#30340;&#20998;&#31867;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11427</link><description>&lt;p&gt;
&#21322;&#23548;&#20307;&#21046;&#36896;&#19994;&#20013;&#26080;&#30417;&#30563;&#25925;&#38556;&#26816;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing. (arXiv:2309.11427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TRACE-GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#19994;&#20013;&#26080;&#30417;&#30563;&#25925;&#38556;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#23884;&#20837;&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#26469;&#39044;&#35757;&#32451;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24322;&#24120;&#24207;&#21015;&#21644;&#27491;&#24120;&#24207;&#21015;&#30340;&#20998;&#31867;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TRACE-GPT&#65288;Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers&#65289;&#65292;&#23427;&#26159;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#26816;&#27979;&#25925;&#38556;&#30340;&#27169;&#22411;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#23884;&#20837;&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#65288;GPT&#65289;&#26469;&#25552;&#21462;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#23545;&#24322;&#24120;&#24207;&#21015;&#21644;&#27491;&#24120;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces TRACE-GPT, which stands for Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor data and detect faults on unlabeled datasets in semiconductor manufacturing. In semiconductor industry, classifying abnormal time-series sensor data from normal data is important because it is directly related to wafer defect. However, small, unlabeled, and even mixed training data without enough anomalies make classification tasks difficult. In this research, we capture features of time-series data with temporal convolutional embedding and Generative Pre-trained Transformer (GPT) to classify abnormal sequences from normal sequences using cross entropy loss. We prove that our model shows better performance than previous unsupervised models with both an open dataset, the University of California Riverside (UCR) time-series classification archive, and the process log of our Ch
&lt;/p&gt;</description></item><item><title>A2V&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#23454;&#29616;&#33041;&#34880;&#31649;&#30340;&#36328;&#27169;&#24577;&#20998;&#21106;&#12290;&#23427;&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#28304;&#22495;&#21040;&#30446;&#26631;&#22495;&#30340;&#22270;&#20687;&#32423;&#33258;&#36866;&#24212;&#65292;&#24182;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06075</link><description>&lt;p&gt;
A2V: &#19968;&#31181;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#29992;&#20110;&#19981;&#21516;&#22270;&#20687;&#27169;&#24577;&#30340;&#33041;&#34880;&#31649;&#20998;&#21106;&#65292;&#36890;&#36807;&#20108;&#38454;&#27573;&#35757;&#32451;&#20174;&#34880;&#31649;&#36896;&#24433;&#21040;&#38745;&#33033;&#36896;&#24433;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel Segmentation via Two-Phase Training Angiography-to-Venography Translation. (arXiv:2309.06075v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06075
&lt;/p&gt;
&lt;p&gt;
A2V&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#23454;&#29616;&#33041;&#34880;&#31649;&#30340;&#36328;&#27169;&#24577;&#20998;&#21106;&#12290;&#23427;&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#28304;&#22495;&#21040;&#30446;&#26631;&#22495;&#30340;&#22270;&#20687;&#32423;&#33258;&#36866;&#24212;&#65292;&#24182;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#19981;&#21516;&#22270;&#20687;&#27169;&#24577;&#20013;&#20998;&#21106;&#33041;&#34880;&#31649;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#38598;&#20013;&#20110;&#21333;&#19968;&#27169;&#24577;&#65292;&#24573;&#35270;&#20102;&#24191;&#27867;&#21487;&#29992;&#30340;&#33041;&#34880;&#31649;&#25104;&#20687;&#25216;&#26415;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#26174;&#33879;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#23545;&#36328;&#27169;&#24577;&#30340;&#27867;&#21270;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#20381;&#36182;&#27880;&#37322;&#30340;&#34880;&#31649;&#36896;&#24433;&#21644;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#37322;&#30340;&#38745;&#33033;&#36896;&#24433;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#21644;&#35821;&#20041;&#20998;&#21106;&#65292;&#21033;&#29992;&#31163;&#25955;&#21270;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#34920;&#31034;&#24322;&#26500;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#28304;&#22495;&#21040;&#30446;&#26631;&#22495;&#30340;&#22270;&#20687;&#32423;&#33258;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#22522;&#20110;&#24490;&#29615;&#30340;&#26550;&#26500;&#30340;&#20856;&#22411;&#22797;&#26434;&#24615;&#65292;&#26368;&#23567;&#21270;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#20351;&#29992;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#31283;&#23450;&#35757;&#32451;&#30340;&#39640;&#25928;&#30452;&#35266;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#30913;&#20849;&#25391;&#34880;&#31649;&#36896;&#24433;&#21644;&#38745;&#33033;&#36896;&#24433;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#29616;&#26368;&#20808;&#36827;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#31283;&#23450;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a semi-supervised domain adaptation framework for brain vessel segmentation from different image modalities. Existing state-of-the-art methods focus on a single modality, despite the wide range of available cerebrovascular imaging techniques. This can lead to significant distribution shifts that negatively impact the generalization across modalities. By relying on annotated angiographies and a limited number of annotated venographies, our framework accomplishes image-to-image translation and semantic segmentation, leveraging a disentangled and semantically rich latent space to represent heterogeneous data and perform image-level adaptation from source to target domains. Moreover, we reduce the typical complexity of cycle-based architectures and minimize the use of adversarial training, which allows us to build an efficient and intuitive model with stable training. We evaluate our method on magnetic resonance angiographies and venographies. While achieving state-of-the-art pe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#33324;&#21270;&#30028;&#38480;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#20449;&#24687;&#35770;&#21644;PAC-Bayesian&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20849;&#21516;&#28857;&#12290;&#36825;&#23545;&#20110;&#29702;&#35770;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#21644;&#26032;&#31639;&#27861;&#30340;&#35774;&#35745;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.04381</link><description>&lt;p&gt;
&#19968;&#33324;&#21270;&#30028;&#38480;&#65306;&#20449;&#24687;&#35770;&#21644;PAC-Bayesian&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Generalization Bounds: Perspectives from Information Theory and PAC-Bayes. (arXiv:2309.04381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04381
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#33324;&#21270;&#30028;&#38480;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#20449;&#24687;&#35770;&#21644;PAC-Bayesian&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20849;&#21516;&#28857;&#12290;&#36825;&#23545;&#20110;&#29702;&#35770;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#21644;&#26032;&#31639;&#27861;&#30340;&#35774;&#35745;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29702;&#35770;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#19968;&#33324;&#21270;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;PAC-Bayesian&#26041;&#27861;&#24050;&#32463;&#34987;&#30830;&#23450;&#20026;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#19968;&#33324;&#21270;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#31639;&#27861;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#23545;&#22810;&#31181;&#23398;&#20064;&#31639;&#27861;&#65288;&#21253;&#25324;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#28508;&#22312;&#36866;&#29992;&#24615;&#65292;&#23427;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36824;&#21457;&#23637;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#65292;&#20854;&#20013;&#24314;&#31435;&#20102;&#19968;&#33324;&#21270;&#19982;&#21508;&#31181;&#20449;&#24687;&#24230;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#20010;&#26694;&#26550;&#19982;PAC-Bayesian&#26041;&#27861;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#26377;&#29420;&#31435;&#21457;&#29616;&#30340;&#24456;&#22810;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#36825;&#31181;&#24378;&#36830;&#25509;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#32479;&#19968;&#30340;&#19968;&#33324;&#21270;&#22788;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#35270;&#35282;&#20849;&#21516;&#25317;&#26377;&#30340;&#25216;&#26415;&#21644;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#21644;&#35299;&#37322;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#36830;&#25509;&#22914;&#20309;&#20135;&#29983;&#26032;&#30340;&#27934;&#35265;&#21644;&#29702;&#35770;&#30340;&#21457;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#20132;&#21449;&#24212;&#29992;&#21644;&#28508;&#22312;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental question in theoretical machine learning is generalization. Over the past decades, the PAC-Bayesian approach has been established as a flexible framework to address the generalization capabilities of machine learning algorithms, and design new ones. Recently, it has garnered increased interest due to its potential applicability for a variety of learning algorithms, including deep neural networks. In parallel, an information-theoretic view of generalization has developed, wherein the relation between generalization and various information measures has been established. This framework is intimately connected to the PAC-Bayesian approach, and a number of results have been independently discovered in both strands. In this monograph, we highlight this strong connection and present a unified treatment of generalization. We present techniques and results that the two perspectives have in common, and discuss the approaches and interpretations that differ. In particular, we demons
&lt;/p&gt;</description></item><item><title>LCANets++&#26159;&#19968;&#31181;&#20351;&#29992;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#23618;&#38388;&#31454;&#20105;&#30340;&#40065;&#26834;&#24615;&#38899;&#39057;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#26469;&#25552;&#39640;&#23545;&#25200;&#21160;&#21644;&#23545;&#25239;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12882</link><description>&lt;p&gt;
LCANets++: &#20351;&#29992;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#23618;&#38388;&#31454;&#20105;&#30340;&#40065;&#26834;&#24615;&#38899;&#39057;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LCANets++: Robust Audio Classification using Multi-layer Neural Networks with Lateral Competition. (arXiv:2308.12882v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12882
&lt;/p&gt;
&lt;p&gt;
LCANets++&#26159;&#19968;&#31181;&#20351;&#29992;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#23618;&#38388;&#31454;&#20105;&#30340;&#40065;&#26834;&#24615;&#38899;&#39057;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#26469;&#25552;&#39640;&#23545;&#25200;&#21160;&#21644;&#23545;&#25239;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#20998;&#31867;&#26088;&#22312;&#35782;&#21035;&#38899;&#39057;&#20449;&#21495;&#65292;&#21253;&#25324;&#35821;&#38899;&#21629;&#20196;&#25110;&#22768;&#38899;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38899;&#39057;&#20998;&#31867;&#22120;&#23481;&#26131;&#21463;&#21040;&#25200;&#21160;&#21644;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#36890;&#24120;&#21463;&#21040;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#36890;&#36807;&#23616;&#37096;&#31454;&#20105;&#31639;&#27861;&#65288;LCA&#65289;&#22312;&#31532;&#19968;&#23618;&#20351;&#29992;&#20102;&#31070;&#32463;&#21551;&#21457;&#24335;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#36827;&#34892;&#31232;&#30095;&#32534;&#30721;&#65292;&#21363;LCANets&#12290;LCANets&#36890;&#36807;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#32452;&#21512;&#26469;&#23398;&#20064;&#65292;&#20943;&#23569;&#23545;&#26631;&#35760;&#26679;&#26412;&#30340;&#20381;&#36182;&#24615;&#12290;&#21463;&#21040;&#21548;&#35273;&#30382;&#23618;&#20063;&#26159;&#31232;&#30095;&#30340;&#20107;&#23454;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;LCANets&#25193;&#23637;&#21040;&#38899;&#39057;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;LCANets++&#65292;&#23427;&#20204;&#26159;CNNs&#65292;&#36890;&#36807;LCA&#22312;&#22810;&#23618;&#27425;&#19978;&#36827;&#34892;&#31232;&#30095;&#32534;&#30721;&#12290;&#25105;&#20204;&#35777;&#26126;LCANets++&#23545;&#20110;&#25200;&#21160;&#65288;&#22914;&#32972;&#26223;&#22122;&#22768;&#65289;&#20197;&#21450;&#40657;&#30418;&#21644;&#30333;&#30418;&#25915;&#20987;&#65288;&#22914;&#36867;&#36991;&#21644;&#30772;&#22351;&#65289;&#27604;&#26631;&#20934;CNN&#21644;LCANets&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio classification aims at recognizing audio signals, including speech commands or sound events. However, current audio classifiers are susceptible to perturbations and adversarial attacks. In addition, real-world audio classification tasks often suffer from limited labeled data. To help bridge these gaps, previous work developed neuro-inspired convolutional neural networks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA) in the first layer (i.e., LCANets) for computer vision. LCANets learn in a combination of supervised and unsupervised learning, reducing dependency on labeled samples. Motivated by the fact that auditory cortex is also sparse, we extend LCANets to audio recognition tasks and introduce LCANets++, which are CNNs that perform sparse coding in multiple layers via LCA. We demonstrate that LCANets++ are more robust than standard CNNs and LCANets against perturbations, e.g., background noise, as well as black-box and white-box attacks, e.g., evasion an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOOD&#30340;&#23454;&#26102;&#31283;&#20581;&#30340;&#20154;&#21592;&#23384;&#22312;&#21644;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#25104;&#26412;&#30340;60 GHz FMCW&#38647;&#36798;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21487;&#21516;&#26102;&#35299;&#20915;&#23384;&#22312;&#26816;&#27979;&#21644;&#31163;&#32676;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#37325;&#26500;&#26550;&#26500;&#21644;&#38647;&#36798;&#22270;&#20687;&#23454;&#29616;&#20934;&#30830;&#26816;&#27979;&#20154;&#31867;&#23384;&#22312;&#65292;&#21516;&#26102;&#22312;&#20154;&#31867;&#19981;&#23384;&#22312;&#26102;&#26816;&#27979;&#31227;&#21160;&#25110;&#38745;&#27490;&#24178;&#25200;&#29289;&#12290;</title><link>http://arxiv.org/abs/2308.02396</link><description>&lt;p&gt;
HOOD: &#23454;&#26102;&#31283;&#20581;&#30340;&#20302;&#25104;&#26412;FMCW&#38647;&#36798;&#20154;&#21592;&#23384;&#22312;&#21644;&#31163;&#32676;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
HOOD: Real-Time Robust Human Presence and Out-of-Distribution Detection with Low-Cost FMCW Radar. (arXiv:2308.02396v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOOD&#30340;&#23454;&#26102;&#31283;&#20581;&#30340;&#20154;&#21592;&#23384;&#22312;&#21644;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#25104;&#26412;&#30340;60 GHz FMCW&#38647;&#36798;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21487;&#21516;&#26102;&#35299;&#20915;&#23384;&#22312;&#26816;&#27979;&#21644;&#31163;&#32676;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#37325;&#26500;&#26550;&#26500;&#21644;&#38647;&#36798;&#22270;&#20687;&#23454;&#29616;&#20934;&#30830;&#26816;&#27979;&#20154;&#31867;&#23384;&#22312;&#65292;&#21516;&#26102;&#22312;&#20154;&#31867;&#19981;&#23384;&#22312;&#26102;&#26816;&#27979;&#31227;&#21160;&#25110;&#38745;&#27490;&#24178;&#25200;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20351;&#29992;&#27627;&#31859;&#27874;&#39057;&#29575;&#35843;&#21046;&#36830;&#32493;&#27874;&#65288;FMCW&#65289;&#38647;&#36798;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23460;&#20869;&#31354;&#38388;&#20013;&#23384;&#22312;&#31227;&#21160;&#21644;&#38745;&#27490;&#30340;&#26434;&#27874;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;HOOD&#8221;&#30340;&#23454;&#26102;&#31283;&#20581;&#30340;&#20154;&#21592;&#23384;&#22312;&#21644;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;60 GHz&#30701;&#36317;&#31163;FMCW&#38647;&#36798;&#12290;&#25105;&#20204;&#23558;&#23384;&#22312;&#26816;&#27979;&#24212;&#29992;&#35270;&#20026;&#31163;&#32676;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21333;&#19968;&#27969;&#31243;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#22522;&#20110;&#37325;&#26500;&#30340;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#38647;&#36798;&#23439;&#35266;&#21644;&#24494;&#35266;&#33539;&#22260;-Doppler&#22270;&#20687;&#65288;RDI&#65289;&#12290;HOOD&#26088;&#22312;&#22312;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#31227;&#21160;&#21644;&#38745;&#27490;&#24178;&#25200;&#29289;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#26816;&#27979;&#20154;&#31867;&#30340;&#8220;&#23384;&#22312;&#8221;&#12290;&#30001;&#20110;&#23427;&#20063;&#26159;&#19968;&#20010;&#31163;&#32676;&#26816;&#27979;&#22120;&#65292;&#23427;&#26088;&#22312;&#23558;&#31227;&#21160;&#25110;&#38745;&#27490;&#26434;&#27874;&#20316;&#20026;&#20154;&#31867;&#19981;&#23384;&#22312;&#30340;&#31163;&#32676;&#26816;&#27979;&#65292;&#24182;&#23558;&#24403;&#21069;&#22330;&#26223;&#30340;&#36755;&#20986;&#39044;&#27979;&#20026;&#8220;&#26080;&#20154;&#23384;&#22312;&#8221;&#12290;HOOD&#26159;&#19968;&#31181;&#26080;&#38656;&#20219;&#20309;&#27963;&#21160;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20154;&#31867;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human presence detection in indoor environments using millimeter-wave frequency-modulated continuous-wave (FMCW) radar is challenging due to the presence of moving and stationary clutters in indoor places. This work proposes "HOOD" as a real-time robust human presence and out-of-distribution (OOD) detection method by exploiting 60 GHz short-range FMCW radar. We approach the presence detection application as an OOD detection problem and solve the two problems simultaneously using a single pipeline. Our solution relies on a reconstruction-based architecture and works with radar macro and micro range-Doppler images (RDIs). HOOD aims to accurately detect the "presence" of humans in the presence or absence of moving and stationary disturbers. Since it is also an OOD detector, it aims to detect moving or stationary clutters as OOD in humans' absence and predicts the current scene's output as "no presence." HOOD is an activity-free approach that performs well in different human scenarios. On 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39640;&#32500;&#38382;&#39064;&#12289;&#22312;&#20219;&#24847;&#25968;&#37327;&#25308;&#21344;&#24237;&#25915;&#20987;&#32773;&#19979;&#30340;&#26032;&#26041;&#27861;&#65292;&#26680;&#24515;&#26159;&#19968;&#31181;&#30452;&#25509;&#30340;&#39640;&#32500;&#21322;&#39564;&#35777;&#22343;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#20855;&#26377;&#26497;&#23567;&#26497;&#20540;&#32479;&#35745;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.13352</link><description>&lt;p&gt;
&#39640;&#32500;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#22312;&#20219;&#24847;&#25968;&#37327;&#25308;&#21344;&#24237;&#25915;&#20987;&#32773;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers. (arXiv:2307.13352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39640;&#32500;&#38382;&#39064;&#12289;&#22312;&#20219;&#24847;&#25968;&#37327;&#25308;&#21344;&#24237;&#25915;&#20987;&#32773;&#19979;&#30340;&#26032;&#26041;&#27861;&#65292;&#26680;&#24515;&#26159;&#19968;&#31181;&#30452;&#25509;&#30340;&#39640;&#32500;&#21322;&#39564;&#35777;&#22343;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#20855;&#26377;&#26497;&#23567;&#26497;&#20540;&#32479;&#35745;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20855;&#26377;&#25308;&#21344;&#24237;&#25925;&#38556;&#30340;&#24378;&#40065;&#26834;&#20998;&#24067;&#24335;&#23398;&#20064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#38480;&#21046;&#65292;&#38543;&#30528;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#20005;&#37325;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39640;&#32500;&#38382;&#39064;&#12289;&#22312;&#20219;&#24847;&#25968;&#37327;&#25308;&#21344;&#24237;&#25915;&#20987;&#32773;&#19979;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#26680;&#24515;&#26159;&#19968;&#31181;&#30452;&#25509;&#30340;&#39640;&#32500;&#21322;&#39564;&#35777;&#22343;&#20540;&#20272;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#39318;&#20808;&#35782;&#21035;&#19968;&#20010;&#23376;&#31354;&#38388;&#65292;&#36890;&#36807;&#24037;&#20316;&#26426;&#19978;&#20256;&#30340;&#26799;&#24230;&#21521;&#37327;&#20272;&#35745;&#19982;&#35813;&#23376;&#31354;&#38388;&#22402;&#30452;&#30340;&#22343;&#20540;&#20998;&#37327;&#65292;&#32780;&#36890;&#36807;&#36741;&#21161;&#25968;&#25454;&#38598;&#20272;&#35745;&#35813;&#23376;&#31354;&#38388;&#20869;&#30340;&#22343;&#20540;&#20998;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#29992;&#20316;&#20998;&#24067;&#24335;&#23398;&#20064;&#38382;&#39064;&#30340;&#32858;&#21512;&#22120;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#26032;&#26041;&#27861;&#20855;&#26377;&#26497;&#23567;&#26497;&#20540;&#32479;&#35745;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#32500;&#24230;&#30340;&#20381;&#36182;&#24615;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust distributed learning with Byzantine failures has attracted extensive research interests in recent years. However, most of existing methods suffer from curse of dimensionality, which is increasingly serious with the growing complexity of modern machine learning models. In this paper, we design a new method that is suitable for high dimensional problems, under arbitrary number of Byzantine attackers. The core of our design is a direct high dimensional semi-verified mean estimation method. Our idea is to identify a subspace first. The components of mean value perpendicular to this subspace can be estimated via gradient vectors uploaded from worker machines, while the components within this subspace are estimated using auxiliary dataset. We then use our new method as the aggregator of distributed learning problems. Our theoretical analysis shows that the new method has minimax optimal statistical rates. In particular, the dependence on dimensionality is significantly improved compar
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Harpa&#30340;&#39640;&#36895;&#29575;&#22320;&#38663;&#30456;&#20301;&#20851;&#32852;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#22330;&#26500;&#24314;&#27874;&#36895;&#21644;&#30456;&#20851;&#36208;&#26102;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#27874;&#36895;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#30456;&#20301;&#20851;&#32852;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#36739;&#23567;&#12289;&#39640;&#36895;&#29575;&#30340;&#22320;&#38663;&#20107;&#20214;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22320;&#19979;&#24377;&#24615;&#20171;&#36136;&#23646;&#24615;&#30340;&#23453;&#36149;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2307.07572</link><description>&lt;p&gt;
Harpa: &#39640;&#36895;&#29575;&#19979;&#30340;&#30456;&#20301;&#20851;&#32852;&#19982;&#36208;&#26102;&#31070;&#32463;&#22330;
&lt;/p&gt;
&lt;p&gt;
Harpa: High-Rate Phase Association with Travel Time Neural Fields. (arXiv:2307.07572v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Harpa&#30340;&#39640;&#36895;&#29575;&#22320;&#38663;&#30456;&#20301;&#20851;&#32852;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#22330;&#26500;&#24314;&#27874;&#36895;&#21644;&#30456;&#20851;&#36208;&#26102;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#27874;&#36895;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#30456;&#20301;&#20851;&#32852;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#36739;&#23567;&#12289;&#39640;&#36895;&#29575;&#30340;&#22320;&#38663;&#20107;&#20214;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22320;&#19979;&#24377;&#24615;&#20171;&#36136;&#23646;&#24615;&#30340;&#23453;&#36149;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20301;&#20851;&#32852;&#26159;&#26681;&#25454;&#20854;&#36215;&#28304;&#22320;&#38663;&#20998;&#32452;&#22320;&#38663;&#27874;&#21040;&#36798;&#30340;&#20219;&#21153;&#12290;&#23427;&#26159;&#22320;&#38663;&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20294;&#23545;&#20110;&#36739;&#23567;&#12289;&#39640;&#36895;&#29575;&#30340;&#22320;&#38663;&#20107;&#20214;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36825;&#20123;&#20107;&#20214;&#25658;&#24102;&#26377;&#20851;&#22320;&#38663;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#22312;&#24120;&#24120;&#20551;&#23450;&#19981;&#20934;&#30830;&#30340;&#27874;&#36895;&#27169;&#22411;&#19979;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#20851;&#32852;&#26041;&#27861;&#37117;&#19987;&#27880;&#20110;&#21457;&#29983;&#29575;&#36739;&#20302;&#19988;&#23481;&#26131;&#20851;&#32852;&#30340;&#36739;&#22823;&#20107;&#20214;&#65292;&#23613;&#31649;&#24494;&#22320;&#38663;&#27963;&#21160;&#25552;&#20379;&#20102;&#20117;&#19979;&#24377;&#24615;&#20171;&#36136;&#23646;&#24615;&#30340;&#23453;&#36149;&#25551;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#27874;&#36895;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#20197;&#27604;&#20197;&#21069;&#25253;&#21578;&#30340;&#26356;&#39640;&#30340;&#36895;&#29575;&#36827;&#34892;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Harpa&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#36895;&#29575;&#22320;&#38663;&#30456;&#20301;&#20851;&#32852;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#22330;&#26500;&#24314;&#27874;&#36895;&#21644;&#30456;&#20851;&#36208;&#26102;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#39318;&#20808;&#35299;&#20915;&#32852;&#21512;&#26102;&#31354;&#28304;&#23450;&#20301;&#21644;&#27874;&#36895;&#24674;&#22797;&#38382;&#39064;&#65292;&#28982;&#21518;&#36827;&#34892;&#30456;&#20301;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phase association groups seismic wave arrivals according to their originating earthquakes. It is a fundamental task in a seismic data processing pipeline, but challenging to perform for smaller, high-rate seismic events which carry fundamental information about earthquake dynamics, especially with a commonly assumed inaccurate wave speed model. As a consequence, most association methods focus on larger events that occur at a lower rate and are thus easier to associate, even though microseismicity provides a valuable description of the elastic medium properties in the subsurface. In this paper, we show that association is possible at rates much higher than previously reported even when the wave speed is unknown. We propose Harpa, a high-rate seismic phase association method which leverages deep neural fields to build generative models of wave speeds and associated travel times, and first solves a joint spatio--temporal source localization and wave speed recovery problem, followed by ass
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2306.16772</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#30340;&#20154;&#31867;&#22242;&#38431;&#27963;&#21160;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Synthetic Human Group Activities. (arXiv:2306.16772v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16772
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#23545;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#30340;&#29702;&#35299;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#20219;&#21153;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#33719;&#21462;&#22823;&#35268;&#27169;&#26631;&#35760;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;M3Act&#37319;&#29992;Unity&#24341;&#25806;&#39537;&#21160;&#65292;&#21253;&#21547;&#21487;&#20379;&#20223;&#30495;&#20351;&#29992;&#30340;&#19977;&#32500;&#22330;&#26223;&#21644;&#20154;&#29289;&#36164;&#28304;&#65292;&#21487;&#37197;&#32622;&#30340;&#29031;&#26126;&#21644;&#25668;&#20687;&#31995;&#32479;&#65292;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22359;&#21270;&#22242;&#38431;&#27963;&#21160;&#65292;&#20197;&#21450;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20855;&#26377;&#22823;&#37327;&#39046;&#22495;&#38543;&#26426;&#21270;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#35270;&#22270;&#12289;&#27169;&#24577;&#65288;RGB&#22270;&#20687;&#12289;2D&#23039;&#21183;&#12289;3D&#21160;&#20316;&#65289;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65288;2D&#36793;&#30028;&#26694;&#12289;&#23454;&#20363;&#20998;&#21106;&#25513;&#27169;&#12289;&#20010;&#20307;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#31867;&#21035;&#65289;&#12290;&#21033;&#29992;M3Act&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision. However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets. To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator. Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process. Our data generator is capable of generating large-scale datasets of human activities with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories). Using M3Act, we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#27169;&#25311;&#21453;&#20107;&#23454;&#20998;&#24067;&#20013;&#30340;&#20540;&#65292;&#21487;&#23545;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#35774;&#23450;&#26465;&#20214;&#65292;&#24182;&#24212;&#29992;&#20110;&#20449;&#29992;&#35780;&#20998;&#20013;&#30340;&#20844;&#24179;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.15328</link><description>&lt;p&gt;
&#27169;&#25311;&#21453;&#20107;&#23454;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Simulating counterfactuals. (arXiv:2306.15328v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15328
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#27169;&#25311;&#21453;&#20107;&#23454;&#20998;&#24067;&#20013;&#30340;&#20540;&#65292;&#21487;&#23545;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#35774;&#23450;&#26465;&#20214;&#65292;&#24182;&#24212;&#29992;&#20110;&#20449;&#29992;&#35780;&#20998;&#20013;&#30340;&#20844;&#24179;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#26029;&#32771;&#34385;&#20102;&#22312;&#19982;&#23454;&#38469;&#19990;&#30028;&#23384;&#22312;&#19968;&#20123;&#35777;&#25454;&#30340;&#24179;&#34892;&#19990;&#30028;&#20013;&#36827;&#34892;&#30340;&#20551;&#35774;&#24615;&#24178;&#39044;&#12290;&#22914;&#26524;&#35777;&#25454;&#22312;&#27969;&#24418;&#19978;&#25351;&#23450;&#20102;&#26465;&#20214;&#20998;&#24067;&#65292;&#21453;&#20107;&#23454;&#21487;&#33021;&#26159;&#35299;&#26512;&#38590;&#35299;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#21453;&#20107;&#23454;&#20998;&#24067;&#20013;&#27169;&#25311;&#20540;&#65292;&#20854;&#20013;&#21487;&#20197;&#23545;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#35774;&#23450;&#26465;&#20214;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#34987;&#21576;&#29616;&#20026;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#20174;&#32780;&#23548;&#33268;&#28176;&#36817;&#26377;&#25928;&#30340;&#25512;&#26029;&#12290;&#35813;&#31639;&#27861;&#34987;&#24212;&#29992;&#20110;&#20449;&#29992;&#35780;&#20998;&#20013;&#30340;&#20844;&#24179;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual inference considers a hypothetical intervention in a parallel world that shares some evidence with the factual world. If the evidence specifies a conditional distribution on a manifold, counterfactuals may be analytically intractable. We present an algorithm for simulating values from a counterfactual distribution where conditions can be set on both discrete and continuous variables. We show that the proposed algorithm can be presented as a particle filter leading to asymptotically valid inference. The algorithm is applied to fairness analysis in credit scoring.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.09459</link><description>&lt;p&gt;
&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Recurrent Memory Decision Transformer. (arXiv:2306.09459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#38761;&#24615;&#27169;&#22411;&#26368;&#21021;&#26159;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32780;&#24320;&#21457;&#30340;&#65292;&#26368;&#36817;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#26159;&#22240;&#20026;&#20195;&#29702;&#30340;&#21382;&#21490;&#21487;&#20197;&#34920;&#31034;&#20026;&#24207;&#21015;&#65292;&#24182;&#19988;&#25972;&#20010;&#20219;&#21153;&#21487;&#20197;&#32553;&#20943;&#20026;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#25805;&#20316;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#19978;&#19979;&#25991;&#30340;&#28508;&#22312;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#20351;&#29992;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;&#35760;&#24518;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;Atari&#28216;&#25103;&#19978;&#26174;&#30528;&#20248;&#20110;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20180;&#32454;&#30740;&#31350;&#20102;&#35760;&#24518;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32489;&#25928;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#24320;&#21457;&#26356;&#39640;&#25928;&#21644;&#26356;&#26377;&#25928;&#30340;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformative models, originally developed for natural language problems, have recently been widely used in offline reinforcement learning tasks. This is due to the fact that the agent's history can be represented as a sequence, and the whole task can be reduced to the sequence modeling task. However, the quadratic complexity of the transformer operation limits the potential increase in context. Therefore, to work with long sequences in a natural language, different versions of the memory mechanism are used. In this paper, we propose the Recurrent Memory Decision Transformer (RMDT), a model that uses a recurrent memory mechanism for reinforcement learning problems. We conduct thorough experiments on Atari games and MoJoCo control problems, and show that our proposed model is significantly superior to its counterparts without the recurrent memory mechanism on Atari games. We also carefully study the effect of memory on the performance of the proposed model. These findings shed light on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;WSAUC&#65292;&#19968;&#31181;&#35299;&#20915;&#24369;&#30417;&#30563;&#19979;AUC&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#12289;&#27491;-&#26080;&#26631;&#31614;&#23398;&#20064;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37096;&#20998;AUC&#8212;&#8212;&#21453;&#36716;&#37096;&#20998;AUC&#65288;rpAUC&#65289;&#65292;&#20316;&#20026;&#40065;&#26834;&#30340;AUC&#26368;&#22823;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20026;&#21508;&#31181;&#24369;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;AUC&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.14258</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#19979;AUC&#20248;&#21270;&#65306;&#32479;&#19968;&#30340;&#37096;&#20998;AUC&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised AUC Optimization: A Unified Partial AUC Approach. (arXiv:2305.14258v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;WSAUC&#65292;&#19968;&#31181;&#35299;&#20915;&#24369;&#30417;&#30563;&#19979;AUC&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#12289;&#27491;-&#26080;&#26631;&#31614;&#23398;&#20064;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37096;&#20998;AUC&#8212;&#8212;&#21453;&#36716;&#37096;&#20998;AUC&#65288;rpAUC&#65289;&#65292;&#20316;&#20026;&#40065;&#26834;&#30340;AUC&#26368;&#22823;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20026;&#21508;&#31181;&#24369;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;AUC&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33719;&#21462;&#23436;&#32654;&#30340;&#30417;&#30563;&#36890;&#24120;&#24456;&#22256;&#38590;&#65292;&#29616;&#23454;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#38754;&#20020;&#19981;&#20934;&#30830;&#12289;&#19981;&#23436;&#25972;&#25110;&#19981;&#31934;&#30830;&#30340;&#30417;&#30563;&#65292;&#32479;&#31216;&#20026;&#24369;&#30417;&#30563;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WSAUC&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#24369;&#30417;&#30563;&#19979;AUC&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23427;&#28085;&#30422;&#20102;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#12289;&#27491;-&#26080;&#26631;&#31614;&#23398;&#20064;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#12290;&#22312;WSAUC&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#21508;&#31181;&#24369;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;AUC&#20248;&#21270;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#26368;&#23567;&#21270;&#21463;&#27745;&#26579;&#38598;&#21512;&#19978;AUC&#39118;&#38505;&#30340;&#24120;&#35265;&#24418;&#24335;&#65292;&#24182;&#35777;&#26126;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#19982;&#30495;&#23454;AUC&#19968;&#33268;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#26032;&#22411;&#30340;&#37096;&#20998;AUC&#65292;&#21363;&#21453;&#36716;&#37096;&#20998;AUC&#65288;rpAUC&#65289;&#65292;&#23427;&#20316;&#20026;&#40065;&#26834;&#30340;AUC&#26368;&#22823;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#22312;&#23384;&#22312;&#27745;&#26579;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#20316;&#29992;&#12290;WSAUC&#20026;&#21508;&#31181;&#24369;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;AUC&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since acquiring perfect supervision is usually difficult, real-world machine learning tasks often confront inaccurate, incomplete, or inexact supervision, collectively referred to as weak supervision. In this work, we present WSAUC, a unified framework for weakly supervised AUC optimization problems, which covers noisy label learning, positive-unlabeled learning, multi-instance learning, and semi-supervised learning scenarios. Within the WSAUC framework, we first frame the AUC optimization problems in various weakly supervised scenarios as a common formulation of minimizing the AUC risk on contaminated sets, and demonstrate that the empirical risk minimization problems are consistent with the true AUC. Then, we introduce a new type of partial AUC, specifically, the reversed partial AUC (rpAUC), which serves as a robust training objective for AUC maximization in the presence of contaminated labels. WSAUC offers a universal solution for AUC optimization in various weakly supervised scena
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#26368;&#23567;&#21270;&#36890;&#20449;&#28040;&#32791;&#30340;&#19977;&#32500;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#26234;&#33021;&#20998;&#24067;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12289;&#26032;&#39062;&#36229;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#36890;&#20449;&#27169;&#22411;&#65292;&#20351;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;GPU&#31354;&#38386;&#26102;&#38388;&#38477;&#20302;&#20102;50&#65285;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2305.13525</link><description>&lt;p&gt;
&#26368;&#23567;&#21270;&#36890;&#20449;&#30340;&#24322;&#27493;&#24352;&#37327;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Communication-minimizing Asynchronous Tensor Parallelism. (arXiv:2305.13525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#26368;&#23567;&#21270;&#36890;&#20449;&#28040;&#32791;&#30340;&#19977;&#32500;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#26234;&#33021;&#20998;&#24067;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12289;&#26032;&#39062;&#36229;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#36890;&#20449;&#27169;&#22411;&#65292;&#20351;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;GPU&#31354;&#38386;&#26102;&#38388;&#38477;&#20302;&#20102;50&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;&#25193;&#22823;&#21040;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65292;&#35774;&#35745;&#33021;&#22815;&#22312;&#22810;GPU&#38598;&#32676;&#19978;&#39640;&#25928;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#30340;&#24182;&#34892;&#31639;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#20840;&#26032;&#30340;&#19977;&#32500;&#65288;3D&#65289;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#22823;&#22411;&#22810;&#21313;&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#24182;&#34892;&#35757;&#32451;&#20013;&#30001;&#36890;&#20449;&#24341;&#36215;&#30340;&#31354;&#38386;&#26102;&#38388;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26234;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20998;&#24067;&#26041;&#24335;&#65292;&#28040;&#38500;&#20102;&#20026;&#28385;&#36275;&#21508;&#23618;&#25968;&#25454;&#20381;&#36182;&#32780;&#38656;&#35201;&#30340;&#36890;&#20449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#35757;&#32451;&#36807;&#31243;&#36229;&#20998;&#35299;&#26041;&#27861;&#65292;&#21033;&#29992;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36890;&#20449;&#19982;&#35745;&#31639;&#30340;&#37325;&#21472;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;GPU&#31354;&#38386;&#26102;&#38388;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#27169;&#22411;&#65292;&#24110;&#21161;&#29992;&#25143;&#20026;&#32473;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#36890;&#20449;&#26368;&#20248;&#30340;&#21487;&#29992;&#30828;&#20214;&#36164;&#28304;&#20998;&#35299;&#12290; &#23545;&#20110;256 A100 GPU&#19978;&#30340;28B&#21442;&#25968;CNN&#65292;&#22312;&#26412;&#25991;&#30340; Tensor3D &#26041;&#27861;&#19979;&#65292;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604; GPU &#31354;&#38386;&#26102;&#38388;&#20063;&#38477;&#20302;&#20102;&#32422;50&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
As state-of-the-art neural networks scale to billions of parameters, designing parallel algorithms that can train these networks efficiently on multi-GPU clusters has become critical. This paper presents Tensor3D, a novel three-dimensional (3D) approach to parallelize tensor computations, that strives to minimize the idle time incurred due to communication in parallel training of large multi-billion parameter models. First, we introduce an intelligent distribution of neural network parameters across GPUs that eliminates communication required for satisfying data dependencies of individual layers. Then, we propose a novel overdecomposition of the parallel training process, using which we achieve significant overlap of communication with computation, thereby reducing GPU idle time. Finally, we present a communication model, which helps users identify communication optimal decompositions of available hardware resources for a given neural network. For a 28B parameter CNN on 256 A100 GPUs, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.03123</link><description>&lt;p&gt;
ChatGPT &#38656;&#35201;&#36827;&#34892;SPADE&#65288;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65289;&#35780;&#20272;&#65306;&#19968;&#39033;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#21478;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#24615;&#33021;&#21644;&#26377;&#25928;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#22312;&#30740;&#31350;&#21644;&#24037;&#19994;&#30028;&#20013;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21457;&#34920;&#65292;&#20197;&#23637;&#31034;ChatGPT&#21644;&#20854;&#20182;LLMs&#30340;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#12289;&#38598;&#25104;&#21644;&#24773;&#24863;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#22823;&#22810;&#25968;&#34987;&#24573;&#35270;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65292;&#24182;&#24314;&#35758;&#19981;&#20165;&#20165;&#26159;ChatGPT&#65292;&#32780;&#26159;&#22312;&#23545;&#35805;&#26426;&#22120;&#20154;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#21518;&#32493;&#20837;&#21475;&#37117;&#24212;&#35813;&#36827;&#34892;SPADE&#35780;&#20272;&#12290;&#26412;&#25991;&#35814;&#32454;&#35752;&#35770;&#20102;&#20851;&#20110;ChatGPT&#30340;&#38382;&#39064;&#21644;&#20851;&#27880;&#28857;&#19982;&#19978;&#36848;&#29305;&#24449;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#21021;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21487;&#35270;&#21270;&#20197;&#21450;&#20551;&#35774;&#30340;&#20107;&#23454;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36824;&#20026;&#27599;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#35821;&#35328;&#29305;&#24449;&#23545;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21021;&#27493;&#25552;&#20379;&#20102;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.02151</link><description>&lt;p&gt;
&#35821;&#35328;&#36317;&#31163;&#19982;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#30340;&#30456;&#20851;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space. (arXiv:2305.02151v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02151
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#35821;&#35328;&#29305;&#24449;&#23545;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21021;&#27493;&#25552;&#20379;&#20102;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#19981;&#21516;&#35821;&#35328;&#29305;&#24449;&#23545;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#31181;&#25928;&#24212;&#22914;&#20309;&#26144;&#23556;&#21040;&#34920;&#31034;&#31354;&#38388;&#20013;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24494;&#35843;&#26399;&#38388;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#23545;&#40784;&#19978;&#30340;&#24433;&#21709;&#65292;&#32780;&#26412;&#30740;&#31350;&#30740;&#31350;&#30340;&#26159;&#30001;MLLMs&#29983;&#25104;&#30340;&#30456;&#24212;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#30340;&#32477;&#23545;&#28436;&#21464;&#12290;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#35821;&#35328;&#29305;&#24449;&#30340;&#20316;&#29992;&#65292;&#24182;&#35843;&#26597;&#20854;&#19982;&#34920;&#31034;&#31354;&#38388;&#21644;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#65292;&#35828;&#26126;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior research has investigated the impact of various linguistic features on cross-lingual transfer performance. In this study, we investigate the manner in which this effect can be mapped onto the representation space. While past studies have focused on the impact on cross-lingual alignment in multilingual language models during fine-tuning, this study examines the absolute evolution of the respective language representation spaces produced by MLLMs. We place a specific emphasis on the role of linguistic characteristics and investigate their inter-correlation with the impact on representation spaces and cross-lingual transfer performance. Additionally, this paper provides preliminary evidence of how these findings can be leveraged to enhance transfer to linguistically distant languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;ECG&#34920;&#24449;&#23398;&#20064;&#19978;&#30340;&#24212;&#29992;&#65292;&#39318;&#27425;&#23545;&#19977;&#20010;&#24120;&#29992;ECG&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#24067;&#20998;&#26512;&#65292;&#23454;&#39564;&#21457;&#29616;SwAV&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#65292;&#33021;&#22815;&#36229;&#36234;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36824;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#26377;&#26395;&#22312;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#20154;&#32676;&#20013;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#12290;</title><link>http://arxiv.org/abs/2304.06427</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;ECG&#34920;&#24449;&#23398;&#20064;&#22312;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;&#65306;&#20998;&#24067;&#20998;&#26512;&#21450;&#23454;&#39564;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
In-Distribution and Out-of-Distribution Self-supervised ECG Representation Learning for Arrhythmia Detection. (arXiv:2304.06427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;ECG&#34920;&#24449;&#23398;&#20064;&#19978;&#30340;&#24212;&#29992;&#65292;&#39318;&#27425;&#23545;&#19977;&#20010;&#24120;&#29992;ECG&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#24067;&#20998;&#26512;&#65292;&#23454;&#39564;&#21457;&#29616;SwAV&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#65292;&#33021;&#22815;&#36229;&#36234;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36824;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#26377;&#26395;&#22312;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#20154;&#32676;&#20013;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24515;&#30005;&#22270;(ECG)&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;(Self-Supervised Learning, SSL)&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#19977;&#20010;&#24120;&#29992;&#30340;ECG&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#24067;&#20998;&#26512;&#65292;&#24182;&#36827;&#34892;&#20102;&#32508;&#21512;&#24615;&#23454;&#39564;&#65292;&#20351;&#29992;&#19981;&#21516;&#22686;&#24378;&#21644;&#21442;&#25968;&#35780;&#20272;&#20102;&#21508;&#31181;SSL&#26041;&#27861;&#65288;&#22914;SimCRL&#12289;BYOL&#21644;SwAV&#65289;&#22312;ECG&#34920;&#24449;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SwAV&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36827;&#34892;&#20102;&#38024;&#23545;In-Distribution (ID)&#21644;Out-of-Distribution (OOD) ECG&#25968;&#25454;&#30340;&#20132;&#21449;&#25968;&#25454;&#38598;&#35757;&#32451;&#21644;&#27979;&#35797;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SSL&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;SwAV&#65292;&#22312;ECG&#34920;&#24449;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#23545;&#19981;&#21516;&#31181;&#31867;&#30340;ECG&#25968;&#25454;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#26377;&#26395;&#22312;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#20154;&#32676;&#20013;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a systematic investigation into the effectiveness of Self-Supervised Learning (SSL) methods for Electrocardiogram (ECG) arrhythmia detection. We begin by conducting a novel distribution analysis on three popular ECG-based arrhythmia datasets: PTB-XL, Chapman, and Ribeiro. To the best of our knowledge, our study is the first to quantify these distributions in this area. We then perform a comprehensive set of experiments using different augmentations and parameters to evaluate the effectiveness of various SSL methods, namely SimCRL, BYOL, and SwAV, for ECG representation learning, where we observe the best performance achieved by SwAV. Furthermore, our analysis shows that SSL methods achieve highly competitive results to those achieved by supervised state-of-the-art methods. To further assess the performance of these methods on both In-Distribution (ID) and Out-of-Distribution (OOD) ECG data, we conduct cross-dataset training and testing experiments. Our comprehensive
&lt;/p&gt;</description></item><item><title>ERM++&#26159;&#19968;&#20010;&#29992;&#20110;&#22495;&#36890;&#29992;&#24615;&#30340;&#25913;&#36827;&#22522;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#21644;&#26435;&#37325;&#31354;&#38388;&#27491;&#21017;&#21270;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27604;&#26631;&#20934;ERM&#26356;&#26377;&#25928;&#65292;&#21516;&#26102;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#34920;&#29616;&#20063;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01973</link><description>&lt;p&gt;
ERM++&#65306;&#29992;&#20110;&#22495;&#36890;&#29992;&#24615;&#30340;&#25913;&#36827;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ERM++: An Improved Baseline for Domain Generalization. (arXiv:2304.01973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01973
&lt;/p&gt;
&lt;p&gt;
ERM++&#26159;&#19968;&#20010;&#29992;&#20110;&#22495;&#36890;&#29992;&#24615;&#30340;&#25913;&#36827;&#22522;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#21644;&#26435;&#37325;&#31354;&#38388;&#27491;&#21017;&#21270;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27604;&#26631;&#20934;ERM&#26356;&#26377;&#25928;&#65292;&#21516;&#26102;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#34920;&#29616;&#20063;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#22495;&#36890;&#29992;&#24615;&#65288;DG&#65289;&#34913;&#37327;&#20998;&#31867;&#22120;&#23545;&#20110;&#23427;&#27809;&#26377;&#25509;&#21463;&#36807;&#35757;&#32451;&#30340;&#26032;&#25968;&#25454;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#20010;&#35757;&#32451;&#22495;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22810;&#28304;DG&#26041;&#27861;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22495;&#26631;&#31614;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#35757;&#32451;&#36807;&#31243;&#65292;&#21363;&#22312;&#28304;&#22495;&#19978;&#31616;&#21333;&#22320;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#65292;&#21487;&#20197;&#32988;&#36807;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;DG&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#20851;&#38190;&#20505;&#36873;&#25216;&#26415;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;ERM&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#21644;&#26435;&#37325;&#31354;&#38388;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#31216;&#20026;ERM ++&#65292;&#24182;&#23637;&#31034;&#23427;&#30456;&#23545;&#20110;&#26631;&#20934;ERM&#22312;&#20116;&#20010;&#22810;&#28304;&#25968;&#25454;&#38598;&#19978;&#23558;DG&#30340;&#24615;&#33021;&#26174;&#30528;&#25552;&#39640;&#20102;5&#65285;&#20197;&#19978;&#65292;&#24182;&#19988;&#23613;&#31649;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#20294;&#20987;&#36133;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;ERM ++&#22312;WILDS-FMOW&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-source Domain Generalization (DG) measures a classifier's ability to generalize to new distributions of data it was not trained on, given several training domains. While several multi-source DG methods have been proposed, they incur additional complexity during training by using domain labels. Recent work has shown that a well-tuned Empirical Risk Minimization (ERM) training procedure, that is simply minimizing the empirical risk on the source domains, can outperform most existing DG methods. We identify several key candidate techniques to further improve ERM performance, such as better utilization of training data, model parameter selection, and weight-space regularization. We call the resulting method ERM++, and show it significantly improves the performance of DG on five multi-source datasets by over 5% compared to standard ERM, and beats state-of-the-art despite being less computationally expensive. Additionally, we demonstrate the efficacy of ERM++ on the WILDS-FMOW dataset,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#25581;&#31034;&#20102;&#20851;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#26222;&#36941;&#35823;&#35299;&#65292;&#24378;&#35843;&#38656;&#35201;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.17251</link><description>&lt;p&gt;
&#25581;&#24320;&#23545;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#35823;&#35299;
&lt;/p&gt;
&lt;p&gt;
Demystifying Misconceptions in Social Bots Research. (arXiv:2303.17251v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17251
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25581;&#31034;&#20102;&#20851;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#26222;&#36941;&#35823;&#35299;&#65292;&#24378;&#35843;&#38656;&#35201;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#26426;&#22120;&#20154;&#31185;&#23398;&#23547;&#27714;&#35299;&#20915;&#32593;&#32476;&#34394;&#20551;&#20449;&#24687;&#26368;&#21463;&#20105;&#35758;&#30340;&#24418;&#24335;&#20043;&#19968;&#30340;&#30693;&#35782;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#21463;&#21040;&#26222;&#36941;&#30340;&#20559;&#35265;&#12289;&#22840;&#22823;&#30340;&#32467;&#26524;&#21644;&#35823;&#35299;&#30340;&#22256;&#25200;&#65292;&#36825;&#20123;&#37117;&#20026;&#27495;&#20041;&#12289;&#19981;&#20999;&#23454;&#38469;&#30340;&#26399;&#26395;&#21644;&#30475;&#20284;&#26080;&#27861;&#35843;&#21644;&#30340;&#21457;&#29616;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#30830;&#20445;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#37325;&#30003;&#31185;&#23398;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20462;&#35746;&#20102;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#20013;&#30340;&#19968;&#20123;&#26368;&#26032;&#32467;&#26524;&#65292;&#24378;&#35843;&#21644;&#32416;&#27491;&#20102;&#20107;&#23454;&#38169;&#35823;&#20197;&#21450;&#26041;&#27861;&#35770;&#21644;&#27010;&#24565;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25581;&#24320;&#20102;&#26222;&#36941;&#30340;&#35823;&#35299;&#65292;&#35299;&#20915;&#20102;&#26377;&#20851;&#22914;&#20309;&#35752;&#35770;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#30830;&#23450;&#24182;&#39539;&#26021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#25903;&#25345;&#32773;&#21644;&#21453;&#23545;&#32773;&#24120;&#29992;&#30340;&#35884;&#35823;&#35770;&#35777;&#65292;&#25903;&#25345;&#36825;&#31181;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The science of social bots seeks knowledge and solutions to one of the most debated forms of online misinformation. Yet, social bots research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. In this contribution we revise some recent results in social bots research, highlighting and correcting factual errors as well as methodological and conceptual issues. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss misinformation research in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ANEDL&#26694;&#26550;&#65292;&#24212;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#24212;&#23545;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.12091</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#36127;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised Learning. (arXiv:2303.12091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ANEDL&#26694;&#26550;&#65292;&#24212;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#24212;&#23545;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#26631;&#35760;&#25968;&#25454;&#12289;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#12290;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#32771;&#34385;&#21040;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#21363;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#21253;&#21547;&#26631;&#35760;&#25968;&#25454;&#20013;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#31867;&#21035;&#65288;&#24322;&#24120;&#20540;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#36866;&#24212;&#24615;&#36127;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;ANEDL&#65289;&#65292;&#20197;&#24212;&#23545;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#22914;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#21644;&#26080;&#27861;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#20316;&#20026;&#19968;&#31181;&#24322;&#24120;&#26816;&#27979;&#22120;&#26469;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#20351;EDL&#26356;&#21152;&#36866;&#21512;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;ANEDL&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) methods assume that labeled data, unlabeled data and test data are from the same distribution. Open-set semi-supervised learning (Open-set SSL) considers a more practical scenario, where unlabeled data and test data contain new categories (outliers) not observed in labeled data (inliers). Most previous works focused on outlier detection via binary classifiers, which suffer from insufficient scalability and inability to distinguish different types of uncertainty. In this paper, we propose a novel framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these limitations. Concretely, we first introduce evidential deep learning (EDL) as an outlier detector to quantify different types of uncertainty, and design different uncertainty metrics for self-training and inference. Furthermore, we propose a novel adaptive negative optimization strategy, making EDL more tailored to the unlabeled dataset containing both inliers and outliers. As demonstrat
&lt;/p&gt;</description></item><item><title>CroSel&#26159;&#19968;&#31181;&#22788;&#29702;&#20266;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#20934;&#30830;&#35782;&#21035;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2303.10365</link><description>&lt;p&gt;
CroSel: &#29992;&#20110;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#30340;&#33258;&#20449;&#20266;&#26631;&#31614;&#30340;&#36328;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning. (arXiv:2303.10365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10365
&lt;/p&gt;
&lt;p&gt;
CroSel&#26159;&#19968;&#31181;&#22788;&#29702;&#20266;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#20934;&#30830;&#35782;&#21035;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;(PLL)&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#23427;&#20801;&#35768;&#27599;&#20010;&#35757;&#32451;&#31034;&#20363;&#26377;&#19968;&#20010;&#20505;&#36873;&#26631;&#31614;&#38598;&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;ground-truth&#26631;&#31614;&#12290;&#24050;&#32463;&#24191;&#27867;&#25506;&#32034;&#20102;&#22522;&#20110;&#35782;&#21035;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;PLL&#20013;&#30340;&#26631;&#31614;&#27495;&#20041;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#23558;&#30495;&#23454;&#26631;&#31614;&#35270;&#20026;&#35201;&#35782;&#21035;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#21644;&#23436;&#25972;&#22320;&#35782;&#21035;&#30495;&#23454;&#26631;&#31614;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#20250;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#23548;&#33268;&#20266;&#26631;&#31614;&#20013;&#30340;&#22122;&#22768;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CroSel&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#27169;&#22411;&#30340;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#26469;&#35782;&#21035;&#22823;&#22810;&#25968;&#35757;&#32451;&#31034;&#20363;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#21449;&#36873;&#25321;&#31574;&#30053;&#65292;&#20351;&#24471;&#20004;&#20010;&#28145;&#24230;&#27169;&#22411;&#21487;&#20197;&#30456;&#20114;&#36873;&#25321;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;co-mix&#65292;&#20197;&#36991;&#20813;&#22240;&#34394;&#20551;&#36873;&#25321;&#32780;&#24341;&#36215;&#30340;&#26679;&#26412;&#28010;&#36153;&#21644;&#24494;&#23567;&#22122;&#22768;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CroSel&#33021;&#22815;&#25361;&#36873;&#20986;&#22823;&#22810;&#25968;&#31034;&#20363;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial-label learning (PLL) is an important weakly supervised learning problem, which allows each training example to have a candidate label set instead of a single ground-truth label. Identification-based methods have been widely explored to tackle label ambiguity issues in PLL, which regard the true label as a latent variable to be identified. However, identifying the true labels accurately and completely remains challenging, causing noise in pseudo labels during model training. In this paper, we propose a new method called CroSel, which leverages historical prediction information from models to identify true labels for most training examples. First, we introduce a cross selection strategy, which enables two deep models to select true labels of partially labeled data for each other. Besides, we propose a novel consistent regularization term called co-mix to avoid sample waste and tiny noise caused by false selection. In this way, CroSel can pick out the true labels of most examples 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#12290;&#36890;&#36807;&#25910;&#38598;&#34987;&#32534;&#36753;&#22270;&#20687;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#25429;&#25417;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#38480;&#21046;&#25152;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09618</link><description>&lt;p&gt;
HIVE&#65306;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
HIVE: Harnessing Human Feedback for Instructional Visual Editing. (arXiv:2303.09618v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#12290;&#36890;&#36807;&#25910;&#38598;&#34987;&#32534;&#36753;&#22270;&#20687;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#25429;&#25417;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#38480;&#21046;&#25152;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#20154;&#31867;&#21453;&#39304;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#23545;&#40784;&#21040;&#20154;&#31867;&#20559;&#22909;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20551;&#35774;&#65292;&#26368;&#20808;&#36827;&#30340;&#25351;&#23548;&#24615;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#65292;&#20854;&#36755;&#20986;&#22522;&#20110;&#36755;&#20837;&#22270;&#20687;&#21644;&#32534;&#36753;&#25351;&#20196;&#65292;&#21516;&#26679;&#21487;&#20197;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#21463;&#30410;&#65292;&#22240;&#20026;&#20854;&#36755;&#20986;&#21487;&#33021;&#19981;&#31526;&#21512;&#29992;&#25143;&#30340;&#27491;&#30830;&#25351;&#20196;&#21644;&#20559;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#65288;HIVE&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#32534;&#36753;&#30340;&#22270;&#20687;&#19978;&#25910;&#38598;&#20154;&#31867;&#21453;&#39304;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#20197;&#25429;&#25417;&#22522;&#30784;&#29992;&#25143;&#20559;&#22909;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#21487;&#25193;&#23637;&#30340;&#25193;&#25955;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#26681;&#25454;&#20272;&#35745;&#30340;&#22870;&#21169;&#20540;&#34701;&#20837;&#20154;&#31867;&#20559;&#22909;&#12290;&#27492;&#22806;&#65292;&#20026;&#20943;&#36731;&#25968;&#25454;&#38480;&#21046;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;1M&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;3.6K&#22870;&#21169;&#25968;&#25454;&#38598;&#20197;&#29992;&#20110;&#22870;&#21169;&#23398;&#20064;&#65292;&#20197;&#21450;1K&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#25351;&#23548;&#24615;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of inst
&lt;/p&gt;</description></item><item><title>CrystalBox&#26159;&#19968;&#31181;&#35299;&#37322;DRL&#32593;&#32476;&#25511;&#21046;&#22120;&#34892;&#20026;&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20351;&#29992;&#26410;&#26469;&#20851;&#38190;&#32593;&#32476;&#24615;&#33021;&#25351;&#26631;&#30340;&#24433;&#21709;&#26469;&#29983;&#25104;&#31616;&#26126;&#32780;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2302.13483</link><description>&lt;p&gt;
CrystalBox&#65306;&#22522;&#20110;&#26410;&#26469;&#30340;DRL&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
CrystalBox: Future-Based Explanations for DRL Network Controllers. (arXiv:2302.13483v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13483
&lt;/p&gt;
&lt;p&gt;
CrystalBox&#26159;&#19968;&#31181;&#35299;&#37322;DRL&#32593;&#32476;&#25511;&#21046;&#22120;&#34892;&#20026;&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20351;&#29992;&#26410;&#26469;&#20851;&#38190;&#32593;&#32476;&#24615;&#33021;&#25351;&#26631;&#30340;&#24433;&#21709;&#26469;&#29983;&#25104;&#31616;&#26126;&#32780;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#19981;&#36275;&#26159;&#38480;&#21046;&#39640;&#25928;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#25511;&#21046;&#22120;&#23454;&#38469;&#37319;&#29992;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#32593;&#32476;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#36804;&#20170;&#20351;&#29992;&#24341;&#20154;&#27880;&#30446;&#30340;&#36755;&#20837;&#29305;&#24449;&#26469;&#35299;&#37322;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#33021;&#23436;&#20840;&#35299;&#37322;&#25511;&#21046;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36890;&#24120;&#65292;&#36816;&#33829;&#21830;&#26377;&#20852;&#36259;&#20102;&#35299;&#25511;&#21046;&#22120;&#23545;&#26410;&#26469;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32780;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#25429;&#25417;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CrystalBox&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20851;&#38190;&#32593;&#32476;&#24615;&#33021;&#25351;&#26631;&#30340;&#26410;&#26469;&#24433;&#21709;&#26469;&#35299;&#37322;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#12290;CrystalBox&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#31616;&#27905;&#32780;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;DRL&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#22870;&#21169;&#32452;&#20214;&#20316;&#20026;&#35299;&#37322;&#30340;&#22522;&#30784;&#65292;&#36825;&#26159;&#36816;&#33829;&#21830;&#26377;&#24847;&#20041;&#30340;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#12290;CrystalBox&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lack of explainability is a key factor limiting the practical adoption of high-performant Deep Reinforcement Learning (DRL) controllers. Explainable RL for networking hitherto used salient input features to interpret a controller's behavior. However, these feature-based solutions do not completely explain the controller's decision-making process. Often, operators are interested in understanding the impact of a controller's actions on performance in the future, which feature-based solutions cannot capture.  In this paper, we present CrystalBox, a framework that explains a controller's behavior in terms of the future impact on key network performance metrics. CrystalBox employs a novel learning-based approach to generate succinct and expressive explanations. We use reward components of the DRL network controller, which are key performance metrics meaningful to operators, as the basis for explanations. CrystalBox is generalizable and can work across both discrete and continuous control en
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#24724;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26356;&#21152;&#40065;&#26834;&#65292;&#20197;&#24212;&#23545;&#35266;&#27979;&#20013;&#30340;&#23545;&#25239;&#24615;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2302.06912</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#24724;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Regret-Based Optimization for Robust Reinforcement Learning. (arXiv:2302.06912v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#24724;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26356;&#21152;&#40065;&#26834;&#65292;&#20197;&#24212;&#23545;&#35266;&#27979;&#20013;&#30340;&#23545;&#25239;&#24615;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#23545;&#35266;&#27979;&#20013;&#30340;&#24494;&#23567;&#23545;&#25239;&#24615;&#22122;&#22768;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#36825;&#31181;&#23545;&#25239;&#24615;&#22122;&#22768;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#21487;&#33021;&#36896;&#25104;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#29616;&#26377;&#30340;&#20351;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#35266;&#27979;&#25200;&#21160;&#30340;&#23545;&#25239;&#31574;&#30053;&#20027;&#35201;&#38598;&#20013;&#22312;&#36845;&#20195;&#25913;&#36827;&#27599;&#20010;&#36845;&#20195;&#20013;&#29983;&#25104;&#30340;&#23545;&#25239;&#31034;&#20363;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#23545;&#26222;&#36890;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#20294;&#23427;&#20204;&#26159;&#34987;&#21160;&#24615;&#30340;&#65292;&#22914;&#26524;&#26576;&#20123;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#20135;&#29983;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#34920;&#29616;&#24471;&#26356;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36861;&#27714;&#19968;&#31181;&#26356;&#31215;&#26497;&#30340;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#30452;&#25509;&#20248;&#21270;&#19968;&#20010;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#40065;&#26834;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable to small adversarial noise in observations. Such adversarial noise can have disastrous consequences in safety-critical environments. For instance, a self-driving car receiving adversarially perturbed sensory observations about nearby signs (e.g., a stop sign physically altered to be perceived as a speed limit sign) or objects (e.g., cars altered to be recognized as trees) can be fatal. Existing approaches for making RL algorithms robust to an observation-perturbing adversary have focused on reactive approaches that iteratively improve against adversarial examples generated at each iteration. While such approaches have been shown to provide improvements over regular RL methods, they are reactive and can fare significantly worse if certain categories of adversarial examples are not generated during training. To that end, we pursue a more proactive approach that relies on directly optimizing a well-studied robustn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#35299;&#37322;&#30340;&#21435;&#20559;&#35265;(B2T)&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#22270;&#20687;&#26631;&#39064;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#27604;&#36739;&#20851;&#38190;&#35789;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#35782;&#21035;&#21644;&#20943;&#32531;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#20559;&#35265;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2301.11104</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#35299;&#37322;&#30340;&#21435;&#20559;&#35265;: &#36890;&#36807;&#35821;&#35328;&#35299;&#37322;&#28040;&#38500;&#26410;&#30693;&#30340;&#35270;&#35273;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Bias-to-Text: Debiasing Unknown Visual Biases through Language Interpretation. (arXiv:2301.11104v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#35299;&#37322;&#30340;&#21435;&#20559;&#35265;(B2T)&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#22270;&#20687;&#26631;&#39064;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#27604;&#36739;&#20851;&#38190;&#35789;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#35782;&#21035;&#21644;&#20943;&#32531;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#20559;&#35265;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#22312;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26102;&#26500;&#25104;&#37325;&#35201;&#38382;&#39064;&#65292;&#20294;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35786;&#26029;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21435;&#20559;&#35265;(B2T)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#35821;&#35328;&#35299;&#37322;&#26469;&#35782;&#21035;&#21644;&#32531;&#35299;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20363;&#22914;&#22270;&#35937;&#20998;&#31867;&#22120;&#21644;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#35270;&#35273;&#20559;&#24046;&#30340;&#35821;&#35328;&#25551;&#36848;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#24418;&#24335;&#65292;&#20351;&#24471;&#33021;&#22815;&#21457;&#29616;&#26032;&#30340;&#20559;&#35265;&#24182;&#26377;&#25928;&#22320;&#23545;&#27169;&#22411;&#36827;&#34892;&#21435;&#20559;&#35265;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#34987;&#35823;&#39044;&#27979;&#25110;&#29983;&#25104;&#30340;&#22270;&#20687;&#26631;&#39064;&#20013;&#30340;&#24120;&#35265;&#20851;&#38190;&#35789;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#36890;&#36807;&#27604;&#36739;&#20559;&#35265;&#20851;&#38190;&#35789;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#36991;&#20813;&#26631;&#39064;&#20013;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;B2T&#26694;&#26550;&#20013;&#30340;&#20559;&#35265;&#20851;&#38190;&#35789;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21435;&#20559;&#35265;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biases in models pose a critical issue when deploying machine learning systems, but diagnosing them in an explainable manner can be challenging. To address this, we introduce the bias-to-text (B2T) framework, which uses language interpretation to identify and mitigate biases in vision models, such as image classifiers and text-to-image generative models. Our language descriptions of visual biases provide explainable forms that enable the discovery of novel biases and effective model debiasing. To achieve this, we analyze common keywords in the captions of mispredicted or generated images. Here, we propose novel score functions to avoid biases in captions by comparing the similarities between bias keywords and those images. Additionally, we present strategies to debias zero-shot classifiers and text-to-image diffusion models using the bias keywords from the B2T framework. We demonstrate the effectiveness of our framework on various image classification and generation tasks. For classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;16&#20010;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#21644;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;2.3&#65285;&#33267;26.7&#65285;&#30340;&#25991;&#31456;&#23558;&#20027;&#39064;&#24402;&#22240;&#20110;&#30005;&#25253;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2301.10856</link><description>&lt;p&gt;
&#37096;&#20998;&#21160;&#21592;&#65306;&#36319;&#36394;&#20420;&#32599;&#26031;&#23186;&#20307;&#21644;&#30005;&#25253;&#20043;&#38388;&#30340;&#22810;&#35821;&#35328;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;16&#20010;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#21644;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;2.3&#65285;&#33267;26.7&#65285;&#30340;&#25991;&#31456;&#23558;&#20027;&#39064;&#24402;&#22240;&#20110;&#30005;&#25253;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20420;&#32599;&#26031;&#20837;&#20405;&#20044;&#20811;&#20848;&#21518;&#65292;&#38024;&#23545;&#20420;&#32599;&#26031;&#22312;&#32447;&#23186;&#20307;&#30340;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#65292;&#21253;&#25324;&#20420;&#32599;&#26031;&#20043;&#22768;&#21644;&#21355;&#26143;&#26032;&#38395;&#22312;&#20869;&#30340;&#20420;&#32599;&#26031;&#23186;&#20307;&#22312;&#27431;&#27954;&#36973;&#21040;&#31105;&#27490;&#12290;&#20026;&#20102;&#20445;&#25345;&#35266;&#20247;&#25968;&#37327;&#65292;&#35768;&#22810;&#20420;&#32599;&#26031;&#23186;&#20307;&#24320;&#22987;&#22312;&#30005;&#25253;&#31561;&#28040;&#24687;&#26381;&#21153;&#19978;&#22823;&#21147;&#23459;&#20256;&#20854;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;2022&#24180;&#26399;&#38388;16&#23478;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#22914;&#20309;&#19982;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20114;&#21160;&#21644;&#21033;&#29992;&#12290;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;MPNet&#12289;DP-means&#32858;&#31867;&#21644;Hawkes&#36807;&#31243;&#65292;&#25105;&#20204;&#36319;&#36394;&#26032;&#38395;&#32593;&#31449;&#21644;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#21465;&#20107;&#20256;&#25773;&#24773;&#20917;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20182;&#20204;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#12290;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#32593;&#31449;&#20013;&#65292;2.3&#65285;&#65288;ura.news&#65289;&#33267;26.7&#65285;&#65288;ukraina.ru&#65289;&#30340;&#25991;&#31456;&#35752;&#35770;&#20102;&#28304;&#20110;/&#23548;&#33268;&#30005;&#25253;&#27963;&#21160;&#30340;&#20869;&#23481;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36319;&#36394;&#20010;&#21035;&#20027;&#39064;&#30340;&#25193;&#25955;&#65292;&#25105;&#20204;&#27979;&#37327;&#26032;&#38395;&#32593;&#31449;&#21457;&#34920;&#25991;&#31456;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to disinformation and propaganda from Russian online media following the Russian invasion of Ukraine, Russian outlets including Russia Today and Sputnik News were banned throughout Europe. To maintain viewership, many of these Russian outlets began to heavily promote their content on messaging services like Telegram. In this work, we study how 16 Russian media outlets interacted with and utilized 732 Telegram channels throughout 2022. Leveraging the foundational model MPNet, DP-means clustering, and Hawkes Processes, we trace how narratives spread between news sites and Telegram channels. We show that news outlets not only propagate existing narratives through Telegram, but that they source material from the messaging platform. Across the sites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of articles discuss content that originated/resulted from activity on Telegram. Finally, tracking the spread of individual topics, we measure the rate at which news website
&lt;/p&gt;</description></item></channel></rss>